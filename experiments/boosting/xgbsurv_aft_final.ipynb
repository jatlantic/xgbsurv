{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to the xgbsurv package\n",
    "\n",
    "This notebook introduces `xgbsurv` using a specific dataset. It structured by the following steps:\n",
    "\n",
    "- Load data\n",
    "- Load model\n",
    "- Fit model\n",
    "- Predict and evaluate model\n",
    "\n",
    "The syntax conveniently follows that of sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgbsurv.datasets import (load_metabric, load_flchain, load_rgbsg, load_support, load_tcga)\n",
    "from xgbsurv import XGBSurv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import uniform as scuniform\n",
    "from scipy.stats import randint as scrandint\n",
    "from scipy.stats import loguniform as scloguniform \n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold, train_test_split\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, LabelBinarizer, OneHotEncoder\n",
    "from sklearn.compose import make_column_transformer, make_column_selector\n",
    "from sklearn.decomposition import PCA\n",
    "# import models\n",
    "from xgbsurv.models.eh_aft_final import aft_likelihood, get_cumulative_hazard_function_aft\n",
    "from pycox.evaluation import EvalSurv\n",
    "from xgbsurv.models.utils import sort_X_y_pandas, transform_back, transform\n",
    "from xgbsurv.preprocessing.dataset_preprocessing import discretizer_df\n",
    "from sklearn.utils.fixes import loguniform\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "'estimator__reg_alpha': scloguniform(1e-10,1),#[1e-10,1], # from hyp augmentation, L1 regularization\n",
    "'estimator__reg_lambda': scloguniform(1e-10,1), #[1e-10,1], #alias l2_regularization, lambda in augmentation\n",
    "'estimator__learning_rate': scloguniform(0.01,1.0), #[0.001,1], # assumed alias eta from augmentation,\n",
    "'estimator__n_estimators':  scrandint(1,500),#00), # corresponds to num_rounds\n",
    "'estimator__gamma': loguniform(0.01,1-0.5),#[0.1,1], # minimum loss reduction required to make a further partition on a leaf node of the tree.\n",
    "'estimator__colsample_bylevel': scuniform(0.1, 1-0.1), #[0.1,1], # from hyp augmentation\n",
    "'estimator__colsample_bynode': scuniform(0.1, 1-0.1), #[0.1,1], # from hyp augmentation, uniform(0.1,1),\n",
    "'estimator__colsample_bytree': scuniform(0.5, 1-0.5),#[0.5,1], # from hyp augmentation, seems to exceed the bound, uniform(0.5,1)\n",
    "'estimator__max_depth': scrandint(1,20),#[1,20], # from hyp augmentation\n",
    "'estimator__max_delta_step': scrandint(0,10),#[0,10], # from hyp augmentation\n",
    "'estimator__min_child_weight' : scloguniform(0.1,20-0.1),#[0.1,20], # from hyp augmentation\n",
    "'estimator__subsample': scuniform(0.01,1-0.01),#[0.01,1], # from hyp augmentation\n",
    "}\n",
    "n_outer_splits = 5\n",
    "n_inner_splits = 5\n",
    "rand_state = 42\n",
    "n_iter = 50 \n",
    "early_stopping_rounds=10\n",
    "base_score = 0.0\n",
    "validation_size = 0.2\n",
    "model = 'aft_'\n",
    "# if only n_estimators it works well for flchain\n",
    "# now testing all but gamma and l1:\n",
    "# 0   0.657119\t0.122939\n",
    "# 1\t0.764429\t0.109948\n",
    "# 2\t0.758871\t0.109038\n",
    "# 3\t0.758103\t0.109116\n",
    "# 4\t0.746572\t0.102472"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scoring_function = make_scorer(aft_likelihood, greater_is_better=False) #changed here\n",
    "def custom_scoring_function(y_true, y_pred):\n",
    "\n",
    "        if not isinstance(y_true, np.ndarray):\n",
    "            y_true = y_true.values\n",
    "        if not isinstance(y_pred, np.ndarray):\n",
    "            y_pred = y_pred.values\n",
    "        # change order of this later\n",
    "        score = aft_likelihood(y_true, y_pred)\n",
    "        return score\n",
    "\n",
    "scoring_function = make_scorer(custom_scoring_function, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSplit(StratifiedKFold):\n",
    "    def __init__(self, n_splits=5, shuffle=True, random_state=None):\n",
    "        super().__init__(n_splits=n_splits, shuffle=shuffle, random_state=random_state)\n",
    "\n",
    "    def split(self, X, y, groups=None):\n",
    "        try:\n",
    "            if y.shape[1]>1:\n",
    "                y = y[:,0]\n",
    "        except:\n",
    "            pass\n",
    "        bins = np.sign(y)\n",
    "        return super().split(X, bins, groups=groups)\n",
    "\n",
    "    def get_n_splits(self, X=None, y=None, groups=None):\n",
    "        return self.n_splits\n",
    "\n",
    "outer_custom_cv = CustomSplit(n_splits=n_outer_splits, shuffle=True, random_state=rand_state)\n",
    "inner_custom_cv = CustomSplit(n_splits=n_outer_splits, shuffle=True, random_state=rand_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set Basic Elements\n",
    "\n",
    "ct = make_column_transformer(\n",
    "        (StandardScaler(), make_column_selector(dtype_include=['float32'])),\n",
    "        #(OneHotEncoder(sparse_output=False, handle_unknown='infrequent_if_exist'), make_column_selector(dtype_include=['category', 'object'])),\n",
    "        remainder='passthrough')\n",
    "\n",
    "estimator = XGBSurv(\n",
    "    objective='aft_objective',\n",
    "    eval_metric='aft_loss',\n",
    "    random_state=rand_state, \n",
    "    disable_default_eval_metric=True,\n",
    "    early_stopping_rounds=early_stopping_rounds, \n",
    "    base_score=base_score,\n",
    "                    )\n",
    "pipe = Pipeline([('scaler',ct),\n",
    "                ('estimator', estimator)])\n",
    "    \n",
    "rs = RandomizedSearchCV(pipe, param_grid, scoring = scoring_function, n_jobs=-1, \n",
    "                             cv=inner_custom_cv, n_iter=n_iter, refit=True, \n",
    "                             random_state=rand_state, verbose=1,\n",
    "                             error_score = 'raise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set Basic Elements\n",
    "# no onehot following Kadra et al.\n",
    "\n",
    "ct = make_column_transformer(\n",
    "        (StandardScaler(), make_column_selector(dtype_include=['float32'])),\n",
    "        #(OneHotEncoder(sparse_output=False, handle_unknown='infrequent_if_exist'), make_column_selector(dtype_include=['category', 'object'])),\n",
    "        remainder='passthrough')\n",
    "\n",
    "estimator = XGBSurv(\n",
    "    objective='aft_objective',\n",
    "    eval_metric='aft_loss',\n",
    "    random_state=rand_state, \n",
    "    disable_default_eval_metric=True,\n",
    "    early_stopping_rounds=early_stopping_rounds, \n",
    "    base_score=base_score,\n",
    "                    )\n",
    "pipe = Pipeline([('scaler',ct),\n",
    "                    ('pca', PCA()),\n",
    "                    ('estimator', estimator)])\n",
    "    \n",
    "rs = RandomizedSearchCV(pipe, param_grid, scoring = scoring_function, n_jobs=-1, \n",
    "                             cv=inner_custom_cv, n_iter=n_iter, refit=True, \n",
    "                             random_state=rand_state, verbose=1,\n",
    "                             error_score = 'raise') # state not preserved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving results\n",
    "metrics_sum = {}\n",
    "agg_metrics_cindex = []\n",
    "agg_metrics_ibs = []"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metabric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "Concordance Index Test 0.6372167771120194\n",
      "Integrated Brier Score Test 0.16794372891930448\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "Concordance Index Test 0.6044956636315365\n",
      "Integrated Brier Score Test 0.1789291972666372\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "Concordance Index Test 0.6360446444501715\n",
      "Integrated Brier Score Test 0.17064991565117915\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "Concordance Index Test 0.654949178401336\n",
      "Integrated Brier Score Test 0.16311283482906555\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "Concordance Index Test 0.6505592572583389\n",
      "Integrated Brier Score Test 0.1627950482572053\n"
     ]
    }
   ],
   "source": [
    "data = load_metabric(path=\"/Users/JUSC/Documents/xgbsurv/xgbsurv/datasets/data/\", as_frame=True)\n",
    "filename = data.filename\n",
    "dataset_name = filename.split('_')[0]\n",
    "outer_scores = {'cindex_train_'+dataset_name:[], 'cindex_test_'+dataset_name:[]}\n",
    "best_params = {'best_params_'+dataset_name:[]}\n",
    "best_model = {'best_model_'+dataset_name:[]}\n",
    "outer_scores = {'cindex_test_'+dataset_name:[],'ibs_test_'+dataset_name:[]}\n",
    "X  = data.data \n",
    "y = data.target \n",
    "\n",
    "for i, (train_index, test_index) in enumerate(outer_custom_cv.split(X, y)):\n",
    "        # Split data into training and testing sets for outer fold\n",
    "\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        np.savetxt('splits/'+model+'train_index_'+str(i)+'_'+filename, train_index, delimiter=',')\n",
    "        np.savetxt('splits/'+model+'test_index_'+str(i)+'_'+filename, test_index, delimiter=',')\n",
    "        \n",
    "        X_train, y_train = sort_X_y_pandas(X_train, y_train)\n",
    "        X_test, y_test = sort_X_y_pandas(X_test, y_test)\n",
    "        rs.fit(X_train, y_train, estimator__eval_test_size=validation_size, estimator__verbose=0)\n",
    "        best_params['best_params_'+dataset_name] += [rs.best_params_]\n",
    "        best_model['best_model_'+dataset_name] += [rs.best_estimator_.get_params]\n",
    "        best_preds_train = rs.best_estimator_.predict(X_train)\n",
    "        best_preds_test = rs.best_estimator_.predict(X_test)\n",
    "\n",
    "        np.savetxt('predictions/'+model+'best_preds_train_'+str(i)+'_'+filename, best_preds_train, delimiter=',')\n",
    "        np.savetxt('predictions/'+model+'best_preds_test_'+str(i)+'_'+filename, best_preds_test, delimiter=',')\n",
    "\n",
    "        cum_hazard_test = get_cumulative_hazard_function_aft(\n",
    "                X_train.values, X_test.values, y_train.values, y_test.values,\n",
    "                best_preds_train, best_preds_test\n",
    "                )\n",
    "        df_survival_test = np.exp(-cum_hazard_test)\n",
    "        durations_test, events_test = transform_back(y_test.values)\n",
    "        time_grid_test = np.linspace(durations_test.min(), durations_test.max(), 100)\n",
    "        ev = EvalSurv(df_survival_test, durations_test, events_test, censor_surv='km')\n",
    "        print('Concordance Index Test',ev.concordance_td('antolini'))\n",
    "        print('Integrated Brier Score Test',ev.integrated_brier_score(time_grid_test))\n",
    "        cindex_score_test = ev.concordance_td('antolini')\n",
    "        ibs_score_test = ev.integrated_brier_score(time_grid_test)\n",
    "        outer_scores['cindex_test_'+dataset_name] += [cindex_score_test]\n",
    "        outer_scores['ibs_test_'+dataset_name] += [ibs_score_test]\n",
    "df_best_params = pd.DataFrame(best_params)\n",
    "df_best_model = pd.DataFrame(best_model)\n",
    "df_outer_scores = pd.DataFrame(outer_scores)\n",
    "df_metrics = pd.concat([df_best_params,df_best_model,df_outer_scores], axis=1)\n",
    "df_metrics.to_csv('metrics/'+model+'metric_summary_'+filename, index=False)\n",
    "# cindex\n",
    "df_agg_metrics_cindex = pd.DataFrame({'dataset':[dataset_name],\n",
    "                                        'cindex_test_mean':df_outer_scores['cindex_test_'+dataset_name].mean(),\n",
    "                                        'cindex_test_std':df_outer_scores['cindex_test_'+dataset_name].std() })\n",
    "# IBS\n",
    "df_agg_metrics_ibs = pd.DataFrame({'dataset':[dataset_name],\n",
    "                                        'ibs_test_mean':df_outer_scores['ibs_test_'+dataset_name].mean(),\n",
    "                                        'ibs_test_std':df_outer_scores['ibs_test_'+dataset_name].std() })\n",
    "agg_metrics_cindex.append(df_agg_metrics_cindex)\n",
    "agg_metrics_ibs.append(df_agg_metrics_ibs)\n",
    "metrics_sum[model+dataset_name] = df_metrics\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "Concordance Index Test 0.7948719391007613\n",
      "Integrated Brier Score Test 0.09663385874241122\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "Concordance Index Test 0.7946433710556365\n",
      "Integrated Brier Score Test 0.09539319087916771\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "Concordance Index Test 0.7855243924805985\n",
      "Integrated Brier Score Test 0.09787810392450727\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
     ]
    }
   ],
   "source": [
    "data = load_flchain(path=\"/Users/JUSC/Documents/xgbsurv/xgbsurv/datasets/data/\", as_frame=True)\n",
    "filename = data.filename\n",
    "dataset_name = filename.split('_')[0]\n",
    "outer_scores = {'cindex_train_'+dataset_name:[], 'cindex_test_'+dataset_name:[]}\n",
    "best_params = {'best_params_'+dataset_name:[]}\n",
    "best_model = {'best_model_'+dataset_name:[]}\n",
    "outer_scores = {'cindex_test_'+dataset_name:[],'ibs_test_'+dataset_name:[]}\n",
    "X  = data.data \n",
    "y = data.target \n",
    "\n",
    "for i, (train_index, test_index) in enumerate(outer_custom_cv.split(X, y)):\n",
    "        # Split data into training and testing sets for outer fold\n",
    "\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        np.savetxt('splits/'+model+'train_index_'+str(i)+'_'+filename, train_index, delimiter=',')\n",
    "        np.savetxt('splits/'+model+'test_index_'+str(i)+'_'+filename, test_index, delimiter=',')\n",
    "        \n",
    "        X_train, y_train = sort_X_y_pandas(X_train, y_train)\n",
    "        X_test, y_test = sort_X_y_pandas(X_test, y_test)\n",
    "        rs.fit(X_train, y_train, estimator__eval_test_size=validation_size, estimator__verbose=0)\n",
    "        best_params['best_params_'+dataset_name] += [rs.best_params_]\n",
    "        best_model['best_model_'+dataset_name] += [rs.best_estimator_.get_params]\n",
    "        best_preds_train = rs.best_estimator_.predict(X_train)\n",
    "        best_preds_test = rs.best_estimator_.predict(X_test)\n",
    "\n",
    "        np.savetxt('predictions/'+model+'best_preds_train_'+str(i)+'_'+filename, best_preds_train, delimiter=',')\n",
    "        np.savetxt('predictions/'+model+'best_preds_test_'+str(i)+'_'+filename, best_preds_test, delimiter=',')\n",
    "\n",
    "        cum_hazard_test = get_cumulative_hazard_function_aft(\n",
    "                X_train.values, X_test.values, y_train.values, y_test.values,\n",
    "                best_preds_train, best_preds_test\n",
    "                )\n",
    "        df_survival_test = np.exp(-cum_hazard_test)\n",
    "        durations_test, events_test = transform_back(y_test.values)\n",
    "        time_grid_test = np.linspace(durations_test.min(), durations_test.max(), 100)\n",
    "        ev = EvalSurv(df_survival_test, durations_test, events_test, censor_surv='km')\n",
    "        print('Concordance Index Test',ev.concordance_td('antolini'))\n",
    "        print('Integrated Brier Score Test',ev.integrated_brier_score(time_grid_test))\n",
    "        cindex_score_test = ev.concordance_td('antolini')\n",
    "        ibs_score_test = ev.integrated_brier_score(time_grid_test)\n",
    "        outer_scores['cindex_test_'+dataset_name] += [cindex_score_test]\n",
    "        outer_scores['ibs_test_'+dataset_name] += [ibs_score_test]\n",
    "df_best_params = pd.DataFrame(best_params)\n",
    "df_best_model = pd.DataFrame(best_model)\n",
    "df_outer_scores = pd.DataFrame(outer_scores)\n",
    "df_metrics = pd.concat([df_best_params,df_best_model,df_outer_scores], axis=1)\n",
    "df_metrics.to_csv('metrics/'+model+'metric_summary_'+filename, index=False)\n",
    "# cindex\n",
    "df_agg_metrics_cindex = pd.DataFrame({'dataset':[dataset_name],\n",
    "                                        'cindex_test_mean':df_outer_scores['cindex_test_'+dataset_name].mean(),\n",
    "                                        'cindex_test_std':df_outer_scores['cindex_test_'+dataset_name].std() })\n",
    "# IBS\n",
    "df_agg_metrics_ibs = pd.DataFrame({'dataset':[dataset_name],\n",
    "                                        'ibs_test_mean':df_outer_scores['ibs_test_'+dataset_name].mean(),\n",
    "                                        'ibs_test_std':df_outer_scores['ibs_test_'+dataset_name].std() })\n",
    "agg_metrics_cindex.append(df_agg_metrics_cindex)\n",
    "agg_metrics_ibs.append(df_agg_metrics_ibs)\n",
    "metrics_sum[model+dataset_name] = df_metrics\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RGBSG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Concordance Index Test 0.6053689464454337\n",
      "Integrated Brier Score Test 0.20176212889495945\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Concordance Index Test 0.0\n",
      "Integrated Brier Score Test 0.4148120690987607\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Concordance Index Test 0.620001335202617\n",
      "Integrated Brier Score Test 0.20235223350294743\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Concordance Index Test 0.5951169614256103\n",
      "Integrated Brier Score Test 0.19900222744356932\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Concordance Index Test 0.538160600315112\n",
      "Integrated Brier Score Test 0.2025100392405161\n"
     ]
    }
   ],
   "source": [
    "data = load_rgbsg(path=\"/Users/JUSC/Documents/xgbsurv/xgbsurv/datasets/data/\", as_frame=True)\n",
    "filename = data.filename\n",
    "dataset_name = filename.split('_')[0]\n",
    "outer_scores = {'cindex_train_'+dataset_name:[], 'cindex_test_'+dataset_name:[]}\n",
    "best_params = {'best_params_'+dataset_name:[]}\n",
    "best_model = {'best_model_'+dataset_name:[]}\n",
    "outer_scores = {'cindex_test_'+dataset_name:[],'ibs_test_'+dataset_name:[]}\n",
    "X  = data.data \n",
    "y = data.target \n",
    "\n",
    "for i, (train_index, test_index) in enumerate(outer_custom_cv.split(X, y)):\n",
    "        # Split data into training and testing sets for outer fold\n",
    "\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        np.savetxt('splits/'+model+'train_index_'+str(i)+'_'+filename, train_index, delimiter=',')\n",
    "        np.savetxt('splits/'+model+'test_index_'+str(i)+'_'+filename, test_index, delimiter=',')\n",
    "        \n",
    "        X_train, y_train = sort_X_y_pandas(X_train, y_train)\n",
    "        X_test, y_test = sort_X_y_pandas(X_test, y_test)\n",
    "        rs.fit(X_train, y_train, estimator__eval_test_size=validation_size, estimator__verbose=0)\n",
    "        best_params['best_params_'+dataset_name] += [rs.best_params_]\n",
    "        best_model['best_model_'+dataset_name] += [rs.best_estimator_.get_params]\n",
    "        best_preds_train = rs.best_estimator_.predict(X_train)\n",
    "        best_preds_test = rs.best_estimator_.predict(X_test)\n",
    "\n",
    "        np.savetxt('predictions/'+model+'best_preds_train_'+str(i)+'_'+filename, best_preds_train, delimiter=',')\n",
    "        np.savetxt('predictions/'+model+'best_preds_test_'+str(i)+'_'+filename, best_preds_test, delimiter=',')\n",
    "\n",
    "        cum_hazard_test = get_cumulative_hazard_function_aft(\n",
    "                X_train.values, X_test.values, y_train.values, y_test.values,\n",
    "                best_preds_train, best_preds_test\n",
    "                )\n",
    "        df_survival_test = np.exp(-cum_hazard_test)\n",
    "        durations_test, events_test = transform_back(y_test.values)\n",
    "        time_grid_test = np.linspace(durations_test.min(), durations_test.max(), 100)\n",
    "        ev = EvalSurv(df_survival_test, durations_test, events_test, censor_surv='km')\n",
    "        print('Concordance Index Test',ev.concordance_td('antolini'))\n",
    "        print('Integrated Brier Score Test',ev.integrated_brier_score(time_grid_test))\n",
    "        cindex_score_test = ev.concordance_td('antolini')\n",
    "        ibs_score_test = ev.integrated_brier_score(time_grid_test)\n",
    "        outer_scores['cindex_test_'+dataset_name] += [cindex_score_test]\n",
    "        outer_scores['ibs_test_'+dataset_name] += [ibs_score_test]\n",
    "df_best_params = pd.DataFrame(best_params)\n",
    "df_best_model = pd.DataFrame(best_model)\n",
    "df_outer_scores = pd.DataFrame(outer_scores)\n",
    "df_metricsmodel = pd.concat([df_best_params,df_best_model,df_outer_scores], axis=1)\n",
    "df_metrics.to_csv('metrics/'+model+'metric_summary_'+filename, index=False)\n",
    "# cindex\n",
    "df_agg_metrics_cindex = pd.DataFrame({'dataset':[dataset_name],\n",
    "                                        'cindex_test_mean':df_outer_scores['cindex_test_'+dataset_name].mean(),\n",
    "                                        'cindex_test_std':df_outer_scores['cindex_test_'+dataset_name].std() })\n",
    "# IBS\n",
    "df_agg_metrics_ibs = pd.DataFrame({'dataset':[dataset_name],\n",
    "\n",
    "                                        'ibs_test_mean':df_outer_scores['ibs_test_'+dataset_name].mean(),\n",
    "                                        'ibs_test_std':df_outer_scores['ibs_test_'+dataset_name].std() })\n",
    "agg_metrics_cindex.append(df_agg_metrics_cindex)\n",
    "agg_metrics_ibs.append(df_agg_metrics_ibs)\n",
    "metrics_sum[model+dataset_name] = df_metrics\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SUPPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Concordance Index Test 0.3221784336008367\n",
      "Integrated Brier Score Test 0.21679043899496458\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Concordance Index Test 0.35969525685577086\n",
      "Integrated Brier Score Test 0.2157368723879445\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Concordance Index Test 0.3434770945504766\n",
      "Integrated Brier Score Test 0.21414484557213184\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Concordance Index Test 0.2998233387583636\n",
      "Integrated Brier Score Test 0.21456145570239665\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Concordance Index Test 0.31727501837501526\n",
      "Integrated Brier Score Test 0.21480511482061643\n"
     ]
    }
   ],
   "source": [
    "data = load_support(path=\"/Users/JUSC/Documents/xgbsurv/xgbsurv/datasets/data/\", as_frame=True)\n",
    "filename = data.filename\n",
    "dataset_name = filename.split('_')[0]\n",
    "outer_scores = {'cindex_train_'+dataset_name:[], 'cindex_test_'+dataset_name:[]}\n",
    "best_params = {'best_params_'+dataset_name:[]}\n",
    "best_model = {'best_model_'+dataset_name:[]}\n",
    "outer_scores = {'cindex_test_'+dataset_name:[],'ibs_test_'+dataset_name:[]}\n",
    "X  = data.data \n",
    "y = data.target \n",
    "\n",
    "for i, (train_index, test_index) in enumerate(outer_custom_cv.split(X, y)):\n",
    "        # Split data into training and testing sets for outer fold\n",
    "\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        np.savetxt('splits/'+model+'train_index_'+str(i)+'_'+filename, train_index, delimiter=',')\n",
    "        np.savetxt('splits/'+model+'test_index_'+str(i)+'_'+filename, test_index, delimiter=',')\n",
    "        \n",
    "        X_train, y_train = sort_X_y_pandas(X_train, y_train)\n",
    "        X_test, y_test = sort_X_y_pandas(X_test, y_test)\n",
    "        rs.fit(X_train, y_train, estimator__eval_test_size=validation_size, estimator__verbose=0)\n",
    "        best_params['best_params_'+dataset_name] += [rs.best_params_]\n",
    "        best_model['best_model_'+dataset_name] += [rs.best_estimator_.get_params]\n",
    "        best_preds_train = rs.best_estimator_.predict(X_train)\n",
    "        best_preds_test = rs.best_estimator_.predict(X_test)\n",
    "\n",
    "        np.savetxt('predictions/'+model+'best_preds_train_'+str(i)+'_'+filename, best_preds_train, delimiter=',')\n",
    "        np.savetxt('predictions/'+model+'best_preds_test_'+str(i)+'_'+filename, best_preds_test, delimiter=',')\n",
    "\n",
    "        cum_hazard_test = get_cumulative_hazard_function_aft(\n",
    "                X_train.values, X_test.values, y_train.values, y_test.values,\n",
    "                best_preds_train, best_preds_test\n",
    "                )\n",
    "        df_survival_test = np.exp(-cum_hazard_test)\n",
    "        durations_test, events_test = transform_back(y_test.values)\n",
    "        time_grid_test = np.linspace(durations_test.min(), durations_test.max(), 100)\n",
    "        ev = EvalSurv(df_survival_test, durations_test, events_test, censor_surv='km')\n",
    "        print('Concordance Index Test',ev.concordance_td('antolini'))\n",
    "        print('Integrated Brier Score Test',ev.integrated_brier_score(time_grid_test))\n",
    "        cindex_score_test = ev.concordance_td('antolini')\n",
    "        ibs_score_test = ev.integrated_brier_score(time_grid_test)\n",
    "        outer_scores['cindex_test_'+dataset_name] += [cindex_score_test]\n",
    "        outer_scores['ibs_test_'+dataset_name] += [ibs_score_test]\n",
    "df_best_params = pd.DataFrame(best_params)\n",
    "df_best_model = pd.DataFrame(best_model)\n",
    "df_outer_scores = pd.DataFrame(outer_scores)\n",
    "df_metricsmodel = pd.concat([df_best_params,df_best_model,df_outer_scores], axis=1)\n",
    "df_metrics.to_csv('metrics/'+model+'metric_summary_'+filename, index=False)\n",
    "# cindex\n",
    "df_agg_metrics_cindex = pd.DataFrame({'dataset':[dataset_name],\n",
    "                                        'cindex_test_mean':df_outer_scores['cindex_test_'+dataset_name].mean(),\n",
    "                                        'cindex_test_std':df_outer_scores['cindex_test_'+dataset_name].std() })\n",
    "# IBS\n",
    "df_agg_metrics_ibs = pd.DataFrame({'dataset':[dataset_name],\n",
    "\n",
    "                                        'ibs_test_mean':df_outer_scores['ibs_test_'+dataset_name].mean(),\n",
    "                                        'ibs_test_std':df_outer_scores['ibs_test_'+dataset_name].std() })\n",
    "agg_metrics_cindex.append(df_agg_metrics_cindex)\n",
    "agg_metrics_ibs.append(df_agg_metrics_ibs)\n",
    "metrics_sum[model+dataset_name] = df_metrics\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>cindex_test_mean</th>\n",
       "      <th>cindex_test_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>METABRIC</td>\n",
       "      <td>0.5550</td>\n",
       "      <td>0.0238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FLCHAIN</td>\n",
       "      <td>0.6065</td>\n",
       "      <td>0.0621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RGBSG</td>\n",
       "      <td>0.4717</td>\n",
       "      <td>0.2655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SUPPORT</td>\n",
       "      <td>0.3285</td>\n",
       "      <td>0.0234</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    dataset  cindex_test_mean  cindex_test_std\n",
       "0  METABRIC            0.5550           0.0238\n",
       "0   FLCHAIN            0.6065           0.0621\n",
       "0     RGBSG            0.4717           0.2655\n",
       "0   SUPPORT            0.3285           0.0234"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final_aft_1_cindex = pd.concat([df for df in agg_metrics_cindex]).round(4)\n",
    "df_final_aft_1_cindex.to_csv('metrics/final_gbdt_aft_1_cindex.csv', index=False)\n",
    "df_final_aft_1_cindex.to_csv('/Users/JUSC/Documents/644928e0fb7e147893e8ec15/05_thesis/tables/final_gbdt_aft_1_cindex.csv', index=False)  #\n",
    "df_final_aft_1_cindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>ibs_test_mean</th>\n",
       "      <th>ibs_test_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>METABRIC</td>\n",
       "      <td>0.1840</td>\n",
       "      <td>0.0074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FLCHAIN</td>\n",
       "      <td>0.1326</td>\n",
       "      <td>0.0025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RGBSG</td>\n",
       "      <td>0.2441</td>\n",
       "      <td>0.0954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SUPPORT</td>\n",
       "      <td>0.2152</td>\n",
       "      <td>0.0011</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    dataset  ibs_test_mean  ibs_test_std\n",
       "0  METABRIC         0.1840        0.0074\n",
       "0   FLCHAIN         0.1326        0.0025\n",
       "0     RGBSG         0.2441        0.0954\n",
       "0   SUPPORT         0.2152        0.0011"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final_aft_1_ibs = pd.concat([df for df in agg_metrics_ibs]).round(4)\n",
    "df_final_aft_1_ibs.to_csv('metrics/final_gbdt_aft_1_ibs.csv', index=False)\n",
    "df_final_aft_1_ibs.to_csv('/Users/JUSC/Documents/644928e0fb7e147893e8ec15/05_thesis/tables/final_gbdt_aft_1_ibs.csv', index=False) \n",
    "df_final_aft_1_ibs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TCGA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "'estimator__reg_alpha': scloguniform(1e-10,1),#[1e-10,1], # from hyp augmentation, L1 regularization\n",
    "'estimator__reg_lambda': scloguniform(1e-10,1), #[1e-10,1], #alias l2_regularization, lambda in augmentation\n",
    "'estimator__learning_rate': scloguniform(0.01,1.0), #[0.001,1], # assumed alias eta from augmentation,\n",
    "'estimator__n_estimators':  scrandint(1,500),#00), # corresponds to num_rounds\n",
    "'estimator__gamma': loguniform(0.01,1-0.5),#[0.1,1], # minimum loss reduction required to make a further partition on a leaf node of the tree.\n",
    "'estimator__colsample_bylevel': scuniform(0.1, 1-0.1), #[0.1,1], # from hyp augmentation\n",
    "'estimator__colsample_bynode': scuniform(0.1, 1-0.1), #[0.1,1], # from hyp augmentation, uniform(0.1,1),\n",
    "'estimator__colsample_bytree': scuniform(0.5, 1-0.5),#[0.5,1], # from hyp augmentation, seems to exceed the bound, uniform(0.5,1)\n",
    "'estimator__max_depth': scrandint(1,20),#[1,20], # from hyp augmentation\n",
    "'estimator__max_delta_step': scrandint(0,10),#[0,10], # from hyp augmentation\n",
    "'estimator__min_child_weight' : scloguniform(0.1,20-0.1),#[0.1,20], # from hyp augmentation\n",
    "'estimator__subsample': scuniform(0.01,1-0.01),#[0.01,1], # from hyp augmentation\n",
    "'pca__n_components': [8, 16, 32, 64]\n",
    "}\n",
    "n_outer_splits = 5\n",
    "n_inner_splits = 5\n",
    "rand_state = 42\n",
    "n_iter = 50 #0\n",
    "early_stopping_rounds=10\n",
    "base_score = 0.0\n",
    "validation_size = 0.2\n",
    "model = 'aft_'\n",
    "data = 'tcga'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_types = ['BLCA',\n",
    "    'BRCA',\n",
    "    'HNSC',\n",
    "    'KIRC',\n",
    "    'LGG',\n",
    "    'LIHC',\n",
    "    'LUAD',\n",
    "    'LUSC',\n",
    "    'OV',\n",
    "    'STAD']\n",
    "agg_metrics_cindex = []\n",
    "agg_metrics_ibs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLCA_adapted.csv\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Concordance Index Test 0.287292817679558\n",
      "Integrated Brier Score Test 0.22784199742966532\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Concordance Index Test 0.2599672310212998\n",
      "Integrated Brier Score Test 0.22521729522927544\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Concordance Index Test 0.18040621266427717\n",
      "Integrated Brier Score Test 0.2237839845058751\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Concordance Index Test 0.2829324169530355\n",
      "Integrated Brier Score Test 0.22326588318606544\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Concordance Index Test 0.2850678733031674\n",
      "Integrated Brier Score Test 0.21147425014035265\n",
      "BRCA_adapted.csv\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Concordance Index Test 0.030381944444444444\n",
      "Integrated Brier Score Test 0.2078867308889194\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Concordance Index Test 0.0\n",
      "Integrated Brier Score Test 0.19692860447218416\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Concordance Index Test 0.015298507462686567\n",
      "Integrated Brier Score Test 0.2018385415292158\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Concordance Index Test 0.0\n",
      "Integrated Brier Score Test 0.18362636611861696\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Concordance Index Test 0.0\n",
      "Integrated Brier Score Test 0.217200535602329\n",
      "HNSC_adapted.csv\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Concordance Index Test 0.15092366678285116\n",
      "Integrated Brier Score Test 0.20792649510311298\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Concordance Index Test 0.3115727002967359\n",
      "Integrated Brier Score Test 0.19706447066499402\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Concordance Index Test 0.3424812030075188\n",
      "Integrated Brier Score Test 0.2047435020353431\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Concordance Index Test 0.524904214559387\n",
      "Integrated Brier Score Test 0.21244018284000796\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Concordance Index Test 0.345679012345679\n",
      "Integrated Brier Score Test 0.20713809220689758\n",
      "KIRC_adapted.csv\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Concordance Index Test 0.223982869379015\n",
      "Integrated Brier Score Test 0.20557470067689265\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Concordance Index Test 0.10341685649202734\n",
      "Integrated Brier Score Test 0.21398489597247208\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Concordance Index Test 0.3725859632595384\n",
      "Integrated Brier Score Test 0.19296966799826457\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Concordance Index Test 0.1303579319487406\n",
      "Integrated Brier Score Test 0.20710202396909724\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Concordance Index Test 0.2047753945770943\n",
      "Integrated Brier Score Test 0.19562785119713655\n",
      "LGG_adapted.csv\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Concordance Index Test 0.6021717670286278\n",
      "Integrated Brier Score Test 0.20028846293862618\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Concordance Index Test 0.30446194225721784\n",
      "Integrated Brier Score Test 0.20642518458736978\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Concordance Index Test 0.5717171717171717\n",
      "Integrated Brier Score Test 0.18082198737974967\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Concordance Index Test 0.5323554788610871\n",
      "Integrated Brier Score Test 0.19707426885155466\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Concordance Index Test 0.4663793103448276\n",
      "Integrated Brier Score Test 0.19212526751081882\n",
      "LIHC_adapted.csv\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Concordance Index Test 0.16474291710388247\n",
      "Integrated Brier Score Test 0.2149079829603628\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Concordance Index Test 0.13583815028901733\n",
      "Integrated Brier Score Test 0.19972659661643258\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Concordance Index Test 0.0\n",
      "Integrated Brier Score Test 0.2287720587097402\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Concordance Index Test 0.0\n",
      "Integrated Brier Score Test 0.20946236630542908\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Concordance Index Test 0.1439084219133279\n",
      "Integrated Brier Score Test 0.22842552679824285\n",
      "LUAD_adapted.csv\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Concordance Index Test 0.49185336048879835\n",
      "Integrated Brier Score Test 0.20421049704067382\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Concordance Index Test 0.4195617316942811\n",
      "Integrated Brier Score Test 0.20196790622522595\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Concordance Index Test 0.3038461538461538\n",
      "Integrated Brier Score Test 0.19268984014127838\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Concordance Index Test 0.3768518518518518\n",
      "Integrated Brier Score Test 0.21518915144767053\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Concordance Index Test 0.48268839103869654\n",
      "Integrated Brier Score Test 0.18571895579194342\n",
      "LUSC_adapted.csv\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Concordance Index Test 0.364258230012826\n",
      "Integrated Brier Score Test 0.2023642262926755\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Concordance Index Test 0.12530712530712532\n",
      "Integrated Brier Score Test 0.1970540885921434\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Concordance Index Test 0.41770519982810483\n",
      "Integrated Brier Score Test 0.20602504266706423\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Concordance Index Test 0.3989283974671213\n",
      "Integrated Brier Score Test 0.19722542330725762\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Concordance Index Test 0.48066298342541436\n",
      "Integrated Brier Score Test 0.21910660293251913\n",
      "OV_adapted.csv\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Concordance Index Test 0.5351299326275265\n",
      "Integrated Brier Score Test 0.1448680083167166\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Concordance Index Test 0.2392894461859979\n",
      "Integrated Brier Score Test 0.12562015228814283\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Concordance Index Test 0.19533527696793002\n",
      "Integrated Brier Score Test 0.151554107650587\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Concordance Index Test 0.4383697813121272\n",
      "Integrated Brier Score Test 0.15108163584553466\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Concordance Index Test 0.42602996254681647\n",
      "Integrated Brier Score Test 0.166540595391334\n",
      "STAD_adapted.csv\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Concordance Index Test 0.2943189596167009\n",
      "Integrated Brier Score Test 0.21338204341623102\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Concordance Index Test 0.4638643067846608\n",
      "Integrated Brier Score Test 0.21545090065609213\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Concordance Index Test 0.07524752475247524\n",
      "Integrated Brier Score Test 0.21407574870859275\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Concordance Index Test 0.39944903581267216\n",
      "Integrated Brier Score Test 0.20704604529544582\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Concordance Index Test 0.25629139072847684\n",
      "Integrated Brier Score Test 0.23122689509064864\n"
     ]
    }
   ],
   "source": [
    "for idx, cancer_type in enumerate(cancer_types):\n",
    "        data = load_tcga(path=\"/Users/JUSC/Documents/xgbsurv/xgbsurv/datasets/data/\", cancer_type=cancer_type, as_frame=True)\n",
    "        filename = data.filename\n",
    "        dataset_name = filename.split('_')[0]\n",
    "        outer_scores = {'cindex_train_'+dataset_name:[], 'cindex_test_'+dataset_name:[]}\n",
    "        best_params = {'best_params_'+dataset_name:[]}\n",
    "        best_model = {'best_model_'+dataset_name:[]}\n",
    "        outer_scores = {'cindex_test_'+dataset_name:[],'ibs_test_'+dataset_name:[]}\n",
    "        X  = data.data \n",
    "        y = data.target \n",
    "\n",
    "        for i, (train_index, test_index) in enumerate(outer_custom_cv.split(X, y)):\n",
    "                # Split data into training and testing sets for outer fold\n",
    "\n",
    "                X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "                y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "                \n",
    "                np.savetxt('splits/'+model+'train_index_'+str(i)+'_'+filename, train_index, delimiter=',')\n",
    "                np.savetxt('splits/'+model+'test_index_'+str(i)+'_'+filename, test_index, delimiter=',')\n",
    "                \n",
    "                X_train, y_train = sort_X_y_pandas(X_train, y_train)\n",
    "                X_test, y_test = sort_X_y_pandas(X_test, y_test)\n",
    "                rs.fit(X_train, y_train, estimator__eval_test_size=validation_size, estimator__verbose=0)\n",
    "                best_params['best_params_'+dataset_name] += [rs.best_params_]\n",
    "                best_model['best_model_'+dataset_name] += [rs.best_estimator_.get_params]\n",
    "                best_preds_train = rs.best_estimator_.predict(X_train)\n",
    "                best_preds_test = rs.best_estimator_.predict(X_test)\n",
    "\n",
    "                np.savetxt('predictions/'+model+'best_preds_train_'+str(i)+'_'+filename, best_preds_train, delimiter=',')\n",
    "                np.savetxt('predictions/'+model+'best_preds_test_'+str(i)+'_'+filename, best_preds_test, delimiter=',')\n",
    "\n",
    "                cum_hazard_test = get_cumulative_hazard_function_aft(\n",
    "                        X_train.values, X_test.values, y_train.values, y_test.values,\n",
    "                        best_preds_train, best_preds_test\n",
    "                        )\n",
    "                df_survival_test = np.exp(-cum_hazard_test)\n",
    "                durations_test, events_test = transform_back(y_test.values)\n",
    "                time_grid_test = np.linspace(durations_test.min(), durations_test.max(), 100)\n",
    "                ev = EvalSurv(df_survival_test, durations_test, events_test, censor_surv='km')\n",
    "                print('Concordance Index Test',ev.concordance_td('antolini'))\n",
    "                print('Integrated Brier Score Test',ev.integrated_brier_score(time_grid_test))\n",
    "                cindex_score_test = ev.concordance_td('antolini')\n",
    "                ibs_score_test = ev.integrated_brier_score(time_grid_test)\n",
    "                outer_scores['cindex_test_'+dataset_name] += [cindex_score_test]\n",
    "                outer_scores['ibs_test_'+dataset_name] += [ibs_score_test]\n",
    "        df_best_params = pd.DataFrame(best_params)\n",
    "        df_best_model = pd.DataFrame(best_model)\n",
    "        df_outer_scores = pd.DataFrame(outer_scores)\n",
    "        df_metricsmodel = pd.concat([df_best_params,df_best_model,df_outer_scores], axis=1)\n",
    "        df_metrics.to_csv('metrics/'+model+'metric_summary_'+filename, index=False)\n",
    "        # cindex\n",
    "        df_agg_metrics_cindex = pd.DataFrame({'dataset':[dataset_name],\n",
    "                                                'cindex_test_mean':df_outer_scores['cindex_test_'+dataset_name].mean(),\n",
    "                                                'cindex_test_std':df_outer_scores['cindex_test_'+dataset_name].std() })\n",
    "        # IBS\n",
    "        df_agg_metrics_ibs = pd.DataFrame({'dataset':[dataset_name],\n",
    "                                                'ibs_test_mean':df_outer_scores['ibs_test_'+dataset_name].mean(),\n",
    "                                                'ibs_test_std':df_outer_scores['ibs_test_'+dataset_name].std() })\n",
    "        agg_metrics_cindex.append(df_agg_metrics_cindex)\n",
    "        agg_metrics_ibs.append(df_agg_metrics_ibs)\n",
    "        metrics_sum[model+cancer_type] = df_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>cindex_test_mean</th>\n",
       "      <th>cindex_test_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BLCA</td>\n",
       "      <td>0.2591</td>\n",
       "      <td>0.0454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BRCA</td>\n",
       "      <td>0.0091</td>\n",
       "      <td>0.0136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HNSC</td>\n",
       "      <td>0.3351</td>\n",
       "      <td>0.1329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KIRC</td>\n",
       "      <td>0.2070</td>\n",
       "      <td>0.1053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LGG</td>\n",
       "      <td>0.4954</td>\n",
       "      <td>0.1182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LIHC</td>\n",
       "      <td>0.0889</td>\n",
       "      <td>0.0818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LUAD</td>\n",
       "      <td>0.4150</td>\n",
       "      <td>0.0780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LUSC</td>\n",
       "      <td>0.3574</td>\n",
       "      <td>0.1364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OV</td>\n",
       "      <td>0.3668</td>\n",
       "      <td>0.1437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>STAD</td>\n",
       "      <td>0.2978</td>\n",
       "      <td>0.1493</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  dataset  cindex_test_mean  cindex_test_std\n",
       "0    BLCA            0.2591           0.0454\n",
       "0    BRCA            0.0091           0.0136\n",
       "0    HNSC            0.3351           0.1329\n",
       "0    KIRC            0.2070           0.1053\n",
       "0     LGG            0.4954           0.1182\n",
       "0    LIHC            0.0889           0.0818\n",
       "0    LUAD            0.4150           0.0780\n",
       "0    LUSC            0.3574           0.1364\n",
       "0      OV            0.3668           0.1437\n",
       "0    STAD            0.2978           0.1493"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final_aft_tcga_cindex = pd.concat([df for df in agg_metrics_cindex]).round(4)\n",
    "df_final_aft_tcga_cindex.to_csv('metrics/final_gbdt_tcga_aft_cindex.csv', index=False)\n",
    "df_final_aft_tcga_cindex.to_csv('/Users/JUSC/Documents/644928e0fb7e147893e8ec15/05_thesis/tables/final_gbdt_tcga_aft_cindex.csv', index=False)  #\n",
    "df_final_aft_tcga_cindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>ibs_test_mean</th>\n",
       "      <th>ibs_test_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BLCA</td>\n",
       "      <td>0.2223</td>\n",
       "      <td>0.0063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BRCA</td>\n",
       "      <td>0.2015</td>\n",
       "      <td>0.0125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HNSC</td>\n",
       "      <td>0.2059</td>\n",
       "      <td>0.0057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KIRC</td>\n",
       "      <td>0.2031</td>\n",
       "      <td>0.0086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LGG</td>\n",
       "      <td>0.1953</td>\n",
       "      <td>0.0096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LIHC</td>\n",
       "      <td>0.2163</td>\n",
       "      <td>0.0125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LUAD</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>0.0113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LUSC</td>\n",
       "      <td>0.2044</td>\n",
       "      <td>0.0091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OV</td>\n",
       "      <td>0.1479</td>\n",
       "      <td>0.0148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>STAD</td>\n",
       "      <td>0.2162</td>\n",
       "      <td>0.0090</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  dataset  ibs_test_mean  ibs_test_std\n",
       "0    BLCA         0.2223        0.0063\n",
       "0    BRCA         0.2015        0.0125\n",
       "0    HNSC         0.2059        0.0057\n",
       "0    KIRC         0.2031        0.0086\n",
       "0     LGG         0.1953        0.0096\n",
       "0    LIHC         0.2163        0.0125\n",
       "0    LUAD         0.2000        0.0113\n",
       "0    LUSC         0.2044        0.0091\n",
       "0      OV         0.1479        0.0148\n",
       "0    STAD         0.2162        0.0090"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final_aft_tcga_ibs = pd.concat([df for df in agg_metrics_ibs]).round(4)\n",
    "df_final_aft_tcga_ibs.to_csv('metrics/final_gbdt_tcga_aft_ibs.csv', index=False)\n",
    "df_final_aft_tcga_ibs.to_csv('/Users/JUSC/Documents/644928e0fb7e147893e8ec15/05_thesis/tables/final_gbdt_tcga_aft_ibs.csv', index=False) \n",
    "df_final_aft_tcga_ibs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xgbsurv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
