{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import savetxt\n",
    "from xgbsurv.datasets import (load_metabric, load_flchain, load_rgbsg, load_support, load_tcga)\n",
    "from xgbsurv.models.utils import sort_X_y_pandas, transform_back, transform\n",
    "from xgbsurv.models.eh_ah_final import get_cumulative_hazard_function_ah\n",
    "import torch\n",
    "from torch import nn\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold, train_test_split\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, LabelBinarizer, OneHotEncoder\n",
    "from sklearn.compose import make_column_transformer, make_column_selector\n",
    "from sklearn.decomposition import PCA\n",
    "from loss_functions_pytorch import AHLoss, ah_likelihood_torch\n",
    "from skorch import NeuralNet\n",
    "from skorch.callbacks import EarlyStopping, Callback, LRScheduler\n",
    "import skorch.callbacks\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import check_cv\n",
    "from numbers import Number\n",
    "import torch.utils.data\n",
    "from skorch.utils import flatten\n",
    "from skorch.utils import is_pandas_ndframe\n",
    "from skorch.utils import check_indexing\n",
    "from skorch.utils import multi_indexing\n",
    "from skorch.utils import to_numpy\n",
    "from skorch.dataset import get_len\n",
    "from skorch.dataset import ValidSplit\n",
    "from pycox.evaluation import EvalSurv\n",
    "from scipy.stats import uniform as scuniform\n",
    "from scipy.stats import randint as scrandint\n",
    "from scipy.stats import loguniform as scloguniform\n",
    "import random\n",
    "import os\n",
    "#torch.set_default_dtype(torch.float64)\n",
    "#torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "# highest prediction with -loss function 8.7\n",
    "#highest prediction without -loss function 8.7"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters, put into function\n",
    "n_outer_splits = 5\n",
    "n_inner_splits = 5\n",
    "rand_state = 42\n",
    "n_iter = 1\n",
    "early_stopping_rounds=10\n",
    "base_score = 0.0\n",
    "\n",
    "# set seed for scipy/numpy\n",
    "np.random.seed(rand_state)\n",
    "\n",
    "param_grid_ah = {\n",
    "    'estimator__module__n_layers': [1, 2, 4],\n",
    "    'estimator__module__num_nodes': [64, 128, 256, 512],\n",
    "    'estimator__module__dropout': scuniform(0.0,0.7),\n",
    "    'estimator__optimizer__weight_decay': [0.4, 0.2, 0.1, 0.05, 0.02, 0.01, 0],\n",
    "    'estimator__batch_size': [64, 128, 256, 512, 1024],\n",
    "    #lr not in paper because of learning rate finder\n",
    "    # note: setting learning rate higher would make exp(partial_hazard) explode\n",
    "    #'estimator__lr': scloguniform(0.001,0.01), # scheduler unten einbauen\n",
    "    # use callback instead\n",
    "    'estimator__lr':[0.001]\n",
    "    #'max_epochs':  scrandint(10,20), # corresponds to num_rounds\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_torch(seed=42):\n",
    "    \"\"\"Sets all seeds within torch and adjacent libraries.\n",
    "\n",
    "    Args:\n",
    "        seed: Random seed to be used by the seeding functions.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    #os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    #torch.backends.cudnn.benchmark = False\n",
    "    #torch.backends.cudnn.deterministic = True\n",
    "    return None\n",
    "\n",
    "\n",
    "class FixSeed(Callback):\n",
    "    def __init__(self, seed):\n",
    "        self.seed = seed\n",
    "\n",
    "    def initialize(self):\n",
    "        seed_torch(self.seed)\n",
    "        return super().initialize()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Scorer\n",
    "def custom_scoring_function(y_true, y_pred):\n",
    "\n",
    "        #y_true = torch.from_numpy(y_true)\n",
    "        if isinstance(y_pred, np.ndarray):\n",
    "            y_pred = torch.from_numpy(y_pred)\n",
    "        if isinstance(y_true, np.ndarray):\n",
    "            y_true = torch.from_numpy(y_true)\n",
    "        if isinstance(y_pred, pd.Series):\n",
    "            y_pred = torch.tensor(y_pred.values)\n",
    "        if isinstance(y_true, pd.Series):\n",
    "            y_true = torch.tensor(y_true.values)\n",
    "        #print('loss function y_pred', y_pred)\n",
    "        #print('loss function y_true', y_true)\n",
    "        score = -ah_likelihood_torch(y_pred, y_true) #.to(torch.float32)\n",
    "        return score.numpy()\n",
    "\n",
    "scoring_function = make_scorer(custom_scoring_function, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set up Custom Splitter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Torch Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SurvivalModel(nn.Module):\n",
    "    def __init__(self, n_layers, input_units, num_nodes, dropout, out_features):\n",
    "        super(SurvivalModel, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.in_features = input_units\n",
    "        self.num_nodes = num_nodes\n",
    "        self.dropout = dropout\n",
    "        self.out_features = out_features\n",
    "        model = []\n",
    "        # first layer\n",
    "        model.append(torch.nn.Linear(input_units, num_nodes))\n",
    "        model.append(torch.nn.ReLU())\n",
    "        model.append(torch.nn.Dropout(dropout))\n",
    "        model.append(torch.nn.BatchNorm1d(num_nodes))\n",
    "\n",
    "        for i in range(n_layers-1):\n",
    "            model.append(torch.nn.Linear(num_nodes, num_nodes))\n",
    "            #init.kaiming_normal_(model[-1].weight, nonlinearity='relu')\n",
    "            model.append(torch.nn.ReLU())\n",
    "            model.append(torch.nn.Dropout(dropout))\n",
    "            model.append(torch.nn.BatchNorm1d(num_nodes))\n",
    "\n",
    "        # output layer\n",
    "        model.append(torch.nn.Linear(num_nodes, out_features))\n",
    "    \n",
    "        self.layers = nn.Sequential(*model)\n",
    "\n",
    "        # for layer in self.layers:\n",
    "        #     if isinstance(layer, nn.Linear):\n",
    "        #         #nn.init.uniform_(layer.weight, a=-0.5, b=0.5)\n",
    "        #         nn.init.kaiming_normal_(layer.weight)\n",
    "\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = X.to(torch.float32)\n",
    "        res = self.layers(X)\n",
    "        #print(res)\n",
    "        return res\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomStandardScaler(StandardScaler):\n",
    "    \n",
    "    def __init__(self, copy=True, with_mean=True, with_std=True):\n",
    "        super().__init__(copy=copy, with_mean=with_mean, with_std=with_std)\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return super().fit(X, y)\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X_transformed = super().transform(X, y)\n",
    "        return X_transformed.astype(np.float32)\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        X_transformed = super().fit_transform(X, y)\n",
    "        return X_transformed.astype(np.float32)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define stratified inner k-fold cross-validation\n",
    "class CustomSplit(StratifiedKFold):\n",
    "    def __init__(self, n_splits=5, shuffle=True, random_state=None):\n",
    "        super().__init__(n_splits=n_splits, shuffle=shuffle, random_state=random_state)\n",
    "\n",
    "    def split(self, X, y, groups=None):\n",
    "        print('split', X.dtypes)\n",
    "        try:\n",
    "            if y.shape[1]>1:\n",
    "                y = y[:,0]\n",
    "        except:\n",
    "            pass\n",
    "        bins = np.sign(y)\n",
    "        return super().split(X, bins, groups=groups)\n",
    "\n",
    "    def get_n_splits(self, X=None, y=None, groups=None):\n",
    "        return self.n_splits\n",
    "\n",
    "outer_custom_cv = CustomSplit(n_splits=n_outer_splits, shuffle=True, random_state=rand_state)\n",
    "inner_custom_cv = CustomSplit(n_splits=n_outer_splits, shuffle=True, random_state=rand_state)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Valid Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class CustomStandardScaler(StandardScaler):\n",
    "    \n",
    "    def __init__(self, copy=True, with_mean=True, with_std=True):\n",
    "        super().__init__(copy=copy, with_mean=with_mean, with_std=with_std)\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return super().fit(X, y)\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X_transformed = super().transform(X, y)\n",
    "        return X_transformed.astype(np.float32)\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        X_transformed = super().fit_transform(X, y)\n",
    "        return X_transformed.astype(np.float32)\n",
    "    \n",
    "class CustomValidSplit():\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            cv=5,\n",
    "            stratified=False,\n",
    "            random_state=None,\n",
    "    ):\n",
    "        self.stratified = stratified\n",
    "        self.random_state = random_state\n",
    "\n",
    "        if isinstance(cv, Number) and (cv <= 0):\n",
    "            raise ValueError(\"Numbers less than 0 are not allowed for cv \"\n",
    "                             \"but ValidSplit got {}\".format(cv))\n",
    "\n",
    "        if not self._is_float(cv) and random_state is not None:\n",
    "            raise ValueError(\n",
    "                \"Setting a random_state has no effect since cv is not a float. \"\n",
    "                \"You should leave random_state to its default (None), or set cv \"\n",
    "                \"to a float value.\",\n",
    "            )\n",
    "\n",
    "        self.cv = cv\n",
    "\n",
    "    def _is_stratified(self, cv):\n",
    "        return isinstance(cv, (StratifiedKFold, StratifiedShuffleSplit))\n",
    "\n",
    "    def _is_float(self, x):\n",
    "        if not isinstance(x, Number):\n",
    "            return False\n",
    "        return not float(x).is_integer()\n",
    "\n",
    "    def _check_cv_float(self):\n",
    "        cv_cls = StratifiedShuffleSplit if self.stratified else ShuffleSplit\n",
    "        return cv_cls(test_size=self.cv, random_state=self.random_state)\n",
    "\n",
    "    def _check_cv_non_float(self, y):\n",
    "        return check_cv(\n",
    "            self.cv,\n",
    "            y=y,\n",
    "            classifier=self.stratified,\n",
    "        )\n",
    "\n",
    "    def check_cv(self, y):\n",
    "        \"\"\"Resolve which cross validation strategy is used.\"\"\"\n",
    "        y_arr = None\n",
    "        if self.stratified:\n",
    "            # Try to convert y to numpy for sklearn's check_cv; if conversion\n",
    "            # doesn't work, still try.\n",
    "            try:\n",
    "                y_arr = to_numpy(y)\n",
    "            except (AttributeError, TypeError):\n",
    "                y_arr = y\n",
    "\n",
    "        if self._is_float(self.cv):\n",
    "            return self._check_cv_float()\n",
    "        return self._check_cv_non_float(y_arr)\n",
    "\n",
    "    def _is_regular(self, x):\n",
    "        return (x is None) or isinstance(x, np.ndarray) or is_pandas_ndframe(x)\n",
    "\n",
    "    def __call__(self, dataset, y=None, groups=None):\n",
    "        # key change here\n",
    "        y = np.sign(y)\n",
    "        bad_y_error = ValueError(\n",
    "            \"Stratified CV requires explicitly passing a suitable y.\")\n",
    "        if (y is None) and self.stratified:\n",
    "            raise bad_y_error\n",
    "\n",
    "        cv = self.check_cv(y)\n",
    "        if self.stratified and not self._is_stratified(cv):\n",
    "            raise bad_y_error\n",
    "\n",
    "        # pylint: disable=invalid-name\n",
    "        len_dataset = get_len(dataset)\n",
    "        if y is not None:\n",
    "            len_y = get_len(y)\n",
    "            if len_dataset != len_y:\n",
    "                raise ValueError(\"Cannot perform a CV split if dataset and y \"\n",
    "                                 \"have different lengths.\")\n",
    "\n",
    "        args = (np.arange(len_dataset),)\n",
    "        if self._is_stratified(cv):\n",
    "            args = args + (to_numpy(y),)\n",
    "\n",
    "        idx_train, idx_valid = next(iter(cv.split(*args, groups=groups)))\n",
    "        dataset_train = torch.utils.data.Subset(dataset, idx_train)\n",
    "        dataset_valid = torch.utils.data.Subset(dataset, idx_valid)\n",
    "        return dataset_train, dataset_valid\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Training Procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_eval(X, y, net, n_iter, filename):\n",
    "        model = 'ah_'\n",
    "        dataset_name = filename.split('_')[0]\n",
    "        # add IBS later\n",
    "        outer_scores = {'cindex_train_'+dataset_name:[], 'cindex_test_'+dataset_name:[],\n",
    "                        'ibs_train_'+dataset_name:[], 'ibs_test_'+dataset_name:[]}\n",
    "        best_params = {'best_params_'+dataset_name:[]}\n",
    "        best_model = {'best_model_'+dataset_name:[]}\n",
    "        ct = make_column_transformer(\n",
    "                #(OneHotEncoder(sparse_output=False), make_column_selector(dtype_include=['category', 'object'])),\n",
    "                (StandardScaler(), make_column_selector(dtype_include=['float32'],dtype_exclude=['category', 'object']))\n",
    "                ,remainder='passthrough')\n",
    "\n",
    "        pipe = Pipeline([('scaler',ct),\n",
    "                        ('estimator', net)])\n",
    "        rs = RandomizedSearchCV(pipe, param_grid_ah, scoring = scoring_function, n_jobs=-1, \n",
    "                                    n_iter=n_iter, refit=True, random_state=rand_state)\n",
    "        \n",
    "        for i, (train_index, test_index) in enumerate(outer_custom_cv.split(X, y)):\n",
    "                # Split data into training and testing sets for outer fold\n",
    "                X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "                y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "                X_train, y_train = sort_X_y_pandas(X_train, y_train)\n",
    "                X_test, y_test = sort_X_y_pandas(X_test, y_test)\n",
    "\n",
    "                # print(X_train.shape, type(X_train))\n",
    "                # print(y_train.shape, type(y_train))\n",
    "                # print(X_test.shape, type(X_test))\n",
    "                # print(y_test.shape, type(y_test))\n",
    "                # save splits and data\n",
    "                savetxt('splits/'+model+'train_index_'+str(i)+'_'+filename, train_index, delimiter=',')\n",
    "                savetxt('splits/'+model+'test_index_'+str(i)+'_'+filename, test_index, delimiter=',')\n",
    "                \n",
    "                # savetxt('splits/X_train_'+str(i)+'_'+filename, X_train, delimiter=',')\n",
    "                # savetxt('splits/X_test_'+str(i)+'_'+filename, X_test, delimiter=',')\n",
    "\n",
    "                # savetxt('splits/y_train_'+str(i)+'_'+filename, y_train, delimiter=',')\n",
    "                # savetxt('splits/y_test_'+str(i)+'_'+filename, y_test, delimiter=',')\n",
    "\n",
    "\n",
    "                rs.fit(X_train, y_train)\n",
    "                best_preds_train = rs.best_estimator_.predict(X_train)\n",
    "                best_preds_test = rs.best_estimator_.predict(X_test)\n",
    "                print('max best test predictions',np.max(best_preds_test.reshape(-1)) )\n",
    "                savetxt('predictions/'+model+'best_preds_train_'+str(i)+'_'+filename, best_preds_train, delimiter=',')\n",
    "                savetxt('predictions/'+model+'best_preds_test_'+str(i)+'_'+filename, best_preds_test, delimiter=',')\n",
    "\n",
    "                # save hyperparameter settings\n",
    "                params = rs.best_estimator_.get_params\n",
    "                best_params['best_params_'+dataset_name] += [rs.best_params_]\n",
    "                best_model['best_model_'+dataset_name] += [params]\n",
    "                # remove training part, cumhazard sollte um 10000 sein ( Integralanzahl/Intervalle)\n",
    "\n",
    "                outer_scores['cindex_train_'+dataset_name] += [np.nan]\n",
    "                outer_scores['ibs_train_'+dataset_name] += [np.nan]\n",
    "                    \n",
    "                #try:\n",
    "                cum_hazard_test = get_cumulative_hazard_function_ah(\n",
    "                        X_train.values, X_test.values, y_train.values, y_test.values,\n",
    "                        best_preds_train.reshape(-1), best_preds_test.reshape(-1)\n",
    "                        )\n",
    "                df_survival_test = np.exp(-cum_hazard_test)\n",
    "                durations_test, events_test = transform_back(y_test.values)\n",
    "                time_grid_test = np.linspace(durations_test.min(), durations_test.max(), 100)\n",
    "                ev = EvalSurv(df_survival_test, durations_test, events_test, censor_surv='km')\n",
    "                print('Concordance Index',ev.concordance_td('antolini'))\n",
    "                print('Integrated Brier Score:',ev.integrated_brier_score(time_grid_test))\n",
    "                cindex_score_test = ev.concordance_td('antolini')\n",
    "                ibs_score_test = ev.integrated_brier_score(time_grid_test)\n",
    "\n",
    "                outer_scores['cindex_test_'+dataset_name] += [cindex_score_test]\n",
    "                outer_scores['ibs_test_'+dataset_name] += [ibs_score_test]\n",
    "                #except: \n",
    "                #outer_scores['cindex_test_'+dataset_name] += [np.nan]\n",
    "                #outer_scores['ibs_test_'+dataset_name] += [np.nan]\n",
    "            \n",
    "        df_best_params = pd.DataFrame(best_params)\n",
    "        df_best_model = pd.DataFrame(best_model)\n",
    "        df_outer_scores = pd.DataFrame(outer_scores)\n",
    "        df_metrics = pd.concat([df_best_params,df_best_model,df_outer_scores], axis=1)\n",
    "        df_metrics.to_csv('metrics/'+model+'metric_summary'+str(i)+'_'+filename, index=False)\n",
    "       \n",
    "        # cindex\n",
    "        df_agg_metrics_cindex = pd.DataFrame({'dataset':[dataset_name],\n",
    "                                              'cindex_train_mean':df_outer_scores['cindex_train_'+dataset_name].mean(),\n",
    "                                              'cindex_train_std':df_outer_scores['cindex_train_'+dataset_name].std(),\n",
    "                                              'cindex_test_mean':df_outer_scores['cindex_test_'+dataset_name].mean(),\n",
    "                                              'cindex_test_std':df_outer_scores['cindex_test_'+dataset_name].std() })\n",
    "        # IBS\n",
    "        df_agg_metrics_ibs = pd.DataFrame({'dataset':[dataset_name],\n",
    "                                              'ibs_train_mean':df_outer_scores['ibs_train_'+dataset_name].mean(),\n",
    "                                              'ibs_train_std':df_outer_scores['ibs_train_'+dataset_name].std(),\n",
    "                                              'ibs_test_mean':df_outer_scores['ibs_test_'+dataset_name].mean(),\n",
    "                                              'ibs_test_std':df_outer_scores['ibs_test_'+dataset_name].std() })\n",
    "\n",
    "        return df_agg_metrics_cindex, df_agg_metrics_ibs, best_model, best_params, outer_scores, best_preds_train, best_preds_test #, X_train, X_test, y_train, y_test\n",
    "\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skorch.callbacks\n",
    "class InputShapeSetter(skorch.callbacks.Callback):\n",
    "    def on_train_begin(self, net, X, y):\n",
    "        net.set_params(module__input_units=X.shape[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_metabric\n",
      "split MKI67                float32\n",
      "EGFR                 float32\n",
      "PGR                  float32\n",
      "ERBB2                float32\n",
      "hormone_treatment      uint8\n",
      "radiotherapy           uint8\n",
      "chemotherapy           uint8\n",
      "ER_positive            uint8\n",
      "age                  float32\n",
      "dtype: object\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.7910\u001b[0m        \u001b[32m2.7636\u001b[0m  0.0226\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.0535\u001b[0m        \u001b[32m3.0443\u001b[0m  0.0244\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.2682\u001b[0m        \u001b[32m3.2331\u001b[0m  0.0257\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.2854\u001b[0m        \u001b[32m3.2303\u001b[0m  0.0245\n",
      "      2        2.8277        \u001b[32m2.7540\u001b[0m  0.0250\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.3113\u001b[0m        \u001b[32m3.2298\u001b[0m  0.0262\n",
      "      2        3.0632        \u001b[32m3.0339\u001b[0m  0.0255\n",
      "      2        \u001b[36m3.2488\u001b[0m        \u001b[32m3.2156\u001b[0m  0.0265\n",
      "      2        3.2842        \u001b[32m3.2205\u001b[0m  0.0356\n",
      "      2        3.3358        \u001b[32m3.2163\u001b[0m  0.0255\n",
      "      3        2.8181        \u001b[32m2.7384\u001b[0m  0.0264\n",
      "      3        \u001b[36m3.0311\u001b[0m        \u001b[32m3.0158\u001b[0m  0.0251\n",
      "      3        3.2752        \u001b[32m3.1945\u001b[0m  0.0235\n",
      "      4        \u001b[36m2.7744\u001b[0m        \u001b[32m2.7280\u001b[0m  0.0216\n",
      "      3        \u001b[36m3.2741\u001b[0m        \u001b[32m3.1936\u001b[0m  0.0231\n",
      "      4        3.0497        \u001b[32m2.9963\u001b[0m  0.0235\n",
      "      4        3.2577        \u001b[32m3.1725\u001b[0m  0.0234\n",
      "      5        2.7864        \u001b[32m2.7186\u001b[0m  0.0216\n",
      "      4        \u001b[36m3.2691\u001b[0m        \u001b[32m3.1710\u001b[0m  0.0225\n",
      "      5        \u001b[36m3.0290\u001b[0m        \u001b[32m2.9760\u001b[0m  0.0217\n",
      "      3        \u001b[36m3.2524\u001b[0m        \u001b[32m3.2008\u001b[0m  0.0655\n",
      "      6        \u001b[36m2.7702\u001b[0m        \u001b[32m2.7142\u001b[0m  0.0224\n",
      "      5        3.2656        \u001b[32m3.1537\u001b[0m  0.0245\n",
      "      5        3.3097        \u001b[32m3.1498\u001b[0m  0.0223\n",
      "      6        3.0483        \u001b[32m2.9629\u001b[0m  0.0216\n",
      "      7        \u001b[36m2.7522\u001b[0m        2.7175  0.0237\n",
      "      6        \u001b[36m3.2282\u001b[0m        \u001b[32m3.1414\u001b[0m  0.0241\n",
      "      6        \u001b[36m3.2579\u001b[0m        \u001b[32m3.1356\u001b[0m  0.0223\n",
      "      7        \u001b[36m3.0171\u001b[0m        \u001b[32m2.9577\u001b[0m  0.0213\n",
      "      8        \u001b[36m2.7502\u001b[0m        2.7211  0.0218\n",
      "      7        \u001b[36m3.2576\u001b[0m        \u001b[32m3.1299\u001b[0m  0.0225\n",
      "      7        3.2544        \u001b[32m3.1366\u001b[0m  0.0239\n",
      "      8        3.0289        2.9586  0.0227\n",
      "      4        \u001b[36m3.2492\u001b[0m        \u001b[32m3.1833\u001b[0m  0.0617\n",
      "      9        2.7515        2.7245  0.0344\n",
      "      8        3.2448        \u001b[32m3.1351\u001b[0m  0.0324\n",
      "      9        \u001b[36m2.9786\u001b[0m        2.9629  0.0299\n",
      "      8        \u001b[36m3.2521\u001b[0m        \u001b[32m3.1279\u001b[0m  0.0365\n",
      "      5        3.2655        \u001b[32m3.1713\u001b[0m  0.0386\n",
      "      9        \u001b[36m3.2063\u001b[0m        3.1375  0.0260\n",
      "     10        \u001b[36m2.9740\u001b[0m        2.9701  0.0246\n",
      "Restoring best model from epoch 6.\n",
      "     10        \u001b[36m2.7445\u001b[0m        2.7301  0.0293\n",
      "Restoring best model from epoch 4.\n",
      "      9        \u001b[36m3.2310\u001b[0m        3.1311  0.0276\n",
      "      6        \u001b[36m3.2264\u001b[0m        \u001b[32m3.1633\u001b[0m  0.0263\n",
      "     10        \u001b[36m3.1897\u001b[0m        3.1440  0.0244\n",
      "Restoring best model from epoch 5.\n",
      "     10        \u001b[36m3.2146\u001b[0m        3.1381  0.0230\n",
      "Restoring best model from epoch 5.\n",
      "      7        \u001b[36m3.1890\u001b[0m        \u001b[32m3.1586\u001b[0m  0.0210\n",
      "      8        3.2012        \u001b[32m3.1566\u001b[0m  0.0212\n",
      "      9        3.1905        3.1577  0.0198\n",
      "     10        \u001b[36m3.1698\u001b[0m        3.1601  0.0189\n",
      "Restoring best model from epoch 6.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.2049\u001b[0m        \u001b[32m3.1454\u001b[0m  0.0433\n",
      "      2        \u001b[36m3.1817\u001b[0m        \u001b[32m3.1089\u001b[0m  0.0427\n",
      "      3        \u001b[36m3.1815\u001b[0m        \u001b[32m3.0842\u001b[0m  0.0434\n",
      "      4        \u001b[36m3.1524\u001b[0m        3.0848  0.0445\n",
      "      5        \u001b[36m3.1152\u001b[0m        3.0997  0.0441\n",
      "      6        3.1192        3.1153  0.0436\n",
      "      7        3.1167        3.1134  0.0437\n",
      "      8        \u001b[36m3.1043\u001b[0m        3.0989  0.0427\n",
      "      9        \u001b[36m3.0832\u001b[0m        \u001b[32m3.0682\u001b[0m  0.0438\n",
      "     10        \u001b[36m3.0759\u001b[0m        \u001b[32m3.0374\u001b[0m  0.0429\n",
      "max best test predictions 0.34879765\n",
      "0.0333241953125\n",
      "integration times 14346\n",
      "Concordance Index 0.37659219872227295\n",
      "Integrated Brier Score: 0.18885412027288148\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.8537\u001b[0m        \u001b[32m2.7793\u001b[0m  0.0239\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.0797\u001b[0m        \u001b[32m3.0204\u001b[0m  0.0269\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.3117\u001b[0m        \u001b[32m3.2235\u001b[0m  0.0286\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.2931\u001b[0m        \u001b[32m3.2104\u001b[0m  0.0244\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.3400\u001b[0m        \u001b[32m3.2448\u001b[0m  0.0261\n",
      "      2        \u001b[36m2.8390\u001b[0m        \u001b[32m2.7705\u001b[0m  0.0262\n",
      "      2        \u001b[36m3.0741\u001b[0m        \u001b[32m3.0096\u001b[0m  0.0232\n",
      "      2        3.3071        \u001b[32m3.1956\u001b[0m  0.0252\n",
      "      2        \u001b[36m3.3016\u001b[0m        \u001b[32m3.2125\u001b[0m  0.0278\n",
      "      3        2.8631        \u001b[32m2.7596\u001b[0m  0.0266\n",
      "      2        \u001b[36m3.3194\u001b[0m        \u001b[32m3.2304\u001b[0m  0.0289\n",
      "      3        \u001b[36m3.0612\u001b[0m        \u001b[32m2.9925\u001b[0m  0.0261\n",
      "      3        \u001b[36m3.2824\u001b[0m        \u001b[32m3.1753\u001b[0m  0.0232\n",
      "      3        \u001b[36m3.2625\u001b[0m        \u001b[32m3.1896\u001b[0m  0.0248\n",
      "      4        \u001b[36m2.8105\u001b[0m        \u001b[32m2.7500\u001b[0m  0.0322\n",
      "      4        \u001b[36m3.2777\u001b[0m        \u001b[32m3.1552\u001b[0m  0.0275\n",
      "      4        \u001b[36m3.0426\u001b[0m        \u001b[32m2.9723\u001b[0m  0.0361\n",
      "      3        3.3595        \u001b[32m3.2087\u001b[0m  0.0408\n",
      "      4        3.2711        \u001b[32m3.1647\u001b[0m  0.0339\n",
      "      5        2.8252        \u001b[32m2.7400\u001b[0m  0.0246\n",
      "      5        3.2902        \u001b[32m3.1359\u001b[0m  0.0383\n",
      "      5        3.0844        \u001b[32m2.9508\u001b[0m  0.0389\n",
      "      4        \u001b[36m3.2589\u001b[0m        \u001b[32m3.1835\u001b[0m  0.0409\n",
      "      5        \u001b[36m3.2384\u001b[0m        \u001b[32m3.1418\u001b[0m  0.0326\n",
      "      6        \u001b[36m2.8058\u001b[0m        \u001b[32m2.7366\u001b[0m  0.0307\n",
      "      6        \u001b[36m3.2769\u001b[0m        \u001b[32m3.1255\u001b[0m  0.0236\n",
      "      6        \u001b[36m3.0416\u001b[0m        \u001b[32m2.9387\u001b[0m  0.0241\n",
      "      5        3.3199        \u001b[32m3.1605\u001b[0m  0.0236\n",
      "      6        3.2500        \u001b[32m3.1253\u001b[0m  0.0259\n",
      "      7        \u001b[36m2.8026\u001b[0m        \u001b[32m2.7358\u001b[0m  0.0254\n",
      "      7        \u001b[36m3.2377\u001b[0m        \u001b[32m3.1200\u001b[0m  0.0233\n",
      "      7        \u001b[36m3.0137\u001b[0m        \u001b[32m2.9322\u001b[0m  0.0273\n",
      "      6        3.2839        \u001b[32m3.1472\u001b[0m  0.0290\n",
      "      8        \u001b[36m2.7801\u001b[0m        \u001b[32m2.7336\u001b[0m  0.0294\n",
      "      7        3.2388        \u001b[32m3.1166\u001b[0m  0.0306\n",
      "      8        3.2702        \u001b[32m3.1172\u001b[0m  0.0326\n",
      "      8        3.0263        \u001b[32m2.9291\u001b[0m  0.0342\n",
      "      7        3.2594        \u001b[32m3.1412\u001b[0m  0.0333\n",
      "      8        \u001b[36m3.2353\u001b[0m        \u001b[32m3.1141\u001b[0m  0.0270\n",
      "      9        2.7848        \u001b[32m2.7332\u001b[0m  0.0284\n",
      "      9        \u001b[36m3.2278\u001b[0m        3.1207  0.0232\n",
      "      9        3.0152        2.9305  0.0244\n",
      "     10        \u001b[36m2.7642\u001b[0m        \u001b[32m2.7313\u001b[0m  0.0227\n",
      "Restoring best model from epoch 4.\n",
      "      8        3.2616        \u001b[32m3.1364\u001b[0m  0.0259\n",
      "      9        \u001b[36m3.2234\u001b[0m        3.1165  0.0245\n",
      "     10        \u001b[36m3.2135\u001b[0m        3.1262  0.0240\n",
      "Restoring best model from epoch 5.\n",
      "     10        \u001b[36m3.0036\u001b[0m        2.9338  0.0299\n",
      "Restoring best model from epoch 6.\n",
      "      9        3.2645        3.1389  0.0323\n",
      "     10        \u001b[36m3.2135\u001b[0m        3.1237  0.0315\n",
      "Restoring best model from epoch 5.\n",
      "     10        \u001b[36m3.2367\u001b[0m        3.1451  0.0199\n",
      "Restoring best model from epoch 5.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.2112\u001b[0m        \u001b[32m3.1521\u001b[0m  0.0422\n",
      "      2        \u001b[36m3.1785\u001b[0m        \u001b[32m3.1119\u001b[0m  0.0431\n",
      "      3        3.1888        \u001b[32m3.0768\u001b[0m  0.0429\n",
      "      4        \u001b[36m3.1377\u001b[0m        \u001b[32m3.0602\u001b[0m  0.0430\n",
      "      5        \u001b[36m3.1359\u001b[0m        3.0653  0.0425\n",
      "      6        3.1473        3.0835  0.0423\n",
      "      7        \u001b[36m3.1055\u001b[0m        3.0941  0.0420\n",
      "      8        \u001b[36m3.0957\u001b[0m        3.0866  0.0423\n",
      "      9        \u001b[36m3.0823\u001b[0m        3.0714  0.0424\n",
      "     10        \u001b[36m3.0702\u001b[0m        \u001b[32m3.0450\u001b[0m  0.0419\n",
      "max best test predictions 0.32913983\n",
      "0.0333241953125\n",
      "integration times 13776\n",
      "Concordance Index 0.3719640504238038\n",
      "Integrated Brier Score: 0.1861868893845967\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.8370\u001b[0m        \u001b[32m2.7767\u001b[0m  0.0215\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.0704\u001b[0m        \u001b[32m3.0424\u001b[0m  0.0227\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.2811\u001b[0m        \u001b[32m3.2056\u001b[0m  0.0264\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.3168\u001b[0m        \u001b[32m3.2083\u001b[0m  0.0246\n",
      "      2        2.8457        \u001b[32m2.7720\u001b[0m  0.0238\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.3093\u001b[0m        \u001b[32m3.2403\u001b[0m  0.0238\n",
      "      2        \u001b[36m3.0518\u001b[0m        \u001b[32m3.0290\u001b[0m  0.0220\n",
      "      2        3.3369        \u001b[32m3.1937\u001b[0m  0.0239\n",
      "      2        \u001b[36m3.2847\u001b[0m        \u001b[32m3.1921\u001b[0m  0.0258\n",
      "      2        3.3176        \u001b[32m3.2255\u001b[0m  0.0265\n",
      "      3        \u001b[36m2.8305\u001b[0m        \u001b[32m2.7653\u001b[0m  0.0284\n",
      "      3        \u001b[36m3.0458\u001b[0m        \u001b[32m3.0105\u001b[0m  0.0293\n",
      "      3        \u001b[36m3.2487\u001b[0m        \u001b[32m3.1741\u001b[0m  0.0274\n",
      "      3        3.3022        \u001b[32m3.1662\u001b[0m  0.0260\n",
      "      4        \u001b[36m2.8102\u001b[0m        \u001b[32m2.7607\u001b[0m  0.0236\n",
      "      3        3.3130        \u001b[32m3.2059\u001b[0m  0.0240\n",
      "      4        \u001b[36m3.0424\u001b[0m        \u001b[32m2.9940\u001b[0m  0.0251\n",
      "      4        \u001b[36m3.2157\u001b[0m        \u001b[32m3.1556\u001b[0m  0.0228\n",
      "      4        \u001b[36m3.2708\u001b[0m        \u001b[32m3.1413\u001b[0m  0.0262\n",
      "      5        2.8248        \u001b[32m2.7556\u001b[0m  0.0249\n",
      "      4        \u001b[36m3.2588\u001b[0m        \u001b[32m3.1876\u001b[0m  0.0261\n",
      "      5        3.0536        \u001b[32m2.9799\u001b[0m  0.0220\n",
      "      5        3.2290        \u001b[32m3.1364\u001b[0m  0.0242\n",
      "      6        \u001b[36m2.7903\u001b[0m        \u001b[32m2.7554\u001b[0m  0.0253\n",
      "      5        3.2901        \u001b[32m3.1194\u001b[0m  0.0313\n",
      "      6        \u001b[36m3.0378\u001b[0m        \u001b[32m2.9734\u001b[0m  0.0272\n",
      "      5        \u001b[36m3.2406\u001b[0m        \u001b[32m3.1719\u001b[0m  0.0318\n",
      "      6        3.2445        \u001b[32m3.1242\u001b[0m  0.0244\n",
      "      7        2.7953        2.7579  0.0229\n",
      "      6        3.2810        \u001b[32m3.1086\u001b[0m  0.0258\n",
      "      7        3.0438        \u001b[32m2.9729\u001b[0m  0.0242\n",
      "      6        3.3033        \u001b[32m3.1610\u001b[0m  0.0255\n",
      "      7        3.2315        \u001b[32m3.1196\u001b[0m  0.0258\n",
      "      8        2.7971        2.7591  0.0251\n",
      "      8        \u001b[36m3.0247\u001b[0m        \u001b[32m2.9728\u001b[0m  0.0271\n",
      "      7        3.2733        \u001b[32m3.1053\u001b[0m  0.0302\n",
      "      7        3.2465        \u001b[32m3.1544\u001b[0m  0.0273\n",
      "      8        3.2357        \u001b[32m3.1184\u001b[0m  0.0313\n",
      "      9        2.7931        2.7626  0.0260\n",
      "      9        \u001b[36m3.0220\u001b[0m        2.9777  0.0247\n",
      "      8        \u001b[36m3.2498\u001b[0m        3.1097  0.0242\n",
      "      8        3.2626        \u001b[32m3.1540\u001b[0m  0.0260\n",
      "      9        \u001b[36m3.1871\u001b[0m        3.1225  0.0235\n",
      "     10        \u001b[36m2.7745\u001b[0m        2.7645  0.0239\n",
      "Restoring best model from epoch 1.\n",
      "      9        \u001b[36m3.2419\u001b[0m        3.1157  0.0231\n",
      "     10        \u001b[36m2.9785\u001b[0m        2.9827  0.0258\n",
      "Restoring best model from epoch 5.\n",
      "      9        \u001b[36m3.2275\u001b[0m        3.1590  0.0236\n",
      "     10        3.2025        3.1307  0.0240\n",
      "Restoring best model from epoch 7.\n",
      "     10        3.2442        3.1268  0.0211\n",
      "Restoring best model from epoch 5.\n",
      "     10        \u001b[36m3.2060\u001b[0m        3.1633  0.0221\n",
      "Restoring best model from epoch 5.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.1967\u001b[0m        \u001b[32m3.1450\u001b[0m  0.0423\n",
      "      2        \u001b[36m3.1803\u001b[0m        \u001b[32m3.1038\u001b[0m  0.0428\n",
      "      3        3.1969        \u001b[32m3.0790\u001b[0m  0.0421\n",
      "      4        \u001b[36m3.1666\u001b[0m        \u001b[32m3.0744\u001b[0m  0.0424\n",
      "      5        \u001b[36m3.1391\u001b[0m        3.0825  0.0423\n",
      "      6        \u001b[36m3.1236\u001b[0m        3.0977  0.0423\n",
      "      7        \u001b[36m3.1190\u001b[0m        3.1024  0.0421\n",
      "      8        \u001b[36m3.1035\u001b[0m        3.0953  0.0418\n",
      "      9        \u001b[36m3.0885\u001b[0m        3.0780  0.0420\n",
      "     10        \u001b[36m3.0669\u001b[0m        \u001b[32m3.0497\u001b[0m  0.0418\n",
      "max best test predictions 0.32471922\n",
      "0.0333241953125\n",
      "integration times 12515\n",
      "Concordance Index 0.36629462958190434\n",
      "Integrated Brier Score: 0.1940272566785273\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.8046\u001b[0m        \u001b[32m2.7710\u001b[0m  0.0222\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.0874\u001b[0m        \u001b[32m3.0419\u001b[0m  0.0238\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.2763\u001b[0m        \u001b[32m3.1943\u001b[0m  0.0219\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.3073\u001b[0m        \u001b[32m3.1970\u001b[0m  0.0249\n",
      "      2        2.8146        \u001b[32m2.7658\u001b[0m  0.0267\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.3111\u001b[0m        \u001b[32m3.2616\u001b[0m  0.0254\n",
      "      2        \u001b[36m3.0657\u001b[0m        \u001b[32m3.0320\u001b[0m  0.0235\n",
      "      2        3.2805        \u001b[32m3.1849\u001b[0m  0.0230\n",
      "      2        3.3151        \u001b[32m3.1834\u001b[0m  0.0254\n",
      "      3        \u001b[36m2.7996\u001b[0m        \u001b[32m2.7554\u001b[0m  0.0241\n",
      "      2        \u001b[36m3.2898\u001b[0m        \u001b[32m3.2484\u001b[0m  0.0225\n",
      "      3        \u001b[36m3.2664\u001b[0m        \u001b[32m3.1682\u001b[0m  0.0218\n",
      "      3        \u001b[36m3.0518\u001b[0m        \u001b[32m3.0178\u001b[0m  0.0250\n",
      "      3        \u001b[36m3.3010\u001b[0m        \u001b[32m3.1610\u001b[0m  0.0245\n",
      "      4        2.8094        \u001b[32m2.7463\u001b[0m  0.0238\n",
      "      3        3.3042        \u001b[32m3.2282\u001b[0m  0.0230\n",
      "      4        \u001b[36m3.2610\u001b[0m        \u001b[32m3.1535\u001b[0m  0.0218\n",
      "      4        \u001b[36m3.0394\u001b[0m        \u001b[32m3.0044\u001b[0m  0.0253\n",
      "      4        3.2954        \u001b[32m3.2094\u001b[0m  0.0228\n",
      "      5        \u001b[36m3.2538\u001b[0m        \u001b[32m3.1393\u001b[0m  0.0226\n",
      "      5        \u001b[36m2.7885\u001b[0m        \u001b[32m2.7376\u001b[0m  0.0241\n",
      "      4        \u001b[36m3.2888\u001b[0m        \u001b[32m3.1373\u001b[0m  0.0262\n",
      "      5        3.0433        \u001b[32m2.9925\u001b[0m  0.0245\n",
      "      6        \u001b[36m3.2229\u001b[0m        \u001b[32m3.1330\u001b[0m  0.0243\n",
      "      6        \u001b[36m2.7714\u001b[0m        \u001b[32m2.7336\u001b[0m  0.0244\n",
      "      5        3.3124        \u001b[32m3.1923\u001b[0m  0.0261\n",
      "      5        3.3139        \u001b[32m3.1103\u001b[0m  0.0252\n",
      "      6        3.0484        \u001b[32m2.9861\u001b[0m  0.0246\n",
      "      7        \u001b[36m2.7625\u001b[0m        2.7349  0.0219\n",
      "      6        \u001b[36m3.2600\u001b[0m        \u001b[32m3.0933\u001b[0m  0.0229\n",
      "      7        \u001b[36m3.2190\u001b[0m        \u001b[32m3.1320\u001b[0m  0.0249\n",
      "      6        \u001b[36m3.2844\u001b[0m        \u001b[32m3.1853\u001b[0m  0.0251\n",
      "      7        3.0438        \u001b[32m2.9860\u001b[0m  0.0233\n",
      "      8        2.7828        2.7377  0.0222\n",
      "      7        3.2724        \u001b[32m3.0847\u001b[0m  0.0238\n",
      "      8        \u001b[36m3.2105\u001b[0m        \u001b[32m3.1312\u001b[0m  0.0244\n",
      "      7        \u001b[36m3.2571\u001b[0m        \u001b[32m3.1845\u001b[0m  0.0249\n",
      "      8        \u001b[36m3.0061\u001b[0m        2.9867  0.0267\n",
      "      9        \u001b[36m2.7589\u001b[0m        2.7438  0.0233\n",
      "      9        \u001b[36m3.2025\u001b[0m        3.1325  0.0241\n",
      "      8        \u001b[36m3.2436\u001b[0m        \u001b[32m3.0823\u001b[0m  0.0249\n",
      "      8        3.2760        3.1886  0.0251\n",
      "      9        3.0075        2.9905  0.0249\n",
      "     10        \u001b[36m2.7564\u001b[0m        2.7509  0.0301\n",
      "Restoring best model from epoch 5.\n",
      "      9        3.2573        3.1983  0.0273\n",
      "     10        \u001b[36m3.1922\u001b[0m        3.1319  0.0301\n",
      "Restoring best model from epoch 4.\n",
      "      9        \u001b[36m3.2261\u001b[0m        3.0857  0.0312\n",
      "     10        \u001b[36m2.9906\u001b[0m        2.9976  0.0277\n",
      "Restoring best model from epoch 4.\n",
      "     10        \u001b[36m3.2377\u001b[0m        3.2108  0.0207\n",
      "Restoring best model from epoch 5.\n",
      "     10        3.2321        3.0936  0.0222\n",
      "Restoring best model from epoch 5.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.2033\u001b[0m        \u001b[32m3.1410\u001b[0m  0.0429\n",
      "      2        \u001b[36m3.1964\u001b[0m        \u001b[32m3.1107\u001b[0m  0.0431\n",
      "      3        \u001b[36m3.1932\u001b[0m        \u001b[32m3.0958\u001b[0m  0.0424\n",
      "      4        \u001b[36m3.1643\u001b[0m        3.0996  0.0569\n",
      "      5        \u001b[36m3.1276\u001b[0m        3.1107  0.0428\n",
      "      6        \u001b[36m3.1173\u001b[0m        3.1175  0.0418\n",
      "      7        \u001b[36m3.1146\u001b[0m        3.1133  0.0420\n",
      "      8        \u001b[36m3.0860\u001b[0m        3.0979  0.0425\n",
      "      9        3.0864        \u001b[32m3.0649\u001b[0m  0.0428\n",
      "     10        \u001b[36m3.0839\u001b[0m        \u001b[32m3.0409\u001b[0m  0.0418\n",
      "max best test predictions 0.36896765\n",
      "0.0333241953125\n",
      "integration times 15426\n",
      "Concordance Index 0.3716677318928726\n",
      "Integrated Brier Score: 0.17390435019098266\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.8320\u001b[0m        \u001b[32m2.7774\u001b[0m  0.0209\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.0654\u001b[0m        \u001b[32m3.0117\u001b[0m  0.0220\n",
      "      2        2.8384        \u001b[32m2.7695\u001b[0m  0.0227\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.2744\u001b[0m        \u001b[32m3.2044\u001b[0m  0.0256\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.3471\u001b[0m        \u001b[32m3.2273\u001b[0m  0.0254\n",
      "      2        3.0986        \u001b[32m2.9964\u001b[0m  0.0229\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.2886\u001b[0m        \u001b[32m3.2338\u001b[0m  0.0253\n",
      "      3        \u001b[36m2.8312\u001b[0m        \u001b[32m2.7584\u001b[0m  0.0262\n",
      "      2        \u001b[36m3.2705\u001b[0m        \u001b[32m3.1935\u001b[0m  0.0270\n",
      "      2        \u001b[36m3.3126\u001b[0m        \u001b[32m3.2141\u001b[0m  0.0259\n",
      "      3        \u001b[36m3.0519\u001b[0m        \u001b[32m2.9753\u001b[0m  0.0225\n",
      "      2        \u001b[36m3.2718\u001b[0m        \u001b[32m3.2207\u001b[0m  0.0234\n",
      "      4        \u001b[36m2.7915\u001b[0m        \u001b[32m2.7486\u001b[0m  0.0237\n",
      "      3        \u001b[36m3.2689\u001b[0m        \u001b[32m3.1769\u001b[0m  0.0261\n",
      "      3        3.3530        \u001b[32m3.1922\u001b[0m  0.0238\n",
      "      4        3.1265        \u001b[32m2.9529\u001b[0m  0.0255\n",
      "      3        \u001b[36m3.2418\u001b[0m        \u001b[32m3.2013\u001b[0m  0.0243\n",
      "      5        2.8062        \u001b[32m2.7401\u001b[0m  0.0222\n",
      "      4        \u001b[36m3.2919\u001b[0m        \u001b[32m3.1669\u001b[0m  0.0222\n",
      "      4        \u001b[36m3.2659\u001b[0m        \u001b[32m3.1593\u001b[0m  0.0238\n",
      "      5        \u001b[36m3.0415\u001b[0m        \u001b[32m2.9304\u001b[0m  0.0265\n",
      "      4        3.2542        \u001b[32m3.1801\u001b[0m  0.0237\n",
      "      6        2.8054        \u001b[32m2.7367\u001b[0m  0.0225\n",
      "      5        3.2859        \u001b[32m3.1426\u001b[0m  0.0225\n",
      "      5        3.3096        \u001b[32m3.1416\u001b[0m  0.0254\n",
      "      6        \u001b[36m3.0402\u001b[0m        \u001b[32m2.9159\u001b[0m  0.0252\n",
      "      5        3.2947        \u001b[32m3.1599\u001b[0m  0.0224\n",
      "      7        \u001b[36m2.7683\u001b[0m        \u001b[32m2.7350\u001b[0m  0.0228\n",
      "      6        \u001b[36m3.2525\u001b[0m        \u001b[32m3.1326\u001b[0m  0.0223\n",
      "      6        3.3039        \u001b[32m3.1290\u001b[0m  0.0263\n",
      "      6        3.2951        \u001b[32m3.1495\u001b[0m  0.0229\n",
      "      7        \u001b[36m3.0313\u001b[0m        \u001b[32m2.9085\u001b[0m  0.0247\n",
      "      8        \u001b[36m2.7620\u001b[0m        \u001b[32m2.7330\u001b[0m  0.0234\n",
      "      7        \u001b[36m3.2497\u001b[0m        \u001b[32m3.1282\u001b[0m  0.0227\n",
      "      7        \u001b[36m3.2697\u001b[0m        \u001b[32m3.1226\u001b[0m  0.0264\n",
      "      7        3.2803        \u001b[32m3.1474\u001b[0m  0.0238\n",
      "      8        \u001b[36m3.0135\u001b[0m        \u001b[32m2.9068\u001b[0m  0.0243\n",
      "      9        \u001b[36m2.7589\u001b[0m        \u001b[32m2.7314\u001b[0m  0.0251\n",
      "      8        \u001b[36m3.2474\u001b[0m        \u001b[32m3.1281\u001b[0m  0.0248\n",
      "      8        3.3257        \u001b[32m3.1222\u001b[0m  0.0253\n",
      "      8        3.2430        3.1514  0.0227\n",
      "      9        \u001b[36m2.9945\u001b[0m        2.9122  0.0246\n",
      "     10        2.7646        \u001b[32m2.7300\u001b[0m  0.0236\n",
      "Restoring best model from epoch 4.\n",
      "      9        \u001b[36m3.2188\u001b[0m        3.1334  0.0246\n",
      "      9        \u001b[36m3.2686\u001b[0m        3.1289  0.0265\n",
      "     10        2.9969        2.9244  0.0226\n",
      "Restoring best model from epoch 5.\n",
      "      9        \u001b[36m3.2130\u001b[0m        3.1565  0.0275\n",
      "     10        \u001b[36m3.2020\u001b[0m        3.1403  0.0237\n",
      "Restoring best model from epoch 4.\n",
      "     10        \u001b[36m3.2615\u001b[0m        3.1413  0.0217\n",
      "Restoring best model from epoch 5.\n",
      "     10        3.2267        3.1635  0.0209\n",
      "Restoring best model from epoch 5.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.2052\u001b[0m        \u001b[32m3.1439\u001b[0m  0.0425\n",
      "      2        \u001b[36m3.1912\u001b[0m        \u001b[32m3.1126\u001b[0m  0.0424\n",
      "      3        \u001b[36m3.1814\u001b[0m        \u001b[32m3.0850\u001b[0m  0.0425\n",
      "      4        \u001b[36m3.1752\u001b[0m        \u001b[32m3.0709\u001b[0m  0.0413\n",
      "      5        \u001b[36m3.1360\u001b[0m        \u001b[32m3.0654\u001b[0m  0.0412\n",
      "      6        \u001b[36m3.1258\u001b[0m        3.0780  0.0415\n",
      "      7        \u001b[36m3.0974\u001b[0m        3.0849  0.0420\n",
      "      8        \u001b[36m3.0950\u001b[0m        3.0792  0.0423\n",
      "      9        \u001b[36m3.0824\u001b[0m        \u001b[32m3.0555\u001b[0m  0.0412\n",
      "     10        \u001b[36m3.0729\u001b[0m        \u001b[32m3.0263\u001b[0m  0.0420\n",
      "max best test predictions 0.3548995\n",
      "0.0333241953125\n",
      "integration times 15036\n",
      "Concordance Index 0.3912799887948456\n",
      "Integrated Brier Score: 0.18306742136580653\n",
      "load_flchain\n",
      "split age           float32\n",
      "sex             uint8\n",
      "sample_yr     float32\n",
      "kappa         float32\n",
      "lambda        float32\n",
      "flc_grp       float32\n",
      "creatinine    float32\n",
      "mgus            uint8\n",
      "dtype: object\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m1.6159\u001b[0m        \u001b[32m1.5670\u001b[0m  0.1301\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.2404\u001b[0m        \u001b[32m2.1608\u001b[0m  0.1431\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.9373\u001b[0m        \u001b[32m2.8368\u001b[0m  0.1422\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.1636\u001b[0m        \u001b[32m3.0564\u001b[0m  0.1393\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.2144\u001b[0m        \u001b[32m3.1169\u001b[0m  0.1407\n",
      "      2        \u001b[36m1.5958\u001b[0m        \u001b[32m1.5562\u001b[0m  0.1326\n",
      "      2        \u001b[36m2.8971\u001b[0m        2.8570  0.1377\n",
      "      2        \u001b[36m2.2059\u001b[0m        \u001b[32m2.1570\u001b[0m  0.1454\n",
      "      2        \u001b[36m3.1229\u001b[0m        \u001b[32m3.0385\u001b[0m  0.1479\n",
      "      2        \u001b[36m3.1748\u001b[0m        \u001b[32m3.1100\u001b[0m  0.1478\n",
      "      3        \u001b[36m1.5905\u001b[0m        \u001b[32m1.5375\u001b[0m  0.1349\n",
      "      3        \u001b[36m2.1883\u001b[0m        \u001b[32m2.1347\u001b[0m  0.1456\n",
      "      3        \u001b[36m3.1482\u001b[0m        \u001b[32m3.0940\u001b[0m  0.1432\n",
      "      3        \u001b[36m2.8737\u001b[0m        \u001b[32m2.8315\u001b[0m  0.1662\n",
      "      3        \u001b[36m3.0943\u001b[0m        \u001b[32m3.0237\u001b[0m  0.1528\n",
      "      4        \u001b[36m1.5744\u001b[0m        1.5508  0.1458\n",
      "      4        \u001b[36m2.1709\u001b[0m        \u001b[32m2.1227\u001b[0m  0.1308\n",
      "      4        \u001b[36m3.0718\u001b[0m        3.0788  0.1357\n",
      "      4        \u001b[36m3.1240\u001b[0m        3.1325  0.1441\n",
      "      4        \u001b[36m2.8505\u001b[0m        2.9211  0.1653\n",
      "      5        \u001b[36m1.5635\u001b[0m        \u001b[32m1.5259\u001b[0m  0.1392\n",
      "      5        \u001b[36m2.1538\u001b[0m        \u001b[32m2.1119\u001b[0m  0.1305\n",
      "      5        \u001b[36m3.0549\u001b[0m        3.0237  0.1451\n",
      "      5        \u001b[36m3.0995\u001b[0m        \u001b[32m3.0915\u001b[0m  0.1444\n",
      "      5        \u001b[36m2.8355\u001b[0m        2.8388  0.1481\n",
      "      6        \u001b[36m1.5504\u001b[0m        \u001b[32m1.5119\u001b[0m  0.1318\n",
      "      6        \u001b[36m3.0772\u001b[0m        3.1053  0.1342\n",
      "      6        \u001b[36m3.0291\u001b[0m        \u001b[32m2.9884\u001b[0m  0.1461\n",
      "      6        \u001b[36m2.1419\u001b[0m        \u001b[32m2.1009\u001b[0m  0.1986\n",
      "      6        \u001b[36m2.8076\u001b[0m        \u001b[32m2.8010\u001b[0m  0.1377\n",
      "      7        \u001b[36m1.5351\u001b[0m        \u001b[32m1.5071\u001b[0m  0.1224\n",
      "      7        \u001b[36m2.9971\u001b[0m        \u001b[32m2.9405\u001b[0m  0.1471\n",
      "      7        \u001b[36m2.1188\u001b[0m        \u001b[32m2.0726\u001b[0m  0.1385\n",
      "      7        \u001b[36m2.7786\u001b[0m        \u001b[32m2.7621\u001b[0m  0.1419\n",
      "      7        \u001b[36m3.0367\u001b[0m        3.2163  0.1824\n",
      "      8        \u001b[36m1.5153\u001b[0m        \u001b[32m1.4631\u001b[0m  0.1428\n",
      "      8        \u001b[36m2.9537\u001b[0m        \u001b[32m2.8796\u001b[0m  0.1504\n",
      "      8        \u001b[36m2.0878\u001b[0m        \u001b[32m2.0434\u001b[0m  0.1545\n",
      "      8        \u001b[36m2.7445\u001b[0m        \u001b[32m2.6993\u001b[0m  0.1455\n",
      "      8        \u001b[36m3.0037\u001b[0m        \u001b[32m2.9830\u001b[0m  0.1373\n",
      "      9        \u001b[36m1.4898\u001b[0m        \u001b[32m1.4326\u001b[0m  0.1329\n",
      "      9        \u001b[36m2.9081\u001b[0m        \u001b[32m2.8605\u001b[0m  0.1431\n",
      "      9        \u001b[36m2.0553\u001b[0m        \u001b[32m1.9869\u001b[0m  0.1395\n",
      "     10        \u001b[36m1.4583\u001b[0m        \u001b[32m1.4124\u001b[0m  0.1273\n",
      "      9        \u001b[36m2.6959\u001b[0m        2.7266  0.1482\n",
      "     10        \u001b[36m2.8481\u001b[0m        \u001b[32m2.7883\u001b[0m  0.1297\n",
      "     10        \u001b[36m2.0159\u001b[0m        \u001b[32m1.9574\u001b[0m  0.1227\n",
      "     10        \u001b[36m2.6478\u001b[0m        \u001b[32m2.6013\u001b[0m  0.1173\n",
      "      9        \u001b[36m2.9554\u001b[0m        \u001b[32m2.9457\u001b[0m  0.2759\n",
      "     10        \u001b[36m2.8912\u001b[0m        \u001b[32m2.8188\u001b[0m  0.1177\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.6541\u001b[0m        \u001b[32m2.5775\u001b[0m  0.1684\n",
      "      2        \u001b[36m2.6115\u001b[0m        \u001b[32m2.5724\u001b[0m  0.1685\n",
      "      3        \u001b[36m2.5984\u001b[0m        \u001b[32m2.5394\u001b[0m  0.1686\n",
      "      4        \u001b[36m2.5600\u001b[0m        \u001b[32m2.5157\u001b[0m  0.1682\n",
      "      5        \u001b[36m2.5367\u001b[0m        \u001b[32m2.5024\u001b[0m  0.1685\n",
      "      6        \u001b[36m2.5046\u001b[0m        \u001b[32m2.4359\u001b[0m  0.1690\n",
      "      7        \u001b[36m2.4574\u001b[0m        \u001b[32m2.4039\u001b[0m  0.1698\n",
      "      8        \u001b[36m2.4031\u001b[0m        \u001b[32m2.3257\u001b[0m  0.1686\n",
      "      9        \u001b[36m2.3301\u001b[0m        \u001b[32m2.2011\u001b[0m  0.1655\n",
      "     10        \u001b[36m2.2541\u001b[0m        \u001b[32m2.1793\u001b[0m  0.1668\n",
      "Restoring best model from epoch 9.\n",
      "max best test predictions 1.8271173\n",
      "0.999999\n",
      "integration times 32418\n",
      "Concordance Index 0.7061892976337796\n",
      "Integrated Brier Score: 0.13260598246350613\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m1.6295\u001b[0m        \u001b[32m1.5636\u001b[0m  0.1287\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.2589\u001b[0m        \u001b[32m2.1695\u001b[0m  0.1403\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.8970\u001b[0m        \u001b[32m2.8121\u001b[0m  0.1424\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.1526\u001b[0m        \u001b[32m3.0485\u001b[0m  0.1512\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.2147\u001b[0m        \u001b[32m3.1143\u001b[0m  0.1551\n",
      "      2        \u001b[36m1.6072\u001b[0m        1.5774  0.1351\n",
      "      2        \u001b[36m2.2268\u001b[0m        \u001b[32m2.1693\u001b[0m  0.1437\n",
      "      2        \u001b[36m2.8663\u001b[0m        \u001b[32m2.8066\u001b[0m  0.1515\n",
      "      2        \u001b[36m3.1182\u001b[0m        3.0536  0.1462\n",
      "      2        \u001b[36m3.1756\u001b[0m        3.1273  0.1479\n",
      "      3        \u001b[36m1.5919\u001b[0m        \u001b[32m1.5607\u001b[0m  0.1364\n",
      "      3        \u001b[36m2.2073\u001b[0m        \u001b[32m2.1316\u001b[0m  0.1372\n",
      "      3        \u001b[36m3.0946\u001b[0m        \u001b[32m3.0111\u001b[0m  0.1436\n",
      "      3        \u001b[36m2.8469\u001b[0m        \u001b[32m2.7675\u001b[0m  0.1558\n",
      "      3        \u001b[36m3.1540\u001b[0m        \u001b[32m3.0620\u001b[0m  0.1504\n",
      "      4        \u001b[36m1.5793\u001b[0m        \u001b[32m1.5572\u001b[0m  0.1338\n",
      "      4        \u001b[36m2.1882\u001b[0m        2.1336  0.1397\n",
      "      4        \u001b[36m3.0669\u001b[0m        \u001b[32m2.9956\u001b[0m  0.1432\n",
      "      4        \u001b[36m2.8247\u001b[0m        2.7776  0.1463\n",
      "      4        \u001b[36m3.1238\u001b[0m        \u001b[32m3.0479\u001b[0m  0.1446\n",
      "      5        \u001b[36m1.5702\u001b[0m        \u001b[32m1.5438\u001b[0m  0.1342\n",
      "      5        \u001b[36m2.1742\u001b[0m        \u001b[32m2.1281\u001b[0m  0.1488\n",
      "      5        \u001b[36m3.0426\u001b[0m        \u001b[32m2.9752\u001b[0m  0.1408\n",
      "      5        \u001b[36m2.8020\u001b[0m        \u001b[32m2.7559\u001b[0m  0.1392\n",
      "      5        \u001b[36m3.1045\u001b[0m        3.0781  0.1526\n",
      "      6        \u001b[36m1.5563\u001b[0m        \u001b[32m1.5157\u001b[0m  0.1339\n",
      "      6        \u001b[36m2.1538\u001b[0m        \u001b[32m2.1272\u001b[0m  0.1372\n",
      "      6        \u001b[36m3.0171\u001b[0m        \u001b[32m2.9544\u001b[0m  0.1389\n",
      "      6        \u001b[36m2.7762\u001b[0m        \u001b[32m2.7224\u001b[0m  0.1502\n",
      "      6        \u001b[36m3.0779\u001b[0m        \u001b[32m3.0076\u001b[0m  0.1386\n",
      "      7        \u001b[36m1.5390\u001b[0m        \u001b[32m1.5053\u001b[0m  0.1344\n",
      "      7        \u001b[36m2.9945\u001b[0m        \u001b[32m2.9498\u001b[0m  0.1412\n",
      "      7        \u001b[36m2.1307\u001b[0m        \u001b[32m2.1102\u001b[0m  0.1856\n",
      "      7        \u001b[36m2.7479\u001b[0m        \u001b[32m2.7099\u001b[0m  0.1429\n",
      "      7        \u001b[36m3.0499\u001b[0m        \u001b[32m2.9735\u001b[0m  0.1386\n",
      "      8        \u001b[36m1.5201\u001b[0m        \u001b[32m1.4820\u001b[0m  0.1190\n",
      "      8        \u001b[36m2.9492\u001b[0m        \u001b[32m2.8439\u001b[0m  0.1322\n",
      "      9        \u001b[36m1.4943\u001b[0m        \u001b[32m1.4432\u001b[0m  0.1196\n",
      "      8        \u001b[36m2.7092\u001b[0m        \u001b[32m2.6723\u001b[0m  0.1347\n",
      "      8        \u001b[36m3.0438\u001b[0m        \u001b[32m2.9661\u001b[0m  0.1366\n",
      "      8        \u001b[36m2.1045\u001b[0m        \u001b[32m2.0731\u001b[0m  0.2253\n",
      "      9        \u001b[36m2.9068\u001b[0m        \u001b[32m2.8193\u001b[0m  0.1383\n",
      "      9        \u001b[36m2.6672\u001b[0m        \u001b[32m2.6205\u001b[0m  0.1473\n",
      "     10        \u001b[36m1.4608\u001b[0m        \u001b[32m1.4130\u001b[0m  0.1483\n",
      "      9        \u001b[36m2.9763\u001b[0m        \u001b[32m2.8953\u001b[0m  0.1446\n",
      "     10        \u001b[36m2.8455\u001b[0m        \u001b[32m2.7401\u001b[0m  0.1328\n",
      "     10        \u001b[36m2.6095\u001b[0m        \u001b[32m2.5544\u001b[0m  0.1294\n",
      "      9        \u001b[36m2.0668\u001b[0m        \u001b[32m2.0106\u001b[0m  0.1919\n",
      "     10        \u001b[36m2.9248\u001b[0m        \u001b[32m2.8078\u001b[0m  0.1307\n",
      "     10        \u001b[36m2.0236\u001b[0m        \u001b[32m1.9744\u001b[0m  0.1135\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.6401\u001b[0m        \u001b[32m2.5710\u001b[0m  0.1714\n",
      "      2        \u001b[36m2.6099\u001b[0m        \u001b[32m2.5627\u001b[0m  0.1702\n",
      "      3        \u001b[36m2.5821\u001b[0m        \u001b[32m2.5371\u001b[0m  0.1679\n",
      "      4        \u001b[36m2.5563\u001b[0m        \u001b[32m2.5172\u001b[0m  0.1681\n",
      "      5        \u001b[36m2.5265\u001b[0m        \u001b[32m2.4657\u001b[0m  0.1677\n",
      "      6        \u001b[36m2.4904\u001b[0m        \u001b[32m2.4537\u001b[0m  0.1691\n",
      "      7        \u001b[36m2.4370\u001b[0m        \u001b[32m2.4158\u001b[0m  0.1703\n",
      "      8        \u001b[36m2.3700\u001b[0m        \u001b[32m2.3175\u001b[0m  0.1704\n",
      "      9        \u001b[36m2.2891\u001b[0m        \u001b[32m2.1577\u001b[0m  0.1687\n",
      "     10        \u001b[36m2.1869\u001b[0m        \u001b[32m2.0784\u001b[0m  0.1705\n",
      "max best test predictions 8.735439\n",
      "0.999999\n",
      "integration times 31862356\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 61\u001b[0m\n\u001b[1;32m     16\u001b[0m X, y \u001b[39m=\u001b[39m sort_X_y_pandas(X, y)\n\u001b[1;32m     18\u001b[0m net \u001b[39m=\u001b[39m NeuralNet(\n\u001b[1;32m     19\u001b[0m     SurvivalModel, \n\u001b[1;32m     20\u001b[0m     module__n_layers \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     59\u001b[0m     verbose\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m\n\u001b[1;32m     60\u001b[0m )\n\u001b[0;32m---> 61\u001b[0m df_agg_metrics_cindex, df_agg_metrics_ibs, best_model,params, outer_scores, best_preds_train, best_preds_test \u001b[39m=\u001b[39m train_eval(X, y, net, n_iter, data\u001b[39m.\u001b[39;49mfilename)\n\u001b[1;32m     62\u001b[0m agg_metrics_cindex\u001b[39m.\u001b[39mappend(df_agg_metrics_cindex)\n\u001b[1;32m     63\u001b[0m agg_metrics_ibs\u001b[39m.\u001b[39mappend(df_agg_metrics_ibs)\n",
      "Cell \u001b[0;32mIn[36], line 58\u001b[0m, in \u001b[0;36mtrain_eval\u001b[0;34m(X, y, net, n_iter, filename)\u001b[0m\n\u001b[1;32m     55\u001b[0m outer_scores[\u001b[39m'\u001b[39m\u001b[39mibs_train_\u001b[39m\u001b[39m'\u001b[39m\u001b[39m+\u001b[39mdataset_name] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m [np\u001b[39m.\u001b[39mnan]\n\u001b[1;32m     57\u001b[0m \u001b[39m#try:\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m cum_hazard_test \u001b[39m=\u001b[39m get_cumulative_hazard_function_ah(\n\u001b[1;32m     59\u001b[0m         X_train\u001b[39m.\u001b[39;49mvalues, X_test\u001b[39m.\u001b[39;49mvalues, y_train\u001b[39m.\u001b[39;49mvalues, y_test\u001b[39m.\u001b[39;49mvalues,\n\u001b[1;32m     60\u001b[0m         best_preds_train\u001b[39m.\u001b[39;49mreshape(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m), best_preds_test\u001b[39m.\u001b[39;49mreshape(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m     61\u001b[0m         )\n\u001b[1;32m     62\u001b[0m df_survival_test \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mexp(\u001b[39m-\u001b[39mcum_hazard_test)\n\u001b[1;32m     63\u001b[0m durations_test, events_test \u001b[39m=\u001b[39m transform_back(y_test\u001b[39m.\u001b[39mvalues)\n",
      "File \u001b[0;32m~/miniconda3/envs/xgbsurv/lib/python3.11/site-packages/xgbsurv/models/eh_ah_final.py:355\u001b[0m, in \u001b[0;36mget_cumulative_hazard_function_ah\u001b[0;34m(X_train, X_test, y_train, y_test, predictor_train, predictor_test)\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mintegration times\u001b[39m\u001b[39m'\u001b[39m,integration_values\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m])\n\u001b[1;32m    352\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, integration_values\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]):\n\u001b[1;32m    353\u001b[0m     integration_values[_] \u001b[39m=\u001b[39m (\n\u001b[1;32m    354\u001b[0m         integration_values[_ \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m]\n\u001b[0;32m--> 355\u001b[0m         \u001b[39m+\u001b[39m quadrature(\n\u001b[1;32m    356\u001b[0m             func\u001b[39m=\u001b[39;49mhazard_function_integrate,\n\u001b[1;32m    357\u001b[0m             a\u001b[39m=\u001b[39;49mintegration_times[_ \u001b[39m-\u001b[39;49m \u001b[39m1\u001b[39;49m],\n\u001b[1;32m    358\u001b[0m             b\u001b[39m=\u001b[39;49mintegration_times[_],\n\u001b[1;32m    359\u001b[0m             vec_func\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    360\u001b[0m         )[\u001b[39m0\u001b[39m]\n\u001b[1;32m    361\u001b[0m     )\n\u001b[1;32m    363\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_samples):\n\u001b[1;32m    364\u001b[0m     cumulative_hazard[_] \u001b[39m=\u001b[39m (\n\u001b[1;32m    365\u001b[0m         integration_values[\n\u001b[1;32m    366\u001b[0m             np\u001b[39m.\u001b[39mdigitize(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    371\u001b[0m         \u001b[39m*\u001b[39m theta_min[_]\n\u001b[1;32m    372\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/xgbsurv/lib/python3.11/site-packages/scipy/integrate/_quadrature.py:365\u001b[0m, in \u001b[0;36mquadrature\u001b[0;34m(func, a, b, args, tol, rtol, maxiter, vec_func, miniter)\u001b[0m\n\u001b[1;32m    363\u001b[0m maxiter \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(miniter\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m, maxiter)\n\u001b[1;32m    364\u001b[0m \u001b[39mfor\u001b[39;00m n \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(miniter, maxiter\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m--> 365\u001b[0m     newval \u001b[39m=\u001b[39m fixed_quad(vfunc, a, b, (), n)[\u001b[39m0\u001b[39m]\n\u001b[1;32m    366\u001b[0m     err \u001b[39m=\u001b[39m \u001b[39mabs\u001b[39m(newval\u001b[39m-\u001b[39mval)\n\u001b[1;32m    367\u001b[0m     val \u001b[39m=\u001b[39m newval\n",
      "File \u001b[0;32m~/miniconda3/envs/xgbsurv/lib/python3.11/site-packages/scipy/integrate/_quadrature.py:245\u001b[0m, in \u001b[0;36mfixed_quad\u001b[0;34m(func, a, b, args, n)\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mGaussian quadrature is only available for \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    243\u001b[0m                      \u001b[39m\"\u001b[39m\u001b[39mfinite limits.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    244\u001b[0m y \u001b[39m=\u001b[39m (b\u001b[39m-\u001b[39ma)\u001b[39m*\u001b[39m(x\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m/\u001b[39m\u001b[39m2.0\u001b[39m \u001b[39m+\u001b[39m a\n\u001b[0;32m--> 245\u001b[0m \u001b[39mreturn\u001b[39;00m (b\u001b[39m-\u001b[39ma)\u001b[39m/\u001b[39m\u001b[39m2.0\u001b[39m \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39msum(w\u001b[39m*\u001b[39mfunc(y, \u001b[39m*\u001b[39;49margs), axis\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m), \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/xgbsurv/lib/python3.11/site-packages/scipy/integrate/_quadrature.py:282\u001b[0m, in \u001b[0;36mvectorize1.<locals>.vfunc\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    280\u001b[0m x \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(x)\n\u001b[1;32m    281\u001b[0m \u001b[39m# call with first point to get output type\u001b[39;00m\n\u001b[0;32m--> 282\u001b[0m y0 \u001b[39m=\u001b[39m func(x[\u001b[39m0\u001b[39;49m], \u001b[39m*\u001b[39;49margs)\n\u001b[1;32m    283\u001b[0m n \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(x)\n\u001b[1;32m    284\u001b[0m dtype \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(y0, \u001b[39m'\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mtype\u001b[39m(y0))\n",
      "File \u001b[0;32m~/miniconda3/envs/xgbsurv/lib/python3.11/site-packages/xgbsurv/models/eh_ah_final.py:333\u001b[0m, in \u001b[0;36mget_cumulative_hazard_function_ah.<locals>.hazard_function_integrate\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mhazard_function_integrate\u001b[39m(s):\n\u001b[0;32m--> 333\u001b[0m     \u001b[39mreturn\u001b[39;00m baseline_hazard_estimator_ah(\n\u001b[1;32m    334\u001b[0m         time\u001b[39m=\u001b[39;49ms,\n\u001b[1;32m    335\u001b[0m         time_train\u001b[39m=\u001b[39;49mtime_train,\n\u001b[1;32m    336\u001b[0m         event_train\u001b[39m=\u001b[39;49mevent_train,\n\u001b[1;32m    337\u001b[0m         predictor_train\u001b[39m=\u001b[39;49mpredictor_train,\n\u001b[1;32m    338\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data_set_fns = [ load_metabric, load_flchain, load_rgbsg, load_support] #load_metabric, , load_flchain, load_rgbsg, load_support, load_tcga]\n",
    "data_set_fns_str = ['load_metabric', 'load_flchain', 'load_rgbsg', 'load_support'] \n",
    "one_hot_dict = {'load_flchain': ['mgus'], 'load_support':['cancer'], 'load_rgbsg':['grade']}\n",
    "agg_metrics_cindex = []\n",
    "agg_metrics_ibs = []\n",
    "\n",
    "for idx, dataset in enumerate(data_set_fns):\n",
    "    # get name of current dataset\n",
    "    data = dataset(path=\"/Users/JUSC/Documents/xgbsurv/xgbsurv/datasets/data/\", as_frame=True)\n",
    "    X  = data.data #.astype(np.float32)\n",
    "    y = data.target #.values #.to_numpy()\n",
    "\n",
    "    print(data_set_fns_str[idx])\n",
    "    #if data_set_fns_str[idx] in one_hot_dict.keys():\n",
    "    #    X = pd.get_dummies(X, columns=one_hot_dict[data_set_fns_str[idx]])\n",
    "    X, y = sort_X_y_pandas(X, y)\n",
    "    \n",
    "    net = NeuralNet(\n",
    "        SurvivalModel, \n",
    "        module__n_layers = 1,\n",
    "        module__input_units = X.shape[1],\n",
    "        #module__num_nodes = 32,\n",
    "        #module__dropout = 0.1, # these could also be removed\n",
    "        module__out_features = 1,\n",
    "        # for split sizes when result size = 1\n",
    "        iterator_train__drop_last=True,\n",
    "        #iterator_valid__drop_last=True,\n",
    "        criterion=AHLoss,\n",
    "        optimizer=torch.optim.AdamW,\n",
    "        optimizer__weight_decay = 0.4,\n",
    "        batch_size=32, # separate train and valid->iterator_train__batch_size=128 and iterator_valid__batch_size=128 ?\n",
    "        callbacks=[\n",
    "            (\n",
    "                \"sched\",\n",
    "                LRScheduler(\n",
    "                    torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
    "                    monitor=\"valid_loss\",\n",
    "                    patience=5,\n",
    "                ),\n",
    "            ),\n",
    "            (\n",
    "                \"es\",\n",
    "                EarlyStopping(\n",
    "                    monitor=\"valid_loss\",\n",
    "                    patience=early_stopping_rounds,\n",
    "                    load_best=True,\n",
    "                    threshold = 0.01,\n",
    "                    #, threshold_mode='rel', \n",
    "                    # lower_is_better=True\n",
    "                ),\n",
    "            ),\n",
    "            (\"seed\", FixSeed(seed=42)),\n",
    "            #(\"Input Shape Setter\",InputShapeSetter())\n",
    "        ],\n",
    "        \n",
    "        train_split = CustomValidSplit(0.2, stratified=True, random_state=rand_state), \n",
    "        #max_epochs=1, #0,#100\n",
    "        #train_split=None,\n",
    "        verbose=2\n",
    "    )\n",
    "    df_agg_metrics_cindex, df_agg_metrics_ibs, best_model,params, outer_scores, best_preds_train, best_preds_test = train_eval(X, y, net, n_iter, data.filename)\n",
    "    agg_metrics_cindex.append(df_agg_metrics_cindex)\n",
    "    agg_metrics_ibs.append(df_agg_metrics_ibs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>cindex_train_mean</th>\n",
       "      <th>cindex_train_std</th>\n",
       "      <th>cindex_test_mean</th>\n",
       "      <th>cindex_test_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>METABRIC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FLCHAIN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RGBSG</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SUPPORT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    dataset  cindex_train_mean  cindex_train_std  cindex_test_mean   \n",
       "0  METABRIC                NaN               NaN               NaN  \\\n",
       "0   FLCHAIN                NaN               NaN               NaN   \n",
       "0     RGBSG                NaN               NaN               NaN   \n",
       "0   SUPPORT                NaN               NaN               NaN   \n",
       "\n",
       "   cindex_test_std  \n",
       "0              NaN  \n",
       "0              NaN  \n",
       "0              NaN  \n",
       "0              NaN  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final_ah_1_cindex = pd.concat([df for df in agg_metrics_cindex]).round(4)\n",
    "df_final_ah_1_cindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>cindex_train_mean</th>\n",
       "      <th>cindex_train_std</th>\n",
       "      <th>cindex_test_mean</th>\n",
       "      <th>cindex_test_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>METABRIC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FLCHAIN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RGBSG</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SUPPORT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    dataset  cindex_train_mean  cindex_train_std  cindex_test_mean   \n",
       "0  METABRIC                NaN               NaN               NaN  \\\n",
       "0   FLCHAIN                NaN               NaN               NaN   \n",
       "0     RGBSG                NaN               NaN               NaN   \n",
       "0   SUPPORT                NaN               NaN               NaN   \n",
       "\n",
       "   cindex_test_std  \n",
       "0              NaN  \n",
       "0              NaN  \n",
       "0              NaN  \n",
       "0              NaN  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final_ah_1_cindex = pd.concat([df for df in agg_metrics_cindex]).round(4)\n",
    "df_final_ah_1_cindex.to_csv('metrics/final_deep_learning_ah_1_cindex.csv', index=False)\n",
    "df_final_ah_1_cindex.to_csv('/Users/JUSC/Documents/644928e0fb7e147893e8ec15/05_thesis/tables/final_deep_learning_ah_1_cindex.csv', index=False)  #\n",
    "df_final_ah_1_cindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>ibs_train_mean</th>\n",
       "      <th>ibs_train_std</th>\n",
       "      <th>ibs_test_mean</th>\n",
       "      <th>ibs_test_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>METABRIC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FLCHAIN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RGBSG</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SUPPORT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    dataset  ibs_train_mean  ibs_train_std  ibs_test_mean  ibs_test_std\n",
       "0  METABRIC             NaN            NaN            NaN           NaN\n",
       "0   FLCHAIN             NaN            NaN            NaN           NaN\n",
       "0     RGBSG             NaN            NaN            NaN           NaN\n",
       "0   SUPPORT             NaN            NaN            NaN           NaN"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final_ah_1_ibs = pd.concat([df for df in agg_metrics_ibs]).round(4)\n",
    "df_final_ah_1_ibs.to_csv('metrics/final_deep_learning_ah_1_ibs.csv', index=False)\n",
    "df_final_ah_1_ibs.to_csv('/Users/JUSC/Documents/644928e0fb7e147893e8ec15/05_thesis/tables/final_deep_learning_ah_1_ibs.csv', index=False) \n",
    "df_final_ah_1_ibs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TCGA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_ah_tcga = {\n",
    "    'estimator__module__n_layers': [1, 2, 4],\n",
    "    'estimator__module__num_nodes': [64, 128, 256, 512],\n",
    "    'estimator__module__dropout': scuniform(0.0,0.7),\n",
    "    'estimator__optimizer__weight_decay': [0.4, 0.2, 0.1, 0.05, 0.02, 0.01, 0],\n",
    "    'estimator__batch_size': [64, 128, 256, 512, 1024],\n",
    "    #lr not in paper because of learning rate finder\n",
    "    # note: setting learning rate higher would make exp(partial_hazard) explode\n",
    "    #'estimator__lr': scloguniform(0.001,0.01), # scheduler unten einbauen\n",
    "    # use callback instead\n",
    "    'estimator__lr':[0.001],\n",
    "    #'estimator__max_epochs':  scrandint(10,20), # corresponds to num_rounds\n",
    "    'pca__n_components':[64, 128, 256, 512, 1024]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_eval(X, y, net, n_iter, filename):\n",
    "        model = 'ah_'\n",
    "        dataset_name = filename.split('_')[0]\n",
    "        # add IBS later\n",
    "        outer_scores = {'cindex_train_'+dataset_name:[], 'cindex_test_'+dataset_name:[],\n",
    "                        'ibs_train_'+dataset_name:[], 'ibs_test_'+dataset_name:[]}\n",
    "        best_params = {'best_params_'+dataset_name:[]}\n",
    "        best_model = {'best_model_'+dataset_name:[]}\n",
    "        ct = make_column_transformer(\n",
    "                #(OneHotEncoder(sparse_output=False), make_column_selector(dtype_include=['category', 'object']))\n",
    "                (StandardScaler(), make_column_selector(dtype_include=['float32']))\n",
    "                ,remainder='drop')\n",
    "        pipe = Pipeline([('scaler',ct),\n",
    "                         ('pca', PCA()),#n_components=10\n",
    "                        ('estimator', net)])\n",
    "        rs = RandomizedSearchCV(pipe, param_grid_ah_tcga, scoring = scoring_function, n_jobs=-1, \n",
    "                                    n_iter=n_iter, refit=True, random_state=rand_state)\n",
    "        for i, (train_index, test_index) in enumerate(outer_custom_cv.split(X, y)):\n",
    "                # Split data into training and testing sets for outer fold\n",
    "                X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "                y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "                X_train, y_train = sort_X_y_pandas(X_train, y_train)\n",
    "                X_test, y_test = sort_X_y_pandas(X_test, y_test)\n",
    "\n",
    "                #print(X_train.shape, type(X_train))\n",
    "                #print(y_train.shape, type(y_train))\n",
    "                #print(X_test.shape, type(X_test))\n",
    "                #print(y_test.shape, type(y_test))\n",
    "                # save splits and data\n",
    "                savetxt('splits/train_index_'+str(i)+'_'+filename, train_index, delimiter=',')\n",
    "                savetxt('splits/test_index_'+str(i)+'_'+filename, test_index, delimiter=',')\n",
    "                \n",
    "                # savetxt('splits/X_train_'+str(i)+'_'+filename, X_train, delimiter=',')\n",
    "                # savetxt('splits/X_test_'+str(i)+'_'+filename, X_test, delimiter=',')\n",
    "\n",
    "                # savetxt('splits/y_train_'+str(i)+'_'+filename, y_train, delimiter=',')\n",
    "                # savetxt('splits/y_test_'+str(i)+'_'+filename, y_test, delimiter=',')\n",
    "\n",
    "                rs.fit(X_train, y_train)\n",
    "                best_preds_train = rs.best_estimator_.predict(X_train)\n",
    "                best_preds_test = rs.best_estimator_.predict(X_test)\n",
    "\n",
    "                savetxt('predictions/'+model+'best_preds_train_'+str(i)+'_'+filename, best_preds_train, delimiter=',')\n",
    "                savetxt('predictions/'+model+'best_preds_test_'+str(i)+'_'+filename, best_preds_test, delimiter=',')\n",
    "\n",
    "                \n",
    "                # save hyperparameter settings\n",
    "                params = rs.best_estimator_.get_params\n",
    "                best_params['best_params_'+dataset_name] += [rs.best_params_]\n",
    "                best_model['best_model_'+dataset_name] += [params]\n",
    "                try:\n",
    "                    cum_hazard_train = get_cumulative_hazard_function_ah(\n",
    "                            X_train.values, X_train.values, y_train.values, y_train.values,\n",
    "                            best_preds_train.reshape(-1), best_preds_train.reshape(-1)\n",
    "                            )\n",
    "\n",
    "                    df_survival_train = np.exp(-cum_hazard_train)\n",
    "                    durations_train, events_train = transform_back(y_train.values)\n",
    "                    time_grid_train = np.linspace(durations_train.min(), durations_train.max(), 100)\n",
    "                    ev = EvalSurv(df_survival_train, durations_train, events_train, censor_surv='km')\n",
    "                    print('Concordance Index',ev.concordance_td('antolini'))\n",
    "                    print('Integrated Brier Score:',ev.integrated_brier_score(time_grid_train))\n",
    "                    cindex_score_train = ev.concordance_td('antolini')\n",
    "                    ibs_score_train = ev.integrated_brier_score(time_grid_train)\n",
    "\n",
    "                    outer_scores['cindex_train_'+dataset_name] += [cindex_score_train]\n",
    "                    outer_scores['ibs_train_'+dataset_name] += [ibs_score_train]\n",
    "\n",
    "                except:\n",
    "                    outer_scores['cindex_train_'+dataset_name] += [np.nan]\n",
    "                    outer_scores['ibs_train_'+dataset_name] += [np.nan]\n",
    "                    \n",
    "                try:\n",
    "                    cum_hazard_test = get_cumulative_hazard_function_ah(\n",
    "                            X_train.values, X_test.values, y_train.values, y_test.values,\n",
    "                            best_preds_train.reshape(-1), best_preds_test.reshape(-1)\n",
    "                            )\n",
    "                    df_survival_test = np.exp(-cum_hazard_test)\n",
    "                    durations_test, events_test = transform_back(y_test.values)\n",
    "                    print('durations',durations_test.min(), durations_test.max())\n",
    "                    time_grid_test = np.linspace(durations_test.min(), durations_test.max(), 100)\n",
    "                    ev = EvalSurv(df_survival_test, durations_test, events_test, censor_surv='km')\n",
    "                    print('Concordance Index',ev.concordance_td('antolini'))\n",
    "                    print('Integrated Brier Score:',ev.integrated_brier_score(time_grid_test))\n",
    "                    cindex_score_test = ev.concordance_td('antolini')\n",
    "                    ibs_score_test = ev.integrated_brier_score(time_grid_test)\n",
    "\n",
    "                    outer_scores['cindex_test_'+dataset_name] += [cindex_score_test]\n",
    "                    outer_scores['ibs_test_'+dataset_name] += [ibs_score_test]\n",
    "                except: \n",
    "                    outer_scores['cindex_test_'+dataset_name] += [np.nan]\n",
    "                    outer_scores['ibs_test_'+dataset_name] += [np.nan]\n",
    "            \n",
    "        df_best_params = pd.DataFrame(best_params)\n",
    "        df_best_model = pd.DataFrame(best_model)\n",
    "        df_outer_scores = pd.DataFrame(outer_scores)\n",
    "        df_metrics = pd.concat([df_best_params,df_best_model,df_outer_scores], axis=1)\n",
    "        df_metrics.to_csv('metrics/'+model+'metric_summary'+str(i)+'_'+filename, index=False)\n",
    "        # cindex\n",
    "        df_agg_metrics_cindex = pd.DataFrame({'dataset':[dataset_name],\n",
    "                                              'cindex_train_mean':df_outer_scores['cindex_train_'+dataset_name].mean(),\n",
    "                                              'cindex_train_std':df_outer_scores['cindex_train_'+dataset_name].std(),\n",
    "                                              'cindex_test_mean':df_outer_scores['cindex_test_'+dataset_name].mean(),\n",
    "                                              'cindex_test_std':df_outer_scores['cindex_test_'+dataset_name].std() })\n",
    "        # IBS\n",
    "        df_agg_metrics_ibs = pd.DataFrame({'dataset':[dataset_name],\n",
    "                                              'ibs_train_mean':df_outer_scores['ibs_train_'+dataset_name].mean(),\n",
    "                                              'ibs_train_std':df_outer_scores['ibs_train_'+dataset_name].std(),\n",
    "                                              'ibs_test_mean':df_outer_scores['ibs_test_'+dataset_name].mean(),\n",
    "                                              'ibs_test_std':df_outer_scores['ibs_test_'+dataset_name].std() })\n",
    "        \n",
    "        return df_agg_metrics_cindex, df_agg_metrics_ibs, best_model, best_params, outer_scores, best_preds_train, best_preds_test \n",
    "\n",
    "                \n",
    "#cv=inner_custom_cv,pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split gex_?|100130426      float32\n",
      "gex_?|100133144      float32\n",
      "gex_?|100134869      float32\n",
      "gex_?|10357          float32\n",
      "gex_?|10431          float32\n",
      "                      ...   \n",
      "gex_ZYG11A|440590    float32\n",
      "gex_ZYG11B|79699     float32\n",
      "gex_ZYX|7791         float32\n",
      "gex_ZZEF1|23140      float32\n",
      "gex_ZZZ3|26009       float32\n",
      "Length: 20531, dtype: object\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "\nAll the 5 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n4 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.11/site-packages/sklearn/pipeline.py\", line 401, in fit\n    Xt = self._fit(X, y, **fit_params_steps)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.11/site-packages/sklearn/pipeline.py\", line 359, in _fit\n    X, fitted_transformer = fit_transform_one_cached(\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.11/site-packages/joblib/memory.py\", line 349, in __call__\n    return self.func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.11/site-packages/sklearn/pipeline.py\", line 893, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **fit_params)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n    data_to_wrap = f(self, X, *args, **kwargs)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.11/site-packages/sklearn/decomposition/_pca.py\", line 462, in fit_transform\n    U, S, Vt = self._fit(X)\n               ^^^^^^^^^^^^\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.11/site-packages/sklearn/decomposition/_pca.py\", line 512, in _fit\n    return self._fit_full(X, n_components)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.11/site-packages/sklearn/decomposition/_pca.py\", line 526, in _fit_full\n    raise ValueError(\nValueError: n_components=1024 must be between 0 and min(n_samples, n_features)=259 with svd_solver='full'\n\n--------------------------------------------------------------------------------\n1 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.11/site-packages/sklearn/pipeline.py\", line 401, in fit\n    Xt = self._fit(X, y, **fit_params_steps)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.11/site-packages/sklearn/pipeline.py\", line 359, in _fit\n    X, fitted_transformer = fit_transform_one_cached(\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.11/site-packages/joblib/memory.py\", line 349, in __call__\n    return self.func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.11/site-packages/sklearn/pipeline.py\", line 893, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **fit_params)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n    data_to_wrap = f(self, X, *args, **kwargs)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.11/site-packages/sklearn/decomposition/_pca.py\", line 462, in fit_transform\n    U, S, Vt = self._fit(X)\n               ^^^^^^^^^^^^\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.11/site-packages/sklearn/decomposition/_pca.py\", line 512, in _fit\n    return self._fit_full(X, n_components)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.11/site-packages/sklearn/decomposition/_pca.py\", line 526, in _fit_full\n    raise ValueError(\nValueError: n_components=1024 must be between 0 and min(n_samples, n_features)=260 with svd_solver='full'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 67\u001b[0m\n\u001b[1;32m     27\u001b[0m X, y \u001b[39m=\u001b[39m sort_X_y_pandas(X, y)\n\u001b[1;32m     29\u001b[0m net \u001b[39m=\u001b[39m NeuralNet(\n\u001b[1;32m     30\u001b[0m     SurvivalModel, \n\u001b[1;32m     31\u001b[0m     module__n_layers \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     65\u001b[0m     verbose\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m\n\u001b[1;32m     66\u001b[0m )\n\u001b[0;32m---> 67\u001b[0m df_agg_metrics_cindex, df_agg_metrics_ibs, best_model,params, outer_scores, best_preds_train, best_preds_test \u001b[39m=\u001b[39m train_eval(X, y, net, n_iter, data\u001b[39m.\u001b[39;49mfilename)\n\u001b[1;32m     68\u001b[0m agg_metrics_cindex\u001b[39m.\u001b[39mappend(df_agg_metrics_cindex)\n\u001b[1;32m     69\u001b[0m agg_metrics_ibs\u001b[39m.\u001b[39mappend(df_agg_metrics_ibs)\n",
      "Cell \u001b[0;32mIn[49], line 39\u001b[0m, in \u001b[0;36mtrain_eval\u001b[0;34m(X, y, net, n_iter, filename)\u001b[0m\n\u001b[1;32m     31\u001b[0m savetxt(\u001b[39m'\u001b[39m\u001b[39msplits/test_index_\u001b[39m\u001b[39m'\u001b[39m\u001b[39m+\u001b[39m\u001b[39mstr\u001b[39m(i)\u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39m_\u001b[39m\u001b[39m'\u001b[39m\u001b[39m+\u001b[39mfilename, test_index, delimiter\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m,\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     33\u001b[0m \u001b[39m# savetxt('splits/X_train_'+str(i)+'_'+filename, X_train, delimiter=',')\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[39m# savetxt('splits/X_test_'+str(i)+'_'+filename, X_test, delimiter=',')\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \n\u001b[1;32m     36\u001b[0m \u001b[39m# savetxt('splits/y_train_'+str(i)+'_'+filename, y_train, delimiter=',')\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[39m# savetxt('splits/y_test_'+str(i)+'_'+filename, y_test, delimiter=',')\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m rs\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[1;32m     40\u001b[0m best_preds_train \u001b[39m=\u001b[39m rs\u001b[39m.\u001b[39mbest_estimator_\u001b[39m.\u001b[39mpredict(X_train)\n\u001b[1;32m     41\u001b[0m best_preds_test \u001b[39m=\u001b[39m rs\u001b[39m.\u001b[39mbest_estimator_\u001b[39m.\u001b[39mpredict(X_test)\n",
      "File \u001b[0;32m~/miniconda3/envs/xgbsurv/lib/python3.11/site-packages/sklearn/model_selection/_search.py:874\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    868\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[1;32m    869\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    870\u001b[0m     )\n\u001b[1;32m    872\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[0;32m--> 874\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[1;32m    876\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    878\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/xgbsurv/lib/python3.11/site-packages/sklearn/model_selection/_search.py:1768\u001b[0m, in \u001b[0;36mRandomizedSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1766\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1767\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1768\u001b[0m     evaluate_candidates(\n\u001b[1;32m   1769\u001b[0m         ParameterSampler(\n\u001b[1;32m   1770\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_distributions, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_iter, random_state\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrandom_state\n\u001b[1;32m   1771\u001b[0m         )\n\u001b[1;32m   1772\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/xgbsurv/lib/python3.11/site-packages/sklearn/model_selection/_search.py:851\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    844\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m!=\u001b[39m n_candidates \u001b[39m*\u001b[39m n_splits:\n\u001b[1;32m    845\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    846\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mcv.split and cv.get_n_splits returned \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    847\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39minconsistent results. Expected \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    848\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39msplits, got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(n_splits, \u001b[39mlen\u001b[39m(out) \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m n_candidates)\n\u001b[1;32m    849\u001b[0m     )\n\u001b[0;32m--> 851\u001b[0m _warn_or_raise_about_fit_failures(out, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49merror_score)\n\u001b[1;32m    853\u001b[0m \u001b[39m# For callable self.scoring, the return type is only know after\u001b[39;00m\n\u001b[1;32m    854\u001b[0m \u001b[39m# calling. If the return type is a dictionary, the error scores\u001b[39;00m\n\u001b[1;32m    855\u001b[0m \u001b[39m# can now be inserted with the correct key. The type checking\u001b[39;00m\n\u001b[1;32m    856\u001b[0m \u001b[39m# of out will be done in `_insert_error_scores`.\u001b[39;00m\n\u001b[1;32m    857\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcallable\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscoring):\n",
      "File \u001b[0;32m~/miniconda3/envs/xgbsurv/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:367\u001b[0m, in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[0;34m(results, error_score)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[39mif\u001b[39;00m num_failed_fits \u001b[39m==\u001b[39m num_fits:\n\u001b[1;32m    361\u001b[0m     all_fits_failed_message \u001b[39m=\u001b[39m (\n\u001b[1;32m    362\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mAll the \u001b[39m\u001b[39m{\u001b[39;00mnum_fits\u001b[39m}\u001b[39;00m\u001b[39m fits failed.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    363\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIt is very likely that your model is misconfigured.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    364\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou can try to debug the error by setting error_score=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mraise\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    365\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBelow are more details about the failures:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mfit_errors_summary\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    366\u001b[0m     )\n\u001b[0;32m--> 367\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[1;32m    369\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    370\u001b[0m     some_fits_failed_message \u001b[39m=\u001b[39m (\n\u001b[1;32m    371\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mnum_failed_fits\u001b[39m}\u001b[39;00m\u001b[39m fits failed out of a total of \u001b[39m\u001b[39m{\u001b[39;00mnum_fits\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    372\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe score on these train-test partitions for these parameters\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBelow are more details about the failures:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mfit_errors_summary\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    377\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: \nAll the 5 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n4 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.11/site-packages/sklearn/pipeline.py\", line 401, in fit\n    Xt = self._fit(X, y, **fit_params_steps)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.11/site-packages/sklearn/pipeline.py\", line 359, in _fit\n    X, fitted_transformer = fit_transform_one_cached(\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.11/site-packages/joblib/memory.py\", line 349, in __call__\n    return self.func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.11/site-packages/sklearn/pipeline.py\", line 893, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **fit_params)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n    data_to_wrap = f(self, X, *args, **kwargs)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.11/site-packages/sklearn/decomposition/_pca.py\", line 462, in fit_transform\n    U, S, Vt = self._fit(X)\n               ^^^^^^^^^^^^\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.11/site-packages/sklearn/decomposition/_pca.py\", line 512, in _fit\n    return self._fit_full(X, n_components)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.11/site-packages/sklearn/decomposition/_pca.py\", line 526, in _fit_full\n    raise ValueError(\nValueError: n_components=1024 must be between 0 and min(n_samples, n_features)=259 with svd_solver='full'\n\n--------------------------------------------------------------------------------\n1 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.11/site-packages/sklearn/pipeline.py\", line 401, in fit\n    Xt = self._fit(X, y, **fit_params_steps)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.11/site-packages/sklearn/pipeline.py\", line 359, in _fit\n    X, fitted_transformer = fit_transform_one_cached(\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.11/site-packages/joblib/memory.py\", line 349, in __call__\n    return self.func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.11/site-packages/sklearn/pipeline.py\", line 893, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **fit_params)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n    data_to_wrap = f(self, X, *args, **kwargs)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.11/site-packages/sklearn/decomposition/_pca.py\", line 462, in fit_transform\n    U, S, Vt = self._fit(X)\n               ^^^^^^^^^^^^\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.11/site-packages/sklearn/decomposition/_pca.py\", line 512, in _fit\n    return self._fit_full(X, n_components)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.11/site-packages/sklearn/decomposition/_pca.py\", line 526, in _fit_full\n    raise ValueError(\nValueError: n_components=1024 must be between 0 and min(n_samples, n_features)=260 with svd_solver='full'\n"
     ]
    }
   ],
   "source": [
    "cancer_types = [\n",
    "    'BLCA',\n",
    "    'BRCA',\n",
    "    'HNSC',\n",
    "    'KIRC',\n",
    "    'LGG',\n",
    "    'LIHC',\n",
    "    'LUAD',\n",
    "    'LUSC',\n",
    "    'OV',\n",
    "    'STAD']\n",
    "\n",
    "agg_metrics_cindex = []\n",
    "agg_metrics_ibs = []\n",
    "\n",
    "\n",
    "class InputShapeSetter(skorch.callbacks.Callback):\n",
    "    def on_train_begin(self, net, X, y):\n",
    "        net.set_params(module__input_units=X.shape[-1])\n",
    "\n",
    "for idx, cancer_type in enumerate(cancer_types):\n",
    "    # get name of current dataset\n",
    "    data = load_tcga(path=\"/Users/JUSC/Documents/xgbsurv/xgbsurv/datasets/data/\", cancer_type=cancer_type, as_frame=True)\n",
    "    X  = data.data #.astype(np.float32)\n",
    "    y = data.target #.values #.to_numpy()\n",
    "\n",
    "    X, y = sort_X_y_pandas(X, y)\n",
    "\n",
    "    net = NeuralNet(\n",
    "        SurvivalModel, \n",
    "        module__n_layers = 1,\n",
    "        module__input_units = X.shape[1],\n",
    "        #module__num_nodes = 32,\n",
    "        #module__dropout = 0.1, # these could also be removed\n",
    "        module__out_features = 1,\n",
    "        # for split sizes when result size = 1\n",
    "        iterator_train__drop_last=True,\n",
    "        #iterator_valid__drop_last=True,\n",
    "        criterion=AHLoss,\n",
    "        optimizer=torch.optim.AdamW,\n",
    "        optimizer__weight_decay = 0.4,\n",
    "        batch_size=32, # separate train and valid->iterator_train__batch_size=128 and iterator_valid__batch_size=128 ?\n",
    "        callbacks=[\n",
    "            (\n",
    "                \"sched\",\n",
    "                LRScheduler(\n",
    "                    torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
    "                    monitor=\"valid_loss\",\n",
    "                    patience=5,\n",
    "                ),\n",
    "            ),\n",
    "            (\n",
    "                \"es\",\n",
    "                EarlyStopping(\n",
    "                    monitor=\"valid_loss\",\n",
    "                    patience=early_stopping_rounds,\n",
    "                    load_best=True,\n",
    "                ),\n",
    "            ),\n",
    "            (\"seed\", FixSeed(seed=42)),\n",
    "            (\"Input Shape Setter\",InputShapeSetter())\n",
    "        ],#[EarlyStopping(patience=10)],#,InputShapeSetter()],\n",
    "        #TODO: enable stratification, verify\n",
    "        train_split=ValidSplit(0.2), # might cause lower performance in metrics, explain in thesis\n",
    "        verbose=2\n",
    "    )\n",
    "    df_agg_metrics_cindex, df_agg_metrics_ibs, best_model,params, outer_scores, best_preds_train, best_preds_test = train_eval(X, y, net, n_iter, data.filename)\n",
    "    agg_metrics_cindex.append(df_agg_metrics_cindex)\n",
    "    agg_metrics_ibs.append(df_agg_metrics_ibs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_ah_1_cindex = pd.concat([df for df in agg_metrics_cindex]).round(4)\n",
    "df_final_ah_1_cindex.to_csv('metrics/final_deep_learning_ah_tcga_cindex.csv', index=False)\n",
    "df_final_ah_1_cindex.to_csv('/Users/JUSC/Documents/644928e0fb7e147893e8ec15/05_thesis/tables/final_deep_learning_ah_tcga_cindex.csv', index=False)  #\n",
    "df_final_ah_1_cindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_ah_1_ibs = pd.concat([df for df in agg_metrics_ibs]).round(4)\n",
    "df_final_ah_1_ibs.to_csv('metrics/final_deep_learning_ah_tcga_ibs.csv', index=False)\n",
    "df_final_ah_1_ibs.to_csv('/Users/JUSC/Documents/644928e0fb7e147893e8ec15/05_thesis/tables/final_deep_learning_ah_tcga_ibs.csv', index=False) \n",
    "df_final_ah_1_ibs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xgbsurv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
