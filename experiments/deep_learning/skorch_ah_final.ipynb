{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import savetxt\n",
    "from xgbsurv.datasets import (load_metabric, load_flchain, load_rgbsg, load_support, load_tcga)\n",
    "from xgbsurv.models.utils import sort_X_y_pandas, transform_back, transform\n",
    "from xgbsurv.models.eh_ah_final import get_cumulative_hazard_function_ah\n",
    "import torch\n",
    "from torch import nn\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold, train_test_split\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, LabelBinarizer, OneHotEncoder\n",
    "from sklearn.compose import make_column_transformer, make_column_selector\n",
    "from sklearn.decomposition import PCA\n",
    "from loss_functions_pytorch import AHLoss, ah_likelihood_torch\n",
    "from skorch import NeuralNet\n",
    "from skorch.callbacks import EarlyStopping, Callback, LRScheduler\n",
    "import skorch.callbacks\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import check_cv\n",
    "from numbers import Number\n",
    "import torch.utils.data\n",
    "from skorch.utils import flatten\n",
    "from skorch.utils import is_pandas_ndframe\n",
    "from skorch.utils import check_indexing\n",
    "from skorch.utils import multi_indexing\n",
    "from skorch.utils import to_numpy\n",
    "from skorch.dataset import get_len\n",
    "from skorch.dataset import ValidSplit\n",
    "from pycox.evaluation import EvalSurv\n",
    "from scipy.stats import uniform as scuniform\n",
    "from scipy.stats import randint as scrandint\n",
    "from scipy.stats import loguniform as scloguniform\n",
    "import random\n",
    "import os\n",
    "#torch.set_default_dtype(torch.float64)\n",
    "#torch.set_default_tensor_type(torch.DoubleTensor)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters, put into function\n",
    "n_outer_splits = 5\n",
    "n_inner_splits = 5\n",
    "rand_state = 42\n",
    "n_iter = 1\n",
    "early_stopping_rounds=10\n",
    "base_score = 0.0\n",
    "\n",
    "# set seed for scipy/numpy\n",
    "np.random.seed(rand_state)\n",
    "\n",
    "param_grid_ah = {\n",
    "    'estimator__module__n_layers': [1, 2, 4],\n",
    "    'estimator__module__num_nodes': [64, 128, 256, 512],\n",
    "    'estimator__module__dropout': scuniform(0.0,0.7),\n",
    "    'estimator__optimizer__weight_decay': [0.4, 0.2, 0.1, 0.05, 0.02, 0.01, 0],\n",
    "    'estimator__batch_size': [64, 128, 256, 512, 1024],\n",
    "    #lr not in paper because of learning rate finder\n",
    "    # note: setting learning rate higher would make exp(partial_hazard) explode\n",
    "    #'estimator__lr': scloguniform(0.001,0.01), # scheduler unten einbauen\n",
    "    # use callback instead\n",
    "    'estimator__lr':[0.01]\n",
    "    #'max_epochs':  scrandint(10,20), # corresponds to num_rounds\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_torch(seed=42):\n",
    "    \"\"\"Sets all seeds within torch and adjacent libraries.\n",
    "\n",
    "    Args:\n",
    "        seed: Random seed to be used by the seeding functions.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    #os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    #torch.backends.cudnn.benchmark = False\n",
    "    #torch.backends.cudnn.deterministic = True\n",
    "    return None\n",
    "\n",
    "\n",
    "class FixSeed(Callback):\n",
    "    def __init__(self, seed):\n",
    "        self.seed = seed\n",
    "\n",
    "    def initialize(self):\n",
    "        seed_torch(self.seed)\n",
    "        return super().initialize()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Scorer\n",
    "def custom_scoring_function(y_true, y_pred):\n",
    "\n",
    "        #y_true = torch.from_numpy(y_true)\n",
    "        if isinstance(y_pred, np.ndarray):\n",
    "            y_pred = torch.from_numpy(y_pred)\n",
    "        if isinstance(y_true, np.ndarray):\n",
    "            y_true = torch.from_numpy(y_true)\n",
    "        if isinstance(y_pred, pd.Series):\n",
    "            y_pred = torch.tensor(y_pred.values)\n",
    "        if isinstance(y_true, pd.Series):\n",
    "            y_true = torch.tensor(y_true.values)\n",
    "        #print('loss function y_pred', y_pred)\n",
    "        #print('loss function y_true', y_true)\n",
    "        score = ah_likelihood_torch(y_pred, y_true).to(torch.float32)\n",
    "        return score.numpy()\n",
    "\n",
    "scoring_function = make_scorer(custom_scoring_function, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set up Custom Splitter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Torch Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SurvivalModel(nn.Module):\n",
    "    def __init__(self, n_layers, input_units, num_nodes, dropout, out_features):\n",
    "        super(SurvivalModel, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.in_features = input_units\n",
    "        self.num_nodes = num_nodes\n",
    "        self.dropout = dropout\n",
    "        self.out_features = out_features\n",
    "        model = []\n",
    "        # first layer\n",
    "        model.append(torch.nn.Linear(input_units, num_nodes))\n",
    "        model.append(torch.nn.ReLU())\n",
    "        model.append(torch.nn.Dropout(dropout))\n",
    "        model.append(torch.nn.BatchNorm1d(num_nodes))\n",
    "\n",
    "        for i in range(n_layers-1):\n",
    "            model.append(torch.nn.Linear(num_nodes, num_nodes))\n",
    "            #init.kaiming_normal_(model[-1].weight, nonlinearity='relu')\n",
    "            model.append(torch.nn.ReLU())\n",
    "            model.append(torch.nn.Dropout(dropout))\n",
    "            model.append(torch.nn.BatchNorm1d(num_nodes))\n",
    "\n",
    "        # output layer\n",
    "        model.append(torch.nn.Linear(num_nodes, out_features))\n",
    "    \n",
    "        self.layers = nn.Sequential(*model)\n",
    "\n",
    "        # for layer in self.layers:\n",
    "        #     if isinstance(layer, nn.Linear):\n",
    "        #         #nn.init.uniform_(layer.weight, a=-0.5, b=0.5)\n",
    "        #         nn.init.kaiming_normal_(layer.weight)\n",
    "\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = X.to(torch.float32)\n",
    "        res = self.layers(X)\n",
    "        #print(res)\n",
    "        return res\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomStandardScaler(StandardScaler):\n",
    "    \n",
    "    def __init__(self, copy=True, with_mean=True, with_std=True):\n",
    "        super().__init__(copy=copy, with_mean=with_mean, with_std=with_std)\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return super().fit(X, y)\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X_transformed = super().transform(X, y)\n",
    "        return X_transformed.astype(np.float32)\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        X_transformed = super().fit_transform(X, y)\n",
    "        return X_transformed.astype(np.float32)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define stratified inner k-fold cross-validation\n",
    "class CustomSplit(StratifiedKFold):\n",
    "    def __init__(self, n_splits=5, shuffle=True, random_state=None):\n",
    "        super().__init__(n_splits=n_splits, shuffle=shuffle, random_state=random_state)\n",
    "\n",
    "    def split(self, X, y, groups=None):\n",
    "        print('split', X.dtypes)\n",
    "        try:\n",
    "            if y.shape[1]>1:\n",
    "                y = y[:,0]\n",
    "        except:\n",
    "            pass\n",
    "        bins = np.sign(y)\n",
    "        return super().split(X, bins, groups=groups)\n",
    "\n",
    "    def get_n_splits(self, X=None, y=None, groups=None):\n",
    "        return self.n_splits\n",
    "\n",
    "outer_custom_cv = CustomSplit(n_splits=n_outer_splits, shuffle=True, random_state=rand_state)\n",
    "inner_custom_cv = CustomSplit(n_splits=n_outer_splits, shuffle=True, random_state=rand_state)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Valid Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class CustomStandardScaler(StandardScaler):\n",
    "    \n",
    "    def __init__(self, copy=True, with_mean=True, with_std=True):\n",
    "        super().__init__(copy=copy, with_mean=with_mean, with_std=with_std)\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return super().fit(X, y)\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X_transformed = super().transform(X, y)\n",
    "        return X_transformed.astype(np.float32)\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        X_transformed = super().fit_transform(X, y)\n",
    "        return X_transformed.astype(np.float32)\n",
    "    \n",
    "class CustomValidSplit():\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            cv=5,\n",
    "            stratified=False,\n",
    "            random_state=None,\n",
    "    ):\n",
    "        self.stratified = stratified\n",
    "        self.random_state = random_state\n",
    "\n",
    "        if isinstance(cv, Number) and (cv <= 0):\n",
    "            raise ValueError(\"Numbers less than 0 are not allowed for cv \"\n",
    "                             \"but ValidSplit got {}\".format(cv))\n",
    "\n",
    "        if not self._is_float(cv) and random_state is not None:\n",
    "            raise ValueError(\n",
    "                \"Setting a random_state has no effect since cv is not a float. \"\n",
    "                \"You should leave random_state to its default (None), or set cv \"\n",
    "                \"to a float value.\",\n",
    "            )\n",
    "\n",
    "        self.cv = cv\n",
    "\n",
    "    def _is_stratified(self, cv):\n",
    "        return isinstance(cv, (StratifiedKFold, StratifiedShuffleSplit))\n",
    "\n",
    "    def _is_float(self, x):\n",
    "        if not isinstance(x, Number):\n",
    "            return False\n",
    "        return not float(x).is_integer()\n",
    "\n",
    "    def _check_cv_float(self):\n",
    "        cv_cls = StratifiedShuffleSplit if self.stratified else ShuffleSplit\n",
    "        return cv_cls(test_size=self.cv, random_state=self.random_state)\n",
    "\n",
    "    def _check_cv_non_float(self, y):\n",
    "        return check_cv(\n",
    "            self.cv,\n",
    "            y=y,\n",
    "            classifier=self.stratified,\n",
    "        )\n",
    "\n",
    "    def check_cv(self, y):\n",
    "        \"\"\"Resolve which cross validation strategy is used.\"\"\"\n",
    "        y_arr = None\n",
    "        if self.stratified:\n",
    "            # Try to convert y to numpy for sklearn's check_cv; if conversion\n",
    "            # doesn't work, still try.\n",
    "            try:\n",
    "                y_arr = to_numpy(y)\n",
    "            except (AttributeError, TypeError):\n",
    "                y_arr = y\n",
    "\n",
    "        if self._is_float(self.cv):\n",
    "            return self._check_cv_float()\n",
    "        return self._check_cv_non_float(y_arr)\n",
    "\n",
    "    def _is_regular(self, x):\n",
    "        return (x is None) or isinstance(x, np.ndarray) or is_pandas_ndframe(x)\n",
    "\n",
    "    def __call__(self, dataset, y=None, groups=None):\n",
    "        # key change here\n",
    "        y = np.sign(y)\n",
    "        bad_y_error = ValueError(\n",
    "            \"Stratified CV requires explicitly passing a suitable y.\")\n",
    "        if (y is None) and self.stratified:\n",
    "            raise bad_y_error\n",
    "\n",
    "        cv = self.check_cv(y)\n",
    "        if self.stratified and not self._is_stratified(cv):\n",
    "            raise bad_y_error\n",
    "\n",
    "        # pylint: disable=invalid-name\n",
    "        len_dataset = get_len(dataset)\n",
    "        if y is not None:\n",
    "            len_y = get_len(y)\n",
    "            if len_dataset != len_y:\n",
    "                raise ValueError(\"Cannot perform a CV split if dataset and y \"\n",
    "                                 \"have different lengths.\")\n",
    "\n",
    "        args = (np.arange(len_dataset),)\n",
    "        if self._is_stratified(cv):\n",
    "            args = args + (to_numpy(y),)\n",
    "\n",
    "        idx_train, idx_valid = next(iter(cv.split(*args, groups=groups)))\n",
    "        dataset_train = torch.utils.data.Subset(dataset, idx_train)\n",
    "        dataset_valid = torch.utils.data.Subset(dataset, idx_valid)\n",
    "        return dataset_train, dataset_valid\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Training Procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_eval(X, y, net, n_iter, filename):\n",
    "        model = 'ah_'\n",
    "        dataset_name = filename.split('_')[0]\n",
    "        # add IBS later\n",
    "        outer_scores = {'cindex_train_'+dataset_name:[], 'cindex_test_'+dataset_name:[],\n",
    "                        'ibs_train_'+dataset_name:[], 'ibs_test_'+dataset_name:[]}\n",
    "        best_params = {'best_params_'+dataset_name:[]}\n",
    "        best_model = {'best_model_'+dataset_name:[]}\n",
    "        ct = make_column_transformer(\n",
    "                #(OneHotEncoder(sparse_output=False), make_column_selector(dtype_include=['category', 'object'])),\n",
    "                (StandardScaler(), make_column_selector(dtype_include=['float32']))\n",
    "                ,remainder='passthrough')\n",
    "\n",
    "        pipe = Pipeline([('scaler',ct),\n",
    "                        ('estimator', net)])\n",
    "        rs = RandomizedSearchCV(pipe, param_grid_ah, scoring = scoring_function, n_jobs=-1, \n",
    "                                    n_iter=2, refit=True)\n",
    "        for i, (train_index, test_index) in enumerate(outer_custom_cv.split(X, y)):\n",
    "                # Split data into training and testing sets for outer fold\n",
    "                X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "                y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "                X_train, y_train = sort_X_y_pandas(X_train, y_train)\n",
    "                X_test, y_test = sort_X_y_pandas(X_test, y_test)\n",
    "\n",
    "                # print(X_train.shape, type(X_train))\n",
    "                # print(y_train.shape, type(y_train))\n",
    "                # print(X_test.shape, type(X_test))\n",
    "                # print(y_test.shape, type(y_test))\n",
    "                # save splits and data\n",
    "                savetxt('splits/'+model+'train_index_'+str(i)+'_'+filename, train_index, delimiter=',')\n",
    "                savetxt('splits/'+model+'test_index_'+str(i)+'_'+filename, test_index, delimiter=',')\n",
    "                \n",
    "                # savetxt('splits/X_train_'+str(i)+'_'+filename, X_train, delimiter=',')\n",
    "                # savetxt('splits/X_test_'+str(i)+'_'+filename, X_test, delimiter=',')\n",
    "\n",
    "                # savetxt('splits/y_train_'+str(i)+'_'+filename, y_train, delimiter=',')\n",
    "                # savetxt('splits/y_test_'+str(i)+'_'+filename, y_test, delimiter=',')\n",
    "\n",
    "\n",
    "                rs.fit(X_train, y_train)\n",
    "                best_preds_train = rs.best_estimator_.predict(X_train)\n",
    "                best_preds_test = rs.best_estimator_.predict(X_test)\n",
    "\n",
    "                savetxt('predictions/'+model+'best_preds_train_'+str(i)+'_'+filename, best_preds_train, delimiter=',')\n",
    "                savetxt('predictions/'+model+'best_preds_test_'+str(i)+'_'+filename, best_preds_test, delimiter=',')\n",
    "\n",
    "                # save hyperparameter settings\n",
    "                params = rs.best_estimator_.get_params\n",
    "                best_params['best_params_'+dataset_name] += [rs.best_params_]\n",
    "                best_model['best_model_'+dataset_name] += [params]\n",
    "                try:\n",
    "                    cum_hazard_train = get_cumulative_hazard_function_ah(\n",
    "                            X_train.values, X_train.values, y_train.values, y_train.values,\n",
    "                            best_preds_train.reshape(-1), best_preds_train.reshape(-1)\n",
    "                            )\n",
    "\n",
    "                    df_survival_train = np.exp(-cum_hazard_train)\n",
    "                    durations_train, events_train = transform_back(y_train.values)\n",
    "                    time_grid_train = np.linspace(durations_train.min(), durations_train.max(), 100)\n",
    "                    ev = EvalSurv(df_survival_train, durations_train, events_train, censor_surv='km')\n",
    "                    print('Concordance Index',ev.concordance_td('antolini'))\n",
    "                    print('Integrated Brier Score:',ev.integrated_brier_score(time_grid_train))\n",
    "                    cindex_score_train = ev.concordance_td('antolini')\n",
    "                    ibs_score_train = ev.integrated_brier_score(time_grid_train)\n",
    "\n",
    "                    outer_scores['cindex_train_'+dataset_name] += [cindex_score_train]\n",
    "                    outer_scores['ibs_train_'+dataset_name] += [ibs_score_train]\n",
    "\n",
    "                except:\n",
    "                    outer_scores['cindex_train_'+dataset_name] += [np.nan]\n",
    "                    outer_scores['ibs_train_'+dataset_name] += [np.nan]\n",
    "                    \n",
    "                try:\n",
    "                    cum_hazard_test = get_cumulative_hazard_function_ah(\n",
    "                            X_train.values, X_test.values, y_train.values, y_test.values,\n",
    "                            best_preds_train.reshape(-1), best_preds_test.reshape(-1)\n",
    "                            )\n",
    "                    df_survival_test = np.exp(-cum_hazard_test)\n",
    "                    durations_test, events_test = transform_back(y_test.values)\n",
    "                    print('durations',durations_test.min(), durations_test.max())\n",
    "                    time_grid_test = np.linspace(durations_test.min(), durations_test.max(), 100)\n",
    "                    ev = EvalSurv(df_survival_test, durations_test, events_test, censor_surv='km')\n",
    "                    print('Concordance Index',ev.concordance_td('antolini'))\n",
    "                    print('Integrated Brier Score:',ev.integrated_brier_score(time_grid_test))\n",
    "                    cindex_score_test = ev.concordance_td('antolini')\n",
    "                    ibs_score_test = ev.integrated_brier_score(time_grid_test)\n",
    "\n",
    "                    outer_scores['cindex_test_'+dataset_name] += [cindex_score_test]\n",
    "                    outer_scores['ibs_test_'+dataset_name] += [ibs_score_test]\n",
    "                except: \n",
    "                    outer_scores['cindex_test_'+dataset_name] += [np.nan]\n",
    "                    outer_scores['ibs_test_'+dataset_name] += [np.nan]\n",
    "            \n",
    "        df_best_params = pd.DataFrame(best_params)\n",
    "        df_best_model = pd.DataFrame(best_model)\n",
    "        df_outer_scores = pd.DataFrame(outer_scores)\n",
    "        df_metrics = pd.concat([df_best_params,df_best_model,df_outer_scores], axis=1)\n",
    "        df_metrics.to_csv('metrics/'+model+'metric_summary'+str(i)+'_'+filename, index=False)\n",
    "       \n",
    "        # cindex\n",
    "        df_agg_metrics_cindex = pd.DataFrame({'dataset':[dataset_name],\n",
    "                                              'cindex_train_mean':df_outer_scores['cindex_train_'+dataset_name].mean(),\n",
    "                                              'cindex_train_std':df_outer_scores['cindex_train_'+dataset_name].std(),\n",
    "                                              'cindex_test_mean':df_outer_scores['cindex_test_'+dataset_name].mean(),\n",
    "                                              'cindex_test_std':df_outer_scores['cindex_test_'+dataset_name].std() })\n",
    "        # IBS\n",
    "        df_agg_metrics_ibs = pd.DataFrame({'dataset':[dataset_name],\n",
    "                                              'ibs_train_mean':df_outer_scores['ibs_train_'+dataset_name].mean(),\n",
    "                                              'ibs_train_std':df_outer_scores['ibs_train_'+dataset_name].std(),\n",
    "                                              'ibs_test_mean':df_outer_scores['ibs_test_'+dataset_name].mean(),\n",
    "                                              'ibs_test_std':df_outer_scores['ibs_test_'+dataset_name].std() })\n",
    "\n",
    "        return df_agg_metrics_cindex, df_agg_metrics_ibs, best_model, best_params, outer_scores, best_preds_train, best_preds_test #, X_train, X_test, y_train, y_test\n",
    "\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skorch.callbacks\n",
    "class InputShapeSetter(skorch.callbacks.Callback):\n",
    "    def on_train_begin(self, net, X, y):\n",
    "        net.set_params(module__input_units=X.shape[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_metabric\n",
      "split MKI67                float32\n",
      "EGFR                 float32\n",
      "PGR                  float32\n",
      "ERBB2                float32\n",
      "hormone_treatment      uint8\n",
      "radiotherapy           uint8\n",
      "chemotherapy           uint8\n",
      "ER_positive            uint8\n",
      "age                  float32\n",
      "dtype: object\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.8545\u001b[0m        \u001b[32m2.7662\u001b[0m  0.0382\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.8512\u001b[0m        \u001b[32m2.7797\u001b[0m  0.0499\n",
      "      2        \u001b[36m2.8290\u001b[0m        \u001b[32m2.7650\u001b[0m  0.0267\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.2106\u001b[0m        \u001b[32m3.2046\u001b[0m  0.0652\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.0722\u001b[0m        \u001b[32m3.0141\u001b[0m  0.0432\n",
      "      2        \u001b[36m2.8486\u001b[0m        \u001b[32m2.7724\u001b[0m  0.0227\n",
      "      3        \u001b[36m2.8089\u001b[0m        2.7657  0.0240\n",
      "      3        \u001b[36m2.8454\u001b[0m        \u001b[32m2.7697\u001b[0m  0.0230\n",
      "      2        \u001b[36m3.0434\u001b[0m        \u001b[32m3.0117\u001b[0m  0.0349\n",
      "      2        \u001b[36m3.1942\u001b[0m        \u001b[32m3.2032\u001b[0m  0.0356\n",
      "      4        \u001b[36m2.8004\u001b[0m        2.7655  0.0237\n",
      "      4        \u001b[36m2.8382\u001b[0m        \u001b[32m2.7669\u001b[0m  0.0245\n",
      "      3        \u001b[36m3.0319\u001b[0m        \u001b[32m3.0028\u001b[0m  0.0271\n",
      "      5        \u001b[36m2.7954\u001b[0m        \u001b[32m2.7639\u001b[0m  0.0180\n",
      "      3        \u001b[36m3.1804\u001b[0m        3.2107  0.0339\n",
      "      6        \u001b[36m2.7917\u001b[0m        \u001b[32m2.7610\u001b[0m  0.0166\n",
      "      4        \u001b[36m3.0245\u001b[0m        \u001b[32m2.9923\u001b[0m  0.0259\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.1928\u001b[0m        \u001b[32m3.2002\u001b[0m  0.0437\n",
      "      5        \u001b[36m2.8199\u001b[0m        \u001b[32m2.7647\u001b[0m  0.0403\n",
      "      4        3.1834        3.2149  0.0301\n",
      "      7        \u001b[36m2.7813\u001b[0m        2.7615  0.0233\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.1852\u001b[0m        \u001b[32m3.1847\u001b[0m  0.0465\n",
      "      5        \u001b[36m3.0162\u001b[0m        \u001b[32m2.9854\u001b[0m  0.0174\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.0818\u001b[0m        \u001b[32m3.0325\u001b[0m  0.0495\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.2026\u001b[0m        \u001b[32m3.2110\u001b[0m  0.0516\n",
      "      2        \u001b[36m3.1652\u001b[0m        \u001b[32m3.1979\u001b[0m  0.0235\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.2299\u001b[0m        \u001b[32m3.2242\u001b[0m  0.0504\n",
      "      6        \u001b[36m3.0063\u001b[0m        \u001b[32m2.9825\u001b[0m  0.0182\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.2200\u001b[0m        \u001b[32m3.2255\u001b[0m  0.0612\n",
      "      8        \u001b[36m2.7774\u001b[0m        \u001b[32m2.7547\u001b[0m  0.0206\n",
      "      6        \u001b[36m2.8178\u001b[0m        \u001b[32m2.7628\u001b[0m  0.0351\n",
      "      2        \u001b[36m3.2028\u001b[0m        \u001b[32m3.2184\u001b[0m  0.0203\n",
      "      3        \u001b[36m3.1541\u001b[0m        \u001b[32m3.1936\u001b[0m  0.0234\n",
      "      2        \u001b[36m3.1657\u001b[0m        \u001b[32m3.1758\u001b[0m  0.0337\n",
      "      5        3.1819        3.2104  0.0413      2        \u001b[36m3.1946\u001b[0m        \u001b[32m3.2060\u001b[0m  0.0299\n",
      "\n",
      "      2        \u001b[36m3.0809\u001b[0m        \u001b[32m3.0282\u001b[0m  0.0354\n",
      "      7        \u001b[36m2.9990\u001b[0m        \u001b[32m2.9793\u001b[0m  0.0203\n",
      "      9        \u001b[36m2.7756\u001b[0m        \u001b[32m2.7470\u001b[0m  0.0298\n",
      "      2        3.2295        \u001b[32m3.2222\u001b[0m  0.0333\n",
      "      7        \u001b[36m2.8164\u001b[0m        \u001b[32m2.7604\u001b[0m  0.0293\n",
      "      3        \u001b[36m3.1516\u001b[0m        \u001b[32m3.1649\u001b[0m  0.0246\n",
      "      4        \u001b[36m3.1429\u001b[0m        \u001b[32m3.1917\u001b[0m  0.0284\n",
      "      3        \u001b[36m3.1880\u001b[0m        \u001b[32m3.2129\u001b[0m  0.0322\n",
      "      8        2.9991        \u001b[32m2.9779\u001b[0m  0.0245\n",
      "      3        \u001b[36m3.0671\u001b[0m        \u001b[32m3.0208\u001b[0m  0.0249\n",
      "     10        \u001b[36m2.7632\u001b[0m        \u001b[32m2.7433\u001b[0m  0.0193      3        \u001b[36m3.1890\u001b[0m        3.2069  0.0334\n",
      "\n",
      "      6        \u001b[36m3.1743\u001b[0m        \u001b[32m3.2020\u001b[0m  0.0374\n",
      "      4        \u001b[36m3.1436\u001b[0m        \u001b[32m3.1639\u001b[0m  0.0208\n",
      "      8        \u001b[36m2.8159\u001b[0m        \u001b[32m2.7571\u001b[0m  0.0279\n",
      "      3        \u001b[36m3.2141\u001b[0m        3.2293  0.0314\n",
      "      9        \u001b[36m2.9855\u001b[0m        2.9794  0.0227\n",
      "      5        \u001b[36m3.1387\u001b[0m        \u001b[32m3.1916\u001b[0m  0.0291\n",
      "      4        3.0676        \u001b[32m3.0131\u001b[0m  0.0264\n",
      "      4        \u001b[36m3.1827\u001b[0m        \u001b[32m3.2088\u001b[0m  0.0354\n",
      "      4        \u001b[36m3.1513\u001b[0m        \u001b[32m3.2015\u001b[0m  0.0313\n",
      "      7        \u001b[36m3.1661\u001b[0m        \u001b[32m3.1872\u001b[0m  0.0344\n",
      "     10        \u001b[36m2.9797\u001b[0m        2.9799  0.0238\n",
      "Restoring best model from epoch 8.\n",
      "      9        \u001b[36m2.8111\u001b[0m        \u001b[32m2.7527\u001b[0m  0.0272\n",
      "      5        \u001b[36m3.1381\u001b[0m        \u001b[32m3.1518\u001b[0m  0.0338\n",
      "      4        3.2149        3.2269  0.0299\n",
      "      6        \u001b[36m3.1286\u001b[0m        3.1923  0.0274\n",
      "      5        \u001b[36m3.1734\u001b[0m        \u001b[32m3.2066\u001b[0m  0.0200\n",
      "      6        \u001b[36m3.1320\u001b[0m        \u001b[32m3.1482\u001b[0m  0.0186\n",
      "      5        3.1690        \u001b[32m3.1924\u001b[0m  0.0364\n",
      "      6        3.1736        \u001b[32m3.2046\u001b[0m  0.0214\n",
      "      7        \u001b[36m3.1251\u001b[0m        \u001b[32m3.1902\u001b[0m  0.0240\n",
      "      5        \u001b[36m3.0539\u001b[0m        3.0141  0.0516\n",
      "      8        \u001b[36m3.1576\u001b[0m        \u001b[32m3.1779\u001b[0m  0.0351\n",
      "     10        \u001b[36m2.7997\u001b[0m        \u001b[32m2.7463\u001b[0m  0.0322\n",
      "      7        \u001b[36m3.1288\u001b[0m        \u001b[32m3.1480\u001b[0m  0.0186\n",
      "      5        3.2204        3.2237  0.0344\n",
      "      7        \u001b[36m3.1710\u001b[0m        \u001b[32m3.2040\u001b[0m  0.0173\n",
      "      6        3.1658        \u001b[32m3.1875\u001b[0m  0.0276\n",
      "      8        \u001b[36m3.1209\u001b[0m        3.1914  0.0236\n",
      "      8        \u001b[36m3.1190\u001b[0m        3.1501  0.0237\n",
      "      6        \u001b[36m3.2030\u001b[0m        3.2228  0.0250\n",
      "      8        \u001b[36m3.1672\u001b[0m        3.2045  0.0179\n",
      "      6        \u001b[36m3.0376\u001b[0m        3.0150  0.0350\n",
      "      9        3.1604        3.1818  0.0365\n",
      "      9        \u001b[36m3.1153\u001b[0m        3.1909  0.0172\n",
      "      7        3.1623        \u001b[32m3.1865\u001b[0m  0.0256\n",
      "      9        \u001b[36m3.1648\u001b[0m        \u001b[32m3.2033\u001b[0m  0.0187\n",
      "      9        \u001b[36m3.1159\u001b[0m        3.1489  0.0213\n",
      "     10        \u001b[36m3.1088\u001b[0m        3.1909  0.0169\n",
      "Restoring best model from epoch 7.\n",
      "      7        \u001b[36m3.1980\u001b[0m        \u001b[32m3.2174\u001b[0m  0.0243\n",
      "     10        \u001b[36m3.1463\u001b[0m        \u001b[32m3.1765\u001b[0m  0.0254\n",
      "     10        \u001b[36m3.1596\u001b[0m        3.2042  0.0176\n",
      "      7        3.0500        3.0177  0.0348\n",
      "Restoring best model from epoch 9.\n",
      "     10        \u001b[36m3.1149\u001b[0m        3.1480  0.0181\n",
      "Restoring best model from epoch 6.\n",
      "      8        3.1642        \u001b[32m3.1806\u001b[0m  0.0242\n",
      "      8        \u001b[36m3.1922\u001b[0m        \u001b[32m3.2121\u001b[0m  0.0240\n",
      "      9        \u001b[36m3.1501\u001b[0m        \u001b[32m3.1781\u001b[0m  0.0240\n",
      "      8        \u001b[36m3.0329\u001b[0m        3.0157  0.0315\n",
      "      9        3.2016        \u001b[32m3.2058\u001b[0m  0.0240\n",
      "     10        \u001b[36m3.1393\u001b[0m        3.1802  0.0238\n",
      "Restoring best model from epoch 9.\n",
      "      9        3.0429        \u001b[32m3.0071\u001b[0m  0.0261\n",
      "     10        3.1975        \u001b[32m3.2037\u001b[0m  0.0256\n",
      "     10        \u001b[36m3.0297\u001b[0m        \u001b[32m3.0069\u001b[0m  0.0254\n",
      "Restoring best model from epoch 9.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.1294\u001b[0m        \u001b[32m3.1336\u001b[0m  0.1119\n",
      "      2        \u001b[36m3.1292\u001b[0m        \u001b[32m3.1144\u001b[0m  0.0934\n",
      "      3        \u001b[36m3.1168\u001b[0m        3.1183  0.0921\n",
      "      4        \u001b[36m3.1133\u001b[0m        \u001b[32m3.1119\u001b[0m  0.0917\n",
      "      5        \u001b[36m3.1011\u001b[0m        \u001b[32m3.1087\u001b[0m  0.0907\n",
      "      6        \u001b[36m3.0976\u001b[0m        \u001b[32m3.1028\u001b[0m  0.0914\n",
      "      7        \u001b[36m3.0945\u001b[0m        \u001b[32m3.0896\u001b[0m  0.0918\n",
      "      8        \u001b[36m3.0872\u001b[0m        \u001b[32m3.0873\u001b[0m  0.0904\n",
      "      9        \u001b[36m3.0830\u001b[0m        \u001b[32m3.0825\u001b[0m  0.0932\n",
      "     10        \u001b[36m3.0754\u001b[0m        \u001b[32m3.0806\u001b[0m  0.0910\n"
     ]
    }
   ],
   "source": [
    "data_set_fns = [load_metabric, load_flchain, load_rgbsg, load_support] #load_metabric, , load_flchain, load_rgbsg, load_support, load_tcga]\n",
    "data_set_fns_str = ['load_metabric', 'load_flchain', 'load_rgbsg', 'load_support'] \n",
    "one_hot_dict = {'load_flchain': ['mgus'], 'load_support':['cancer'], 'load_rgbsg':['grade']}\n",
    "agg_metrics_cindex = []\n",
    "agg_metrics_ibs = []\n",
    "\n",
    "for idx, dataset in enumerate(data_set_fns):\n",
    "    # get name of current dataset\n",
    "    data = dataset(path=\"/Users/JUSC/Documents/xgbsurv/xgbsurv/datasets/data/\", as_frame=True)\n",
    "    X  = data.data #.astype(np.float32)\n",
    "    y = data.target #.values #.to_numpy()\n",
    "\n",
    "    print(data_set_fns_str[idx])\n",
    "    #if data_set_fns_str[idx] in one_hot_dict.keys():\n",
    "    #    X = pd.get_dummies(X, columns=one_hot_dict[data_set_fns_str[idx]])\n",
    "    X, y = sort_X_y_pandas(X, y)\n",
    "    \n",
    "    net = NeuralNet(\n",
    "        SurvivalModel, \n",
    "        module__n_layers = 1,\n",
    "        module__input_units = X.shape[1],\n",
    "        #module__num_nodes = 32,\n",
    "        #module__dropout = 0.1, # these could also be removed\n",
    "        module__out_features = 1,\n",
    "        # for split sizes when result size = 1\n",
    "        iterator_train__drop_last=True,\n",
    "        #iterator_valid__drop_last=True,\n",
    "        criterion=AHLoss,\n",
    "        optimizer=torch.optim.AdamW,\n",
    "        optimizer__weight_decay = 0.4,\n",
    "        batch_size=32, # separate train and valid->iterator_train__batch_size=128 and iterator_valid__batch_size=128 ?\n",
    "        callbacks=[\n",
    "            (\n",
    "                \"sched\",\n",
    "                LRScheduler(\n",
    "                    torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
    "                    monitor=\"valid_loss\",\n",
    "                    patience=5,\n",
    "                ),\n",
    "            ),\n",
    "            (\n",
    "                \"es\",\n",
    "                EarlyStopping(\n",
    "                    monitor=\"valid_loss\",\n",
    "                    patience=early_stopping_rounds,\n",
    "                    load_best=True,\n",
    "                ),\n",
    "            ),\n",
    "            (\"seed\", FixSeed(seed=42)),\n",
    "            #(\"Input Shape Setter\",InputShapeSetter())\n",
    "        ],\n",
    "        \n",
    "        #[EarlyStopping(patience=10)],\n",
    "        # add extensive callback, and random number seed\n",
    "        #TODO: enable stratification, verify\n",
    "        train_split = CustomValidSplit(0.2, stratified=True, random_state=rand_state), \n",
    "        #max_epochs=1, #0,#100\n",
    "        #train_split=None,\n",
    "        verbose=2\n",
    "    )\n",
    "    df_agg_metrics_cindex, df_agg_metrics_ibs, best_model,params, outer_scores, best_preds_train, best_preds_test = train_eval(X, y, net, n_iter, data.filename)\n",
    "    agg_metrics_cindex.append(df_agg_metrics_cindex)\n",
    "    agg_metrics_ibs.append(df_agg_metrics_ibs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_ah_1_cindex = pd.concat([df for df in agg_metrics_cindex]).round(4)\n",
    "df_final_ah_1_cindex.to_csv('metrics/final_deep_learning_ah_1_cindex.csv', index=False)\n",
    "df_final_ah_1_cindex.to_csv('/Users/JUSC/Documents/644928e0fb7e147893e8ec15/05_thesis/tables/final_deep_learning_ah_1_cindex.csv', index=False)  #\n",
    "df_final_ah_1_cindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_ah_1_ibs = pd.concat([df for df in agg_metrics_ibs]).round(4)\n",
    "df_final_ah_1_ibs.to_csv('metrics/final_deep_learning_ah_1_ibs.csv', index=False)\n",
    "df_final_ah_1_ibs.to_csv('/Users/JUSC/Documents/644928e0fb7e147893e8ec15/05_thesis/tables/final_deep_learning_ah_1_ibs.csv', index=False) \n",
    "df_final_ah_1_ibs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TCGA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_ah_tcga = {\n",
    "    'estimator__module__n_layers': [1, 2, 4],\n",
    "    'estimator__module__num_nodes': [64, 128, 256, 512],\n",
    "    'estimator__module__dropout': scuniform(0.0,0.7),\n",
    "    'estimator__optimizer__weight_decay': [0.4, 0.2, 0.1, 0.05, 0.02, 0.01, 0],\n",
    "    'estimator__batch_size': [64, 128, 256, 512, 1024],\n",
    "    #lr not in paper because of learning rate finder\n",
    "    # note: setting learning rate higher would make exp(partial_hazard) explode\n",
    "    #'estimator__lr': scloguniform(0.001,0.01), # scheduler unten einbauen\n",
    "    # use callback instead\n",
    "    'estimator__lr':[0.01],\n",
    "    #'estimator__max_epochs':  scrandint(10,20), # corresponds to num_rounds\n",
    "    'pca__n_components':[64, 128, 256, 512, 1024]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_eval(X, y, net, n_iter, filename):\n",
    "        model = 'ah_'\n",
    "        dataset_name = filename.split('_')[0]\n",
    "        # add IBS later\n",
    "        outer_scores = {'cindex_train_'+dataset_name:[], 'cindex_test_'+dataset_name:[],\n",
    "                        'ibs_train_'+dataset_name:[], 'ibs_test_'+dataset_name:[]}\n",
    "        best_params = {'best_params_'+dataset_name:[]}\n",
    "        best_model = {'best_model_'+dataset_name:[]}\n",
    "        ct = make_column_transformer(\n",
    "                #(OneHotEncoder(sparse_output=False), make_column_selector(dtype_include=['category', 'object']))\n",
    "                (StandardScaler(), make_column_selector(dtype_include=['float32']))\n",
    "                ,remainder='drop')\n",
    "        pipe = Pipeline([('scaler',ct),\n",
    "                         ('pca', PCA()),#n_components=10\n",
    "                        ('estimator', net)])\n",
    "        rs = RandomizedSearchCV(pipe, param_grid_ah_tcga, scoring = scoring_function, n_jobs=-1, \n",
    "                                    n_iter=n_iter, refit=True)\n",
    "        for i, (train_index, test_index) in enumerate(outer_custom_cv.split(X, y)):\n",
    "                # Split data into training and testing sets for outer fold\n",
    "                X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "                y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "                X_train, y_train = sort_X_y_pandas(X_train, y_train)\n",
    "                X_test, y_test = sort_X_y_pandas(X_test, y_test)\n",
    "\n",
    "                #print(X_train.shape, type(X_train))\n",
    "                #print(y_train.shape, type(y_train))\n",
    "                #print(X_test.shape, type(X_test))\n",
    "                #print(y_test.shape, type(y_test))\n",
    "                # save splits and data\n",
    "                savetxt('splits/train_index_'+str(i)+'_'+filename, train_index, delimiter=',')\n",
    "                savetxt('splits/test_index_'+str(i)+'_'+filename, test_index, delimiter=',')\n",
    "                \n",
    "                # savetxt('splits/X_train_'+str(i)+'_'+filename, X_train, delimiter=',')\n",
    "                # savetxt('splits/X_test_'+str(i)+'_'+filename, X_test, delimiter=',')\n",
    "\n",
    "                # savetxt('splits/y_train_'+str(i)+'_'+filename, y_train, delimiter=',')\n",
    "                # savetxt('splits/y_test_'+str(i)+'_'+filename, y_test, delimiter=',')\n",
    "\n",
    "                rs.fit(X_train, y_train)\n",
    "                best_preds_train = rs.best_estimator_.predict(X_train)\n",
    "                best_preds_test = rs.best_estimator_.predict(X_test)\n",
    "\n",
    "                savetxt('predictions/'+model+'best_preds_train_'+str(i)+'_'+filename, best_preds_train, delimiter=',')\n",
    "                savetxt('predictions/'+model+'best_preds_test_'+str(i)+'_'+filename, best_preds_test, delimiter=',')\n",
    "\n",
    "                \n",
    "                # save hyperparameter settings\n",
    "                params = rs.best_estimator_.get_params\n",
    "                best_params['best_params_'+dataset_name] += [rs.best_params_]\n",
    "                best_model['best_model_'+dataset_name] += [params]\n",
    "                try:\n",
    "                    cum_hazard_train = get_cumulative_hazard_function_ah(\n",
    "                            X_train.values, X_train.values, y_train.values, y_train.values,\n",
    "                            best_preds_train.reshape(-1), best_preds_train.reshape(-1)\n",
    "                            )\n",
    "\n",
    "                    df_survival_train = np.exp(-cum_hazard_train)\n",
    "                    durations_train, events_train = transform_back(y_train.values)\n",
    "                    time_grid_train = np.linspace(durations_train.min(), durations_train.max(), 100)\n",
    "                    ev = EvalSurv(df_survival_train, durations_train, events_train, censor_surv='km')\n",
    "                    print('Concordance Index',ev.concordance_td('antolini'))\n",
    "                    print('Integrated Brier Score:',ev.integrated_brier_score(time_grid_train))\n",
    "                    cindex_score_train = ev.concordance_td('antolini')\n",
    "                    ibs_score_train = ev.integrated_brier_score(time_grid_train)\n",
    "\n",
    "                    outer_scores['cindex_train_'+dataset_name] += [cindex_score_train]\n",
    "                    outer_scores['ibs_train_'+dataset_name] += [ibs_score_train]\n",
    "\n",
    "                except:\n",
    "                    outer_scores['cindex_train_'+dataset_name] += [np.nan]\n",
    "                    outer_scores['ibs_train_'+dataset_name] += [np.nan]\n",
    "                    \n",
    "                try:\n",
    "                    cum_hazard_test = get_cumulative_hazard_function_ah(\n",
    "                            X_train.values, X_test.values, y_train.values, y_test.values,\n",
    "                            best_preds_train.reshape(-1), best_preds_test.reshape(-1)\n",
    "                            )\n",
    "                    df_survival_test = np.exp(-cum_hazard_test)\n",
    "                    durations_test, events_test = transform_back(y_test.values)\n",
    "                    print('durations',durations_test.min(), durations_test.max())\n",
    "                    time_grid_test = np.linspace(durations_test.min(), durations_test.max(), 100)\n",
    "                    ev = EvalSurv(df_survival_test, durations_test, events_test, censor_surv='km')\n",
    "                    print('Concordance Index',ev.concordance_td('antolini'))\n",
    "                    print('Integrated Brier Score:',ev.integrated_brier_score(time_grid_test))\n",
    "                    cindex_score_test = ev.concordance_td('antolini')\n",
    "                    ibs_score_test = ev.integrated_brier_score(time_grid_test)\n",
    "\n",
    "                    outer_scores['cindex_test_'+dataset_name] += [cindex_score_test]\n",
    "                    outer_scores['ibs_test_'+dataset_name] += [ibs_score_test]\n",
    "                except: \n",
    "                    outer_scores['cindex_test_'+dataset_name] += [np.nan]\n",
    "                    outer_scores['ibs_test_'+dataset_name] += [np.nan]\n",
    "            \n",
    "        df_best_params = pd.DataFrame(best_params)\n",
    "        df_best_model = pd.DataFrame(best_model)\n",
    "        df_outer_scores = pd.DataFrame(outer_scores)\n",
    "        df_metrics = pd.concat([df_best_params,df_best_model,df_outer_scores], axis=1)\n",
    "        df_metrics.to_csv('metrics/'+model+'metric_summary'+str(i)+'_'+filename, index=False)\n",
    "        # cindex\n",
    "        df_agg_metrics_cindex = pd.DataFrame({'dataset':[dataset_name],\n",
    "                                              'cindex_train_mean':df_outer_scores['cindex_train_'+dataset_name].mean(),\n",
    "                                              'cindex_train_std':df_outer_scores['cindex_train_'+dataset_name].std(),\n",
    "                                              'cindex_test_mean':df_outer_scores['cindex_test_'+dataset_name].mean(),\n",
    "                                              'cindex_test_std':df_outer_scores['cindex_test_'+dataset_name].std() })\n",
    "        # IBS\n",
    "        df_agg_metrics_ibs = pd.DataFrame({'dataset':[dataset_name],\n",
    "                                              'ibs_train_mean':df_outer_scores['ibs_train_'+dataset_name].mean(),\n",
    "                                              'ibs_train_std':df_outer_scores['ibs_train_'+dataset_name].std(),\n",
    "                                              'ibs_test_mean':df_outer_scores['ibs_test_'+dataset_name].mean(),\n",
    "                                              'ibs_test_std':df_outer_scores['ibs_test_'+dataset_name].std() })\n",
    "        \n",
    "        return df_agg_metrics_cindex, df_agg_metrics_ibs, best_model, best_params, outer_scores, best_preds_train, best_preds_test \n",
    "\n",
    "                \n",
    "#cv=inner_custom_cv,pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split gex_?|100130426      float32\n",
      "gex_?|100133144      float32\n",
      "gex_?|100134869      float32\n",
      "gex_?|10357          float32\n",
      "gex_?|10431          float32\n",
      "                      ...   \n",
      "gex_ZYG11A|440590    float32\n",
      "gex_ZYG11B|79699     float32\n",
      "gex_ZYX|7791         float32\n",
      "gex_ZZEF1|23140      float32\n",
      "gex_ZZZ3|26009       float32\n",
      "Length: 20531, dtype: object\n",
      "(324, 20531) <class 'pandas.core.frame.DataFrame'>\n",
      "(324,) <class 'pandas.core.series.Series'>\n",
      "(82, 20531) <class 'pandas.core.frame.DataFrame'>\n",
      "(82,) <class 'pandas.core.series.Series'>\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m2.8164\u001b[0m  0.0204\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      2        2.8164  0.0132\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.3685\u001b[0m  0.0198\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      3        2.8164  0.0172\n",
      "      2        3.3685  0.0121\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.4617\u001b[0m  0.0143      4        2.8164  0.0139\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.1876\u001b[0m  0.0172\n",
      "\n",
      "      3        3.3685  0.0104\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.6805\u001b[0m  0.0136\n",
      "      5        2.8164  0.0122\n",
      "      4        3.3685  0.0104\n",
      "      2        3.1876  0.0140\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      2        3.4617  0.0134\n",
      "      2        3.6805  0.0112\n",
      "      6        2.8164  0.0110\n",
      "      5        3.3685  0.0131\n",
      "      3        3.4617  0.0117\n",
      "      3        3.1876  0.0118\n",
      "      7        2.8164  0.0126\n",
      "      4        3.1876  0.0103\n",
      "      6        3.3685  0.0120\n",
      "      4        3.4617  0.0163\n",
      "      3        3.6805  0.0214\n",
      "      8        2.8164  0.0098\n",
      "      5        3.4617  0.0121\n",
      "      7        3.3685  0.0171\n",
      "      4        3.6805  0.0137\n",
      "      5        3.1876  0.0204\n",
      "      9        2.8164  0.0135\n",
      "      6        3.1876  0.0119\n",
      "      6        3.4617  0.0150\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      5        3.6805  0.0182\n",
      "      8        3.3685  0.0204\n",
      "     10        2.8164  0.0177\n",
      "      7        3.4617  0.0099\n",
      "Restoring best model from epoch 1.\n",
      "      7        3.1876  0.0123\n",
      "      6        3.6805  0.0157\n",
      "      9        3.3685  0.0163\n",
      "      8        3.1876  0.0130\n",
      "      9        3.1876  0.0123\n",
      "     10        3.3685  0.0209\n",
      "Restoring best model from epoch 1.\n",
      "      7        3.6805  0.0242\n",
      "      8        3.4617  0.0377\n",
      "     10        3.1876  0.0134\n",
      "Restoring best model from epoch 1.\n",
      "      9        3.4617  0.0131\n",
      "     10        3.4617  0.0104\n",
      "Restoring best model from epoch 1.\n",
      "      8        3.6805  0.0291\n",
      "      9        3.6805  0.0150\n",
      "     10        3.6805  0.0100\n",
      "Restoring best model from epoch 1.\n",
      "  epoch    train_loss    valid_loss     dur  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.3877\u001b[0m        \u001b[32m3.5927\u001b[0m  0.6382\n",
      "\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.5272\u001b[0m        \u001b[32m3.2168\u001b[0m  0.6514\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.2293\u001b[0m        \u001b[32m2.7725\u001b[0m  0.6793\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.9809\u001b[0m        \u001b[32m3.4337\u001b[0m  0.6980\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.4704\u001b[0m        \u001b[32m3.3368\u001b[0m  0.6214\n",
      "      2        \u001b[36m3.4156\u001b[0m        3.3621  0.3073\n",
      "      2        2.5287        3.2454  0.3660\n",
      "      2        \u001b[36m3.3166\u001b[0m        3.6245  0.3710\n",
      "      2        \u001b[36m3.1397\u001b[0m        2.8036  0.3906\n",
      "      2        \u001b[36m2.8947\u001b[0m        \u001b[32m3.4305\u001b[0m  0.4159\n",
      "      3        \u001b[36m3.3772\u001b[0m        \u001b[32m3.3325\u001b[0m  0.2370\n",
      "      3        \u001b[36m2.4701\u001b[0m        3.2590  0.2440\n",
      "      3        \u001b[36m3.2368\u001b[0m        3.6378  0.2484\n",
      "      3        \u001b[36m3.0796\u001b[0m        2.8060  0.2385\n",
      "      3        \u001b[36m2.8349\u001b[0m        3.4459  0.2295\n",
      "      4        \u001b[36m3.3490\u001b[0m        3.3379  0.2371\n",
      "      4        2.5130        3.2637  0.2395\n",
      "      4        \u001b[36m3.1938\u001b[0m        3.6691  0.2342\n",
      "      4        \u001b[36m3.0461\u001b[0m        2.8214  0.2307\n",
      "      4        \u001b[36m2.8136\u001b[0m        3.4467  0.2308\n",
      "      5        3.3522        \u001b[32m3.3268\u001b[0m  0.2276\n",
      "      5        \u001b[36m2.4171\u001b[0m        3.2668  0.2388\n",
      "      5        \u001b[36m2.9206\u001b[0m        2.8436  0.2290\n",
      "      5        \u001b[36m3.1147\u001b[0m        3.6820  0.2394\n",
      "      5        \u001b[36m2.7774\u001b[0m        3.4414  0.2330\n",
      "      6        \u001b[36m3.3063\u001b[0m        \u001b[32m3.3072\u001b[0m  0.2332\n",
      "      6        \u001b[36m3.0929\u001b[0m        3.6784  0.2311\n",
      "      6        \u001b[36m2.4079\u001b[0m        3.2692  0.2373\n",
      "      6        2.9331        2.8676  0.2400\n",
      "      6        \u001b[36m2.7175\u001b[0m        3.4368  0.2300\n",
      "      7        3.3100        \u001b[32m3.2950\u001b[0m  0.2340\n",
      "      7        \u001b[36m3.0459\u001b[0m        3.6659  0.2349\n",
      "      7        2.4197        3.2669  0.2347\n",
      "      7        \u001b[36m2.9165\u001b[0m        2.8768  0.2346\n",
      "      7        \u001b[36m2.7053\u001b[0m        3.4340  0.2311\n",
      "      8        3.3371        \u001b[32m3.2908\u001b[0m  0.2293\n",
      "      8        \u001b[36m2.9600\u001b[0m        3.6546  0.2372\n",
      "      8        2.4233        3.2706  0.2378\n",
      "      8        \u001b[36m2.8289\u001b[0m        2.8845  0.2403\n",
      "      8        \u001b[36m2.6418\u001b[0m        \u001b[32m3.4277\u001b[0m  0.2365\n",
      "      9        3.3337        \u001b[32m3.2781\u001b[0m  0.2383\n",
      "      9        2.9929        3.6460  0.2468\n",
      "      9        2.4355        3.2769  0.2459\n",
      "      9        2.8701        2.8815  0.2469\n",
      "      9        2.6568        \u001b[32m3.4254\u001b[0m  0.2430\n",
      "     10        \u001b[36m3.2857\u001b[0m        \u001b[32m3.2583\u001b[0m  0.2316\n",
      "     10        2.4264        3.2788  0.2465\n",
      "     10        \u001b[36m2.9525\u001b[0m        3.6371  0.2498\n",
      "     10        2.8853        2.8667  0.2414\n",
      "Restoring best model from epoch 1.\n",
      "Restoring best model from epoch 1.\n",
      "Restoring best model from epoch 1.\n",
      "     10        \u001b[36m2.5947\u001b[0m        3.4303  0.2386\n",
      "Restoring best model from epoch 9.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m2.8832\u001b[0m  0.0146\n",
      "      2        2.8832  0.0103\n",
      "      3        2.8832  0.0078\n",
      "      4        2.8832  0.0093\n",
      "      5        2.8832  0.0108\n",
      "      6        2.8832  0.0085\n",
      "      7        2.8832  0.0083\n",
      "      8        2.8832  0.0085\n",
      "      9        2.8832  0.0081\n",
      "     10        2.8832  0.0088\n",
      "Restoring best model from epoch 1.\n",
      "y_train breslow final [  -13.   -15.    19.   -20.    20.   -35.   -46.   -55.    56.   -59.\n",
      "    62.   -64.    65.   -67.   -68.    68.    69.    76.   -82.   -84.\n",
      "    88.   -89.    90.    92.    93.    98.    99.  -105.  -110.  -117.\n",
      "   118.   122.   128.  -129.   142.   144.   146.   149.   154.   154.\n",
      "  -158.   163.  -163.   168.   168.   173.   182.  -187.   191.   200.\n",
      "   206.   213.   220.  -224.   232.   237.   246.   248.   250.   251.\n",
      "  -251.   254.   254.   258.   259.   261.   262.   272.   272.   273.\n",
      "   274.  -276.   278.  -293.   294.   303.   311.   321.   324.   324.\n",
      "   328.  -330.   332.  -333.  -337.   344.   356.  -359.  -361.   364.\n",
      "  -364.  -365.  -366.  -368.  -368.  -369.   370.  -370.   370.  -372.\n",
      "  -372.  -373.  -376.  -377.  -379.  -382.  -383.  -384.  -384.   385.\n",
      "   386.  -389.   391.   393.  -394.  -398.  -398.   400.   406.   408.\n",
      "   413.  -415.  -415.  -416.   418.  -425.  -428.  -433.   434.   437.\n",
      "   453.   454.  -455.  -460.   460.  -466.  -466.   467.  -467.   467.\n",
      "  -469.   474.  -474.  -475.  -477.   486.   495.  -495.  -495.  -503.\n",
      "  -507.   508.  -508.   510.   510.  -512.   522.  -522.  -536.   536.\n",
      "  -536.  -539.  -540.   544.   544.  -546.   547.   547.   550.  -560.\n",
      "  -562.   565.  -573.   575.   577.  -578.  -578.  -580.  -581.  -582.\n",
      "  -588.   590.  -590.   599.  -603.  -610.   617.  -618.   623.   630.\n",
      "  -636.  -638.  -641.  -642.  -649.   651.  -665.   665.   674.   680.\n",
      "   685.   690.   696.  -700.   706.   712.   719.  -731.   734.   739.\n",
      "  -750.  -758.  -773.   778.  -789.   795.  -798.  -799.  -812.  -813.\n",
      "   819.  -820.   823.  -832.  -832.  -842.   859.   864.  -864.  -887.\n",
      "  -893.  -897.   904.  -906.  -921.   941.  -945.   974.  -997.  1004.\n",
      "  1005.  1008. -1029.  1036. -1048.  1077. -1090. -1094. -1108. -1110.\n",
      " -1115. -1127.  1163. -1174. -1181.  1270. -1326.  1348. -1350. -1370.\n",
      "  1420.  1423. -1429. -1454. -1455. -1529. -1538. -1542.  1556. -1582.\n",
      " -1604. -1621. -1639. -1649.  1670. -1708. -1714.  1718. -1761. -1792.\n",
      "  1804. -1806. -1830. -1845.  1869. -1884. -1912. -1947. -1949. -1952.\n",
      "  1971. -2009.  2020. -2020. -2024. -2027. -2044. -2049. -2139. -2177.\n",
      " -2312. -2330. -2380. -2423. -2625.  2641. -2656. -2703. -2790.  2828.\n",
      " -2868. -2886.  2954. -2964. -3011.  3183. -3314. -3364. -3432. -3817.\n",
      " -3981. -4967. -5041. -5050.]\n",
      "Concordance Index 0.5110439175485821\n",
      "Integrated Brier Score: 0.21601485971940018\n",
      "y_train breslow final [  -13.   -15.    19.   -20.    20.   -35.   -46.   -55.    56.   -59.\n",
      "    62.   -64.    65.   -67.   -68.    68.    69.    76.   -82.   -84.\n",
      "    88.   -89.    90.    92.    93.    98.    99.  -105.  -110.  -117.\n",
      "   118.   122.   128.  -129.   142.   144.   146.   149.   154.   154.\n",
      "  -158.   163.  -163.   168.   168.   173.   182.  -187.   191.   200.\n",
      "   206.   213.   220.  -224.   232.   237.   246.   248.   250.   251.\n",
      "  -251.   254.   254.   258.   259.   261.   262.   272.   272.   273.\n",
      "   274.  -276.   278.  -293.   294.   303.   311.   321.   324.   324.\n",
      "   328.  -330.   332.  -333.  -337.   344.   356.  -359.  -361.   364.\n",
      "  -364.  -365.  -366.  -368.  -368.  -369.   370.  -370.   370.  -372.\n",
      "  -372.  -373.  -376.  -377.  -379.  -382.  -383.  -384.  -384.   385.\n",
      "   386.  -389.   391.   393.  -394.  -398.  -398.   400.   406.   408.\n",
      "   413.  -415.  -415.  -416.   418.  -425.  -428.  -433.   434.   437.\n",
      "   453.   454.  -455.  -460.   460.  -466.  -466.   467.  -467.   467.\n",
      "  -469.   474.  -474.  -475.  -477.   486.   495.  -495.  -495.  -503.\n",
      "  -507.   508.  -508.   510.   510.  -512.   522.  -522.  -536.   536.\n",
      "  -536.  -539.  -540.   544.   544.  -546.   547.   547.   550.  -560.\n",
      "  -562.   565.  -573.   575.   577.  -578.  -578.  -580.  -581.  -582.\n",
      "  -588.   590.  -590.   599.  -603.  -610.   617.  -618.   623.   630.\n",
      "  -636.  -638.  -641.  -642.  -649.   651.  -665.   665.   674.   680.\n",
      "   685.   690.   696.  -700.   706.   712.   719.  -731.   734.   739.\n",
      "  -750.  -758.  -773.   778.  -789.   795.  -798.  -799.  -812.  -813.\n",
      "   819.  -820.   823.  -832.  -832.  -842.   859.   864.  -864.  -887.\n",
      "  -893.  -897.   904.  -906.  -921.   941.  -945.   974.  -997.  1004.\n",
      "  1005.  1008. -1029.  1036. -1048.  1077. -1090. -1094. -1108. -1110.\n",
      " -1115. -1127.  1163. -1174. -1181.  1270. -1326.  1348. -1350. -1370.\n",
      "  1420.  1423. -1429. -1454. -1455. -1529. -1538. -1542.  1556. -1582.\n",
      " -1604. -1621. -1639. -1649.  1670. -1708. -1714.  1718. -1761. -1792.\n",
      "  1804. -1806. -1830. -1845.  1869. -1884. -1912. -1947. -1949. -1952.\n",
      "  1971. -2009.  2020. -2020. -2024. -2027. -2044. -2049. -2139. -2177.\n",
      " -2312. -2330. -2380. -2423. -2625.  2641. -2656. -2703. -2790.  2828.\n",
      " -2868. -2886.  2954. -2964. -3011.  3183. -3314. -3364. -3432. -3817.\n",
      " -3981. -4967. -5041. -5050.]\n",
      "durations 17.0 4343.0\n",
      "Concordance Index 0.5359116022099447\n",
      "Integrated Brier Score: 0.22965272704184764\n",
      "(325, 20531) <class 'pandas.core.frame.DataFrame'>\n",
      "(325,) <class 'pandas.core.series.Series'>\n",
      "(81, 20531) <class 'pandas.core.frame.DataFrame'>\n",
      "(81,) <class 'pandas.core.series.Series'>\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.0969\u001b[0m        \u001b[32m3.9548\u001b[0m  0.2958\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.8771\u001b[0m        \u001b[32m4.0285\u001b[0m  0.3847\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.7865\u001b[0m        \u001b[32m4.0309\u001b[0m  0.3596\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.7678\u001b[0m        \u001b[32m4.0869\u001b[0m  0.3187\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.8967\u001b[0m        \u001b[32m3.9998\u001b[0m  0.3564\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.3588\u001b[0m        \u001b[32m3.3310\u001b[0m  0.3583\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.0684\u001b[0m        \u001b[32m3.9666\u001b[0m  0.3120\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.3548\u001b[0m        \u001b[32m3.3004\u001b[0m  0.4885\n",
      "      2        \u001b[36m3.0810\u001b[0m        3.9616  0.3170\n",
      "      2        \u001b[36m2.8329\u001b[0m        4.0436  0.1942\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.1493\u001b[0m        \u001b[32m3.5008\u001b[0m  0.5461\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.1499\u001b[0m        \u001b[32m3.4599\u001b[0m  0.5002\n",
      "      2        \u001b[36m2.7783\u001b[0m        \u001b[32m4.0181\u001b[0m  0.2184\n",
      "      2        \u001b[36m2.7635\u001b[0m        4.0915  0.2022\n",
      "      2        \u001b[36m2.8962\u001b[0m        4.0048  0.2125\n",
      "      2        \u001b[36m3.2674\u001b[0m        3.3402  0.2136\n",
      "      2        \u001b[36m3.0335\u001b[0m        3.9764  0.1893\n",
      "      3        3.1008        3.9623  0.1713\n",
      "      2        \u001b[36m3.1359\u001b[0m        3.5132  0.1830\n",
      "      3        \u001b[36m2.7592\u001b[0m        4.0640  0.1979\n",
      "      2        \u001b[36m3.3330\u001b[0m        3.3153  0.2155\n",
      "      2        3.1585        3.4620  0.1943\n",
      "      4        \u001b[36m3.0654\u001b[0m        3.9598  0.1386\n",
      "      3        \u001b[36m2.6926\u001b[0m        4.0916  0.1930\n",
      "      3        \u001b[36m2.7740\u001b[0m        \u001b[32m3.9995\u001b[0m  0.2069\n",
      "      3        \u001b[36m3.2483\u001b[0m        3.3358  0.1868\n",
      "      3        2.9035        4.0029  0.1888\n",
      "      3        \u001b[36m3.0770\u001b[0m        3.5034  0.1462\n",
      "      3        \u001b[36m2.9619\u001b[0m        3.9826  0.1867\n",
      "      4        \u001b[36m2.7228\u001b[0m        4.0725  0.1657\n",
      "      3        \u001b[36m3.1174\u001b[0m        3.4788  0.1600\n",
      "      3        \u001b[36m3.3329\u001b[0m        3.3214  0.1800\n",
      "      5        3.0676        3.9578  0.1582\n",
      "      4        \u001b[36m2.8824\u001b[0m        \u001b[32m3.9939\u001b[0m  0.1526\n",
      "      4        \u001b[36m2.9428\u001b[0m        3.9857  0.1691\n",
      "      4        \u001b[36m2.9979\u001b[0m        \u001b[32m3.4995\u001b[0m  0.1637\n",
      "      4        \u001b[36m2.7539\u001b[0m        \u001b[32m3.9673\u001b[0m  0.1795\n",
      "      4        \u001b[36m3.1557\u001b[0m        \u001b[32m3.3292\u001b[0m  0.1791\n",
      "      4        3.1259        3.4937  0.1455\n",
      "      4        2.7194        \u001b[32m4.0725\u001b[0m  0.1890\n",
      "      5        2.7320        4.0711  0.1603\n",
      "      4        3.3403        3.3191  0.1408\n",
      "      6        \u001b[36m3.0039\u001b[0m        3.9556  0.1444\n",
      "      5        \u001b[36m2.9613\u001b[0m        3.5096  0.1401\n",
      "      5        \u001b[36m2.8612\u001b[0m        3.9785  0.1542\n",
      "      5        3.1344        3.5000  0.1466\n",
      "      5        \u001b[36m2.8624\u001b[0m        3.9973  0.1784\n",
      "      5        \u001b[36m3.1500\u001b[0m        \u001b[32m3.3237\u001b[0m  0.1528\n",
      "      5        \u001b[36m2.6435\u001b[0m        \u001b[32m4.0543\u001b[0m  0.1674\n",
      "      5        \u001b[36m2.7308\u001b[0m        \u001b[32m3.9491\u001b[0m  0.1860\n",
      "      5        \u001b[36m3.3090\u001b[0m        3.3187  0.1753\n",
      "      6        \u001b[36m2.6302\u001b[0m        4.0694  0.1893\n",
      "      6        \u001b[36m2.9455\u001b[0m        3.5233  0.1343\n",
      "      7        \u001b[36m2.9950\u001b[0m        \u001b[32m3.9537\u001b[0m  0.1756\n",
      "      6        \u001b[36m3.1103\u001b[0m        3.5089  0.1459\n",
      "      6        \u001b[36m2.8137\u001b[0m        \u001b[32m3.9937\u001b[0m  0.1458\n",
      "      6        \u001b[36m2.8521\u001b[0m        3.9668  0.1591\n",
      "      6        \u001b[36m2.6175\u001b[0m        \u001b[32m4.0425\u001b[0m  0.1351\n",
      "      7        \u001b[36m2.5661\u001b[0m        4.0692  0.1342\n",
      "      6        \u001b[36m2.6768\u001b[0m        \u001b[32m3.9434\u001b[0m  0.1508\n",
      "      6        \u001b[36m3.0982\u001b[0m        \u001b[32m3.3195\u001b[0m  0.2404\n",
      "      8        2.9965        3.9565  0.1432\n",
      "      6        \u001b[36m3.2749\u001b[0m        3.3154  0.2230\n",
      "      7        2.9968        3.5347  0.1627\n",
      "      7        2.8752        \u001b[32m3.9545\u001b[0m  0.1368\n",
      "      7        \u001b[36m2.7870\u001b[0m        3.9950  0.1573\n",
      "      7        3.1145        3.5113  0.1621\n",
      "      7        2.6687        4.0457  0.1693\n",
      "      8        \u001b[36m2.5588\u001b[0m        4.0713  0.1502\n",
      "      7        2.7321        \u001b[32m3.9407\u001b[0m  0.1655\n",
      "      7        \u001b[36m3.0373\u001b[0m        \u001b[32m3.3168\u001b[0m  0.1610\n",
      "      7        \u001b[36m3.2494\u001b[0m        3.3128  0.1605\n",
      "      8        \u001b[36m2.9122\u001b[0m        3.5370  0.1586\n",
      "      9        \u001b[36m2.9806\u001b[0m        3.9591  0.1725\n",
      "      8        \u001b[36m2.8105\u001b[0m        3.9560  0.1524\n",
      "      8        2.7985        3.9997  0.1640\n",
      "      8        \u001b[36m3.0169\u001b[0m        3.5108  0.1626\n",
      "      9        2.5898        4.0723  0.1370\n",
      "      8        \u001b[36m2.5931\u001b[0m        4.0482  0.1522\n",
      "      8        2.7401        \u001b[32m3.9399\u001b[0m  0.1718\n",
      "      8        \u001b[36m2.9600\u001b[0m        3.3295  0.1694\n",
      "      9        \u001b[36m2.9110\u001b[0m        3.5381  0.1597\n",
      "      8        3.3149        3.3127  0.1735\n",
      "     10        3.0211        3.9612  0.1781\n",
      "      9        2.8176        3.9621  0.1718\n",
      "Restoring best model from epoch 7.\n",
      "      9        \u001b[36m2.7785\u001b[0m        4.0027  0.1606\n",
      "      9        2.6039        4.0568  0.1586\n",
      "      9        3.0624        3.5114  0.1847\n",
      "     10        \u001b[36m2.5405\u001b[0m        4.0731  0.1867\n",
      "Restoring best model from epoch 1.\n",
      "      9        2.6891        \u001b[32m3.9392\u001b[0m  0.1650\n",
      "      9        2.9683        3.3356  0.1570\n",
      "     10        \u001b[36m2.8661\u001b[0m        3.5413  0.1632\n",
      "      9        \u001b[36m3.2476\u001b[0m        3.3132  0.1587\n",
      "Restoring best model from epoch 4.\n",
      "     10        \u001b[36m2.8093\u001b[0m        3.9648  0.1694\n",
      "Restoring best model from epoch 7.\n",
      "     10        2.6723        4.0633  0.1595\n",
      "Restoring best model from epoch 6.\n",
      "     10        2.8014        4.0074  0.1817\n",
      "Restoring best model from epoch 4.\n",
      "     10        3.0537        3.5119  0.1670\n",
      "Restoring best model from epoch 1.\n",
      "     10        2.7072        3.9397  0.1446\n",
      "Restoring best model from epoch 9.\n",
      "     10        \u001b[36m2.9043\u001b[0m        3.3353  0.1356\n",
      "Restoring best model from epoch 7.\n",
      "     10        \u001b[36m3.2242\u001b[0m        3.3151  0.1317\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.4397\u001b[0m        \u001b[32m2.6377\u001b[0m  0.1293\n",
      "      2        \u001b[36m3.4064\u001b[0m        \u001b[32m2.5985\u001b[0m  0.0835\n",
      "      3        \u001b[36m3.4035\u001b[0m        \u001b[32m2.5916\u001b[0m  0.0795\n",
      "      4        \u001b[36m3.3946\u001b[0m        2.5940  0.0826\n",
      "      5        \u001b[36m3.3778\u001b[0m        2.5959  0.0792\n",
      "      6        \u001b[36m3.3746\u001b[0m        \u001b[32m2.5891\u001b[0m  0.0863\n",
      "      7        3.3753        \u001b[32m2.5851\u001b[0m  0.0852\n",
      "      8        \u001b[36m3.3570\u001b[0m        \u001b[32m2.5793\u001b[0m  0.0819\n",
      "      9        3.3752        \u001b[32m2.5753\u001b[0m  0.0808\n",
      "     10        \u001b[36m3.3407\u001b[0m        \u001b[32m2.5737\u001b[0m  0.0895\n",
      "y_train breslow final [  -13.   -17.    19.   -20.   -28.   -35.   -37.   -46.   -55.    56.\n",
      "    56.    57.   -59.   -64.   -64.    65.   -67.   -68.    68.    69.\n",
      "    76.    81.   -82.   -84.   -89.    90.    92.    93.   -95.  -105.\n",
      "   106.  -110.  -117.   118.   122.   128.  -129.   131.   149.   154.\n",
      "  -158.   163.  -163.   168.   168.   182.  -187.  -189.   191.   200.\n",
      "   205.   206.   211.   213.   216.   220.   223.  -224.   232.   232.\n",
      "   237.   246.   248.   250.   251.  -251.   253.   254.   254.   259.\n",
      "   261.   272.   272.   272.   273.   274.  -276.   278.   294.   294.\n",
      "   311.   321.   324.   324.  -330.   332.  -337.   340.   344.   344.\n",
      "  -345.  -345.   356.  -359.  -361.   362.  -364.  -366.  -368.  -368.\n",
      "  -369.   370.  -372.  -372.  -373.  -377.  -382.  -383.  -384.   385.\n",
      "   385.   386.   388.   393.  -394.  -398.  -398.  -399.   406.  -410.\n",
      "   413.   413.   413.   415.  -415.  -416.   418.  -423.  -425.  -428.\n",
      "  -428.   453.   455.  -457.   460.  -466.  -466.   467.  -467.   467.\n",
      "   474.  -474.  -475.  -477.  -480.  -481.  -484.  -485.   486.  -491.\n",
      "   492.   495.  -495.  -495.  -503.   508.  -508.   510.  -512.  -524.\n",
      "   530.  -536.   536.  -536.   539.  -540.  -542.   544.   544.  -546.\n",
      "   547.   550.  -560.  -562.   565.   565.  -572.  -572.   577.  -578.\n",
      "   579.  -580.  -581.  -582.   590.   593.   599.   602.  -603.  -610.\n",
      "   612.   615.  -618.   623.   630.  -633.  -636.  -640.  -641.  -642.\n",
      "  -646.  -648.  -649.   651.  -665.   665.   674.   680.   685.   690.\n",
      "  -691.   696.  -699.  -700.   706.   712.   719.  -731.   734.  -750.\n",
      "  -758.  -761.  -773.   778.  -783.  -789.   795.  -799.  -812.  -813.\n",
      "   819.   823.  -832.   835.  -840.  -842.  -851.   864.  -864.  -873.\n",
      "  -887.  -893.  -899.  -906.   941.  -945.   949.   974.  -997. -1003.\n",
      "  1005.  1008. -1029.  1036. -1048.  1064. -1072.  1077. -1090. -1108.\n",
      " -1110. -1115. -1127. -1186. -1219.  1270. -1326.  1348.  1367.  1420.\n",
      "  1423. -1429. -1454. -1460. -1522. -1538. -1542.  1556. -1561. -1582.\n",
      " -1621. -1639. -1649. -1708.  1718. -1792.  1804. -1830. -1845.  1869.\n",
      " -1884. -1912. -2008.  2020. -2020. -2024. -2027. -2044. -2049. -2109.\n",
      " -2139. -2177. -2293. -2330. -2380. -2423. -2625.  2641. -2656. -2703.\n",
      "  2828. -2868.  2954. -2964. -3011.  3183. -3314. -3364. -3420. -3432.\n",
      " -3981. -4343. -4967. -5041. -5050.]\n",
      "Concordance Index 0.4802448525319978\n",
      "Integrated Brier Score: 0.35961620512556347\n",
      "y_train breslow final [  -13.   -17.    19.   -20.   -28.   -35.   -37.   -46.   -55.    56.\n",
      "    56.    57.   -59.   -64.   -64.    65.   -67.   -68.    68.    69.\n",
      "    76.    81.   -82.   -84.   -89.    90.    92.    93.   -95.  -105.\n",
      "   106.  -110.  -117.   118.   122.   128.  -129.   131.   149.   154.\n",
      "  -158.   163.  -163.   168.   168.   182.  -187.  -189.   191.   200.\n",
      "   205.   206.   211.   213.   216.   220.   223.  -224.   232.   232.\n",
      "   237.   246.   248.   250.   251.  -251.   253.   254.   254.   259.\n",
      "   261.   272.   272.   272.   273.   274.  -276.   278.   294.   294.\n",
      "   311.   321.   324.   324.  -330.   332.  -337.   340.   344.   344.\n",
      "  -345.  -345.   356.  -359.  -361.   362.  -364.  -366.  -368.  -368.\n",
      "  -369.   370.  -372.  -372.  -373.  -377.  -382.  -383.  -384.   385.\n",
      "   385.   386.   388.   393.  -394.  -398.  -398.  -399.   406.  -410.\n",
      "   413.   413.   413.   415.  -415.  -416.   418.  -423.  -425.  -428.\n",
      "  -428.   453.   455.  -457.   460.  -466.  -466.   467.  -467.   467.\n",
      "   474.  -474.  -475.  -477.  -480.  -481.  -484.  -485.   486.  -491.\n",
      "   492.   495.  -495.  -495.  -503.   508.  -508.   510.  -512.  -524.\n",
      "   530.  -536.   536.  -536.   539.  -540.  -542.   544.   544.  -546.\n",
      "   547.   550.  -560.  -562.   565.   565.  -572.  -572.   577.  -578.\n",
      "   579.  -580.  -581.  -582.   590.   593.   599.   602.  -603.  -610.\n",
      "   612.   615.  -618.   623.   630.  -633.  -636.  -640.  -641.  -642.\n",
      "  -646.  -648.  -649.   651.  -665.   665.   674.   680.   685.   690.\n",
      "  -691.   696.  -699.  -700.   706.   712.   719.  -731.   734.  -750.\n",
      "  -758.  -761.  -773.   778.  -783.  -789.   795.  -799.  -812.  -813.\n",
      "   819.   823.  -832.   835.  -840.  -842.  -851.   864.  -864.  -873.\n",
      "  -887.  -893.  -899.  -906.   941.  -945.   949.   974.  -997. -1003.\n",
      "  1005.  1008. -1029.  1036. -1048.  1064. -1072.  1077. -1090. -1108.\n",
      " -1110. -1115. -1127. -1186. -1219.  1270. -1326.  1348.  1367.  1420.\n",
      "  1423. -1429. -1454. -1460. -1522. -1538. -1542.  1556. -1561. -1582.\n",
      " -1621. -1639. -1649. -1708.  1718. -1792.  1804. -1830. -1845.  1869.\n",
      " -1884. -1912. -2008.  2020. -2020. -2024. -2027. -2044. -2049. -2109.\n",
      " -2139. -2177. -2293. -2330. -2380. -2423. -2625.  2641. -2656. -2703.\n",
      "  2828. -2868.  2954. -2964. -3011.  3183. -3314. -3364. -3420. -3432.\n",
      " -3981. -4343. -4967. -5041. -5050.]\n",
      "durations 15.0 3817.0\n",
      "Concordance Index 0.541780447842709\n",
      "Integrated Brier Score: 0.22687378857644963\n",
      "(325, 20531) <class 'pandas.core.frame.DataFrame'>\n",
      "(325,) <class 'pandas.core.series.Series'>\n",
      "(81, 20531) <class 'pandas.core.frame.DataFrame'>\n",
      "(81,) <class 'pandas.core.series.Series'>\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur  epoch    train_loss    valid_loss     dur\n",
      "\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.4648\u001b[0m        \u001b[32m2.4277\u001b[0m  0.2021\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.2908\u001b[0m        \u001b[32m2.3378\u001b[0m  0.2001\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.4501\u001b[0m        \u001b[32m2.4060\u001b[0m  0.2349\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.0457\u001b[0m        \u001b[32m2.9725\u001b[0m  0.2518\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.2725\u001b[0m        \u001b[32m2.3915\u001b[0m  0.3237\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.5689\u001b[0m        \u001b[32m2.5166\u001b[0m  0.3992\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.0402\u001b[0m        \u001b[32m3.0141\u001b[0m  0.4141\n",
      "      2        \u001b[36m3.2062\u001b[0m        2.3512  0.2429\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.5478\u001b[0m        \u001b[32m2.5525\u001b[0m  0.3786\n",
      "      2        \u001b[36m3.4442\u001b[0m        \u001b[32m2.4131\u001b[0m  0.2686\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.2729\u001b[0m        \u001b[32m2.8775\u001b[0m  0.4202\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.2571\u001b[0m        \u001b[32m2.8766\u001b[0m  0.4054\n",
      "      2        \u001b[36m3.3879\u001b[0m        2.4138  0.2439\n",
      "      2        \u001b[36m3.0351\u001b[0m        2.9933  0.1998\n",
      "      2        \u001b[36m3.2045\u001b[0m        2.4065  0.1991\n",
      "      2        \u001b[36m3.5372\u001b[0m        \u001b[32m2.5109\u001b[0m  0.2048\n",
      "      3        3.2377        2.3584  0.2015\n",
      "      2        \u001b[36m3.5108\u001b[0m        2.5536  0.1927\n",
      "      2        3.0707        3.0169  0.2294\n",
      "      3        \u001b[36m3.3497\u001b[0m        2.4343  0.1642\n",
      "      2        \u001b[36m3.2362\u001b[0m        \u001b[32m2.8600\u001b[0m  0.1954\n",
      "      3        3.4558        \u001b[32m2.4123\u001b[0m  0.2090\n",
      "      3        3.0434        \u001b[32m2.9689\u001b[0m  0.1583\n",
      "      2        \u001b[36m3.2344\u001b[0m        2.8943  0.1961\n",
      "      3        \u001b[36m3.1945\u001b[0m        2.3966  0.1554\n",
      "      3        \u001b[36m3.5046\u001b[0m        \u001b[32m2.5062\u001b[0m  0.1457\n",
      "      3        \u001b[36m3.4232\u001b[0m        2.5627  0.1401\n",
      "      3        3.2416        2.8716  0.1587\n",
      "      4        \u001b[36m3.4250\u001b[0m        2.4131  0.1600\n",
      "      4        \u001b[36m3.3368\u001b[0m        2.4342  0.1669\n",
      "      4        3.0447        \u001b[32m2.9538\u001b[0m  0.1584\n",
      "      4        3.2290        2.3601  0.1986\n",
      "      3        \u001b[36m3.0088\u001b[0m        3.0199  0.1758\n",
      "      3        3.2446        2.8920  0.1563\n",
      "      4        \u001b[36m3.0897\u001b[0m        2.3916  0.1621\n",
      "      4        \u001b[36m3.3875\u001b[0m        2.5621  0.1766\n",
      "      4        3.5205        \u001b[32m2.5053\u001b[0m  0.2169\n",
      "      5        \u001b[36m3.2019\u001b[0m        2.3652  0.1644\n",
      "      5        \u001b[36m3.2588\u001b[0m        2.4290  0.1683\n",
      "      5        \u001b[36m3.0125\u001b[0m        \u001b[32m2.9434\u001b[0m  0.1747\n",
      "      4        \u001b[36m3.2176\u001b[0m        2.8753  0.1876\n",
      "      4        3.0203        3.0167  0.1807\n",
      "      5        \u001b[36m3.3977\u001b[0m        2.4152  0.1858\n",
      "      4        \u001b[36m3.2314\u001b[0m        2.8853  0.1878\n",
      "      5        \u001b[36m3.3685\u001b[0m        2.5573  0.1629\n",
      "      5        3.1115        2.3916  0.1989\n",
      "      6        \u001b[36m3.2017\u001b[0m        2.3694  0.1575\n",
      "      6        \u001b[36m3.2055\u001b[0m        2.4267  0.1585\n",
      "      5        \u001b[36m3.4629\u001b[0m        2.5091  0.1615\n",
      "      5        \u001b[36m3.2141\u001b[0m        2.8775  0.1650\n",
      "      6        3.0340        2.9450  0.1708\n",
      "      5        \u001b[36m2.9652\u001b[0m        \u001b[32m3.0003\u001b[0m  0.1780\n",
      "      6        \u001b[36m3.3951\u001b[0m        2.4138  0.1780\n",
      "      5        \u001b[36m3.2006\u001b[0m        2.8812  0.1851\n",
      "      6        \u001b[36m3.0674\u001b[0m        \u001b[32m2.3901\u001b[0m  0.1623\n",
      "      6        \u001b[36m3.3586\u001b[0m        \u001b[32m2.5503\u001b[0m  0.1856\n",
      "      7        3.2494        2.4258  0.1575\n",
      "      7        3.2088        2.3721  0.1652\n",
      "      6        3.4699        2.5066  0.1658\n",
      "      6        \u001b[36m3.1939\u001b[0m        2.8769  0.1643\n",
      "      7        3.0663        2.9474  0.1733\n",
      "      7        3.4127        \u001b[32m2.4122\u001b[0m  0.1632\n",
      "      6        2.9870        \u001b[32m2.9902\u001b[0m  0.1688\n",
      "      6        \u001b[36m3.1779\u001b[0m        2.8832  0.1659\n",
      "      7        3.0767        \u001b[32m2.3862\u001b[0m  0.1406\n",
      "      7        \u001b[36m3.3522\u001b[0m        \u001b[32m2.5382\u001b[0m  0.1406\n",
      "      8        \u001b[36m3.1566\u001b[0m        2.4223  0.1597\n",
      "      8        \u001b[36m3.1650\u001b[0m        2.3742  0.1604\n",
      "      7        3.4937        \u001b[32m2.5035\u001b[0m  0.1580\n",
      "      8        \u001b[36m3.3760\u001b[0m        2.4136  0.1523\n",
      "      7        3.1990        2.8775  0.1756\n",
      "      8        \u001b[36m2.9822\u001b[0m        2.9562  0.1665\n",
      "      7        \u001b[36m3.1597\u001b[0m        2.8847  0.1514\n",
      "      7        2.9842        \u001b[32m2.9792\u001b[0m  0.1770\n",
      "      8        \u001b[36m3.0274\u001b[0m        \u001b[32m2.3838\u001b[0m  0.1326\n",
      "      8        \u001b[36m3.3032\u001b[0m        \u001b[32m2.5317\u001b[0m  0.1392\n",
      "      8        3.4855        \u001b[32m2.5027\u001b[0m  0.1429\n",
      "      9        \u001b[36m3.1408\u001b[0m        2.4147  0.1588\n",
      "      9        \u001b[36m3.1569\u001b[0m        2.3756  0.1718\n",
      "      9        2.9933        2.9592  0.1395\n",
      "      9        3.3851        2.4157  0.1575\n",
      "      8        \u001b[36m3.1721\u001b[0m        2.8776  0.1750\n",
      "      8        \u001b[36m3.0879\u001b[0m        2.8874  0.1740\n",
      "      8        \u001b[36m2.9373\u001b[0m        \u001b[32m2.9765\u001b[0m  0.1631\n",
      "      9        \u001b[36m3.0013\u001b[0m        \u001b[32m2.3831\u001b[0m  0.1491\n",
      "      9        3.3337        \u001b[32m2.5304\u001b[0m  0.1568\n",
      "     10        \u001b[36m3.1400\u001b[0m        2.4078  0.1641\n",
      "      9        3.4648        2.5066  0.1685\n",
      "Restoring best model from epoch 1.\n",
      "     10        \u001b[36m2.9815\u001b[0m        2.9643  0.1480\n",
      "Restoring best model from epoch 5.\n",
      "     10        3.1712        2.3770  0.1595\n",
      "Restoring best model from epoch 1.\n",
      "     10        3.4130        2.4170  0.1566\n",
      "Restoring best model from epoch 3.\n",
      "      9        \u001b[36m3.1264\u001b[0m        2.8776  0.1456\n",
      "      9        3.0933        2.8885  0.1560\n",
      "      9        \u001b[36m2.9228\u001b[0m        2.9819  0.1635\n",
      "     10        \u001b[36m2.9847\u001b[0m        \u001b[32m2.3816\u001b[0m  0.1568\n",
      "     10        3.3247        2.5311  0.1577\n",
      "Restoring best model from epoch 9.\n",
      "     10        3.4833        2.5084  0.1350\n",
      "Restoring best model from epoch 8.\n",
      "     10        3.1588        2.8777  0.1426\n",
      "Restoring best model from epoch 2.\n",
      "     10        3.1332        2.8901  0.1446\n",
      "     10        \u001b[36m2.9034\u001b[0m        2.9912  0.1355\n",
      "Restoring best model from epoch 1.\n",
      "Restoring best model from epoch 8.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.2975\u001b[0m        \u001b[32m3.0518\u001b[0m  0.1462\n",
      "      2        \u001b[36m3.2321\u001b[0m        3.0757  0.0843\n",
      "      3        \u001b[36m3.2281\u001b[0m        3.1096  0.0797\n",
      "      4        \u001b[36m3.1718\u001b[0m        3.1228  0.0855\n",
      "      5        \u001b[36m3.1177\u001b[0m        3.1086  0.0918\n",
      "      6        \u001b[36m3.0951\u001b[0m        3.1015  0.0779\n",
      "      7        \u001b[36m3.0605\u001b[0m        3.1025  0.0899\n",
      "      8        \u001b[36m3.0460\u001b[0m        3.1018  0.0862\n",
      "      9        3.0531        3.1107  0.0794\n",
      "     10        \u001b[36m2.9982\u001b[0m        3.1197  0.0801\n",
      "Restoring best model from epoch 1.\n",
      "y_train breslow final [  -13.   -15.   -17.    19.   -20.    20.   -28.   -35.   -37.   -46.\n",
      "   -55.    56.    56.    57.    62.   -64.   -64.    65.   -68.    68.\n",
      "    81.   -82.    88.   -89.    90.   -95.    98.    99.  -105.   106.\n",
      "  -110.   118.   128.  -129.   131.   142.   144.   146.   149.   154.\n",
      "  -158.   163.  -163.   168.   173.  -189.   191.   200.   200.   205.\n",
      "   206.   211.   216.   220.   223.  -224.   232.   232.   237.   250.\n",
      "   253.   254.   258.   259.   261.   262.   272.   272.   273.   274.\n",
      "  -293.   294.   294.   303.   321.   324.   328.  -330.   332.  -333.\n",
      "   340.   344.   344.  -345.  -345.   356.  -361.   362.   364.  -365.\n",
      "  -366.  -368.  -369.   370.  -370.   370.  -373.  -376.  -377.  -379.\n",
      "  -382.  -384.   385.   386.   388.  -389.   391.  -398.  -399.   400.\n",
      "   406.   408.  -410.   413.   413.  -415.   415.  -415.  -416.   418.\n",
      "  -423.  -428.  -428.  -433.   434.   437.   454.  -455.   455.  -457.\n",
      "  -460.  -466.   467.  -467.   467.  -469.  -474.  -475.  -480.  -481.\n",
      "  -484.  -485.  -491.   492.   495.  -495.  -503.  -507.   508.   510.\n",
      "  -512.   522.  -522.  -524.   530.   536.  -536.   539.  -539.  -542.\n",
      "   544.  -546.   547.   550.  -560.  -562.   565.   565.  -572.  -572.\n",
      "  -573.   575.   577.  -578.   579.  -581.  -582.  -588.   590.  -590.\n",
      "   593.   599.   602.  -603.  -610.   612.   615.   617.   623.   630.\n",
      "  -633.  -636.  -638.  -640.  -641.  -642.  -646.  -648.  -649.   651.\n",
      "  -665.   665.   680.   685.  -691.   696.  -699.  -700.   706.   719.\n",
      "  -731.   734.   739.  -750.  -758.  -761.   778.  -783.  -789.   795.\n",
      "  -798.  -812.  -813.   819.  -820.   823.  -832.  -832.   835.  -840.\n",
      "  -842.  -851.   859.   864.  -873.  -887.  -893.  -897.  -899.   904.\n",
      "  -906.  -921.   941.   949.   974.  -997. -1003.  1004.  1005.  1008.\n",
      " -1029.  1036. -1048.  1064. -1072. -1094. -1110. -1115. -1127.  1163.\n",
      " -1174. -1181. -1186. -1219.  1270.  1348. -1350.  1367. -1370.  1420.\n",
      " -1429. -1454. -1455. -1460. -1522. -1529. -1538. -1561. -1582. -1604.\n",
      " -1621. -1649.  1670. -1708. -1714.  1718. -1761.  1804. -1806. -1830.\n",
      " -1845.  1869. -1884. -1912. -1947. -1949. -1952.  1971. -2008. -2009.\n",
      " -2020. -2024. -2027. -2044. -2109. -2139. -2177. -2293. -2312. -2330.\n",
      " -2423.  2641. -2790.  2828. -2868. -2886. -2964. -3011.  3183. -3364.\n",
      " -3420. -3432. -3817. -4343. -5041.]\n",
      "Concordance Index 0.39078337677381986\n",
      "Integrated Brier Score: 0.42494232729779596\n",
      "y_train breslow final [  -13.   -15.   -17.    19.   -20.    20.   -28.   -35.   -37.   -46.\n",
      "   -55.    56.    56.    57.    62.   -64.   -64.    65.   -68.    68.\n",
      "    81.   -82.    88.   -89.    90.   -95.    98.    99.  -105.   106.\n",
      "  -110.   118.   128.  -129.   131.   142.   144.   146.   149.   154.\n",
      "  -158.   163.  -163.   168.   173.  -189.   191.   200.   200.   205.\n",
      "   206.   211.   216.   220.   223.  -224.   232.   232.   237.   250.\n",
      "   253.   254.   258.   259.   261.   262.   272.   272.   273.   274.\n",
      "  -293.   294.   294.   303.   321.   324.   328.  -330.   332.  -333.\n",
      "   340.   344.   344.  -345.  -345.   356.  -361.   362.   364.  -365.\n",
      "  -366.  -368.  -369.   370.  -370.   370.  -373.  -376.  -377.  -379.\n",
      "  -382.  -384.   385.   386.   388.  -389.   391.  -398.  -399.   400.\n",
      "   406.   408.  -410.   413.   413.  -415.   415.  -415.  -416.   418.\n",
      "  -423.  -428.  -428.  -433.   434.   437.   454.  -455.   455.  -457.\n",
      "  -460.  -466.   467.  -467.   467.  -469.  -474.  -475.  -480.  -481.\n",
      "  -484.  -485.  -491.   492.   495.  -495.  -503.  -507.   508.   510.\n",
      "  -512.   522.  -522.  -524.   530.   536.  -536.   539.  -539.  -542.\n",
      "   544.  -546.   547.   550.  -560.  -562.   565.   565.  -572.  -572.\n",
      "  -573.   575.   577.  -578.   579.  -581.  -582.  -588.   590.  -590.\n",
      "   593.   599.   602.  -603.  -610.   612.   615.   617.   623.   630.\n",
      "  -633.  -636.  -638.  -640.  -641.  -642.  -646.  -648.  -649.   651.\n",
      "  -665.   665.   680.   685.  -691.   696.  -699.  -700.   706.   719.\n",
      "  -731.   734.   739.  -750.  -758.  -761.   778.  -783.  -789.   795.\n",
      "  -798.  -812.  -813.   819.  -820.   823.  -832.  -832.   835.  -840.\n",
      "  -842.  -851.   859.   864.  -873.  -887.  -893.  -897.  -899.   904.\n",
      "  -906.  -921.   941.   949.   974.  -997. -1003.  1004.  1005.  1008.\n",
      " -1029.  1036. -1048.  1064. -1072. -1094. -1110. -1115. -1127.  1163.\n",
      " -1174. -1181. -1186. -1219.  1270.  1348. -1350.  1367. -1370.  1420.\n",
      " -1429. -1454. -1455. -1460. -1522. -1529. -1538. -1561. -1582. -1604.\n",
      " -1621. -1649.  1670. -1708. -1714.  1718. -1761.  1804. -1806. -1830.\n",
      " -1845.  1869. -1884. -1912. -1947. -1949. -1952.  1971. -2008. -2009.\n",
      " -2020. -2024. -2027. -2044. -2109. -2139. -2177. -2293. -2312. -2330.\n",
      " -2423.  2641. -2790.  2828. -2868. -2886. -2964. -3011.  3183. -3364.\n",
      " -3420. -3432. -3817. -4343. -5041.]\n",
      "durations 59.0 5050.0\n",
      "Concordance Index 0.4635603345280765\n",
      "Integrated Brier Score: 0.3771829616644717\n",
      "(325, 20531) <class 'pandas.core.frame.DataFrame'>\n",
      "(325,) <class 'pandas.core.series.Series'>\n",
      "(81, 20531) <class 'pandas.core.frame.DataFrame'>\n",
      "(81,) <class 'pandas.core.series.Series'>\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.2503\u001b[0m        \u001b[32m3.8125\u001b[0m  0.1952\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.3442\u001b[0m        \u001b[32m3.5237\u001b[0m  0.1858\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.4401\u001b[0m        \u001b[32m3.2161\u001b[0m  0.2516\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.2853\u001b[0m        \u001b[32m3.3622\u001b[0m  0.3190\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.4534\u001b[0m        \u001b[32m3.2658\u001b[0m  0.2785\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.2627\u001b[0m        \u001b[32m3.3396\u001b[0m  0.3339\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.3141\u001b[0m        \u001b[32m3.5503\u001b[0m  0.3431\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.0964\u001b[0m        \u001b[32m2.8577\u001b[0m  0.2933\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.2442\u001b[0m        \u001b[32m3.8148\u001b[0m  0.3039\n",
      "      2        \u001b[36m3.2258\u001b[0m        \u001b[32m3.8101\u001b[0m  0.1763\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.1137\u001b[0m        \u001b[32m2.8305\u001b[0m  0.3061\n",
      "      2        \u001b[36m3.4370\u001b[0m        \u001b[32m3.2124\u001b[0m  0.1701\n",
      "      2        \u001b[36m3.2915\u001b[0m        \u001b[32m3.5142\u001b[0m  0.2115\n",
      "      2        \u001b[36m3.1913\u001b[0m        \u001b[32m3.3503\u001b[0m  0.1860\n",
      "      2        \u001b[36m3.3879\u001b[0m        \u001b[32m3.2588\u001b[0m  0.1677\n",
      "      2        \u001b[36m3.2516\u001b[0m        \u001b[32m3.3221\u001b[0m  0.1788\n",
      "      3        3.2340        \u001b[32m3.8087\u001b[0m  0.1744\n",
      "      2        3.1035        2.8670  0.1995\n",
      "      2        \u001b[36m3.2396\u001b[0m        \u001b[32m3.8024\u001b[0m  0.1913\n",
      "      2        \u001b[36m3.2823\u001b[0m        \u001b[32m3.5485\u001b[0m  0.2194\n",
      "      3        \u001b[36m3.3752\u001b[0m        \u001b[32m3.1931\u001b[0m  0.1546\n",
      "      2        \u001b[36m3.0985\u001b[0m        2.8488  0.1932\n",
      "      3        \u001b[36m3.1972\u001b[0m        \u001b[32m3.4928\u001b[0m  0.1668\n",
      "      3        \u001b[36m3.1653\u001b[0m        \u001b[32m3.2601\u001b[0m  0.1604\n",
      "      3        \u001b[36m3.3513\u001b[0m        3.2640  0.1622\n",
      "      3        \u001b[36m3.2264\u001b[0m        \u001b[32m3.2906\u001b[0m  0.1554\n",
      "      4        3.2278        3.8121  0.1700\n",
      "      3        \u001b[36m3.2562\u001b[0m        \u001b[32m3.5306\u001b[0m  0.1610\n",
      "      3        \u001b[36m3.1822\u001b[0m        3.8044  0.1747\n",
      "      3        \u001b[36m3.0720\u001b[0m        \u001b[32m2.8189\u001b[0m  0.1645\n",
      "      4        3.2003        \u001b[32m3.4707\u001b[0m  0.1564\n",
      "      4        \u001b[36m3.1133\u001b[0m        \u001b[32m3.2470\u001b[0m  0.1294\n",
      "      4        \u001b[36m3.2893\u001b[0m        3.2655  0.1469\n",
      "      3        \u001b[36m3.0923\u001b[0m        2.8602  0.2629\n",
      "      4        3.2483        \u001b[32m3.2771\u001b[0m  0.1415\n",
      "      4        3.4135        \u001b[32m3.1831\u001b[0m  0.2775\n",
      "      5        \u001b[36m3.2224\u001b[0m        3.8170  0.1384\n",
      "      4        3.2081        3.8090  0.1377\n",
      "      4        3.2886        \u001b[32m3.5241\u001b[0m  0.1459\n",
      "      4        \u001b[36m3.0267\u001b[0m        \u001b[32m2.7923\u001b[0m  0.1361\n",
      "      5        \u001b[36m3.1406\u001b[0m        \u001b[32m3.4634\u001b[0m  0.1622\n",
      "      5        \u001b[36m3.0869\u001b[0m        \u001b[32m3.2464\u001b[0m  0.1761\n",
      "      5        \u001b[36m3.1926\u001b[0m        \u001b[32m3.2643\u001b[0m  0.1638\n",
      "      4        \u001b[36m3.0875\u001b[0m        \u001b[32m2.8441\u001b[0m  0.1808\n",
      "      5        \u001b[36m3.2400\u001b[0m        3.2764  0.1939\n",
      "      5        3.4100        \u001b[32m3.1780\u001b[0m  0.1738\n",
      "      5        \u001b[36m3.1231\u001b[0m        3.8091  0.1764\n",
      "      5        \u001b[36m3.2073\u001b[0m        \u001b[32m3.5196\u001b[0m  0.1824\n",
      "      5        \u001b[36m2.9760\u001b[0m        \u001b[32m2.7901\u001b[0m  0.1804\n",
      "      6        \u001b[36m3.2175\u001b[0m        3.8180  0.2016\n",
      "      6        \u001b[36m3.1095\u001b[0m        \u001b[32m3.4614\u001b[0m  0.1864\n",
      "      6        \u001b[36m3.0596\u001b[0m        \u001b[32m3.2437\u001b[0m  0.1443\n",
      "      6        3.2328        \u001b[32m3.2594\u001b[0m  0.1396\n",
      "      6        3.2615        3.2798  0.1625\n",
      "      6        3.1398        3.8081  0.1257\n",
      "      6        3.2534        \u001b[32m3.5176\u001b[0m  0.1359\n",
      "      6        3.0111        2.7923  0.1477\n",
      "      6        \u001b[36m3.3491\u001b[0m        3.1802  0.1861\n",
      "      5        \u001b[36m3.0555\u001b[0m        \u001b[32m2.8258\u001b[0m  0.2125\n",
      "      7        3.0821        \u001b[32m3.2390\u001b[0m  0.1335\n",
      "      7        \u001b[36m3.2150\u001b[0m        3.8169  0.2000\n",
      "      7        3.2020        \u001b[32m3.2559\u001b[0m  0.1373\n",
      "      7        \u001b[36m3.0744\u001b[0m        3.4651  0.1598\n",
      "      7        \u001b[36m3.1132\u001b[0m        3.8098  0.1414\n",
      "      7        \u001b[36m3.2276\u001b[0m        3.2807  0.1464\n",
      "      7        \u001b[36m3.3384\u001b[0m        3.1807  0.1377\n",
      "      7        3.2236        \u001b[32m3.5098\u001b[0m  0.1492\n",
      "      7        2.9813        2.7974  0.1463\n",
      "      8        3.2186        3.8151  0.1253\n",
      "      6        3.0932        \u001b[32m2.8133\u001b[0m  0.1975\n",
      "      8        \u001b[36m3.1571\u001b[0m        \u001b[32m3.2509\u001b[0m  0.1506\n",
      "      8        \u001b[36m2.9675\u001b[0m        3.4766  0.1538\n",
      "      8        \u001b[36m3.0602\u001b[0m        3.8105  0.1616\n",
      "      8        \u001b[36m3.3192\u001b[0m        3.1882  0.1341\n",
      "      8        2.9975        2.8049  0.1399\n",
      "      8        3.2088        \u001b[32m3.5029\u001b[0m  0.1626\n",
      "      9        3.2206        3.8150  0.1316\n",
      "      8        \u001b[36m3.0003\u001b[0m        \u001b[32m3.2350\u001b[0m  0.2567\n",
      "      9        3.1661        3.2513  0.1409\n",
      "      8        \u001b[36m3.1687\u001b[0m        3.2801  0.2470\n",
      "      7        3.0996        \u001b[32m2.8075\u001b[0m  0.1511\n",
      "      9        3.0292        3.4878  0.1548\n",
      "      9        \u001b[36m3.3028\u001b[0m        3.1911  0.1430\n",
      "      9        2.9977        2.8115  0.1422\n",
      "      9        3.1081        3.8103  0.1588\n",
      "     10        \u001b[36m3.2064\u001b[0m        3.8149  0.1509\n",
      "Restoring best model from epoch 3.\n",
      "      9        \u001b[36m3.1959\u001b[0m        \u001b[32m3.5022\u001b[0m  0.1588\n",
      "      9        3.0348        3.2355  0.1570\n",
      "     10        3.1777        \u001b[32m3.2502\u001b[0m  0.1527\n",
      "      9        3.1906        3.2748  0.1588\n",
      "      8        3.0719        \u001b[32m2.8015\u001b[0m  0.1534\n",
      "     10        3.0044        3.4959  0.1617\n",
      "Restoring best model from epoch 6.\n",
      "     10        \u001b[36m3.2992\u001b[0m        3.1953  0.1380\n",
      "Restoring best model from epoch 5.\n",
      "     10        \u001b[36m2.9399\u001b[0m        2.8173  0.1513\n",
      "Restoring best model from epoch 5.\n",
      "     10        \u001b[36m3.0371\u001b[0m        3.8108  0.1559\n",
      "Restoring best model from epoch 2.\n",
      "     10        3.0137        \u001b[32m3.2340\u001b[0m  0.1642\n",
      "     10        \u001b[36m3.1903\u001b[0m        \u001b[32m3.5005\u001b[0m  0.1731\n",
      "      9        3.0695        \u001b[32m2.7975\u001b[0m  0.1383\n",
      "     10        3.2128        3.2685  0.1769\n",
      "Restoring best model from epoch 2.\n",
      "     10        3.0630        \u001b[32m2.7964\u001b[0m  0.1198\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.3123\u001b[0m        \u001b[32m3.1772\u001b[0m  0.1055\n",
      "      2        \u001b[36m3.2718\u001b[0m        \u001b[32m3.1305\u001b[0m  0.0963\n",
      "      3        \u001b[36m3.2313\u001b[0m        \u001b[32m3.1099\u001b[0m  0.0793\n",
      "      4        \u001b[36m3.2209\u001b[0m        3.1194  0.0783\n",
      "      5        \u001b[36m3.2207\u001b[0m        3.1455  0.0852\n",
      "      6        \u001b[36m3.2035\u001b[0m        3.1608  0.0832\n",
      "      7        3.2306        3.1746  0.0867\n",
      "      8        \u001b[36m3.1777\u001b[0m        3.1887  0.0832\n",
      "      9        3.1812        3.1970  0.0831\n",
      "     10        3.1860        3.2033  0.0743\n",
      "Restoring best model from epoch 3.\n",
      "y_train breslow final [  -13.   -15.   -17.    19.   -20.    20.   -28.   -35.   -37.   -46.\n",
      "    56.    56.    57.   -59.    62.   -64.   -64.   -67.    69.    76.\n",
      "    81.   -82.   -84.    88.    92.    93.   -95.    98.    99.  -105.\n",
      "   106.  -110.  -117.   118.   122.   131.   142.   144.   146.   154.\n",
      "   154.  -158.   168.   168.   173.   182.  -187.  -189.   200.   200.\n",
      "   205.   211.   213.   216.   223.   232.   246.   248.   251.  -251.\n",
      "   253.   254.   254.   258.   259.   262.   272.   272.   272.   273.\n",
      "  -276.   278.  -293.   294.   294.   303.   311.   324.   328.  -330.\n",
      "  -333.  -337.   340.   344.  -345.  -345.  -359.   362.   364.  -364.\n",
      "  -365.  -368.   370.  -370.   370.  -372.  -372.  -376.  -379.  -383.\n",
      "  -384.  -384.   385.   385.   386.   388.  -389.   391.   393.  -394.\n",
      "  -398.  -398.  -399.   400.   408.  -410.   413.   413.   413.  -415.\n",
      "   415.  -415.  -416.  -423.  -425.  -428.  -433.   434.   437.   453.\n",
      "   454.  -455.   455.  -457.  -460.   460.  -466.  -467.  -469.   474.\n",
      "  -475.  -477.  -480.  -481.  -484.  -485.   486.  -491.   492.  -495.\n",
      "  -495.  -507.   508.  -508.   510.   510.   522.  -522.  -524.   530.\n",
      "  -536.   539.  -539.  -540.  -542.   544.  -546.   547.   547.   565.\n",
      "   565.  -572.  -572.  -573.   575.   577.  -578.  -578.   579.  -580.\n",
      "  -588.   590.  -590.   593.   599.   602.   612.   615.   617.  -618.\n",
      "   623.   630.  -633.  -638.  -640.  -641.  -642.  -646.  -648.  -649.\n",
      "  -665.   665.   674.   680.   685.   690.  -691.   696.  -699.   706.\n",
      "   712.  -731.   739.  -750.  -761.  -773.  -783.   795.  -798.  -799.\n",
      "  -812.   819.  -820.   823.  -832.   835.  -840.  -851.   859.  -864.\n",
      "  -873.  -897.  -899.   904.  -906.  -921.   941.  -945.   949. -1003.\n",
      "  1004.  1008.  1036. -1048.  1064. -1072.  1077. -1090. -1094. -1108.\n",
      " -1110. -1127.  1163. -1174. -1181. -1186. -1219.  1270. -1326.  1348.\n",
      " -1350.  1367. -1370.  1423. -1429. -1454. -1455. -1460. -1522. -1529.\n",
      " -1538. -1542.  1556. -1561. -1604. -1621. -1639. -1649.  1670. -1708.\n",
      " -1714. -1761. -1792.  1804. -1806. -1845.  1869. -1884. -1947. -1949.\n",
      " -1952.  1971. -2008. -2009.  2020. -2024. -2027. -2044. -2049. -2109.\n",
      " -2177. -2293. -2312. -2380. -2625.  2641. -2656. -2703. -2790.  2828.\n",
      " -2868. -2886.  2954. -2964. -3011.  3183. -3314. -3364. -3420. -3432.\n",
      " -3817. -3981. -4343. -4967. -5050.]\n",
      "Concordance Index 0.41333528400570574\n",
      "Integrated Brier Score: 0.31748422891009365\n",
      "y_train breslow final [  -13.   -15.   -17.    19.   -20.    20.   -28.   -35.   -37.   -46.\n",
      "    56.    56.    57.   -59.    62.   -64.   -64.   -67.    69.    76.\n",
      "    81.   -82.   -84.    88.    92.    93.   -95.    98.    99.  -105.\n",
      "   106.  -110.  -117.   118.   122.   131.   142.   144.   146.   154.\n",
      "   154.  -158.   168.   168.   173.   182.  -187.  -189.   200.   200.\n",
      "   205.   211.   213.   216.   223.   232.   246.   248.   251.  -251.\n",
      "   253.   254.   254.   258.   259.   262.   272.   272.   272.   273.\n",
      "  -276.   278.  -293.   294.   294.   303.   311.   324.   328.  -330.\n",
      "  -333.  -337.   340.   344.  -345.  -345.  -359.   362.   364.  -364.\n",
      "  -365.  -368.   370.  -370.   370.  -372.  -372.  -376.  -379.  -383.\n",
      "  -384.  -384.   385.   385.   386.   388.  -389.   391.   393.  -394.\n",
      "  -398.  -398.  -399.   400.   408.  -410.   413.   413.   413.  -415.\n",
      "   415.  -415.  -416.  -423.  -425.  -428.  -433.   434.   437.   453.\n",
      "   454.  -455.   455.  -457.  -460.   460.  -466.  -467.  -469.   474.\n",
      "  -475.  -477.  -480.  -481.  -484.  -485.   486.  -491.   492.  -495.\n",
      "  -495.  -507.   508.  -508.   510.   510.   522.  -522.  -524.   530.\n",
      "  -536.   539.  -539.  -540.  -542.   544.  -546.   547.   547.   565.\n",
      "   565.  -572.  -572.  -573.   575.   577.  -578.  -578.   579.  -580.\n",
      "  -588.   590.  -590.   593.   599.   602.   612.   615.   617.  -618.\n",
      "   623.   630.  -633.  -638.  -640.  -641.  -642.  -646.  -648.  -649.\n",
      "  -665.   665.   674.   680.   685.   690.  -691.   696.  -699.   706.\n",
      "   712.  -731.   739.  -750.  -761.  -773.  -783.   795.  -798.  -799.\n",
      "  -812.   819.  -820.   823.  -832.   835.  -840.  -851.   859.  -864.\n",
      "  -873.  -897.  -899.   904.  -906.  -921.   941.  -945.   949. -1003.\n",
      "  1004.  1008.  1036. -1048.  1064. -1072.  1077. -1090. -1094. -1108.\n",
      " -1110. -1127.  1163. -1174. -1181. -1186. -1219.  1270. -1326.  1348.\n",
      " -1350.  1367. -1370.  1423. -1429. -1454. -1455. -1460. -1522. -1529.\n",
      " -1538. -1542.  1556. -1561. -1604. -1621. -1639. -1649.  1670. -1708.\n",
      " -1714. -1761. -1792.  1804. -1806. -1845.  1869. -1884. -1947. -1949.\n",
      " -1952.  1971. -2008. -2009.  2020. -2024. -2027. -2044. -2049. -2109.\n",
      " -2177. -2293. -2312. -2380. -2625.  2641. -2656. -2703. -2790.  2828.\n",
      " -2868. -2886.  2954. -2964. -3011.  3183. -3314. -3364. -3420. -3432.\n",
      " -3817. -3981. -4343. -4967. -5050.]\n",
      "durations 55.0 5041.0\n",
      "Concordance Index 0.4616265750286369\n",
      "Integrated Brier Score: 0.2849208525047959\n",
      "(325, 20531) <class 'pandas.core.frame.DataFrame'>\n",
      "(325,) <class 'pandas.core.series.Series'>\n",
      "(81, 20531) <class 'pandas.core.frame.DataFrame'>\n",
      "(81,) <class 'pandas.core.series.Series'>\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.3850\u001b[0m        \u001b[32m3.2720\u001b[0m  0.2066\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.8631\u001b[0m        \u001b[32m2.4964\u001b[0m  0.1572\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.1427\u001b[0m        \u001b[32m2.8928\u001b[0m  0.1647\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.3638\u001b[0m        \u001b[32m3.2393\u001b[0m  0.1655\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.2326\u001b[0m        \u001b[32m2.9067\u001b[0m  0.1975\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.5535\u001b[0m        \u001b[32m3.4475\u001b[0m  0.2398\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.5500\u001b[0m        \u001b[32m3.4520\u001b[0m  0.2326\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.1328\u001b[0m        \u001b[32m2.8877\u001b[0m  0.2251\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.8917\u001b[0m        \u001b[32m2.4820\u001b[0m  0.2475\n",
      "      2        3.4125        \u001b[32m3.2574\u001b[0m  0.1608\n",
      "      2        \u001b[36m2.8541\u001b[0m        2.5034  0.1657\n",
      "      2        \u001b[36m3.2996\u001b[0m        3.2734  0.1487\n",
      "      2        3.1490        \u001b[32m2.8911\u001b[0m  0.1722\n",
      "      2        3.2412        2.9178  0.1534\n",
      "      2        3.5624        \u001b[32m3.4477\u001b[0m  0.1540\n",
      "      2        \u001b[36m3.5531\u001b[0m        3.4507  0.1759\n",
      "      2        \u001b[36m3.0485\u001b[0m        2.8947  0.1655\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.2316\u001b[0m        \u001b[32m2.9136\u001b[0m  0.2523\n",
      "      2        \u001b[36m2.8858\u001b[0m        2.5041  0.2225\n",
      "      3        \u001b[36m2.8525\u001b[0m        \u001b[32m2.4945\u001b[0m  0.2261\n",
      "      3        3.3854        \u001b[32m3.2313\u001b[0m  0.2826\n",
      "      3        3.5808        3.4546  0.2094\n",
      "      3        \u001b[36m3.2164\u001b[0m        3.2809  0.2856\n",
      "      2        \u001b[36m3.1126\u001b[0m        \u001b[32m2.8983\u001b[0m  0.1908\n",
      "      3        \u001b[36m3.2213\u001b[0m        2.9081  0.2956\n",
      "      3        \u001b[36m3.0967\u001b[0m        \u001b[32m2.8830\u001b[0m  0.3168\n",
      "      3        \u001b[36m3.5441\u001b[0m        \u001b[32m3.4406\u001b[0m  0.2844\n",
      "      3        \u001b[36m3.0145\u001b[0m        2.9018  0.2443\n",
      "      3        \u001b[36m2.8561\u001b[0m        2.4976  0.1840\n",
      "      4        \u001b[36m2.8007\u001b[0m        \u001b[32m2.4873\u001b[0m  0.1884\n",
      "      4        \u001b[36m3.3768\u001b[0m        \u001b[32m3.2170\u001b[0m  0.1517\n",
      "      4        3.5597        3.4479  0.1787\n",
      "      4        3.2288        3.2826  0.1912\n",
      "      3        \u001b[36m3.0778\u001b[0m        \u001b[32m2.8977\u001b[0m  0.1685\n",
      "      4        \u001b[36m3.1910\u001b[0m        \u001b[32m2.9024\u001b[0m  0.1739\n",
      "      4        \u001b[36m3.5143\u001b[0m        3.4422  0.1807\n",
      "      4        \u001b[36m2.9528\u001b[0m        2.9053  0.1726\n",
      "      4        3.1180        \u001b[32m2.8825\u001b[0m  0.2133\n",
      "      4        2.8755        2.4892  0.1727\n",
      "      5        2.8015        2.4876  0.1733\n",
      "      5        \u001b[36m3.2778\u001b[0m        \u001b[32m3.2081\u001b[0m  0.1739\n",
      "      5        3.5550        \u001b[32m3.4377\u001b[0m  0.1542\n",
      "      5        \u001b[36m3.1173\u001b[0m        3.2775  0.1934\n",
      "      5        \u001b[36m3.4363\u001b[0m        3.4465  0.1578\n",
      "      4        \u001b[36m3.0473\u001b[0m        \u001b[32m2.8937\u001b[0m  0.1977\n",
      "      5        \u001b[36m3.1452\u001b[0m        \u001b[32m2.9021\u001b[0m  0.1789\n",
      "      5        \u001b[36m2.9288\u001b[0m        2.9073  0.1713\n",
      "      5        \u001b[36m3.0702\u001b[0m        \u001b[32m2.8786\u001b[0m  0.1587\n",
      "      5        \u001b[36m2.8174\u001b[0m        2.4844  0.1692\n",
      "      6        \u001b[36m2.7672\u001b[0m        \u001b[32m2.4872\u001b[0m  0.1508\n",
      "      6        3.5633        \u001b[32m3.4314\u001b[0m  0.1494\n",
      "      6        3.2983        \u001b[32m3.2005\u001b[0m  0.1604\n",
      "      6        3.1305        3.2640  0.1618\n",
      "      5        \u001b[36m3.0279\u001b[0m        \u001b[32m2.8903\u001b[0m  0.1545\n",
      "      6        3.1674        \u001b[32m2.9005\u001b[0m  0.1536\n",
      "      6        3.4696        3.4485  0.1750\n",
      "      6        \u001b[36m3.0480\u001b[0m        2.8797  0.1469\n",
      "      6        \u001b[36m2.8170\u001b[0m        2.4823  0.1515\n",
      "      6        \u001b[36m2.8504\u001b[0m        2.9058  0.1766\n",
      "      7        \u001b[36m2.7574\u001b[0m        2.4880  0.1498\n",
      "      7        \u001b[36m3.5444\u001b[0m        3.4333  0.1407\n",
      "      7        3.3089        3.2010  0.1490\n",
      "      6        \u001b[36m2.9779\u001b[0m        2.8916  0.1531\n",
      "      7        3.1440        3.2585  0.1635\n",
      "      7        \u001b[36m3.4237\u001b[0m        3.4441  0.1439\n",
      "      7        \u001b[36m3.1206\u001b[0m        2.9006  0.1692\n",
      "      7        3.0739        2.8795  0.1727\n",
      "      7        \u001b[36m2.8024\u001b[0m        \u001b[32m2.4770\u001b[0m  0.1602\n",
      "      7        2.8671        2.9014  0.1574\n",
      "      8        2.7778        \u001b[32m2.4865\u001b[0m  0.1487\n",
      "      8        3.5520        3.4353  0.1477\n",
      "      8        \u001b[36m3.2682\u001b[0m        3.2034  0.1418\n",
      "      8        \u001b[36m3.3914\u001b[0m        \u001b[32m3.4380\u001b[0m  0.1393\n",
      "      8        \u001b[36m3.0590\u001b[0m        3.2463  0.1493\n",
      "      7        \u001b[36m2.9554\u001b[0m        2.8973  0.1565\n",
      "      8        2.8253        2.4772  0.1470\n",
      "      8        3.1274        2.9009  0.1754\n",
      "      8        \u001b[36m3.0398\u001b[0m        2.8796  0.1659\n",
      "      9        2.7679        \u001b[32m2.4826\u001b[0m  0.1553\n",
      "      9        3.3077        3.2078  0.1527\n",
      "      9        \u001b[36m3.4746\u001b[0m        3.4386  0.1625\n",
      "      8        \u001b[36m2.8230\u001b[0m        2.8995  0.1809\n",
      "      9        \u001b[36m3.3765\u001b[0m        \u001b[32m3.4330\u001b[0m  0.1372\n",
      "      8        \u001b[36m2.8902\u001b[0m        2.8960  0.1459\n",
      "      9        \u001b[36m3.0579\u001b[0m        \u001b[32m3.2337\u001b[0m  0.1555\n",
      "      9        \u001b[36m2.7935\u001b[0m        2.4781  0.1567\n",
      "      9        \u001b[36m3.1193\u001b[0m        \u001b[32m2.9003\u001b[0m  0.1523\n",
      "      9        \u001b[36m3.0386\u001b[0m        \u001b[32m2.8786\u001b[0m  0.1509\n",
      "     10        \u001b[36m2.7263\u001b[0m        \u001b[32m2.4797\u001b[0m  0.1533\n",
      "     10        3.3047        3.2100  0.1497\n",
      "Restoring best model from epoch 6.\n",
      "      9        \u001b[36m2.7900\u001b[0m        2.9022  0.1529\n",
      "     10        3.4895        3.4408  0.1681\n",
      "Restoring best model from epoch 6.\n",
      "     10        3.4069        3.4360  0.1449\n",
      "Restoring best model from epoch 9.\n",
      "     10        \u001b[36m3.0482\u001b[0m        \u001b[32m3.2236\u001b[0m  0.1501\n",
      "      9        2.9227        2.8983  0.1754\n",
      "     10        2.8059        \u001b[32m2.4728\u001b[0m  0.1407\n",
      "     10        3.1688        \u001b[32m2.8986\u001b[0m  0.1388\n",
      "     10        3.0587        \u001b[32m2.8774\u001b[0m  0.1530\n",
      "     10        \u001b[36m2.7781\u001b[0m        2.9096  0.1497\n",
      "Restoring best model from epoch 1.\n",
      "     10        \u001b[36m2.8634\u001b[0m        2.9008  0.1295\n",
      "Restoring best model from epoch 5.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.1702\u001b[0m        \u001b[32m3.5096\u001b[0m  0.1398\n",
      "      2        \u001b[36m3.1358\u001b[0m        \u001b[32m3.5072\u001b[0m  0.0892\n",
      "      3        \u001b[36m3.1009\u001b[0m        \u001b[32m3.5054\u001b[0m  0.0793\n",
      "      4        3.1013        \u001b[32m3.5046\u001b[0m  0.0799\n",
      "      5        3.1071        3.5051  0.0812\n",
      "      6        3.1036        3.5067  0.0885\n",
      "      7        \u001b[36m3.0969\u001b[0m        3.5104  0.0875\n",
      "      8        \u001b[36m3.0754\u001b[0m        3.5125  0.0794\n",
      "      9        \u001b[36m3.0520\u001b[0m        3.5136  0.0907\n",
      "     10        \u001b[36m3.0326\u001b[0m        3.5139  0.0784\n",
      "Restoring best model from epoch 4.\n",
      "y_train breslow final [  -15.   -17.    20.   -28.   -37.   -55.    56.    57.   -59.    62.\n",
      "   -64.    65.   -67.   -68.    68.    69.    76.    81.   -84.    88.\n",
      "   -89.    90.    92.    93.   -95.    98.    99.   106.  -117.   122.\n",
      "   128.  -129.   131.   142.   144.   146.   149.   154.   154.   163.\n",
      "  -163.   168.   173.   182.  -187.  -189.   191.   200.   200.   205.\n",
      "   206.   211.   213.   216.   220.   223.  -224.   232.   232.   237.\n",
      "   246.   248.   250.   251.  -251.   253.   254.   258.   261.   262.\n",
      "   272.   272.   274.  -276.   278.  -293.   294.   303.   311.   321.\n",
      "   324.   324.   328.   332.  -333.  -337.   340.   344.   344.  -345.\n",
      "  -345.   356.  -359.  -361.   362.   364.  -364.  -365.  -366.  -368.\n",
      "  -368.  -369.  -370.   370.  -372.  -372.  -373.  -376.  -377.  -379.\n",
      "  -382.  -383.  -384.  -384.   385.   385.   388.  -389.   391.   393.\n",
      "  -394.  -398.  -399.   400.   406.   408.  -410.   413.   413.   413.\n",
      "  -415.   415.   418.  -423.  -425.  -428.  -428.  -433.   434.   437.\n",
      "   453.   454.  -455.   455.  -457.  -460.   460.  -466.  -466.   467.\n",
      "   467.  -469.   474.  -474.  -477.  -480.  -481.  -484.  -485.   486.\n",
      "  -491.   492.   495.  -495.  -503.  -507.  -508.   510.   510.  -512.\n",
      "   522.  -522.  -524.   530.  -536.   536.  -536.   539.  -539.  -540.\n",
      "  -542.   544.   544.   547.   547.   550.  -560.  -562.   565.  -572.\n",
      "  -572.  -573.   575.  -578.  -578.   579.  -580.  -581.  -582.  -588.\n",
      "  -590.   593.   602.  -603.  -610.   612.   615.   617.  -618.  -633.\n",
      "  -636.  -638.  -640.  -646.  -648.   651.   674.   690.  -691.  -699.\n",
      "  -700.   712.   719.   734.   739.  -758.  -761.  -773.   778.  -783.\n",
      "  -789.  -798.  -799.  -813.  -820.  -832.  -832.   835.  -840.  -842.\n",
      "  -851.   859.   864.  -864.  -873.  -887.  -893.  -897.  -899.   904.\n",
      "  -921.  -945.   949.   974.  -997. -1003.  1004.  1005. -1029.  1064.\n",
      " -1072.  1077. -1090. -1094. -1108. -1115.  1163. -1174. -1181. -1186.\n",
      " -1219. -1326. -1350.  1367. -1370.  1420.  1423. -1455. -1460. -1522.\n",
      " -1529. -1542.  1556. -1561. -1582. -1604. -1639.  1670. -1714.  1718.\n",
      " -1761. -1792. -1806. -1830. -1912. -1947. -1949. -1952.  1971. -2008.\n",
      " -2009.  2020. -2020. -2049. -2109. -2139. -2293. -2312. -2330. -2380.\n",
      " -2423. -2625. -2656. -2703. -2790. -2886.  2954. -3314. -3420. -3817.\n",
      " -3981. -4343. -4967. -5041. -5050.]\n",
      "Concordance Index 0.38825477347988446\n",
      "Integrated Brier Score: 0.3367424411065787\n",
      "y_train breslow final [  -15.   -17.    20.   -28.   -37.   -55.    56.    57.   -59.    62.\n",
      "   -64.    65.   -67.   -68.    68.    69.    76.    81.   -84.    88.\n",
      "   -89.    90.    92.    93.   -95.    98.    99.   106.  -117.   122.\n",
      "   128.  -129.   131.   142.   144.   146.   149.   154.   154.   163.\n",
      "  -163.   168.   173.   182.  -187.  -189.   191.   200.   200.   205.\n",
      "   206.   211.   213.   216.   220.   223.  -224.   232.   232.   237.\n",
      "   246.   248.   250.   251.  -251.   253.   254.   258.   261.   262.\n",
      "   272.   272.   274.  -276.   278.  -293.   294.   303.   311.   321.\n",
      "   324.   324.   328.   332.  -333.  -337.   340.   344.   344.  -345.\n",
      "  -345.   356.  -359.  -361.   362.   364.  -364.  -365.  -366.  -368.\n",
      "  -368.  -369.  -370.   370.  -372.  -372.  -373.  -376.  -377.  -379.\n",
      "  -382.  -383.  -384.  -384.   385.   385.   388.  -389.   391.   393.\n",
      "  -394.  -398.  -399.   400.   406.   408.  -410.   413.   413.   413.\n",
      "  -415.   415.   418.  -423.  -425.  -428.  -428.  -433.   434.   437.\n",
      "   453.   454.  -455.   455.  -457.  -460.   460.  -466.  -466.   467.\n",
      "   467.  -469.   474.  -474.  -477.  -480.  -481.  -484.  -485.   486.\n",
      "  -491.   492.   495.  -495.  -503.  -507.  -508.   510.   510.  -512.\n",
      "   522.  -522.  -524.   530.  -536.   536.  -536.   539.  -539.  -540.\n",
      "  -542.   544.   544.   547.   547.   550.  -560.  -562.   565.  -572.\n",
      "  -572.  -573.   575.  -578.  -578.   579.  -580.  -581.  -582.  -588.\n",
      "  -590.   593.   602.  -603.  -610.   612.   615.   617.  -618.  -633.\n",
      "  -636.  -638.  -640.  -646.  -648.   651.   674.   690.  -691.  -699.\n",
      "  -700.   712.   719.   734.   739.  -758.  -761.  -773.   778.  -783.\n",
      "  -789.  -798.  -799.  -813.  -820.  -832.  -832.   835.  -840.  -842.\n",
      "  -851.   859.   864.  -864.  -873.  -887.  -893.  -897.  -899.   904.\n",
      "  -921.  -945.   949.   974.  -997. -1003.  1004.  1005. -1029.  1064.\n",
      " -1072.  1077. -1090. -1094. -1108. -1115.  1163. -1174. -1181. -1186.\n",
      " -1219. -1326. -1350.  1367. -1370.  1420.  1423. -1455. -1460. -1522.\n",
      " -1529. -1542.  1556. -1561. -1582. -1604. -1639.  1670. -1714.  1718.\n",
      " -1761. -1792. -1806. -1830. -1912. -1947. -1949. -1952.  1971. -2008.\n",
      " -2009.  2020. -2020. -2049. -2109. -2139. -2293. -2312. -2330. -2380.\n",
      " -2423. -2625. -2656. -2703. -2790. -2886.  2954. -3314. -3420. -3817.\n",
      " -3981. -4343. -4967. -5041. -5050.]\n",
      "durations 13.0 3432.0\n",
      "Concordance Index 0.3652230122818358\n",
      "Integrated Brier Score: 0.25346063297927995\n",
      "split gex_?|100130426      float32\n",
      "gex_?|100133144      float32\n",
      "gex_?|100134869      float32\n",
      "gex_?|10357          float32\n",
      "gex_?|10431          float32\n",
      "                      ...   \n",
      "gex_ZYG11A|440590    float32\n",
      "gex_ZYG11B|79699     float32\n",
      "gex_ZYX|7791         float32\n",
      "gex_ZZEF1|23140      float32\n",
      "gex_ZZZ3|26009       float32\n",
      "Length: 20531, dtype: object\n",
      "(864, 20531) <class 'pandas.core.frame.DataFrame'>\n",
      "(864,) <class 'pandas.core.series.Series'>\n",
      "(217, 20531) <class 'pandas.core.frame.DataFrame'>\n",
      "(217,) <class 'pandas.core.series.Series'>\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m1.4693\u001b[0m        \u001b[32m1.2610\u001b[0m  0.9935\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m1.3390\u001b[0m        \u001b[32m1.1105\u001b[0m  0.9684\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m1.4614\u001b[0m        \u001b[32m1.2462\u001b[0m  0.7900\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m1.0408\u001b[0m        \u001b[32m0.9484\u001b[0m  0.9532\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m1.2618\u001b[0m        \u001b[32m1.0573\u001b[0m  0.9528\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m1.0414\u001b[0m        \u001b[32m0.9174\u001b[0m  0.7875\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m1.2710\u001b[0m        \u001b[32m1.0418\u001b[0m  1.0160\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m1.4296\u001b[0m        \u001b[32m1.0583\u001b[0m  0.9522\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m1.3450\u001b[0m        \u001b[32m1.1193\u001b[0m  0.9644\n",
      "      2        \u001b[36m1.4528\u001b[0m        \u001b[32m1.2579\u001b[0m  0.7076\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m1.4285\u001b[0m        \u001b[32m1.0668\u001b[0m  1.4187\n",
      "      2        1.3465        \u001b[32m1.1083\u001b[0m  0.6400\n",
      "      2        \u001b[36m1.4525\u001b[0m        1.2495  0.6019\n",
      "      2        1.0428        0.9535  0.6495\n",
      "      2        \u001b[36m1.2597\u001b[0m        1.0603  0.6492\n",
      "      2        \u001b[36m1.0281\u001b[0m        0.9245  0.6106\n",
      "      2        \u001b[36m1.2637\u001b[0m        1.0460  0.6537\n",
      "      2        \u001b[36m1.3361\u001b[0m        \u001b[32m1.1126\u001b[0m  0.6121\n",
      "      2        \u001b[36m1.4216\u001b[0m        1.0592  0.6443\n",
      "      3        1.4583        \u001b[32m1.2568\u001b[0m  0.5891\n",
      "      2        \u001b[36m1.4193\u001b[0m        \u001b[32m1.0658\u001b[0m  0.6460\n",
      "      3        \u001b[36m1.3370\u001b[0m        1.1105  0.6030\n",
      "      3        \u001b[36m1.4390\u001b[0m        1.2516  0.5986\n",
      "      3        \u001b[36m1.0307\u001b[0m        0.9559  0.6247\n",
      "      3        1.2625        1.0599  0.6372\n",
      "      3        \u001b[36m1.0198\u001b[0m        0.9251  0.6136\n",
      "      3        \u001b[36m1.2571\u001b[0m        1.0516  0.6440\n",
      "      3        \u001b[36m1.3271\u001b[0m        \u001b[32m1.0965\u001b[0m  0.6371\n",
      "      3        \u001b[36m1.3990\u001b[0m        1.0603  0.6257\n",
      "      4        \u001b[36m1.4521\u001b[0m        \u001b[32m1.2568\u001b[0m  0.6624\n",
      "      3        \u001b[36m1.4178\u001b[0m        \u001b[32m1.0632\u001b[0m  0.6164\n",
      "      4        \u001b[36m1.3293\u001b[0m        1.1112  0.6104\n",
      "      4        \u001b[36m1.4201\u001b[0m        1.2488  0.6113\n",
      "      4        1.2621        1.0593  0.6328\n",
      "      4        \u001b[36m1.0303\u001b[0m        0.9528  0.6721\n",
      "      4        \u001b[36m1.0159\u001b[0m        0.9195  0.6332\n",
      "      4        \u001b[36m1.2513\u001b[0m        1.0578  0.6515\n",
      "      4        1.3328        1.1030  0.6799\n",
      "      4        \u001b[36m1.3854\u001b[0m        1.0665  0.6890\n",
      "      5        \u001b[36m1.4507\u001b[0m        1.2589  0.6387\n",
      "      5        \u001b[36m1.3248\u001b[0m        1.1105  0.6550\n",
      "      4        \u001b[36m1.4064\u001b[0m        \u001b[32m1.0595\u001b[0m  0.6757\n",
      "      5        \u001b[36m1.4098\u001b[0m        \u001b[32m1.2407\u001b[0m  0.6806\n",
      "      5        \u001b[36m1.0164\u001b[0m        \u001b[32m0.9456\u001b[0m  0.6473\n",
      "      5        \u001b[36m1.2579\u001b[0m        1.0617  0.6930\n",
      "      5        \u001b[36m1.0016\u001b[0m        0.9261  0.6060\n",
      "      5        \u001b[36m1.2415\u001b[0m        1.0485  0.6095\n",
      "      5        \u001b[36m1.3706\u001b[0m        1.0681  0.6061\n",
      "      5        \u001b[36m1.3077\u001b[0m        1.1012  0.6433\n",
      "      6        \u001b[36m1.4433\u001b[0m        1.2613  0.6035\n",
      "      6        \u001b[36m1.3848\u001b[0m        \u001b[32m1.2373\u001b[0m  0.5670\n",
      "      5        1.4133        \u001b[32m1.0566\u001b[0m  0.6387\n",
      "      6        1.3293        1.1113  0.6736\n",
      "      6        \u001b[36m1.0156\u001b[0m        \u001b[32m0.9384\u001b[0m  0.6357\n",
      "      6        \u001b[36m1.2464\u001b[0m        1.0612  0.6421\n",
      "      6        \u001b[36m0.9822\u001b[0m        0.9319  0.6038\n",
      "      6        \u001b[36m1.2233\u001b[0m        \u001b[32m1.0389\u001b[0m  0.6267\n",
      "      6        \u001b[36m1.3623\u001b[0m        1.0598  0.6529\n",
      "      6        1.3108        1.1104  0.7053\n",
      "      7        \u001b[36m1.4389\u001b[0m        1.2578  0.6537\n",
      "      7        \u001b[36m1.3741\u001b[0m        1.2378  0.6333\n",
      "      6        \u001b[36m1.4057\u001b[0m        \u001b[32m1.0558\u001b[0m  0.6673\n",
      "      7        1.3262        1.1120  0.6833\n",
      "      7        1.2470        1.0595  0.6195\n",
      "      7        1.0234        \u001b[32m0.9317\u001b[0m  0.6395\n",
      "      7        \u001b[36m0.9778\u001b[0m        0.9257  0.6192\n",
      "      7        \u001b[36m1.2195\u001b[0m        \u001b[32m1.0340\u001b[0m  0.6432\n",
      "      7        1.3716        \u001b[32m1.0574\u001b[0m  0.6152\n",
      "      8        \u001b[36m1.4343\u001b[0m        \u001b[32m1.2561\u001b[0m  0.6195\n",
      "      8        \u001b[36m1.3685\u001b[0m        1.2400  0.5667\n",
      "      7        1.4115        \u001b[32m1.0552\u001b[0m  0.5477\n",
      "      7        1.3109        1.1150  0.6850\n",
      "      8        \u001b[36m1.3247\u001b[0m        1.1109  0.5653\n",
      "      8        \u001b[36m1.2436\u001b[0m        1.0596  0.6416\n",
      "      8        \u001b[36m0.9774\u001b[0m        0.9206  0.6557\n",
      "      8        \u001b[36m1.0105\u001b[0m        \u001b[32m0.9249\u001b[0m  0.7246\n",
      "      8        1.2294        1.0408  0.5909\n",
      "      9        \u001b[36m1.3357\u001b[0m        1.2445  0.5348\n",
      "      9        \u001b[36m1.4334\u001b[0m        \u001b[32m1.2515\u001b[0m  0.6016\n",
      "      8        \u001b[36m1.3408\u001b[0m        1.0579  0.7430\n",
      "      8        \u001b[36m1.3913\u001b[0m        1.0568  0.6077\n",
      "      9        \u001b[36m1.3148\u001b[0m        1.1120  0.6134\n",
      "      8        \u001b[36m1.2936\u001b[0m        1.1092  0.6985\n",
      "      9        \u001b[36m0.9955\u001b[0m        \u001b[32m0.9206\u001b[0m  0.5714\n",
      "      9        \u001b[36m1.1968\u001b[0m        1.0463  0.5411\n",
      "      9        \u001b[36m1.2340\u001b[0m        1.0622  0.6814\n",
      "      9        \u001b[36m0.9721\u001b[0m        0.9295  0.7214\n",
      "     10        1.3556        1.2473  0.6254\n",
      "Restoring best model from epoch 6.\n",
      "      9        \u001b[36m1.3375\u001b[0m        \u001b[32m1.0561\u001b[0m  0.5936\n",
      "     10        \u001b[36m1.3143\u001b[0m        1.1122  0.5934\n",
      "      9        1.3037        1.1062  0.5479\n",
      "Restoring best model from epoch 2.\n",
      "      9        1.4069        1.0606  0.6870\n",
      "     10        \u001b[36m1.4233\u001b[0m        \u001b[32m1.2493\u001b[0m  0.7206\n",
      "     10        \u001b[36m1.1841\u001b[0m        1.0430  0.6088\n",
      "Restoring best model from epoch 7.\n",
      "     10        1.2391        1.0618  0.6132\n",
      "Restoring best model from epoch 1.\n",
      "     10        0.9988        \u001b[32m0.9181\u001b[0m  0.6499\n",
      "     10        0.9752        0.9238  0.5946\n",
      "Restoring best model from epoch 1.\n",
      "     10        \u001b[36m1.3258\u001b[0m        \u001b[32m1.0517\u001b[0m  0.5663\n",
      "     10        \u001b[36m1.2893\u001b[0m        1.1107  0.5445\n",
      "Restoring best model from epoch 3.\n",
      "     10        \u001b[36m1.3738\u001b[0m        1.0603  0.5443\n",
      "Restoring best model from epoch 7.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m1.2031\u001b[0m        \u001b[32m1.3997\u001b[0m  0.2971\n",
      "      2        \u001b[36m1.1929\u001b[0m        1.4001  0.2569\n",
      "      3        1.1937        1.4100  0.2334\n",
      "      4        1.1982        1.4063  0.2296\n",
      "      5        1.1967        1.4057  0.2517\n",
      "      6        \u001b[36m1.1853\u001b[0m        1.4111  0.2426\n",
      "      7        1.1862        1.4147  0.2320\n",
      "      8        \u001b[36m1.1839\u001b[0m        1.4136  0.2520\n",
      "      9        \u001b[36m1.1784\u001b[0m        1.4110  0.2551\n",
      "     10        \u001b[36m1.1780\u001b[0m        1.4098  0.2265\n",
      "Restoring best model from epoch 1.\n",
      "y_train breslow final [-1.000e+00 -1.000e+00 -5.000e+00 -5.000e+00 -7.000e+00 -9.000e+00\n",
      " -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01\n",
      " -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01\n",
      " -1.100e+01 -1.600e+01 -1.900e+01 -2.100e+01 -2.400e+01 -2.600e+01\n",
      " -3.000e+01 -3.000e+01 -3.000e+01 -3.000e+01 -3.100e+01 -3.100e+01\n",
      " -3.100e+01 -3.100e+01 -3.400e+01 -3.400e+01 -4.900e+01 -5.100e+01\n",
      " -5.200e+01 -5.900e+01 -6.400e+01 -7.000e+01 -7.200e+01 -7.600e+01\n",
      " -7.800e+01 -7.800e+01 -7.900e+01 -8.000e+01 -9.000e+01 -9.200e+01\n",
      " -9.800e+01 -1.180e+02 -1.340e+02 -1.490e+02  1.580e+02 -1.600e+02\n",
      "  1.600e+02 -1.620e+02 -1.630e+02  1.720e+02 -1.720e+02  1.740e+02\n",
      " -1.780e+02 -1.870e+02 -1.960e+02  1.970e+02 -2.000e+02 -2.100e+02\n",
      " -2.130e+02 -2.150e+02 -2.170e+02 -2.180e+02 -2.220e+02 -2.240e+02\n",
      "  2.240e+02 -2.250e+02  2.270e+02 -2.270e+02 -2.310e+02 -2.420e+02\n",
      " -2.430e+02 -2.500e+02  2.550e+02 -2.580e+02 -2.590e+02 -2.660e+02\n",
      "  2.660e+02 -2.710e+02 -2.730e+02 -2.750e+02 -2.850e+02 -2.870e+02\n",
      " -2.880e+02 -2.930e+02  2.950e+02 -2.970e+02 -3.000e+02 -3.020e+02\n",
      "  3.020e+02 -3.030e+02 -3.030e+02 -3.040e+02 -3.040e+02 -3.040e+02\n",
      " -3.040e+02 -3.070e+02 -3.130e+02 -3.170e+02 -3.170e+02  3.200e+02\n",
      " -3.200e+02  3.220e+02 -3.220e+02 -3.230e+02 -3.260e+02 -3.280e+02\n",
      " -3.320e+02 -3.340e+02  3.360e+02 -3.360e+02 -3.370e+02 -3.390e+02\n",
      " -3.430e+02 -3.430e+02 -3.450e+02 -3.450e+02  3.480e+02 -3.480e+02\n",
      " -3.500e+02 -3.580e+02 -3.580e+02 -3.600e+02  3.620e+02 -3.630e+02\n",
      " -3.640e+02 -3.650e+02 -3.650e+02 -3.650e+02  3.650e+02 -3.650e+02\n",
      " -3.660e+02 -3.680e+02 -3.700e+02 -3.710e+02 -3.710e+02 -3.730e+02\n",
      " -3.730e+02 -3.750e+02 -3.750e+02 -3.750e+02 -3.750e+02 -3.760e+02\n",
      " -3.760e+02 -3.790e+02 -3.800e+02 -3.800e+02 -3.820e+02 -3.830e+02\n",
      " -3.830e+02  3.850e+02 -3.850e+02  3.850e+02 -3.850e+02 -3.850e+02\n",
      " -3.930e+02 -3.930e+02 -3.940e+02 -3.940e+02 -3.950e+02 -3.960e+02\n",
      " -3.960e+02 -3.960e+02 -3.970e+02 -3.980e+02 -4.030e+02 -4.030e+02\n",
      " -4.040e+02 -4.050e+02 -4.080e+02 -4.080e+02 -4.110e+02 -4.120e+02\n",
      " -4.130e+02 -4.140e+02 -4.160e+02 -4.180e+02 -4.210e+02 -4.240e+02\n",
      " -4.250e+02 -4.260e+02  4.260e+02 -4.260e+02 -4.280e+02 -4.280e+02\n",
      " -4.310e+02 -4.310e+02 -4.330e+02 -4.340e+02 -4.350e+02 -4.370e+02\n",
      " -4.390e+02 -4.390e+02 -4.390e+02 -4.400e+02 -4.410e+02 -4.410e+02\n",
      " -4.410e+02 -4.460e+02  4.460e+02 -4.470e+02 -4.470e+02 -4.470e+02\n",
      " -4.480e+02 -4.480e+02 -4.500e+02 -4.510e+02 -4.540e+02 -4.550e+02\n",
      " -4.560e+02 -4.570e+02 -4.580e+02 -4.610e+02 -4.630e+02 -4.660e+02\n",
      " -4.670e+02 -4.700e+02 -4.710e+02 -4.720e+02 -4.750e+02 -4.760e+02\n",
      " -4.770e+02 -4.770e+02 -4.770e+02 -4.800e+02 -4.820e+02 -4.880e+02\n",
      " -4.880e+02 -4.890e+02 -4.920e+02 -4.950e+02 -4.960e+02 -4.960e+02\n",
      " -4.990e+02 -4.990e+02 -5.010e+02 -5.010e+02 -5.020e+02 -5.040e+02\n",
      " -5.060e+02 -5.080e+02 -5.090e+02 -5.100e+02 -5.130e+02 -5.160e+02\n",
      " -5.180e+02 -5.180e+02 -5.180e+02 -5.190e+02 -5.190e+02 -5.210e+02\n",
      " -5.220e+02 -5.230e+02  5.240e+02 -5.260e+02 -5.280e+02 -5.280e+02\n",
      " -5.290e+02 -5.300e+02 -5.320e+02 -5.320e+02 -5.330e+02 -5.380e+02\n",
      " -5.380e+02  5.380e+02 -5.470e+02  5.480e+02 -5.490e+02 -5.510e+02\n",
      " -5.520e+02 -5.520e+02 -5.540e+02 -5.540e+02  5.580e+02 -5.620e+02\n",
      " -5.630e+02  5.630e+02 -5.650e+02 -5.670e+02 -5.680e+02  5.710e+02\n",
      " -5.720e+02  5.730e+02 -5.740e+02 -5.750e+02 -5.750e+02 -5.760e+02\n",
      " -5.770e+02 -5.790e+02 -5.790e+02 -5.840e+02  5.840e+02 -5.840e+02\n",
      " -5.850e+02 -5.860e+02 -5.880e+02 -5.880e+02 -5.900e+02 -5.910e+02\n",
      " -5.940e+02 -5.950e+02 -5.950e+02 -5.960e+02 -5.980e+02 -6.020e+02\n",
      " -6.060e+02 -6.070e+02 -6.080e+02 -6.110e+02 -6.110e+02 -6.120e+02\n",
      "  6.120e+02 -6.120e+02 -6.120e+02  6.140e+02  6.160e+02 -6.160e+02\n",
      " -6.180e+02 -6.200e+02 -6.200e+02 -6.200e+02 -6.220e+02 -6.220e+02\n",
      " -6.240e+02 -6.260e+02 -6.260e+02 -6.290e+02 -6.300e+02 -6.310e+02\n",
      " -6.350e+02 -6.350e+02 -6.390e+02  6.390e+02 -6.400e+02 -6.430e+02\n",
      " -6.430e+02 -6.440e+02 -6.480e+02 -6.520e+02 -6.580e+02 -6.590e+02\n",
      " -6.590e+02 -6.600e+02 -6.610e+02 -6.620e+02 -6.640e+02 -6.720e+02\n",
      " -6.750e+02 -6.770e+02  6.780e+02 -6.790e+02 -6.800e+02 -6.810e+02\n",
      " -6.940e+02 -6.980e+02 -7.010e+02 -7.020e+02 -7.060e+02 -7.070e+02\n",
      " -7.070e+02 -7.090e+02 -7.100e+02 -7.140e+02 -7.140e+02 -7.150e+02\n",
      " -7.150e+02  7.230e+02 -7.240e+02 -7.250e+02 -7.260e+02 -7.270e+02\n",
      " -7.270e+02 -7.270e+02 -7.280e+02 -7.380e+02 -7.420e+02 -7.450e+02\n",
      " -7.470e+02 -7.470e+02 -7.520e+02 -7.520e+02  7.540e+02 -7.540e+02\n",
      " -7.540e+02 -7.550e+02 -7.590e+02 -7.590e+02 -7.600e+02 -7.600e+02\n",
      " -7.610e+02 -7.620e+02 -7.620e+02 -7.620e+02 -7.650e+02 -7.670e+02\n",
      " -7.690e+02 -7.770e+02 -7.830e+02 -7.850e+02  7.850e+02  7.860e+02\n",
      " -7.890e+02 -7.910e+02  7.920e+02  7.920e+02 -7.920e+02 -8.030e+02\n",
      " -8.060e+02  8.110e+02 -8.120e+02 -8.190e+02  8.210e+02 -8.220e+02\n",
      "  8.250e+02 -8.290e+02 -8.370e+02 -8.390e+02 -8.470e+02 -8.490e+02\n",
      " -8.520e+02 -8.580e+02  8.600e+02 -8.650e+02 -8.670e+02 -8.750e+02\n",
      " -8.780e+02 -8.810e+02  8.830e+02 -8.830e+02 -8.890e+02 -8.900e+02\n",
      "  9.040e+02 -9.060e+02 -9.070e+02 -9.110e+02  9.120e+02 -9.120e+02\n",
      " -9.120e+02 -9.150e+02  9.210e+02 -9.230e+02 -9.260e+02 -9.310e+02\n",
      " -9.310e+02 -9.420e+02 -9.430e+02  9.430e+02 -9.430e+02 -9.520e+02\n",
      " -9.540e+02 -9.650e+02 -9.650e+02 -9.680e+02 -9.720e+02 -9.730e+02\n",
      " -9.740e+02 -9.740e+02 -9.750e+02 -9.750e+02  9.760e+02 -9.890e+02\n",
      "  9.910e+02  9.910e+02 -9.960e+02 -9.960e+02 -1.000e+03 -1.001e+03\n",
      " -1.001e+03 -1.001e+03  1.004e+03 -1.004e+03 -1.004e+03 -1.005e+03\n",
      " -1.006e+03 -1.007e+03  1.009e+03 -1.009e+03 -1.015e+03 -1.025e+03\n",
      " -1.026e+03 -1.026e+03 -1.027e+03  1.032e+03  1.034e+03  1.034e+03\n",
      " -1.035e+03 -1.039e+03 -1.043e+03 -1.043e+03 -1.047e+03  1.048e+03\n",
      " -1.051e+03 -1.051e+03 -1.059e+03 -1.062e+03 -1.063e+03 -1.066e+03\n",
      "  1.072e+03 -1.074e+03 -1.079e+03 -1.085e+03 -1.088e+03 -1.093e+03\n",
      "  1.093e+03 -1.101e+03  1.104e+03 -1.106e+03 -1.120e+03 -1.121e+03\n",
      " -1.124e+03  1.127e+03 -1.132e+03 -1.133e+03 -1.138e+03 -1.139e+03\n",
      " -1.140e+03 -1.141e+03  1.142e+03 -1.148e+03 -1.150e+03 -1.156e+03\n",
      " -1.156e+03 -1.157e+03 -1.161e+03 -1.162e+03 -1.163e+03 -1.165e+03\n",
      " -1.167e+03 -1.174e+03 -1.179e+03 -1.185e+03 -1.186e+03 -1.189e+03\n",
      " -1.191e+03 -1.196e+03 -1.198e+03 -1.203e+03 -1.206e+03 -1.208e+03\n",
      " -1.210e+03 -1.217e+03 -1.220e+03 -1.220e+03 -1.229e+03 -1.229e+03\n",
      " -1.232e+03 -1.239e+03 -1.246e+03 -1.247e+03 -1.248e+03 -1.251e+03\n",
      " -1.269e+03  1.275e+03 -1.277e+03 -1.285e+03  1.286e+03 -1.288e+03\n",
      " -1.291e+03 -1.301e+03 -1.308e+03 -1.308e+03 -1.308e+03 -1.309e+03\n",
      " -1.309e+03 -1.318e+03 -1.321e+03  1.324e+03 -1.326e+03 -1.330e+03\n",
      " -1.347e+03 -1.351e+03 -1.359e+03 -1.363e+03  1.365e+03 -1.371e+03\n",
      " -1.371e+03  1.388e+03 -1.417e+03 -1.417e+03 -1.419e+03 -1.434e+03\n",
      " -1.437e+03  1.439e+03 -1.448e+03 -1.461e+03 -1.463e+03 -1.471e+03\n",
      " -1.474e+03 -1.474e+03 -1.476e+03 -1.477e+03 -1.492e+03  1.508e+03\n",
      " -1.519e+03 -1.522e+03 -1.523e+03 -1.528e+03 -1.532e+03 -1.534e+03\n",
      " -1.542e+03 -1.545e+03 -1.545e+03 -1.546e+03 -1.547e+03 -1.548e+03\n",
      " -1.550e+03  1.556e+03 -1.561e+03 -1.563e+03 -1.569e+03 -1.572e+03\n",
      " -1.587e+03 -1.596e+03 -1.604e+03 -1.604e+03 -1.611e+03 -1.611e+03\n",
      " -1.611e+03 -1.612e+03 -1.613e+03 -1.616e+03 -1.620e+03 -1.631e+03\n",
      " -1.639e+03 -1.640e+03  1.642e+03 -1.642e+03 -1.644e+03 -1.648e+03\n",
      "  1.649e+03 -1.662e+03 -1.673e+03 -1.673e+03  1.673e+03 -1.682e+03\n",
      " -1.683e+03 -1.686e+03 -1.688e+03  1.692e+03 -1.692e+03  1.699e+03\n",
      " -1.712e+03 -1.728e+03 -1.728e+03 -1.732e+03 -1.742e+03  1.759e+03\n",
      " -1.780e+03  1.781e+03 -1.783e+03 -1.800e+03  1.812e+03 -1.820e+03\n",
      " -1.836e+03 -1.842e+03 -1.847e+03 -1.853e+03 -1.855e+03 -1.864e+03\n",
      " -1.866e+03 -1.871e+03 -1.873e+03 -1.876e+03 -1.882e+03 -1.887e+03\n",
      "  1.900e+03 -1.914e+03 -1.919e+03 -1.925e+03 -1.928e+03 -1.935e+03\n",
      " -1.953e+03 -1.980e+03  1.993e+03 -1.996e+03  2.009e+03 -2.012e+03\n",
      " -2.019e+03 -2.031e+03 -2.033e+03 -2.041e+03 -2.064e+03 -2.072e+03\n",
      " -2.082e+03 -2.091e+03 -2.108e+03 -2.109e+03 -2.124e+03  2.127e+03\n",
      " -2.128e+03 -2.136e+03 -2.140e+03 -2.142e+03 -2.155e+03 -2.156e+03\n",
      " -2.161e+03 -2.184e+03 -2.190e+03 -2.190e+03 -2.191e+03  2.192e+03\n",
      " -2.193e+03 -2.197e+03  2.207e+03 -2.222e+03 -2.231e+03 -2.240e+03\n",
      " -2.246e+03 -2.248e+03 -2.255e+03 -2.263e+03 -2.278e+03 -2.288e+03\n",
      " -2.289e+03  2.296e+03 -2.306e+03 -2.311e+03 -2.329e+03 -2.330e+03\n",
      " -2.335e+03  2.348e+03  2.361e+03 -2.362e+03 -2.365e+03 -2.371e+03\n",
      " -2.372e+03 -2.385e+03 -2.403e+03 -2.426e+03 -2.442e+03  2.469e+03\n",
      " -2.477e+03  2.483e+03 -2.513e+03 -2.515e+03  2.520e+03  2.534e+03\n",
      " -2.535e+03 -2.541e+03  2.551e+03 -2.558e+03 -2.559e+03  2.573e+03\n",
      " -2.596e+03 -2.596e+03 -2.605e+03 -2.612e+03 -2.618e+03 -2.629e+03\n",
      " -2.632e+03 -2.632e+03  2.636e+03 -2.645e+03 -2.645e+03 -2.650e+03\n",
      " -2.653e+03 -2.654e+03 -2.695e+03 -2.709e+03  2.712e+03 -2.721e+03\n",
      " -2.759e+03  2.763e+03 -2.767e+03 -2.770e+03  2.798e+03 -2.813e+03\n",
      " -2.838e+03  2.854e+03  2.866e+03 -2.868e+03  2.911e+03 -2.920e+03\n",
      "  2.965e+03  2.965e+03 -2.971e+03 -2.976e+03 -2.991e+03 -3.001e+03\n",
      " -3.004e+03 -3.009e+03 -3.011e+03 -3.015e+03 -3.021e+03 -3.022e+03\n",
      " -3.030e+03 -3.035e+03  3.063e+03 -3.072e+03 -3.091e+03 -3.094e+03\n",
      " -3.102e+03 -3.112e+03 -3.128e+03 -3.152e+03 -3.159e+03 -3.172e+03\n",
      " -3.202e+03 -3.203e+03 -3.204e+03 -3.226e+03 -3.247e+03 -3.248e+03\n",
      " -3.256e+03 -3.261e+03  3.262e+03 -3.276e+03 -3.283e+03 -3.283e+03\n",
      " -3.286e+03 -3.287e+03 -3.296e+03 -3.307e+03 -3.342e+03 -3.361e+03\n",
      " -3.364e+03  3.409e+03 -3.430e+03 -3.456e+03  3.461e+03  3.462e+03\n",
      "  3.472e+03  3.492e+03 -3.519e+03 -3.603e+03 -3.660e+03  3.669e+03\n",
      " -3.709e+03  3.736e+03 -3.807e+03  3.873e+03  3.926e+03  3.945e+03\n",
      " -3.957e+03  3.959e+03 -4.005e+03 -4.047e+03 -4.052e+03 -4.080e+03\n",
      " -4.088e+03 -4.159e+03 -4.275e+03 -4.285e+03 -4.361e+03  4.456e+03\n",
      " -4.929e+03 -5.042e+03 -5.062e+03 -5.156e+03 -5.739e+03  6.456e+03\n",
      "  6.593e+03 -7.106e+03  7.455e+03 -8.008e+03 -8.391e+03 -8.556e+03]\n",
      "Concordance Index 0.4547549077230516\n",
      "Integrated Brier Score: 0.27593472780663797\n",
      "y_train breslow final [-1.000e+00 -1.000e+00 -5.000e+00 -5.000e+00 -7.000e+00 -9.000e+00\n",
      " -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01\n",
      " -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01\n",
      " -1.100e+01 -1.600e+01 -1.900e+01 -2.100e+01 -2.400e+01 -2.600e+01\n",
      " -3.000e+01 -3.000e+01 -3.000e+01 -3.000e+01 -3.100e+01 -3.100e+01\n",
      " -3.100e+01 -3.100e+01 -3.400e+01 -3.400e+01 -4.900e+01 -5.100e+01\n",
      " -5.200e+01 -5.900e+01 -6.400e+01 -7.000e+01 -7.200e+01 -7.600e+01\n",
      " -7.800e+01 -7.800e+01 -7.900e+01 -8.000e+01 -9.000e+01 -9.200e+01\n",
      " -9.800e+01 -1.180e+02 -1.340e+02 -1.490e+02  1.580e+02 -1.600e+02\n",
      "  1.600e+02 -1.620e+02 -1.630e+02  1.720e+02 -1.720e+02  1.740e+02\n",
      " -1.780e+02 -1.870e+02 -1.960e+02  1.970e+02 -2.000e+02 -2.100e+02\n",
      " -2.130e+02 -2.150e+02 -2.170e+02 -2.180e+02 -2.220e+02 -2.240e+02\n",
      "  2.240e+02 -2.250e+02  2.270e+02 -2.270e+02 -2.310e+02 -2.420e+02\n",
      " -2.430e+02 -2.500e+02  2.550e+02 -2.580e+02 -2.590e+02 -2.660e+02\n",
      "  2.660e+02 -2.710e+02 -2.730e+02 -2.750e+02 -2.850e+02 -2.870e+02\n",
      " -2.880e+02 -2.930e+02  2.950e+02 -2.970e+02 -3.000e+02 -3.020e+02\n",
      "  3.020e+02 -3.030e+02 -3.030e+02 -3.040e+02 -3.040e+02 -3.040e+02\n",
      " -3.040e+02 -3.070e+02 -3.130e+02 -3.170e+02 -3.170e+02  3.200e+02\n",
      " -3.200e+02  3.220e+02 -3.220e+02 -3.230e+02 -3.260e+02 -3.280e+02\n",
      " -3.320e+02 -3.340e+02  3.360e+02 -3.360e+02 -3.370e+02 -3.390e+02\n",
      " -3.430e+02 -3.430e+02 -3.450e+02 -3.450e+02  3.480e+02 -3.480e+02\n",
      " -3.500e+02 -3.580e+02 -3.580e+02 -3.600e+02  3.620e+02 -3.630e+02\n",
      " -3.640e+02 -3.650e+02 -3.650e+02 -3.650e+02  3.650e+02 -3.650e+02\n",
      " -3.660e+02 -3.680e+02 -3.700e+02 -3.710e+02 -3.710e+02 -3.730e+02\n",
      " -3.730e+02 -3.750e+02 -3.750e+02 -3.750e+02 -3.750e+02 -3.760e+02\n",
      " -3.760e+02 -3.790e+02 -3.800e+02 -3.800e+02 -3.820e+02 -3.830e+02\n",
      " -3.830e+02  3.850e+02 -3.850e+02  3.850e+02 -3.850e+02 -3.850e+02\n",
      " -3.930e+02 -3.930e+02 -3.940e+02 -3.940e+02 -3.950e+02 -3.960e+02\n",
      " -3.960e+02 -3.960e+02 -3.970e+02 -3.980e+02 -4.030e+02 -4.030e+02\n",
      " -4.040e+02 -4.050e+02 -4.080e+02 -4.080e+02 -4.110e+02 -4.120e+02\n",
      " -4.130e+02 -4.140e+02 -4.160e+02 -4.180e+02 -4.210e+02 -4.240e+02\n",
      " -4.250e+02 -4.260e+02  4.260e+02 -4.260e+02 -4.280e+02 -4.280e+02\n",
      " -4.310e+02 -4.310e+02 -4.330e+02 -4.340e+02 -4.350e+02 -4.370e+02\n",
      " -4.390e+02 -4.390e+02 -4.390e+02 -4.400e+02 -4.410e+02 -4.410e+02\n",
      " -4.410e+02 -4.460e+02  4.460e+02 -4.470e+02 -4.470e+02 -4.470e+02\n",
      " -4.480e+02 -4.480e+02 -4.500e+02 -4.510e+02 -4.540e+02 -4.550e+02\n",
      " -4.560e+02 -4.570e+02 -4.580e+02 -4.610e+02 -4.630e+02 -4.660e+02\n",
      " -4.670e+02 -4.700e+02 -4.710e+02 -4.720e+02 -4.750e+02 -4.760e+02\n",
      " -4.770e+02 -4.770e+02 -4.770e+02 -4.800e+02 -4.820e+02 -4.880e+02\n",
      " -4.880e+02 -4.890e+02 -4.920e+02 -4.950e+02 -4.960e+02 -4.960e+02\n",
      " -4.990e+02 -4.990e+02 -5.010e+02 -5.010e+02 -5.020e+02 -5.040e+02\n",
      " -5.060e+02 -5.080e+02 -5.090e+02 -5.100e+02 -5.130e+02 -5.160e+02\n",
      " -5.180e+02 -5.180e+02 -5.180e+02 -5.190e+02 -5.190e+02 -5.210e+02\n",
      " -5.220e+02 -5.230e+02  5.240e+02 -5.260e+02 -5.280e+02 -5.280e+02\n",
      " -5.290e+02 -5.300e+02 -5.320e+02 -5.320e+02 -5.330e+02 -5.380e+02\n",
      " -5.380e+02  5.380e+02 -5.470e+02  5.480e+02 -5.490e+02 -5.510e+02\n",
      " -5.520e+02 -5.520e+02 -5.540e+02 -5.540e+02  5.580e+02 -5.620e+02\n",
      " -5.630e+02  5.630e+02 -5.650e+02 -5.670e+02 -5.680e+02  5.710e+02\n",
      " -5.720e+02  5.730e+02 -5.740e+02 -5.750e+02 -5.750e+02 -5.760e+02\n",
      " -5.770e+02 -5.790e+02 -5.790e+02 -5.840e+02  5.840e+02 -5.840e+02\n",
      " -5.850e+02 -5.860e+02 -5.880e+02 -5.880e+02 -5.900e+02 -5.910e+02\n",
      " -5.940e+02 -5.950e+02 -5.950e+02 -5.960e+02 -5.980e+02 -6.020e+02\n",
      " -6.060e+02 -6.070e+02 -6.080e+02 -6.110e+02 -6.110e+02 -6.120e+02\n",
      "  6.120e+02 -6.120e+02 -6.120e+02  6.140e+02  6.160e+02 -6.160e+02\n",
      " -6.180e+02 -6.200e+02 -6.200e+02 -6.200e+02 -6.220e+02 -6.220e+02\n",
      " -6.240e+02 -6.260e+02 -6.260e+02 -6.290e+02 -6.300e+02 -6.310e+02\n",
      " -6.350e+02 -6.350e+02 -6.390e+02  6.390e+02 -6.400e+02 -6.430e+02\n",
      " -6.430e+02 -6.440e+02 -6.480e+02 -6.520e+02 -6.580e+02 -6.590e+02\n",
      " -6.590e+02 -6.600e+02 -6.610e+02 -6.620e+02 -6.640e+02 -6.720e+02\n",
      " -6.750e+02 -6.770e+02  6.780e+02 -6.790e+02 -6.800e+02 -6.810e+02\n",
      " -6.940e+02 -6.980e+02 -7.010e+02 -7.020e+02 -7.060e+02 -7.070e+02\n",
      " -7.070e+02 -7.090e+02 -7.100e+02 -7.140e+02 -7.140e+02 -7.150e+02\n",
      " -7.150e+02  7.230e+02 -7.240e+02 -7.250e+02 -7.260e+02 -7.270e+02\n",
      " -7.270e+02 -7.270e+02 -7.280e+02 -7.380e+02 -7.420e+02 -7.450e+02\n",
      " -7.470e+02 -7.470e+02 -7.520e+02 -7.520e+02  7.540e+02 -7.540e+02\n",
      " -7.540e+02 -7.550e+02 -7.590e+02 -7.590e+02 -7.600e+02 -7.600e+02\n",
      " -7.610e+02 -7.620e+02 -7.620e+02 -7.620e+02 -7.650e+02 -7.670e+02\n",
      " -7.690e+02 -7.770e+02 -7.830e+02 -7.850e+02  7.850e+02  7.860e+02\n",
      " -7.890e+02 -7.910e+02  7.920e+02  7.920e+02 -7.920e+02 -8.030e+02\n",
      " -8.060e+02  8.110e+02 -8.120e+02 -8.190e+02  8.210e+02 -8.220e+02\n",
      "  8.250e+02 -8.290e+02 -8.370e+02 -8.390e+02 -8.470e+02 -8.490e+02\n",
      " -8.520e+02 -8.580e+02  8.600e+02 -8.650e+02 -8.670e+02 -8.750e+02\n",
      " -8.780e+02 -8.810e+02  8.830e+02 -8.830e+02 -8.890e+02 -8.900e+02\n",
      "  9.040e+02 -9.060e+02 -9.070e+02 -9.110e+02  9.120e+02 -9.120e+02\n",
      " -9.120e+02 -9.150e+02  9.210e+02 -9.230e+02 -9.260e+02 -9.310e+02\n",
      " -9.310e+02 -9.420e+02 -9.430e+02  9.430e+02 -9.430e+02 -9.520e+02\n",
      " -9.540e+02 -9.650e+02 -9.650e+02 -9.680e+02 -9.720e+02 -9.730e+02\n",
      " -9.740e+02 -9.740e+02 -9.750e+02 -9.750e+02  9.760e+02 -9.890e+02\n",
      "  9.910e+02  9.910e+02 -9.960e+02 -9.960e+02 -1.000e+03 -1.001e+03\n",
      " -1.001e+03 -1.001e+03  1.004e+03 -1.004e+03 -1.004e+03 -1.005e+03\n",
      " -1.006e+03 -1.007e+03  1.009e+03 -1.009e+03 -1.015e+03 -1.025e+03\n",
      " -1.026e+03 -1.026e+03 -1.027e+03  1.032e+03  1.034e+03  1.034e+03\n",
      " -1.035e+03 -1.039e+03 -1.043e+03 -1.043e+03 -1.047e+03  1.048e+03\n",
      " -1.051e+03 -1.051e+03 -1.059e+03 -1.062e+03 -1.063e+03 -1.066e+03\n",
      "  1.072e+03 -1.074e+03 -1.079e+03 -1.085e+03 -1.088e+03 -1.093e+03\n",
      "  1.093e+03 -1.101e+03  1.104e+03 -1.106e+03 -1.120e+03 -1.121e+03\n",
      " -1.124e+03  1.127e+03 -1.132e+03 -1.133e+03 -1.138e+03 -1.139e+03\n",
      " -1.140e+03 -1.141e+03  1.142e+03 -1.148e+03 -1.150e+03 -1.156e+03\n",
      " -1.156e+03 -1.157e+03 -1.161e+03 -1.162e+03 -1.163e+03 -1.165e+03\n",
      " -1.167e+03 -1.174e+03 -1.179e+03 -1.185e+03 -1.186e+03 -1.189e+03\n",
      " -1.191e+03 -1.196e+03 -1.198e+03 -1.203e+03 -1.206e+03 -1.208e+03\n",
      " -1.210e+03 -1.217e+03 -1.220e+03 -1.220e+03 -1.229e+03 -1.229e+03\n",
      " -1.232e+03 -1.239e+03 -1.246e+03 -1.247e+03 -1.248e+03 -1.251e+03\n",
      " -1.269e+03  1.275e+03 -1.277e+03 -1.285e+03  1.286e+03 -1.288e+03\n",
      " -1.291e+03 -1.301e+03 -1.308e+03 -1.308e+03 -1.308e+03 -1.309e+03\n",
      " -1.309e+03 -1.318e+03 -1.321e+03  1.324e+03 -1.326e+03 -1.330e+03\n",
      " -1.347e+03 -1.351e+03 -1.359e+03 -1.363e+03  1.365e+03 -1.371e+03\n",
      " -1.371e+03  1.388e+03 -1.417e+03 -1.417e+03 -1.419e+03 -1.434e+03\n",
      " -1.437e+03  1.439e+03 -1.448e+03 -1.461e+03 -1.463e+03 -1.471e+03\n",
      " -1.474e+03 -1.474e+03 -1.476e+03 -1.477e+03 -1.492e+03  1.508e+03\n",
      " -1.519e+03 -1.522e+03 -1.523e+03 -1.528e+03 -1.532e+03 -1.534e+03\n",
      " -1.542e+03 -1.545e+03 -1.545e+03 -1.546e+03 -1.547e+03 -1.548e+03\n",
      " -1.550e+03  1.556e+03 -1.561e+03 -1.563e+03 -1.569e+03 -1.572e+03\n",
      " -1.587e+03 -1.596e+03 -1.604e+03 -1.604e+03 -1.611e+03 -1.611e+03\n",
      " -1.611e+03 -1.612e+03 -1.613e+03 -1.616e+03 -1.620e+03 -1.631e+03\n",
      " -1.639e+03 -1.640e+03  1.642e+03 -1.642e+03 -1.644e+03 -1.648e+03\n",
      "  1.649e+03 -1.662e+03 -1.673e+03 -1.673e+03  1.673e+03 -1.682e+03\n",
      " -1.683e+03 -1.686e+03 -1.688e+03  1.692e+03 -1.692e+03  1.699e+03\n",
      " -1.712e+03 -1.728e+03 -1.728e+03 -1.732e+03 -1.742e+03  1.759e+03\n",
      " -1.780e+03  1.781e+03 -1.783e+03 -1.800e+03  1.812e+03 -1.820e+03\n",
      " -1.836e+03 -1.842e+03 -1.847e+03 -1.853e+03 -1.855e+03 -1.864e+03\n",
      " -1.866e+03 -1.871e+03 -1.873e+03 -1.876e+03 -1.882e+03 -1.887e+03\n",
      "  1.900e+03 -1.914e+03 -1.919e+03 -1.925e+03 -1.928e+03 -1.935e+03\n",
      " -1.953e+03 -1.980e+03  1.993e+03 -1.996e+03  2.009e+03 -2.012e+03\n",
      " -2.019e+03 -2.031e+03 -2.033e+03 -2.041e+03 -2.064e+03 -2.072e+03\n",
      " -2.082e+03 -2.091e+03 -2.108e+03 -2.109e+03 -2.124e+03  2.127e+03\n",
      " -2.128e+03 -2.136e+03 -2.140e+03 -2.142e+03 -2.155e+03 -2.156e+03\n",
      " -2.161e+03 -2.184e+03 -2.190e+03 -2.190e+03 -2.191e+03  2.192e+03\n",
      " -2.193e+03 -2.197e+03  2.207e+03 -2.222e+03 -2.231e+03 -2.240e+03\n",
      " -2.246e+03 -2.248e+03 -2.255e+03 -2.263e+03 -2.278e+03 -2.288e+03\n",
      " -2.289e+03  2.296e+03 -2.306e+03 -2.311e+03 -2.329e+03 -2.330e+03\n",
      " -2.335e+03  2.348e+03  2.361e+03 -2.362e+03 -2.365e+03 -2.371e+03\n",
      " -2.372e+03 -2.385e+03 -2.403e+03 -2.426e+03 -2.442e+03  2.469e+03\n",
      " -2.477e+03  2.483e+03 -2.513e+03 -2.515e+03  2.520e+03  2.534e+03\n",
      " -2.535e+03 -2.541e+03  2.551e+03 -2.558e+03 -2.559e+03  2.573e+03\n",
      " -2.596e+03 -2.596e+03 -2.605e+03 -2.612e+03 -2.618e+03 -2.629e+03\n",
      " -2.632e+03 -2.632e+03  2.636e+03 -2.645e+03 -2.645e+03 -2.650e+03\n",
      " -2.653e+03 -2.654e+03 -2.695e+03 -2.709e+03  2.712e+03 -2.721e+03\n",
      " -2.759e+03  2.763e+03 -2.767e+03 -2.770e+03  2.798e+03 -2.813e+03\n",
      " -2.838e+03  2.854e+03  2.866e+03 -2.868e+03  2.911e+03 -2.920e+03\n",
      "  2.965e+03  2.965e+03 -2.971e+03 -2.976e+03 -2.991e+03 -3.001e+03\n",
      " -3.004e+03 -3.009e+03 -3.011e+03 -3.015e+03 -3.021e+03 -3.022e+03\n",
      " -3.030e+03 -3.035e+03  3.063e+03 -3.072e+03 -3.091e+03 -3.094e+03\n",
      " -3.102e+03 -3.112e+03 -3.128e+03 -3.152e+03 -3.159e+03 -3.172e+03\n",
      " -3.202e+03 -3.203e+03 -3.204e+03 -3.226e+03 -3.247e+03 -3.248e+03\n",
      " -3.256e+03 -3.261e+03  3.262e+03 -3.276e+03 -3.283e+03 -3.283e+03\n",
      " -3.286e+03 -3.287e+03 -3.296e+03 -3.307e+03 -3.342e+03 -3.361e+03\n",
      " -3.364e+03  3.409e+03 -3.430e+03 -3.456e+03  3.461e+03  3.462e+03\n",
      "  3.472e+03  3.492e+03 -3.519e+03 -3.603e+03 -3.660e+03  3.669e+03\n",
      " -3.709e+03  3.736e+03 -3.807e+03  3.873e+03  3.926e+03  3.945e+03\n",
      " -3.957e+03  3.959e+03 -4.005e+03 -4.047e+03 -4.052e+03 -4.080e+03\n",
      " -4.088e+03 -4.159e+03 -4.275e+03 -4.285e+03 -4.361e+03  4.456e+03\n",
      " -4.929e+03 -5.042e+03 -5.062e+03 -5.156e+03 -5.739e+03  6.456e+03\n",
      "  6.593e+03 -7.106e+03  7.455e+03 -8.008e+03 -8.391e+03 -8.556e+03]\n",
      "durations 5.0 8605.0\n",
      "Concordance Index 0.4852430555555556\n",
      "Integrated Brier Score: 0.20980220411857547\n",
      "(865, 20531) <class 'pandas.core.frame.DataFrame'>\n",
      "(865,) <class 'pandas.core.series.Series'>\n",
      "(216, 20531) <class 'pandas.core.frame.DataFrame'>\n",
      "(216,) <class 'pandas.core.series.Series'>\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m1.3717\u001b[0m        \u001b[32m1.7201\u001b[0m  0.6884\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m1.2430\u001b[0m        \u001b[32m1.2139\u001b[0m  0.7653\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m1.2253\u001b[0m        \u001b[32m0.8404\u001b[0m  0.7688\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m1.2744\u001b[0m        \u001b[32m1.5519\u001b[0m  0.6992\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m1.0819\u001b[0m        \u001b[32m0.7651\u001b[0m  0.7681\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m1.3735\u001b[0m        \u001b[32m1.7167\u001b[0m  0.6840\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m1.2298\u001b[0m        \u001b[32m0.8847\u001b[0m  0.7021\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m1.0849\u001b[0m        \u001b[32m0.7813\u001b[0m  0.8032\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m1.2363\u001b[0m        \u001b[32m1.2481\u001b[0m  0.8774\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m1.2779\u001b[0m        \u001b[32m1.5260\u001b[0m  1.0370\n",
      "      2        \u001b[36m1.2250\u001b[0m        1.2288  0.5946\n",
      "      2        \u001b[36m1.2726\u001b[0m        \u001b[32m1.5497\u001b[0m  0.5907\n",
      "      2        \u001b[36m1.2208\u001b[0m        0.8412  0.6325\n",
      "      2        1.3738        1.7202  0.6980\n",
      "      2        \u001b[36m1.3705\u001b[0m        1.7290  0.6244\n",
      "      2        \u001b[36m1.2209\u001b[0m        \u001b[32m0.8782\u001b[0m  0.5981      2        \u001b[36m1.0676\u001b[0m        \u001b[32m0.7610\u001b[0m  0.6643\n",
      "\n",
      "      2        \u001b[36m1.0661\u001b[0m        \u001b[32m0.7752\u001b[0m  0.5757\n",
      "      2        1.2394        1.2495  0.6989\n",
      "      2        \u001b[36m1.2709\u001b[0m        1.5337  0.6510\n",
      "      3        1.2881        \u001b[32m1.5442\u001b[0m  0.5973\n",
      "      3        1.2281        1.2460  0.6883\n",
      "      3        1.3751        \u001b[32m1.7138\u001b[0m  0.6510\n",
      "      3        \u001b[36m1.1993\u001b[0m        0.8496  0.7082\n",
      "      3        \u001b[36m1.3594\u001b[0m        1.7259  0.6768\n",
      "      3        \u001b[36m1.2196\u001b[0m        \u001b[32m0.8746\u001b[0m  0.6575\n",
      "      3        \u001b[36m1.0648\u001b[0m        \u001b[32m0.7749\u001b[0m  0.6603\n",
      "      3        1.0676        0.7628  0.7016\n",
      "      3        1.2409        1.2488  0.5889\n",
      "      3        \u001b[36m1.2701\u001b[0m        1.5333  0.6367\n",
      "      4        \u001b[36m1.2692\u001b[0m        \u001b[32m1.5332\u001b[0m  0.6191\n",
      "      4        \u001b[36m1.2002\u001b[0m        1.2543  0.6511\n",
      "      4        \u001b[36m1.3663\u001b[0m        \u001b[32m1.7117\u001b[0m  0.5893\n",
      "      4        \u001b[36m1.2128\u001b[0m        0.8767  0.5585\n",
      "      4        1.2027        0.8554  0.6435\n",
      "      4        \u001b[36m1.3521\u001b[0m        \u001b[32m1.7154\u001b[0m  0.6051\n",
      "      4        \u001b[36m1.0584\u001b[0m        0.7751  0.5967\n",
      "      4        \u001b[36m1.0446\u001b[0m        0.7637  0.6144\n",
      "      4        1.2405        1.2507  0.6511\n",
      "      4        \u001b[36m1.2579\u001b[0m        1.5276  0.6068\n",
      "      5        1.2693        \u001b[32m1.5247\u001b[0m  0.5709\n",
      "      5        \u001b[36m1.3462\u001b[0m        1.7158  0.6023\n",
      "      5        1.2208        0.8778  0.5778\n",
      "      5        \u001b[36m1.1982\u001b[0m        0.8613  0.6031\n",
      "      5        1.0594        0.7758  0.5804\n",
      "      5        \u001b[36m1.3413\u001b[0m        \u001b[32m1.7113\u001b[0m  0.6231\n",
      "      5        1.0500        \u001b[32m0.7586\u001b[0m  0.5863\n",
      "      5        1.2027        1.2535  0.7399\n",
      "      5        \u001b[36m1.2343\u001b[0m        1.2520  0.5110\n",
      "      5        \u001b[36m1.2504\u001b[0m        1.5272  0.5932\n",
      "      6        \u001b[36m1.2692\u001b[0m        1.5249  0.5060\n",
      "      6        1.3665        1.7209  0.5407\n",
      "      6        1.2228        0.8772  0.5492\n",
      "      6        \u001b[36m1.1886\u001b[0m        0.8640  0.5881\n",
      "      6        \u001b[36m1.3345\u001b[0m        1.7149  0.5817\n",
      "      6        \u001b[36m1.0456\u001b[0m        0.7770  0.5983\n",
      "      6        \u001b[36m1.0190\u001b[0m        \u001b[32m0.7556\u001b[0m  0.5965\n",
      "      6        1.2385        1.2525  0.5529\n",
      "      6        \u001b[36m1.1987\u001b[0m        1.2526  0.6100\n",
      "      6        \u001b[36m1.2281\u001b[0m        \u001b[32m1.5211\u001b[0m  0.6289\n",
      "      7        1.2754        1.5275  0.6029\n",
      "      7        1.3505        1.7260  0.5739\n",
      "      7        1.2137        0.8758  0.5735\n",
      "      7        1.0486        0.7767  0.5440\n",
      "      7        \u001b[36m1.1749\u001b[0m        0.8669  0.6313\n",
      "      7        1.0204        \u001b[32m0.7489\u001b[0m  0.5686\n",
      "      7        1.3432        1.7201  0.6396\n",
      "      7        1.2387        1.2516  0.6434\n",
      "      7        1.1990        1.2488  0.7605\n",
      "      8        1.2767        1.5308  0.5926\n",
      "      7        1.2428        \u001b[32m1.5158\u001b[0m  0.6659\n",
      "      8        1.3597        1.7290  0.6213\n",
      "      8        \u001b[36m1.0147\u001b[0m        \u001b[32m0.7454\u001b[0m  0.5477\n",
      "      8        1.2192        \u001b[32m0.8732\u001b[0m  0.7242\n",
      "      8        \u001b[36m1.1723\u001b[0m        0.8675  0.6724\n",
      "      8        \u001b[36m1.0247\u001b[0m        0.7772  0.7321\n",
      "      8        1.2413        1.2505  0.6192\n",
      "      8        \u001b[36m1.3176\u001b[0m        1.7175  0.7442\n",
      "      9        \u001b[36m1.2688\u001b[0m        1.5336  0.5774\n",
      "      8        \u001b[36m1.2077\u001b[0m        \u001b[32m1.5092\u001b[0m  0.6206\n",
      "      8        \u001b[36m1.1930\u001b[0m        1.2453  0.7807\n",
      "      9        1.2139        \u001b[32m0.8701\u001b[0m  0.5462\n",
      "      9        1.3511        1.7297  0.7087\n",
      "      9        \u001b[36m0.9923\u001b[0m        0.7538  0.6204\n",
      "      9        \u001b[36m1.0246\u001b[0m        0.7771  0.6056\n",
      "      9        1.1756        0.8639  0.6141\n",
      "      9        1.2390        1.2490  0.5672\n",
      "     10        \u001b[36m1.2432\u001b[0m        1.5354  0.5383\n",
      "Restoring best model from epoch 5.\n",
      "      9        1.3392        1.7132  0.6579\n",
      "      9        \u001b[36m1.1915\u001b[0m        1.2431  0.5804\n",
      "      9        1.2161        1.5117  0.6165\n",
      "     10        \u001b[36m1.2108\u001b[0m        \u001b[32m0.8687\u001b[0m  0.5421\n",
      "     10        \u001b[36m0.9914\u001b[0m        0.7545  0.5867\n",
      "Restoring best model from epoch 8.\n",
      "     10        \u001b[36m1.3328\u001b[0m        1.7303  0.6516\n",
      "Restoring best model from epoch 4.\n",
      "     10        \u001b[36m0.9983\u001b[0m        0.7767  0.5694\n",
      "Restoring best model from epoch 3.\n",
      "     10        \u001b[36m1.1633\u001b[0m        0.8524  0.6050\n",
      "Restoring best model from epoch 1.\n",
      "     10        \u001b[36m1.2302\u001b[0m        \u001b[32m1.2478\u001b[0m  0.6300\n",
      "     10        \u001b[36m1.3145\u001b[0m        \u001b[32m1.7112\u001b[0m  0.5226\n",
      "Restoring best model from epoch 5.\n",
      "     10        \u001b[36m1.1889\u001b[0m        1.2386  0.5021\n",
      "Restoring best model from epoch 1.\n",
      "     10        \u001b[36m1.1987\u001b[0m        1.5220  0.5558\n",
      "Restoring best model from epoch 8.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m1.2264\u001b[0m        \u001b[32m1.3547\u001b[0m  0.2026\n",
      "      2        \u001b[36m1.2255\u001b[0m        1.3589  0.1598\n",
      "      3        \u001b[36m1.2212\u001b[0m        1.3602  0.1586\n",
      "      4        \u001b[36m1.2110\u001b[0m        1.3592  0.1552\n",
      "      5        1.2170        1.3575  0.1594\n",
      "      6        \u001b[36m1.2097\u001b[0m        1.3566  0.1591\n",
      "      7        1.2239        1.3557  0.1591\n",
      "      8        1.2182        1.3549  0.1625\n",
      "      9        \u001b[36m1.2023\u001b[0m        1.3548  0.1558\n",
      "     10        1.2173        1.3550  0.1578\n",
      "Restoring best model from epoch 1.\n",
      "y_train breslow final [-1.000e+00 -1.000e+00 -5.000e+00 -5.000e+00 -8.000e+00 -9.000e+00\n",
      " -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01\n",
      " -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01 -1.100e+01\n",
      " -1.600e+01 -1.900e+01 -2.100e+01 -2.400e+01 -2.600e+01 -3.000e+01\n",
      " -3.000e+01 -3.100e+01 -3.100e+01 -3.400e+01 -4.000e+01 -5.100e+01\n",
      " -5.200e+01 -5.400e+01 -5.900e+01 -7.200e+01 -7.600e+01 -7.800e+01\n",
      " -7.800e+01 -7.800e+01 -7.900e+01 -8.000e+01 -8.400e+01 -9.800e+01\n",
      "  1.160e+02 -1.180e+02 -1.340e+02 -1.490e+02  1.580e+02 -1.600e+02\n",
      "  1.600e+02 -1.620e+02 -1.630e+02 -1.700e+02 -1.700e+02  1.720e+02\n",
      " -1.720e+02  1.740e+02 -1.780e+02 -1.860e+02 -1.870e+02 -1.960e+02\n",
      "  1.970e+02 -2.000e+02 -2.130e+02 -2.140e+02 -2.160e+02 -2.170e+02\n",
      " -2.180e+02 -2.220e+02 -2.240e+02  2.240e+02 -2.250e+02  2.270e+02\n",
      "  2.390e+02 -2.420e+02 -2.420e+02 -2.430e+02 -2.520e+02  2.550e+02\n",
      " -2.580e+02 -2.590e+02  2.660e+02 -2.710e+02 -2.730e+02 -2.730e+02\n",
      " -2.740e+02 -2.750e+02 -2.850e+02 -2.880e+02 -2.930e+02 -2.930e+02\n",
      " -2.970e+02 -3.020e+02  3.020e+02 -3.030e+02 -3.030e+02 -3.040e+02\n",
      " -3.040e+02  3.040e+02 -3.070e+02 -3.130e+02 -3.130e+02 -3.170e+02\n",
      "  3.200e+02 -3.200e+02 -3.220e+02 -3.220e+02 -3.230e+02 -3.320e+02\n",
      "  3.360e+02 -3.360e+02 -3.370e+02 -3.390e+02 -3.430e+02 -3.430e+02\n",
      " -3.450e+02 -3.470e+02  3.480e+02 -3.480e+02 -3.500e+02 -3.520e+02\n",
      " -3.580e+02 -3.580e+02 -3.600e+02  3.620e+02 -3.630e+02 -3.640e+02\n",
      " -3.650e+02 -3.650e+02 -3.650e+02 -3.650e+02 -3.660e+02 -3.660e+02\n",
      " -3.680e+02 -3.700e+02 -3.710e+02 -3.710e+02 -3.730e+02 -3.750e+02\n",
      " -3.750e+02 -3.750e+02 -3.750e+02 -3.750e+02 -3.760e+02  3.770e+02\n",
      " -3.790e+02 -3.800e+02 -3.810e+02 -3.810e+02 -3.820e+02 -3.830e+02\n",
      " -3.830e+02 -3.850e+02 -3.850e+02 -3.850e+02  3.850e+02 -3.850e+02\n",
      " -3.850e+02 -3.920e+02 -3.930e+02 -3.940e+02 -3.940e+02 -3.960e+02\n",
      " -3.960e+02 -3.960e+02 -3.960e+02 -3.970e+02 -3.980e+02 -4.020e+02\n",
      " -4.040e+02 -4.050e+02 -4.080e+02 -4.080e+02 -4.090e+02 -4.100e+02\n",
      " -4.100e+02 -4.110e+02 -4.130e+02 -4.140e+02 -4.160e+02 -4.170e+02\n",
      " -4.180e+02 -4.230e+02 -4.240e+02 -4.240e+02 -4.260e+02 -4.260e+02\n",
      " -4.280e+02 -4.300e+02 -4.310e+02 -4.330e+02 -4.340e+02 -4.350e+02\n",
      " -4.390e+02 -4.400e+02 -4.410e+02 -4.410e+02 -4.460e+02  4.460e+02\n",
      " -4.470e+02 -4.470e+02 -4.480e+02 -4.500e+02 -4.500e+02 -4.550e+02\n",
      " -4.570e+02 -4.610e+02 -4.610e+02 -4.630e+02 -4.630e+02 -4.660e+02\n",
      " -4.660e+02 -4.700e+02 -4.750e+02 -4.760e+02 -4.770e+02 -4.770e+02\n",
      " -4.800e+02 -4.880e+02 -4.880e+02 -4.920e+02 -4.950e+02 -4.960e+02\n",
      " -4.960e+02 -4.990e+02 -4.990e+02 -5.010e+02 -5.030e+02 -5.040e+02\n",
      " -5.060e+02 -5.080e+02 -5.090e+02 -5.100e+02 -5.110e+02 -5.130e+02\n",
      " -5.160e+02 -5.180e+02 -5.180e+02 -5.180e+02 -5.190e+02 -5.210e+02\n",
      " -5.220e+02  5.240e+02 -5.250e+02 -5.260e+02 -5.280e+02 -5.290e+02\n",
      " -5.300e+02 -5.300e+02 -5.310e+02 -5.320e+02 -5.320e+02 -5.320e+02\n",
      " -5.330e+02  5.380e+02 -5.410e+02 -5.410e+02 -5.450e+02 -5.460e+02\n",
      " -5.470e+02  5.480e+02 -5.510e+02 -5.520e+02 -5.520e+02 -5.520e+02\n",
      " -5.540e+02 -5.540e+02 -5.540e+02 -5.540e+02 -5.540e+02  5.580e+02\n",
      " -5.620e+02 -5.620e+02  5.630e+02 -5.650e+02 -5.660e+02  5.710e+02\n",
      " -5.720e+02 -5.740e+02 -5.740e+02 -5.750e+02 -5.750e+02 -5.750e+02\n",
      " -5.760e+02 -5.770e+02 -5.770e+02 -5.770e+02 -5.790e+02 -5.790e+02\n",
      " -5.840e+02 -5.840e+02  5.840e+02 -5.840e+02 -5.850e+02 -5.860e+02\n",
      " -5.880e+02 -5.880e+02 -5.900e+02 -5.910e+02 -5.950e+02 -5.950e+02\n",
      " -5.960e+02 -5.980e+02 -6.000e+02 -6.020e+02 -6.060e+02 -6.070e+02\n",
      " -6.070e+02 -6.080e+02  6.120e+02 -6.120e+02 -6.120e+02 -6.140e+02\n",
      "  6.140e+02  6.160e+02 -6.160e+02 -6.180e+02 -6.200e+02 -6.200e+02\n",
      " -6.220e+02 -6.220e+02 -6.240e+02 -6.260e+02 -6.260e+02 -6.270e+02\n",
      " -6.290e+02 -6.300e+02 -6.310e+02 -6.350e+02 -6.350e+02 -6.390e+02\n",
      " -6.400e+02 -6.400e+02 -6.410e+02 -6.430e+02 -6.430e+02 -6.440e+02\n",
      " -6.460e+02 -6.470e+02 -6.480e+02 -6.510e+02 -6.520e+02 -6.550e+02\n",
      " -6.590e+02 -6.590e+02 -6.600e+02 -6.610e+02 -6.620e+02 -6.640e+02\n",
      " -6.660e+02 -6.660e+02 -6.720e+02 -6.750e+02 -6.750e+02 -6.770e+02\n",
      "  6.780e+02 -6.790e+02 -6.790e+02 -6.800e+02 -6.810e+02 -6.830e+02\n",
      " -6.940e+02 -6.940e+02 -6.980e+02 -7.010e+02 -7.020e+02 -7.030e+02\n",
      " -7.060e+02 -7.070e+02 -7.070e+02 -7.090e+02 -7.100e+02 -7.140e+02\n",
      " -7.150e+02 -7.180e+02 -7.220e+02 -7.240e+02 -7.260e+02 -7.270e+02\n",
      " -7.270e+02 -7.270e+02 -7.340e+02 -7.380e+02 -7.420e+02 -7.470e+02\n",
      " -7.470e+02 -7.480e+02  7.490e+02 -7.540e+02 -7.540e+02 -7.550e+02\n",
      " -7.590e+02 -7.600e+02 -7.600e+02 -7.610e+02 -7.620e+02 -7.620e+02\n",
      "  7.630e+02 -7.670e+02 -7.670e+02 -7.690e+02 -7.770e+02 -7.830e+02\n",
      "  7.850e+02  7.860e+02 -7.880e+02 -7.890e+02 -7.910e+02  7.920e+02\n",
      " -7.920e+02 -7.950e+02 -7.980e+02 -8.030e+02 -8.060e+02  8.110e+02\n",
      " -8.120e+02 -8.200e+02 -8.220e+02  8.250e+02 -8.290e+02 -8.470e+02\n",
      " -8.490e+02 -8.560e+02 -8.580e+02 -8.600e+02 -8.650e+02 -8.650e+02\n",
      " -8.780e+02 -8.780e+02  8.790e+02 -8.810e+02  8.830e+02 -8.830e+02\n",
      " -8.890e+02 -8.900e+02 -8.990e+02  9.040e+02 -9.070e+02 -9.080e+02\n",
      " -9.110e+02 -9.120e+02 -9.150e+02 -9.180e+02  9.210e+02 -9.230e+02\n",
      " -9.310e+02 -9.420e+02  9.430e+02 -9.430e+02  9.590e+02 -9.650e+02\n",
      " -9.650e+02  9.670e+02 -9.680e+02 -9.730e+02 -9.740e+02 -9.750e+02\n",
      "  9.760e+02 -9.840e+02 -9.870e+02 -9.890e+02 -9.900e+02  9.910e+02\n",
      "  9.910e+02 -9.960e+02 -9.970e+02 -1.000e+03 -1.001e+03 -1.001e+03\n",
      "  1.004e+03 -1.004e+03 -1.004e+03 -1.004e+03 -1.005e+03 -1.006e+03\n",
      " -1.007e+03  1.009e+03 -1.009e+03 -1.010e+03 -1.013e+03 -1.025e+03\n",
      " -1.026e+03 -1.026e+03 -1.027e+03  1.034e+03  1.034e+03 -1.034e+03\n",
      " -1.039e+03 -1.042e+03 -1.043e+03 -1.047e+03 -1.051e+03 -1.051e+03\n",
      " -1.059e+03 -1.062e+03 -1.063e+03 -1.066e+03  1.072e+03 -1.074e+03\n",
      " -1.079e+03 -1.085e+03 -1.088e+03 -1.093e+03  1.093e+03 -1.099e+03\n",
      " -1.101e+03 -1.102e+03 -1.106e+03 -1.112e+03 -1.119e+03 -1.120e+03\n",
      " -1.120e+03 -1.121e+03 -1.124e+03 -1.132e+03 -1.133e+03 -1.138e+03\n",
      " -1.140e+03  1.142e+03 -1.148e+03  1.148e+03 -1.148e+03 -1.150e+03\n",
      "  1.152e+03 -1.156e+03 -1.158e+03 -1.161e+03 -1.162e+03 -1.163e+03\n",
      " -1.165e+03 -1.167e+03 -1.174e+03  1.174e+03 -1.185e+03 -1.186e+03\n",
      " -1.189e+03 -1.191e+03 -1.196e+03 -1.198e+03 -1.203e+03 -1.203e+03\n",
      " -1.206e+03 -1.208e+03 -1.208e+03 -1.210e+03 -1.217e+03 -1.219e+03\n",
      " -1.220e+03 -1.224e+03 -1.229e+03 -1.229e+03 -1.232e+03 -1.233e+03\n",
      " -1.234e+03 -1.234e+03 -1.239e+03 -1.246e+03 -1.248e+03 -1.251e+03\n",
      " -1.266e+03 -1.269e+03 -1.270e+03  1.272e+03  1.275e+03 -1.277e+03\n",
      "  1.286e+03 -1.291e+03 -1.301e+03 -1.308e+03 -1.308e+03 -1.308e+03\n",
      " -1.309e+03 -1.321e+03  1.324e+03 -1.325e+03 -1.326e+03 -1.330e+03\n",
      " -1.347e+03 -1.351e+03 -1.359e+03 -1.363e+03 -1.363e+03  1.365e+03\n",
      " -1.369e+03 -1.371e+03 -1.371e+03  1.388e+03  1.411e+03 -1.417e+03\n",
      " -1.417e+03  1.430e+03 -1.434e+03  1.439e+03 -1.449e+03 -1.461e+03\n",
      " -1.463e+03 -1.467e+03 -1.471e+03 -1.474e+03 -1.492e+03 -1.505e+03\n",
      "  1.508e+03 -1.516e+03 -1.522e+03 -1.528e+03 -1.532e+03 -1.534e+03\n",
      " -1.535e+03  1.542e+03 -1.542e+03 -1.545e+03 -1.545e+03 -1.546e+03\n",
      " -1.547e+03 -1.550e+03  1.556e+03 -1.561e+03  1.563e+03 -1.563e+03\n",
      " -1.569e+03 -1.572e+03 -1.587e+03 -1.596e+03 -1.604e+03 -1.604e+03\n",
      " -1.611e+03 -1.611e+03 -1.612e+03 -1.613e+03 -1.614e+03 -1.616e+03\n",
      " -1.631e+03 -1.639e+03 -1.640e+03 -1.642e+03 -1.644e+03 -1.648e+03\n",
      "  1.649e+03 -1.673e+03 -1.673e+03  1.673e+03 -1.683e+03 -1.686e+03\n",
      "  1.688e+03 -1.688e+03  1.692e+03 -1.692e+03  1.694e+03 -1.728e+03\n",
      " -1.732e+03 -1.780e+03  1.781e+03 -1.783e+03  1.793e+03  1.812e+03\n",
      " -1.820e+03 -1.836e+03 -1.842e+03 -1.853e+03 -1.855e+03 -1.864e+03\n",
      " -1.866e+03 -1.871e+03 -1.873e+03 -1.876e+03 -1.882e+03  1.884e+03\n",
      " -1.887e+03 -1.914e+03 -1.919e+03  1.920e+03 -1.925e+03 -1.926e+03\n",
      "  1.927e+03 -1.935e+03 -1.935e+03 -1.953e+03 -1.972e+03 -1.980e+03\n",
      " -1.988e+03  1.993e+03 -1.996e+03  2.009e+03 -2.012e+03 -2.019e+03\n",
      " -2.031e+03 -2.033e+03 -2.041e+03 -2.048e+03 -2.064e+03 -2.082e+03\n",
      " -2.091e+03  2.097e+03 -2.108e+03 -2.109e+03 -2.124e+03  2.127e+03\n",
      " -2.128e+03 -2.140e+03 -2.142e+03 -2.155e+03 -2.156e+03 -2.164e+03\n",
      " -2.184e+03 -2.190e+03 -2.190e+03 -2.191e+03  2.192e+03 -2.193e+03\n",
      " -2.197e+03  2.207e+03 -2.222e+03 -2.231e+03 -2.236e+03 -2.240e+03\n",
      " -2.246e+03 -2.248e+03 -2.255e+03 -2.255e+03 -2.255e+03 -2.263e+03\n",
      "  2.273e+03 -2.281e+03 -2.282e+03 -2.288e+03 -2.306e+03 -2.311e+03\n",
      " -2.329e+03 -2.335e+03  2.361e+03 -2.362e+03 -2.365e+03 -2.371e+03\n",
      " -2.372e+03 -2.372e+03  2.373e+03 -2.381e+03 -2.385e+03 -2.403e+03\n",
      " -2.406e+03  2.417e+03 -2.426e+03 -2.442e+03 -2.442e+03  2.469e+03\n",
      " -2.477e+03  2.483e+03 -2.483e+03 -2.486e+03 -2.489e+03 -2.513e+03\n",
      " -2.515e+03  2.520e+03 -2.541e+03  2.551e+03 -2.559e+03 -2.590e+03\n",
      " -2.596e+03 -2.605e+03 -2.612e+03 -2.629e+03 -2.632e+03  2.636e+03\n",
      " -2.645e+03 -2.645e+03 -2.650e+03 -2.653e+03 -2.707e+03 -2.709e+03\n",
      "  2.712e+03 -2.721e+03 -2.759e+03  2.763e+03 -2.767e+03 -2.770e+03\n",
      "  2.798e+03 -2.813e+03 -2.838e+03  2.854e+03 -2.856e+03  2.866e+03\n",
      " -2.868e+03  2.911e+03 -2.920e+03 -2.953e+03  2.965e+03  2.965e+03\n",
      " -2.971e+03 -2.976e+03 -2.989e+03 -2.991e+03 -3.001e+03 -3.004e+03\n",
      " -3.009e+03 -3.011e+03 -3.017e+03 -3.022e+03 -3.035e+03  3.063e+03\n",
      " -3.088e+03 -3.094e+03 -3.102e+03 -3.121e+03  3.126e+03 -3.128e+03\n",
      " -3.152e+03 -3.159e+03 -3.172e+03 -3.202e+03 -3.203e+03 -3.204e+03\n",
      " -3.226e+03 -3.248e+03 -3.256e+03 -3.261e+03 -3.276e+03 -3.283e+03\n",
      " -3.286e+03 -3.287e+03 -3.296e+03 -3.307e+03 -3.316e+03 -3.342e+03\n",
      " -3.361e+03 -3.364e+03  3.418e+03 -3.430e+03  3.461e+03  3.462e+03\n",
      "  3.492e+03 -3.506e+03 -3.603e+03 -3.607e+03  3.669e+03  3.736e+03\n",
      " -3.807e+03  3.873e+03  3.926e+03  3.941e+03  3.945e+03 -3.957e+03\n",
      "  3.959e+03 -4.005e+03 -4.047e+03 -4.080e+03 -4.159e+03 -4.233e+03\n",
      "  4.267e+03 -4.275e+03 -4.285e+03 -4.354e+03 -4.361e+03 -4.894e+03\n",
      " -4.929e+03 -5.156e+03 -5.176e+03 -5.739e+03 -6.292e+03  6.456e+03\n",
      "  6.593e+03 -7.106e+03 -7.126e+03 -7.777e+03 -8.391e+03 -8.556e+03\n",
      " -8.605e+03]\n",
      "Concordance Index 0.569199022793577\n",
      "Integrated Brier Score: 0.27194555531383235\n",
      "y_train breslow final [-1.000e+00 -1.000e+00 -5.000e+00 -5.000e+00 -8.000e+00 -9.000e+00\n",
      " -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01\n",
      " -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01 -1.100e+01\n",
      " -1.600e+01 -1.900e+01 -2.100e+01 -2.400e+01 -2.600e+01 -3.000e+01\n",
      " -3.000e+01 -3.100e+01 -3.100e+01 -3.400e+01 -4.000e+01 -5.100e+01\n",
      " -5.200e+01 -5.400e+01 -5.900e+01 -7.200e+01 -7.600e+01 -7.800e+01\n",
      " -7.800e+01 -7.800e+01 -7.900e+01 -8.000e+01 -8.400e+01 -9.800e+01\n",
      "  1.160e+02 -1.180e+02 -1.340e+02 -1.490e+02  1.580e+02 -1.600e+02\n",
      "  1.600e+02 -1.620e+02 -1.630e+02 -1.700e+02 -1.700e+02  1.720e+02\n",
      " -1.720e+02  1.740e+02 -1.780e+02 -1.860e+02 -1.870e+02 -1.960e+02\n",
      "  1.970e+02 -2.000e+02 -2.130e+02 -2.140e+02 -2.160e+02 -2.170e+02\n",
      " -2.180e+02 -2.220e+02 -2.240e+02  2.240e+02 -2.250e+02  2.270e+02\n",
      "  2.390e+02 -2.420e+02 -2.420e+02 -2.430e+02 -2.520e+02  2.550e+02\n",
      " -2.580e+02 -2.590e+02  2.660e+02 -2.710e+02 -2.730e+02 -2.730e+02\n",
      " -2.740e+02 -2.750e+02 -2.850e+02 -2.880e+02 -2.930e+02 -2.930e+02\n",
      " -2.970e+02 -3.020e+02  3.020e+02 -3.030e+02 -3.030e+02 -3.040e+02\n",
      " -3.040e+02  3.040e+02 -3.070e+02 -3.130e+02 -3.130e+02 -3.170e+02\n",
      "  3.200e+02 -3.200e+02 -3.220e+02 -3.220e+02 -3.230e+02 -3.320e+02\n",
      "  3.360e+02 -3.360e+02 -3.370e+02 -3.390e+02 -3.430e+02 -3.430e+02\n",
      " -3.450e+02 -3.470e+02  3.480e+02 -3.480e+02 -3.500e+02 -3.520e+02\n",
      " -3.580e+02 -3.580e+02 -3.600e+02  3.620e+02 -3.630e+02 -3.640e+02\n",
      " -3.650e+02 -3.650e+02 -3.650e+02 -3.650e+02 -3.660e+02 -3.660e+02\n",
      " -3.680e+02 -3.700e+02 -3.710e+02 -3.710e+02 -3.730e+02 -3.750e+02\n",
      " -3.750e+02 -3.750e+02 -3.750e+02 -3.750e+02 -3.760e+02  3.770e+02\n",
      " -3.790e+02 -3.800e+02 -3.810e+02 -3.810e+02 -3.820e+02 -3.830e+02\n",
      " -3.830e+02 -3.850e+02 -3.850e+02 -3.850e+02  3.850e+02 -3.850e+02\n",
      " -3.850e+02 -3.920e+02 -3.930e+02 -3.940e+02 -3.940e+02 -3.960e+02\n",
      " -3.960e+02 -3.960e+02 -3.960e+02 -3.970e+02 -3.980e+02 -4.020e+02\n",
      " -4.040e+02 -4.050e+02 -4.080e+02 -4.080e+02 -4.090e+02 -4.100e+02\n",
      " -4.100e+02 -4.110e+02 -4.130e+02 -4.140e+02 -4.160e+02 -4.170e+02\n",
      " -4.180e+02 -4.230e+02 -4.240e+02 -4.240e+02 -4.260e+02 -4.260e+02\n",
      " -4.280e+02 -4.300e+02 -4.310e+02 -4.330e+02 -4.340e+02 -4.350e+02\n",
      " -4.390e+02 -4.400e+02 -4.410e+02 -4.410e+02 -4.460e+02  4.460e+02\n",
      " -4.470e+02 -4.470e+02 -4.480e+02 -4.500e+02 -4.500e+02 -4.550e+02\n",
      " -4.570e+02 -4.610e+02 -4.610e+02 -4.630e+02 -4.630e+02 -4.660e+02\n",
      " -4.660e+02 -4.700e+02 -4.750e+02 -4.760e+02 -4.770e+02 -4.770e+02\n",
      " -4.800e+02 -4.880e+02 -4.880e+02 -4.920e+02 -4.950e+02 -4.960e+02\n",
      " -4.960e+02 -4.990e+02 -4.990e+02 -5.010e+02 -5.030e+02 -5.040e+02\n",
      " -5.060e+02 -5.080e+02 -5.090e+02 -5.100e+02 -5.110e+02 -5.130e+02\n",
      " -5.160e+02 -5.180e+02 -5.180e+02 -5.180e+02 -5.190e+02 -5.210e+02\n",
      " -5.220e+02  5.240e+02 -5.250e+02 -5.260e+02 -5.280e+02 -5.290e+02\n",
      " -5.300e+02 -5.300e+02 -5.310e+02 -5.320e+02 -5.320e+02 -5.320e+02\n",
      " -5.330e+02  5.380e+02 -5.410e+02 -5.410e+02 -5.450e+02 -5.460e+02\n",
      " -5.470e+02  5.480e+02 -5.510e+02 -5.520e+02 -5.520e+02 -5.520e+02\n",
      " -5.540e+02 -5.540e+02 -5.540e+02 -5.540e+02 -5.540e+02  5.580e+02\n",
      " -5.620e+02 -5.620e+02  5.630e+02 -5.650e+02 -5.660e+02  5.710e+02\n",
      " -5.720e+02 -5.740e+02 -5.740e+02 -5.750e+02 -5.750e+02 -5.750e+02\n",
      " -5.760e+02 -5.770e+02 -5.770e+02 -5.770e+02 -5.790e+02 -5.790e+02\n",
      " -5.840e+02 -5.840e+02  5.840e+02 -5.840e+02 -5.850e+02 -5.860e+02\n",
      " -5.880e+02 -5.880e+02 -5.900e+02 -5.910e+02 -5.950e+02 -5.950e+02\n",
      " -5.960e+02 -5.980e+02 -6.000e+02 -6.020e+02 -6.060e+02 -6.070e+02\n",
      " -6.070e+02 -6.080e+02  6.120e+02 -6.120e+02 -6.120e+02 -6.140e+02\n",
      "  6.140e+02  6.160e+02 -6.160e+02 -6.180e+02 -6.200e+02 -6.200e+02\n",
      " -6.220e+02 -6.220e+02 -6.240e+02 -6.260e+02 -6.260e+02 -6.270e+02\n",
      " -6.290e+02 -6.300e+02 -6.310e+02 -6.350e+02 -6.350e+02 -6.390e+02\n",
      " -6.400e+02 -6.400e+02 -6.410e+02 -6.430e+02 -6.430e+02 -6.440e+02\n",
      " -6.460e+02 -6.470e+02 -6.480e+02 -6.510e+02 -6.520e+02 -6.550e+02\n",
      " -6.590e+02 -6.590e+02 -6.600e+02 -6.610e+02 -6.620e+02 -6.640e+02\n",
      " -6.660e+02 -6.660e+02 -6.720e+02 -6.750e+02 -6.750e+02 -6.770e+02\n",
      "  6.780e+02 -6.790e+02 -6.790e+02 -6.800e+02 -6.810e+02 -6.830e+02\n",
      " -6.940e+02 -6.940e+02 -6.980e+02 -7.010e+02 -7.020e+02 -7.030e+02\n",
      " -7.060e+02 -7.070e+02 -7.070e+02 -7.090e+02 -7.100e+02 -7.140e+02\n",
      " -7.150e+02 -7.180e+02 -7.220e+02 -7.240e+02 -7.260e+02 -7.270e+02\n",
      " -7.270e+02 -7.270e+02 -7.340e+02 -7.380e+02 -7.420e+02 -7.470e+02\n",
      " -7.470e+02 -7.480e+02  7.490e+02 -7.540e+02 -7.540e+02 -7.550e+02\n",
      " -7.590e+02 -7.600e+02 -7.600e+02 -7.610e+02 -7.620e+02 -7.620e+02\n",
      "  7.630e+02 -7.670e+02 -7.670e+02 -7.690e+02 -7.770e+02 -7.830e+02\n",
      "  7.850e+02  7.860e+02 -7.880e+02 -7.890e+02 -7.910e+02  7.920e+02\n",
      " -7.920e+02 -7.950e+02 -7.980e+02 -8.030e+02 -8.060e+02  8.110e+02\n",
      " -8.120e+02 -8.200e+02 -8.220e+02  8.250e+02 -8.290e+02 -8.470e+02\n",
      " -8.490e+02 -8.560e+02 -8.580e+02 -8.600e+02 -8.650e+02 -8.650e+02\n",
      " -8.780e+02 -8.780e+02  8.790e+02 -8.810e+02  8.830e+02 -8.830e+02\n",
      " -8.890e+02 -8.900e+02 -8.990e+02  9.040e+02 -9.070e+02 -9.080e+02\n",
      " -9.110e+02 -9.120e+02 -9.150e+02 -9.180e+02  9.210e+02 -9.230e+02\n",
      " -9.310e+02 -9.420e+02  9.430e+02 -9.430e+02  9.590e+02 -9.650e+02\n",
      " -9.650e+02  9.670e+02 -9.680e+02 -9.730e+02 -9.740e+02 -9.750e+02\n",
      "  9.760e+02 -9.840e+02 -9.870e+02 -9.890e+02 -9.900e+02  9.910e+02\n",
      "  9.910e+02 -9.960e+02 -9.970e+02 -1.000e+03 -1.001e+03 -1.001e+03\n",
      "  1.004e+03 -1.004e+03 -1.004e+03 -1.004e+03 -1.005e+03 -1.006e+03\n",
      " -1.007e+03  1.009e+03 -1.009e+03 -1.010e+03 -1.013e+03 -1.025e+03\n",
      " -1.026e+03 -1.026e+03 -1.027e+03  1.034e+03  1.034e+03 -1.034e+03\n",
      " -1.039e+03 -1.042e+03 -1.043e+03 -1.047e+03 -1.051e+03 -1.051e+03\n",
      " -1.059e+03 -1.062e+03 -1.063e+03 -1.066e+03  1.072e+03 -1.074e+03\n",
      " -1.079e+03 -1.085e+03 -1.088e+03 -1.093e+03  1.093e+03 -1.099e+03\n",
      " -1.101e+03 -1.102e+03 -1.106e+03 -1.112e+03 -1.119e+03 -1.120e+03\n",
      " -1.120e+03 -1.121e+03 -1.124e+03 -1.132e+03 -1.133e+03 -1.138e+03\n",
      " -1.140e+03  1.142e+03 -1.148e+03  1.148e+03 -1.148e+03 -1.150e+03\n",
      "  1.152e+03 -1.156e+03 -1.158e+03 -1.161e+03 -1.162e+03 -1.163e+03\n",
      " -1.165e+03 -1.167e+03 -1.174e+03  1.174e+03 -1.185e+03 -1.186e+03\n",
      " -1.189e+03 -1.191e+03 -1.196e+03 -1.198e+03 -1.203e+03 -1.203e+03\n",
      " -1.206e+03 -1.208e+03 -1.208e+03 -1.210e+03 -1.217e+03 -1.219e+03\n",
      " -1.220e+03 -1.224e+03 -1.229e+03 -1.229e+03 -1.232e+03 -1.233e+03\n",
      " -1.234e+03 -1.234e+03 -1.239e+03 -1.246e+03 -1.248e+03 -1.251e+03\n",
      " -1.266e+03 -1.269e+03 -1.270e+03  1.272e+03  1.275e+03 -1.277e+03\n",
      "  1.286e+03 -1.291e+03 -1.301e+03 -1.308e+03 -1.308e+03 -1.308e+03\n",
      " -1.309e+03 -1.321e+03  1.324e+03 -1.325e+03 -1.326e+03 -1.330e+03\n",
      " -1.347e+03 -1.351e+03 -1.359e+03 -1.363e+03 -1.363e+03  1.365e+03\n",
      " -1.369e+03 -1.371e+03 -1.371e+03  1.388e+03  1.411e+03 -1.417e+03\n",
      " -1.417e+03  1.430e+03 -1.434e+03  1.439e+03 -1.449e+03 -1.461e+03\n",
      " -1.463e+03 -1.467e+03 -1.471e+03 -1.474e+03 -1.492e+03 -1.505e+03\n",
      "  1.508e+03 -1.516e+03 -1.522e+03 -1.528e+03 -1.532e+03 -1.534e+03\n",
      " -1.535e+03  1.542e+03 -1.542e+03 -1.545e+03 -1.545e+03 -1.546e+03\n",
      " -1.547e+03 -1.550e+03  1.556e+03 -1.561e+03  1.563e+03 -1.563e+03\n",
      " -1.569e+03 -1.572e+03 -1.587e+03 -1.596e+03 -1.604e+03 -1.604e+03\n",
      " -1.611e+03 -1.611e+03 -1.612e+03 -1.613e+03 -1.614e+03 -1.616e+03\n",
      " -1.631e+03 -1.639e+03 -1.640e+03 -1.642e+03 -1.644e+03 -1.648e+03\n",
      "  1.649e+03 -1.673e+03 -1.673e+03  1.673e+03 -1.683e+03 -1.686e+03\n",
      "  1.688e+03 -1.688e+03  1.692e+03 -1.692e+03  1.694e+03 -1.728e+03\n",
      " -1.732e+03 -1.780e+03  1.781e+03 -1.783e+03  1.793e+03  1.812e+03\n",
      " -1.820e+03 -1.836e+03 -1.842e+03 -1.853e+03 -1.855e+03 -1.864e+03\n",
      " -1.866e+03 -1.871e+03 -1.873e+03 -1.876e+03 -1.882e+03  1.884e+03\n",
      " -1.887e+03 -1.914e+03 -1.919e+03  1.920e+03 -1.925e+03 -1.926e+03\n",
      "  1.927e+03 -1.935e+03 -1.935e+03 -1.953e+03 -1.972e+03 -1.980e+03\n",
      " -1.988e+03  1.993e+03 -1.996e+03  2.009e+03 -2.012e+03 -2.019e+03\n",
      " -2.031e+03 -2.033e+03 -2.041e+03 -2.048e+03 -2.064e+03 -2.082e+03\n",
      " -2.091e+03  2.097e+03 -2.108e+03 -2.109e+03 -2.124e+03  2.127e+03\n",
      " -2.128e+03 -2.140e+03 -2.142e+03 -2.155e+03 -2.156e+03 -2.164e+03\n",
      " -2.184e+03 -2.190e+03 -2.190e+03 -2.191e+03  2.192e+03 -2.193e+03\n",
      " -2.197e+03  2.207e+03 -2.222e+03 -2.231e+03 -2.236e+03 -2.240e+03\n",
      " -2.246e+03 -2.248e+03 -2.255e+03 -2.255e+03 -2.255e+03 -2.263e+03\n",
      "  2.273e+03 -2.281e+03 -2.282e+03 -2.288e+03 -2.306e+03 -2.311e+03\n",
      " -2.329e+03 -2.335e+03  2.361e+03 -2.362e+03 -2.365e+03 -2.371e+03\n",
      " -2.372e+03 -2.372e+03  2.373e+03 -2.381e+03 -2.385e+03 -2.403e+03\n",
      " -2.406e+03  2.417e+03 -2.426e+03 -2.442e+03 -2.442e+03  2.469e+03\n",
      " -2.477e+03  2.483e+03 -2.483e+03 -2.486e+03 -2.489e+03 -2.513e+03\n",
      " -2.515e+03  2.520e+03 -2.541e+03  2.551e+03 -2.559e+03 -2.590e+03\n",
      " -2.596e+03 -2.605e+03 -2.612e+03 -2.629e+03 -2.632e+03  2.636e+03\n",
      " -2.645e+03 -2.645e+03 -2.650e+03 -2.653e+03 -2.707e+03 -2.709e+03\n",
      "  2.712e+03 -2.721e+03 -2.759e+03  2.763e+03 -2.767e+03 -2.770e+03\n",
      "  2.798e+03 -2.813e+03 -2.838e+03  2.854e+03 -2.856e+03  2.866e+03\n",
      " -2.868e+03  2.911e+03 -2.920e+03 -2.953e+03  2.965e+03  2.965e+03\n",
      " -2.971e+03 -2.976e+03 -2.989e+03 -2.991e+03 -3.001e+03 -3.004e+03\n",
      " -3.009e+03 -3.011e+03 -3.017e+03 -3.022e+03 -3.035e+03  3.063e+03\n",
      " -3.088e+03 -3.094e+03 -3.102e+03 -3.121e+03  3.126e+03 -3.128e+03\n",
      " -3.152e+03 -3.159e+03 -3.172e+03 -3.202e+03 -3.203e+03 -3.204e+03\n",
      " -3.226e+03 -3.248e+03 -3.256e+03 -3.261e+03 -3.276e+03 -3.283e+03\n",
      " -3.286e+03 -3.287e+03 -3.296e+03 -3.307e+03 -3.316e+03 -3.342e+03\n",
      " -3.361e+03 -3.364e+03  3.418e+03 -3.430e+03  3.461e+03  3.462e+03\n",
      "  3.492e+03 -3.506e+03 -3.603e+03 -3.607e+03  3.669e+03  3.736e+03\n",
      " -3.807e+03  3.873e+03  3.926e+03  3.941e+03  3.945e+03 -3.957e+03\n",
      "  3.959e+03 -4.005e+03 -4.047e+03 -4.080e+03 -4.159e+03 -4.233e+03\n",
      "  4.267e+03 -4.275e+03 -4.285e+03 -4.354e+03 -4.361e+03 -4.894e+03\n",
      " -4.929e+03 -5.156e+03 -5.176e+03 -5.739e+03 -6.292e+03  6.456e+03\n",
      "  6.593e+03 -7.106e+03 -7.126e+03 -7.777e+03 -8.391e+03 -8.556e+03\n",
      " -8.605e+03]\n",
      "durations 5.0 8008.0\n",
      "Concordance Index 0.5927435792906645\n",
      "Integrated Brier Score: 0.20250400523116358\n",
      "(865, 20531) <class 'pandas.core.frame.DataFrame'>\n",
      "(865,) <class 'pandas.core.series.Series'>\n",
      "(216, 20531) <class 'pandas.core.frame.DataFrame'>\n",
      "(216,) <class 'pandas.core.series.Series'>\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m1.4268\u001b[0m        \u001b[32m1.7186\u001b[0m  0.5535\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m1.2248\u001b[0m        \u001b[32m1.2297\u001b[0m  0.6137\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.9844\u001b[0m        \u001b[32m1.0395\u001b[0m  0.6490\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m1.4303\u001b[0m        \u001b[32m1.7162\u001b[0m  0.6442\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.9806\u001b[0m        \u001b[32m1.0575\u001b[0m  0.6105\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m1.2380\u001b[0m        \u001b[32m1.1156\u001b[0m  0.6546\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m1.2898\u001b[0m        \u001b[32m1.5419\u001b[0m  0.6077\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m1.2282\u001b[0m        \u001b[32m1.1056\u001b[0m  0.6013\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m1.2918\u001b[0m        \u001b[32m1.5653\u001b[0m  0.6788\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m1.2254\u001b[0m        \u001b[32m1.2197\u001b[0m  0.6856\n",
      "      2        \u001b[36m1.4230\u001b[0m        1.7188  0.5434\n",
      "      2        0.9893        1.0667  0.5276\n",
      "      2        \u001b[36m0.9742\u001b[0m        \u001b[32m1.0373\u001b[0m  0.5539\n",
      "      2        \u001b[36m1.2812\u001b[0m        \u001b[32m1.5418\u001b[0m  0.5447\n",
      "      2        \u001b[36m1.2060\u001b[0m        \u001b[32m1.2120\u001b[0m  0.5897\n",
      "      2        1.2415        1.1182  0.5678\n",
      "      2        \u001b[36m1.2275\u001b[0m        \u001b[32m1.0985\u001b[0m  0.5651\n",
      "      2        \u001b[36m1.4072\u001b[0m        1.7168  0.6059\n",
      "      2        \u001b[36m1.2842\u001b[0m        \u001b[32m1.5545\u001b[0m  0.6003\n",
      "      2        \u001b[36m1.2216\u001b[0m        1.2227  0.5517\n",
      "      3        \u001b[36m1.4155\u001b[0m        1.7196  0.5442\n",
      "      3        \u001b[36m0.9719\u001b[0m        1.0653  0.5477\n",
      "      3        \u001b[36m1.2339\u001b[0m        1.1205  0.5187\n",
      "      3        1.2834        1.5495  0.5860\n",
      "      3        0.9793        \u001b[32m1.0333\u001b[0m  0.6145\n",
      "      3        \u001b[36m1.2029\u001b[0m        \u001b[32m1.2108\u001b[0m  0.5897\n",
      "      3        1.2361        \u001b[32m1.0941\u001b[0m  0.5745\n",
      "      3        \u001b[36m1.3871\u001b[0m        1.7194  0.5696\n",
      "      3        \u001b[36m1.2818\u001b[0m        \u001b[32m1.5423\u001b[0m  0.5930\n",
      "      3        \u001b[36m1.2141\u001b[0m        1.2332  0.5190\n",
      "      4        \u001b[36m1.4146\u001b[0m        1.7201  0.5368\n",
      "      4        0.9767        1.0641  0.5381\n",
      "      4        1.2416        1.1224  0.5308\n",
      "      4        1.2822        1.5511  0.5264\n",
      "      4        \u001b[36m0.9727\u001b[0m        \u001b[32m1.0329\u001b[0m  0.5709\n",
      "      4        \u001b[36m1.2218\u001b[0m        1.1000  0.5588\n",
      "      4        \u001b[36m1.1910\u001b[0m        \u001b[32m1.2069\u001b[0m  0.5728\n",
      "      4        1.3903        1.7181  0.5884\n",
      "      4        \u001b[36m1.2682\u001b[0m        \u001b[32m1.5398\u001b[0m  0.5587\n",
      "      4        1.2148        1.2425  0.5381\n",
      "      5        \u001b[36m1.4085\u001b[0m        1.7214  0.5222\n",
      "      5        \u001b[36m0.9645\u001b[0m        1.0635  0.5012\n",
      "      5        \u001b[36m1.2292\u001b[0m        1.1223  0.5242\n",
      "      5        \u001b[36m1.2394\u001b[0m        1.5512  0.5364\n",
      "      5        \u001b[36m0.9665\u001b[0m        \u001b[32m1.0303\u001b[0m  0.5875\n",
      "      5        1.2277        1.1039  0.5920\n",
      "      5        1.2068        1.2078  0.5908\n",
      "      5        1.3893        1.7196  0.5694\n",
      "      5        1.2710        1.5417  0.5519\n",
      "      5        1.2195        1.2466  0.5466\n",
      "      6        1.4134        1.7210  0.5061\n",
      "      6        \u001b[36m0.9644\u001b[0m        1.0615  0.5364\n",
      "      6        1.2358        1.1233  0.5206\n",
      "      6        1.2655        1.5511  0.5249\n",
      "      6        \u001b[36m0.9602\u001b[0m        1.0309  0.5898\n",
      "      6        \u001b[36m1.2144\u001b[0m        1.1026  0.5901\n",
      "      6        \u001b[36m1.2635\u001b[0m        1.5424  0.5816\n",
      "      6        \u001b[36m1.3855\u001b[0m        1.7191  0.5958\n",
      "      6        1.2183        1.2507  0.5224\n",
      "      6        1.1981        1.2079  0.6083\n",
      "      7        \u001b[36m1.4008\u001b[0m        1.7200  0.5266\n",
      "      7        0.9734        1.0603  0.5036\n",
      "      7        1.2377        1.1246  0.5185\n",
      "      7        1.2631        1.5512  0.4979\n",
      "      7        1.2154        1.2508  0.5307\n",
      "      7        \u001b[36m0.9561\u001b[0m        1.0322  0.5681\n",
      "      7        1.2187        1.1035  0.6011\n",
      "      7        \u001b[36m1.2543\u001b[0m        1.5464  0.6013\n",
      "      7        \u001b[36m1.1793\u001b[0m        1.2088  0.5971\n",
      "      7        1.3880        \u001b[32m1.7155\u001b[0m  0.6082\n",
      "      8        1.4009        1.7196  0.5196\n",
      "      8        \u001b[36m0.9492\u001b[0m        1.0596  0.5599\n",
      "      8        1.2352        1.1249  0.5352\n",
      "      8        1.2621        1.5517  0.5224\n",
      "      8        \u001b[36m1.2122\u001b[0m        1.2517  0.5173\n",
      "      8        \u001b[36m0.9535\u001b[0m        1.0349  0.5708\n",
      "      8        \u001b[36m1.2084\u001b[0m        1.1023  0.5623\n",
      "      8        \u001b[36m1.2400\u001b[0m        1.5515  0.5671\n",
      "      8        \u001b[36m1.1693\u001b[0m        1.2077  0.5728\n",
      "      8        \u001b[36m1.3670\u001b[0m        1.7174  0.5746\n",
      "      9        \u001b[36m1.3935\u001b[0m        \u001b[32m1.7184\u001b[0m  0.5156\n",
      "      9        0.9543        1.0595  0.5171\n",
      "      9        1.2351        1.1234  0.5264\n",
      "      9        1.2496        1.5514  0.5391\n",
      "      9        1.2129        1.2536  0.5224\n",
      "      9        \u001b[36m0.9392\u001b[0m        1.0332  0.5568\n",
      "     10        \u001b[36m1.3930\u001b[0m        \u001b[32m1.7167\u001b[0m  0.5050\n",
      "      9        \u001b[36m1.2330\u001b[0m        1.5518  0.5919\n",
      "      9        \u001b[36m1.2065\u001b[0m        1.1091  0.6122\n",
      "      9        1.1736        1.2085  0.5964\n",
      "      9        \u001b[36m1.3483\u001b[0m        1.7197  0.5885\n",
      "     10        \u001b[36m0.9350\u001b[0m        1.0604  0.5249\n",
      "Restoring best model from epoch 1.\n",
      "     10        \u001b[36m1.2281\u001b[0m        1.1220  0.5388\n",
      "Restoring best model from epoch 1.\n",
      "     10        \u001b[36m1.2374\u001b[0m        1.5524  0.5140\n",
      "Restoring best model from epoch 1.\n",
      "     10        \u001b[36m1.2107\u001b[0m        1.2557  0.5220\n",
      "Restoring best model from epoch 1.\n",
      "     10        \u001b[36m0.9300\u001b[0m        \u001b[32m1.0292\u001b[0m  0.5767\n",
      "     10        1.3594        1.7196  0.5463\n",
      "     10        \u001b[36m1.1899\u001b[0m        1.1157  0.5587\n",
      "Restoring best model from epoch 3.\n",
      "Restoring best model from epoch 7.\n",
      "     10        \u001b[36m1.2233\u001b[0m        1.5484  0.5761\n",
      "Restoring best model from epoch 4.\n",
      "     10        \u001b[36m1.1693\u001b[0m        1.2088  0.5823\n",
      "Restoring best model from epoch 4.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m1.2296\u001b[0m        \u001b[32m1.3996\u001b[0m  0.1818\n",
      "      2        1.2326        1.4076  0.1593\n",
      "      3        1.2370        1.4081  0.1521\n",
      "      4        1.2311        1.4087  0.1487\n",
      "      5        1.2369        1.4104  0.1482\n",
      "      6        1.2338        1.4120  0.1493\n",
      "      7        \u001b[36m1.2218\u001b[0m        1.4126  0.1674\n",
      "      8        \u001b[36m1.2089\u001b[0m        1.4120  0.1452\n",
      "      9        1.2334        1.4108  0.1610\n",
      "     10        1.2330        1.4088  0.1768\n",
      "Restoring best model from epoch 1.\n",
      "y_train breslow final [-1.000e+00 -1.000e+00 -5.000e+00 -5.000e+00 -7.000e+00 -8.000e+00\n",
      " -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01\n",
      " -1.000e+01 -1.000e+01 -1.000e+01 -1.100e+01 -1.600e+01 -2.100e+01\n",
      " -2.400e+01 -2.600e+01 -3.000e+01 -3.000e+01 -3.000e+01 -3.100e+01\n",
      " -3.100e+01 -3.100e+01 -3.400e+01 -3.400e+01 -4.000e+01 -4.900e+01\n",
      " -5.100e+01 -5.200e+01 -5.400e+01 -5.900e+01 -6.400e+01 -7.000e+01\n",
      " -7.200e+01 -7.800e+01 -7.800e+01 -7.900e+01 -8.000e+01 -8.400e+01\n",
      " -9.000e+01 -9.200e+01 -9.800e+01  1.160e+02 -1.180e+02 -1.340e+02\n",
      "  1.580e+02  1.600e+02 -1.620e+02 -1.630e+02 -1.700e+02 -1.700e+02\n",
      "  1.720e+02 -1.720e+02  1.740e+02 -1.860e+02 -1.870e+02 -1.960e+02\n",
      "  1.970e+02 -2.000e+02 -2.100e+02 -2.130e+02 -2.140e+02 -2.150e+02\n",
      " -2.160e+02 -2.170e+02 -2.180e+02 -2.240e+02 -2.250e+02  2.270e+02\n",
      " -2.270e+02 -2.310e+02  2.390e+02 -2.420e+02 -2.420e+02 -2.430e+02\n",
      " -2.500e+02 -2.520e+02 -2.580e+02 -2.590e+02 -2.660e+02  2.660e+02\n",
      " -2.730e+02 -2.730e+02 -2.740e+02 -2.870e+02 -2.880e+02 -2.930e+02\n",
      " -2.930e+02  2.950e+02 -2.970e+02 -3.000e+02 -3.020e+02 -3.030e+02\n",
      " -3.030e+02 -3.040e+02 -3.040e+02  3.040e+02 -3.070e+02 -3.130e+02\n",
      " -3.130e+02 -3.170e+02  3.200e+02 -3.200e+02  3.220e+02 -3.220e+02\n",
      " -3.220e+02 -3.230e+02 -3.260e+02 -3.280e+02 -3.340e+02  3.360e+02\n",
      " -3.360e+02 -3.370e+02 -3.390e+02 -3.430e+02 -3.430e+02 -3.450e+02\n",
      " -3.450e+02 -3.470e+02  3.480e+02 -3.500e+02 -3.520e+02 -3.580e+02\n",
      " -3.580e+02 -3.600e+02  3.620e+02 -3.630e+02 -3.650e+02 -3.650e+02\n",
      "  3.650e+02 -3.650e+02 -3.650e+02 -3.660e+02 -3.660e+02 -3.680e+02\n",
      " -3.700e+02 -3.710e+02 -3.730e+02 -3.750e+02 -3.750e+02 -3.750e+02\n",
      " -3.760e+02 -3.760e+02  3.770e+02 -3.790e+02 -3.800e+02 -3.800e+02\n",
      " -3.810e+02 -3.810e+02 -3.820e+02  3.850e+02 -3.850e+02 -3.850e+02\n",
      " -3.850e+02  3.850e+02 -3.850e+02 -3.850e+02 -3.920e+02 -3.930e+02\n",
      " -3.940e+02 -3.940e+02 -3.950e+02 -3.960e+02 -3.960e+02 -3.960e+02\n",
      " -4.020e+02 -4.030e+02 -4.030e+02 -4.040e+02 -4.050e+02 -4.080e+02\n",
      " -4.090e+02 -4.100e+02 -4.100e+02 -4.120e+02 -4.130e+02 -4.160e+02\n",
      " -4.170e+02 -4.180e+02 -4.210e+02 -4.230e+02 -4.240e+02 -4.240e+02\n",
      " -4.250e+02 -4.260e+02  4.260e+02 -4.260e+02 -4.280e+02 -4.300e+02\n",
      " -4.310e+02 -4.310e+02 -4.330e+02 -4.340e+02 -4.350e+02 -4.370e+02\n",
      " -4.390e+02 -4.390e+02 -4.390e+02 -4.400e+02 -4.410e+02 -4.410e+02\n",
      "  4.460e+02 -4.470e+02 -4.470e+02 -4.470e+02 -4.480e+02 -4.480e+02\n",
      " -4.500e+02 -4.510e+02 -4.540e+02 -4.560e+02 -4.570e+02 -4.580e+02\n",
      " -4.610e+02 -4.630e+02 -4.630e+02 -4.660e+02 -4.660e+02 -4.670e+02\n",
      " -4.710e+02 -4.720e+02 -4.750e+02 -4.760e+02 -4.770e+02 -4.770e+02\n",
      " -4.770e+02 -4.820e+02 -4.880e+02 -4.890e+02 -4.960e+02 -5.010e+02\n",
      " -5.010e+02 -5.020e+02 -5.030e+02 -5.040e+02 -5.040e+02 -5.110e+02\n",
      " -5.130e+02 -5.160e+02 -5.180e+02 -5.190e+02 -5.190e+02 -5.210e+02\n",
      " -5.220e+02 -5.230e+02 -5.250e+02 -5.280e+02 -5.290e+02 -5.300e+02\n",
      " -5.300e+02 -5.310e+02 -5.320e+02 -5.320e+02 -5.320e+02 -5.330e+02\n",
      " -5.330e+02 -5.380e+02 -5.380e+02 -5.410e+02 -5.410e+02 -5.450e+02\n",
      " -5.460e+02 -5.470e+02  5.480e+02 -5.490e+02 -5.520e+02 -5.520e+02\n",
      " -5.540e+02 -5.540e+02 -5.540e+02 -5.540e+02 -5.540e+02  5.580e+02\n",
      " -5.620e+02 -5.620e+02 -5.630e+02 -5.660e+02 -5.670e+02 -5.680e+02\n",
      " -5.720e+02  5.730e+02 -5.740e+02 -5.740e+02 -5.750e+02 -5.750e+02\n",
      " -5.750e+02 -5.760e+02 -5.770e+02 -5.770e+02 -5.770e+02 -5.790e+02\n",
      " -5.790e+02 -5.840e+02  5.840e+02 -5.860e+02 -5.880e+02 -5.880e+02\n",
      " -5.900e+02 -5.910e+02 -5.940e+02 -5.950e+02 -5.950e+02 -5.980e+02\n",
      " -6.000e+02 -6.020e+02 -6.070e+02 -6.070e+02 -6.080e+02 -6.080e+02\n",
      " -6.110e+02 -6.110e+02 -6.120e+02 -6.120e+02 -6.140e+02 -6.160e+02\n",
      " -6.160e+02 -6.180e+02 -6.200e+02 -6.200e+02 -6.220e+02 -6.260e+02\n",
      " -6.260e+02 -6.270e+02 -6.290e+02 -6.350e+02 -6.350e+02 -6.350e+02\n",
      " -6.390e+02  6.390e+02 -6.400e+02 -6.410e+02 -6.460e+02 -6.470e+02\n",
      " -6.480e+02 -6.510e+02 -6.550e+02 -6.580e+02 -6.590e+02 -6.590e+02\n",
      " -6.600e+02 -6.610e+02 -6.640e+02 -6.660e+02 -6.660e+02 -6.750e+02\n",
      " -6.770e+02 -6.770e+02  6.780e+02 -6.790e+02 -6.790e+02 -6.800e+02\n",
      " -6.810e+02 -6.830e+02 -6.940e+02 -6.980e+02 -7.010e+02 -7.020e+02\n",
      " -7.030e+02 -7.060e+02 -7.070e+02 -7.070e+02 -7.090e+02 -7.140e+02\n",
      " -7.150e+02 -7.150e+02 -7.180e+02 -7.220e+02  7.230e+02 -7.250e+02\n",
      " -7.260e+02 -7.270e+02 -7.280e+02 -7.340e+02 -7.380e+02 -7.450e+02\n",
      " -7.470e+02 -7.470e+02 -7.480e+02  7.490e+02 -7.520e+02 -7.520e+02\n",
      "  7.540e+02 -7.540e+02 -7.540e+02 -7.550e+02 -7.590e+02 -7.590e+02\n",
      " -7.600e+02 -7.620e+02 -7.620e+02  7.630e+02 -7.650e+02 -7.670e+02\n",
      " -7.770e+02 -7.830e+02 -7.850e+02  7.850e+02  7.860e+02 -7.880e+02\n",
      " -7.890e+02  7.920e+02  7.920e+02 -7.920e+02 -7.950e+02 -7.980e+02\n",
      " -8.060e+02 -8.120e+02 -8.190e+02 -8.200e+02  8.210e+02  8.250e+02\n",
      " -8.290e+02 -8.370e+02 -8.390e+02 -8.470e+02 -8.490e+02 -8.520e+02\n",
      " -8.560e+02 -8.580e+02 -8.600e+02  8.600e+02 -8.650e+02 -8.650e+02\n",
      " -8.670e+02 -8.750e+02 -8.780e+02 -8.780e+02  8.790e+02 -8.810e+02\n",
      "  8.830e+02 -8.830e+02 -8.990e+02  9.040e+02 -9.060e+02 -9.070e+02\n",
      " -9.080e+02  9.120e+02 -9.120e+02 -9.120e+02 -9.150e+02 -9.180e+02\n",
      "  9.210e+02 -9.260e+02 -9.310e+02 -9.310e+02 -9.420e+02 -9.430e+02\n",
      "  9.430e+02 -9.430e+02 -9.430e+02 -9.520e+02 -9.540e+02  9.590e+02\n",
      " -9.650e+02 -9.650e+02  9.670e+02 -9.680e+02 -9.720e+02 -9.740e+02\n",
      " -9.740e+02 -9.750e+02 -9.750e+02 -9.840e+02 -9.870e+02 -9.890e+02\n",
      " -9.900e+02  9.910e+02  9.910e+02 -9.960e+02 -9.960e+02 -9.970e+02\n",
      " -1.001e+03 -1.001e+03 -1.004e+03 -1.004e+03 -1.004e+03 -1.005e+03\n",
      "  1.009e+03 -1.009e+03 -1.010e+03 -1.013e+03 -1.015e+03 -1.025e+03\n",
      " -1.026e+03 -1.026e+03 -1.027e+03  1.032e+03  1.034e+03 -1.034e+03\n",
      " -1.035e+03 -1.039e+03 -1.042e+03 -1.043e+03 -1.043e+03 -1.043e+03\n",
      " -1.047e+03  1.048e+03 -1.051e+03 -1.051e+03 -1.051e+03 -1.059e+03\n",
      " -1.062e+03 -1.062e+03 -1.063e+03 -1.066e+03  1.072e+03 -1.074e+03\n",
      " -1.079e+03 -1.085e+03 -1.093e+03 -1.099e+03 -1.101e+03 -1.102e+03\n",
      "  1.104e+03 -1.106e+03 -1.112e+03 -1.119e+03 -1.120e+03 -1.121e+03\n",
      " -1.124e+03  1.127e+03 -1.133e+03 -1.139e+03 -1.140e+03 -1.141e+03\n",
      "  1.142e+03 -1.148e+03  1.148e+03 -1.148e+03  1.152e+03 -1.156e+03\n",
      " -1.156e+03 -1.157e+03 -1.158e+03 -1.161e+03 -1.165e+03 -1.167e+03\n",
      " -1.174e+03  1.174e+03 -1.179e+03 -1.185e+03 -1.186e+03 -1.191e+03\n",
      " -1.196e+03 -1.198e+03 -1.203e+03 -1.203e+03 -1.206e+03 -1.208e+03\n",
      " -1.208e+03 -1.210e+03 -1.217e+03 -1.219e+03 -1.220e+03 -1.220e+03\n",
      " -1.224e+03 -1.232e+03 -1.233e+03 -1.234e+03 -1.234e+03 -1.246e+03\n",
      " -1.247e+03 -1.248e+03 -1.251e+03 -1.266e+03 -1.269e+03 -1.270e+03\n",
      "  1.272e+03 -1.285e+03 -1.288e+03 -1.291e+03 -1.301e+03 -1.308e+03\n",
      " -1.308e+03 -1.308e+03 -1.309e+03 -1.309e+03 -1.318e+03 -1.325e+03\n",
      " -1.326e+03 -1.330e+03 -1.347e+03 -1.359e+03 -1.363e+03 -1.369e+03\n",
      " -1.371e+03 -1.371e+03  1.388e+03  1.411e+03 -1.417e+03 -1.417e+03\n",
      " -1.419e+03  1.430e+03 -1.437e+03  1.439e+03 -1.448e+03 -1.449e+03\n",
      " -1.461e+03 -1.463e+03 -1.467e+03 -1.471e+03 -1.474e+03 -1.474e+03\n",
      " -1.476e+03 -1.477e+03 -1.505e+03 -1.516e+03 -1.519e+03 -1.523e+03\n",
      " -1.532e+03 -1.534e+03 -1.535e+03  1.542e+03 -1.542e+03 -1.545e+03\n",
      " -1.546e+03 -1.547e+03 -1.548e+03 -1.550e+03  1.556e+03 -1.561e+03\n",
      "  1.563e+03 -1.563e+03 -1.569e+03 -1.572e+03 -1.587e+03 -1.596e+03\n",
      " -1.604e+03 -1.611e+03 -1.611e+03 -1.614e+03 -1.616e+03 -1.620e+03\n",
      " -1.631e+03 -1.639e+03 -1.640e+03  1.642e+03 -1.644e+03 -1.648e+03\n",
      "  1.649e+03 -1.662e+03 -1.673e+03 -1.673e+03  1.673e+03 -1.682e+03\n",
      " -1.683e+03  1.688e+03 -1.688e+03 -1.692e+03  1.694e+03  1.699e+03\n",
      " -1.712e+03 -1.728e+03 -1.728e+03 -1.732e+03 -1.742e+03  1.759e+03\n",
      "  1.793e+03 -1.800e+03 -1.820e+03 -1.836e+03 -1.847e+03 -1.855e+03\n",
      " -1.864e+03 -1.866e+03 -1.871e+03 -1.873e+03 -1.876e+03 -1.882e+03\n",
      "  1.884e+03 -1.887e+03  1.900e+03 -1.919e+03  1.920e+03 -1.925e+03\n",
      " -1.926e+03  1.927e+03 -1.928e+03 -1.935e+03 -1.935e+03 -1.972e+03\n",
      " -1.988e+03  1.993e+03 -1.996e+03 -2.012e+03 -2.019e+03 -2.031e+03\n",
      " -2.033e+03 -2.041e+03 -2.048e+03 -2.064e+03 -2.072e+03 -2.082e+03\n",
      " -2.091e+03  2.097e+03 -2.108e+03 -2.128e+03 -2.136e+03 -2.155e+03\n",
      " -2.156e+03 -2.161e+03 -2.164e+03 -2.184e+03 -2.190e+03 -2.190e+03\n",
      " -2.191e+03  2.192e+03 -2.193e+03  2.207e+03 -2.222e+03 -2.231e+03\n",
      " -2.236e+03 -2.240e+03 -2.246e+03 -2.255e+03 -2.255e+03 -2.263e+03\n",
      "  2.273e+03 -2.278e+03 -2.281e+03 -2.282e+03 -2.288e+03 -2.289e+03\n",
      "  2.296e+03 -2.311e+03 -2.330e+03  2.348e+03  2.361e+03 -2.362e+03\n",
      " -2.371e+03 -2.372e+03  2.373e+03 -2.381e+03 -2.403e+03 -2.406e+03\n",
      "  2.417e+03 -2.426e+03 -2.442e+03  2.469e+03 -2.477e+03  2.483e+03\n",
      " -2.483e+03 -2.486e+03 -2.489e+03  2.520e+03  2.534e+03 -2.535e+03\n",
      " -2.541e+03  2.551e+03 -2.558e+03 -2.559e+03  2.573e+03 -2.590e+03\n",
      " -2.596e+03 -2.596e+03 -2.605e+03 -2.612e+03 -2.618e+03 -2.629e+03\n",
      " -2.632e+03 -2.632e+03  2.636e+03 -2.650e+03 -2.653e+03 -2.654e+03\n",
      " -2.695e+03 -2.707e+03  2.712e+03 -2.721e+03  2.763e+03 -2.767e+03\n",
      "  2.798e+03 -2.813e+03 -2.838e+03  2.854e+03 -2.856e+03  2.866e+03\n",
      " -2.868e+03 -2.953e+03  2.965e+03  2.965e+03 -2.971e+03 -2.976e+03\n",
      " -2.989e+03 -2.991e+03 -2.991e+03 -3.001e+03 -3.015e+03 -3.017e+03\n",
      " -3.021e+03 -3.022e+03 -3.030e+03 -3.035e+03 -3.072e+03 -3.088e+03\n",
      " -3.091e+03 -3.094e+03 -3.112e+03 -3.121e+03  3.126e+03 -3.128e+03\n",
      " -3.152e+03 -3.159e+03 -3.172e+03 -3.203e+03 -3.226e+03 -3.247e+03\n",
      " -3.248e+03 -3.256e+03 -3.261e+03  3.262e+03 -3.276e+03 -3.283e+03\n",
      " -3.283e+03 -3.286e+03 -3.296e+03 -3.316e+03 -3.361e+03 -3.364e+03\n",
      "  3.409e+03  3.418e+03 -3.430e+03 -3.456e+03  3.461e+03  3.462e+03\n",
      "  3.472e+03  3.492e+03 -3.506e+03 -3.519e+03 -3.607e+03 -3.660e+03\n",
      "  3.669e+03 -3.709e+03  3.926e+03  3.941e+03  3.945e+03 -3.957e+03\n",
      " -4.052e+03 -4.080e+03 -4.088e+03 -4.159e+03 -4.233e+03  4.267e+03\n",
      " -4.275e+03 -4.285e+03 -4.354e+03  4.456e+03 -4.894e+03 -5.042e+03\n",
      " -5.062e+03 -5.176e+03 -5.739e+03 -6.292e+03  6.456e+03  6.593e+03\n",
      " -7.106e+03 -7.126e+03  7.455e+03 -7.777e+03 -8.008e+03 -8.391e+03\n",
      " -8.605e+03]\n",
      "Concordance Index 0.5159740921330326\n",
      "Integrated Brier Score: 0.24794278233927416\n",
      "y_train breslow final [-1.000e+00 -1.000e+00 -5.000e+00 -5.000e+00 -7.000e+00 -8.000e+00\n",
      " -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01\n",
      " -1.000e+01 -1.000e+01 -1.000e+01 -1.100e+01 -1.600e+01 -2.100e+01\n",
      " -2.400e+01 -2.600e+01 -3.000e+01 -3.000e+01 -3.000e+01 -3.100e+01\n",
      " -3.100e+01 -3.100e+01 -3.400e+01 -3.400e+01 -4.000e+01 -4.900e+01\n",
      " -5.100e+01 -5.200e+01 -5.400e+01 -5.900e+01 -6.400e+01 -7.000e+01\n",
      " -7.200e+01 -7.800e+01 -7.800e+01 -7.900e+01 -8.000e+01 -8.400e+01\n",
      " -9.000e+01 -9.200e+01 -9.800e+01  1.160e+02 -1.180e+02 -1.340e+02\n",
      "  1.580e+02  1.600e+02 -1.620e+02 -1.630e+02 -1.700e+02 -1.700e+02\n",
      "  1.720e+02 -1.720e+02  1.740e+02 -1.860e+02 -1.870e+02 -1.960e+02\n",
      "  1.970e+02 -2.000e+02 -2.100e+02 -2.130e+02 -2.140e+02 -2.150e+02\n",
      " -2.160e+02 -2.170e+02 -2.180e+02 -2.240e+02 -2.250e+02  2.270e+02\n",
      " -2.270e+02 -2.310e+02  2.390e+02 -2.420e+02 -2.420e+02 -2.430e+02\n",
      " -2.500e+02 -2.520e+02 -2.580e+02 -2.590e+02 -2.660e+02  2.660e+02\n",
      " -2.730e+02 -2.730e+02 -2.740e+02 -2.870e+02 -2.880e+02 -2.930e+02\n",
      " -2.930e+02  2.950e+02 -2.970e+02 -3.000e+02 -3.020e+02 -3.030e+02\n",
      " -3.030e+02 -3.040e+02 -3.040e+02  3.040e+02 -3.070e+02 -3.130e+02\n",
      " -3.130e+02 -3.170e+02  3.200e+02 -3.200e+02  3.220e+02 -3.220e+02\n",
      " -3.220e+02 -3.230e+02 -3.260e+02 -3.280e+02 -3.340e+02  3.360e+02\n",
      " -3.360e+02 -3.370e+02 -3.390e+02 -3.430e+02 -3.430e+02 -3.450e+02\n",
      " -3.450e+02 -3.470e+02  3.480e+02 -3.500e+02 -3.520e+02 -3.580e+02\n",
      " -3.580e+02 -3.600e+02  3.620e+02 -3.630e+02 -3.650e+02 -3.650e+02\n",
      "  3.650e+02 -3.650e+02 -3.650e+02 -3.660e+02 -3.660e+02 -3.680e+02\n",
      " -3.700e+02 -3.710e+02 -3.730e+02 -3.750e+02 -3.750e+02 -3.750e+02\n",
      " -3.760e+02 -3.760e+02  3.770e+02 -3.790e+02 -3.800e+02 -3.800e+02\n",
      " -3.810e+02 -3.810e+02 -3.820e+02  3.850e+02 -3.850e+02 -3.850e+02\n",
      " -3.850e+02  3.850e+02 -3.850e+02 -3.850e+02 -3.920e+02 -3.930e+02\n",
      " -3.940e+02 -3.940e+02 -3.950e+02 -3.960e+02 -3.960e+02 -3.960e+02\n",
      " -4.020e+02 -4.030e+02 -4.030e+02 -4.040e+02 -4.050e+02 -4.080e+02\n",
      " -4.090e+02 -4.100e+02 -4.100e+02 -4.120e+02 -4.130e+02 -4.160e+02\n",
      " -4.170e+02 -4.180e+02 -4.210e+02 -4.230e+02 -4.240e+02 -4.240e+02\n",
      " -4.250e+02 -4.260e+02  4.260e+02 -4.260e+02 -4.280e+02 -4.300e+02\n",
      " -4.310e+02 -4.310e+02 -4.330e+02 -4.340e+02 -4.350e+02 -4.370e+02\n",
      " -4.390e+02 -4.390e+02 -4.390e+02 -4.400e+02 -4.410e+02 -4.410e+02\n",
      "  4.460e+02 -4.470e+02 -4.470e+02 -4.470e+02 -4.480e+02 -4.480e+02\n",
      " -4.500e+02 -4.510e+02 -4.540e+02 -4.560e+02 -4.570e+02 -4.580e+02\n",
      " -4.610e+02 -4.630e+02 -4.630e+02 -4.660e+02 -4.660e+02 -4.670e+02\n",
      " -4.710e+02 -4.720e+02 -4.750e+02 -4.760e+02 -4.770e+02 -4.770e+02\n",
      " -4.770e+02 -4.820e+02 -4.880e+02 -4.890e+02 -4.960e+02 -5.010e+02\n",
      " -5.010e+02 -5.020e+02 -5.030e+02 -5.040e+02 -5.040e+02 -5.110e+02\n",
      " -5.130e+02 -5.160e+02 -5.180e+02 -5.190e+02 -5.190e+02 -5.210e+02\n",
      " -5.220e+02 -5.230e+02 -5.250e+02 -5.280e+02 -5.290e+02 -5.300e+02\n",
      " -5.300e+02 -5.310e+02 -5.320e+02 -5.320e+02 -5.320e+02 -5.330e+02\n",
      " -5.330e+02 -5.380e+02 -5.380e+02 -5.410e+02 -5.410e+02 -5.450e+02\n",
      " -5.460e+02 -5.470e+02  5.480e+02 -5.490e+02 -5.520e+02 -5.520e+02\n",
      " -5.540e+02 -5.540e+02 -5.540e+02 -5.540e+02 -5.540e+02  5.580e+02\n",
      " -5.620e+02 -5.620e+02 -5.630e+02 -5.660e+02 -5.670e+02 -5.680e+02\n",
      " -5.720e+02  5.730e+02 -5.740e+02 -5.740e+02 -5.750e+02 -5.750e+02\n",
      " -5.750e+02 -5.760e+02 -5.770e+02 -5.770e+02 -5.770e+02 -5.790e+02\n",
      " -5.790e+02 -5.840e+02  5.840e+02 -5.860e+02 -5.880e+02 -5.880e+02\n",
      " -5.900e+02 -5.910e+02 -5.940e+02 -5.950e+02 -5.950e+02 -5.980e+02\n",
      " -6.000e+02 -6.020e+02 -6.070e+02 -6.070e+02 -6.080e+02 -6.080e+02\n",
      " -6.110e+02 -6.110e+02 -6.120e+02 -6.120e+02 -6.140e+02 -6.160e+02\n",
      " -6.160e+02 -6.180e+02 -6.200e+02 -6.200e+02 -6.220e+02 -6.260e+02\n",
      " -6.260e+02 -6.270e+02 -6.290e+02 -6.350e+02 -6.350e+02 -6.350e+02\n",
      " -6.390e+02  6.390e+02 -6.400e+02 -6.410e+02 -6.460e+02 -6.470e+02\n",
      " -6.480e+02 -6.510e+02 -6.550e+02 -6.580e+02 -6.590e+02 -6.590e+02\n",
      " -6.600e+02 -6.610e+02 -6.640e+02 -6.660e+02 -6.660e+02 -6.750e+02\n",
      " -6.770e+02 -6.770e+02  6.780e+02 -6.790e+02 -6.790e+02 -6.800e+02\n",
      " -6.810e+02 -6.830e+02 -6.940e+02 -6.980e+02 -7.010e+02 -7.020e+02\n",
      " -7.030e+02 -7.060e+02 -7.070e+02 -7.070e+02 -7.090e+02 -7.140e+02\n",
      " -7.150e+02 -7.150e+02 -7.180e+02 -7.220e+02  7.230e+02 -7.250e+02\n",
      " -7.260e+02 -7.270e+02 -7.280e+02 -7.340e+02 -7.380e+02 -7.450e+02\n",
      " -7.470e+02 -7.470e+02 -7.480e+02  7.490e+02 -7.520e+02 -7.520e+02\n",
      "  7.540e+02 -7.540e+02 -7.540e+02 -7.550e+02 -7.590e+02 -7.590e+02\n",
      " -7.600e+02 -7.620e+02 -7.620e+02  7.630e+02 -7.650e+02 -7.670e+02\n",
      " -7.770e+02 -7.830e+02 -7.850e+02  7.850e+02  7.860e+02 -7.880e+02\n",
      " -7.890e+02  7.920e+02  7.920e+02 -7.920e+02 -7.950e+02 -7.980e+02\n",
      " -8.060e+02 -8.120e+02 -8.190e+02 -8.200e+02  8.210e+02  8.250e+02\n",
      " -8.290e+02 -8.370e+02 -8.390e+02 -8.470e+02 -8.490e+02 -8.520e+02\n",
      " -8.560e+02 -8.580e+02 -8.600e+02  8.600e+02 -8.650e+02 -8.650e+02\n",
      " -8.670e+02 -8.750e+02 -8.780e+02 -8.780e+02  8.790e+02 -8.810e+02\n",
      "  8.830e+02 -8.830e+02 -8.990e+02  9.040e+02 -9.060e+02 -9.070e+02\n",
      " -9.080e+02  9.120e+02 -9.120e+02 -9.120e+02 -9.150e+02 -9.180e+02\n",
      "  9.210e+02 -9.260e+02 -9.310e+02 -9.310e+02 -9.420e+02 -9.430e+02\n",
      "  9.430e+02 -9.430e+02 -9.430e+02 -9.520e+02 -9.540e+02  9.590e+02\n",
      " -9.650e+02 -9.650e+02  9.670e+02 -9.680e+02 -9.720e+02 -9.740e+02\n",
      " -9.740e+02 -9.750e+02 -9.750e+02 -9.840e+02 -9.870e+02 -9.890e+02\n",
      " -9.900e+02  9.910e+02  9.910e+02 -9.960e+02 -9.960e+02 -9.970e+02\n",
      " -1.001e+03 -1.001e+03 -1.004e+03 -1.004e+03 -1.004e+03 -1.005e+03\n",
      "  1.009e+03 -1.009e+03 -1.010e+03 -1.013e+03 -1.015e+03 -1.025e+03\n",
      " -1.026e+03 -1.026e+03 -1.027e+03  1.032e+03  1.034e+03 -1.034e+03\n",
      " -1.035e+03 -1.039e+03 -1.042e+03 -1.043e+03 -1.043e+03 -1.043e+03\n",
      " -1.047e+03  1.048e+03 -1.051e+03 -1.051e+03 -1.051e+03 -1.059e+03\n",
      " -1.062e+03 -1.062e+03 -1.063e+03 -1.066e+03  1.072e+03 -1.074e+03\n",
      " -1.079e+03 -1.085e+03 -1.093e+03 -1.099e+03 -1.101e+03 -1.102e+03\n",
      "  1.104e+03 -1.106e+03 -1.112e+03 -1.119e+03 -1.120e+03 -1.121e+03\n",
      " -1.124e+03  1.127e+03 -1.133e+03 -1.139e+03 -1.140e+03 -1.141e+03\n",
      "  1.142e+03 -1.148e+03  1.148e+03 -1.148e+03  1.152e+03 -1.156e+03\n",
      " -1.156e+03 -1.157e+03 -1.158e+03 -1.161e+03 -1.165e+03 -1.167e+03\n",
      " -1.174e+03  1.174e+03 -1.179e+03 -1.185e+03 -1.186e+03 -1.191e+03\n",
      " -1.196e+03 -1.198e+03 -1.203e+03 -1.203e+03 -1.206e+03 -1.208e+03\n",
      " -1.208e+03 -1.210e+03 -1.217e+03 -1.219e+03 -1.220e+03 -1.220e+03\n",
      " -1.224e+03 -1.232e+03 -1.233e+03 -1.234e+03 -1.234e+03 -1.246e+03\n",
      " -1.247e+03 -1.248e+03 -1.251e+03 -1.266e+03 -1.269e+03 -1.270e+03\n",
      "  1.272e+03 -1.285e+03 -1.288e+03 -1.291e+03 -1.301e+03 -1.308e+03\n",
      " -1.308e+03 -1.308e+03 -1.309e+03 -1.309e+03 -1.318e+03 -1.325e+03\n",
      " -1.326e+03 -1.330e+03 -1.347e+03 -1.359e+03 -1.363e+03 -1.369e+03\n",
      " -1.371e+03 -1.371e+03  1.388e+03  1.411e+03 -1.417e+03 -1.417e+03\n",
      " -1.419e+03  1.430e+03 -1.437e+03  1.439e+03 -1.448e+03 -1.449e+03\n",
      " -1.461e+03 -1.463e+03 -1.467e+03 -1.471e+03 -1.474e+03 -1.474e+03\n",
      " -1.476e+03 -1.477e+03 -1.505e+03 -1.516e+03 -1.519e+03 -1.523e+03\n",
      " -1.532e+03 -1.534e+03 -1.535e+03  1.542e+03 -1.542e+03 -1.545e+03\n",
      " -1.546e+03 -1.547e+03 -1.548e+03 -1.550e+03  1.556e+03 -1.561e+03\n",
      "  1.563e+03 -1.563e+03 -1.569e+03 -1.572e+03 -1.587e+03 -1.596e+03\n",
      " -1.604e+03 -1.611e+03 -1.611e+03 -1.614e+03 -1.616e+03 -1.620e+03\n",
      " -1.631e+03 -1.639e+03 -1.640e+03  1.642e+03 -1.644e+03 -1.648e+03\n",
      "  1.649e+03 -1.662e+03 -1.673e+03 -1.673e+03  1.673e+03 -1.682e+03\n",
      " -1.683e+03  1.688e+03 -1.688e+03 -1.692e+03  1.694e+03  1.699e+03\n",
      " -1.712e+03 -1.728e+03 -1.728e+03 -1.732e+03 -1.742e+03  1.759e+03\n",
      "  1.793e+03 -1.800e+03 -1.820e+03 -1.836e+03 -1.847e+03 -1.855e+03\n",
      " -1.864e+03 -1.866e+03 -1.871e+03 -1.873e+03 -1.876e+03 -1.882e+03\n",
      "  1.884e+03 -1.887e+03  1.900e+03 -1.919e+03  1.920e+03 -1.925e+03\n",
      " -1.926e+03  1.927e+03 -1.928e+03 -1.935e+03 -1.935e+03 -1.972e+03\n",
      " -1.988e+03  1.993e+03 -1.996e+03 -2.012e+03 -2.019e+03 -2.031e+03\n",
      " -2.033e+03 -2.041e+03 -2.048e+03 -2.064e+03 -2.072e+03 -2.082e+03\n",
      " -2.091e+03  2.097e+03 -2.108e+03 -2.128e+03 -2.136e+03 -2.155e+03\n",
      " -2.156e+03 -2.161e+03 -2.164e+03 -2.184e+03 -2.190e+03 -2.190e+03\n",
      " -2.191e+03  2.192e+03 -2.193e+03  2.207e+03 -2.222e+03 -2.231e+03\n",
      " -2.236e+03 -2.240e+03 -2.246e+03 -2.255e+03 -2.255e+03 -2.263e+03\n",
      "  2.273e+03 -2.278e+03 -2.281e+03 -2.282e+03 -2.288e+03 -2.289e+03\n",
      "  2.296e+03 -2.311e+03 -2.330e+03  2.348e+03  2.361e+03 -2.362e+03\n",
      " -2.371e+03 -2.372e+03  2.373e+03 -2.381e+03 -2.403e+03 -2.406e+03\n",
      "  2.417e+03 -2.426e+03 -2.442e+03  2.469e+03 -2.477e+03  2.483e+03\n",
      " -2.483e+03 -2.486e+03 -2.489e+03  2.520e+03  2.534e+03 -2.535e+03\n",
      " -2.541e+03  2.551e+03 -2.558e+03 -2.559e+03  2.573e+03 -2.590e+03\n",
      " -2.596e+03 -2.596e+03 -2.605e+03 -2.612e+03 -2.618e+03 -2.629e+03\n",
      " -2.632e+03 -2.632e+03  2.636e+03 -2.650e+03 -2.653e+03 -2.654e+03\n",
      " -2.695e+03 -2.707e+03  2.712e+03 -2.721e+03  2.763e+03 -2.767e+03\n",
      "  2.798e+03 -2.813e+03 -2.838e+03  2.854e+03 -2.856e+03  2.866e+03\n",
      " -2.868e+03 -2.953e+03  2.965e+03  2.965e+03 -2.971e+03 -2.976e+03\n",
      " -2.989e+03 -2.991e+03 -2.991e+03 -3.001e+03 -3.015e+03 -3.017e+03\n",
      " -3.021e+03 -3.022e+03 -3.030e+03 -3.035e+03 -3.072e+03 -3.088e+03\n",
      " -3.091e+03 -3.094e+03 -3.112e+03 -3.121e+03  3.126e+03 -3.128e+03\n",
      " -3.152e+03 -3.159e+03 -3.172e+03 -3.203e+03 -3.226e+03 -3.247e+03\n",
      " -3.248e+03 -3.256e+03 -3.261e+03  3.262e+03 -3.276e+03 -3.283e+03\n",
      " -3.283e+03 -3.286e+03 -3.296e+03 -3.316e+03 -3.361e+03 -3.364e+03\n",
      "  3.409e+03  3.418e+03 -3.430e+03 -3.456e+03  3.461e+03  3.462e+03\n",
      "  3.472e+03  3.492e+03 -3.506e+03 -3.519e+03 -3.607e+03 -3.660e+03\n",
      "  3.669e+03 -3.709e+03  3.926e+03  3.941e+03  3.945e+03 -3.957e+03\n",
      " -4.052e+03 -4.080e+03 -4.088e+03 -4.159e+03 -4.233e+03  4.267e+03\n",
      " -4.275e+03 -4.285e+03 -4.354e+03  4.456e+03 -4.894e+03 -5.042e+03\n",
      " -5.062e+03 -5.176e+03 -5.739e+03 -6.292e+03  6.456e+03  6.593e+03\n",
      " -7.106e+03 -7.126e+03  7.455e+03 -7.777e+03 -8.008e+03 -8.391e+03\n",
      " -8.605e+03]\n",
      "durations 5.0 8556.0\n",
      "Concordance Index 0.5600746268656717\n",
      "Integrated Brier Score: 0.15377665900492749\n",
      "(865, 20531) <class 'pandas.core.frame.DataFrame'>\n",
      "(865,) <class 'pandas.core.series.Series'>\n",
      "(216, 20531) <class 'pandas.core.frame.DataFrame'>\n",
      "(216,) <class 'pandas.core.series.Series'>\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m1.3614\u001b[0m        \u001b[32m1.4150\u001b[0m  0.5131\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m1.0386\u001b[0m        \u001b[32m0.9948\u001b[0m  0.5902\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m1.1147\u001b[0m        \u001b[32m1.1785\u001b[0m  0.5660  epoch    train_loss    valid_loss     dur\n",
      "\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m1.2194\u001b[0m        \u001b[32m1.3619\u001b[0m  0.5835\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m1.3144\u001b[0m        \u001b[32m1.6665\u001b[0m  0.5750\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m1.3632\u001b[0m        \u001b[32m1.4318\u001b[0m  0.6071\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m1.3189\u001b[0m        \u001b[32m1.6618\u001b[0m  0.6548\n",
      "      2        1.3657        \u001b[32m1.4125\u001b[0m  0.5175\n",
      "      2        1.3196        1.6669  0.5118\n",
      "      2        \u001b[36m1.2119\u001b[0m        1.3638  0.5553\n",
      "      2        1.3683        1.4319  0.5411\n",
      "      2        \u001b[36m1.1096\u001b[0m        \u001b[32m1.1729\u001b[0m  0.6057\n",
      "      2        1.0496        \u001b[32m0.9942\u001b[0m  0.6605\n",
      "      2        \u001b[36m1.3100\u001b[0m        1.6638  0.5633\n",
      "      3        \u001b[36m1.3459\u001b[0m        \u001b[32m1.4120\u001b[0m  0.4874\n",
      "      3        1.3252        \u001b[32m1.6663\u001b[0m  0.5040\n",
      "      3        \u001b[36m1.2048\u001b[0m        1.3660  0.5426\n",
      "      3        1.1134        \u001b[32m1.1701\u001b[0m  0.5181\n",
      "      3        1.3646        1.4325  0.5427\n",
      "      3        \u001b[36m1.0259\u001b[0m        \u001b[32m0.9917\u001b[0m  0.5372\n",
      "      3        \u001b[36m1.2999\u001b[0m        1.6633  0.5452\n",
      "      4        1.3540        \u001b[32m1.4112\u001b[0m  0.4712\n",
      "      4        \u001b[36m1.3112\u001b[0m        1.6665  0.4744\n",
      "      4        1.2083        \u001b[32m1.3546\u001b[0m  0.5059\n",
      "      4        \u001b[36m1.1089\u001b[0m        1.1743  0.5259\n",
      "      4        \u001b[36m1.3626\u001b[0m        1.4350  0.5167\n",
      "      4        1.0263        \u001b[32m0.9858\u001b[0m  0.5072\n",
      "      4        \u001b[36m1.2985\u001b[0m        1.6645  0.5296\n",
      "      5        1.3530        1.4127  0.4974\n",
      "      5        \u001b[36m1.3077\u001b[0m        \u001b[32m1.6650\u001b[0m  0.4682\n",
      "      5        1.2053        1.3550  0.5050\n",
      "      5        1.1112        1.1755  0.5345\n",
      "      5        \u001b[36m1.3586\u001b[0m        1.4339  0.5455\n",
      "      5        \u001b[36m1.0017\u001b[0m        0.9878  0.5239\n",
      "      5        \u001b[36m1.2906\u001b[0m        \u001b[32m1.6579\u001b[0m  0.5186\n",
      "      6        \u001b[36m1.3376\u001b[0m        \u001b[32m1.4105\u001b[0m  0.4743\n",
      "      6        1.3172        \u001b[32m1.6644\u001b[0m  0.5029\n",
      "      6        1.2091        1.3583  0.5254\n",
      "      6        \u001b[36m1.0998\u001b[0m        1.1769  0.5144\n",
      "      6        \u001b[36m1.3457\u001b[0m        \u001b[32m1.4245\u001b[0m  0.5415\n",
      "      6        \u001b[36m0.9927\u001b[0m        0.9875  0.5424\n",
      "      6        1.2920        \u001b[32m1.6457\u001b[0m  0.5432\n",
      "      7        \u001b[36m1.3334\u001b[0m        \u001b[32m1.4088\u001b[0m  0.5397\n",
      "      7        1.3089        \u001b[32m1.6639\u001b[0m  0.4879\n",
      "      7        \u001b[36m1.1883\u001b[0m        1.3586  0.5637\n",
      "      7        1.1054        1.1787  0.5380\n",
      "      7        \u001b[36m1.3451\u001b[0m        \u001b[32m1.4235\u001b[0m  0.5093\n",
      "      7        \u001b[36m0.9879\u001b[0m        0.9861  0.5200\n",
      "      7        \u001b[36m1.2862\u001b[0m        \u001b[32m1.6396\u001b[0m  0.5058\n",
      "      8        1.3489        \u001b[32m1.4075\u001b[0m  0.5154\n",
      "      8        1.3168        \u001b[32m1.6634\u001b[0m  0.4889\n",
      "      8        1.1908        1.3599  0.5148\n",
      "      8        \u001b[36m1.0959\u001b[0m        1.1737  0.5018\n",
      "      8        \u001b[36m0.9791\u001b[0m        \u001b[32m0.9855\u001b[0m  0.5080\n",
      "      8        \u001b[36m1.3380\u001b[0m        \u001b[32m1.4196\u001b[0m  0.5304\n",
      "      8        \u001b[36m1.2847\u001b[0m        \u001b[32m1.6364\u001b[0m  0.5013\n",
      "      9        \u001b[36m1.3037\u001b[0m        \u001b[32m1.6612\u001b[0m  0.4873\n",
      "      9        1.3506        \u001b[32m1.4064\u001b[0m  0.5433\n",
      "      9        1.1920        1.3601  0.5507\n",
      "      9        \u001b[36m1.0957\u001b[0m        1.1732  0.5356\n",
      "      9        \u001b[36m0.9608\u001b[0m        0.9857  0.5559\n",
      "      9        1.3456        \u001b[32m1.4180\u001b[0m  0.5574\n",
      "      9        \u001b[36m1.2787\u001b[0m        1.6423  0.5694\n",
      "     10        \u001b[36m1.3020\u001b[0m        \u001b[32m1.6592\u001b[0m  0.5191\n",
      "     10        1.3448        \u001b[32m1.4051\u001b[0m  0.4991\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "     10        \u001b[36m1.1747\u001b[0m        1.3587  0.5786\n",
      "Restoring best model from epoch 4.\n",
      "     10        \u001b[36m1.0861\u001b[0m        1.1747  0.5831\n",
      "Restoring best model from epoch 3.\n",
      "     10        \u001b[36m0.9510\u001b[0m        \u001b[32m0.9828\u001b[0m  0.5534\n",
      "     10        1.3438        \u001b[32m1.4146\u001b[0m  0.5768\n",
      "     10        \u001b[36m1.2699\u001b[0m        1.6463  0.5832\n",
      "Restoring best model from epoch 8.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m1.1140\u001b[0m        \u001b[32m1.1720\u001b[0m  0.5235\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m1.0336\u001b[0m        \u001b[32m0.9933\u001b[0m  0.5189\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m1.2104\u001b[0m        \u001b[32m1.3623\u001b[0m  0.5472\n",
      "      2        \u001b[36m1.1110\u001b[0m        \u001b[32m1.1683\u001b[0m  0.4333\n",
      "      2        \u001b[36m1.0305\u001b[0m        \u001b[32m0.9912\u001b[0m  0.4339\n",
      "      2        1.2220        1.3683  0.4343\n",
      "      3        \u001b[36m1.1107\u001b[0m        1.1699  0.4200\n",
      "      3        \u001b[36m1.0271\u001b[0m        0.9916  0.4261\n",
      "      3        1.2170        1.3704  0.4194\n",
      "      4        1.1169        1.1752  0.4176\n",
      "      4        1.2145        1.3692  0.4149\n",
      "      4        1.0407        \u001b[32m0.9904\u001b[0m  0.4211\n",
      "      5        1.1211        1.1758  0.4185\n",
      "      5        1.2163        1.3696  0.4115\n",
      "      5        \u001b[36m1.0213\u001b[0m        \u001b[32m0.9901\u001b[0m  0.4181\n",
      "      6        1.1120        1.1766  0.4189\n",
      "      6        1.2139        1.3704  0.4157\n",
      "      6        \u001b[36m1.0199\u001b[0m        \u001b[32m0.9899\u001b[0m  0.4123\n",
      "      7        1.1124        1.1790  0.4195\n",
      "      7        \u001b[36m1.0019\u001b[0m        \u001b[32m0.9886\u001b[0m  0.4117\n",
      "      7        1.2115        1.3690  0.4224\n",
      "      8        1.1141        1.1796  0.4201\n",
      "      8        1.0043        \u001b[32m0.9871\u001b[0m  0.4132\n",
      "      8        \u001b[36m1.2076\u001b[0m        1.3670  0.4205\n",
      "      9        1.1177        1.1797  0.4243\n",
      "      9        1.0095        \u001b[32m0.9849\u001b[0m  0.4167\n",
      "      9        1.2160        1.3633  0.4225\n",
      "     10        \u001b[36m0.9822\u001b[0m        \u001b[32m0.9823\u001b[0m  0.4204\n",
      "     10        1.1133        1.1799  0.4262\n",
      "Restoring best model from epoch 2.\n",
      "     10        \u001b[36m1.1956\u001b[0m        \u001b[32m1.3590\u001b[0m  0.4223\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m1.2636\u001b[0m        \u001b[32m1.1176\u001b[0m  0.1815\n",
      "      2        \u001b[36m1.2614\u001b[0m        1.1228  0.1574\n",
      "      3        1.2622        1.1180  0.1521\n",
      "      4        1.2635        \u001b[32m1.1152\u001b[0m  0.1630\n",
      "      5        \u001b[36m1.2478\u001b[0m        1.1160  0.1677\n",
      "      6        \u001b[36m1.2450\u001b[0m        1.1168  0.1695\n",
      "      7        \u001b[36m1.2293\u001b[0m        1.1169  0.1696\n",
      "      8        1.2508        1.1167  0.1819\n",
      "      9        1.2347        1.1167  0.1530\n",
      "     10        1.2364        1.1159  0.1689\n",
      "Restoring best model from epoch 4.\n",
      "y_train breslow final [-1.000e+00 -5.000e+00 -5.000e+00 -5.000e+00 -7.000e+00 -8.000e+00\n",
      " -9.000e+00 -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01\n",
      " -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01\n",
      " -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01 -1.100e+01 -1.900e+01\n",
      " -2.100e+01 -2.600e+01 -3.000e+01 -3.000e+01 -3.000e+01 -3.100e+01\n",
      " -3.100e+01 -3.100e+01 -3.400e+01 -3.400e+01 -4.000e+01 -4.900e+01\n",
      " -5.200e+01 -5.400e+01 -6.400e+01 -7.000e+01 -7.200e+01 -7.600e+01\n",
      " -7.800e+01 -7.800e+01 -7.900e+01 -8.400e+01 -9.000e+01 -9.200e+01\n",
      " -9.800e+01  1.160e+02 -1.340e+02 -1.490e+02 -1.600e+02 -1.700e+02\n",
      " -1.700e+02  1.720e+02 -1.720e+02  1.740e+02 -1.780e+02 -1.860e+02\n",
      " -2.000e+02 -2.100e+02 -2.140e+02 -2.150e+02 -2.160e+02 -2.220e+02\n",
      " -2.240e+02  2.240e+02 -2.250e+02 -2.270e+02 -2.310e+02  2.390e+02\n",
      " -2.420e+02 -2.430e+02 -2.500e+02 -2.520e+02  2.550e+02 -2.580e+02\n",
      " -2.590e+02 -2.660e+02  2.660e+02 -2.710e+02 -2.730e+02 -2.740e+02\n",
      " -2.750e+02 -2.850e+02 -2.870e+02 -2.930e+02  2.950e+02 -3.000e+02\n",
      "  3.020e+02 -3.030e+02 -3.030e+02 -3.040e+02 -3.040e+02 -3.040e+02\n",
      " -3.040e+02  3.040e+02 -3.130e+02 -3.170e+02 -3.170e+02  3.200e+02\n",
      "  3.220e+02 -3.220e+02 -3.220e+02 -3.230e+02 -3.260e+02 -3.280e+02\n",
      " -3.320e+02 -3.340e+02 -3.360e+02 -3.450e+02 -3.470e+02 -3.480e+02\n",
      " -3.500e+02 -3.520e+02 -3.580e+02 -3.580e+02 -3.580e+02 -3.600e+02\n",
      " -3.640e+02 -3.650e+02 -3.650e+02  3.650e+02 -3.650e+02 -3.650e+02\n",
      " -3.660e+02 -3.660e+02 -3.710e+02 -3.710e+02 -3.730e+02 -3.730e+02\n",
      " -3.750e+02 -3.750e+02 -3.750e+02 -3.750e+02 -3.760e+02 -3.760e+02\n",
      "  3.770e+02 -3.790e+02 -3.800e+02 -3.800e+02 -3.810e+02 -3.810e+02\n",
      " -3.830e+02 -3.830e+02  3.850e+02 -3.850e+02 -3.850e+02 -3.850e+02\n",
      " -3.920e+02 -3.930e+02 -3.930e+02 -3.940e+02 -3.940e+02 -3.940e+02\n",
      " -3.950e+02 -3.960e+02 -3.960e+02 -3.960e+02 -3.960e+02 -3.960e+02\n",
      " -3.970e+02 -3.980e+02 -4.020e+02 -4.030e+02 -4.030e+02 -4.050e+02\n",
      " -4.080e+02 -4.080e+02 -4.090e+02 -4.100e+02 -4.100e+02 -4.110e+02\n",
      " -4.120e+02 -4.130e+02 -4.140e+02 -4.170e+02 -4.180e+02 -4.210e+02\n",
      " -4.230e+02 -4.240e+02 -4.250e+02  4.260e+02 -4.280e+02 -4.280e+02\n",
      " -4.300e+02 -4.310e+02 -4.330e+02 -4.340e+02 -4.350e+02 -4.370e+02\n",
      " -4.390e+02 -4.390e+02 -4.390e+02 -4.410e+02 -4.410e+02 -4.410e+02\n",
      " -4.460e+02  4.460e+02 -4.470e+02 -4.470e+02 -4.470e+02 -4.480e+02\n",
      " -4.480e+02 -4.500e+02 -4.500e+02 -4.510e+02 -4.540e+02 -4.550e+02\n",
      " -4.560e+02 -4.580e+02 -4.610e+02 -4.610e+02 -4.630e+02 -4.660e+02\n",
      " -4.660e+02 -4.670e+02 -4.700e+02 -4.710e+02 -4.720e+02 -4.750e+02\n",
      " -4.760e+02 -4.770e+02 -4.800e+02 -4.820e+02 -4.880e+02 -4.880e+02\n",
      " -4.890e+02 -4.920e+02 -4.950e+02 -4.960e+02 -4.960e+02 -4.990e+02\n",
      " -4.990e+02 -5.010e+02 -5.020e+02 -5.030e+02 -5.040e+02 -5.040e+02\n",
      " -5.060e+02 -5.080e+02 -5.090e+02 -5.100e+02 -5.110e+02 -5.180e+02\n",
      " -5.180e+02 -5.180e+02 -5.190e+02 -5.210e+02 -5.220e+02 -5.230e+02\n",
      "  5.240e+02 -5.250e+02 -5.260e+02 -5.280e+02 -5.280e+02 -5.290e+02\n",
      " -5.300e+02 -5.300e+02 -5.310e+02 -5.320e+02 -5.320e+02 -5.330e+02\n",
      " -5.330e+02 -5.380e+02 -5.380e+02  5.380e+02 -5.410e+02 -5.410e+02\n",
      " -5.450e+02 -5.460e+02 -5.470e+02  5.480e+02 -5.490e+02 -5.510e+02\n",
      " -5.520e+02 -5.520e+02 -5.540e+02 -5.540e+02 -5.540e+02 -5.620e+02\n",
      " -5.630e+02  5.630e+02 -5.650e+02 -5.660e+02 -5.670e+02 -5.680e+02\n",
      "  5.710e+02 -5.720e+02  5.730e+02 -5.740e+02 -5.750e+02 -5.750e+02\n",
      " -5.750e+02 -5.750e+02 -5.770e+02 -5.770e+02 -5.770e+02 -5.790e+02\n",
      " -5.790e+02 -5.840e+02 -5.840e+02  5.840e+02 -5.840e+02 -5.850e+02\n",
      " -5.880e+02 -5.880e+02 -5.880e+02 -5.900e+02 -5.940e+02 -5.950e+02\n",
      " -5.960e+02 -5.980e+02 -6.000e+02 -6.060e+02 -6.070e+02 -6.080e+02\n",
      " -6.080e+02 -6.110e+02 -6.110e+02 -6.120e+02  6.120e+02 -6.120e+02\n",
      " -6.140e+02  6.140e+02  6.160e+02 -6.160e+02 -6.160e+02 -6.200e+02\n",
      " -6.200e+02 -6.220e+02 -6.220e+02 -6.240e+02 -6.260e+02 -6.260e+02\n",
      " -6.270e+02 -6.300e+02 -6.310e+02 -6.350e+02 -6.350e+02 -6.390e+02\n",
      "  6.390e+02 -6.400e+02 -6.400e+02 -6.410e+02 -6.430e+02 -6.430e+02\n",
      " -6.440e+02 -6.460e+02 -6.470e+02 -6.510e+02 -6.520e+02 -6.550e+02\n",
      " -6.580e+02 -6.590e+02 -6.590e+02 -6.600e+02 -6.610e+02 -6.620e+02\n",
      " -6.640e+02 -6.660e+02 -6.660e+02 -6.720e+02 -6.750e+02 -6.750e+02\n",
      " -6.770e+02 -6.770e+02  6.780e+02 -6.790e+02 -6.790e+02 -6.810e+02\n",
      " -6.830e+02 -6.940e+02 -6.940e+02 -7.010e+02 -7.030e+02 -7.060e+02\n",
      " -7.100e+02 -7.140e+02 -7.140e+02 -7.150e+02 -7.180e+02 -7.220e+02\n",
      "  7.230e+02 -7.240e+02 -7.250e+02 -7.260e+02 -7.270e+02 -7.270e+02\n",
      " -7.280e+02 -7.340e+02 -7.380e+02 -7.420e+02 -7.450e+02 -7.470e+02\n",
      " -7.470e+02 -7.470e+02 -7.480e+02  7.490e+02 -7.520e+02 -7.520e+02\n",
      "  7.540e+02 -7.540e+02 -7.550e+02 -7.590e+02 -7.600e+02 -7.600e+02\n",
      " -7.610e+02 -7.620e+02 -7.620e+02  7.630e+02 -7.650e+02 -7.670e+02\n",
      " -7.670e+02 -7.690e+02 -7.830e+02 -7.850e+02  7.850e+02 -7.880e+02\n",
      " -7.890e+02 -7.910e+02  7.920e+02 -7.920e+02 -7.950e+02 -7.980e+02\n",
      " -8.030e+02  8.110e+02 -8.190e+02 -8.200e+02  8.210e+02 -8.220e+02\n",
      "  8.250e+02 -8.370e+02 -8.390e+02 -8.520e+02 -8.560e+02 -8.580e+02\n",
      " -8.600e+02  8.600e+02 -8.650e+02 -8.650e+02 -8.670e+02 -8.750e+02\n",
      " -8.780e+02 -8.780e+02  8.790e+02 -8.810e+02 -8.830e+02 -8.890e+02\n",
      " -8.900e+02 -8.990e+02  9.040e+02 -9.060e+02 -9.080e+02 -9.110e+02\n",
      "  9.120e+02 -9.120e+02 -9.180e+02  9.210e+02 -9.230e+02 -9.260e+02\n",
      " -9.310e+02 -9.420e+02 -9.430e+02 -9.430e+02 -9.430e+02 -9.520e+02\n",
      " -9.540e+02  9.590e+02 -9.650e+02 -9.650e+02  9.670e+02 -9.680e+02\n",
      " -9.720e+02 -9.730e+02 -9.740e+02 -9.750e+02 -9.750e+02  9.760e+02\n",
      " -9.840e+02 -9.870e+02 -9.900e+02 -9.960e+02 -9.960e+02 -9.970e+02\n",
      " -1.000e+03 -1.001e+03 -1.001e+03  1.004e+03 -1.004e+03 -1.004e+03\n",
      " -1.005e+03 -1.006e+03 -1.007e+03  1.009e+03 -1.010e+03 -1.013e+03\n",
      " -1.015e+03 -1.025e+03 -1.027e+03  1.032e+03  1.034e+03 -1.034e+03\n",
      " -1.035e+03 -1.042e+03 -1.043e+03 -1.043e+03 -1.043e+03  1.048e+03\n",
      " -1.051e+03 -1.051e+03 -1.051e+03 -1.062e+03 -1.062e+03 -1.063e+03\n",
      "  1.072e+03 -1.085e+03 -1.088e+03 -1.093e+03  1.093e+03 -1.099e+03\n",
      " -1.101e+03 -1.102e+03  1.104e+03 -1.112e+03 -1.119e+03 -1.120e+03\n",
      " -1.120e+03 -1.121e+03 -1.124e+03  1.127e+03 -1.132e+03 -1.138e+03\n",
      " -1.139e+03 -1.141e+03 -1.148e+03  1.148e+03 -1.148e+03 -1.150e+03\n",
      "  1.152e+03 -1.156e+03 -1.156e+03 -1.157e+03 -1.158e+03 -1.161e+03\n",
      " -1.162e+03 -1.163e+03 -1.165e+03 -1.167e+03  1.174e+03 -1.179e+03\n",
      " -1.185e+03 -1.186e+03 -1.189e+03 -1.191e+03 -1.196e+03 -1.203e+03\n",
      " -1.203e+03 -1.208e+03 -1.217e+03 -1.219e+03 -1.220e+03 -1.224e+03\n",
      " -1.229e+03 -1.229e+03 -1.233e+03 -1.234e+03 -1.234e+03 -1.239e+03\n",
      " -1.246e+03 -1.247e+03 -1.248e+03 -1.266e+03 -1.269e+03 -1.270e+03\n",
      "  1.272e+03  1.275e+03 -1.277e+03 -1.285e+03  1.286e+03 -1.288e+03\n",
      " -1.291e+03 -1.309e+03 -1.318e+03 -1.321e+03  1.324e+03 -1.325e+03\n",
      " -1.347e+03 -1.351e+03 -1.359e+03 -1.363e+03 -1.363e+03  1.365e+03\n",
      " -1.369e+03 -1.371e+03  1.388e+03  1.411e+03 -1.419e+03  1.430e+03\n",
      " -1.434e+03 -1.437e+03 -1.448e+03 -1.449e+03 -1.461e+03 -1.467e+03\n",
      " -1.471e+03 -1.474e+03 -1.476e+03 -1.477e+03 -1.492e+03 -1.505e+03\n",
      "  1.508e+03 -1.516e+03 -1.519e+03 -1.522e+03 -1.523e+03 -1.528e+03\n",
      " -1.534e+03 -1.535e+03  1.542e+03 -1.545e+03 -1.545e+03 -1.547e+03\n",
      " -1.548e+03 -1.561e+03  1.563e+03 -1.569e+03 -1.587e+03 -1.604e+03\n",
      " -1.604e+03 -1.611e+03 -1.611e+03 -1.611e+03 -1.612e+03 -1.613e+03\n",
      " -1.614e+03 -1.620e+03 -1.631e+03  1.642e+03 -1.642e+03 -1.644e+03\n",
      " -1.648e+03  1.649e+03 -1.662e+03 -1.673e+03  1.673e+03 -1.682e+03\n",
      " -1.686e+03  1.688e+03 -1.688e+03  1.692e+03  1.694e+03  1.699e+03\n",
      " -1.712e+03 -1.728e+03 -1.742e+03  1.759e+03 -1.780e+03  1.781e+03\n",
      " -1.783e+03  1.793e+03 -1.800e+03  1.812e+03 -1.820e+03 -1.836e+03\n",
      " -1.842e+03 -1.847e+03 -1.853e+03 -1.864e+03 -1.866e+03 -1.871e+03\n",
      " -1.876e+03  1.884e+03 -1.887e+03  1.900e+03 -1.914e+03 -1.919e+03\n",
      "  1.920e+03 -1.926e+03  1.927e+03 -1.928e+03 -1.935e+03 -1.953e+03\n",
      " -1.972e+03 -1.980e+03 -1.988e+03  2.009e+03 -2.012e+03 -2.019e+03\n",
      " -2.031e+03 -2.033e+03 -2.048e+03 -2.072e+03  2.097e+03 -2.109e+03\n",
      " -2.124e+03  2.127e+03 -2.128e+03 -2.136e+03 -2.140e+03 -2.142e+03\n",
      " -2.155e+03 -2.156e+03 -2.161e+03 -2.164e+03 -2.184e+03 -2.190e+03\n",
      " -2.197e+03  2.207e+03 -2.236e+03 -2.248e+03 -2.255e+03 -2.255e+03\n",
      " -2.255e+03 -2.263e+03  2.273e+03 -2.278e+03 -2.281e+03 -2.282e+03\n",
      " -2.288e+03 -2.289e+03  2.296e+03 -2.306e+03 -2.311e+03 -2.329e+03\n",
      " -2.330e+03 -2.335e+03  2.348e+03  2.361e+03 -2.365e+03 -2.372e+03\n",
      " -2.372e+03  2.373e+03 -2.381e+03 -2.385e+03 -2.406e+03  2.417e+03\n",
      " -2.426e+03 -2.442e+03 -2.442e+03  2.469e+03 -2.477e+03  2.483e+03\n",
      " -2.483e+03 -2.486e+03 -2.489e+03 -2.513e+03 -2.515e+03  2.520e+03\n",
      "  2.534e+03 -2.535e+03 -2.541e+03  2.551e+03 -2.558e+03 -2.559e+03\n",
      "  2.573e+03 -2.590e+03 -2.596e+03 -2.596e+03 -2.618e+03 -2.632e+03\n",
      " -2.645e+03 -2.645e+03 -2.650e+03 -2.653e+03 -2.654e+03 -2.695e+03\n",
      " -2.707e+03 -2.709e+03 -2.759e+03  2.763e+03 -2.770e+03 -2.813e+03\n",
      " -2.838e+03 -2.856e+03  2.866e+03  2.911e+03 -2.920e+03 -2.953e+03\n",
      "  2.965e+03  2.965e+03 -2.971e+03 -2.989e+03 -2.991e+03 -2.991e+03\n",
      " -3.001e+03 -3.004e+03 -3.009e+03 -3.011e+03 -3.015e+03 -3.017e+03\n",
      " -3.021e+03 -3.030e+03 -3.035e+03  3.063e+03 -3.072e+03 -3.088e+03\n",
      " -3.091e+03 -3.102e+03 -3.112e+03 -3.121e+03  3.126e+03 -3.128e+03\n",
      " -3.152e+03 -3.172e+03 -3.202e+03 -3.203e+03 -3.204e+03 -3.226e+03\n",
      " -3.247e+03 -3.256e+03  3.262e+03 -3.283e+03 -3.283e+03 -3.287e+03\n",
      " -3.307e+03 -3.316e+03 -3.342e+03 -3.364e+03  3.409e+03  3.418e+03\n",
      " -3.430e+03 -3.456e+03  3.461e+03  3.462e+03  3.472e+03 -3.506e+03\n",
      " -3.519e+03 -3.603e+03 -3.607e+03 -3.660e+03  3.669e+03 -3.709e+03\n",
      "  3.736e+03 -3.807e+03  3.873e+03  3.941e+03 -3.957e+03  3.959e+03\n",
      " -4.005e+03 -4.047e+03 -4.052e+03 -4.080e+03 -4.088e+03 -4.233e+03\n",
      "  4.267e+03 -4.285e+03 -4.354e+03 -4.361e+03  4.456e+03 -4.894e+03\n",
      " -4.929e+03 -5.042e+03 -5.062e+03 -5.156e+03 -5.176e+03 -6.292e+03\n",
      " -7.126e+03  7.455e+03 -7.777e+03 -8.008e+03 -8.391e+03 -8.556e+03\n",
      " -8.605e+03]\n",
      "Concordance Index 0.6273008794747925\n",
      "Integrated Brier Score: 0.3066632253760786\n",
      "y_train breslow final [-1.000e+00 -5.000e+00 -5.000e+00 -5.000e+00 -7.000e+00 -8.000e+00\n",
      " -9.000e+00 -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01\n",
      " -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01\n",
      " -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01 -1.100e+01 -1.900e+01\n",
      " -2.100e+01 -2.600e+01 -3.000e+01 -3.000e+01 -3.000e+01 -3.100e+01\n",
      " -3.100e+01 -3.100e+01 -3.400e+01 -3.400e+01 -4.000e+01 -4.900e+01\n",
      " -5.200e+01 -5.400e+01 -6.400e+01 -7.000e+01 -7.200e+01 -7.600e+01\n",
      " -7.800e+01 -7.800e+01 -7.900e+01 -8.400e+01 -9.000e+01 -9.200e+01\n",
      " -9.800e+01  1.160e+02 -1.340e+02 -1.490e+02 -1.600e+02 -1.700e+02\n",
      " -1.700e+02  1.720e+02 -1.720e+02  1.740e+02 -1.780e+02 -1.860e+02\n",
      " -2.000e+02 -2.100e+02 -2.140e+02 -2.150e+02 -2.160e+02 -2.220e+02\n",
      " -2.240e+02  2.240e+02 -2.250e+02 -2.270e+02 -2.310e+02  2.390e+02\n",
      " -2.420e+02 -2.430e+02 -2.500e+02 -2.520e+02  2.550e+02 -2.580e+02\n",
      " -2.590e+02 -2.660e+02  2.660e+02 -2.710e+02 -2.730e+02 -2.740e+02\n",
      " -2.750e+02 -2.850e+02 -2.870e+02 -2.930e+02  2.950e+02 -3.000e+02\n",
      "  3.020e+02 -3.030e+02 -3.030e+02 -3.040e+02 -3.040e+02 -3.040e+02\n",
      " -3.040e+02  3.040e+02 -3.130e+02 -3.170e+02 -3.170e+02  3.200e+02\n",
      "  3.220e+02 -3.220e+02 -3.220e+02 -3.230e+02 -3.260e+02 -3.280e+02\n",
      " -3.320e+02 -3.340e+02 -3.360e+02 -3.450e+02 -3.470e+02 -3.480e+02\n",
      " -3.500e+02 -3.520e+02 -3.580e+02 -3.580e+02 -3.580e+02 -3.600e+02\n",
      " -3.640e+02 -3.650e+02 -3.650e+02  3.650e+02 -3.650e+02 -3.650e+02\n",
      " -3.660e+02 -3.660e+02 -3.710e+02 -3.710e+02 -3.730e+02 -3.730e+02\n",
      " -3.750e+02 -3.750e+02 -3.750e+02 -3.750e+02 -3.760e+02 -3.760e+02\n",
      "  3.770e+02 -3.790e+02 -3.800e+02 -3.800e+02 -3.810e+02 -3.810e+02\n",
      " -3.830e+02 -3.830e+02  3.850e+02 -3.850e+02 -3.850e+02 -3.850e+02\n",
      " -3.920e+02 -3.930e+02 -3.930e+02 -3.940e+02 -3.940e+02 -3.940e+02\n",
      " -3.950e+02 -3.960e+02 -3.960e+02 -3.960e+02 -3.960e+02 -3.960e+02\n",
      " -3.970e+02 -3.980e+02 -4.020e+02 -4.030e+02 -4.030e+02 -4.050e+02\n",
      " -4.080e+02 -4.080e+02 -4.090e+02 -4.100e+02 -4.100e+02 -4.110e+02\n",
      " -4.120e+02 -4.130e+02 -4.140e+02 -4.170e+02 -4.180e+02 -4.210e+02\n",
      " -4.230e+02 -4.240e+02 -4.250e+02  4.260e+02 -4.280e+02 -4.280e+02\n",
      " -4.300e+02 -4.310e+02 -4.330e+02 -4.340e+02 -4.350e+02 -4.370e+02\n",
      " -4.390e+02 -4.390e+02 -4.390e+02 -4.410e+02 -4.410e+02 -4.410e+02\n",
      " -4.460e+02  4.460e+02 -4.470e+02 -4.470e+02 -4.470e+02 -4.480e+02\n",
      " -4.480e+02 -4.500e+02 -4.500e+02 -4.510e+02 -4.540e+02 -4.550e+02\n",
      " -4.560e+02 -4.580e+02 -4.610e+02 -4.610e+02 -4.630e+02 -4.660e+02\n",
      " -4.660e+02 -4.670e+02 -4.700e+02 -4.710e+02 -4.720e+02 -4.750e+02\n",
      " -4.760e+02 -4.770e+02 -4.800e+02 -4.820e+02 -4.880e+02 -4.880e+02\n",
      " -4.890e+02 -4.920e+02 -4.950e+02 -4.960e+02 -4.960e+02 -4.990e+02\n",
      " -4.990e+02 -5.010e+02 -5.020e+02 -5.030e+02 -5.040e+02 -5.040e+02\n",
      " -5.060e+02 -5.080e+02 -5.090e+02 -5.100e+02 -5.110e+02 -5.180e+02\n",
      " -5.180e+02 -5.180e+02 -5.190e+02 -5.210e+02 -5.220e+02 -5.230e+02\n",
      "  5.240e+02 -5.250e+02 -5.260e+02 -5.280e+02 -5.280e+02 -5.290e+02\n",
      " -5.300e+02 -5.300e+02 -5.310e+02 -5.320e+02 -5.320e+02 -5.330e+02\n",
      " -5.330e+02 -5.380e+02 -5.380e+02  5.380e+02 -5.410e+02 -5.410e+02\n",
      " -5.450e+02 -5.460e+02 -5.470e+02  5.480e+02 -5.490e+02 -5.510e+02\n",
      " -5.520e+02 -5.520e+02 -5.540e+02 -5.540e+02 -5.540e+02 -5.620e+02\n",
      " -5.630e+02  5.630e+02 -5.650e+02 -5.660e+02 -5.670e+02 -5.680e+02\n",
      "  5.710e+02 -5.720e+02  5.730e+02 -5.740e+02 -5.750e+02 -5.750e+02\n",
      " -5.750e+02 -5.750e+02 -5.770e+02 -5.770e+02 -5.770e+02 -5.790e+02\n",
      " -5.790e+02 -5.840e+02 -5.840e+02  5.840e+02 -5.840e+02 -5.850e+02\n",
      " -5.880e+02 -5.880e+02 -5.880e+02 -5.900e+02 -5.940e+02 -5.950e+02\n",
      " -5.960e+02 -5.980e+02 -6.000e+02 -6.060e+02 -6.070e+02 -6.080e+02\n",
      " -6.080e+02 -6.110e+02 -6.110e+02 -6.120e+02  6.120e+02 -6.120e+02\n",
      " -6.140e+02  6.140e+02  6.160e+02 -6.160e+02 -6.160e+02 -6.200e+02\n",
      " -6.200e+02 -6.220e+02 -6.220e+02 -6.240e+02 -6.260e+02 -6.260e+02\n",
      " -6.270e+02 -6.300e+02 -6.310e+02 -6.350e+02 -6.350e+02 -6.390e+02\n",
      "  6.390e+02 -6.400e+02 -6.400e+02 -6.410e+02 -6.430e+02 -6.430e+02\n",
      " -6.440e+02 -6.460e+02 -6.470e+02 -6.510e+02 -6.520e+02 -6.550e+02\n",
      " -6.580e+02 -6.590e+02 -6.590e+02 -6.600e+02 -6.610e+02 -6.620e+02\n",
      " -6.640e+02 -6.660e+02 -6.660e+02 -6.720e+02 -6.750e+02 -6.750e+02\n",
      " -6.770e+02 -6.770e+02  6.780e+02 -6.790e+02 -6.790e+02 -6.810e+02\n",
      " -6.830e+02 -6.940e+02 -6.940e+02 -7.010e+02 -7.030e+02 -7.060e+02\n",
      " -7.100e+02 -7.140e+02 -7.140e+02 -7.150e+02 -7.180e+02 -7.220e+02\n",
      "  7.230e+02 -7.240e+02 -7.250e+02 -7.260e+02 -7.270e+02 -7.270e+02\n",
      " -7.280e+02 -7.340e+02 -7.380e+02 -7.420e+02 -7.450e+02 -7.470e+02\n",
      " -7.470e+02 -7.470e+02 -7.480e+02  7.490e+02 -7.520e+02 -7.520e+02\n",
      "  7.540e+02 -7.540e+02 -7.550e+02 -7.590e+02 -7.600e+02 -7.600e+02\n",
      " -7.610e+02 -7.620e+02 -7.620e+02  7.630e+02 -7.650e+02 -7.670e+02\n",
      " -7.670e+02 -7.690e+02 -7.830e+02 -7.850e+02  7.850e+02 -7.880e+02\n",
      " -7.890e+02 -7.910e+02  7.920e+02 -7.920e+02 -7.950e+02 -7.980e+02\n",
      " -8.030e+02  8.110e+02 -8.190e+02 -8.200e+02  8.210e+02 -8.220e+02\n",
      "  8.250e+02 -8.370e+02 -8.390e+02 -8.520e+02 -8.560e+02 -8.580e+02\n",
      " -8.600e+02  8.600e+02 -8.650e+02 -8.650e+02 -8.670e+02 -8.750e+02\n",
      " -8.780e+02 -8.780e+02  8.790e+02 -8.810e+02 -8.830e+02 -8.890e+02\n",
      " -8.900e+02 -8.990e+02  9.040e+02 -9.060e+02 -9.080e+02 -9.110e+02\n",
      "  9.120e+02 -9.120e+02 -9.180e+02  9.210e+02 -9.230e+02 -9.260e+02\n",
      " -9.310e+02 -9.420e+02 -9.430e+02 -9.430e+02 -9.430e+02 -9.520e+02\n",
      " -9.540e+02  9.590e+02 -9.650e+02 -9.650e+02  9.670e+02 -9.680e+02\n",
      " -9.720e+02 -9.730e+02 -9.740e+02 -9.750e+02 -9.750e+02  9.760e+02\n",
      " -9.840e+02 -9.870e+02 -9.900e+02 -9.960e+02 -9.960e+02 -9.970e+02\n",
      " -1.000e+03 -1.001e+03 -1.001e+03  1.004e+03 -1.004e+03 -1.004e+03\n",
      " -1.005e+03 -1.006e+03 -1.007e+03  1.009e+03 -1.010e+03 -1.013e+03\n",
      " -1.015e+03 -1.025e+03 -1.027e+03  1.032e+03  1.034e+03 -1.034e+03\n",
      " -1.035e+03 -1.042e+03 -1.043e+03 -1.043e+03 -1.043e+03  1.048e+03\n",
      " -1.051e+03 -1.051e+03 -1.051e+03 -1.062e+03 -1.062e+03 -1.063e+03\n",
      "  1.072e+03 -1.085e+03 -1.088e+03 -1.093e+03  1.093e+03 -1.099e+03\n",
      " -1.101e+03 -1.102e+03  1.104e+03 -1.112e+03 -1.119e+03 -1.120e+03\n",
      " -1.120e+03 -1.121e+03 -1.124e+03  1.127e+03 -1.132e+03 -1.138e+03\n",
      " -1.139e+03 -1.141e+03 -1.148e+03  1.148e+03 -1.148e+03 -1.150e+03\n",
      "  1.152e+03 -1.156e+03 -1.156e+03 -1.157e+03 -1.158e+03 -1.161e+03\n",
      " -1.162e+03 -1.163e+03 -1.165e+03 -1.167e+03  1.174e+03 -1.179e+03\n",
      " -1.185e+03 -1.186e+03 -1.189e+03 -1.191e+03 -1.196e+03 -1.203e+03\n",
      " -1.203e+03 -1.208e+03 -1.217e+03 -1.219e+03 -1.220e+03 -1.224e+03\n",
      " -1.229e+03 -1.229e+03 -1.233e+03 -1.234e+03 -1.234e+03 -1.239e+03\n",
      " -1.246e+03 -1.247e+03 -1.248e+03 -1.266e+03 -1.269e+03 -1.270e+03\n",
      "  1.272e+03  1.275e+03 -1.277e+03 -1.285e+03  1.286e+03 -1.288e+03\n",
      " -1.291e+03 -1.309e+03 -1.318e+03 -1.321e+03  1.324e+03 -1.325e+03\n",
      " -1.347e+03 -1.351e+03 -1.359e+03 -1.363e+03 -1.363e+03  1.365e+03\n",
      " -1.369e+03 -1.371e+03  1.388e+03  1.411e+03 -1.419e+03  1.430e+03\n",
      " -1.434e+03 -1.437e+03 -1.448e+03 -1.449e+03 -1.461e+03 -1.467e+03\n",
      " -1.471e+03 -1.474e+03 -1.476e+03 -1.477e+03 -1.492e+03 -1.505e+03\n",
      "  1.508e+03 -1.516e+03 -1.519e+03 -1.522e+03 -1.523e+03 -1.528e+03\n",
      " -1.534e+03 -1.535e+03  1.542e+03 -1.545e+03 -1.545e+03 -1.547e+03\n",
      " -1.548e+03 -1.561e+03  1.563e+03 -1.569e+03 -1.587e+03 -1.604e+03\n",
      " -1.604e+03 -1.611e+03 -1.611e+03 -1.611e+03 -1.612e+03 -1.613e+03\n",
      " -1.614e+03 -1.620e+03 -1.631e+03  1.642e+03 -1.642e+03 -1.644e+03\n",
      " -1.648e+03  1.649e+03 -1.662e+03 -1.673e+03  1.673e+03 -1.682e+03\n",
      " -1.686e+03  1.688e+03 -1.688e+03  1.692e+03  1.694e+03  1.699e+03\n",
      " -1.712e+03 -1.728e+03 -1.742e+03  1.759e+03 -1.780e+03  1.781e+03\n",
      " -1.783e+03  1.793e+03 -1.800e+03  1.812e+03 -1.820e+03 -1.836e+03\n",
      " -1.842e+03 -1.847e+03 -1.853e+03 -1.864e+03 -1.866e+03 -1.871e+03\n",
      " -1.876e+03  1.884e+03 -1.887e+03  1.900e+03 -1.914e+03 -1.919e+03\n",
      "  1.920e+03 -1.926e+03  1.927e+03 -1.928e+03 -1.935e+03 -1.953e+03\n",
      " -1.972e+03 -1.980e+03 -1.988e+03  2.009e+03 -2.012e+03 -2.019e+03\n",
      " -2.031e+03 -2.033e+03 -2.048e+03 -2.072e+03  2.097e+03 -2.109e+03\n",
      " -2.124e+03  2.127e+03 -2.128e+03 -2.136e+03 -2.140e+03 -2.142e+03\n",
      " -2.155e+03 -2.156e+03 -2.161e+03 -2.164e+03 -2.184e+03 -2.190e+03\n",
      " -2.197e+03  2.207e+03 -2.236e+03 -2.248e+03 -2.255e+03 -2.255e+03\n",
      " -2.255e+03 -2.263e+03  2.273e+03 -2.278e+03 -2.281e+03 -2.282e+03\n",
      " -2.288e+03 -2.289e+03  2.296e+03 -2.306e+03 -2.311e+03 -2.329e+03\n",
      " -2.330e+03 -2.335e+03  2.348e+03  2.361e+03 -2.365e+03 -2.372e+03\n",
      " -2.372e+03  2.373e+03 -2.381e+03 -2.385e+03 -2.406e+03  2.417e+03\n",
      " -2.426e+03 -2.442e+03 -2.442e+03  2.469e+03 -2.477e+03  2.483e+03\n",
      " -2.483e+03 -2.486e+03 -2.489e+03 -2.513e+03 -2.515e+03  2.520e+03\n",
      "  2.534e+03 -2.535e+03 -2.541e+03  2.551e+03 -2.558e+03 -2.559e+03\n",
      "  2.573e+03 -2.590e+03 -2.596e+03 -2.596e+03 -2.618e+03 -2.632e+03\n",
      " -2.645e+03 -2.645e+03 -2.650e+03 -2.653e+03 -2.654e+03 -2.695e+03\n",
      " -2.707e+03 -2.709e+03 -2.759e+03  2.763e+03 -2.770e+03 -2.813e+03\n",
      " -2.838e+03 -2.856e+03  2.866e+03  2.911e+03 -2.920e+03 -2.953e+03\n",
      "  2.965e+03  2.965e+03 -2.971e+03 -2.989e+03 -2.991e+03 -2.991e+03\n",
      " -3.001e+03 -3.004e+03 -3.009e+03 -3.011e+03 -3.015e+03 -3.017e+03\n",
      " -3.021e+03 -3.030e+03 -3.035e+03  3.063e+03 -3.072e+03 -3.088e+03\n",
      " -3.091e+03 -3.102e+03 -3.112e+03 -3.121e+03  3.126e+03 -3.128e+03\n",
      " -3.152e+03 -3.172e+03 -3.202e+03 -3.203e+03 -3.204e+03 -3.226e+03\n",
      " -3.247e+03 -3.256e+03  3.262e+03 -3.283e+03 -3.283e+03 -3.287e+03\n",
      " -3.307e+03 -3.316e+03 -3.342e+03 -3.364e+03  3.409e+03  3.418e+03\n",
      " -3.430e+03 -3.456e+03  3.461e+03  3.462e+03  3.472e+03 -3.506e+03\n",
      " -3.519e+03 -3.603e+03 -3.607e+03 -3.660e+03  3.669e+03 -3.709e+03\n",
      "  3.736e+03 -3.807e+03  3.873e+03  3.941e+03 -3.957e+03  3.959e+03\n",
      " -4.005e+03 -4.047e+03 -4.052e+03 -4.080e+03 -4.088e+03 -4.233e+03\n",
      "  4.267e+03 -4.285e+03 -4.354e+03 -4.361e+03  4.456e+03 -4.894e+03\n",
      " -4.929e+03 -5.042e+03 -5.062e+03 -5.156e+03 -5.176e+03 -6.292e+03\n",
      " -7.126e+03  7.455e+03 -7.777e+03 -8.008e+03 -8.391e+03 -8.556e+03\n",
      " -8.605e+03]\n",
      "durations 1.0 7106.0\n",
      "Concordance Index 0.5060996863018473\n",
      "Integrated Brier Score: 0.25859097323627017\n",
      "(865, 20531) <class 'pandas.core.frame.DataFrame'>\n",
      "(865,) <class 'pandas.core.series.Series'>\n",
      "(216, 20531) <class 'pandas.core.frame.DataFrame'>\n",
      "(216,) <class 'pandas.core.series.Series'>\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m1.3054\u001b[0m        \u001b[32m1.5572\u001b[0m  0.4844\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m1.1328\u001b[0m        \u001b[32m0.8169\u001b[0m  0.5466\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m1.2552\u001b[0m        \u001b[32m1.2392\u001b[0m  0.5414\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m1.4007\u001b[0m        \u001b[32m1.5783\u001b[0m  0.5438\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m1.2560\u001b[0m        \u001b[32m1.2356\u001b[0m  0.4855\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m1.4131\u001b[0m        \u001b[32m1.6094\u001b[0m  0.4823\n",
      "      2        1.3084        1.5578  0.4838\n",
      "      2        \u001b[36m1.4061\u001b[0m        \u001b[32m1.5999\u001b[0m  0.4966\n",
      "      2        1.2631        1.2404  0.5118\n",
      "      2        \u001b[36m1.3951\u001b[0m        1.5795  0.5393\n",
      "      2        \u001b[36m1.1160\u001b[0m        \u001b[32m0.8145\u001b[0m  0.5501\n",
      "      2        \u001b[36m1.2402\u001b[0m        1.2444  0.5527\n",
      "      3        \u001b[36m1.2986\u001b[0m        1.5582  0.4890\n",
      "      3        1.4098        1.6050  0.4646\n",
      "      3        1.2619        1.2414  0.4873\n",
      "      3        1.2531        \u001b[32m1.2336\u001b[0m  0.5242\n",
      "      3        1.3987        \u001b[32m1.5761\u001b[0m  0.5308\n",
      "      3        \u001b[36m1.0916\u001b[0m        \u001b[32m0.8061\u001b[0m  0.5328\n",
      "      4        \u001b[36m1.2922\u001b[0m        \u001b[32m1.5502\u001b[0m  0.4992\n",
      "      4        1.4071        1.6054  0.5138\n",
      "      4        1.2569        1.2420  0.4934\n",
      "      4        1.2452        \u001b[32m1.2289\u001b[0m  0.5303\n",
      "      4        1.0933        0.8113  0.5421\n",
      "      4        \u001b[36m1.3920\u001b[0m        1.5810  0.5524\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      5        1.2943        \u001b[32m1.5441\u001b[0m  0.4810\n",
      "      5        1.4076        1.6046  0.4694\n",
      "      5        \u001b[36m1.2543\u001b[0m        1.2425  0.4720\n",
      "      5        \u001b[36m1.0677\u001b[0m        0.8171  0.5217\n",
      "      5        \u001b[36m1.3893\u001b[0m        1.5833  0.5174\n",
      "      5        \u001b[36m1.2371\u001b[0m        \u001b[32m1.2263\u001b[0m  0.5443\n",
      "      6        1.2972        \u001b[32m1.5425\u001b[0m  0.4742\n",
      "      6        \u001b[36m1.2499\u001b[0m        1.2371  0.4810\n",
      "      6        1.4105        1.6021  0.5071\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m1.1152\u001b[0m        \u001b[32m1.1004\u001b[0m  0.5615\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m1.2991\u001b[0m        \u001b[32m1.5506\u001b[0m  0.5976\n",
      "      6        \u001b[36m1.3734\u001b[0m        1.5822  0.5419\n",
      "      6        1.2429        \u001b[32m1.2230\u001b[0m  0.5563\n",
      "      6        \u001b[36m1.0559\u001b[0m        0.8165  0.5778\n",
      "      7        \u001b[36m1.2876\u001b[0m        1.5433  0.5203\n",
      "      7        1.2531        \u001b[32m1.2332\u001b[0m  0.5326\n",
      "      7        1.4102        \u001b[32m1.5953\u001b[0m  0.5252\n",
      "      2        \u001b[36m1.1018\u001b[0m        \u001b[32m1.0953\u001b[0m  0.5854\n",
      "      2        \u001b[36m1.2904\u001b[0m        \u001b[32m1.5486\u001b[0m  0.5768\n",
      "      7        \u001b[36m1.3690\u001b[0m        1.5822  0.5619\n",
      "      7        \u001b[36m1.0392\u001b[0m        0.8158  0.5885\n",
      "      7        \u001b[36m1.2258\u001b[0m        \u001b[32m1.2167\u001b[0m  0.5990\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      8        1.2921        1.5430  0.5842\n",
      "      8        \u001b[36m1.3939\u001b[0m        \u001b[32m1.5911\u001b[0m  0.5610\n",
      "      8        \u001b[36m1.2409\u001b[0m        \u001b[32m1.2304\u001b[0m  0.5920\n",
      "      3        1.1054        1.0998  0.5859\n",
      "      3        \u001b[36m1.2883\u001b[0m        \u001b[32m1.5421\u001b[0m  0.6109\n",
      "      8        \u001b[36m1.3590\u001b[0m        1.5769  0.5636\n",
      "      8        \u001b[36m1.0388\u001b[0m        0.8196  0.5918\n",
      "      8        1.2303        \u001b[32m1.2131\u001b[0m  0.6242\n",
      "      9        \u001b[36m1.2801\u001b[0m        \u001b[32m1.5414\u001b[0m  0.5648\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m1.1383\u001b[0m        \u001b[32m0.8133\u001b[0m  0.6310\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m1.1131\u001b[0m        \u001b[32m1.1283\u001b[0m  0.6424\n",
      "      9        1.4055        \u001b[32m1.5865\u001b[0m  0.5906\n",
      "      9        1.2519        \u001b[32m1.2278\u001b[0m  0.5817\n",
      "      4        \u001b[36m1.0900\u001b[0m        1.1015  0.6399\n",
      "      4        1.2934        \u001b[32m1.5389\u001b[0m  0.6137\n",
      "      9        1.3636        \u001b[32m1.5740\u001b[0m  0.6041\n",
      "      9        \u001b[36m1.0012\u001b[0m        0.8176  0.5740\n",
      "      9        1.2327        \u001b[32m1.2101\u001b[0m  0.5992\n",
      "     10        \u001b[36m1.2786\u001b[0m        \u001b[32m1.5395\u001b[0m  0.5473\n",
      "      2        \u001b[36m1.1350\u001b[0m        \u001b[32m0.8076\u001b[0m  0.5314\n",
      "      2        \u001b[36m1.1110\u001b[0m        \u001b[32m1.1245\u001b[0m  0.5356\n",
      "     10        1.3999        \u001b[32m1.5803\u001b[0m  0.5334\n",
      "     10        1.2519        \u001b[32m1.2262\u001b[0m  0.5383\n",
      "      5        \u001b[36m1.2856\u001b[0m        1.5446  0.5852\n",
      "      5        1.0908        1.1015  0.5981\n",
      "     10        1.3627        \u001b[32m1.5651\u001b[0m  0.5609\n",
      "     10        1.0167        0.8088  0.5484\n",
      "Restoring best model from epoch 3.\n",
      "     10        \u001b[36m1.2232\u001b[0m        \u001b[32m1.2092\u001b[0m  0.5350\n",
      "      3        \u001b[36m1.1255\u001b[0m        0.8077  0.4781\n",
      "      3        1.1138        1.1274  0.4962\n",
      "      6        1.2880        1.5531  0.5161\n",
      "      6        \u001b[36m1.0864\u001b[0m        1.1005  0.5207\n",
      "      4        \u001b[36m1.1241\u001b[0m        \u001b[32m0.8056\u001b[0m  0.4782\n",
      "      4        \u001b[36m1.1085\u001b[0m        1.1303  0.4784\n",
      "      7        1.0911        1.1026  0.5180\n",
      "      7        \u001b[36m1.2825\u001b[0m        1.5417  0.5314\n",
      "      5        \u001b[36m1.0999\u001b[0m        1.1322  0.8437\n",
      "      5        \u001b[36m1.1156\u001b[0m        \u001b[32m0.8047\u001b[0m  0.8926\n",
      "      8        1.2909        \u001b[32m1.5298\u001b[0m  0.9708\n",
      "      8        \u001b[36m1.0802\u001b[0m        1.1041  0.9891\n",
      "      6        \u001b[36m1.0998\u001b[0m        1.1330  0.5100\n",
      "      6        \u001b[36m1.1067\u001b[0m        0.8063  0.5074\n",
      "      9        \u001b[36m1.2596\u001b[0m        \u001b[32m1.5248\u001b[0m  0.4882\n",
      "      9        \u001b[36m1.0771\u001b[0m        1.1028  0.4792\n",
      "      7        1.1025        1.1326  0.4547\n",
      "      7        1.1078        0.8072  0.4475\n",
      "     10        1.2626        1.5250  0.4980\n",
      "Restoring best model from epoch 9.\n",
      "     10        \u001b[36m1.0751\u001b[0m        1.0993  0.4977\n",
      "Restoring best model from epoch 2.\n",
      "      8        \u001b[36m1.0938\u001b[0m        0.8078  0.4622\n",
      "      8        1.1105        1.1319  0.4645\n",
      "      9        1.1016        1.1304  0.4392\n",
      "      9        1.0954        0.8082  0.4440\n",
      "     10        \u001b[36m1.0940\u001b[0m        1.1298  0.4214\n",
      "Restoring best model from epoch 2.\n",
      "     10        \u001b[36m1.0790\u001b[0m        0.8077  0.4255\n",
      "Restoring best model from epoch 5.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m1.2847\u001b[0m        \u001b[32m1.1000\u001b[0m  0.1887\n",
      "      2        \u001b[36m1.2718\u001b[0m        \u001b[32m1.0956\u001b[0m  0.1709\n",
      "      3        1.2805        \u001b[32m1.0917\u001b[0m  0.1548\n",
      "      4        \u001b[36m1.2704\u001b[0m        \u001b[32m1.0914\u001b[0m  0.1633\n",
      "      5        1.2729        1.0916  0.1632\n",
      "      6        1.2706        1.0929  0.1610\n",
      "      7        \u001b[36m1.2461\u001b[0m        1.0924  0.1737\n",
      "      8        1.2588        1.0932  0.1647\n",
      "      9        1.2512        1.0977  0.1722\n",
      "     10        \u001b[36m1.2455\u001b[0m        1.1014  0.1741\n",
      "Restoring best model from epoch 4.\n",
      "y_train breslow final [-1.000e+00 -5.000e+00 -5.000e+00 -5.000e+00 -7.000e+00 -8.000e+00\n",
      " -9.000e+00 -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01\n",
      " -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01\n",
      " -1.000e+01 -1.000e+01 -1.600e+01 -1.900e+01 -2.400e+01 -3.000e+01\n",
      " -3.000e+01 -3.000e+01 -3.000e+01 -3.100e+01 -3.100e+01 -3.100e+01\n",
      " -3.100e+01 -3.400e+01 -4.000e+01 -4.900e+01 -5.100e+01 -5.400e+01\n",
      " -5.900e+01 -6.400e+01 -7.000e+01 -7.600e+01 -7.800e+01 -7.800e+01\n",
      " -7.800e+01 -8.000e+01 -8.400e+01 -9.000e+01 -9.200e+01  1.160e+02\n",
      " -1.180e+02 -1.490e+02  1.580e+02 -1.600e+02  1.600e+02 -1.620e+02\n",
      " -1.630e+02 -1.700e+02 -1.700e+02 -1.780e+02 -1.860e+02 -1.870e+02\n",
      " -1.960e+02  1.970e+02 -2.100e+02 -2.130e+02 -2.140e+02 -2.150e+02\n",
      " -2.160e+02 -2.170e+02 -2.180e+02 -2.220e+02  2.240e+02  2.270e+02\n",
      " -2.270e+02 -2.310e+02  2.390e+02 -2.420e+02 -2.420e+02 -2.500e+02\n",
      " -2.520e+02  2.550e+02 -2.660e+02 -2.710e+02 -2.730e+02 -2.730e+02\n",
      " -2.740e+02 -2.750e+02 -2.850e+02 -2.870e+02 -2.880e+02 -2.930e+02\n",
      " -2.930e+02  2.950e+02 -2.970e+02 -3.000e+02 -3.020e+02  3.020e+02\n",
      " -3.040e+02 -3.040e+02 -3.040e+02 -3.040e+02  3.040e+02 -3.070e+02\n",
      " -3.130e+02 -3.130e+02 -3.170e+02 -3.170e+02 -3.200e+02  3.220e+02\n",
      " -3.220e+02 -3.260e+02 -3.280e+02 -3.320e+02 -3.340e+02  3.360e+02\n",
      " -3.370e+02 -3.390e+02 -3.430e+02 -3.430e+02 -3.450e+02 -3.450e+02\n",
      " -3.470e+02  3.480e+02 -3.480e+02 -3.520e+02 -3.580e+02 -3.580e+02\n",
      " -3.580e+02  3.620e+02 -3.630e+02 -3.640e+02 -3.650e+02 -3.650e+02\n",
      " -3.650e+02  3.650e+02 -3.650e+02 -3.660e+02 -3.680e+02 -3.700e+02\n",
      " -3.710e+02 -3.730e+02 -3.730e+02 -3.750e+02 -3.750e+02 -3.750e+02\n",
      " -3.750e+02 -3.760e+02  3.770e+02 -3.800e+02 -3.810e+02 -3.810e+02\n",
      " -3.820e+02 -3.830e+02 -3.830e+02  3.850e+02 -3.850e+02 -3.850e+02\n",
      " -3.850e+02  3.850e+02 -3.850e+02 -3.920e+02 -3.930e+02 -3.930e+02\n",
      " -3.940e+02 -3.940e+02 -3.940e+02 -3.950e+02 -3.960e+02 -3.960e+02\n",
      " -3.960e+02 -3.960e+02 -3.960e+02 -3.970e+02 -3.980e+02 -4.020e+02\n",
      " -4.030e+02 -4.030e+02 -4.040e+02 -4.080e+02 -4.090e+02 -4.100e+02\n",
      " -4.100e+02 -4.110e+02 -4.120e+02 -4.140e+02 -4.160e+02 -4.170e+02\n",
      " -4.210e+02 -4.230e+02 -4.240e+02 -4.240e+02 -4.250e+02 -4.260e+02\n",
      "  4.260e+02 -4.260e+02 -4.280e+02 -4.280e+02 -4.300e+02 -4.310e+02\n",
      " -4.310e+02 -4.370e+02 -4.390e+02 -4.390e+02 -4.400e+02 -4.410e+02\n",
      " -4.410e+02 -4.460e+02 -4.470e+02 -4.480e+02 -4.500e+02 -4.500e+02\n",
      " -4.510e+02 -4.540e+02 -4.550e+02 -4.560e+02 -4.570e+02 -4.580e+02\n",
      " -4.610e+02 -4.610e+02 -4.630e+02 -4.630e+02 -4.660e+02 -4.670e+02\n",
      " -4.700e+02 -4.710e+02 -4.720e+02 -4.770e+02 -4.770e+02 -4.770e+02\n",
      " -4.800e+02 -4.820e+02 -4.880e+02 -4.890e+02 -4.920e+02 -4.950e+02\n",
      " -4.960e+02 -4.990e+02 -4.990e+02 -5.010e+02 -5.010e+02 -5.020e+02\n",
      " -5.030e+02 -5.040e+02 -5.040e+02 -5.060e+02 -5.080e+02 -5.090e+02\n",
      " -5.100e+02 -5.110e+02 -5.130e+02 -5.160e+02 -5.180e+02 -5.180e+02\n",
      " -5.190e+02 -5.190e+02 -5.230e+02  5.240e+02 -5.250e+02 -5.260e+02\n",
      " -5.280e+02 -5.280e+02 -5.300e+02 -5.310e+02 -5.320e+02 -5.320e+02\n",
      " -5.330e+02 -5.330e+02 -5.380e+02 -5.380e+02  5.380e+02 -5.410e+02\n",
      " -5.410e+02 -5.450e+02 -5.460e+02 -5.490e+02 -5.510e+02 -5.520e+02\n",
      " -5.520e+02 -5.520e+02 -5.540e+02 -5.540e+02 -5.540e+02 -5.540e+02\n",
      " -5.540e+02  5.580e+02 -5.620e+02 -5.620e+02 -5.630e+02  5.630e+02\n",
      " -5.650e+02 -5.660e+02 -5.670e+02 -5.680e+02  5.710e+02  5.730e+02\n",
      " -5.740e+02 -5.740e+02 -5.750e+02 -5.750e+02 -5.750e+02 -5.750e+02\n",
      " -5.760e+02 -5.770e+02 -5.770e+02 -5.840e+02 -5.840e+02 -5.840e+02\n",
      " -5.850e+02 -5.860e+02 -5.880e+02 -5.880e+02 -5.880e+02 -5.910e+02\n",
      " -5.940e+02 -5.950e+02 -5.960e+02 -6.000e+02 -6.020e+02 -6.060e+02\n",
      " -6.070e+02 -6.070e+02 -6.080e+02 -6.080e+02 -6.110e+02 -6.110e+02\n",
      " -6.120e+02  6.120e+02 -6.120e+02 -6.120e+02 -6.140e+02  6.140e+02\n",
      "  6.160e+02 -6.160e+02 -6.160e+02 -6.180e+02 -6.200e+02 -6.200e+02\n",
      " -6.200e+02 -6.220e+02 -6.240e+02 -6.270e+02 -6.290e+02 -6.300e+02\n",
      " -6.310e+02 -6.350e+02 -6.350e+02 -6.350e+02  6.390e+02 -6.400e+02\n",
      " -6.400e+02 -6.410e+02 -6.430e+02 -6.430e+02 -6.440e+02 -6.460e+02\n",
      " -6.470e+02 -6.480e+02 -6.510e+02 -6.520e+02 -6.550e+02 -6.580e+02\n",
      " -6.620e+02 -6.660e+02 -6.660e+02 -6.720e+02 -6.750e+02 -6.750e+02\n",
      " -6.770e+02 -6.770e+02 -6.790e+02 -6.800e+02 -6.830e+02 -6.940e+02\n",
      " -6.940e+02 -6.980e+02 -7.020e+02 -7.030e+02 -7.070e+02 -7.070e+02\n",
      " -7.090e+02 -7.100e+02 -7.140e+02 -7.140e+02 -7.150e+02 -7.150e+02\n",
      " -7.180e+02 -7.220e+02  7.230e+02 -7.240e+02 -7.250e+02 -7.270e+02\n",
      " -7.270e+02 -7.270e+02 -7.280e+02 -7.340e+02 -7.420e+02 -7.450e+02\n",
      " -7.470e+02 -7.470e+02 -7.470e+02 -7.480e+02  7.490e+02 -7.520e+02\n",
      " -7.520e+02  7.540e+02 -7.540e+02 -7.590e+02 -7.590e+02 -7.600e+02\n",
      " -7.610e+02 -7.620e+02 -7.620e+02 -7.620e+02  7.630e+02 -7.650e+02\n",
      " -7.670e+02 -7.670e+02 -7.690e+02 -7.770e+02 -7.850e+02  7.860e+02\n",
      " -7.880e+02 -7.910e+02  7.920e+02  7.920e+02 -7.950e+02 -7.980e+02\n",
      " -8.030e+02 -8.060e+02  8.110e+02 -8.120e+02 -8.190e+02 -8.200e+02\n",
      "  8.210e+02 -8.220e+02 -8.290e+02 -8.370e+02 -8.390e+02 -8.470e+02\n",
      " -8.490e+02 -8.520e+02 -8.560e+02 -8.600e+02  8.600e+02 -8.650e+02\n",
      " -8.670e+02 -8.750e+02 -8.780e+02  8.790e+02  8.830e+02 -8.890e+02\n",
      " -8.900e+02 -8.990e+02 -9.060e+02 -9.070e+02 -9.080e+02 -9.110e+02\n",
      "  9.120e+02 -9.120e+02 -9.120e+02 -9.150e+02 -9.180e+02 -9.230e+02\n",
      " -9.260e+02 -9.310e+02 -9.310e+02 -9.430e+02  9.430e+02 -9.430e+02\n",
      " -9.430e+02 -9.520e+02 -9.540e+02  9.590e+02  9.670e+02 -9.720e+02\n",
      " -9.730e+02 -9.740e+02 -9.740e+02 -9.750e+02  9.760e+02 -9.840e+02\n",
      " -9.870e+02 -9.890e+02 -9.900e+02  9.910e+02  9.910e+02 -9.960e+02\n",
      " -9.970e+02 -1.000e+03 -1.001e+03 -1.001e+03 -1.001e+03  1.004e+03\n",
      " -1.004e+03 -1.004e+03 -1.006e+03 -1.007e+03 -1.009e+03 -1.010e+03\n",
      " -1.013e+03 -1.015e+03 -1.026e+03 -1.026e+03  1.032e+03  1.034e+03\n",
      "  1.034e+03 -1.034e+03 -1.035e+03 -1.039e+03 -1.042e+03 -1.043e+03\n",
      " -1.043e+03 -1.043e+03 -1.047e+03  1.048e+03 -1.051e+03 -1.051e+03\n",
      " -1.059e+03 -1.062e+03 -1.062e+03 -1.066e+03 -1.074e+03 -1.079e+03\n",
      " -1.088e+03  1.093e+03 -1.099e+03 -1.102e+03  1.104e+03 -1.106e+03\n",
      " -1.112e+03 -1.119e+03 -1.120e+03 -1.120e+03  1.127e+03 -1.132e+03\n",
      " -1.133e+03 -1.138e+03 -1.139e+03 -1.140e+03 -1.141e+03  1.142e+03\n",
      " -1.148e+03  1.148e+03 -1.150e+03  1.152e+03 -1.156e+03 -1.157e+03\n",
      " -1.158e+03 -1.162e+03 -1.163e+03 -1.174e+03  1.174e+03 -1.179e+03\n",
      " -1.189e+03 -1.198e+03 -1.203e+03 -1.206e+03 -1.208e+03 -1.208e+03\n",
      " -1.210e+03 -1.219e+03 -1.220e+03 -1.220e+03 -1.224e+03 -1.229e+03\n",
      " -1.229e+03 -1.232e+03 -1.233e+03 -1.234e+03 -1.234e+03 -1.239e+03\n",
      " -1.247e+03 -1.251e+03 -1.266e+03 -1.270e+03  1.272e+03  1.275e+03\n",
      " -1.277e+03 -1.285e+03  1.286e+03 -1.288e+03 -1.301e+03 -1.308e+03\n",
      " -1.308e+03 -1.308e+03 -1.309e+03 -1.309e+03 -1.318e+03 -1.321e+03\n",
      "  1.324e+03 -1.325e+03 -1.326e+03 -1.330e+03 -1.351e+03 -1.363e+03\n",
      " -1.363e+03  1.365e+03 -1.369e+03 -1.371e+03  1.411e+03 -1.417e+03\n",
      " -1.417e+03 -1.419e+03  1.430e+03 -1.434e+03 -1.437e+03  1.439e+03\n",
      " -1.448e+03 -1.449e+03 -1.463e+03 -1.467e+03 -1.474e+03 -1.474e+03\n",
      " -1.476e+03 -1.477e+03 -1.492e+03 -1.505e+03  1.508e+03 -1.516e+03\n",
      " -1.519e+03 -1.522e+03 -1.523e+03 -1.528e+03 -1.532e+03 -1.535e+03\n",
      "  1.542e+03 -1.542e+03 -1.545e+03 -1.546e+03 -1.548e+03 -1.550e+03\n",
      "  1.556e+03  1.563e+03 -1.563e+03 -1.572e+03 -1.596e+03 -1.604e+03\n",
      " -1.611e+03 -1.611e+03 -1.612e+03 -1.613e+03 -1.614e+03 -1.616e+03\n",
      " -1.620e+03 -1.639e+03 -1.640e+03  1.642e+03 -1.642e+03 -1.662e+03\n",
      " -1.673e+03 -1.682e+03 -1.683e+03 -1.686e+03  1.688e+03  1.692e+03\n",
      " -1.692e+03  1.694e+03  1.699e+03 -1.712e+03 -1.728e+03 -1.728e+03\n",
      " -1.732e+03 -1.742e+03  1.759e+03 -1.780e+03  1.781e+03 -1.783e+03\n",
      "  1.793e+03 -1.800e+03  1.812e+03 -1.842e+03 -1.847e+03 -1.853e+03\n",
      " -1.855e+03 -1.873e+03 -1.882e+03  1.884e+03  1.900e+03 -1.914e+03\n",
      "  1.920e+03 -1.925e+03 -1.926e+03  1.927e+03 -1.928e+03 -1.935e+03\n",
      " -1.935e+03 -1.953e+03 -1.972e+03 -1.980e+03 -1.988e+03  1.993e+03\n",
      " -1.996e+03  2.009e+03 -2.041e+03 -2.048e+03 -2.064e+03 -2.072e+03\n",
      " -2.082e+03 -2.091e+03  2.097e+03 -2.108e+03 -2.109e+03 -2.124e+03\n",
      "  2.127e+03 -2.136e+03 -2.140e+03 -2.142e+03 -2.161e+03 -2.164e+03\n",
      " -2.190e+03 -2.191e+03  2.192e+03 -2.193e+03 -2.197e+03 -2.222e+03\n",
      " -2.231e+03 -2.236e+03 -2.240e+03 -2.246e+03 -2.248e+03 -2.255e+03\n",
      " -2.255e+03 -2.255e+03  2.273e+03 -2.278e+03 -2.281e+03 -2.282e+03\n",
      " -2.289e+03  2.296e+03 -2.306e+03 -2.329e+03 -2.330e+03 -2.335e+03\n",
      "  2.348e+03 -2.362e+03 -2.365e+03 -2.371e+03 -2.372e+03 -2.372e+03\n",
      "  2.373e+03 -2.381e+03 -2.385e+03 -2.403e+03 -2.406e+03  2.417e+03\n",
      " -2.442e+03 -2.442e+03 -2.483e+03 -2.486e+03 -2.489e+03 -2.513e+03\n",
      " -2.515e+03  2.534e+03 -2.535e+03 -2.558e+03  2.573e+03 -2.590e+03\n",
      " -2.596e+03 -2.605e+03 -2.612e+03 -2.618e+03 -2.629e+03 -2.632e+03\n",
      " -2.632e+03  2.636e+03 -2.645e+03 -2.645e+03 -2.654e+03 -2.695e+03\n",
      " -2.707e+03 -2.709e+03  2.712e+03 -2.721e+03 -2.759e+03 -2.767e+03\n",
      " -2.770e+03  2.798e+03  2.854e+03 -2.856e+03 -2.868e+03  2.911e+03\n",
      " -2.920e+03 -2.953e+03 -2.976e+03 -2.989e+03 -2.991e+03 -2.991e+03\n",
      " -3.004e+03 -3.009e+03 -3.011e+03 -3.015e+03 -3.017e+03 -3.021e+03\n",
      " -3.022e+03 -3.030e+03  3.063e+03 -3.072e+03 -3.088e+03 -3.091e+03\n",
      " -3.094e+03 -3.102e+03 -3.112e+03 -3.121e+03  3.126e+03 -3.159e+03\n",
      " -3.202e+03 -3.204e+03 -3.247e+03 -3.248e+03 -3.261e+03  3.262e+03\n",
      " -3.276e+03 -3.283e+03 -3.286e+03 -3.287e+03 -3.296e+03 -3.307e+03\n",
      " -3.316e+03 -3.342e+03 -3.361e+03  3.409e+03  3.418e+03 -3.456e+03\n",
      "  3.472e+03  3.492e+03 -3.506e+03 -3.519e+03 -3.603e+03 -3.607e+03\n",
      " -3.660e+03 -3.709e+03  3.736e+03 -3.807e+03  3.873e+03  3.926e+03\n",
      "  3.941e+03  3.945e+03  3.959e+03 -4.005e+03 -4.047e+03 -4.052e+03\n",
      " -4.088e+03 -4.159e+03 -4.233e+03  4.267e+03 -4.275e+03 -4.354e+03\n",
      " -4.361e+03  4.456e+03 -4.894e+03 -4.929e+03 -5.042e+03 -5.062e+03\n",
      " -5.156e+03 -5.176e+03 -5.739e+03 -6.292e+03  6.456e+03  6.593e+03\n",
      " -7.106e+03 -7.126e+03  7.455e+03 -7.777e+03 -8.008e+03 -8.556e+03\n",
      " -8.605e+03]\n",
      "Concordance Index 0.6033159275212359\n",
      "Integrated Brier Score: 0.260299910737358\n",
      "y_train breslow final [-1.000e+00 -5.000e+00 -5.000e+00 -5.000e+00 -7.000e+00 -8.000e+00\n",
      " -9.000e+00 -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01\n",
      " -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01\n",
      " -1.000e+01 -1.000e+01 -1.600e+01 -1.900e+01 -2.400e+01 -3.000e+01\n",
      " -3.000e+01 -3.000e+01 -3.000e+01 -3.100e+01 -3.100e+01 -3.100e+01\n",
      " -3.100e+01 -3.400e+01 -4.000e+01 -4.900e+01 -5.100e+01 -5.400e+01\n",
      " -5.900e+01 -6.400e+01 -7.000e+01 -7.600e+01 -7.800e+01 -7.800e+01\n",
      " -7.800e+01 -8.000e+01 -8.400e+01 -9.000e+01 -9.200e+01  1.160e+02\n",
      " -1.180e+02 -1.490e+02  1.580e+02 -1.600e+02  1.600e+02 -1.620e+02\n",
      " -1.630e+02 -1.700e+02 -1.700e+02 -1.780e+02 -1.860e+02 -1.870e+02\n",
      " -1.960e+02  1.970e+02 -2.100e+02 -2.130e+02 -2.140e+02 -2.150e+02\n",
      " -2.160e+02 -2.170e+02 -2.180e+02 -2.220e+02  2.240e+02  2.270e+02\n",
      " -2.270e+02 -2.310e+02  2.390e+02 -2.420e+02 -2.420e+02 -2.500e+02\n",
      " -2.520e+02  2.550e+02 -2.660e+02 -2.710e+02 -2.730e+02 -2.730e+02\n",
      " -2.740e+02 -2.750e+02 -2.850e+02 -2.870e+02 -2.880e+02 -2.930e+02\n",
      " -2.930e+02  2.950e+02 -2.970e+02 -3.000e+02 -3.020e+02  3.020e+02\n",
      " -3.040e+02 -3.040e+02 -3.040e+02 -3.040e+02  3.040e+02 -3.070e+02\n",
      " -3.130e+02 -3.130e+02 -3.170e+02 -3.170e+02 -3.200e+02  3.220e+02\n",
      " -3.220e+02 -3.260e+02 -3.280e+02 -3.320e+02 -3.340e+02  3.360e+02\n",
      " -3.370e+02 -3.390e+02 -3.430e+02 -3.430e+02 -3.450e+02 -3.450e+02\n",
      " -3.470e+02  3.480e+02 -3.480e+02 -3.520e+02 -3.580e+02 -3.580e+02\n",
      " -3.580e+02  3.620e+02 -3.630e+02 -3.640e+02 -3.650e+02 -3.650e+02\n",
      " -3.650e+02  3.650e+02 -3.650e+02 -3.660e+02 -3.680e+02 -3.700e+02\n",
      " -3.710e+02 -3.730e+02 -3.730e+02 -3.750e+02 -3.750e+02 -3.750e+02\n",
      " -3.750e+02 -3.760e+02  3.770e+02 -3.800e+02 -3.810e+02 -3.810e+02\n",
      " -3.820e+02 -3.830e+02 -3.830e+02  3.850e+02 -3.850e+02 -3.850e+02\n",
      " -3.850e+02  3.850e+02 -3.850e+02 -3.920e+02 -3.930e+02 -3.930e+02\n",
      " -3.940e+02 -3.940e+02 -3.940e+02 -3.950e+02 -3.960e+02 -3.960e+02\n",
      " -3.960e+02 -3.960e+02 -3.960e+02 -3.970e+02 -3.980e+02 -4.020e+02\n",
      " -4.030e+02 -4.030e+02 -4.040e+02 -4.080e+02 -4.090e+02 -4.100e+02\n",
      " -4.100e+02 -4.110e+02 -4.120e+02 -4.140e+02 -4.160e+02 -4.170e+02\n",
      " -4.210e+02 -4.230e+02 -4.240e+02 -4.240e+02 -4.250e+02 -4.260e+02\n",
      "  4.260e+02 -4.260e+02 -4.280e+02 -4.280e+02 -4.300e+02 -4.310e+02\n",
      " -4.310e+02 -4.370e+02 -4.390e+02 -4.390e+02 -4.400e+02 -4.410e+02\n",
      " -4.410e+02 -4.460e+02 -4.470e+02 -4.480e+02 -4.500e+02 -4.500e+02\n",
      " -4.510e+02 -4.540e+02 -4.550e+02 -4.560e+02 -4.570e+02 -4.580e+02\n",
      " -4.610e+02 -4.610e+02 -4.630e+02 -4.630e+02 -4.660e+02 -4.670e+02\n",
      " -4.700e+02 -4.710e+02 -4.720e+02 -4.770e+02 -4.770e+02 -4.770e+02\n",
      " -4.800e+02 -4.820e+02 -4.880e+02 -4.890e+02 -4.920e+02 -4.950e+02\n",
      " -4.960e+02 -4.990e+02 -4.990e+02 -5.010e+02 -5.010e+02 -5.020e+02\n",
      " -5.030e+02 -5.040e+02 -5.040e+02 -5.060e+02 -5.080e+02 -5.090e+02\n",
      " -5.100e+02 -5.110e+02 -5.130e+02 -5.160e+02 -5.180e+02 -5.180e+02\n",
      " -5.190e+02 -5.190e+02 -5.230e+02  5.240e+02 -5.250e+02 -5.260e+02\n",
      " -5.280e+02 -5.280e+02 -5.300e+02 -5.310e+02 -5.320e+02 -5.320e+02\n",
      " -5.330e+02 -5.330e+02 -5.380e+02 -5.380e+02  5.380e+02 -5.410e+02\n",
      " -5.410e+02 -5.450e+02 -5.460e+02 -5.490e+02 -5.510e+02 -5.520e+02\n",
      " -5.520e+02 -5.520e+02 -5.540e+02 -5.540e+02 -5.540e+02 -5.540e+02\n",
      " -5.540e+02  5.580e+02 -5.620e+02 -5.620e+02 -5.630e+02  5.630e+02\n",
      " -5.650e+02 -5.660e+02 -5.670e+02 -5.680e+02  5.710e+02  5.730e+02\n",
      " -5.740e+02 -5.740e+02 -5.750e+02 -5.750e+02 -5.750e+02 -5.750e+02\n",
      " -5.760e+02 -5.770e+02 -5.770e+02 -5.840e+02 -5.840e+02 -5.840e+02\n",
      " -5.850e+02 -5.860e+02 -5.880e+02 -5.880e+02 -5.880e+02 -5.910e+02\n",
      " -5.940e+02 -5.950e+02 -5.960e+02 -6.000e+02 -6.020e+02 -6.060e+02\n",
      " -6.070e+02 -6.070e+02 -6.080e+02 -6.080e+02 -6.110e+02 -6.110e+02\n",
      " -6.120e+02  6.120e+02 -6.120e+02 -6.120e+02 -6.140e+02  6.140e+02\n",
      "  6.160e+02 -6.160e+02 -6.160e+02 -6.180e+02 -6.200e+02 -6.200e+02\n",
      " -6.200e+02 -6.220e+02 -6.240e+02 -6.270e+02 -6.290e+02 -6.300e+02\n",
      " -6.310e+02 -6.350e+02 -6.350e+02 -6.350e+02  6.390e+02 -6.400e+02\n",
      " -6.400e+02 -6.410e+02 -6.430e+02 -6.430e+02 -6.440e+02 -6.460e+02\n",
      " -6.470e+02 -6.480e+02 -6.510e+02 -6.520e+02 -6.550e+02 -6.580e+02\n",
      " -6.620e+02 -6.660e+02 -6.660e+02 -6.720e+02 -6.750e+02 -6.750e+02\n",
      " -6.770e+02 -6.770e+02 -6.790e+02 -6.800e+02 -6.830e+02 -6.940e+02\n",
      " -6.940e+02 -6.980e+02 -7.020e+02 -7.030e+02 -7.070e+02 -7.070e+02\n",
      " -7.090e+02 -7.100e+02 -7.140e+02 -7.140e+02 -7.150e+02 -7.150e+02\n",
      " -7.180e+02 -7.220e+02  7.230e+02 -7.240e+02 -7.250e+02 -7.270e+02\n",
      " -7.270e+02 -7.270e+02 -7.280e+02 -7.340e+02 -7.420e+02 -7.450e+02\n",
      " -7.470e+02 -7.470e+02 -7.470e+02 -7.480e+02  7.490e+02 -7.520e+02\n",
      " -7.520e+02  7.540e+02 -7.540e+02 -7.590e+02 -7.590e+02 -7.600e+02\n",
      " -7.610e+02 -7.620e+02 -7.620e+02 -7.620e+02  7.630e+02 -7.650e+02\n",
      " -7.670e+02 -7.670e+02 -7.690e+02 -7.770e+02 -7.850e+02  7.860e+02\n",
      " -7.880e+02 -7.910e+02  7.920e+02  7.920e+02 -7.950e+02 -7.980e+02\n",
      " -8.030e+02 -8.060e+02  8.110e+02 -8.120e+02 -8.190e+02 -8.200e+02\n",
      "  8.210e+02 -8.220e+02 -8.290e+02 -8.370e+02 -8.390e+02 -8.470e+02\n",
      " -8.490e+02 -8.520e+02 -8.560e+02 -8.600e+02  8.600e+02 -8.650e+02\n",
      " -8.670e+02 -8.750e+02 -8.780e+02  8.790e+02  8.830e+02 -8.890e+02\n",
      " -8.900e+02 -8.990e+02 -9.060e+02 -9.070e+02 -9.080e+02 -9.110e+02\n",
      "  9.120e+02 -9.120e+02 -9.120e+02 -9.150e+02 -9.180e+02 -9.230e+02\n",
      " -9.260e+02 -9.310e+02 -9.310e+02 -9.430e+02  9.430e+02 -9.430e+02\n",
      " -9.430e+02 -9.520e+02 -9.540e+02  9.590e+02  9.670e+02 -9.720e+02\n",
      " -9.730e+02 -9.740e+02 -9.740e+02 -9.750e+02  9.760e+02 -9.840e+02\n",
      " -9.870e+02 -9.890e+02 -9.900e+02  9.910e+02  9.910e+02 -9.960e+02\n",
      " -9.970e+02 -1.000e+03 -1.001e+03 -1.001e+03 -1.001e+03  1.004e+03\n",
      " -1.004e+03 -1.004e+03 -1.006e+03 -1.007e+03 -1.009e+03 -1.010e+03\n",
      " -1.013e+03 -1.015e+03 -1.026e+03 -1.026e+03  1.032e+03  1.034e+03\n",
      "  1.034e+03 -1.034e+03 -1.035e+03 -1.039e+03 -1.042e+03 -1.043e+03\n",
      " -1.043e+03 -1.043e+03 -1.047e+03  1.048e+03 -1.051e+03 -1.051e+03\n",
      " -1.059e+03 -1.062e+03 -1.062e+03 -1.066e+03 -1.074e+03 -1.079e+03\n",
      " -1.088e+03  1.093e+03 -1.099e+03 -1.102e+03  1.104e+03 -1.106e+03\n",
      " -1.112e+03 -1.119e+03 -1.120e+03 -1.120e+03  1.127e+03 -1.132e+03\n",
      " -1.133e+03 -1.138e+03 -1.139e+03 -1.140e+03 -1.141e+03  1.142e+03\n",
      " -1.148e+03  1.148e+03 -1.150e+03  1.152e+03 -1.156e+03 -1.157e+03\n",
      " -1.158e+03 -1.162e+03 -1.163e+03 -1.174e+03  1.174e+03 -1.179e+03\n",
      " -1.189e+03 -1.198e+03 -1.203e+03 -1.206e+03 -1.208e+03 -1.208e+03\n",
      " -1.210e+03 -1.219e+03 -1.220e+03 -1.220e+03 -1.224e+03 -1.229e+03\n",
      " -1.229e+03 -1.232e+03 -1.233e+03 -1.234e+03 -1.234e+03 -1.239e+03\n",
      " -1.247e+03 -1.251e+03 -1.266e+03 -1.270e+03  1.272e+03  1.275e+03\n",
      " -1.277e+03 -1.285e+03  1.286e+03 -1.288e+03 -1.301e+03 -1.308e+03\n",
      " -1.308e+03 -1.308e+03 -1.309e+03 -1.309e+03 -1.318e+03 -1.321e+03\n",
      "  1.324e+03 -1.325e+03 -1.326e+03 -1.330e+03 -1.351e+03 -1.363e+03\n",
      " -1.363e+03  1.365e+03 -1.369e+03 -1.371e+03  1.411e+03 -1.417e+03\n",
      " -1.417e+03 -1.419e+03  1.430e+03 -1.434e+03 -1.437e+03  1.439e+03\n",
      " -1.448e+03 -1.449e+03 -1.463e+03 -1.467e+03 -1.474e+03 -1.474e+03\n",
      " -1.476e+03 -1.477e+03 -1.492e+03 -1.505e+03  1.508e+03 -1.516e+03\n",
      " -1.519e+03 -1.522e+03 -1.523e+03 -1.528e+03 -1.532e+03 -1.535e+03\n",
      "  1.542e+03 -1.542e+03 -1.545e+03 -1.546e+03 -1.548e+03 -1.550e+03\n",
      "  1.556e+03  1.563e+03 -1.563e+03 -1.572e+03 -1.596e+03 -1.604e+03\n",
      " -1.611e+03 -1.611e+03 -1.612e+03 -1.613e+03 -1.614e+03 -1.616e+03\n",
      " -1.620e+03 -1.639e+03 -1.640e+03  1.642e+03 -1.642e+03 -1.662e+03\n",
      " -1.673e+03 -1.682e+03 -1.683e+03 -1.686e+03  1.688e+03  1.692e+03\n",
      " -1.692e+03  1.694e+03  1.699e+03 -1.712e+03 -1.728e+03 -1.728e+03\n",
      " -1.732e+03 -1.742e+03  1.759e+03 -1.780e+03  1.781e+03 -1.783e+03\n",
      "  1.793e+03 -1.800e+03  1.812e+03 -1.842e+03 -1.847e+03 -1.853e+03\n",
      " -1.855e+03 -1.873e+03 -1.882e+03  1.884e+03  1.900e+03 -1.914e+03\n",
      "  1.920e+03 -1.925e+03 -1.926e+03  1.927e+03 -1.928e+03 -1.935e+03\n",
      " -1.935e+03 -1.953e+03 -1.972e+03 -1.980e+03 -1.988e+03  1.993e+03\n",
      " -1.996e+03  2.009e+03 -2.041e+03 -2.048e+03 -2.064e+03 -2.072e+03\n",
      " -2.082e+03 -2.091e+03  2.097e+03 -2.108e+03 -2.109e+03 -2.124e+03\n",
      "  2.127e+03 -2.136e+03 -2.140e+03 -2.142e+03 -2.161e+03 -2.164e+03\n",
      " -2.190e+03 -2.191e+03  2.192e+03 -2.193e+03 -2.197e+03 -2.222e+03\n",
      " -2.231e+03 -2.236e+03 -2.240e+03 -2.246e+03 -2.248e+03 -2.255e+03\n",
      " -2.255e+03 -2.255e+03  2.273e+03 -2.278e+03 -2.281e+03 -2.282e+03\n",
      " -2.289e+03  2.296e+03 -2.306e+03 -2.329e+03 -2.330e+03 -2.335e+03\n",
      "  2.348e+03 -2.362e+03 -2.365e+03 -2.371e+03 -2.372e+03 -2.372e+03\n",
      "  2.373e+03 -2.381e+03 -2.385e+03 -2.403e+03 -2.406e+03  2.417e+03\n",
      " -2.442e+03 -2.442e+03 -2.483e+03 -2.486e+03 -2.489e+03 -2.513e+03\n",
      " -2.515e+03  2.534e+03 -2.535e+03 -2.558e+03  2.573e+03 -2.590e+03\n",
      " -2.596e+03 -2.605e+03 -2.612e+03 -2.618e+03 -2.629e+03 -2.632e+03\n",
      " -2.632e+03  2.636e+03 -2.645e+03 -2.645e+03 -2.654e+03 -2.695e+03\n",
      " -2.707e+03 -2.709e+03  2.712e+03 -2.721e+03 -2.759e+03 -2.767e+03\n",
      " -2.770e+03  2.798e+03  2.854e+03 -2.856e+03 -2.868e+03  2.911e+03\n",
      " -2.920e+03 -2.953e+03 -2.976e+03 -2.989e+03 -2.991e+03 -2.991e+03\n",
      " -3.004e+03 -3.009e+03 -3.011e+03 -3.015e+03 -3.017e+03 -3.021e+03\n",
      " -3.022e+03 -3.030e+03  3.063e+03 -3.072e+03 -3.088e+03 -3.091e+03\n",
      " -3.094e+03 -3.102e+03 -3.112e+03 -3.121e+03  3.126e+03 -3.159e+03\n",
      " -3.202e+03 -3.204e+03 -3.247e+03 -3.248e+03 -3.261e+03  3.262e+03\n",
      " -3.276e+03 -3.283e+03 -3.286e+03 -3.287e+03 -3.296e+03 -3.307e+03\n",
      " -3.316e+03 -3.342e+03 -3.361e+03  3.409e+03  3.418e+03 -3.456e+03\n",
      "  3.472e+03  3.492e+03 -3.506e+03 -3.519e+03 -3.603e+03 -3.607e+03\n",
      " -3.660e+03 -3.709e+03  3.736e+03 -3.807e+03  3.873e+03  3.926e+03\n",
      "  3.941e+03  3.945e+03  3.959e+03 -4.005e+03 -4.047e+03 -4.052e+03\n",
      " -4.088e+03 -4.159e+03 -4.233e+03  4.267e+03 -4.275e+03 -4.354e+03\n",
      " -4.361e+03  4.456e+03 -4.894e+03 -4.929e+03 -5.042e+03 -5.062e+03\n",
      " -5.156e+03 -5.176e+03 -5.739e+03 -6.292e+03  6.456e+03  6.593e+03\n",
      " -7.106e+03 -7.126e+03  7.455e+03 -7.777e+03 -8.008e+03 -8.556e+03\n",
      " -8.605e+03]\n",
      "durations 1.0 8391.0\n",
      "Concordance Index 0.5494333724110981\n",
      "Integrated Brier Score: 0.20598709267221646\n",
      "split gex_?|100130426      float32\n",
      "gex_?|100133144      float32\n",
      "gex_?|100134869      float32\n",
      "gex_?|10357          float32\n",
      "gex_?|10431          float32\n",
      "                      ...   \n",
      "gex_ZYG11A|440590    float32\n",
      "gex_ZYG11B|79699     float32\n",
      "gex_ZYX|7791         float32\n",
      "gex_ZZEF1|23140      float32\n",
      "gex_ZZZ3|26009       float32\n",
      "Length: 20531, dtype: object\n",
      "(415, 20531) <class 'pandas.core.frame.DataFrame'>\n",
      "(415,) <class 'pandas.core.series.Series'>\n",
      "(104, 20531) <class 'pandas.core.frame.DataFrame'>\n",
      "(104,) <class 'pandas.core.series.Series'>\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.7605\u001b[0m        \u001b[32m2.5238\u001b[0m  0.2973\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.1085\u001b[0m        \u001b[32m3.8383\u001b[0m  0.3414\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.3965\u001b[0m        \u001b[32m3.5326\u001b[0m  0.3502\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.1086\u001b[0m        \u001b[32m3.8257\u001b[0m  0.3363\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.0833\u001b[0m        \u001b[32m3.3333\u001b[0m  0.3359\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.3851\u001b[0m        \u001b[32m3.5026\u001b[0m  0.3364\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.3020\u001b[0m        \u001b[32m4.1044\u001b[0m  0.3492\n",
      "      2        2.7680        \u001b[32m2.5185\u001b[0m  0.2983\n",
      "      2        \u001b[36m3.0891\u001b[0m        3.8358  0.2650\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.0896\u001b[0m        \u001b[32m3.3388\u001b[0m  0.4504\n",
      "      2        \u001b[36m3.0547\u001b[0m        3.8459  0.3736\n",
      "      2        \u001b[36m3.0677\u001b[0m        \u001b[32m3.3293\u001b[0m  0.3652\n",
      "      2        \u001b[36m3.3630\u001b[0m        \u001b[32m3.5286\u001b[0m  0.4698\n",
      "      2        \u001b[36m3.3692\u001b[0m        \u001b[32m3.4875\u001b[0m  0.3668\n",
      "      2        \u001b[36m3.2925\u001b[0m        4.1223  0.3338\n",
      "      3        \u001b[36m3.0739\u001b[0m        3.8430  0.3832\n",
      "      2        \u001b[36m3.0755\u001b[0m        \u001b[32m3.3345\u001b[0m  0.3773\n",
      "      3        3.0693        \u001b[32m3.3237\u001b[0m  0.3085\n",
      "      3        \u001b[36m3.2905\u001b[0m        4.1199  0.3029\n",
      "      3        2.7669        \u001b[32m2.5101\u001b[0m  0.5730\n",
      "      3        3.0640        3.8501  0.5247\n",
      "      3        \u001b[36m3.3636\u001b[0m        3.4903  0.4279\n",
      "      3        \u001b[36m3.3271\u001b[0m        3.5303  0.4955\n",
      "      4        3.0821        3.8483  0.3765\n",
      "      4        \u001b[36m3.0561\u001b[0m        \u001b[32m3.3160\u001b[0m  0.3385\n",
      "      4        \u001b[36m2.7424\u001b[0m        \u001b[32m2.5021\u001b[0m  0.3718\n",
      "      4        \u001b[36m3.2589\u001b[0m        4.1094  0.4057\n",
      "      4        \u001b[36m3.3515\u001b[0m        3.4879  0.2963\n",
      "      3        \u001b[36m3.0527\u001b[0m        \u001b[32m3.3234\u001b[0m  0.4774\n",
      "      4        \u001b[36m3.0453\u001b[0m        3.8556  0.3304\n",
      "      4        \u001b[36m3.3121\u001b[0m        3.5322  0.3466\n",
      "      5        3.0841        \u001b[32m3.3135\u001b[0m  0.3016\n",
      "      5        2.7568        \u001b[32m2.4989\u001b[0m  0.3006\n",
      "      5        \u001b[36m3.0613\u001b[0m        3.8477  0.4086\n",
      "      5        \u001b[36m3.2540\u001b[0m        4.1053  0.3201\n",
      "      4        \u001b[36m3.0281\u001b[0m        \u001b[32m3.3189\u001b[0m  0.3288\n",
      "      5        3.3572        3.4937  0.3471\n",
      "      5        \u001b[36m2.9969\u001b[0m        3.8554  0.3735\n",
      "      5        3.3247        3.5328  0.3946\n",
      "      6        3.0623        3.3141  0.3469\n",
      "      6        \u001b[36m3.0574\u001b[0m        3.8480  0.2700\n",
      "      5        \u001b[36m3.0013\u001b[0m        \u001b[32m3.3136\u001b[0m  0.3205\n",
      "      6        \u001b[36m3.2388\u001b[0m        \u001b[32m4.1033\u001b[0m  0.3469\n",
      "      6        \u001b[36m2.7314\u001b[0m        \u001b[32m2.4966\u001b[0m  0.3774\n",
      "      6        \u001b[36m3.3503\u001b[0m        3.4970  0.3436\n",
      "      6        3.0102        3.8512  0.3278\n",
      "      6        \u001b[36m3.3039\u001b[0m        \u001b[32m3.5283\u001b[0m  0.2997\n",
      "      7        3.0666        3.8477  0.2544\n",
      "      7        \u001b[36m3.0511\u001b[0m        3.3159  0.2799\n",
      "      6        3.0080        \u001b[32m3.3083\u001b[0m  0.2618\n",
      "      7        3.3593        3.4992  0.2548\n",
      "      7        3.2689        \u001b[32m4.1031\u001b[0m  0.2806\n",
      "      7        2.7531        2.4968  0.2797\n",
      "      7        3.0055        3.8487  0.2795\n",
      "      7        \u001b[36m3.2920\u001b[0m        \u001b[32m3.5216\u001b[0m  0.2790\n",
      "      8        \u001b[36m3.0520\u001b[0m        3.8439  0.2732\n",
      "      8        3.0656        3.3162  0.2743\n",
      "      7        \u001b[36m2.9951\u001b[0m        \u001b[32m3.3006\u001b[0m  0.3076\n",
      "      8        3.2507        \u001b[32m4.1012\u001b[0m  0.2979\n",
      "      8        \u001b[36m3.3414\u001b[0m        3.4962  0.3129\n",
      "      8        2.7319        2.4990  0.3146\n",
      "      8        \u001b[36m2.9924\u001b[0m        3.8507  0.3153\n",
      "      9        \u001b[36m3.0107\u001b[0m        3.8409  0.2830\n",
      "      8        3.3010        \u001b[32m3.5147\u001b[0m  0.3001\n",
      "      9        \u001b[36m3.0365\u001b[0m        3.3152  0.3489\n",
      "      9        \u001b[36m3.2185\u001b[0m        \u001b[32m4.0995\u001b[0m  0.2758\n",
      "      9        \u001b[36m3.3289\u001b[0m        3.4927  0.3148\n",
      "      8        3.0022        \u001b[32m3.2918\u001b[0m  0.3450\n",
      "      9        2.7346        2.4977  0.3155\n",
      "      9        \u001b[36m2.9142\u001b[0m        3.8536  0.3840\n",
      "      9        \u001b[36m3.2576\u001b[0m        \u001b[32m3.5079\u001b[0m  0.3598\n",
      "     10        3.0145        3.8360  0.3979\n",
      "Restoring best model from epoch 1.\n",
      "     10        3.0417        3.3154  0.3506\n",
      "     10        \u001b[36m3.2082\u001b[0m        \u001b[32m4.0960\u001b[0m  0.2914\n",
      "Restoring best model from epoch 5.\n",
      "     10        2.7431        \u001b[32m2.4958\u001b[0m  0.3782\n",
      "     10        \u001b[36m3.3191\u001b[0m        3.4878  0.4115\n",
      "Restoring best model from epoch 2.\n",
      "      9        2.9990        \u001b[32m3.2868\u001b[0m  0.4092\n",
      "     10        2.9516        3.8528  0.3315\n",
      "Restoring best model from epoch 1.\n",
      "     10        \u001b[36m3.2412\u001b[0m        \u001b[32m3.4994\u001b[0m  0.3568\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "     10        \u001b[36m2.9739\u001b[0m        \u001b[32m3.2855\u001b[0m  0.2637\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.3016\u001b[0m        \u001b[32m4.1400\u001b[0m  0.2676\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.7803\u001b[0m        \u001b[32m2.5247\u001b[0m  0.2635\n",
      "      2        \u001b[36m2.7674\u001b[0m        \u001b[32m2.4974\u001b[0m  0.2248\n",
      "      2        \u001b[36m3.2600\u001b[0m        \u001b[32m4.1386\u001b[0m  0.2303\n",
      "      3        3.2663        \u001b[32m4.1386\u001b[0m  0.2169\n",
      "      3        \u001b[36m2.7521\u001b[0m        \u001b[32m2.4732\u001b[0m  0.2236\n",
      "      4        \u001b[36m3.2047\u001b[0m        4.1423  0.2148\n",
      "      4        2.7561        \u001b[32m2.4644\u001b[0m  0.2206\n",
      "      5        3.2144        4.1432  0.2114\n",
      "      5        \u001b[36m2.7462\u001b[0m        \u001b[32m2.4631\u001b[0m  0.2246\n",
      "      6        \u001b[36m3.1668\u001b[0m        4.1427  0.2110\n",
      "      6        2.7511        2.4677  0.2226\n",
      "      7        \u001b[36m3.1450\u001b[0m        4.1388  0.2126\n",
      "      7        \u001b[36m2.7147\u001b[0m        2.4694  0.2211\n",
      "      8        3.1691        \u001b[32m4.1380\u001b[0m  0.2114\n",
      "      8        \u001b[36m2.7022\u001b[0m        2.4716  0.2262\n",
      "      9        \u001b[36m3.1199\u001b[0m        \u001b[32m4.1359\u001b[0m  0.3580\n",
      "      9        2.7159        2.4738  0.3641\n",
      "     10        \u001b[36m3.1109\u001b[0m        \u001b[32m4.1311\u001b[0m  0.2401\n",
      "     10        \u001b[36m2.6957\u001b[0m        2.4763  0.2241\n",
      "Restoring best model from epoch 5.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.1753\u001b[0m        \u001b[32m3.5180\u001b[0m  0.2417\n",
      "      2        \u001b[36m3.1699\u001b[0m        3.5211  0.0827\n",
      "      3        \u001b[36m3.1639\u001b[0m        3.5226  0.1609\n",
      "      4        \u001b[36m3.1575\u001b[0m        \u001b[32m3.5149\u001b[0m  0.2367\n",
      "      5        3.1637        \u001b[32m3.5055\u001b[0m  0.1445\n",
      "      6        3.1579        \u001b[32m3.4990\u001b[0m  0.1143\n",
      "      7        \u001b[36m3.1462\u001b[0m        \u001b[32m3.4920\u001b[0m  0.0969\n",
      "      8        3.1498        \u001b[32m3.4868\u001b[0m  0.0950\n",
      "      9        \u001b[36m3.1227\u001b[0m        \u001b[32m3.4819\u001b[0m  0.0837\n",
      "     10        3.1463        \u001b[32m3.4780\u001b[0m  0.0949\n",
      "y_train breslow final [ 2.000e+00  2.000e+00 -1.100e+01  1.100e+01 -1.400e+01  1.400e+01\n",
      "  3.200e+01  5.600e+01 -5.600e+01  6.200e+01  6.400e+01  6.500e+01\n",
      "  6.900e+01 -7.700e+01  8.200e+01  8.900e+01  9.000e+01 -9.200e+01\n",
      "  9.400e+01 -1.020e+02 -1.050e+02  1.070e+02  1.080e+02 -1.110e+02\n",
      "  1.120e+02  1.260e+02  1.290e+02  1.300e+02 -1.340e+02  1.420e+02\n",
      "  1.440e+02  1.510e+02 -1.560e+02  1.600e+02  1.660e+02  1.710e+02\n",
      "  1.730e+02 -1.810e+02  1.820e+02  1.830e+02  1.850e+02 -1.870e+02\n",
      "  1.910e+02  1.940e+02  1.940e+02  1.970e+02  1.980e+02  2.050e+02\n",
      "  2.090e+02  2.150e+02  2.150e+02 -2.160e+02  2.170e+02  2.170e+02\n",
      "  2.180e+02  2.220e+02 -2.250e+02 -2.320e+02 -2.340e+02 -2.420e+02\n",
      "  2.430e+02 -2.530e+02  2.530e+02  2.610e+02  2.680e+02  2.760e+02\n",
      " -2.780e+02  2.790e+02  2.810e+02  2.820e+02  2.840e+02  2.920e+02\n",
      "  2.940e+02  2.950e+02 -3.110e+02  3.170e+02  3.170e+02 -3.180e+02\n",
      "  3.210e+02  3.270e+02  3.270e+02  3.420e+02 -3.450e+02  3.480e+02\n",
      " -3.510e+02  3.510e+02  3.520e+02  3.530e+02 -3.540e+02  3.570e+02\n",
      " -3.580e+02  3.590e+02 -3.600e+02  3.610e+02  3.660e+02 -3.670e+02\n",
      " -3.680e+02  3.710e+02 -3.760e+02  3.770e+02 -3.780e+02  3.790e+02\n",
      " -3.790e+02  3.800e+02 -3.840e+02  3.850e+02 -3.860e+02  3.870e+02\n",
      " -3.870e+02 -3.890e+02 -3.920e+02 -3.920e+02  3.930e+02 -3.930e+02\n",
      "  3.950e+02  3.950e+02  3.970e+02 -4.000e+02 -4.030e+02  4.030e+02\n",
      "  4.030e+02  4.060e+02  4.070e+02 -4.120e+02 -4.140e+02 -4.170e+02\n",
      "  4.210e+02 -4.210e+02 -4.240e+02  4.240e+02  4.360e+02 -4.400e+02\n",
      " -4.410e+02 -4.430e+02  4.460e+02  4.530e+02  4.560e+02 -4.580e+02\n",
      "  4.590e+02 -4.610e+02 -4.660e+02 -4.690e+02 -4.710e+02  4.720e+02\n",
      "  4.790e+02 -4.790e+02  4.800e+02  4.840e+02 -4.850e+02  4.870e+02\n",
      " -4.910e+02  4.930e+02  4.950e+02 -4.990e+02  5.060e+02 -5.090e+02\n",
      " -5.160e+02 -5.180e+02 -5.200e+02  5.210e+02  5.210e+02  5.220e+02\n",
      "  5.260e+02  5.300e+02 -5.390e+02 -5.420e+02 -5.430e+02  5.430e+02\n",
      "  5.450e+02  5.460e+02  5.460e+02 -5.470e+02 -5.490e+02 -5.520e+02\n",
      " -5.590e+02  5.600e+02 -5.680e+02  5.720e+02 -5.730e+02 -5.760e+02\n",
      "  5.770e+02  5.770e+02 -5.780e+02  5.840e+02 -5.860e+02 -5.940e+02\n",
      "  6.010e+02  6.020e+02  6.060e+02 -6.060e+02 -6.160e+02 -6.190e+02\n",
      " -6.220e+02 -6.220e+02 -6.230e+02 -6.230e+02 -6.240e+02  6.250e+02\n",
      " -6.250e+02  6.310e+02 -6.350e+02 -6.390e+02 -6.410e+02 -6.460e+02\n",
      "  6.540e+02 -6.580e+02  6.630e+02  6.660e+02 -6.670e+02 -6.700e+02\n",
      "  6.750e+02 -6.790e+02  6.800e+02 -6.820e+02 -6.850e+02 -6.930e+02\n",
      " -6.950e+02  6.950e+02 -7.010e+02 -7.040e+02 -7.070e+02 -7.100e+02\n",
      " -7.130e+02 -7.140e+02 -7.170e+02 -7.250e+02  7.270e+02  7.390e+02\n",
      " -7.420e+02 -7.500e+02 -7.540e+02 -7.590e+02 -7.590e+02 -7.590e+02\n",
      " -7.590e+02 -7.700e+02 -7.730e+02 -7.820e+02 -7.830e+02  7.890e+02\n",
      " -7.960e+02 -7.970e+02 -7.990e+02  8.040e+02  8.060e+02 -8.170e+02\n",
      "  8.230e+02 -8.270e+02  8.360e+02  8.390e+02 -8.510e+02 -8.520e+02\n",
      " -8.660e+02 -8.750e+02  8.820e+02 -8.930e+02 -8.960e+02 -9.040e+02\n",
      " -9.060e+02 -9.100e+02 -9.110e+02 -9.130e+02 -9.140e+02  9.150e+02\n",
      " -9.180e+02  9.270e+02 -9.300e+02 -9.470e+02 -9.500e+02 -9.540e+02\n",
      " -9.640e+02  9.800e+02  9.850e+02 -9.870e+02  9.880e+02  9.930e+02\n",
      " -9.970e+02  9.980e+02 -1.000e+03 -1.021e+03 -1.022e+03 -1.025e+03\n",
      " -1.027e+03 -1.030e+03 -1.036e+03  1.037e+03 -1.038e+03 -1.045e+03\n",
      " -1.057e+03 -1.065e+03 -1.066e+03 -1.077e+03  1.079e+03  1.081e+03\n",
      "  1.090e+03  1.093e+03 -1.117e+03 -1.120e+03  1.133e+03  1.134e+03\n",
      " -1.138e+03 -1.152e+03 -1.157e+03 -1.168e+03 -1.172e+03 -1.179e+03\n",
      " -1.183e+03 -1.191e+03  1.202e+03 -1.224e+03 -1.245e+03 -1.273e+03\n",
      " -1.278e+03  1.289e+03 -1.309e+03 -1.315e+03 -1.346e+03 -1.353e+03\n",
      " -1.358e+03 -1.368e+03  1.394e+03  1.398e+03 -1.399e+03 -1.409e+03\n",
      " -1.409e+03 -1.413e+03 -1.425e+03 -1.428e+03 -1.430e+03 -1.435e+03\n",
      " -1.440e+03  1.459e+03 -1.461e+03  1.466e+03 -1.472e+03 -1.478e+03\n",
      " -1.483e+03  1.504e+03 -1.506e+03 -1.508e+03 -1.512e+03 -1.521e+03\n",
      " -1.523e+03 -1.527e+03 -1.555e+03 -1.559e+03 -1.560e+03 -1.584e+03\n",
      " -1.584e+03 -1.586e+03  1.591e+03 -1.593e+03 -1.628e+03  1.641e+03\n",
      " -1.660e+03 -1.663e+03 -1.663e+03 -1.665e+03  1.671e+03 -1.686e+03\n",
      " -1.690e+03 -1.699e+03 -1.721e+03 -1.722e+03 -1.724e+03  1.732e+03\n",
      "  1.748e+03 -1.777e+03 -1.778e+03  1.838e+03 -1.840e+03 -1.897e+03\n",
      " -1.899e+03 -1.971e+03  1.972e+03  2.002e+03 -2.016e+03  2.064e+03\n",
      "  2.083e+03  2.120e+03 -2.133e+03 -2.143e+03  2.166e+03 -2.182e+03\n",
      " -2.239e+03 -2.265e+03 -2.298e+03  2.319e+03 -2.327e+03 -2.347e+03\n",
      " -2.359e+03 -2.437e+03  2.570e+03 -2.641e+03  2.703e+03  2.717e+03\n",
      " -2.727e+03  2.741e+03 -2.784e+03 -2.886e+03  3.059e+03 -3.221e+03\n",
      " -3.270e+03  3.314e+03 -3.381e+03 -3.930e+03 -3.981e+03 -4.241e+03\n",
      " -4.282e+03  4.680e+03  4.856e+03  5.152e+03 -5.252e+03 -5.480e+03\n",
      "  6.417e+03]\n",
      "Concordance Index 0.3749009127352615\n",
      "Integrated Brier Score: 0.3042672330309679\n",
      "y_train breslow final [ 2.000e+00  2.000e+00 -1.100e+01  1.100e+01 -1.400e+01  1.400e+01\n",
      "  3.200e+01  5.600e+01 -5.600e+01  6.200e+01  6.400e+01  6.500e+01\n",
      "  6.900e+01 -7.700e+01  8.200e+01  8.900e+01  9.000e+01 -9.200e+01\n",
      "  9.400e+01 -1.020e+02 -1.050e+02  1.070e+02  1.080e+02 -1.110e+02\n",
      "  1.120e+02  1.260e+02  1.290e+02  1.300e+02 -1.340e+02  1.420e+02\n",
      "  1.440e+02  1.510e+02 -1.560e+02  1.600e+02  1.660e+02  1.710e+02\n",
      "  1.730e+02 -1.810e+02  1.820e+02  1.830e+02  1.850e+02 -1.870e+02\n",
      "  1.910e+02  1.940e+02  1.940e+02  1.970e+02  1.980e+02  2.050e+02\n",
      "  2.090e+02  2.150e+02  2.150e+02 -2.160e+02  2.170e+02  2.170e+02\n",
      "  2.180e+02  2.220e+02 -2.250e+02 -2.320e+02 -2.340e+02 -2.420e+02\n",
      "  2.430e+02 -2.530e+02  2.530e+02  2.610e+02  2.680e+02  2.760e+02\n",
      " -2.780e+02  2.790e+02  2.810e+02  2.820e+02  2.840e+02  2.920e+02\n",
      "  2.940e+02  2.950e+02 -3.110e+02  3.170e+02  3.170e+02 -3.180e+02\n",
      "  3.210e+02  3.270e+02  3.270e+02  3.420e+02 -3.450e+02  3.480e+02\n",
      " -3.510e+02  3.510e+02  3.520e+02  3.530e+02 -3.540e+02  3.570e+02\n",
      " -3.580e+02  3.590e+02 -3.600e+02  3.610e+02  3.660e+02 -3.670e+02\n",
      " -3.680e+02  3.710e+02 -3.760e+02  3.770e+02 -3.780e+02  3.790e+02\n",
      " -3.790e+02  3.800e+02 -3.840e+02  3.850e+02 -3.860e+02  3.870e+02\n",
      " -3.870e+02 -3.890e+02 -3.920e+02 -3.920e+02  3.930e+02 -3.930e+02\n",
      "  3.950e+02  3.950e+02  3.970e+02 -4.000e+02 -4.030e+02  4.030e+02\n",
      "  4.030e+02  4.060e+02  4.070e+02 -4.120e+02 -4.140e+02 -4.170e+02\n",
      "  4.210e+02 -4.210e+02 -4.240e+02  4.240e+02  4.360e+02 -4.400e+02\n",
      " -4.410e+02 -4.430e+02  4.460e+02  4.530e+02  4.560e+02 -4.580e+02\n",
      "  4.590e+02 -4.610e+02 -4.660e+02 -4.690e+02 -4.710e+02  4.720e+02\n",
      "  4.790e+02 -4.790e+02  4.800e+02  4.840e+02 -4.850e+02  4.870e+02\n",
      " -4.910e+02  4.930e+02  4.950e+02 -4.990e+02  5.060e+02 -5.090e+02\n",
      " -5.160e+02 -5.180e+02 -5.200e+02  5.210e+02  5.210e+02  5.220e+02\n",
      "  5.260e+02  5.300e+02 -5.390e+02 -5.420e+02 -5.430e+02  5.430e+02\n",
      "  5.450e+02  5.460e+02  5.460e+02 -5.470e+02 -5.490e+02 -5.520e+02\n",
      " -5.590e+02  5.600e+02 -5.680e+02  5.720e+02 -5.730e+02 -5.760e+02\n",
      "  5.770e+02  5.770e+02 -5.780e+02  5.840e+02 -5.860e+02 -5.940e+02\n",
      "  6.010e+02  6.020e+02  6.060e+02 -6.060e+02 -6.160e+02 -6.190e+02\n",
      " -6.220e+02 -6.220e+02 -6.230e+02 -6.230e+02 -6.240e+02  6.250e+02\n",
      " -6.250e+02  6.310e+02 -6.350e+02 -6.390e+02 -6.410e+02 -6.460e+02\n",
      "  6.540e+02 -6.580e+02  6.630e+02  6.660e+02 -6.670e+02 -6.700e+02\n",
      "  6.750e+02 -6.790e+02  6.800e+02 -6.820e+02 -6.850e+02 -6.930e+02\n",
      " -6.950e+02  6.950e+02 -7.010e+02 -7.040e+02 -7.070e+02 -7.100e+02\n",
      " -7.130e+02 -7.140e+02 -7.170e+02 -7.250e+02  7.270e+02  7.390e+02\n",
      " -7.420e+02 -7.500e+02 -7.540e+02 -7.590e+02 -7.590e+02 -7.590e+02\n",
      " -7.590e+02 -7.700e+02 -7.730e+02 -7.820e+02 -7.830e+02  7.890e+02\n",
      " -7.960e+02 -7.970e+02 -7.990e+02  8.040e+02  8.060e+02 -8.170e+02\n",
      "  8.230e+02 -8.270e+02  8.360e+02  8.390e+02 -8.510e+02 -8.520e+02\n",
      " -8.660e+02 -8.750e+02  8.820e+02 -8.930e+02 -8.960e+02 -9.040e+02\n",
      " -9.060e+02 -9.100e+02 -9.110e+02 -9.130e+02 -9.140e+02  9.150e+02\n",
      " -9.180e+02  9.270e+02 -9.300e+02 -9.470e+02 -9.500e+02 -9.540e+02\n",
      " -9.640e+02  9.800e+02  9.850e+02 -9.870e+02  9.880e+02  9.930e+02\n",
      " -9.970e+02  9.980e+02 -1.000e+03 -1.021e+03 -1.022e+03 -1.025e+03\n",
      " -1.027e+03 -1.030e+03 -1.036e+03  1.037e+03 -1.038e+03 -1.045e+03\n",
      " -1.057e+03 -1.065e+03 -1.066e+03 -1.077e+03  1.079e+03  1.081e+03\n",
      "  1.090e+03  1.093e+03 -1.117e+03 -1.120e+03  1.133e+03  1.134e+03\n",
      " -1.138e+03 -1.152e+03 -1.157e+03 -1.168e+03 -1.172e+03 -1.179e+03\n",
      " -1.183e+03 -1.191e+03  1.202e+03 -1.224e+03 -1.245e+03 -1.273e+03\n",
      " -1.278e+03  1.289e+03 -1.309e+03 -1.315e+03 -1.346e+03 -1.353e+03\n",
      " -1.358e+03 -1.368e+03  1.394e+03  1.398e+03 -1.399e+03 -1.409e+03\n",
      " -1.409e+03 -1.413e+03 -1.425e+03 -1.428e+03 -1.430e+03 -1.435e+03\n",
      " -1.440e+03  1.459e+03 -1.461e+03  1.466e+03 -1.472e+03 -1.478e+03\n",
      " -1.483e+03  1.504e+03 -1.506e+03 -1.508e+03 -1.512e+03 -1.521e+03\n",
      " -1.523e+03 -1.527e+03 -1.555e+03 -1.559e+03 -1.560e+03 -1.584e+03\n",
      " -1.584e+03 -1.586e+03  1.591e+03 -1.593e+03 -1.628e+03  1.641e+03\n",
      " -1.660e+03 -1.663e+03 -1.663e+03 -1.665e+03  1.671e+03 -1.686e+03\n",
      " -1.690e+03 -1.699e+03 -1.721e+03 -1.722e+03 -1.724e+03  1.732e+03\n",
      "  1.748e+03 -1.777e+03 -1.778e+03  1.838e+03 -1.840e+03 -1.897e+03\n",
      " -1.899e+03 -1.971e+03  1.972e+03  2.002e+03 -2.016e+03  2.064e+03\n",
      "  2.083e+03  2.120e+03 -2.133e+03 -2.143e+03  2.166e+03 -2.182e+03\n",
      " -2.239e+03 -2.265e+03 -2.298e+03  2.319e+03 -2.327e+03 -2.347e+03\n",
      " -2.359e+03 -2.437e+03  2.570e+03 -2.641e+03  2.703e+03  2.717e+03\n",
      " -2.727e+03  2.741e+03 -2.784e+03 -2.886e+03  3.059e+03 -3.221e+03\n",
      " -3.270e+03  3.314e+03 -3.381e+03 -3.930e+03 -3.981e+03 -4.241e+03\n",
      " -4.282e+03  4.680e+03  4.856e+03  5.152e+03 -5.252e+03 -5.480e+03\n",
      "  6.417e+03]\n",
      "durations 23.0 4760.0\n",
      "Concordance Index 0.3963053328685953\n",
      "Integrated Brier Score: 0.2684320242038202\n",
      "(415, 20531) <class 'pandas.core.frame.DataFrame'>\n",
      "(415,) <class 'pandas.core.series.Series'>\n",
      "(104, 20531) <class 'pandas.core.frame.DataFrame'>\n",
      "(104,) <class 'pandas.core.series.Series'>\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.3317\u001b[0m  0.0359\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      2        3.3317  0.0339\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      3        3.3317  0.0178\n",
      "      4        3.3317  0.0157\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      5        3.3317  0.0146\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.9687\u001b[0m  0.0262\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      6        3.3317  0.0224\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2        3.9687  0.0193\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      7        3.3317  0.0183\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.4513\u001b[0m  0.0252\n",
      "      3        3.9687  0.0264\n",
      "      8        3.3317  0.0161\n",
      "      4        3.9687  0.0158\n",
      "      9        3.3317  0.0161\n",
      "      5        3.9687  0.0159\n",
      "      2        3.4513  0.0427\n",
      "     10        3.3317  0.0148\n",
      "Restoring best model from epoch 1.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.7393\u001b[0m  0.0634\n",
      "      3        3.4513  0.0176\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      6        3.9687  0.0457\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      4        3.4513  0.0255\n",
      "      2        3.7393  0.0355\n",
      "      7        3.9687  0.0172\n",
      "      5        3.4513  0.0220\n",
      "      3        3.7393  0.0177\n",
      "      8        3.9687  0.0137\n",
      "      9        3.9687  0.0166\n",
      "      4        3.7393  0.0268\n",
      "      6        3.4513  0.0277\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m2.6153\u001b[0m  0.0304\n",
      "      5        3.7393  0.0152\n",
      "      7        3.4513  0.0155\n",
      "     10        3.9687  0.0184\n",
      "Restoring best model from epoch 1.\n",
      "      2        2.6153  0.0198\n",
      "      8        3.4513  0.0160\n",
      "      6        3.7393  0.0299\n",
      "      9        3.4513  0.0151\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.5334\u001b[0m        \u001b[32m3.9725\u001b[0m  0.2235\n",
      "      3        2.6153  0.0448\n",
      "     10        3.4513  0.0209\n",
      "Restoring best model from epoch 1.\n",
      "      7        3.7393  0.0430\n",
      "      8        3.7393  0.0309\n",
      "      9        3.7393  0.0152\n",
      "      4        2.6153  0.0820\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.9323\u001b[0m        \u001b[32m2.6159\u001b[0m  0.3029\n",
      "      5        2.6153  0.0226\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.2225\u001b[0m        \u001b[32m3.4541\u001b[0m  0.3321\n",
      "     10        3.7393  0.0595\n",
      "      6        2.6153  0.0249\n",
      "Restoring best model from epoch 1.\n",
      "      7        2.6153  0.0225\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.9596\u001b[0m        \u001b[32m3.3325\u001b[0m  0.3569\n",
      "      8        2.6153  0.0238\n",
      "      9        2.6153  0.0145\n",
      "     10        2.6153  0.0124\n",
      "Restoring best model from epoch 1.\n",
      "      2        \u001b[36m3.5172\u001b[0m        3.9750  0.2221\n",
      "      2        \u001b[36m2.9153\u001b[0m        2.6176  0.1834\n",
      "      2        \u001b[36m3.2038\u001b[0m        \u001b[32m3.4530\u001b[0m  0.1741\n",
      "      2        \u001b[36m2.9495\u001b[0m        3.3351  0.1681\n",
      "      3        3.5233        3.9756  0.1433\n",
      "      3        2.9290        2.6178  0.1322\n",
      "      3        \u001b[36m3.1999\u001b[0m        \u001b[32m3.4499\u001b[0m  0.1296\n",
      "      3        \u001b[36m2.9302\u001b[0m        3.3360  0.1355\n",
      "      4        \u001b[36m3.4724\u001b[0m        3.9773  0.1296\n",
      "      4        \u001b[36m2.9125\u001b[0m        2.6167  0.1299\n",
      "      4        \u001b[36m3.1883\u001b[0m        3.4504  0.1290\n",
      "      4        2.9559        3.3351  0.1332\n",
      "      5        3.4905        3.9764  0.1295\n",
      "      5        \u001b[36m2.9088\u001b[0m        \u001b[32m2.6157\u001b[0m  0.1327\n",
      "      5        3.2148        3.4500  0.1394\n",
      "      5        2.9377        3.3352  0.1440\n",
      "      6        \u001b[36m3.4687\u001b[0m        3.9763  0.1468\n",
      "      6        2.9209        \u001b[32m2.6153\u001b[0m  0.1416\n",
      "      6        3.1967        \u001b[32m3.4480\u001b[0m  0.1440\n",
      "      6        2.9332        3.3357  0.1397\n",
      "      7        3.5048        3.9757  0.1337\n",
      "      7        \u001b[36m2.8926\u001b[0m        \u001b[32m2.6148\u001b[0m  0.1396\n",
      "      7        3.2051        \u001b[32m3.4447\u001b[0m  0.1362\n",
      "      7        \u001b[36m2.9259\u001b[0m        3.3363  0.1343\n",
      "      8        \u001b[36m3.4498\u001b[0m        3.9744  0.1288\n",
      "      8        2.9025        2.6149  0.1278\n",
      "      8        \u001b[36m3.1847\u001b[0m        \u001b[32m3.4415\u001b[0m  0.1384\n",
      "      8        \u001b[36m2.9127\u001b[0m        3.3361  0.1300\n",
      "      9        3.4578        3.9736  0.1281\n",
      "      9        \u001b[36m2.8885\u001b[0m        2.6153  0.1341\n",
      "      9        \u001b[36m3.1793\u001b[0m        \u001b[32m3.4391\u001b[0m  0.1337\n",
      "      9        \u001b[36m2.9015\u001b[0m        3.3353  0.1392\n",
      "     10        3.4552        3.9739  0.1361\n",
      "Restoring best model from epoch 1.\n",
      "     10        2.8970        2.6153  0.1319\n",
      "Restoring best model from epoch 7.\n",
      "     10        \u001b[36m3.1542\u001b[0m        \u001b[32m3.4367\u001b[0m  0.1367\n",
      "     10        2.9236        3.3342  0.1322\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.2934\u001b[0m        \u001b[32m3.7404\u001b[0m  0.1342\n",
      "      2        \u001b[36m3.2432\u001b[0m        \u001b[32m3.7390\u001b[0m  0.1190\n",
      "      3        3.2620        \u001b[32m3.7365\u001b[0m  0.1159\n",
      "      4        3.2578        \u001b[32m3.7359\u001b[0m  0.1148\n",
      "      5        \u001b[36m3.2277\u001b[0m        3.7365  0.1156\n",
      "      6        \u001b[36m3.2068\u001b[0m        3.7365  0.1172\n",
      "      7        3.2214        3.7360  0.1147\n",
      "      8        3.2268        \u001b[32m3.7338\u001b[0m  0.1177\n",
      "      9        3.2428        \u001b[32m3.7319\u001b[0m  0.1145\n",
      "     10        3.2190        \u001b[32m3.7297\u001b[0m  0.1145\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.8280\u001b[0m  0.0163\n",
      "      2        3.8280  0.0147\n",
      "      3        3.8280  0.0102\n",
      "      4        3.8280  0.0189\n",
      "      5        3.8280  0.0268\n",
      "      6        3.8280  0.0103\n",
      "      7        3.8280  0.0259\n",
      "      8        3.8280  0.0145\n",
      "      9        3.8280  0.0121\n",
      "     10        3.8280  0.0111\n",
      "Restoring best model from epoch 1.\n",
      "y_train breslow final [ 2.000e+00  2.000e+00 -1.100e+01  1.100e+01  1.400e+01  2.300e+01\n",
      " -3.000e+01  3.200e+01  5.600e+01  6.200e+01  6.500e+01  6.900e+01\n",
      "  7.600e+01 -7.700e+01  8.200e+01  8.600e+01  9.000e+01 -9.200e+01\n",
      "  9.400e+01  9.500e+01 -1.020e+02 -1.050e+02  1.070e+02 -1.100e+02\n",
      " -1.110e+02  1.120e+02 -1.220e+02  1.260e+02  1.290e+02  1.290e+02\n",
      "  1.300e+02 -1.340e+02  1.420e+02  1.440e+02  1.440e+02 -1.460e+02\n",
      " -1.560e+02  1.600e+02  1.660e+02  1.730e+02 -1.810e+02  1.820e+02\n",
      "  1.850e+02 -1.870e+02  1.940e+02  1.940e+02  1.970e+02  2.050e+02\n",
      "  2.090e+02 -2.100e+02  2.150e+02 -2.160e+02 -2.160e+02  2.170e+02\n",
      "  2.170e+02  2.180e+02  2.220e+02 -2.250e+02 -2.340e+02 -2.420e+02\n",
      "  2.430e+02 -2.530e+02  2.530e+02  2.560e+02  2.590e+02  2.610e+02\n",
      "  2.680e+02  2.740e+02  2.760e+02 -2.780e+02  2.790e+02  2.830e+02\n",
      "  2.840e+02 -2.920e+02  2.940e+02  2.950e+02 -3.110e+02  3.170e+02\n",
      " -3.180e+02 -3.210e+02  3.210e+02  3.270e+02  3.270e+02  3.300e+02\n",
      "  3.340e+02  3.360e+02  3.370e+02  3.410e+02 -3.450e+02  3.480e+02\n",
      "  3.510e+02  3.530e+02 -3.540e+02  3.570e+02  3.600e+02 -3.600e+02\n",
      "  3.610e+02 -3.620e+02 -3.670e+02 -3.680e+02  3.710e+02 -3.760e+02\n",
      "  3.770e+02  3.770e+02 -3.780e+02  3.790e+02 -3.790e+02  3.800e+02\n",
      "  3.840e+02 -3.840e+02  3.850e+02 -3.860e+02 -3.890e+02 -3.920e+02\n",
      " -3.920e+02  3.930e+02 -3.930e+02  3.950e+02  3.950e+02 -4.000e+02\n",
      "  4.030e+02  4.030e+02  4.060e+02  4.060e+02 -4.070e+02 -4.140e+02\n",
      "  4.150e+02  4.210e+02 -4.210e+02 -4.240e+02  4.240e+02  4.270e+02\n",
      "  4.300e+02  4.360e+02 -4.370e+02 -4.410e+02 -4.430e+02  4.460e+02\n",
      "  4.490e+02  4.510e+02  4.530e+02  4.560e+02 -4.580e+02  4.590e+02\n",
      " -4.610e+02  4.620e+02  4.640e+02 -4.660e+02 -4.690e+02 -4.710e+02\n",
      "  4.720e+02  4.790e+02  4.800e+02  4.840e+02  4.870e+02  4.890e+02\n",
      " -4.910e+02  4.950e+02 -4.990e+02  5.060e+02 -5.140e+02 -5.160e+02\n",
      " -5.180e+02  5.210e+02  5.210e+02  5.220e+02  5.260e+02 -5.390e+02\n",
      " -5.430e+02  5.430e+02  5.460e+02 -5.470e+02  5.480e+02  5.600e+02\n",
      "  5.640e+02 -5.680e+02  5.720e+02  5.770e+02 -5.780e+02 -5.790e+02\n",
      "  5.800e+02  5.840e+02 -5.940e+02 -6.000e+02  6.020e+02 -6.030e+02\n",
      "  6.060e+02 -6.060e+02 -6.160e+02 -6.190e+02 -6.230e+02 -6.230e+02\n",
      "  6.240e+02  6.250e+02 -6.250e+02  6.310e+02 -6.350e+02 -6.370e+02\n",
      " -6.390e+02 -6.410e+02 -6.410e+02  6.410e+02 -6.440e+02 -6.450e+02\n",
      " -6.450e+02 -6.460e+02 -6.530e+02  6.540e+02  6.660e+02 -6.700e+02\n",
      "  6.750e+02 -6.790e+02  6.800e+02 -6.820e+02 -6.870e+02 -6.930e+02\n",
      "  6.950e+02 -7.010e+02 -7.010e+02 -7.040e+02 -7.070e+02 -7.130e+02\n",
      " -7.170e+02 -7.220e+02 -7.420e+02 -7.500e+02 -7.540e+02 -7.590e+02\n",
      " -7.590e+02 -7.590e+02  7.630e+02 -7.700e+02  7.730e+02 -7.730e+02\n",
      "  7.890e+02 -7.970e+02 -7.990e+02  8.040e+02 -8.170e+02  8.230e+02\n",
      " -8.330e+02  8.360e+02 -8.360e+02  8.390e+02 -8.500e+02 -8.510e+02\n",
      " -8.520e+02  8.530e+02 -8.550e+02  8.620e+02 -8.660e+02 -8.750e+02\n",
      " -8.890e+02 -8.930e+02 -8.950e+02 -8.960e+02 -8.970e+02 -9.040e+02\n",
      " -9.060e+02 -9.100e+02 -9.110e+02 -9.130e+02 -9.140e+02  9.150e+02\n",
      " -9.180e+02  9.270e+02 -9.330e+02  9.410e+02 -9.500e+02 -9.540e+02\n",
      " -9.640e+02 -9.720e+02  9.800e+02 -9.870e+02  9.880e+02  9.930e+02\n",
      " -9.930e+02 -9.950e+02 -9.970e+02  9.980e+02 -1.021e+03 -1.022e+03\n",
      " -1.025e+03 -1.027e+03 -1.036e+03  1.037e+03 -1.038e+03 -1.045e+03\n",
      " -1.050e+03 -1.057e+03 -1.065e+03 -1.066e+03 -1.075e+03  1.090e+03\n",
      " -1.095e+03 -1.117e+03 -1.120e+03 -1.125e+03 -1.131e+03  1.133e+03\n",
      "  1.134e+03 -1.138e+03 -1.143e+03 -1.147e+03 -1.152e+03 -1.157e+03\n",
      " -1.172e+03 -1.179e+03 -1.190e+03 -1.191e+03  1.202e+03 -1.218e+03\n",
      " -1.224e+03 -1.245e+03 -1.253e+03 -1.265e+03 -1.273e+03 -1.274e+03\n",
      " -1.288e+03 -1.309e+03 -1.311e+03 -1.346e+03 -1.358e+03 -1.368e+03\n",
      "  1.398e+03 -1.399e+03 -1.409e+03 -1.409e+03 -1.413e+03 -1.425e+03\n",
      " -1.428e+03  1.430e+03 -1.430e+03 -1.435e+03 -1.440e+03 -1.443e+03\n",
      " -1.460e+03 -1.461e+03 -1.466e+03  1.466e+03 -1.478e+03 -1.483e+03\n",
      "  1.504e+03 -1.506e+03 -1.508e+03 -1.512e+03 -1.521e+03 -1.523e+03\n",
      " -1.527e+03 -1.560e+03 -1.584e+03 -1.586e+03  1.591e+03 -1.593e+03\n",
      " -1.628e+03  1.641e+03 -1.660e+03 -1.663e+03 -1.663e+03 -1.665e+03\n",
      "  1.671e+03  1.718e+03 -1.722e+03  1.732e+03 -1.737e+03  1.748e+03\n",
      "  1.762e+03 -1.777e+03 -1.778e+03 -1.840e+03 -1.897e+03 -1.899e+03\n",
      " -1.971e+03  1.972e+03 -1.995e+03  2.002e+03 -2.016e+03 -2.016e+03\n",
      "  2.064e+03  2.083e+03  2.120e+03 -2.133e+03 -2.143e+03 -2.161e+03\n",
      "  2.166e+03 -2.169e+03 -2.239e+03 -2.265e+03 -2.298e+03 -2.347e+03\n",
      " -2.359e+03 -2.437e+03 -2.562e+03  2.570e+03 -2.641e+03  2.717e+03\n",
      " -2.727e+03  2.741e+03 -2.784e+03 -2.886e+03  2.900e+03  3.059e+03\n",
      " -3.221e+03 -3.270e+03 -3.381e+03 -3.930e+03 -3.981e+03 -4.241e+03\n",
      "  4.680e+03  4.760e+03  4.856e+03  5.152e+03 -5.252e+03 -5.480e+03\n",
      "  6.417e+03]\n",
      "Concordance Index 0.5139932045779685\n",
      "Integrated Brier Score: 0.18877306631966811\n",
      "y_train breslow final [ 2.000e+00  2.000e+00 -1.100e+01  1.100e+01  1.400e+01  2.300e+01\n",
      " -3.000e+01  3.200e+01  5.600e+01  6.200e+01  6.500e+01  6.900e+01\n",
      "  7.600e+01 -7.700e+01  8.200e+01  8.600e+01  9.000e+01 -9.200e+01\n",
      "  9.400e+01  9.500e+01 -1.020e+02 -1.050e+02  1.070e+02 -1.100e+02\n",
      " -1.110e+02  1.120e+02 -1.220e+02  1.260e+02  1.290e+02  1.290e+02\n",
      "  1.300e+02 -1.340e+02  1.420e+02  1.440e+02  1.440e+02 -1.460e+02\n",
      " -1.560e+02  1.600e+02  1.660e+02  1.730e+02 -1.810e+02  1.820e+02\n",
      "  1.850e+02 -1.870e+02  1.940e+02  1.940e+02  1.970e+02  2.050e+02\n",
      "  2.090e+02 -2.100e+02  2.150e+02 -2.160e+02 -2.160e+02  2.170e+02\n",
      "  2.170e+02  2.180e+02  2.220e+02 -2.250e+02 -2.340e+02 -2.420e+02\n",
      "  2.430e+02 -2.530e+02  2.530e+02  2.560e+02  2.590e+02  2.610e+02\n",
      "  2.680e+02  2.740e+02  2.760e+02 -2.780e+02  2.790e+02  2.830e+02\n",
      "  2.840e+02 -2.920e+02  2.940e+02  2.950e+02 -3.110e+02  3.170e+02\n",
      " -3.180e+02 -3.210e+02  3.210e+02  3.270e+02  3.270e+02  3.300e+02\n",
      "  3.340e+02  3.360e+02  3.370e+02  3.410e+02 -3.450e+02  3.480e+02\n",
      "  3.510e+02  3.530e+02 -3.540e+02  3.570e+02  3.600e+02 -3.600e+02\n",
      "  3.610e+02 -3.620e+02 -3.670e+02 -3.680e+02  3.710e+02 -3.760e+02\n",
      "  3.770e+02  3.770e+02 -3.780e+02  3.790e+02 -3.790e+02  3.800e+02\n",
      "  3.840e+02 -3.840e+02  3.850e+02 -3.860e+02 -3.890e+02 -3.920e+02\n",
      " -3.920e+02  3.930e+02 -3.930e+02  3.950e+02  3.950e+02 -4.000e+02\n",
      "  4.030e+02  4.030e+02  4.060e+02  4.060e+02 -4.070e+02 -4.140e+02\n",
      "  4.150e+02  4.210e+02 -4.210e+02 -4.240e+02  4.240e+02  4.270e+02\n",
      "  4.300e+02  4.360e+02 -4.370e+02 -4.410e+02 -4.430e+02  4.460e+02\n",
      "  4.490e+02  4.510e+02  4.530e+02  4.560e+02 -4.580e+02  4.590e+02\n",
      " -4.610e+02  4.620e+02  4.640e+02 -4.660e+02 -4.690e+02 -4.710e+02\n",
      "  4.720e+02  4.790e+02  4.800e+02  4.840e+02  4.870e+02  4.890e+02\n",
      " -4.910e+02  4.950e+02 -4.990e+02  5.060e+02 -5.140e+02 -5.160e+02\n",
      " -5.180e+02  5.210e+02  5.210e+02  5.220e+02  5.260e+02 -5.390e+02\n",
      " -5.430e+02  5.430e+02  5.460e+02 -5.470e+02  5.480e+02  5.600e+02\n",
      "  5.640e+02 -5.680e+02  5.720e+02  5.770e+02 -5.780e+02 -5.790e+02\n",
      "  5.800e+02  5.840e+02 -5.940e+02 -6.000e+02  6.020e+02 -6.030e+02\n",
      "  6.060e+02 -6.060e+02 -6.160e+02 -6.190e+02 -6.230e+02 -6.230e+02\n",
      "  6.240e+02  6.250e+02 -6.250e+02  6.310e+02 -6.350e+02 -6.370e+02\n",
      " -6.390e+02 -6.410e+02 -6.410e+02  6.410e+02 -6.440e+02 -6.450e+02\n",
      " -6.450e+02 -6.460e+02 -6.530e+02  6.540e+02  6.660e+02 -6.700e+02\n",
      "  6.750e+02 -6.790e+02  6.800e+02 -6.820e+02 -6.870e+02 -6.930e+02\n",
      "  6.950e+02 -7.010e+02 -7.010e+02 -7.040e+02 -7.070e+02 -7.130e+02\n",
      " -7.170e+02 -7.220e+02 -7.420e+02 -7.500e+02 -7.540e+02 -7.590e+02\n",
      " -7.590e+02 -7.590e+02  7.630e+02 -7.700e+02  7.730e+02 -7.730e+02\n",
      "  7.890e+02 -7.970e+02 -7.990e+02  8.040e+02 -8.170e+02  8.230e+02\n",
      " -8.330e+02  8.360e+02 -8.360e+02  8.390e+02 -8.500e+02 -8.510e+02\n",
      " -8.520e+02  8.530e+02 -8.550e+02  8.620e+02 -8.660e+02 -8.750e+02\n",
      " -8.890e+02 -8.930e+02 -8.950e+02 -8.960e+02 -8.970e+02 -9.040e+02\n",
      " -9.060e+02 -9.100e+02 -9.110e+02 -9.130e+02 -9.140e+02  9.150e+02\n",
      " -9.180e+02  9.270e+02 -9.330e+02  9.410e+02 -9.500e+02 -9.540e+02\n",
      " -9.640e+02 -9.720e+02  9.800e+02 -9.870e+02  9.880e+02  9.930e+02\n",
      " -9.930e+02 -9.950e+02 -9.970e+02  9.980e+02 -1.021e+03 -1.022e+03\n",
      " -1.025e+03 -1.027e+03 -1.036e+03  1.037e+03 -1.038e+03 -1.045e+03\n",
      " -1.050e+03 -1.057e+03 -1.065e+03 -1.066e+03 -1.075e+03  1.090e+03\n",
      " -1.095e+03 -1.117e+03 -1.120e+03 -1.125e+03 -1.131e+03  1.133e+03\n",
      "  1.134e+03 -1.138e+03 -1.143e+03 -1.147e+03 -1.152e+03 -1.157e+03\n",
      " -1.172e+03 -1.179e+03 -1.190e+03 -1.191e+03  1.202e+03 -1.218e+03\n",
      " -1.224e+03 -1.245e+03 -1.253e+03 -1.265e+03 -1.273e+03 -1.274e+03\n",
      " -1.288e+03 -1.309e+03 -1.311e+03 -1.346e+03 -1.358e+03 -1.368e+03\n",
      "  1.398e+03 -1.399e+03 -1.409e+03 -1.409e+03 -1.413e+03 -1.425e+03\n",
      " -1.428e+03  1.430e+03 -1.430e+03 -1.435e+03 -1.440e+03 -1.443e+03\n",
      " -1.460e+03 -1.461e+03 -1.466e+03  1.466e+03 -1.478e+03 -1.483e+03\n",
      "  1.504e+03 -1.506e+03 -1.508e+03 -1.512e+03 -1.521e+03 -1.523e+03\n",
      " -1.527e+03 -1.560e+03 -1.584e+03 -1.586e+03  1.591e+03 -1.593e+03\n",
      " -1.628e+03  1.641e+03 -1.660e+03 -1.663e+03 -1.663e+03 -1.665e+03\n",
      "  1.671e+03  1.718e+03 -1.722e+03  1.732e+03 -1.737e+03  1.748e+03\n",
      "  1.762e+03 -1.777e+03 -1.778e+03 -1.840e+03 -1.897e+03 -1.899e+03\n",
      " -1.971e+03  1.972e+03 -1.995e+03  2.002e+03 -2.016e+03 -2.016e+03\n",
      "  2.064e+03  2.083e+03  2.120e+03 -2.133e+03 -2.143e+03 -2.161e+03\n",
      "  2.166e+03 -2.169e+03 -2.239e+03 -2.265e+03 -2.298e+03 -2.347e+03\n",
      " -2.359e+03 -2.437e+03 -2.562e+03  2.570e+03 -2.641e+03  2.717e+03\n",
      " -2.727e+03  2.741e+03 -2.784e+03 -2.886e+03  2.900e+03  3.059e+03\n",
      " -3.221e+03 -3.270e+03 -3.381e+03 -3.930e+03 -3.981e+03 -4.241e+03\n",
      "  4.680e+03  4.760e+03  4.856e+03  5.152e+03 -5.252e+03 -5.480e+03\n",
      "  6.417e+03]\n",
      "durations 14.0 4282.0\n",
      "Concordance Index 0.4202522255192878\n",
      "Integrated Brier Score: 0.1936552870013823\n",
      "(415, 20531) <class 'pandas.core.frame.DataFrame'>\n",
      "(415,) <class 'pandas.core.series.Series'>\n",
      "(104, 20531) <class 'pandas.core.frame.DataFrame'>\n",
      "(104,) <class 'pandas.core.series.Series'>\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.1753\u001b[0m  0.0203\n",
      "      2        3.1753  0.0157\n",
      "      3        3.1753  0.0175\n",
      "      4        3.1753  0.0208\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      5        3.1753  0.0307\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      6        3.1753  0.0142\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.3750\u001b[0m  0.0245\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      7        3.1753  0.0168\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m2.9155\u001b[0m  0.0259\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2        3.3750  0.0158\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      8        3.1753  0.0142\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      3        3.3750  0.0159\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2        2.9155  0.0208\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.2441\u001b[0m  0.0201\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.8988\u001b[0m  0.0231\n",
      "      9        3.1753  0.0221\n",
      "      3        2.9155  0.0151\n",
      "      4        3.3750  0.0209\n",
      "      2        3.2441  0.0181\n",
      "     10        3.1753  0.0146\n",
      "Restoring best model from epoch 1.\n",
      "      5        3.3750  0.0168\n",
      "      2        3.8988  0.0317\n",
      "      3        3.2441  0.0170\n",
      "      4        2.9155  0.0337\n",
      "      6        3.3750  0.0205\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      3        3.8988  0.0174\n",
      "      4        3.2441  0.0203\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      7        3.3750  0.0177\n",
      "      4        3.8988  0.0156\n",
      "      5        2.9155  0.0270\n",
      "      5        3.2441  0.0234\n",
      "      8        3.3750  0.0185\n",
      "      6        2.9155  0.0166\n",
      "      5        3.8988  0.0195\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      9        3.3750  0.0205\n",
      "      6        3.2441  0.0255\n",
      "      6        3.8988  0.0167\n",
      "      7        2.9155  0.0239\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      7        3.8988  0.0161\n",
      "     10        3.3750  0.0183\n",
      "      7        3.2441  0.0177\n",
      "Restoring best model from epoch 1.\n",
      "      8        2.9155  0.0221\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      8        3.8988  0.0175\n",
      "      8        3.2441  0.0171\n",
      "      9        2.9155  0.0271\n",
      "      9        3.2441  0.0190\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      9        3.8988  0.0307\n",
      "     10        2.9155  0.0171\n",
      "Restoring best model from epoch 1.\n",
      "     10        3.2441  0.0183\n",
      "Restoring best model from epoch 1.\n",
      "     10        3.8988  0.0176\n",
      "Restoring best model from epoch 1.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.6993\u001b[0m        \u001b[32m2.9154\u001b[0m  0.2735\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.4267\u001b[0m        \u001b[32m3.3737\u001b[0m  0.2246\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.1083\u001b[0m        \u001b[32m3.2447\u001b[0m  0.2769\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.4133\u001b[0m        \u001b[32m3.8981\u001b[0m  0.2265\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.3125\u001b[0m        \u001b[32m3.1771\u001b[0m  0.2934\n",
      "      2        \u001b[36m2.6780\u001b[0m        2.9158  0.1549\n",
      "      2        \u001b[36m3.4264\u001b[0m        3.3743  0.1451\n",
      "      2        \u001b[36m3.0944\u001b[0m        3.2474  0.1508\n",
      "      2        \u001b[36m3.4080\u001b[0m        3.8994  0.1316\n",
      "      2        \u001b[36m3.3076\u001b[0m        3.1786  0.1363\n",
      "      3        \u001b[36m2.6584\u001b[0m        2.9163  0.1344\n",
      "      3        \u001b[36m3.3819\u001b[0m        3.3750  0.1266\n",
      "      3        \u001b[36m3.0834\u001b[0m        3.2478  0.1272\n",
      "      3        \u001b[36m3.3654\u001b[0m        3.9015  0.1240\n",
      "      3        \u001b[36m3.2914\u001b[0m        3.1798  0.1260\n",
      "      4        2.6585        2.9165  0.1246\n",
      "      4        3.3841        3.3747  0.1240\n",
      "      4        \u001b[36m3.0669\u001b[0m        3.2472  0.1222\n",
      "      4        3.3846        3.9032  0.1197\n",
      "      5        2.7047        2.9167  0.1244\n",
      "      4        \u001b[36m3.2873\u001b[0m        3.1805  0.1428\n",
      "      5        3.3931        3.3747  0.1276\n",
      "      5        3.0777        3.2468  0.1266\n",
      "      5        3.3901        3.9047  0.1245\n",
      "      6        \u001b[36m2.6407\u001b[0m        2.9163  0.1234\n",
      "      5        3.2932        3.1800  0.1236\n",
      "      6        \u001b[36m3.3656\u001b[0m        3.3752  0.1222\n",
      "      6        \u001b[36m3.0638\u001b[0m        3.2463  0.1242\n",
      "      6        \u001b[36m3.3586\u001b[0m        3.9050  0.1237\n",
      "      7        2.6643        2.9159  0.1229\n",
      "      6        \u001b[36m3.2828\u001b[0m        3.1796  0.1272\n",
      "      7        3.3723        3.3747  0.1211\n",
      "      7        \u001b[36m3.0376\u001b[0m        3.2455  0.1309\n",
      "      7        3.3641        3.9057  0.1314\n",
      "      8        2.6511        \u001b[32m2.9153\u001b[0m  0.1553\n",
      "      7        3.2944        3.1788  0.1475\n",
      "      8        \u001b[36m3.3372\u001b[0m        \u001b[32m3.3731\u001b[0m  0.1467\n",
      "      8        3.0471        3.2447  0.1449\n",
      "      8        \u001b[36m3.3057\u001b[0m        3.9073  0.1405\n",
      "      9        2.6462        \u001b[32m2.9147\u001b[0m  0.1267\n",
      "      8        \u001b[36m3.2717\u001b[0m        3.1778  0.1334\n",
      "      9        3.3724        \u001b[32m3.3715\u001b[0m  0.1315\n",
      "      9        3.0553        \u001b[32m3.2440\u001b[0m  0.1296\n",
      "      9        3.3597        3.9086  0.1278\n",
      "     10        2.6737        \u001b[32m2.9145\u001b[0m  0.1273\n",
      "Restoring best model from epoch 9.\n",
      "      9        \u001b[36m3.2704\u001b[0m        \u001b[32m3.1770\u001b[0m  0.1287\n",
      "     10        3.3483        \u001b[32m3.3697\u001b[0m  0.1238\n",
      "     10        \u001b[36m3.0340\u001b[0m        \u001b[32m3.2428\u001b[0m  0.1403\n",
      "     10        3.3358        3.9099  0.1240\n",
      "Restoring best model from epoch 1.\n",
      "     10        3.2815        \u001b[32m3.1759\u001b[0m  0.1235\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.2753\u001b[0m  0.0139\n",
      "      2        3.2753  0.0132\n",
      "      3        3.2753  0.0115\n",
      "      4        3.2753  0.0117\n",
      "      5        3.2753  0.0123\n",
      "      6        3.2753  0.0103\n",
      "      7        3.2753  0.0244\n",
      "      8        3.2753  0.0130\n",
      "      9        3.2753  0.0100\n",
      "     10        3.2753  0.0093\n",
      "Restoring best model from epoch 1.\n",
      "y_train breslow final [ 2.000e+00  2.000e+00 -1.400e+01  1.400e+01  2.300e+01 -3.000e+01\n",
      "  5.600e+01 -5.600e+01  6.200e+01  6.400e+01  6.500e+01  7.600e+01\n",
      "  8.200e+01  8.600e+01  8.900e+01  9.000e+01 -9.200e+01  9.400e+01\n",
      "  9.500e+01 -1.020e+02 -1.050e+02  1.070e+02  1.080e+02 -1.100e+02\n",
      " -1.110e+02  1.120e+02 -1.220e+02  1.290e+02  1.290e+02 -1.340e+02\n",
      "  1.420e+02  1.440e+02 -1.460e+02  1.510e+02 -1.560e+02  1.600e+02\n",
      "  1.710e+02 -1.810e+02  1.820e+02  1.830e+02  1.910e+02  1.940e+02\n",
      "  1.940e+02  1.980e+02  2.050e+02 -2.100e+02  2.150e+02 -2.160e+02\n",
      "  2.170e+02  2.180e+02 -2.250e+02 -2.320e+02  2.430e+02  2.530e+02\n",
      "  2.560e+02  2.590e+02  2.610e+02  2.680e+02  2.740e+02  2.760e+02\n",
      "  2.760e+02 -2.780e+02  2.790e+02  2.810e+02  2.820e+02  2.830e+02\n",
      "  2.840e+02 -2.920e+02  2.920e+02  2.940e+02  2.950e+02  3.170e+02\n",
      "  3.170e+02 -3.180e+02 -3.210e+02  3.270e+02  3.270e+02  3.300e+02\n",
      "  3.340e+02  3.360e+02  3.370e+02  3.410e+02  3.420e+02 -3.510e+02\n",
      "  3.510e+02  3.520e+02 -3.540e+02  3.570e+02 -3.580e+02  3.590e+02\n",
      "  3.600e+02 -3.600e+02  3.610e+02 -3.620e+02  3.660e+02 -3.670e+02\n",
      " -3.680e+02  3.710e+02  3.770e+02  3.770e+02 -3.780e+02  3.790e+02\n",
      " -3.790e+02  3.800e+02  3.840e+02 -3.840e+02  3.850e+02  3.850e+02\n",
      " -3.860e+02  3.870e+02 -3.870e+02 -3.890e+02 -3.920e+02 -3.920e+02\n",
      " -3.920e+02  3.930e+02  3.970e+02 -4.000e+02 -4.030e+02  4.030e+02\n",
      "  4.060e+02  4.070e+02 -4.070e+02 -4.120e+02 -4.140e+02  4.150e+02\n",
      " -4.170e+02  4.210e+02 -4.210e+02  4.270e+02  4.300e+02  4.360e+02\n",
      " -4.370e+02 -4.400e+02 -4.410e+02 -4.430e+02  4.460e+02  4.490e+02\n",
      "  4.510e+02  4.530e+02  4.560e+02  4.590e+02  4.620e+02  4.640e+02\n",
      " -4.660e+02 -4.690e+02  4.790e+02 -4.790e+02  4.840e+02 -4.850e+02\n",
      "  4.890e+02 -4.910e+02  4.930e+02  4.950e+02 -4.990e+02  5.060e+02\n",
      " -5.090e+02 -5.140e+02 -5.160e+02 -5.180e+02 -5.200e+02  5.210e+02\n",
      "  5.220e+02  5.260e+02  5.300e+02 -5.420e+02  5.430e+02  5.450e+02\n",
      "  5.460e+02  5.460e+02 -5.470e+02  5.480e+02 -5.490e+02 -5.520e+02\n",
      " -5.590e+02  5.640e+02 -5.680e+02  5.720e+02 -5.730e+02 -5.760e+02\n",
      "  5.770e+02  5.770e+02 -5.780e+02 -5.790e+02  5.800e+02  5.840e+02\n",
      " -5.860e+02 -6.000e+02  6.010e+02 -6.030e+02  6.060e+02 -6.060e+02\n",
      " -6.160e+02 -6.160e+02 -6.190e+02 -6.220e+02 -6.220e+02 -6.240e+02\n",
      "  6.240e+02  6.250e+02  6.310e+02 -6.370e+02 -6.390e+02 -6.410e+02\n",
      "  6.410e+02 -6.440e+02 -6.450e+02 -6.450e+02 -6.460e+02 -6.530e+02\n",
      " -6.580e+02  6.630e+02  6.660e+02 -6.670e+02 -6.700e+02  6.750e+02\n",
      " -6.790e+02  6.800e+02 -6.820e+02 -6.850e+02 -6.870e+02 -6.930e+02\n",
      " -6.950e+02  6.950e+02 -7.010e+02 -7.040e+02 -7.070e+02 -7.100e+02\n",
      " -7.140e+02 -7.220e+02 -7.250e+02  7.270e+02  7.390e+02 -7.420e+02\n",
      " -7.590e+02 -7.590e+02 -7.590e+02  7.630e+02  7.730e+02 -7.820e+02\n",
      " -7.830e+02 -7.960e+02 -7.970e+02 -7.990e+02  8.060e+02 -8.170e+02\n",
      "  8.230e+02 -8.270e+02 -8.330e+02  8.360e+02 -8.360e+02 -8.500e+02\n",
      " -8.510e+02 -8.520e+02  8.530e+02 -8.550e+02  8.620e+02 -8.660e+02\n",
      " -8.750e+02  8.820e+02 -8.890e+02 -8.930e+02 -8.950e+02 -8.960e+02\n",
      " -8.970e+02 -9.100e+02  9.150e+02 -9.180e+02 -9.300e+02 -9.330e+02\n",
      "  9.410e+02 -9.470e+02 -9.500e+02 -9.540e+02 -9.720e+02  9.800e+02\n",
      "  9.850e+02 -9.870e+02  9.880e+02  9.930e+02 -9.930e+02 -9.950e+02\n",
      " -9.970e+02  9.980e+02 -1.000e+03 -1.021e+03 -1.022e+03 -1.025e+03\n",
      " -1.027e+03 -1.030e+03 -1.036e+03  1.037e+03 -1.038e+03 -1.045e+03\n",
      " -1.050e+03 -1.057e+03 -1.065e+03 -1.066e+03 -1.075e+03 -1.077e+03\n",
      "  1.079e+03  1.081e+03  1.093e+03 -1.095e+03 -1.117e+03 -1.125e+03\n",
      " -1.131e+03  1.133e+03  1.134e+03 -1.138e+03 -1.143e+03 -1.147e+03\n",
      " -1.152e+03 -1.157e+03 -1.168e+03 -1.172e+03 -1.179e+03 -1.183e+03\n",
      " -1.190e+03 -1.191e+03  1.202e+03 -1.218e+03 -1.224e+03 -1.245e+03\n",
      " -1.253e+03 -1.265e+03 -1.273e+03 -1.274e+03 -1.278e+03 -1.288e+03\n",
      "  1.289e+03 -1.311e+03 -1.315e+03 -1.346e+03 -1.353e+03 -1.358e+03\n",
      " -1.368e+03  1.394e+03 -1.409e+03 -1.409e+03 -1.425e+03  1.430e+03\n",
      " -1.435e+03 -1.440e+03 -1.443e+03  1.459e+03 -1.460e+03 -1.466e+03\n",
      "  1.466e+03 -1.472e+03 -1.478e+03 -1.483e+03  1.504e+03 -1.508e+03\n",
      " -1.512e+03 -1.523e+03 -1.527e+03 -1.555e+03 -1.559e+03 -1.560e+03\n",
      " -1.584e+03  1.591e+03 -1.593e+03 -1.628e+03  1.641e+03 -1.660e+03\n",
      " -1.663e+03 -1.663e+03 -1.665e+03 -1.686e+03 -1.690e+03 -1.699e+03\n",
      "  1.718e+03 -1.721e+03 -1.724e+03  1.732e+03 -1.737e+03  1.748e+03\n",
      "  1.762e+03  1.838e+03 -1.840e+03 -1.897e+03 -1.899e+03 -1.971e+03\n",
      "  1.972e+03 -1.995e+03 -2.016e+03  2.064e+03 -2.143e+03 -2.161e+03\n",
      " -2.169e+03 -2.182e+03 -2.239e+03 -2.265e+03 -2.298e+03  2.319e+03\n",
      " -2.327e+03 -2.359e+03 -2.562e+03  2.570e+03  2.703e+03 -2.727e+03\n",
      "  2.741e+03 -2.784e+03  2.900e+03  3.059e+03 -3.221e+03  3.314e+03\n",
      " -3.381e+03 -3.930e+03 -4.241e+03 -4.282e+03  4.760e+03  5.152e+03\n",
      " -5.480e+03]\n",
      "Concordance Index 0.4972002488667674\n",
      "Integrated Brier Score: 0.18719630833010428\n",
      "y_train breslow final [ 2.000e+00  2.000e+00 -1.400e+01  1.400e+01  2.300e+01 -3.000e+01\n",
      "  5.600e+01 -5.600e+01  6.200e+01  6.400e+01  6.500e+01  7.600e+01\n",
      "  8.200e+01  8.600e+01  8.900e+01  9.000e+01 -9.200e+01  9.400e+01\n",
      "  9.500e+01 -1.020e+02 -1.050e+02  1.070e+02  1.080e+02 -1.100e+02\n",
      " -1.110e+02  1.120e+02 -1.220e+02  1.290e+02  1.290e+02 -1.340e+02\n",
      "  1.420e+02  1.440e+02 -1.460e+02  1.510e+02 -1.560e+02  1.600e+02\n",
      "  1.710e+02 -1.810e+02  1.820e+02  1.830e+02  1.910e+02  1.940e+02\n",
      "  1.940e+02  1.980e+02  2.050e+02 -2.100e+02  2.150e+02 -2.160e+02\n",
      "  2.170e+02  2.180e+02 -2.250e+02 -2.320e+02  2.430e+02  2.530e+02\n",
      "  2.560e+02  2.590e+02  2.610e+02  2.680e+02  2.740e+02  2.760e+02\n",
      "  2.760e+02 -2.780e+02  2.790e+02  2.810e+02  2.820e+02  2.830e+02\n",
      "  2.840e+02 -2.920e+02  2.920e+02  2.940e+02  2.950e+02  3.170e+02\n",
      "  3.170e+02 -3.180e+02 -3.210e+02  3.270e+02  3.270e+02  3.300e+02\n",
      "  3.340e+02  3.360e+02  3.370e+02  3.410e+02  3.420e+02 -3.510e+02\n",
      "  3.510e+02  3.520e+02 -3.540e+02  3.570e+02 -3.580e+02  3.590e+02\n",
      "  3.600e+02 -3.600e+02  3.610e+02 -3.620e+02  3.660e+02 -3.670e+02\n",
      " -3.680e+02  3.710e+02  3.770e+02  3.770e+02 -3.780e+02  3.790e+02\n",
      " -3.790e+02  3.800e+02  3.840e+02 -3.840e+02  3.850e+02  3.850e+02\n",
      " -3.860e+02  3.870e+02 -3.870e+02 -3.890e+02 -3.920e+02 -3.920e+02\n",
      " -3.920e+02  3.930e+02  3.970e+02 -4.000e+02 -4.030e+02  4.030e+02\n",
      "  4.060e+02  4.070e+02 -4.070e+02 -4.120e+02 -4.140e+02  4.150e+02\n",
      " -4.170e+02  4.210e+02 -4.210e+02  4.270e+02  4.300e+02  4.360e+02\n",
      " -4.370e+02 -4.400e+02 -4.410e+02 -4.430e+02  4.460e+02  4.490e+02\n",
      "  4.510e+02  4.530e+02  4.560e+02  4.590e+02  4.620e+02  4.640e+02\n",
      " -4.660e+02 -4.690e+02  4.790e+02 -4.790e+02  4.840e+02 -4.850e+02\n",
      "  4.890e+02 -4.910e+02  4.930e+02  4.950e+02 -4.990e+02  5.060e+02\n",
      " -5.090e+02 -5.140e+02 -5.160e+02 -5.180e+02 -5.200e+02  5.210e+02\n",
      "  5.220e+02  5.260e+02  5.300e+02 -5.420e+02  5.430e+02  5.450e+02\n",
      "  5.460e+02  5.460e+02 -5.470e+02  5.480e+02 -5.490e+02 -5.520e+02\n",
      " -5.590e+02  5.640e+02 -5.680e+02  5.720e+02 -5.730e+02 -5.760e+02\n",
      "  5.770e+02  5.770e+02 -5.780e+02 -5.790e+02  5.800e+02  5.840e+02\n",
      " -5.860e+02 -6.000e+02  6.010e+02 -6.030e+02  6.060e+02 -6.060e+02\n",
      " -6.160e+02 -6.160e+02 -6.190e+02 -6.220e+02 -6.220e+02 -6.240e+02\n",
      "  6.240e+02  6.250e+02  6.310e+02 -6.370e+02 -6.390e+02 -6.410e+02\n",
      "  6.410e+02 -6.440e+02 -6.450e+02 -6.450e+02 -6.460e+02 -6.530e+02\n",
      " -6.580e+02  6.630e+02  6.660e+02 -6.670e+02 -6.700e+02  6.750e+02\n",
      " -6.790e+02  6.800e+02 -6.820e+02 -6.850e+02 -6.870e+02 -6.930e+02\n",
      " -6.950e+02  6.950e+02 -7.010e+02 -7.040e+02 -7.070e+02 -7.100e+02\n",
      " -7.140e+02 -7.220e+02 -7.250e+02  7.270e+02  7.390e+02 -7.420e+02\n",
      " -7.590e+02 -7.590e+02 -7.590e+02  7.630e+02  7.730e+02 -7.820e+02\n",
      " -7.830e+02 -7.960e+02 -7.970e+02 -7.990e+02  8.060e+02 -8.170e+02\n",
      "  8.230e+02 -8.270e+02 -8.330e+02  8.360e+02 -8.360e+02 -8.500e+02\n",
      " -8.510e+02 -8.520e+02  8.530e+02 -8.550e+02  8.620e+02 -8.660e+02\n",
      " -8.750e+02  8.820e+02 -8.890e+02 -8.930e+02 -8.950e+02 -8.960e+02\n",
      " -8.970e+02 -9.100e+02  9.150e+02 -9.180e+02 -9.300e+02 -9.330e+02\n",
      "  9.410e+02 -9.470e+02 -9.500e+02 -9.540e+02 -9.720e+02  9.800e+02\n",
      "  9.850e+02 -9.870e+02  9.880e+02  9.930e+02 -9.930e+02 -9.950e+02\n",
      " -9.970e+02  9.980e+02 -1.000e+03 -1.021e+03 -1.022e+03 -1.025e+03\n",
      " -1.027e+03 -1.030e+03 -1.036e+03  1.037e+03 -1.038e+03 -1.045e+03\n",
      " -1.050e+03 -1.057e+03 -1.065e+03 -1.066e+03 -1.075e+03 -1.077e+03\n",
      "  1.079e+03  1.081e+03  1.093e+03 -1.095e+03 -1.117e+03 -1.125e+03\n",
      " -1.131e+03  1.133e+03  1.134e+03 -1.138e+03 -1.143e+03 -1.147e+03\n",
      " -1.152e+03 -1.157e+03 -1.168e+03 -1.172e+03 -1.179e+03 -1.183e+03\n",
      " -1.190e+03 -1.191e+03  1.202e+03 -1.218e+03 -1.224e+03 -1.245e+03\n",
      " -1.253e+03 -1.265e+03 -1.273e+03 -1.274e+03 -1.278e+03 -1.288e+03\n",
      "  1.289e+03 -1.311e+03 -1.315e+03 -1.346e+03 -1.353e+03 -1.358e+03\n",
      " -1.368e+03  1.394e+03 -1.409e+03 -1.409e+03 -1.425e+03  1.430e+03\n",
      " -1.435e+03 -1.440e+03 -1.443e+03  1.459e+03 -1.460e+03 -1.466e+03\n",
      "  1.466e+03 -1.472e+03 -1.478e+03 -1.483e+03  1.504e+03 -1.508e+03\n",
      " -1.512e+03 -1.523e+03 -1.527e+03 -1.555e+03 -1.559e+03 -1.560e+03\n",
      " -1.584e+03  1.591e+03 -1.593e+03 -1.628e+03  1.641e+03 -1.660e+03\n",
      " -1.663e+03 -1.663e+03 -1.665e+03 -1.686e+03 -1.690e+03 -1.699e+03\n",
      "  1.718e+03 -1.721e+03 -1.724e+03  1.732e+03 -1.737e+03  1.748e+03\n",
      "  1.762e+03  1.838e+03 -1.840e+03 -1.897e+03 -1.899e+03 -1.971e+03\n",
      "  1.972e+03 -1.995e+03 -2.016e+03  2.064e+03 -2.143e+03 -2.161e+03\n",
      " -2.169e+03 -2.182e+03 -2.239e+03 -2.265e+03 -2.298e+03  2.319e+03\n",
      " -2.327e+03 -2.359e+03 -2.562e+03  2.570e+03  2.703e+03 -2.727e+03\n",
      "  2.741e+03 -2.784e+03  2.900e+03  3.059e+03 -3.221e+03  3.314e+03\n",
      " -3.381e+03 -3.930e+03 -4.241e+03 -4.282e+03  4.760e+03  5.152e+03\n",
      " -5.480e+03]\n",
      "durations 11.0 6417.0\n",
      "Concordance Index 0.4849624060150376\n",
      "Integrated Brier Score: 0.20654197679514275\n",
      "(415, 20531) <class 'pandas.core.frame.DataFrame'>\n",
      "(415,) <class 'pandas.core.series.Series'>\n",
      "(104, 20531) <class 'pandas.core.frame.DataFrame'>\n",
      "(104,) <class 'pandas.core.series.Series'>\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.1633\u001b[0m  0.0141\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.4272\u001b[0m  0.0156\n",
      "      2        3.1633  0.0179\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      3        3.1633  0.0150\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2        3.4272  0.0195\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.3970\u001b[0m  0.0266\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      3        3.4272  0.0251\n",
      "      2        3.3970  0.0258\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      4        3.1633  0.0420\n",
      "      3        3.3970  0.0148\n",
      "      4        3.4272  0.0257\n",
      "      5        3.1633  0.0149\n",
      "      4        3.3970  0.0169\n",
      "      5        3.4272  0.0138\n",
      "      6        3.1633  0.0126\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m2.8248\u001b[0m  0.0378\n",
      "      5        3.3970  0.0127\n",
      "      6        3.4272  0.0125\n",
      "      7        3.1633  0.0133\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      7        3.4272  0.0160\n",
      "      6        3.3970  0.0185\n",
      "      8        3.1633  0.0154\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      9        3.1633  0.0148\n",
      "      8        3.4272  0.0180\n",
      "      7        3.3970  0.0161\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2        2.8248  0.0382\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "     10        3.1633  0.0195\n",
      "      9        3.4272  0.0204\n",
      "Restoring best model from epoch 1.\n",
      "      8        3.3970  0.0212\n",
      "      3        2.8248  0.0199\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.4321\u001b[0m        \u001b[32m3.3956\u001b[0m  0.1957\n",
      "      9        3.3970  0.0145\n",
      "      4        2.8248  0.0164\n",
      "     10        3.4272  0.0194\n",
      "Restoring best model from epoch 1.\n",
      "     10        3.3970  0.0170\n",
      "Restoring best model from epoch 1.\n",
      "      5        2.8248  0.0174\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.2252\u001b[0m  0.0412\n",
      "      6        2.8248  0.0121\n",
      "      2        3.2252  0.0133\n",
      "      3        3.2252  0.0115\n",
      "      7        2.8248  0.0215\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.2794\u001b[0m        \u001b[32m3.2207\u001b[0m  0.2117\n",
      "      4        3.2252  0.0170\n",
      "      8        2.8248  0.0127\n",
      "      5        3.2252  0.0122\n",
      "      6        3.2252  0.0112\n",
      "      9        2.8248  0.0255\n",
      "      7        3.2252  0.0110\n",
      "     10        2.8248  0.0176\n",
      "      8        3.2252  0.0111\n",
      "Restoring best model from epoch 1.\n",
      "      9        3.2252  0.0114\n",
      "     10        3.2252  0.0167\n",
      "Restoring best model from epoch 1.\n",
      "      2        3.4471        \u001b[32m3.3926\u001b[0m  0.1537\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.0702\u001b[0m        \u001b[32m3.1630\u001b[0m  0.2074\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.7329\u001b[0m        \u001b[32m2.8247\u001b[0m  0.1905\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.5509\u001b[0m        \u001b[32m3.4238\u001b[0m  0.2240\n",
      "      2        \u001b[36m3.2733\u001b[0m        \u001b[32m3.2164\u001b[0m  0.1425\n",
      "      3        3.4454        \u001b[32m3.3901\u001b[0m  0.1360\n",
      "      2        \u001b[36m3.0622\u001b[0m        \u001b[32m3.1620\u001b[0m  0.1459\n",
      "      2        \u001b[36m2.7262\u001b[0m        \u001b[32m2.8228\u001b[0m  0.1563\n",
      "      3        \u001b[36m3.2551\u001b[0m        \u001b[32m3.2125\u001b[0m  0.1377\n",
      "      2        \u001b[36m3.5447\u001b[0m        \u001b[32m3.4158\u001b[0m  0.1575\n",
      "      4        \u001b[36m3.4236\u001b[0m        3.3902  0.1345\n",
      "      3        \u001b[36m3.0538\u001b[0m        \u001b[32m3.1606\u001b[0m  0.1307\n",
      "      3        \u001b[36m2.7246\u001b[0m        \u001b[32m2.8213\u001b[0m  0.1293\n",
      "      3        \u001b[36m3.5163\u001b[0m        \u001b[32m3.4109\u001b[0m  0.1253\n",
      "      4        3.2601        \u001b[32m3.2089\u001b[0m  0.1280\n",
      "      5        3.4381        \u001b[32m3.3900\u001b[0m  0.1283\n",
      "      4        \u001b[36m3.0486\u001b[0m        \u001b[32m3.1601\u001b[0m  0.1228\n",
      "      4        2.7450        2.8216  0.1198\n",
      "      4        3.5198        \u001b[32m3.4087\u001b[0m  0.1289\n",
      "      5        3.2617        \u001b[32m3.2067\u001b[0m  0.1280\n",
      "      6        \u001b[36m3.4228\u001b[0m        3.3909  0.1239\n",
      "      5        3.0597        \u001b[32m3.1592\u001b[0m  0.1196\n",
      "      5        2.7304        2.8216  0.1218\n",
      "      5        \u001b[36m3.5101\u001b[0m        \u001b[32m3.4074\u001b[0m  0.1287\n",
      "      6        \u001b[36m3.2527\u001b[0m        \u001b[32m3.2046\u001b[0m  0.1308\n",
      "      7        \u001b[36m3.4119\u001b[0m        3.3912  0.1252\n",
      "      6        3.0553        \u001b[32m3.1585\u001b[0m  0.1191\n",
      "      6        2.7372        2.8215  0.1211\n",
      "      6        \u001b[36m3.4724\u001b[0m        3.4076  0.1259\n",
      "      7        3.2593        \u001b[32m3.2025\u001b[0m  0.1296\n",
      "      8        3.4208        3.3908  0.1233\n",
      "      7        \u001b[36m3.0136\u001b[0m        \u001b[32m3.1577\u001b[0m  0.1302\n",
      "      7        \u001b[36m2.7099\u001b[0m        \u001b[32m2.8212\u001b[0m  0.1399\n",
      "      7        3.5083        3.4077  0.1288\n",
      "      8        \u001b[36m3.2202\u001b[0m        \u001b[32m3.2009\u001b[0m  0.1244\n",
      "      9        3.4140        \u001b[32m3.3896\u001b[0m  0.1246\n",
      "      8        3.0483        \u001b[32m3.1561\u001b[0m  0.1246\n",
      "      8        \u001b[36m2.7016\u001b[0m        \u001b[32m2.8206\u001b[0m  0.1253\n",
      "      8        3.4865        \u001b[32m3.4074\u001b[0m  0.1212\n",
      "      9        3.2315        \u001b[32m3.1981\u001b[0m  0.1275\n",
      "     10        \u001b[36m3.3921\u001b[0m        \u001b[32m3.3879\u001b[0m  0.1229\n",
      "      9        3.0346        \u001b[32m3.1541\u001b[0m  0.1228\n",
      "      9        2.7104        2.8206  0.1206\n",
      "      9        3.5024        \u001b[32m3.4070\u001b[0m  0.1207\n",
      "     10        \u001b[36m3.2141\u001b[0m        \u001b[32m3.1940\u001b[0m  0.1196\n",
      "     10        \u001b[36m3.0004\u001b[0m        \u001b[32m3.1510\u001b[0m  0.1190\n",
      "     10        2.7399        \u001b[32m2.8205\u001b[0m  0.1180\n",
      "Restoring best model from epoch 8.\n",
      "     10        \u001b[36m3.4675\u001b[0m        \u001b[32m3.4059\u001b[0m  0.1190\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.7204\u001b[0m  0.0147\n",
      "      2        3.7204  0.0115\n",
      "      3        3.7204  0.0257\n",
      "      4        3.7204  0.0348\n",
      "      5        3.7204  0.0112\n",
      "      6        3.7204  0.0122\n",
      "      7        3.7204  0.0116\n",
      "      8        3.7204  0.0213\n",
      "      9        3.7204  0.0105\n",
      "     10        3.7204  0.0239\n",
      "Restoring best model from epoch 1.\n",
      "y_train breslow final [  -11.    11.   -14.    14.    23.   -30.    32.   -56.    62.    64.\n",
      "    69.    76.   -77.    82.    86.    89.    90.    95.  -105.   107.\n",
      "   108.  -110.   112.  -122.   126.   129.   129.   130.   142.   144.\n",
      "   144.  -146.   151.  -156.   166.   171.   173.  -181.   182.   183.\n",
      "   185.  -187.   191.   197.   198.   205.   209.  -210.   215.   215.\n",
      "  -216.  -216.   217.   217.   222.  -225.  -232.  -234.  -242.  -253.\n",
      "   253.   256.   259.   261.   274.   276.   276.   279.   281.   282.\n",
      "   283.  -292.   292.   295.  -311.   317.  -321.   321.   327.   330.\n",
      "   334.   336.   337.   341.   342.  -345.   348.  -351.   352.   353.\n",
      "  -354.   357.  -358.   359.   360.  -362.   366.  -367.   371.  -376.\n",
      "   377.  -379.   380.   384.   385.   385.  -386.   387.  -387.  -389.\n",
      "  -392.  -392.  -392.  -393.   395.   395.   397.  -400.  -403.   403.\n",
      "   406.   406.   407.  -407.  -412.  -414.   415.  -417.  -424.   424.\n",
      "   427.   430.   436.  -437.  -440.  -441.   446.   449.   451.   456.\n",
      "  -458.   459.  -461.   462.   464.  -471.   472.  -479.   480.  -485.\n",
      "   487.   489.   493.  -499.   506.  -509.  -514.  -520.   521.   521.\n",
      "   522.   530.  -539.  -542.  -543.   545.   546.   548.  -549.  -552.\n",
      "  -559.   560.   564.  -573.  -576.   577.  -578.  -579.   580.   584.\n",
      "  -586.  -594.  -600.   601.   602.  -603.  -606.  -616.  -616.  -622.\n",
      "  -622.  -623.  -623.  -624.   624.  -625.  -635.  -637.  -639.  -641.\n",
      "  -641.   641.  -644.  -645.  -645.  -646.  -653.   654.  -658.   663.\n",
      "  -667.   675.  -685.  -687.  -695.   695.  -701.  -701.  -704.  -710.\n",
      "  -713.  -714.  -717.  -722.  -725.   727.   739.  -750.  -754.  -759.\n",
      "  -759.  -759.   763.  -770.   773.  -773.  -782.  -783.   789.  -796.\n",
      "   804.   806.   823.  -827.  -833.   836.  -836.   839.  -850.  -852.\n",
      "   853.  -855.   862.  -866.  -875.   882.  -889.  -895.  -896.  -897.\n",
      "  -904.  -906.  -911.  -913.  -914.   915.  -918.   927.  -930.  -933.\n",
      "   941.  -947.  -954.  -964.  -972.   985.  -987.   988.  -993.  -995.\n",
      "   998. -1000. -1022. -1025. -1027. -1030.  1037. -1045. -1050. -1057.\n",
      " -1065. -1066. -1075. -1077.  1079.  1081.  1090.  1093. -1095. -1117.\n",
      " -1120. -1125. -1131.  1134. -1143. -1147. -1152. -1157. -1168. -1183.\n",
      " -1190. -1191. -1218. -1253. -1265. -1274. -1278. -1288.  1289. -1309.\n",
      " -1311. -1315. -1353. -1358.  1394.  1398. -1399. -1413. -1428.  1430.\n",
      " -1430. -1435. -1440. -1443.  1459. -1460. -1461. -1466. -1472. -1478.\n",
      "  1504. -1506. -1512. -1521. -1523. -1555. -1559. -1560. -1584. -1584.\n",
      " -1586.  1591. -1593.  1641. -1660. -1663. -1665.  1671. -1686. -1690.\n",
      " -1699.  1718. -1721. -1722. -1724. -1737.  1748.  1762. -1777. -1778.\n",
      "  1838. -1897. -1899. -1995.  2002. -2016. -2016.  2083.  2120. -2133.\n",
      " -2143. -2161.  2166. -2169. -2182. -2239. -2265. -2298.  2319. -2327.\n",
      " -2347. -2359. -2437. -2562.  2570. -2641.  2703.  2717.  2741. -2784.\n",
      " -2886.  2900.  3059. -3221. -3270.  3314. -3381. -3981. -4282.  4680.\n",
      "  4760.  4856.  5152. -5252.  6417.]\n",
      "Concordance Index 0.4932168784029038\n",
      "Integrated Brier Score: 0.1743186432757114\n",
      "y_train breslow final [  -11.    11.   -14.    14.    23.   -30.    32.   -56.    62.    64.\n",
      "    69.    76.   -77.    82.    86.    89.    90.    95.  -105.   107.\n",
      "   108.  -110.   112.  -122.   126.   129.   129.   130.   142.   144.\n",
      "   144.  -146.   151.  -156.   166.   171.   173.  -181.   182.   183.\n",
      "   185.  -187.   191.   197.   198.   205.   209.  -210.   215.   215.\n",
      "  -216.  -216.   217.   217.   222.  -225.  -232.  -234.  -242.  -253.\n",
      "   253.   256.   259.   261.   274.   276.   276.   279.   281.   282.\n",
      "   283.  -292.   292.   295.  -311.   317.  -321.   321.   327.   330.\n",
      "   334.   336.   337.   341.   342.  -345.   348.  -351.   352.   353.\n",
      "  -354.   357.  -358.   359.   360.  -362.   366.  -367.   371.  -376.\n",
      "   377.  -379.   380.   384.   385.   385.  -386.   387.  -387.  -389.\n",
      "  -392.  -392.  -392.  -393.   395.   395.   397.  -400.  -403.   403.\n",
      "   406.   406.   407.  -407.  -412.  -414.   415.  -417.  -424.   424.\n",
      "   427.   430.   436.  -437.  -440.  -441.   446.   449.   451.   456.\n",
      "  -458.   459.  -461.   462.   464.  -471.   472.  -479.   480.  -485.\n",
      "   487.   489.   493.  -499.   506.  -509.  -514.  -520.   521.   521.\n",
      "   522.   530.  -539.  -542.  -543.   545.   546.   548.  -549.  -552.\n",
      "  -559.   560.   564.  -573.  -576.   577.  -578.  -579.   580.   584.\n",
      "  -586.  -594.  -600.   601.   602.  -603.  -606.  -616.  -616.  -622.\n",
      "  -622.  -623.  -623.  -624.   624.  -625.  -635.  -637.  -639.  -641.\n",
      "  -641.   641.  -644.  -645.  -645.  -646.  -653.   654.  -658.   663.\n",
      "  -667.   675.  -685.  -687.  -695.   695.  -701.  -701.  -704.  -710.\n",
      "  -713.  -714.  -717.  -722.  -725.   727.   739.  -750.  -754.  -759.\n",
      "  -759.  -759.   763.  -770.   773.  -773.  -782.  -783.   789.  -796.\n",
      "   804.   806.   823.  -827.  -833.   836.  -836.   839.  -850.  -852.\n",
      "   853.  -855.   862.  -866.  -875.   882.  -889.  -895.  -896.  -897.\n",
      "  -904.  -906.  -911.  -913.  -914.   915.  -918.   927.  -930.  -933.\n",
      "   941.  -947.  -954.  -964.  -972.   985.  -987.   988.  -993.  -995.\n",
      "   998. -1000. -1022. -1025. -1027. -1030.  1037. -1045. -1050. -1057.\n",
      " -1065. -1066. -1075. -1077.  1079.  1081.  1090.  1093. -1095. -1117.\n",
      " -1120. -1125. -1131.  1134. -1143. -1147. -1152. -1157. -1168. -1183.\n",
      " -1190. -1191. -1218. -1253. -1265. -1274. -1278. -1288.  1289. -1309.\n",
      " -1311. -1315. -1353. -1358.  1394.  1398. -1399. -1413. -1428.  1430.\n",
      " -1430. -1435. -1440. -1443.  1459. -1460. -1461. -1466. -1472. -1478.\n",
      "  1504. -1506. -1512. -1521. -1523. -1555. -1559. -1560. -1584. -1584.\n",
      " -1586.  1591. -1593.  1641. -1660. -1663. -1665.  1671. -1686. -1690.\n",
      " -1699.  1718. -1721. -1722. -1724. -1737.  1748.  1762. -1777. -1778.\n",
      "  1838. -1897. -1899. -1995.  2002. -2016. -2016.  2083.  2120. -2133.\n",
      " -2143. -2161.  2166. -2169. -2182. -2239. -2265. -2298.  2319. -2327.\n",
      " -2347. -2359. -2437. -2562.  2570. -2641.  2703.  2717.  2741. -2784.\n",
      " -2886.  2900.  3059. -3221. -3270.  3314. -3381. -3981. -4282.  4680.\n",
      "  4760.  4856.  5152. -5252.  6417.]\n",
      "durations 2.0 5480.0\n",
      "Concordance Index 0.4952978056426332\n",
      "Integrated Brier Score: 0.21235834970634263\n",
      "(416, 20531) <class 'pandas.core.frame.DataFrame'>\n",
      "(416,) <class 'pandas.core.series.Series'>\n",
      "(103, 20531) <class 'pandas.core.frame.DataFrame'>\n",
      "(103,) <class 'pandas.core.series.Series'>\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.4875\u001b[0m  0.0182\n",
      "      2        3.4875  0.0327\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      3        3.4875  0.0136\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      4        3.4875  0.0140\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.7152\u001b[0m  0.0164\n",
      "      2        3.7152  0.0309\n",
      "      5        3.4875  0.0409\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      3        3.7152  0.0202\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      6        3.4875  0.0230\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      4        3.7152  0.0148\n",
      "      7        3.4875  0.0160\n",
      "      5        3.7152  0.0192\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      8        3.4875  0.0162\n",
      "      6        3.7152  0.0179\n",
      "      9        3.4875  0.0162\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "     10        3.4875  0.0133\n",
      "Restoring best model from epoch 1.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.0517\u001b[0m  0.0637\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.6792\u001b[0m  0.0575\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      7        3.7152  0.0340\n",
      "      2        3.0517  0.0161\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m2.5716\u001b[0m  0.0233\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2        3.6792  0.0271\n",
      "      8        3.7152  0.0222\n",
      "      3        3.0517  0.0178\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      2        2.5716  0.0166\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      4        3.0517  0.0154\n",
      "      3        3.6792  0.0224\n",
      "      3        2.5716  0.0175\n",
      "      9        3.7152  0.0272\n",
      "      5        3.0517  0.0174\n",
      "      4        3.6792  0.0248\n",
      "     10        3.7152  0.0218\n",
      "      4        2.5716  0.0251\n",
      "      6        3.0517  0.0205\n",
      "Restoring best model from epoch 1.\n",
      "      5        3.6792  0.0156\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      5        2.5716  0.0212\n",
      "      7        3.0517  0.0184\n",
      "      6        3.6792  0.0185\n",
      "      6        2.5716  0.0203\n",
      "      8        3.0517  0.0194\n",
      "      7        3.6792  0.0159\n",
      "      9        3.0517  0.0152\n",
      "      7        2.5716  0.0306\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.3410\u001b[0m        \u001b[32m3.6842\u001b[0m  0.2930\n",
      "      8        3.6792  0.0319\n",
      "     10        3.0517  0.0220\n",
      "Restoring best model from epoch 1.\n",
      "      8        2.5716  0.0185\n",
      "      9        3.6792  0.0155\n",
      "     10        3.6792  0.0143\n",
      "Restoring best model from epoch 1.\n",
      "      9        2.5716  0.0202\n",
      "     10        2.5716  0.0140\n",
      "Restoring best model from epoch 1.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.4372\u001b[0m        \u001b[32m3.7156\u001b[0m  0.2088\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.8806\u001b[0m        \u001b[32m2.5683\u001b[0m  0.2530\n",
      "      2        3.3437        3.6862  0.1558\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.0239\u001b[0m        \u001b[32m3.0526\u001b[0m  0.2904\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.2021\u001b[0m        \u001b[32m3.4855\u001b[0m  0.2518\n",
      "      2        \u001b[36m3.4317\u001b[0m        \u001b[32m3.7150\u001b[0m  0.1463\n",
      "      2        \u001b[36m2.8764\u001b[0m        \u001b[32m2.5634\u001b[0m  0.1502\n",
      "      3        \u001b[36m3.3285\u001b[0m        3.6850  0.1396\n",
      "      2        \u001b[36m3.0213\u001b[0m        3.0536  0.1374\n",
      "      2        \u001b[36m3.1883\u001b[0m        \u001b[32m3.4840\u001b[0m  0.1307\n",
      "      3        \u001b[36m3.4213\u001b[0m        \u001b[32m3.7130\u001b[0m  0.1281\n",
      "      3        \u001b[36m2.8666\u001b[0m        \u001b[32m2.5631\u001b[0m  0.1263\n",
      "      4        \u001b[36m3.3072\u001b[0m        3.6844  0.1442\n",
      "      3        \u001b[36m2.9931\u001b[0m        3.0554  0.1273\n",
      "      4        \u001b[36m3.4021\u001b[0m        \u001b[32m3.7111\u001b[0m  0.1266\n",
      "      3        3.1978        \u001b[32m3.4836\u001b[0m  0.1546\n",
      "      4        \u001b[36m2.8619\u001b[0m        2.5645  0.1383\n",
      "      4        3.0101        3.0574  0.1339\n",
      "      5        3.3115        \u001b[32m3.6830\u001b[0m  0.1380\n",
      "      5        3.4151        \u001b[32m3.7082\u001b[0m  0.1356\n",
      "      4        \u001b[36m3.1881\u001b[0m        3.4844  0.1332\n",
      "      5        2.8681        2.5660  0.1238\n",
      "      5        3.0099        3.0588  0.1257\n",
      "      6        \u001b[36m3.3037\u001b[0m        \u001b[32m3.6818\u001b[0m  0.1297\n",
      "      6        \u001b[36m3.3551\u001b[0m        \u001b[32m3.7068\u001b[0m  0.1235\n",
      "      5        \u001b[36m3.1723\u001b[0m        3.4855  0.1217\n",
      "      6        2.8726        2.5681  0.1241\n",
      "      6        \u001b[36m2.9869\u001b[0m        3.0610  0.1242\n",
      "      7        \u001b[36m3.2926\u001b[0m        \u001b[32m3.6816\u001b[0m  0.1224\n",
      "      7        3.3887        \u001b[32m3.7057\u001b[0m  0.1222\n",
      "      6        \u001b[36m3.1551\u001b[0m        3.4844  0.1220\n",
      "      7        2.8685        2.5694  0.1240\n",
      "      7        \u001b[36m2.9732\u001b[0m        3.0624  0.1251\n",
      "      8        \u001b[36m3.2736\u001b[0m        3.6820  0.1274\n",
      "      8        3.3875        \u001b[32m3.7054\u001b[0m  0.1282\n",
      "      7        3.1832        3.4856  0.1298\n",
      "      8        \u001b[36m2.8347\u001b[0m        2.5709  0.1198\n",
      "      8        \u001b[36m2.9658\u001b[0m        3.0639  0.1273\n",
      "      9        3.2811        3.6831  0.1248\n",
      "      9        3.3944        3.7062  0.1267\n",
      "      8        3.1625        3.4874  0.1298\n",
      "      9        2.8513        2.5727  0.1282\n",
      "      9        2.9781        3.0653  0.1332\n",
      "     10        3.2899        3.6843  0.1322\n",
      "Restoring best model from epoch 6.\n",
      "     10        3.3860        3.7077  0.1275\n",
      "Restoring best model from epoch 7.\n",
      "      9        3.1763        3.4881  0.1292\n",
      "     10        2.8396        2.5743  0.1216\n",
      "Restoring best model from epoch 3.\n",
      "     10        2.9758        3.0668  0.1238\n",
      "Restoring best model from epoch 1.\n",
      "     10        \u001b[36m3.1465\u001b[0m        3.4893  0.1208\n",
      "Restoring best model from epoch 3.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m2.6732\u001b[0m  0.0151\n",
      "      2        2.6732  0.0155\n",
      "      3        2.6732  0.0148\n",
      "      4        2.6732  0.0117\n",
      "      5        2.6732  0.0119\n",
      "      6        2.6732  0.0120\n",
      "      7        2.6732  0.0159\n",
      "      8        2.6732  0.0099\n",
      "      9        2.6732  0.0142\n",
      "     10        2.6732  0.0124\n",
      "Restoring best model from epoch 1.\n",
      "y_train breslow final [ 2.000e+00  2.000e+00 -1.100e+01  1.100e+01 -1.400e+01  2.300e+01\n",
      " -3.000e+01  3.200e+01  5.600e+01 -5.600e+01  6.400e+01  6.500e+01\n",
      "  6.900e+01  7.600e+01 -7.700e+01  8.600e+01  8.900e+01 -9.200e+01\n",
      "  9.400e+01  9.500e+01 -1.020e+02  1.080e+02 -1.100e+02 -1.110e+02\n",
      " -1.220e+02  1.260e+02  1.290e+02  1.300e+02 -1.340e+02  1.440e+02\n",
      "  1.440e+02 -1.460e+02  1.510e+02  1.600e+02  1.660e+02  1.710e+02\n",
      "  1.730e+02  1.830e+02  1.850e+02 -1.870e+02  1.910e+02  1.940e+02\n",
      "  1.940e+02  1.970e+02  1.980e+02  2.090e+02 -2.100e+02  2.150e+02\n",
      "  2.150e+02 -2.160e+02 -2.160e+02  2.170e+02  2.180e+02  2.220e+02\n",
      " -2.320e+02 -2.340e+02 -2.420e+02  2.430e+02 -2.530e+02  2.560e+02\n",
      "  2.590e+02  2.680e+02  2.740e+02  2.760e+02  2.760e+02 -2.780e+02\n",
      "  2.810e+02  2.820e+02  2.830e+02  2.840e+02 -2.920e+02  2.920e+02\n",
      "  2.940e+02 -3.110e+02  3.170e+02  3.170e+02 -3.180e+02 -3.210e+02\n",
      "  3.210e+02  3.270e+02  3.300e+02  3.340e+02  3.360e+02  3.370e+02\n",
      "  3.410e+02  3.420e+02 -3.450e+02  3.480e+02 -3.510e+02  3.510e+02\n",
      "  3.520e+02  3.530e+02 -3.580e+02  3.590e+02  3.600e+02 -3.600e+02\n",
      "  3.610e+02 -3.620e+02  3.660e+02 -3.680e+02 -3.760e+02  3.770e+02\n",
      "  3.770e+02 -3.780e+02  3.790e+02  3.840e+02 -3.840e+02  3.850e+02\n",
      "  3.850e+02  3.870e+02 -3.870e+02 -3.920e+02 -3.920e+02  3.930e+02\n",
      " -3.930e+02  3.950e+02  3.950e+02  3.970e+02 -4.030e+02  4.030e+02\n",
      "  4.030e+02  4.060e+02  4.060e+02  4.070e+02 -4.070e+02 -4.120e+02\n",
      "  4.150e+02 -4.170e+02  4.210e+02 -4.210e+02 -4.240e+02  4.240e+02\n",
      "  4.270e+02  4.300e+02 -4.370e+02 -4.400e+02 -4.430e+02  4.490e+02\n",
      "  4.510e+02  4.530e+02 -4.580e+02 -4.610e+02  4.620e+02  4.640e+02\n",
      " -4.660e+02 -4.690e+02 -4.710e+02  4.720e+02  4.790e+02 -4.790e+02\n",
      "  4.800e+02  4.840e+02 -4.850e+02  4.870e+02  4.890e+02 -4.910e+02\n",
      "  4.930e+02  4.950e+02 -5.090e+02 -5.140e+02 -5.160e+02 -5.180e+02\n",
      " -5.200e+02  5.210e+02  5.260e+02  5.300e+02 -5.390e+02 -5.420e+02\n",
      " -5.430e+02  5.430e+02  5.450e+02  5.460e+02  5.460e+02 -5.470e+02\n",
      "  5.480e+02 -5.490e+02 -5.520e+02 -5.590e+02  5.600e+02  5.640e+02\n",
      " -5.680e+02  5.720e+02 -5.730e+02 -5.760e+02  5.770e+02  5.770e+02\n",
      " -5.790e+02  5.800e+02 -5.860e+02 -5.940e+02 -6.000e+02  6.010e+02\n",
      "  6.020e+02 -6.030e+02  6.060e+02 -6.160e+02 -6.160e+02 -6.190e+02\n",
      " -6.220e+02 -6.220e+02 -6.230e+02 -6.230e+02 -6.240e+02  6.240e+02\n",
      "  6.250e+02 -6.250e+02  6.310e+02 -6.350e+02 -6.370e+02 -6.410e+02\n",
      " -6.410e+02  6.410e+02 -6.440e+02 -6.450e+02 -6.450e+02 -6.530e+02\n",
      "  6.540e+02 -6.580e+02  6.630e+02  6.660e+02 -6.670e+02 -6.700e+02\n",
      " -6.790e+02  6.800e+02 -6.820e+02 -6.850e+02 -6.870e+02 -6.930e+02\n",
      " -6.950e+02 -7.010e+02 -7.010e+02 -7.070e+02 -7.100e+02 -7.130e+02\n",
      " -7.140e+02 -7.170e+02 -7.220e+02 -7.250e+02  7.270e+02  7.390e+02\n",
      " -7.420e+02 -7.500e+02 -7.540e+02 -7.590e+02 -7.590e+02 -7.590e+02\n",
      "  7.630e+02 -7.700e+02  7.730e+02 -7.730e+02 -7.820e+02 -7.830e+02\n",
      "  7.890e+02 -7.960e+02 -7.970e+02 -7.990e+02  8.040e+02  8.060e+02\n",
      " -8.170e+02 -8.270e+02 -8.330e+02 -8.360e+02  8.390e+02 -8.500e+02\n",
      " -8.510e+02  8.530e+02 -8.550e+02  8.620e+02  8.820e+02 -8.890e+02\n",
      " -8.930e+02 -8.950e+02 -8.970e+02 -9.040e+02 -9.060e+02 -9.100e+02\n",
      " -9.110e+02 -9.130e+02 -9.140e+02  9.270e+02 -9.300e+02 -9.330e+02\n",
      "  9.410e+02 -9.470e+02 -9.500e+02 -9.640e+02 -9.720e+02  9.800e+02\n",
      "  9.850e+02  9.930e+02 -9.930e+02 -9.950e+02 -9.970e+02 -1.000e+03\n",
      " -1.021e+03 -1.030e+03 -1.036e+03 -1.038e+03 -1.050e+03 -1.075e+03\n",
      " -1.077e+03  1.079e+03  1.081e+03  1.090e+03  1.093e+03 -1.095e+03\n",
      " -1.120e+03 -1.125e+03 -1.131e+03  1.133e+03 -1.138e+03 -1.143e+03\n",
      " -1.147e+03 -1.168e+03 -1.172e+03 -1.179e+03 -1.183e+03 -1.190e+03\n",
      "  1.202e+03 -1.218e+03 -1.224e+03 -1.245e+03 -1.253e+03 -1.265e+03\n",
      " -1.273e+03 -1.274e+03 -1.278e+03 -1.288e+03  1.289e+03 -1.309e+03\n",
      " -1.311e+03 -1.315e+03 -1.346e+03 -1.353e+03 -1.368e+03  1.394e+03\n",
      "  1.398e+03 -1.399e+03 -1.409e+03 -1.409e+03 -1.413e+03 -1.425e+03\n",
      " -1.428e+03  1.430e+03 -1.430e+03 -1.443e+03  1.459e+03 -1.460e+03\n",
      " -1.461e+03 -1.466e+03  1.466e+03 -1.472e+03 -1.483e+03 -1.506e+03\n",
      " -1.508e+03 -1.521e+03 -1.527e+03 -1.555e+03 -1.559e+03 -1.584e+03\n",
      " -1.584e+03 -1.586e+03 -1.628e+03 -1.663e+03  1.671e+03 -1.686e+03\n",
      " -1.690e+03 -1.699e+03  1.718e+03 -1.721e+03 -1.722e+03 -1.724e+03\n",
      "  1.732e+03 -1.737e+03  1.762e+03 -1.777e+03 -1.778e+03  1.838e+03\n",
      " -1.840e+03 -1.971e+03  1.972e+03 -1.995e+03  2.002e+03 -2.016e+03\n",
      " -2.016e+03  2.064e+03  2.083e+03  2.120e+03 -2.133e+03 -2.161e+03\n",
      "  2.166e+03 -2.169e+03 -2.182e+03  2.319e+03 -2.327e+03 -2.347e+03\n",
      " -2.437e+03 -2.562e+03 -2.641e+03  2.703e+03  2.717e+03 -2.727e+03\n",
      " -2.886e+03  2.900e+03 -3.270e+03  3.314e+03 -3.930e+03 -3.981e+03\n",
      " -4.241e+03 -4.282e+03  4.680e+03  4.760e+03  4.856e+03 -5.252e+03\n",
      " -5.480e+03  6.417e+03]\n",
      "Concordance Index 0.4759172512719367\n",
      "Integrated Brier Score: 0.18360595080970746\n",
      "y_train breslow final [ 2.000e+00  2.000e+00 -1.100e+01  1.100e+01 -1.400e+01  2.300e+01\n",
      " -3.000e+01  3.200e+01  5.600e+01 -5.600e+01  6.400e+01  6.500e+01\n",
      "  6.900e+01  7.600e+01 -7.700e+01  8.600e+01  8.900e+01 -9.200e+01\n",
      "  9.400e+01  9.500e+01 -1.020e+02  1.080e+02 -1.100e+02 -1.110e+02\n",
      " -1.220e+02  1.260e+02  1.290e+02  1.300e+02 -1.340e+02  1.440e+02\n",
      "  1.440e+02 -1.460e+02  1.510e+02  1.600e+02  1.660e+02  1.710e+02\n",
      "  1.730e+02  1.830e+02  1.850e+02 -1.870e+02  1.910e+02  1.940e+02\n",
      "  1.940e+02  1.970e+02  1.980e+02  2.090e+02 -2.100e+02  2.150e+02\n",
      "  2.150e+02 -2.160e+02 -2.160e+02  2.170e+02  2.180e+02  2.220e+02\n",
      " -2.320e+02 -2.340e+02 -2.420e+02  2.430e+02 -2.530e+02  2.560e+02\n",
      "  2.590e+02  2.680e+02  2.740e+02  2.760e+02  2.760e+02 -2.780e+02\n",
      "  2.810e+02  2.820e+02  2.830e+02  2.840e+02 -2.920e+02  2.920e+02\n",
      "  2.940e+02 -3.110e+02  3.170e+02  3.170e+02 -3.180e+02 -3.210e+02\n",
      "  3.210e+02  3.270e+02  3.300e+02  3.340e+02  3.360e+02  3.370e+02\n",
      "  3.410e+02  3.420e+02 -3.450e+02  3.480e+02 -3.510e+02  3.510e+02\n",
      "  3.520e+02  3.530e+02 -3.580e+02  3.590e+02  3.600e+02 -3.600e+02\n",
      "  3.610e+02 -3.620e+02  3.660e+02 -3.680e+02 -3.760e+02  3.770e+02\n",
      "  3.770e+02 -3.780e+02  3.790e+02  3.840e+02 -3.840e+02  3.850e+02\n",
      "  3.850e+02  3.870e+02 -3.870e+02 -3.920e+02 -3.920e+02  3.930e+02\n",
      " -3.930e+02  3.950e+02  3.950e+02  3.970e+02 -4.030e+02  4.030e+02\n",
      "  4.030e+02  4.060e+02  4.060e+02  4.070e+02 -4.070e+02 -4.120e+02\n",
      "  4.150e+02 -4.170e+02  4.210e+02 -4.210e+02 -4.240e+02  4.240e+02\n",
      "  4.270e+02  4.300e+02 -4.370e+02 -4.400e+02 -4.430e+02  4.490e+02\n",
      "  4.510e+02  4.530e+02 -4.580e+02 -4.610e+02  4.620e+02  4.640e+02\n",
      " -4.660e+02 -4.690e+02 -4.710e+02  4.720e+02  4.790e+02 -4.790e+02\n",
      "  4.800e+02  4.840e+02 -4.850e+02  4.870e+02  4.890e+02 -4.910e+02\n",
      "  4.930e+02  4.950e+02 -5.090e+02 -5.140e+02 -5.160e+02 -5.180e+02\n",
      " -5.200e+02  5.210e+02  5.260e+02  5.300e+02 -5.390e+02 -5.420e+02\n",
      " -5.430e+02  5.430e+02  5.450e+02  5.460e+02  5.460e+02 -5.470e+02\n",
      "  5.480e+02 -5.490e+02 -5.520e+02 -5.590e+02  5.600e+02  5.640e+02\n",
      " -5.680e+02  5.720e+02 -5.730e+02 -5.760e+02  5.770e+02  5.770e+02\n",
      " -5.790e+02  5.800e+02 -5.860e+02 -5.940e+02 -6.000e+02  6.010e+02\n",
      "  6.020e+02 -6.030e+02  6.060e+02 -6.160e+02 -6.160e+02 -6.190e+02\n",
      " -6.220e+02 -6.220e+02 -6.230e+02 -6.230e+02 -6.240e+02  6.240e+02\n",
      "  6.250e+02 -6.250e+02  6.310e+02 -6.350e+02 -6.370e+02 -6.410e+02\n",
      " -6.410e+02  6.410e+02 -6.440e+02 -6.450e+02 -6.450e+02 -6.530e+02\n",
      "  6.540e+02 -6.580e+02  6.630e+02  6.660e+02 -6.670e+02 -6.700e+02\n",
      " -6.790e+02  6.800e+02 -6.820e+02 -6.850e+02 -6.870e+02 -6.930e+02\n",
      " -6.950e+02 -7.010e+02 -7.010e+02 -7.070e+02 -7.100e+02 -7.130e+02\n",
      " -7.140e+02 -7.170e+02 -7.220e+02 -7.250e+02  7.270e+02  7.390e+02\n",
      " -7.420e+02 -7.500e+02 -7.540e+02 -7.590e+02 -7.590e+02 -7.590e+02\n",
      "  7.630e+02 -7.700e+02  7.730e+02 -7.730e+02 -7.820e+02 -7.830e+02\n",
      "  7.890e+02 -7.960e+02 -7.970e+02 -7.990e+02  8.040e+02  8.060e+02\n",
      " -8.170e+02 -8.270e+02 -8.330e+02 -8.360e+02  8.390e+02 -8.500e+02\n",
      " -8.510e+02  8.530e+02 -8.550e+02  8.620e+02  8.820e+02 -8.890e+02\n",
      " -8.930e+02 -8.950e+02 -8.970e+02 -9.040e+02 -9.060e+02 -9.100e+02\n",
      " -9.110e+02 -9.130e+02 -9.140e+02  9.270e+02 -9.300e+02 -9.330e+02\n",
      "  9.410e+02 -9.470e+02 -9.500e+02 -9.640e+02 -9.720e+02  9.800e+02\n",
      "  9.850e+02  9.930e+02 -9.930e+02 -9.950e+02 -9.970e+02 -1.000e+03\n",
      " -1.021e+03 -1.030e+03 -1.036e+03 -1.038e+03 -1.050e+03 -1.075e+03\n",
      " -1.077e+03  1.079e+03  1.081e+03  1.090e+03  1.093e+03 -1.095e+03\n",
      " -1.120e+03 -1.125e+03 -1.131e+03  1.133e+03 -1.138e+03 -1.143e+03\n",
      " -1.147e+03 -1.168e+03 -1.172e+03 -1.179e+03 -1.183e+03 -1.190e+03\n",
      "  1.202e+03 -1.218e+03 -1.224e+03 -1.245e+03 -1.253e+03 -1.265e+03\n",
      " -1.273e+03 -1.274e+03 -1.278e+03 -1.288e+03  1.289e+03 -1.309e+03\n",
      " -1.311e+03 -1.315e+03 -1.346e+03 -1.353e+03 -1.368e+03  1.394e+03\n",
      "  1.398e+03 -1.399e+03 -1.409e+03 -1.409e+03 -1.413e+03 -1.425e+03\n",
      " -1.428e+03  1.430e+03 -1.430e+03 -1.443e+03  1.459e+03 -1.460e+03\n",
      " -1.461e+03 -1.466e+03  1.466e+03 -1.472e+03 -1.483e+03 -1.506e+03\n",
      " -1.508e+03 -1.521e+03 -1.527e+03 -1.555e+03 -1.559e+03 -1.584e+03\n",
      " -1.584e+03 -1.586e+03 -1.628e+03 -1.663e+03  1.671e+03 -1.686e+03\n",
      " -1.690e+03 -1.699e+03  1.718e+03 -1.721e+03 -1.722e+03 -1.724e+03\n",
      "  1.732e+03 -1.737e+03  1.762e+03 -1.777e+03 -1.778e+03  1.838e+03\n",
      " -1.840e+03 -1.971e+03  1.972e+03 -1.995e+03  2.002e+03 -2.016e+03\n",
      " -2.016e+03  2.064e+03  2.083e+03  2.120e+03 -2.133e+03 -2.161e+03\n",
      "  2.166e+03 -2.169e+03 -2.182e+03  2.319e+03 -2.327e+03 -2.347e+03\n",
      " -2.437e+03 -2.562e+03 -2.641e+03  2.703e+03  2.717e+03 -2.727e+03\n",
      " -2.886e+03  2.900e+03 -3.270e+03  3.314e+03 -3.930e+03 -3.981e+03\n",
      " -4.241e+03 -4.282e+03  4.680e+03  4.760e+03  4.856e+03 -5.252e+03\n",
      " -5.480e+03  6.417e+03]\n",
      "durations 14.0 5152.0\n",
      "Concordance Index 0.5740740740740741\n",
      "Integrated Brier Score: 0.2063345736214503\n",
      "split gex_?|100130426      float32\n",
      "gex_?|100133144      float32\n",
      "gex_?|100134869      float32\n",
      "gex_?|10357          float32\n",
      "gex_?|10431          float32\n",
      "                      ...   \n",
      "gex_ZYG11A|440590    float32\n",
      "gex_ZYG11B|79699     float32\n",
      "gex_ZYX|7791         float32\n",
      "gex_ZZEF1|23140      float32\n",
      "gex_ZZZ3|26009       float32\n",
      "Length: 20531, dtype: object\n",
      "(424, 20531) <class 'pandas.core.frame.DataFrame'>\n",
      "(424,) <class 'pandas.core.series.Series'>\n",
      "(107, 20531) <class 'pandas.core.frame.DataFrame'>\n",
      "(107,) <class 'pandas.core.series.Series'>\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.7027\u001b[0m        \u001b[32m1.5855\u001b[0m  0.4345\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.9511\u001b[0m        \u001b[32m2.1177\u001b[0m  0.4363\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.0185\u001b[0m        \u001b[32m2.5978\u001b[0m  0.4492\n",
      "      2        \u001b[36m2.6327\u001b[0m        \u001b[32m1.5846\u001b[0m  0.2283\n",
      "      2        \u001b[36m2.9791\u001b[0m        2.6739  0.3016\n",
      "      2        \u001b[36m2.9136\u001b[0m        \u001b[32m2.0452\u001b[0m  0.3051\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.6043\u001b[0m        \u001b[32m1.7715\u001b[0m  0.7013\n",
      "      3        \u001b[36m2.5732\u001b[0m        1.6174  0.2345\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.7994\u001b[0m        \u001b[32m2.5412\u001b[0m  0.6666\n",
      "      3        \u001b[36m2.9320\u001b[0m        2.6669  0.2520\n",
      "      3        \u001b[36m2.8703\u001b[0m        2.0849  0.2478\n",
      "      2        \u001b[36m2.5127\u001b[0m        1.8122  0.2064\n",
      "      2        \u001b[36m2.6611\u001b[0m        2.5540  0.1802\n",
      "      4        \u001b[36m2.5064\u001b[0m        1.6249  0.1992\n",
      "      4        \u001b[36m2.8597\u001b[0m        2.6446  0.1880\n",
      "      4        \u001b[36m2.7923\u001b[0m        2.1498  0.1855\n",
      "      3        \u001b[36m2.4737\u001b[0m        1.8160  0.1952\n",
      "      3        \u001b[36m2.6349\u001b[0m        2.5640  0.1816\n",
      "      5        \u001b[36m2.4431\u001b[0m        1.6207  0.1902\n",
      "      5        \u001b[36m2.7138\u001b[0m        2.2019  0.1926\n",
      "      5        \u001b[36m2.8250\u001b[0m        2.6711  0.2019\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.7695\u001b[0m        \u001b[32m2.5831\u001b[0m  1.3417\n",
      "      4        \u001b[36m2.4288\u001b[0m        1.8484  0.2059\n",
      "      4        \u001b[36m2.5991\u001b[0m        2.5639  0.1960\n",
      "      6        \u001b[36m2.4311\u001b[0m        1.6180  0.2115\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.9258\u001b[0m        \u001b[32m2.1417\u001b[0m  1.5101\n",
      "      6        \u001b[36m2.6571\u001b[0m        2.2081  0.1672\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.6866\u001b[0m        \u001b[32m1.5817\u001b[0m  1.4214\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.6212\u001b[0m        \u001b[32m1.9282\u001b[0m  1.4409\n",
      "      6        \u001b[36m2.7835\u001b[0m        2.6924  0.2679\n",
      "      5        \u001b[36m2.3733\u001b[0m        1.8600  0.1979\n",
      "      5        \u001b[36m2.5368\u001b[0m        2.5669  0.2033\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.0284\u001b[0m        \u001b[32m2.7742\u001b[0m  1.4721\n",
      "      7        \u001b[36m2.3852\u001b[0m        1.6136  0.2220\n",
      "      7        \u001b[36m2.6338\u001b[0m        2.2101  0.1951\n",
      "      7        \u001b[36m2.7410\u001b[0m        2.6999  0.1704\n",
      "      6        \u001b[36m2.3243\u001b[0m        1.8714  0.2094\n",
      "      6        \u001b[36m2.4732\u001b[0m        \u001b[32m2.5302\u001b[0m  0.2152\n",
      "      8        \u001b[36m2.3723\u001b[0m        1.6112  0.1885\n",
      "      8        \u001b[36m2.5990\u001b[0m        2.2077  0.2040\n",
      "      8        \u001b[36m2.6965\u001b[0m        2.7091  0.2262\n",
      "      9        \u001b[36m2.3314\u001b[0m        1.6197  0.2243\n",
      "      7        \u001b[36m2.2972\u001b[0m        1.8734  0.2679\n",
      "      7        \u001b[36m2.4173\u001b[0m        2.5707  0.2714\n",
      "      9        \u001b[36m2.5846\u001b[0m        2.2062  0.2298\n",
      "      9        \u001b[36m2.6755\u001b[0m        2.7171  0.2658\n",
      "     10        \u001b[36m2.3224\u001b[0m        1.6296  0.1986\n",
      "Restoring best model from epoch 2.\n",
      "      8        \u001b[36m2.3741\u001b[0m        2.5842  0.2254\n",
      "     10        \u001b[36m2.5515\u001b[0m        2.2038  0.2137\n",
      "Restoring best model from epoch 2.\n",
      "      8        \u001b[36m2.2738\u001b[0m        1.8686  0.2886\n",
      "     10        2.6775        2.7145  0.2551\n",
      "      9        \u001b[36m2.3507\u001b[0m        2.5786  0.2097\n",
      "Restoring best model from epoch 1.\n",
      "      2        \u001b[36m2.7781\u001b[0m        \u001b[32m2.1201\u001b[0m  1.0668\n",
      "      2        \u001b[36m2.6305\u001b[0m        2.6107  1.2290\n",
      "      9        \u001b[36m2.2699\u001b[0m        1.8660  0.2274\n",
      "      2        \u001b[36m2.5912\u001b[0m        \u001b[32m1.5507\u001b[0m  1.1464\n",
      "      2        \u001b[36m2.5823\u001b[0m        1.9325  1.1372\n",
      "     10        \u001b[36m2.3253\u001b[0m        2.5830  0.1595\n",
      "Restoring best model from epoch 6.\n",
      "      2        \u001b[36m2.9261\u001b[0m        \u001b[32m2.7465\u001b[0m  1.0567\n",
      "     10        \u001b[36m2.2330\u001b[0m        1.8744  0.1371\n",
      "Restoring best model from epoch 1.\n",
      "      3        \u001b[36m2.7172\u001b[0m        2.1349  0.6597\n",
      "      3        \u001b[36m2.6174\u001b[0m        2.5932  0.6336\n",
      "      3        \u001b[36m2.5370\u001b[0m        \u001b[32m1.8910\u001b[0m  0.6136\n",
      "      3        \u001b[36m2.5391\u001b[0m        \u001b[32m1.5424\u001b[0m  0.6237\n",
      "      3        \u001b[36m2.8839\u001b[0m        \u001b[32m2.7341\u001b[0m  0.6020\n",
      "      4        \u001b[36m2.5239\u001b[0m        \u001b[32m2.5592\u001b[0m  0.5947\n",
      "      4        \u001b[36m2.6941\u001b[0m        2.1253  0.6161\n",
      "      4        \u001b[36m2.4953\u001b[0m        \u001b[32m1.4957\u001b[0m  0.5846\n",
      "      4        \u001b[36m2.8369\u001b[0m        2.7484  0.6028\n",
      "      4        2.5419        \u001b[32m1.8879\u001b[0m  0.6144\n",
      "      5        2.7039        \u001b[32m2.0655\u001b[0m  0.5435\n",
      "      5        \u001b[36m2.5084\u001b[0m        \u001b[32m2.4919\u001b[0m  0.5644\n",
      "      5        2.4996        \u001b[32m1.4890\u001b[0m  0.5537\n",
      "      5        \u001b[36m2.7980\u001b[0m        \u001b[32m2.7155\u001b[0m  0.5502\n",
      "      5        \u001b[36m2.5082\u001b[0m        1.9285  0.5538\n",
      "      6        \u001b[36m2.4804\u001b[0m        2.5098  0.5549\n",
      "      6        \u001b[36m2.6631\u001b[0m        \u001b[32m2.0587\u001b[0m  0.5620\n",
      "      6        \u001b[36m2.4766\u001b[0m        \u001b[32m1.4287\u001b[0m  0.5589\n",
      "      6        \u001b[36m2.7958\u001b[0m        \u001b[32m2.6983\u001b[0m  0.5647\n",
      "      6        \u001b[36m2.4655\u001b[0m        \u001b[32m1.8714\u001b[0m  0.5641\n",
      "      7        \u001b[36m2.4576\u001b[0m        2.5247  0.5599\n",
      "      7        2.6795        2.0961  0.5617\n",
      "      7        \u001b[36m2.4238\u001b[0m        \u001b[32m1.3858\u001b[0m  0.5872\n",
      "      7        \u001b[36m2.4382\u001b[0m        1.8814  0.5602\n",
      "      7        2.8069        2.7001  0.5643\n",
      "      8        \u001b[36m2.6590\u001b[0m        2.0637  0.5517\n",
      "      8        \u001b[36m2.4340\u001b[0m        2.5549  0.5667\n",
      "      8        \u001b[36m2.7940\u001b[0m        2.7083  0.5581\n",
      "      8        \u001b[36m2.4033\u001b[0m        1.8752  0.5993\n",
      "      8        2.4507        1.4245  0.6008\n",
      "      9        2.4638        2.5242  0.5715\n",
      "      9        2.6755        2.0875  0.6042\n",
      "      9        \u001b[36m2.7646\u001b[0m        2.7043  0.5474\n",
      "      9        \u001b[36m2.3964\u001b[0m        \u001b[32m1.8463\u001b[0m  0.5964\n",
      "      9        2.4482        1.4073  0.6015\n",
      "     10        \u001b[36m2.4212\u001b[0m        \u001b[32m2.4750\u001b[0m  0.6129\n",
      "     10        2.7771        \u001b[32m2.6465\u001b[0m  0.5602\n",
      "     10        \u001b[36m2.6317\u001b[0m        \u001b[32m2.0363\u001b[0m  0.6140\n",
      "     10        \u001b[36m2.3635\u001b[0m        1.4088  0.5803\n",
      "Restoring best model from epoch 7.\n",
      "     10        \u001b[36m2.3681\u001b[0m        \u001b[32m1.8222\u001b[0m  0.5881\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.6205\u001b[0m        \u001b[32m2.9213\u001b[0m  0.0875\n",
      "      2        \u001b[36m2.5584\u001b[0m        \u001b[32m2.8984\u001b[0m  0.0717\n",
      "      3        \u001b[36m2.4947\u001b[0m        \u001b[32m2.8887\u001b[0m  0.0626\n",
      "      4        \u001b[36m2.4536\u001b[0m        2.9257  0.0622\n",
      "      5        \u001b[36m2.4046\u001b[0m        2.9418  0.0783\n",
      "      6        \u001b[36m2.3567\u001b[0m        2.9308  0.0625\n",
      "      7        \u001b[36m2.3301\u001b[0m        2.9250  0.0623\n",
      "      8        \u001b[36m2.3026\u001b[0m        2.9328  0.0566\n",
      "      9        \u001b[36m2.2815\u001b[0m        2.9343  0.0619\n",
      "     10        \u001b[36m2.2540\u001b[0m        2.9290  0.0678\n",
      "Restoring best model from epoch 3.\n",
      "y_train breslow final [ 2.000e+00 -3.000e+00 -7.000e+00 -1.100e+01 -1.300e+01 -1.600e+01\n",
      " -1.600e+01 -1.800e+01  1.800e+01 -2.600e+01 -2.900e+01 -3.800e+01\n",
      "  4.100e+01  4.200e+01  4.300e+01 -4.300e+01  5.100e+01 -5.300e+01\n",
      "  5.900e+01 -6.100e+01  6.500e+01  6.800e+01  6.900e+01  7.300e+01\n",
      "  7.700e+01  9.300e+01  1.060e+02  1.090e+02 -1.270e+02  1.370e+02\n",
      "  1.390e+02  1.390e+02 -1.410e+02 -1.500e+02 -1.550e+02  1.620e+02\n",
      " -1.660e+02  1.660e+02  1.680e+02 -1.750e+02 -1.770e+02  1.820e+02\n",
      " -1.930e+02 -1.940e+02  2.040e+02 -2.050e+02  2.240e+02  2.380e+02\n",
      "  2.420e+02  2.450e+02 -2.560e+02 -2.930e+02  3.070e+02  3.110e+02\n",
      "  3.130e+02  3.130e+02 -3.190e+02  3.200e+02 -3.230e+02  3.290e+02\n",
      "  3.330e+02  3.340e+02  3.360e+02  3.420e+02  3.440e+02 -3.540e+02\n",
      " -3.560e+02 -3.620e+02 -3.650e+02 -3.650e+02 -3.690e+02 -3.700e+02\n",
      " -3.720e+02 -3.730e+02 -3.740e+02  3.750e+02 -3.850e+02 -3.860e+02\n",
      " -4.000e+02 -4.060e+02 -4.080e+02 -4.110e+02  4.310e+02 -4.310e+02\n",
      " -4.330e+02  4.450e+02  4.460e+02  4.540e+02 -4.540e+02  4.590e+02\n",
      " -4.690e+02  4.780e+02  4.800e+02  4.800e+02  4.850e+02 -4.950e+02\n",
      " -4.980e+02 -5.010e+02 -5.050e+02 -5.070e+02 -5.110e+02 -5.190e+02\n",
      " -5.280e+02 -5.510e+02  5.520e+02  5.610e+02  5.630e+02 -5.630e+02\n",
      " -5.670e+02 -5.740e+02  5.740e+02  5.780e+02  5.870e+02 -5.910e+02\n",
      " -6.030e+02 -6.070e+02 -6.170e+02 -6.300e+02  6.370e+02  6.450e+02\n",
      " -6.480e+02 -6.560e+02 -6.620e+02 -6.650e+02  6.790e+02  6.830e+02\n",
      " -6.850e+02 -6.890e+02 -6.930e+02  7.010e+02  7.090e+02 -7.140e+02\n",
      "  7.220e+02 -7.220e+02  7.350e+02 -7.350e+02 -7.370e+02 -7.490e+02\n",
      " -7.550e+02 -7.620e+02  7.680e+02 -7.850e+02  7.930e+02  8.190e+02\n",
      "  8.220e+02  8.280e+02 -8.300e+02  8.340e+02 -8.400e+02  8.410e+02\n",
      "  8.450e+02 -8.610e+02  8.660e+02 -8.730e+02 -8.740e+02  8.780e+02\n",
      "  8.830e+02  8.850e+02  8.850e+02 -9.090e+02 -9.100e+02 -9.190e+02\n",
      "  9.270e+02  9.320e+02 -9.320e+02 -9.450e+02  9.460e+02 -9.510e+02\n",
      "  9.520e+02 -9.520e+02 -9.670e+02 -9.920e+02  9.920e+02  1.003e+03\n",
      " -1.014e+03 -1.018e+03 -1.018e+03  1.019e+03  1.034e+03  1.045e+03\n",
      " -1.046e+03 -1.063e+03 -1.071e+03  1.075e+03  1.091e+03 -1.092e+03\n",
      "  1.092e+03  1.097e+03 -1.107e+03  1.111e+03 -1.120e+03 -1.124e+03\n",
      " -1.124e+03 -1.126e+03 -1.130e+03 -1.133e+03  1.133e+03 -1.133e+03\n",
      " -1.137e+03 -1.140e+03 -1.143e+03 -1.168e+03 -1.169e+03  1.170e+03\n",
      " -1.175e+03 -1.186e+03  1.191e+03  1.200e+03  1.200e+03 -1.217e+03\n",
      " -1.218e+03 -1.238e+03  1.238e+03 -1.266e+03 -1.274e+03 -1.290e+03\n",
      " -1.314e+03  1.317e+03 -1.329e+03  1.337e+03  1.343e+03 -1.367e+03\n",
      " -1.371e+03  1.371e+03  1.378e+03 -1.380e+03 -1.385e+03 -1.398e+03\n",
      "  1.404e+03 -1.411e+03 -1.413e+03 -1.413e+03 -1.416e+03  1.417e+03\n",
      " -1.423e+03  1.432e+03 -1.433e+03 -1.435e+03 -1.436e+03 -1.450e+03\n",
      " -1.462e+03  1.463e+03 -1.471e+03 -1.476e+03 -1.478e+03 -1.485e+03\n",
      " -1.487e+03 -1.493e+03 -1.496e+03 -1.498e+03 -1.499e+03 -1.502e+03\n",
      " -1.508e+03 -1.516e+03 -1.520e+03 -1.520e+03 -1.521e+03 -1.525e+03\n",
      " -1.528e+03 -1.531e+03 -1.552e+03 -1.560e+03  1.584e+03  1.588e+03\n",
      "  1.589e+03  1.598e+03 -1.608e+03  1.610e+03  1.620e+03 -1.621e+03\n",
      "  1.625e+03  1.625e+03  1.626e+03  1.639e+03  1.657e+03 -1.657e+03\n",
      "  1.661e+03 -1.683e+03  1.696e+03  1.714e+03  1.724e+03 -1.729e+03\n",
      " -1.733e+03 -1.746e+03 -1.755e+03 -1.778e+03 -1.782e+03 -1.785e+03\n",
      " -1.790e+03 -1.794e+03 -1.843e+03 -1.862e+03 -1.871e+03 -1.876e+03\n",
      " -1.879e+03 -1.883e+03 -1.885e+03 -1.886e+03 -1.888e+03 -1.889e+03\n",
      " -1.893e+03 -1.905e+03 -1.906e+03  1.912e+03  1.913e+03 -1.924e+03\n",
      " -1.928e+03 -1.946e+03 -1.955e+03  1.964e+03  1.980e+03  1.986e+03\n",
      " -1.993e+03 -1.997e+03 -2.004e+03 -2.009e+03 -2.014e+03 -2.016e+03\n",
      " -2.017e+03 -2.038e+03 -2.067e+03 -2.080e+03 -2.087e+03  2.090e+03\n",
      " -2.128e+03 -2.133e+03  2.145e+03 -2.150e+03 -2.172e+03 -2.172e+03\n",
      " -2.184e+03 -2.186e+03  2.190e+03 -2.208e+03 -2.217e+03 -2.226e+03\n",
      "  2.227e+03 -2.231e+03  2.241e+03 -2.246e+03  2.256e+03 -2.257e+03\n",
      " -2.263e+03 -2.271e+03 -2.274e+03 -2.283e+03  2.299e+03  2.343e+03\n",
      " -2.372e+03 -2.378e+03  2.386e+03 -2.392e+03 -2.412e+03  2.419e+03\n",
      " -2.422e+03 -2.423e+03 -2.430e+03 -2.439e+03  2.454e+03 -2.470e+03\n",
      " -2.489e+03 -2.504e+03 -2.508e+03 -2.531e+03 -2.552e+03 -2.554e+03\n",
      "  2.564e+03 -2.609e+03 -2.609e+03 -2.630e+03 -2.660e+03 -2.688e+03\n",
      " -2.718e+03 -2.722e+03 -2.746e+03  2.752e+03 -2.754e+03  2.764e+03\n",
      " -2.782e+03 -2.789e+03 -2.799e+03 -2.839e+03 -2.859e+03 -2.865e+03\n",
      " -2.868e+03 -2.873e+03 -2.881e+03 -2.964e+03 -3.074e+03 -3.146e+03\n",
      " -3.205e+03 -3.205e+03 -3.229e+03 -3.267e+03 -3.302e+03 -3.328e+03\n",
      " -3.331e+03 -3.341e+03 -3.343e+03 -3.377e+03 -3.392e+03 -3.409e+03\n",
      " -3.451e+03 -3.480e+03 -3.498e+03 -3.519e+03  3.554e+03 -3.583e+03\n",
      "  3.615e+03 -3.631e+03 -3.639e+03 -3.728e+03 -3.736e+03 -3.744e+03\n",
      " -3.834e+03 -3.841e+03 -3.936e+03 -3.944e+03 -3.974e+03 -3.987e+03\n",
      " -3.989e+03 -4.067e+03 -4.074e+03 -4.537e+03]\n",
      "Concordance Index 0.4404093760316936\n",
      "Integrated Brier Score: 0.36616686010889543\n",
      "y_train breslow final [ 2.000e+00 -3.000e+00 -7.000e+00 -1.100e+01 -1.300e+01 -1.600e+01\n",
      " -1.600e+01 -1.800e+01  1.800e+01 -2.600e+01 -2.900e+01 -3.800e+01\n",
      "  4.100e+01  4.200e+01  4.300e+01 -4.300e+01  5.100e+01 -5.300e+01\n",
      "  5.900e+01 -6.100e+01  6.500e+01  6.800e+01  6.900e+01  7.300e+01\n",
      "  7.700e+01  9.300e+01  1.060e+02  1.090e+02 -1.270e+02  1.370e+02\n",
      "  1.390e+02  1.390e+02 -1.410e+02 -1.500e+02 -1.550e+02  1.620e+02\n",
      " -1.660e+02  1.660e+02  1.680e+02 -1.750e+02 -1.770e+02  1.820e+02\n",
      " -1.930e+02 -1.940e+02  2.040e+02 -2.050e+02  2.240e+02  2.380e+02\n",
      "  2.420e+02  2.450e+02 -2.560e+02 -2.930e+02  3.070e+02  3.110e+02\n",
      "  3.130e+02  3.130e+02 -3.190e+02  3.200e+02 -3.230e+02  3.290e+02\n",
      "  3.330e+02  3.340e+02  3.360e+02  3.420e+02  3.440e+02 -3.540e+02\n",
      " -3.560e+02 -3.620e+02 -3.650e+02 -3.650e+02 -3.690e+02 -3.700e+02\n",
      " -3.720e+02 -3.730e+02 -3.740e+02  3.750e+02 -3.850e+02 -3.860e+02\n",
      " -4.000e+02 -4.060e+02 -4.080e+02 -4.110e+02  4.310e+02 -4.310e+02\n",
      " -4.330e+02  4.450e+02  4.460e+02  4.540e+02 -4.540e+02  4.590e+02\n",
      " -4.690e+02  4.780e+02  4.800e+02  4.800e+02  4.850e+02 -4.950e+02\n",
      " -4.980e+02 -5.010e+02 -5.050e+02 -5.070e+02 -5.110e+02 -5.190e+02\n",
      " -5.280e+02 -5.510e+02  5.520e+02  5.610e+02  5.630e+02 -5.630e+02\n",
      " -5.670e+02 -5.740e+02  5.740e+02  5.780e+02  5.870e+02 -5.910e+02\n",
      " -6.030e+02 -6.070e+02 -6.170e+02 -6.300e+02  6.370e+02  6.450e+02\n",
      " -6.480e+02 -6.560e+02 -6.620e+02 -6.650e+02  6.790e+02  6.830e+02\n",
      " -6.850e+02 -6.890e+02 -6.930e+02  7.010e+02  7.090e+02 -7.140e+02\n",
      "  7.220e+02 -7.220e+02  7.350e+02 -7.350e+02 -7.370e+02 -7.490e+02\n",
      " -7.550e+02 -7.620e+02  7.680e+02 -7.850e+02  7.930e+02  8.190e+02\n",
      "  8.220e+02  8.280e+02 -8.300e+02  8.340e+02 -8.400e+02  8.410e+02\n",
      "  8.450e+02 -8.610e+02  8.660e+02 -8.730e+02 -8.740e+02  8.780e+02\n",
      "  8.830e+02  8.850e+02  8.850e+02 -9.090e+02 -9.100e+02 -9.190e+02\n",
      "  9.270e+02  9.320e+02 -9.320e+02 -9.450e+02  9.460e+02 -9.510e+02\n",
      "  9.520e+02 -9.520e+02 -9.670e+02 -9.920e+02  9.920e+02  1.003e+03\n",
      " -1.014e+03 -1.018e+03 -1.018e+03  1.019e+03  1.034e+03  1.045e+03\n",
      " -1.046e+03 -1.063e+03 -1.071e+03  1.075e+03  1.091e+03 -1.092e+03\n",
      "  1.092e+03  1.097e+03 -1.107e+03  1.111e+03 -1.120e+03 -1.124e+03\n",
      " -1.124e+03 -1.126e+03 -1.130e+03 -1.133e+03  1.133e+03 -1.133e+03\n",
      " -1.137e+03 -1.140e+03 -1.143e+03 -1.168e+03 -1.169e+03  1.170e+03\n",
      " -1.175e+03 -1.186e+03  1.191e+03  1.200e+03  1.200e+03 -1.217e+03\n",
      " -1.218e+03 -1.238e+03  1.238e+03 -1.266e+03 -1.274e+03 -1.290e+03\n",
      " -1.314e+03  1.317e+03 -1.329e+03  1.337e+03  1.343e+03 -1.367e+03\n",
      " -1.371e+03  1.371e+03  1.378e+03 -1.380e+03 -1.385e+03 -1.398e+03\n",
      "  1.404e+03 -1.411e+03 -1.413e+03 -1.413e+03 -1.416e+03  1.417e+03\n",
      " -1.423e+03  1.432e+03 -1.433e+03 -1.435e+03 -1.436e+03 -1.450e+03\n",
      " -1.462e+03  1.463e+03 -1.471e+03 -1.476e+03 -1.478e+03 -1.485e+03\n",
      " -1.487e+03 -1.493e+03 -1.496e+03 -1.498e+03 -1.499e+03 -1.502e+03\n",
      " -1.508e+03 -1.516e+03 -1.520e+03 -1.520e+03 -1.521e+03 -1.525e+03\n",
      " -1.528e+03 -1.531e+03 -1.552e+03 -1.560e+03  1.584e+03  1.588e+03\n",
      "  1.589e+03  1.598e+03 -1.608e+03  1.610e+03  1.620e+03 -1.621e+03\n",
      "  1.625e+03  1.625e+03  1.626e+03  1.639e+03  1.657e+03 -1.657e+03\n",
      "  1.661e+03 -1.683e+03  1.696e+03  1.714e+03  1.724e+03 -1.729e+03\n",
      " -1.733e+03 -1.746e+03 -1.755e+03 -1.778e+03 -1.782e+03 -1.785e+03\n",
      " -1.790e+03 -1.794e+03 -1.843e+03 -1.862e+03 -1.871e+03 -1.876e+03\n",
      " -1.879e+03 -1.883e+03 -1.885e+03 -1.886e+03 -1.888e+03 -1.889e+03\n",
      " -1.893e+03 -1.905e+03 -1.906e+03  1.912e+03  1.913e+03 -1.924e+03\n",
      " -1.928e+03 -1.946e+03 -1.955e+03  1.964e+03  1.980e+03  1.986e+03\n",
      " -1.993e+03 -1.997e+03 -2.004e+03 -2.009e+03 -2.014e+03 -2.016e+03\n",
      " -2.017e+03 -2.038e+03 -2.067e+03 -2.080e+03 -2.087e+03  2.090e+03\n",
      " -2.128e+03 -2.133e+03  2.145e+03 -2.150e+03 -2.172e+03 -2.172e+03\n",
      " -2.184e+03 -2.186e+03  2.190e+03 -2.208e+03 -2.217e+03 -2.226e+03\n",
      "  2.227e+03 -2.231e+03  2.241e+03 -2.246e+03  2.256e+03 -2.257e+03\n",
      " -2.263e+03 -2.271e+03 -2.274e+03 -2.283e+03  2.299e+03  2.343e+03\n",
      " -2.372e+03 -2.378e+03  2.386e+03 -2.392e+03 -2.412e+03  2.419e+03\n",
      " -2.422e+03 -2.423e+03 -2.430e+03 -2.439e+03  2.454e+03 -2.470e+03\n",
      " -2.489e+03 -2.504e+03 -2.508e+03 -2.531e+03 -2.552e+03 -2.554e+03\n",
      "  2.564e+03 -2.609e+03 -2.609e+03 -2.630e+03 -2.660e+03 -2.688e+03\n",
      " -2.718e+03 -2.722e+03 -2.746e+03  2.752e+03 -2.754e+03  2.764e+03\n",
      " -2.782e+03 -2.789e+03 -2.799e+03 -2.839e+03 -2.859e+03 -2.865e+03\n",
      " -2.868e+03 -2.873e+03 -2.881e+03 -2.964e+03 -3.074e+03 -3.146e+03\n",
      " -3.205e+03 -3.205e+03 -3.229e+03 -3.267e+03 -3.302e+03 -3.328e+03\n",
      " -3.331e+03 -3.341e+03 -3.343e+03 -3.377e+03 -3.392e+03 -3.409e+03\n",
      " -3.451e+03 -3.480e+03 -3.498e+03 -3.519e+03  3.554e+03 -3.583e+03\n",
      "  3.615e+03 -3.631e+03 -3.639e+03 -3.728e+03 -3.736e+03 -3.744e+03\n",
      " -3.834e+03 -3.841e+03 -3.936e+03 -3.944e+03 -3.974e+03 -3.987e+03\n",
      " -3.989e+03 -4.067e+03 -4.074e+03 -4.537e+03]\n",
      "durations 3.0 3431.0\n",
      "Concordance Index 0.5207708779443255\n",
      "Integrated Brier Score: 0.36175138775984345\n",
      "(425, 20531) <class 'pandas.core.frame.DataFrame'>\n",
      "(425,) <class 'pandas.core.series.Series'>\n",
      "(106, 20531) <class 'pandas.core.frame.DataFrame'>\n",
      "(106,) <class 'pandas.core.series.Series'>\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.8709\u001b[0m        \u001b[32m2.9603\u001b[0m  0.2672\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.6051\u001b[0m        \u001b[32m1.7727\u001b[0m  0.4236\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.6518\u001b[0m        \u001b[32m1.7924\u001b[0m  0.4210\n",
      "      2        2.8833        2.9617  0.1926\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.8438\u001b[0m        \u001b[32m2.2202\u001b[0m  0.4768\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.7468\u001b[0m        \u001b[32m3.1323\u001b[0m  0.3949\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.5857\u001b[0m        \u001b[32m1.7049\u001b[0m  0.5044\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.7579\u001b[0m        \u001b[32m3.2389\u001b[0m  0.5324\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.9044\u001b[0m        \u001b[32m2.3184\u001b[0m  0.5467\n",
      "      2        \u001b[36m2.5464\u001b[0m        \u001b[32m1.7666\u001b[0m  0.1937\n",
      "      2        \u001b[36m2.6451\u001b[0m        \u001b[32m1.7918\u001b[0m  0.2014\n",
      "      2        \u001b[36m2.7053\u001b[0m        3.1793  0.1454\n",
      "      3        \u001b[36m2.8639\u001b[0m        2.9614  0.1901\n",
      "      2        \u001b[36m2.7724\u001b[0m        2.2476  0.1810\n",
      "      2        2.6162        1.7061  0.1970\n",
      "      2        2.9156        \u001b[32m2.3174\u001b[0m  0.2142\n",
      "      3        \u001b[36m2.5395\u001b[0m        1.7727  0.1527\n",
      "      2        2.7685        3.2402  0.2528\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.6032\u001b[0m        \u001b[32m1.6885\u001b[0m  0.6489\n",
      "      3        \u001b[36m2.6992\u001b[0m        3.1882  0.1765\n",
      "      3        \u001b[36m2.7604\u001b[0m        2.2722  0.1876\n",
      "      3        \u001b[36m2.6326\u001b[0m        1.7927  0.2255\n",
      "      4        2.8658        2.9615  0.2450\n",
      "      4        \u001b[36m2.5018\u001b[0m        \u001b[32m1.7644\u001b[0m  0.1794\n",
      "      3        2.6093        1.7067  0.2703\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.8160\u001b[0m        \u001b[32m2.8209\u001b[0m  0.4679\n",
      "      3        \u001b[36m2.9035\u001b[0m        2.3187  0.2383\n",
      "      2        2.6033        \u001b[32m1.6746\u001b[0m  0.2023\n",
      "      4        \u001b[36m2.6849\u001b[0m        3.2091  0.2083\n",
      "      3        \u001b[36m2.7484\u001b[0m        3.2393  0.2613\n",
      "      4        \u001b[36m2.7376\u001b[0m        2.2832  0.2020\n",
      "      4        2.6597        1.7927  0.2344\n",
      "      5        \u001b[36m2.4925\u001b[0m        \u001b[32m1.7557\u001b[0m  0.1766\n",
      "      5        \u001b[36m2.7101\u001b[0m        2.2932  0.1751\n",
      "      5        2.8833        2.9612  0.3418\n",
      "      4        2.9155        2.3177  0.2444\n",
      "      5        \u001b[36m2.6749\u001b[0m        3.2221  0.2302\n",
      "      4        2.5995        1.7072  0.2864\n",
      "      4        2.7599        3.2393  0.2601\n",
      "      6        \u001b[36m2.4586\u001b[0m        \u001b[32m1.7467\u001b[0m  0.1813\n",
      "      5        2.6448        1.7919  0.2668\n",
      "      6        \u001b[36m2.6480\u001b[0m        3.2210  0.1267\n",
      "      6        2.8668        2.9608  0.1860\n",
      "      5        2.6128        1.7079  0.1888\n",
      "      5        2.9208        \u001b[32m2.3157\u001b[0m  0.2435\n",
      "      3        \u001b[36m2.5752\u001b[0m        \u001b[32m1.6732\u001b[0m  0.4848\n",
      "      6        \u001b[36m2.6979\u001b[0m        2.2977  0.2836\n",
      "      5        \u001b[36m2.7416\u001b[0m        \u001b[32m3.2388\u001b[0m  0.2141\n",
      "      7        \u001b[36m2.6138\u001b[0m        3.2219  0.1555\n",
      "      6        2.6335        \u001b[32m1.7910\u001b[0m  0.2069\n",
      "      7        \u001b[36m2.8620\u001b[0m        2.9605  0.2007\n",
      "      4        \u001b[36m2.5601\u001b[0m        \u001b[32m1.6702\u001b[0m  0.1540\n",
      "      7        \u001b[36m2.6799\u001b[0m        2.3007  0.1570\n",
      "      7        \u001b[36m2.4551\u001b[0m        \u001b[32m1.7450\u001b[0m  0.3505\n",
      "      6        2.5978        1.7085  0.2177\n",
      "      8        2.6427        3.2310  0.1457\n",
      "      2        \u001b[36m2.7328\u001b[0m        2.8731  0.7492\n",
      "      6        2.7479        \u001b[32m3.2381\u001b[0m  0.2612\n",
      "      5        \u001b[36m2.5347\u001b[0m        1.6713  0.1530\n",
      "      8        2.4697        1.7489  0.2196\n",
      "      8        \u001b[36m2.6799\u001b[0m        2.3024  0.2387\n",
      "      8        2.8819        \u001b[32m2.9603\u001b[0m  0.2776\n",
      "      7        2.6547        \u001b[32m1.7906\u001b[0m  0.3337\n",
      "      6        2.9085        \u001b[32m2.3140\u001b[0m  0.4210\n",
      "      9        \u001b[36m2.6040\u001b[0m        3.2344  0.2370\n",
      "      3        \u001b[36m2.7074\u001b[0m        2.9088  0.2136\n",
      "      7        2.6128        1.7086  0.2910\n",
      "      6        2.5433        1.6737  0.2254\n",
      "      7        2.7490        \u001b[32m3.2378\u001b[0m  0.2708\n",
      "      9        2.4630        1.7542  0.1728\n",
      "      9        \u001b[36m2.6435\u001b[0m        2.3039  0.1714\n",
      "      4        \u001b[36m2.6849\u001b[0m        2.9253  0.1327\n",
      "     10        2.6286        3.2359  0.1705\n",
      "Restoring best model from epoch 1.\n",
      "      9        2.8799        2.9611  0.2356\n",
      "      8        2.6594        \u001b[32m1.7901\u001b[0m  0.2507\n",
      "      7        \u001b[36m2.5231\u001b[0m        1.6781  0.1552\n",
      "      7        2.9089        \u001b[32m2.3133\u001b[0m  0.2715\n",
      "      8        2.5908        1.7092  0.2271\n",
      "     10        2.6621        2.3037  0.1589\n",
      "     10        \u001b[36m2.4407\u001b[0m        1.7548  0.1657\n",
      "Restoring best model from epoch 1.\n",
      "Restoring best model from epoch 7.\n",
      "      8        2.7538        \u001b[32m3.2376\u001b[0m  0.2048\n",
      "      5        2.6888        2.9335  0.1713\n",
      "     10        2.8719        2.9613  0.1736\n",
      "Restoring best model from epoch 1.\n",
      "      8        \u001b[36m2.5103\u001b[0m        1.6839  0.1301\n",
      "      9        2.6540        \u001b[32m1.7899\u001b[0m  0.1810\n",
      "      8        2.9182        2.3135  0.1829\n",
      "      9        2.6157        1.7094  0.1986\n",
      "      6        \u001b[36m2.6712\u001b[0m        2.9381  0.1669\n",
      "      9        \u001b[36m2.4809\u001b[0m        1.6890  0.1172\n",
      "      9        2.7594        \u001b[32m3.2375\u001b[0m  0.1879\n",
      "     10        2.6326        \u001b[32m1.7893\u001b[0m  0.1610\n",
      "      9        2.9221        \u001b[32m2.3125\u001b[0m  0.1649\n",
      "     10        2.4886        1.6943  0.1062\n",
      "Restoring best model from epoch 4.\n",
      "     10        2.6028        1.7096  0.1527\n",
      "Restoring best model from epoch 1.\n",
      "      7        \u001b[36m2.6191\u001b[0m        2.9369  0.1334\n",
      "     10        2.7490        \u001b[32m3.2375\u001b[0m  0.1492\n",
      "Restoring best model from epoch 8.\n",
      "      8        2.6265        2.9310  0.0954\n",
      "     10        \u001b[36m2.9032\u001b[0m        2.3127  0.1405\n",
      "Restoring best model from epoch 9.\n",
      "      9        \u001b[36m2.5927\u001b[0m        2.9288  0.1078\n",
      "     10        2.6049        2.9243  0.0812\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.5866\u001b[0m        \u001b[32m3.1506\u001b[0m  0.1485\n",
      "      2        2.5940        \u001b[32m3.1497\u001b[0m  0.1125\n",
      "      3        2.5921        3.1506  0.0801\n",
      "      4        2.5968        3.1505  0.0772\n",
      "      5        \u001b[36m2.5829\u001b[0m        3.1505  0.0774\n",
      "      6        2.5906        3.1503  0.0719\n",
      "      7        2.6006        3.1501  0.0728\n",
      "      8        2.5988        3.1503  0.0692\n",
      "      9        2.6032        3.1499  0.0707\n",
      "     10        \u001b[36m2.5816\u001b[0m        3.1498  0.0637\n",
      "Restoring best model from epoch 2.\n",
      "y_train breslow final [ 2.000e+00 -3.000e+00 -1.300e+01 -1.600e+01 -1.600e+01 -1.600e+01\n",
      "  1.800e+01 -2.600e+01 -2.900e+01 -3.600e+01  4.100e+01  4.200e+01\n",
      "  4.300e+01 -4.300e+01  5.100e+01 -5.300e+01  6.200e+01  6.500e+01\n",
      "  6.900e+01  9.900e+01  1.010e+02  1.060e+02  1.100e+02 -1.180e+02\n",
      " -1.190e+02 -1.270e+02  1.370e+02  1.390e+02  1.390e+02 -1.410e+02\n",
      " -1.500e+02 -1.550e+02  1.620e+02  1.640e+02 -1.820e+02  1.820e+02\n",
      "  1.830e+02 -1.930e+02  2.020e+02  2.040e+02 -2.040e+02 -2.050e+02\n",
      "  2.060e+02 -2.060e+02  2.110e+02 -2.110e+02  2.220e+02  2.380e+02\n",
      "  2.420e+02  2.450e+02 -2.560e+02 -2.930e+02  3.070e+02  3.130e+02\n",
      "  3.130e+02 -3.230e+02  3.290e+02  3.300e+02  3.330e+02  3.340e+02\n",
      "  3.360e+02 -3.400e+02  3.420e+02  3.440e+02 -3.540e+02 -3.550e+02\n",
      "  3.620e+02 -3.620e+02 -3.650e+02 -3.650e+02 -3.690e+02 -3.700e+02\n",
      " -3.730e+02 -3.740e+02  3.750e+02 -3.860e+02 -4.000e+02 -4.060e+02\n",
      " -4.110e+02  4.310e+02 -4.330e+02 -4.350e+02  4.450e+02 -4.540e+02\n",
      "  4.590e+02 -4.680e+02  4.750e+02  4.780e+02  4.800e+02  4.850e+02\n",
      " -4.950e+02 -5.010e+02 -5.050e+02 -5.070e+02  5.100e+02 -5.190e+02\n",
      " -5.230e+02 -5.510e+02  5.520e+02  5.610e+02  5.610e+02  5.620e+02\n",
      "  5.630e+02 -5.630e+02 -5.670e+02  5.710e+02  5.720e+02 -5.740e+02\n",
      "  5.740e+02  5.780e+02  5.870e+02  6.000e+02 -6.030e+02 -6.070e+02\n",
      " -6.200e+02 -6.300e+02  6.370e+02  6.450e+02  6.460e+02 -6.480e+02\n",
      " -6.620e+02  6.790e+02  6.830e+02 -6.850e+02 -6.890e+02 -6.930e+02\n",
      " -6.930e+02 -7.000e+02 -7.140e+02  7.220e+02 -7.220e+02  7.270e+02\n",
      " -7.350e+02 -7.490e+02 -7.500e+02 -7.550e+02 -7.620e+02  7.680e+02\n",
      "  7.700e+02 -7.740e+02 -7.770e+02  7.820e+02 -7.850e+02 -7.880e+02\n",
      "  7.930e+02  8.190e+02  8.220e+02 -8.220e+02  8.280e+02 -8.300e+02\n",
      "  8.410e+02  8.450e+02  8.660e+02  8.780e+02  8.830e+02  8.850e+02\n",
      " -9.090e+02 -9.100e+02 -9.190e+02  9.270e+02  9.320e+02 -9.320e+02\n",
      " -9.320e+02 -9.450e+02  9.520e+02 -9.520e+02  9.530e+02 -9.630e+02\n",
      " -9.670e+02 -9.700e+02 -9.720e+02 -9.920e+02  9.920e+02  1.003e+03\n",
      " -1.011e+03 -1.018e+03 -1.018e+03  1.019e+03  1.045e+03 -1.046e+03\n",
      " -1.063e+03 -1.071e+03  1.075e+03  1.091e+03  1.092e+03 -1.106e+03\n",
      "  1.111e+03  1.111e+03 -1.120e+03  1.121e+03 -1.124e+03 -1.124e+03\n",
      " -1.126e+03 -1.130e+03 -1.130e+03 -1.132e+03 -1.133e+03  1.133e+03\n",
      " -1.133e+03 -1.137e+03 -1.140e+03 -1.165e+03 -1.168e+03 -1.169e+03\n",
      " -1.175e+03 -1.177e+03 -1.186e+03  1.200e+03  1.200e+03  1.230e+03\n",
      " -1.238e+03 -1.266e+03  1.270e+03 -1.274e+03 -1.290e+03 -1.291e+03\n",
      " -1.299e+03 -1.307e+03 -1.308e+03 -1.314e+03 -1.329e+03  1.337e+03\n",
      "  1.343e+03 -1.355e+03 -1.367e+03 -1.371e+03  1.371e+03 -1.373e+03\n",
      "  1.378e+03 -1.380e+03 -1.384e+03 -1.385e+03 -1.385e+03 -1.398e+03\n",
      "  1.404e+03 -1.411e+03 -1.413e+03 -1.413e+03 -1.416e+03  1.417e+03\n",
      " -1.423e+03  1.432e+03 -1.435e+03 -1.450e+03 -1.459e+03  1.463e+03\n",
      " -1.471e+03 -1.476e+03 -1.478e+03 -1.485e+03 -1.485e+03 -1.487e+03\n",
      " -1.489e+03 -1.491e+03 -1.493e+03  1.493e+03 -1.495e+03 -1.496e+03\n",
      " -1.498e+03 -1.499e+03 -1.502e+03 -1.508e+03 -1.516e+03 -1.520e+03\n",
      " -1.521e+03 -1.525e+03 -1.528e+03 -1.531e+03 -1.552e+03 -1.559e+03\n",
      "  1.567e+03  1.588e+03  1.589e+03  1.590e+03  1.598e+03 -1.604e+03\n",
      " -1.608e+03  1.610e+03  1.620e+03 -1.621e+03 -1.624e+03  1.625e+03\n",
      "  1.625e+03 -1.632e+03  1.657e+03 -1.657e+03 -1.666e+03 -1.670e+03\n",
      " -1.683e+03  1.696e+03 -1.729e+03 -1.731e+03 -1.733e+03 -1.746e+03\n",
      " -1.759e+03 -1.778e+03 -1.779e+03 -1.782e+03 -1.785e+03 -1.787e+03\n",
      " -1.789e+03 -1.790e+03 -1.793e+03 -1.834e+03 -1.843e+03 -1.854e+03\n",
      " -1.871e+03 -1.879e+03 -1.883e+03 -1.885e+03 -1.886e+03 -1.888e+03\n",
      " -1.889e+03 -1.893e+03  1.912e+03  1.912e+03 -1.924e+03 -1.928e+03\n",
      " -1.929e+03 -1.935e+03 -1.952e+03 -1.955e+03 -1.955e+03  1.964e+03\n",
      "  1.980e+03 -1.993e+03 -1.997e+03 -2.004e+03 -2.009e+03 -2.014e+03\n",
      " -2.016e+03 -2.017e+03 -2.038e+03 -2.067e+03 -2.080e+03 -2.087e+03\n",
      "  2.090e+03  2.105e+03 -2.133e+03 -2.172e+03 -2.172e+03 -2.186e+03\n",
      "  2.190e+03 -2.208e+03 -2.226e+03  2.227e+03 -2.231e+03  2.241e+03\n",
      " -2.246e+03  2.256e+03 -2.257e+03 -2.259e+03 -2.263e+03 -2.270e+03\n",
      " -2.274e+03 -2.283e+03  2.299e+03  2.343e+03 -2.361e+03 -2.378e+03\n",
      "  2.386e+03 -2.392e+03 -2.412e+03  2.419e+03 -2.423e+03 -2.430e+03\n",
      "  2.454e+03 -2.461e+03 -2.489e+03 -2.504e+03 -2.508e+03 -2.531e+03\n",
      " -2.552e+03 -2.554e+03  2.564e+03  2.601e+03 -2.609e+03 -2.630e+03\n",
      " -2.688e+03 -2.718e+03 -2.722e+03  2.752e+03 -2.754e+03  2.764e+03\n",
      " -2.782e+03 -2.789e+03  2.830e+03 -2.839e+03 -2.859e+03 -2.865e+03\n",
      " -2.868e+03 -2.873e+03 -2.881e+03 -2.964e+03 -3.037e+03 -3.074e+03\n",
      " -3.205e+03 -3.205e+03 -3.222e+03 -3.229e+03 -3.271e+03 -3.302e+03\n",
      " -3.328e+03 -3.341e+03 -3.377e+03 -3.392e+03 -3.409e+03 -3.431e+03\n",
      " -3.451e+03 -3.498e+03 -3.519e+03  3.554e+03  3.615e+03 -3.631e+03\n",
      " -3.639e+03 -3.728e+03 -3.736e+03 -3.744e+03 -3.834e+03 -3.841e+03\n",
      " -3.936e+03 -3.944e+03 -3.987e+03 -3.989e+03 -4.067e+03]\n",
      "Concordance Index 0.37071684977700425\n",
      "Integrated Brier Score: 0.20525779235062755\n",
      "y_train breslow final [ 2.000e+00 -3.000e+00 -1.300e+01 -1.600e+01 -1.600e+01 -1.600e+01\n",
      "  1.800e+01 -2.600e+01 -2.900e+01 -3.600e+01  4.100e+01  4.200e+01\n",
      "  4.300e+01 -4.300e+01  5.100e+01 -5.300e+01  6.200e+01  6.500e+01\n",
      "  6.900e+01  9.900e+01  1.010e+02  1.060e+02  1.100e+02 -1.180e+02\n",
      " -1.190e+02 -1.270e+02  1.370e+02  1.390e+02  1.390e+02 -1.410e+02\n",
      " -1.500e+02 -1.550e+02  1.620e+02  1.640e+02 -1.820e+02  1.820e+02\n",
      "  1.830e+02 -1.930e+02  2.020e+02  2.040e+02 -2.040e+02 -2.050e+02\n",
      "  2.060e+02 -2.060e+02  2.110e+02 -2.110e+02  2.220e+02  2.380e+02\n",
      "  2.420e+02  2.450e+02 -2.560e+02 -2.930e+02  3.070e+02  3.130e+02\n",
      "  3.130e+02 -3.230e+02  3.290e+02  3.300e+02  3.330e+02  3.340e+02\n",
      "  3.360e+02 -3.400e+02  3.420e+02  3.440e+02 -3.540e+02 -3.550e+02\n",
      "  3.620e+02 -3.620e+02 -3.650e+02 -3.650e+02 -3.690e+02 -3.700e+02\n",
      " -3.730e+02 -3.740e+02  3.750e+02 -3.860e+02 -4.000e+02 -4.060e+02\n",
      " -4.110e+02  4.310e+02 -4.330e+02 -4.350e+02  4.450e+02 -4.540e+02\n",
      "  4.590e+02 -4.680e+02  4.750e+02  4.780e+02  4.800e+02  4.850e+02\n",
      " -4.950e+02 -5.010e+02 -5.050e+02 -5.070e+02  5.100e+02 -5.190e+02\n",
      " -5.230e+02 -5.510e+02  5.520e+02  5.610e+02  5.610e+02  5.620e+02\n",
      "  5.630e+02 -5.630e+02 -5.670e+02  5.710e+02  5.720e+02 -5.740e+02\n",
      "  5.740e+02  5.780e+02  5.870e+02  6.000e+02 -6.030e+02 -6.070e+02\n",
      " -6.200e+02 -6.300e+02  6.370e+02  6.450e+02  6.460e+02 -6.480e+02\n",
      " -6.620e+02  6.790e+02  6.830e+02 -6.850e+02 -6.890e+02 -6.930e+02\n",
      " -6.930e+02 -7.000e+02 -7.140e+02  7.220e+02 -7.220e+02  7.270e+02\n",
      " -7.350e+02 -7.490e+02 -7.500e+02 -7.550e+02 -7.620e+02  7.680e+02\n",
      "  7.700e+02 -7.740e+02 -7.770e+02  7.820e+02 -7.850e+02 -7.880e+02\n",
      "  7.930e+02  8.190e+02  8.220e+02 -8.220e+02  8.280e+02 -8.300e+02\n",
      "  8.410e+02  8.450e+02  8.660e+02  8.780e+02  8.830e+02  8.850e+02\n",
      " -9.090e+02 -9.100e+02 -9.190e+02  9.270e+02  9.320e+02 -9.320e+02\n",
      " -9.320e+02 -9.450e+02  9.520e+02 -9.520e+02  9.530e+02 -9.630e+02\n",
      " -9.670e+02 -9.700e+02 -9.720e+02 -9.920e+02  9.920e+02  1.003e+03\n",
      " -1.011e+03 -1.018e+03 -1.018e+03  1.019e+03  1.045e+03 -1.046e+03\n",
      " -1.063e+03 -1.071e+03  1.075e+03  1.091e+03  1.092e+03 -1.106e+03\n",
      "  1.111e+03  1.111e+03 -1.120e+03  1.121e+03 -1.124e+03 -1.124e+03\n",
      " -1.126e+03 -1.130e+03 -1.130e+03 -1.132e+03 -1.133e+03  1.133e+03\n",
      " -1.133e+03 -1.137e+03 -1.140e+03 -1.165e+03 -1.168e+03 -1.169e+03\n",
      " -1.175e+03 -1.177e+03 -1.186e+03  1.200e+03  1.200e+03  1.230e+03\n",
      " -1.238e+03 -1.266e+03  1.270e+03 -1.274e+03 -1.290e+03 -1.291e+03\n",
      " -1.299e+03 -1.307e+03 -1.308e+03 -1.314e+03 -1.329e+03  1.337e+03\n",
      "  1.343e+03 -1.355e+03 -1.367e+03 -1.371e+03  1.371e+03 -1.373e+03\n",
      "  1.378e+03 -1.380e+03 -1.384e+03 -1.385e+03 -1.385e+03 -1.398e+03\n",
      "  1.404e+03 -1.411e+03 -1.413e+03 -1.413e+03 -1.416e+03  1.417e+03\n",
      " -1.423e+03  1.432e+03 -1.435e+03 -1.450e+03 -1.459e+03  1.463e+03\n",
      " -1.471e+03 -1.476e+03 -1.478e+03 -1.485e+03 -1.485e+03 -1.487e+03\n",
      " -1.489e+03 -1.491e+03 -1.493e+03  1.493e+03 -1.495e+03 -1.496e+03\n",
      " -1.498e+03 -1.499e+03 -1.502e+03 -1.508e+03 -1.516e+03 -1.520e+03\n",
      " -1.521e+03 -1.525e+03 -1.528e+03 -1.531e+03 -1.552e+03 -1.559e+03\n",
      "  1.567e+03  1.588e+03  1.589e+03  1.590e+03  1.598e+03 -1.604e+03\n",
      " -1.608e+03  1.610e+03  1.620e+03 -1.621e+03 -1.624e+03  1.625e+03\n",
      "  1.625e+03 -1.632e+03  1.657e+03 -1.657e+03 -1.666e+03 -1.670e+03\n",
      " -1.683e+03  1.696e+03 -1.729e+03 -1.731e+03 -1.733e+03 -1.746e+03\n",
      " -1.759e+03 -1.778e+03 -1.779e+03 -1.782e+03 -1.785e+03 -1.787e+03\n",
      " -1.789e+03 -1.790e+03 -1.793e+03 -1.834e+03 -1.843e+03 -1.854e+03\n",
      " -1.871e+03 -1.879e+03 -1.883e+03 -1.885e+03 -1.886e+03 -1.888e+03\n",
      " -1.889e+03 -1.893e+03  1.912e+03  1.912e+03 -1.924e+03 -1.928e+03\n",
      " -1.929e+03 -1.935e+03 -1.952e+03 -1.955e+03 -1.955e+03  1.964e+03\n",
      "  1.980e+03 -1.993e+03 -1.997e+03 -2.004e+03 -2.009e+03 -2.014e+03\n",
      " -2.016e+03 -2.017e+03 -2.038e+03 -2.067e+03 -2.080e+03 -2.087e+03\n",
      "  2.090e+03  2.105e+03 -2.133e+03 -2.172e+03 -2.172e+03 -2.186e+03\n",
      "  2.190e+03 -2.208e+03 -2.226e+03  2.227e+03 -2.231e+03  2.241e+03\n",
      " -2.246e+03  2.256e+03 -2.257e+03 -2.259e+03 -2.263e+03 -2.270e+03\n",
      " -2.274e+03 -2.283e+03  2.299e+03  2.343e+03 -2.361e+03 -2.378e+03\n",
      "  2.386e+03 -2.392e+03 -2.412e+03  2.419e+03 -2.423e+03 -2.430e+03\n",
      "  2.454e+03 -2.461e+03 -2.489e+03 -2.504e+03 -2.508e+03 -2.531e+03\n",
      " -2.552e+03 -2.554e+03  2.564e+03  2.601e+03 -2.609e+03 -2.630e+03\n",
      " -2.688e+03 -2.718e+03 -2.722e+03  2.752e+03 -2.754e+03  2.764e+03\n",
      " -2.782e+03 -2.789e+03  2.830e+03 -2.839e+03 -2.859e+03 -2.865e+03\n",
      " -2.868e+03 -2.873e+03 -2.881e+03 -2.964e+03 -3.037e+03 -3.074e+03\n",
      " -3.205e+03 -3.205e+03 -3.222e+03 -3.229e+03 -3.271e+03 -3.302e+03\n",
      " -3.328e+03 -3.341e+03 -3.377e+03 -3.392e+03 -3.409e+03 -3.431e+03\n",
      " -3.451e+03 -3.498e+03 -3.519e+03  3.554e+03  3.615e+03 -3.631e+03\n",
      " -3.639e+03 -3.728e+03 -3.736e+03 -3.744e+03 -3.834e+03 -3.841e+03\n",
      " -3.936e+03 -3.944e+03 -3.987e+03 -3.989e+03 -4.067e+03]\n",
      "durations 3.0 4537.0\n",
      "Concordance Index 0.3835990888382688\n",
      "Integrated Brier Score: 0.22022759570757625\n",
      "(425, 20531) <class 'pandas.core.frame.DataFrame'>\n",
      "(425,) <class 'pandas.core.series.Series'>\n",
      "(106, 20531) <class 'pandas.core.frame.DataFrame'>\n",
      "(106,) <class 'pandas.core.series.Series'>\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m2.1510\u001b[0m  0.0501\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2        2.1510  0.0409\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m2.2177\u001b[0m  0.0551\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      3        2.1510  0.1324\n",
      "      2        2.2177  0.0484\n",
      "      3        2.2177  0.0337\n",
      "      4        2.2177  0.0339\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      4        2.1510  0.1771\n",
      "      5        2.1510  0.0355\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m2.0183\u001b[0m  0.0974\n",
      "      5        2.2177  0.1456\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2        2.0183  0.0508\n",
      "      6        2.2177  0.0419\n",
      "      6        2.1510  0.0547\n",
      "      7        2.2177  0.0344\n",
      "      8        2.2177  0.0344\n",
      "      7        2.1510  0.0955\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m2.4953\u001b[0m  0.1826\n",
      "      9        2.2177  0.0368\n",
      "      3        2.0183  0.1198\n",
      "      8        2.1510  0.0336\n",
      "     10        2.2177  0.0306\n",
      "Restoring best model from epoch 1.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m2.2541\u001b[0m  0.0866\n",
      "      2        2.4953  0.1043\n",
      "      9        2.1510  0.1223\n",
      "      4        2.0183  0.1517\n",
      "      3        2.4953  0.1101\n",
      "     10        2.1510  0.0699\n",
      "Restoring best model from epoch 1.      2        2.2541  0.1293\n",
      "\n",
      "      5        2.0183  0.0703\n",
      "      4        2.4953  0.0336\n",
      "      6        2.0183  0.0408\n",
      "      5        2.4953  0.0443\n",
      "      3        2.2541  0.0684\n",
      "      7        2.0183  0.0413\n",
      "      8        2.0183  0.0309\n",
      "      9        2.0183  0.0723\n",
      "      6        2.4953  0.1372\n",
      "      4        2.2541  0.1394\n",
      "      7        2.4953  0.0496\n",
      "     10        2.0183  0.0594\n",
      "Restoring best model from epoch 1.\n",
      "      8        2.4953  0.0328\n",
      "      9        2.4953  0.0325\n",
      "      5        2.2541  0.1282\n",
      "     10        2.4953  0.0422\n",
      "Restoring best model from epoch 1.\n",
      "      6        2.2541  0.0477\n",
      "      7        2.2541  0.0445\n",
      "      8        2.2541  0.0363\n",
      "      9        2.2541  0.0356\n",
      "     10        2.2541  0.0446\n",
      "Restoring best model from epoch 1.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.9095\u001b[0m        \u001b[32m1.9922\u001b[0m  1.7532\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.4279\u001b[0m        \u001b[32m2.2655\u001b[0m  1.7878\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.8433\u001b[0m        \u001b[32m2.1952\u001b[0m  1.7361\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.7413\u001b[0m        \u001b[32m2.1191\u001b[0m  1.8716\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.0922\u001b[0m        \u001b[32m2.4799\u001b[0m  1.4769\n",
      "      2        2.9172        \u001b[32m1.9893\u001b[0m  0.5898\n",
      "      2        2.4323        \u001b[32m2.2605\u001b[0m  0.5823\n",
      "      2        \u001b[36m2.8420\u001b[0m        \u001b[32m2.1952\u001b[0m  0.5861\n",
      "      2        \u001b[36m2.7263\u001b[0m        \u001b[32m2.1083\u001b[0m  0.5985\n",
      "      2        \u001b[36m3.0917\u001b[0m        \u001b[32m2.4773\u001b[0m  0.5964\n",
      "      3        2.9159        \u001b[32m1.9786\u001b[0m  0.5508\n",
      "      3        2.4490        \u001b[32m2.2545\u001b[0m  0.5793\n",
      "      3        \u001b[36m2.8241\u001b[0m        \u001b[32m2.1951\u001b[0m  0.5464\n",
      "      3        \u001b[36m3.0659\u001b[0m        \u001b[32m2.4724\u001b[0m  0.5553\n",
      "      3        2.7310        \u001b[32m2.1005\u001b[0m  0.5752\n",
      "      4        \u001b[36m2.8725\u001b[0m        \u001b[32m1.9603\u001b[0m  0.5401\n",
      "      4        2.4615        2.2567  0.5446\n",
      "      4        \u001b[36m2.8105\u001b[0m        \u001b[32m2.1850\u001b[0m  0.5469\n",
      "      4        3.0738        \u001b[32m2.4650\u001b[0m  0.5579\n",
      "      4        \u001b[36m2.6946\u001b[0m        \u001b[32m2.0849\u001b[0m  0.5530\n",
      "      5        2.8929        \u001b[32m1.9393\u001b[0m  0.5386\n",
      "      5        2.4320        2.2552  0.5488\n",
      "      5        \u001b[36m2.7951\u001b[0m        \u001b[32m2.1676\u001b[0m  0.5497\n",
      "      5        3.0683        \u001b[32m2.4571\u001b[0m  0.5315\n",
      "      5        2.7025        \u001b[32m2.0673\u001b[0m  0.5648\n",
      "      6        2.8862        \u001b[32m1.9201\u001b[0m  0.5386\n",
      "      6        2.4528        \u001b[32m2.2518\u001b[0m  0.5731\n",
      "      6        2.8006        \u001b[32m2.1481\u001b[0m  0.5545\n",
      "      6        \u001b[36m3.0546\u001b[0m        \u001b[32m2.4469\u001b[0m  0.5468\n",
      "      6        2.7179        \u001b[32m2.0580\u001b[0m  0.5631\n",
      "      7        \u001b[36m2.8476\u001b[0m        \u001b[32m1.9131\u001b[0m  0.5650\n",
      "      7        \u001b[36m2.4234\u001b[0m        2.2521  0.5667\n",
      "      7        \u001b[36m2.7679\u001b[0m        \u001b[32m2.1343\u001b[0m  0.5599\n",
      "      7        \u001b[36m3.0370\u001b[0m        \u001b[32m2.4367\u001b[0m  0.5496\n",
      "      7        \u001b[36m2.6752\u001b[0m        \u001b[32m2.0536\u001b[0m  0.5607\n",
      "      8        2.8586        \u001b[32m1.9049\u001b[0m  0.5575\n",
      "      8        2.4441        \u001b[32m2.2473\u001b[0m  0.5600\n",
      "      8        2.7915        \u001b[32m2.1204\u001b[0m  0.5532\n",
      "      8        3.0533        \u001b[32m2.4361\u001b[0m  0.5648\n",
      "      8        \u001b[36m2.6725\u001b[0m        \u001b[32m2.0508\u001b[0m  0.5562\n",
      "      9        \u001b[36m2.8257\u001b[0m        \u001b[32m1.8903\u001b[0m  0.5656\n",
      "      9        2.4490        \u001b[32m2.2438\u001b[0m  0.5710\n",
      "      9        2.7821        \u001b[32m2.1150\u001b[0m  0.5499\n",
      "      9        \u001b[36m3.0191\u001b[0m        \u001b[32m2.4296\u001b[0m  0.5350\n",
      "      9        \u001b[36m2.6467\u001b[0m        \u001b[32m2.0487\u001b[0m  0.5512\n",
      "     10        2.8483        \u001b[32m1.8734\u001b[0m  0.5513\n",
      "     10        2.4436        \u001b[32m2.2402\u001b[0m  0.5751\n",
      "     10        2.7702        \u001b[32m2.1088\u001b[0m  0.5793\n",
      "     10        \u001b[36m3.0099\u001b[0m        \u001b[32m2.4205\u001b[0m  0.5692\n",
      "     10        \u001b[36m2.6416\u001b[0m        \u001b[32m2.0424\u001b[0m  0.5309\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m2.9527\u001b[0m  0.0217\n",
      "      2        2.9527  0.0224\n",
      "      3        2.9527  0.0171\n",
      "      4        2.9527  0.0206\n",
      "      5        2.9527  0.0219\n",
      "      6        2.9527  0.0180\n",
      "      7        2.9527  0.0221\n",
      "      8        2.9527  0.0767\n",
      "      9        2.9527  0.0192\n",
      "     10        2.9527  0.0172\n",
      "Restoring best model from epoch 1.\n",
      "y_train breslow final [ 2.000e+00 -3.000e+00 -3.000e+00 -7.000e+00 -1.100e+01 -1.300e+01\n",
      " -1.600e+01 -1.600e+01 -1.800e+01  1.800e+01 -2.600e+01 -3.600e+01\n",
      " -3.800e+01  4.200e+01  4.300e+01 -4.300e+01  5.100e+01 -5.300e+01\n",
      "  5.900e+01 -6.100e+01  6.200e+01  6.500e+01  6.800e+01  6.900e+01\n",
      "  7.300e+01  7.700e+01  9.300e+01  9.900e+01  1.010e+02  1.060e+02\n",
      "  1.090e+02  1.100e+02 -1.180e+02 -1.190e+02 -1.270e+02  1.370e+02\n",
      "  1.390e+02  1.390e+02 -1.410e+02 -1.500e+02  1.620e+02  1.640e+02\n",
      " -1.660e+02  1.660e+02  1.680e+02 -1.750e+02 -1.770e+02 -1.820e+02\n",
      "  1.820e+02  1.830e+02 -1.930e+02 -1.940e+02  2.020e+02 -2.040e+02\n",
      "  2.060e+02 -2.060e+02  2.110e+02 -2.110e+02  2.220e+02  2.240e+02\n",
      "  2.380e+02  2.420e+02  2.450e+02  3.070e+02  3.110e+02  3.130e+02\n",
      " -3.190e+02  3.200e+02  3.290e+02  3.300e+02  3.340e+02  3.360e+02\n",
      " -3.400e+02  3.440e+02 -3.540e+02 -3.550e+02 -3.560e+02  3.620e+02\n",
      " -3.620e+02 -3.650e+02 -3.700e+02 -3.720e+02 -3.740e+02  3.750e+02\n",
      " -3.850e+02 -3.860e+02 -4.060e+02 -4.080e+02 -4.110e+02 -4.310e+02\n",
      " -4.350e+02  4.450e+02  4.460e+02  4.540e+02 -4.540e+02 -4.680e+02\n",
      " -4.690e+02  4.750e+02  4.780e+02  4.800e+02  4.850e+02 -4.950e+02\n",
      " -4.980e+02 -5.010e+02 -5.050e+02 -5.070e+02  5.100e+02 -5.110e+02\n",
      " -5.190e+02 -5.230e+02 -5.280e+02 -5.510e+02  5.610e+02  5.620e+02\n",
      " -5.670e+02  5.710e+02  5.720e+02 -5.740e+02  5.780e+02  5.870e+02\n",
      " -5.910e+02  6.000e+02 -6.030e+02 -6.070e+02 -6.170e+02 -6.200e+02\n",
      "  6.370e+02  6.450e+02  6.460e+02 -6.560e+02 -6.620e+02 -6.650e+02\n",
      "  6.790e+02  6.830e+02 -6.890e+02 -6.930e+02 -7.000e+02  7.010e+02\n",
      "  7.090e+02 -7.140e+02  7.270e+02  7.350e+02 -7.370e+02 -7.490e+02\n",
      " -7.500e+02 -7.550e+02 -7.620e+02  7.680e+02  7.700e+02 -7.740e+02\n",
      " -7.770e+02  7.820e+02 -7.850e+02 -7.880e+02  7.930e+02 -8.220e+02\n",
      "  8.340e+02 -8.400e+02 -8.610e+02  8.660e+02 -8.730e+02 -8.740e+02\n",
      "  8.780e+02  8.830e+02  8.850e+02 -9.090e+02 -9.100e+02  9.270e+02\n",
      "  9.320e+02 -9.320e+02 -9.450e+02  9.460e+02 -9.510e+02  9.530e+02\n",
      " -9.630e+02 -9.670e+02 -9.700e+02 -9.720e+02  9.920e+02  1.003e+03\n",
      " -1.011e+03 -1.014e+03 -1.018e+03  1.019e+03  1.034e+03  1.045e+03\n",
      " -1.046e+03 -1.063e+03 -1.071e+03  1.075e+03  1.091e+03 -1.092e+03\n",
      "  1.092e+03  1.097e+03 -1.106e+03 -1.107e+03  1.111e+03  1.111e+03\n",
      "  1.121e+03 -1.124e+03 -1.126e+03 -1.130e+03 -1.132e+03  1.133e+03\n",
      " -1.133e+03 -1.137e+03 -1.140e+03 -1.143e+03 -1.165e+03 -1.168e+03\n",
      " -1.169e+03  1.170e+03 -1.177e+03 -1.186e+03  1.191e+03 -1.217e+03\n",
      " -1.218e+03  1.230e+03 -1.238e+03  1.238e+03 -1.266e+03  1.270e+03\n",
      " -1.274e+03 -1.290e+03 -1.291e+03 -1.299e+03 -1.307e+03 -1.308e+03\n",
      " -1.314e+03  1.317e+03 -1.329e+03  1.343e+03 -1.355e+03 -1.367e+03\n",
      " -1.371e+03  1.371e+03 -1.373e+03 -1.380e+03 -1.384e+03 -1.385e+03\n",
      " -1.385e+03 -1.398e+03 -1.413e+03 -1.416e+03  1.417e+03  1.432e+03\n",
      " -1.433e+03 -1.435e+03 -1.436e+03 -1.459e+03 -1.462e+03 -1.478e+03\n",
      " -1.485e+03 -1.485e+03 -1.487e+03 -1.489e+03 -1.491e+03 -1.493e+03\n",
      "  1.493e+03 -1.495e+03 -1.496e+03 -1.498e+03 -1.502e+03 -1.516e+03\n",
      " -1.520e+03 -1.525e+03 -1.525e+03 -1.528e+03 -1.531e+03 -1.552e+03\n",
      " -1.559e+03 -1.560e+03  1.567e+03  1.584e+03  1.588e+03  1.589e+03\n",
      "  1.590e+03 -1.604e+03 -1.608e+03  1.610e+03  1.620e+03 -1.621e+03\n",
      " -1.624e+03  1.625e+03  1.625e+03  1.626e+03 -1.632e+03  1.639e+03\n",
      "  1.657e+03 -1.657e+03  1.661e+03 -1.666e+03 -1.670e+03 -1.683e+03\n",
      "  1.696e+03  1.714e+03  1.724e+03 -1.729e+03 -1.731e+03 -1.755e+03\n",
      " -1.759e+03 -1.778e+03 -1.779e+03 -1.782e+03 -1.785e+03 -1.787e+03\n",
      " -1.789e+03 -1.793e+03 -1.794e+03 -1.834e+03 -1.854e+03 -1.862e+03\n",
      " -1.876e+03 -1.879e+03 -1.886e+03 -1.888e+03 -1.905e+03 -1.906e+03\n",
      "  1.912e+03  1.913e+03 -1.924e+03 -1.928e+03 -1.929e+03 -1.935e+03\n",
      " -1.946e+03 -1.952e+03 -1.955e+03 -1.955e+03  1.964e+03  1.980e+03\n",
      "  1.986e+03 -1.993e+03 -2.009e+03 -2.038e+03 -2.067e+03 -2.080e+03\n",
      " -2.087e+03  2.090e+03  2.105e+03 -2.128e+03 -2.133e+03  2.145e+03\n",
      " -2.150e+03 -2.172e+03 -2.184e+03 -2.186e+03  2.190e+03 -2.208e+03\n",
      " -2.217e+03  2.227e+03 -2.231e+03  2.241e+03 -2.246e+03 -2.257e+03\n",
      " -2.259e+03 -2.263e+03 -2.270e+03 -2.271e+03 -2.274e+03 -2.283e+03\n",
      "  2.343e+03 -2.361e+03 -2.372e+03 -2.378e+03 -2.392e+03 -2.412e+03\n",
      " -2.422e+03 -2.430e+03 -2.439e+03  2.454e+03 -2.461e+03 -2.470e+03\n",
      " -2.504e+03 -2.531e+03 -2.554e+03  2.564e+03  2.601e+03 -2.609e+03\n",
      " -2.609e+03 -2.660e+03 -2.718e+03 -2.722e+03 -2.746e+03 -2.754e+03\n",
      "  2.764e+03 -2.782e+03 -2.789e+03 -2.799e+03  2.830e+03 -2.865e+03\n",
      " -2.868e+03 -2.873e+03 -2.881e+03 -2.964e+03 -3.037e+03 -3.074e+03\n",
      " -3.146e+03 -3.205e+03 -3.205e+03 -3.222e+03 -3.229e+03 -3.267e+03\n",
      " -3.271e+03 -3.302e+03 -3.331e+03 -3.341e+03 -3.343e+03 -3.377e+03\n",
      " -3.392e+03 -3.409e+03 -3.431e+03 -3.451e+03 -3.480e+03 -3.519e+03\n",
      " -3.583e+03 -3.639e+03 -3.744e+03 -3.834e+03 -3.841e+03 -3.944e+03\n",
      " -3.974e+03 -3.989e+03 -4.067e+03 -4.074e+03 -4.537e+03]\n",
      "Concordance Index 0.5561973655797884\n",
      "Integrated Brier Score: 0.20990481973088673\n",
      "y_train breslow final [ 2.000e+00 -3.000e+00 -3.000e+00 -7.000e+00 -1.100e+01 -1.300e+01\n",
      " -1.600e+01 -1.600e+01 -1.800e+01  1.800e+01 -2.600e+01 -3.600e+01\n",
      " -3.800e+01  4.200e+01  4.300e+01 -4.300e+01  5.100e+01 -5.300e+01\n",
      "  5.900e+01 -6.100e+01  6.200e+01  6.500e+01  6.800e+01  6.900e+01\n",
      "  7.300e+01  7.700e+01  9.300e+01  9.900e+01  1.010e+02  1.060e+02\n",
      "  1.090e+02  1.100e+02 -1.180e+02 -1.190e+02 -1.270e+02  1.370e+02\n",
      "  1.390e+02  1.390e+02 -1.410e+02 -1.500e+02  1.620e+02  1.640e+02\n",
      " -1.660e+02  1.660e+02  1.680e+02 -1.750e+02 -1.770e+02 -1.820e+02\n",
      "  1.820e+02  1.830e+02 -1.930e+02 -1.940e+02  2.020e+02 -2.040e+02\n",
      "  2.060e+02 -2.060e+02  2.110e+02 -2.110e+02  2.220e+02  2.240e+02\n",
      "  2.380e+02  2.420e+02  2.450e+02  3.070e+02  3.110e+02  3.130e+02\n",
      " -3.190e+02  3.200e+02  3.290e+02  3.300e+02  3.340e+02  3.360e+02\n",
      " -3.400e+02  3.440e+02 -3.540e+02 -3.550e+02 -3.560e+02  3.620e+02\n",
      " -3.620e+02 -3.650e+02 -3.700e+02 -3.720e+02 -3.740e+02  3.750e+02\n",
      " -3.850e+02 -3.860e+02 -4.060e+02 -4.080e+02 -4.110e+02 -4.310e+02\n",
      " -4.350e+02  4.450e+02  4.460e+02  4.540e+02 -4.540e+02 -4.680e+02\n",
      " -4.690e+02  4.750e+02  4.780e+02  4.800e+02  4.850e+02 -4.950e+02\n",
      " -4.980e+02 -5.010e+02 -5.050e+02 -5.070e+02  5.100e+02 -5.110e+02\n",
      " -5.190e+02 -5.230e+02 -5.280e+02 -5.510e+02  5.610e+02  5.620e+02\n",
      " -5.670e+02  5.710e+02  5.720e+02 -5.740e+02  5.780e+02  5.870e+02\n",
      " -5.910e+02  6.000e+02 -6.030e+02 -6.070e+02 -6.170e+02 -6.200e+02\n",
      "  6.370e+02  6.450e+02  6.460e+02 -6.560e+02 -6.620e+02 -6.650e+02\n",
      "  6.790e+02  6.830e+02 -6.890e+02 -6.930e+02 -7.000e+02  7.010e+02\n",
      "  7.090e+02 -7.140e+02  7.270e+02  7.350e+02 -7.370e+02 -7.490e+02\n",
      " -7.500e+02 -7.550e+02 -7.620e+02  7.680e+02  7.700e+02 -7.740e+02\n",
      " -7.770e+02  7.820e+02 -7.850e+02 -7.880e+02  7.930e+02 -8.220e+02\n",
      "  8.340e+02 -8.400e+02 -8.610e+02  8.660e+02 -8.730e+02 -8.740e+02\n",
      "  8.780e+02  8.830e+02  8.850e+02 -9.090e+02 -9.100e+02  9.270e+02\n",
      "  9.320e+02 -9.320e+02 -9.450e+02  9.460e+02 -9.510e+02  9.530e+02\n",
      " -9.630e+02 -9.670e+02 -9.700e+02 -9.720e+02  9.920e+02  1.003e+03\n",
      " -1.011e+03 -1.014e+03 -1.018e+03  1.019e+03  1.034e+03  1.045e+03\n",
      " -1.046e+03 -1.063e+03 -1.071e+03  1.075e+03  1.091e+03 -1.092e+03\n",
      "  1.092e+03  1.097e+03 -1.106e+03 -1.107e+03  1.111e+03  1.111e+03\n",
      "  1.121e+03 -1.124e+03 -1.126e+03 -1.130e+03 -1.132e+03  1.133e+03\n",
      " -1.133e+03 -1.137e+03 -1.140e+03 -1.143e+03 -1.165e+03 -1.168e+03\n",
      " -1.169e+03  1.170e+03 -1.177e+03 -1.186e+03  1.191e+03 -1.217e+03\n",
      " -1.218e+03  1.230e+03 -1.238e+03  1.238e+03 -1.266e+03  1.270e+03\n",
      " -1.274e+03 -1.290e+03 -1.291e+03 -1.299e+03 -1.307e+03 -1.308e+03\n",
      " -1.314e+03  1.317e+03 -1.329e+03  1.343e+03 -1.355e+03 -1.367e+03\n",
      " -1.371e+03  1.371e+03 -1.373e+03 -1.380e+03 -1.384e+03 -1.385e+03\n",
      " -1.385e+03 -1.398e+03 -1.413e+03 -1.416e+03  1.417e+03  1.432e+03\n",
      " -1.433e+03 -1.435e+03 -1.436e+03 -1.459e+03 -1.462e+03 -1.478e+03\n",
      " -1.485e+03 -1.485e+03 -1.487e+03 -1.489e+03 -1.491e+03 -1.493e+03\n",
      "  1.493e+03 -1.495e+03 -1.496e+03 -1.498e+03 -1.502e+03 -1.516e+03\n",
      " -1.520e+03 -1.525e+03 -1.525e+03 -1.528e+03 -1.531e+03 -1.552e+03\n",
      " -1.559e+03 -1.560e+03  1.567e+03  1.584e+03  1.588e+03  1.589e+03\n",
      "  1.590e+03 -1.604e+03 -1.608e+03  1.610e+03  1.620e+03 -1.621e+03\n",
      " -1.624e+03  1.625e+03  1.625e+03  1.626e+03 -1.632e+03  1.639e+03\n",
      "  1.657e+03 -1.657e+03  1.661e+03 -1.666e+03 -1.670e+03 -1.683e+03\n",
      "  1.696e+03  1.714e+03  1.724e+03 -1.729e+03 -1.731e+03 -1.755e+03\n",
      " -1.759e+03 -1.778e+03 -1.779e+03 -1.782e+03 -1.785e+03 -1.787e+03\n",
      " -1.789e+03 -1.793e+03 -1.794e+03 -1.834e+03 -1.854e+03 -1.862e+03\n",
      " -1.876e+03 -1.879e+03 -1.886e+03 -1.888e+03 -1.905e+03 -1.906e+03\n",
      "  1.912e+03  1.913e+03 -1.924e+03 -1.928e+03 -1.929e+03 -1.935e+03\n",
      " -1.946e+03 -1.952e+03 -1.955e+03 -1.955e+03  1.964e+03  1.980e+03\n",
      "  1.986e+03 -1.993e+03 -2.009e+03 -2.038e+03 -2.067e+03 -2.080e+03\n",
      " -2.087e+03  2.090e+03  2.105e+03 -2.128e+03 -2.133e+03  2.145e+03\n",
      " -2.150e+03 -2.172e+03 -2.184e+03 -2.186e+03  2.190e+03 -2.208e+03\n",
      " -2.217e+03  2.227e+03 -2.231e+03  2.241e+03 -2.246e+03 -2.257e+03\n",
      " -2.259e+03 -2.263e+03 -2.270e+03 -2.271e+03 -2.274e+03 -2.283e+03\n",
      "  2.343e+03 -2.361e+03 -2.372e+03 -2.378e+03 -2.392e+03 -2.412e+03\n",
      " -2.422e+03 -2.430e+03 -2.439e+03  2.454e+03 -2.461e+03 -2.470e+03\n",
      " -2.504e+03 -2.531e+03 -2.554e+03  2.564e+03  2.601e+03 -2.609e+03\n",
      " -2.609e+03 -2.660e+03 -2.718e+03 -2.722e+03 -2.746e+03 -2.754e+03\n",
      "  2.764e+03 -2.782e+03 -2.789e+03 -2.799e+03  2.830e+03 -2.865e+03\n",
      " -2.868e+03 -2.873e+03 -2.881e+03 -2.964e+03 -3.037e+03 -3.074e+03\n",
      " -3.146e+03 -3.205e+03 -3.205e+03 -3.222e+03 -3.229e+03 -3.267e+03\n",
      " -3.271e+03 -3.302e+03 -3.331e+03 -3.341e+03 -3.343e+03 -3.377e+03\n",
      " -3.392e+03 -3.409e+03 -3.431e+03 -3.451e+03 -3.480e+03 -3.519e+03\n",
      " -3.583e+03 -3.639e+03 -3.744e+03 -3.834e+03 -3.841e+03 -3.944e+03\n",
      " -3.974e+03 -3.989e+03 -4.067e+03 -4.074e+03 -4.537e+03]\n",
      "durations 16.0 3987.0\n",
      "Concordance Index 0.5544041450777202\n",
      "Integrated Brier Score: 0.1941697816870491\n",
      "(425, 20531) <class 'pandas.core.frame.DataFrame'>\n",
      "(425,) <class 'pandas.core.series.Series'>\n",
      "(106, 20531) <class 'pandas.core.frame.DataFrame'>\n",
      "(106,) <class 'pandas.core.series.Series'>\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.2308\u001b[0m  0.0585\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2        3.2308  0.0379\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m1.9275\u001b[0m  0.0344\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      3        3.2308  0.0471\n",
      "      2        1.9275  0.0315\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m2.3148\u001b[0m  0.0732\n",
      "      4        3.2308  0.0357\n",
      "      3        1.9275  0.0360\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m1.9173\u001b[0m  0.0591\n",
      "      2        2.3148  0.0548\n",
      "      5        3.2308  0.0366\n",
      "      4        1.9275  0.0406\n",
      "      2        1.9173  0.0343\n",
      "      6        3.2308  0.0329\n",
      "      3        2.3148  0.0359\n",
      "      5        1.9275  0.0340\n",
      "      3        1.9173  0.0316\n",
      "      4        2.3148  0.0323\n",
      "      7        3.2308  0.0725\n",
      "      5        2.3148  0.0376\n",
      "      6        1.9275  0.0635\n",
      "      4        1.9173  0.0525\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m2.6119\u001b[0m  0.0345\n",
      "      6        2.3148  0.0394\n",
      "      5        1.9173  0.0354\n",
      "      7        1.9275  0.0390\n",
      "      8        3.2308  0.0444\n",
      "      2        2.6119  0.0469\n",
      "      8        1.9275  0.0346\n",
      "      6        1.9173  0.0377\n",
      "      7        2.3148  0.0399\n",
      "      9        3.2308  0.0422\n",
      "      3        2.6119  0.0312\n",
      "      7        1.9173  0.0361\n",
      "      8        2.3148  0.0370\n",
      "     10        3.2308  0.0355\n",
      "Restoring best model from epoch 1.\n",
      "      9        2.3148  0.0324\n",
      "      8        1.9173  0.0377\n",
      "      9        1.9275  0.1021\n",
      "      4        2.6119  0.0969\n",
      "      9        1.9173  0.0934\n",
      "      5        2.6119  0.0442\n",
      "     10        2.3148  0.1003\n",
      "Restoring best model from epoch 1.\n",
      "     10        1.9275  0.0875\n",
      "Restoring best model from epoch 1.\n",
      "      6        2.6119  0.0425\n",
      "     10        1.9173  0.0628\n",
      "Restoring best model from epoch 1.\n",
      "      7        2.6119  0.0538\n",
      "      8        2.6119  0.0308\n",
      "      9        2.6119  0.1707\n",
      "     10        2.6119  0.0335\n",
      "Restoring best model from epoch 1.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.8496\u001b[0m        \u001b[32m2.6066\u001b[0m  1.1585\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.8997\u001b[0m        \u001b[32m3.2387\u001b[0m  1.2519\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.5653\u001b[0m        \u001b[32m1.9190\u001b[0m  1.2992\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.9761\u001b[0m        \u001b[32m2.3097\u001b[0m  1.3851\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.6608\u001b[0m        \u001b[32m1.9377\u001b[0m  1.5968\n",
      "      2        2.8682        2.6110  0.6196\n",
      "      2        2.9132        \u001b[32m3.2369\u001b[0m  0.6382\n",
      "      2        2.5885        1.9207  0.7310\n",
      "      2        2.9879        \u001b[32m2.3006\u001b[0m  0.6979\n",
      "      2        2.6839        \u001b[32m1.9369\u001b[0m  0.5934\n",
      "      3        2.8838        2.6067  0.5989\n",
      "      3        \u001b[36m2.8995\u001b[0m        \u001b[32m3.2359\u001b[0m  0.6038\n",
      "      3        2.5919        1.9195  0.6410\n",
      "      3        2.9880        \u001b[32m2.2932\u001b[0m  0.6266\n",
      "      3        2.6712        \u001b[32m1.9365\u001b[0m  0.6168\n",
      "      4        2.8546        \u001b[32m2.6035\u001b[0m  0.6570\n",
      "      4        2.9013        \u001b[32m3.2335\u001b[0m  0.6378\n",
      "      4        \u001b[36m2.9501\u001b[0m        \u001b[32m2.2837\u001b[0m  0.6320\n",
      "      4        2.5782        \u001b[32m1.9163\u001b[0m  0.6462\n",
      "      4        2.6646        1.9376  0.6394\n",
      "      5        2.8627        \u001b[32m2.5993\u001b[0m  0.6323\n",
      "      5        2.9030        \u001b[32m3.2324\u001b[0m  0.6239\n",
      "      5        2.5722        \u001b[32m1.9114\u001b[0m  0.5989\n",
      "      5        2.9805        \u001b[32m2.2775\u001b[0m  0.6083\n",
      "      5        2.6736        1.9402  0.6063\n",
      "      6        2.8763        \u001b[32m2.5891\u001b[0m  0.5927\n",
      "      6        2.9058        \u001b[32m3.2321\u001b[0m  0.5790\n",
      "      6        2.5781        \u001b[32m1.9105\u001b[0m  0.6064\n",
      "      6        2.9684        \u001b[32m2.2673\u001b[0m  0.6094\n",
      "      6        2.6733        1.9409  0.5861\n",
      "      7        2.8649        \u001b[32m2.5718\u001b[0m  0.5716\n",
      "      7        \u001b[36m2.8814\u001b[0m        \u001b[32m3.2299\u001b[0m  0.5656\n",
      "      7        \u001b[36m2.5438\u001b[0m        \u001b[32m1.9104\u001b[0m  0.5891\n",
      "      7        \u001b[36m2.9127\u001b[0m        \u001b[32m2.2622\u001b[0m  0.5859\n",
      "      7        \u001b[36m2.6602\u001b[0m        1.9411  0.5587\n",
      "      8        2.8500        \u001b[32m2.5583\u001b[0m  0.5644\n",
      "      8        2.9010        3.2303  0.5716\n",
      "      8        \u001b[36m2.5343\u001b[0m        \u001b[32m1.9097\u001b[0m  0.5876\n",
      "      8        2.9363        \u001b[32m2.2564\u001b[0m  0.5961\n",
      "      8        2.6669        1.9457  0.5903\n",
      "      9        \u001b[36m2.8318\u001b[0m        \u001b[32m2.5560\u001b[0m  0.5968\n",
      "      9        2.8863        \u001b[32m3.2279\u001b[0m  0.5833\n",
      "      9        2.5483        \u001b[32m1.9071\u001b[0m  0.5750\n",
      "      9        2.9488        \u001b[32m2.2523\u001b[0m  0.5715\n",
      "      9        2.6842        1.9493  0.5538\n",
      "     10        \u001b[36m2.8028\u001b[0m        \u001b[32m2.5488\u001b[0m  0.5502\n",
      "     10        2.8920        3.2287  0.5346\n",
      "Restoring best model from epoch 9.\n",
      "     10        \u001b[36m2.9072\u001b[0m        \u001b[32m2.2485\u001b[0m  0.5393\n",
      "     10        2.5365        \u001b[32m1.9003\u001b[0m  0.5524\n",
      "     10        2.6640        1.9508  0.5430\n",
      "Restoring best model from epoch 3.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m2.9704\u001b[0m  0.0210\n",
      "      2        2.9704  0.0200\n",
      "      3        2.9704  0.0213\n",
      "      4        2.9704  0.0211\n",
      "      5        2.9704  0.0436\n",
      "      6        2.9704  0.0215\n",
      "      7        2.9704  0.0185\n",
      "      8        2.9704  0.0181\n",
      "      9        2.9704  0.0181\n",
      "     10        2.9704  0.0161\n",
      "Restoring best model from epoch 1.\n",
      "y_train breslow final [ 2.000e+00 -3.000e+00 -3.000e+00 -7.000e+00 -1.100e+01 -1.600e+01\n",
      " -1.600e+01 -1.800e+01  1.800e+01 -2.600e+01 -2.900e+01 -3.600e+01\n",
      " -3.800e+01  4.100e+01  4.300e+01  5.900e+01 -6.100e+01  6.200e+01\n",
      "  6.800e+01  7.300e+01  7.700e+01  9.300e+01  9.900e+01  1.010e+02\n",
      "  1.090e+02  1.100e+02 -1.180e+02 -1.190e+02  1.370e+02  1.390e+02\n",
      " -1.550e+02  1.640e+02 -1.660e+02  1.660e+02  1.680e+02 -1.750e+02\n",
      " -1.770e+02 -1.820e+02  1.830e+02 -1.940e+02  2.020e+02  2.040e+02\n",
      " -2.040e+02 -2.050e+02  2.060e+02 -2.060e+02  2.110e+02 -2.110e+02\n",
      "  2.220e+02  2.240e+02  2.450e+02 -2.560e+02 -2.930e+02  3.070e+02\n",
      "  3.110e+02  3.130e+02 -3.190e+02  3.200e+02 -3.230e+02  3.290e+02\n",
      "  3.300e+02  3.330e+02  3.340e+02  3.360e+02 -3.400e+02  3.420e+02\n",
      " -3.550e+02 -3.560e+02  3.620e+02 -3.620e+02 -3.650e+02 -3.690e+02\n",
      " -3.700e+02 -3.720e+02 -3.730e+02 -3.740e+02  3.750e+02 -3.850e+02\n",
      " -3.860e+02 -4.000e+02 -4.080e+02 -4.110e+02  4.310e+02 -4.310e+02\n",
      " -4.330e+02 -4.350e+02  4.460e+02  4.540e+02  4.590e+02 -4.680e+02\n",
      " -4.690e+02  4.750e+02  4.780e+02  4.800e+02  4.800e+02  4.850e+02\n",
      " -4.980e+02 -5.010e+02 -5.050e+02 -5.070e+02  5.100e+02 -5.110e+02\n",
      " -5.190e+02 -5.230e+02 -5.280e+02  5.520e+02  5.610e+02  5.610e+02\n",
      "  5.620e+02  5.630e+02 -5.630e+02 -5.670e+02  5.710e+02  5.720e+02\n",
      " -5.740e+02  5.740e+02  5.870e+02 -5.910e+02  6.000e+02 -6.170e+02\n",
      " -6.200e+02 -6.300e+02  6.370e+02  6.460e+02 -6.480e+02 -6.560e+02\n",
      " -6.620e+02 -6.650e+02 -6.850e+02 -6.930e+02 -6.930e+02 -7.000e+02\n",
      "  7.010e+02  7.090e+02  7.220e+02 -7.220e+02  7.270e+02  7.350e+02\n",
      " -7.350e+02 -7.370e+02 -7.500e+02  7.700e+02 -7.740e+02 -7.770e+02\n",
      "  7.820e+02 -7.880e+02  7.930e+02  8.190e+02  8.220e+02 -8.220e+02\n",
      "  8.280e+02 -8.300e+02  8.340e+02 -8.400e+02  8.410e+02  8.450e+02\n",
      " -8.610e+02 -8.730e+02 -8.740e+02  8.780e+02  8.830e+02  8.850e+02\n",
      "  8.850e+02 -9.190e+02  9.320e+02 -9.320e+02 -9.320e+02  9.460e+02\n",
      " -9.510e+02  9.520e+02 -9.520e+02  9.530e+02 -9.630e+02 -9.670e+02\n",
      " -9.700e+02 -9.720e+02 -9.920e+02 -1.011e+03 -1.014e+03 -1.018e+03\n",
      " -1.018e+03  1.034e+03  1.045e+03 -1.092e+03  1.097e+03 -1.106e+03\n",
      " -1.107e+03  1.111e+03  1.111e+03 -1.120e+03  1.121e+03 -1.124e+03\n",
      " -1.124e+03 -1.126e+03 -1.130e+03 -1.130e+03 -1.132e+03 -1.133e+03\n",
      "  1.133e+03 -1.133e+03 -1.137e+03 -1.140e+03 -1.143e+03 -1.165e+03\n",
      "  1.170e+03 -1.175e+03 -1.177e+03 -1.186e+03  1.191e+03  1.200e+03\n",
      "  1.200e+03 -1.217e+03 -1.218e+03  1.230e+03  1.238e+03  1.270e+03\n",
      " -1.291e+03 -1.299e+03 -1.307e+03 -1.308e+03  1.317e+03  1.337e+03\n",
      "  1.343e+03 -1.355e+03 -1.371e+03 -1.373e+03  1.378e+03 -1.384e+03\n",
      " -1.385e+03 -1.385e+03 -1.398e+03  1.404e+03 -1.411e+03 -1.413e+03\n",
      " -1.416e+03  1.417e+03 -1.423e+03  1.432e+03 -1.433e+03 -1.435e+03\n",
      " -1.436e+03 -1.450e+03 -1.459e+03 -1.462e+03  1.463e+03 -1.471e+03\n",
      " -1.476e+03 -1.485e+03 -1.485e+03 -1.489e+03 -1.491e+03 -1.493e+03\n",
      "  1.493e+03 -1.495e+03 -1.496e+03 -1.499e+03 -1.502e+03 -1.508e+03\n",
      " -1.516e+03 -1.520e+03 -1.520e+03 -1.521e+03 -1.525e+03 -1.525e+03\n",
      " -1.531e+03 -1.559e+03 -1.560e+03  1.567e+03  1.584e+03  1.588e+03\n",
      "  1.590e+03  1.598e+03 -1.604e+03  1.610e+03 -1.621e+03 -1.624e+03\n",
      "  1.625e+03  1.625e+03  1.626e+03 -1.632e+03  1.639e+03 -1.657e+03\n",
      "  1.661e+03 -1.666e+03 -1.670e+03  1.696e+03  1.714e+03  1.724e+03\n",
      " -1.729e+03 -1.731e+03 -1.733e+03 -1.746e+03 -1.755e+03 -1.759e+03\n",
      " -1.778e+03 -1.779e+03 -1.785e+03 -1.787e+03 -1.789e+03 -1.790e+03\n",
      " -1.793e+03 -1.794e+03 -1.834e+03 -1.843e+03 -1.854e+03 -1.862e+03\n",
      " -1.871e+03 -1.876e+03 -1.879e+03 -1.883e+03 -1.885e+03 -1.889e+03\n",
      " -1.893e+03 -1.905e+03 -1.906e+03  1.912e+03  1.912e+03  1.913e+03\n",
      " -1.924e+03 -1.928e+03 -1.929e+03 -1.935e+03 -1.946e+03 -1.952e+03\n",
      " -1.955e+03 -1.955e+03  1.964e+03  1.986e+03 -1.997e+03 -2.004e+03\n",
      " -2.014e+03 -2.016e+03 -2.017e+03  2.090e+03  2.105e+03 -2.128e+03\n",
      " -2.133e+03  2.145e+03 -2.150e+03 -2.172e+03 -2.172e+03 -2.184e+03\n",
      " -2.186e+03  2.190e+03 -2.208e+03 -2.217e+03 -2.226e+03 -2.231e+03\n",
      "  2.241e+03 -2.246e+03  2.256e+03 -2.257e+03 -2.259e+03 -2.263e+03\n",
      " -2.270e+03 -2.271e+03 -2.283e+03  2.299e+03 -2.361e+03 -2.372e+03\n",
      " -2.378e+03  2.386e+03 -2.392e+03  2.419e+03 -2.422e+03 -2.423e+03\n",
      " -2.439e+03 -2.461e+03 -2.470e+03 -2.489e+03 -2.504e+03 -2.508e+03\n",
      " -2.552e+03 -2.554e+03  2.564e+03  2.601e+03 -2.609e+03 -2.609e+03\n",
      " -2.630e+03 -2.660e+03 -2.688e+03 -2.722e+03 -2.746e+03  2.752e+03\n",
      " -2.782e+03 -2.799e+03  2.830e+03 -2.839e+03 -2.859e+03 -2.868e+03\n",
      " -2.873e+03 -2.964e+03 -3.037e+03 -3.074e+03 -3.146e+03 -3.205e+03\n",
      " -3.222e+03 -3.267e+03 -3.271e+03 -3.302e+03 -3.328e+03 -3.331e+03\n",
      " -3.341e+03 -3.343e+03 -3.377e+03 -3.392e+03 -3.431e+03 -3.451e+03\n",
      " -3.480e+03 -3.498e+03 -3.519e+03  3.554e+03 -3.583e+03  3.615e+03\n",
      " -3.631e+03 -3.639e+03 -3.728e+03 -3.736e+03 -3.834e+03 -3.936e+03\n",
      " -3.944e+03 -3.974e+03 -3.987e+03 -4.074e+03 -4.537e+03]\n",
      "Concordance Index 0.5535332492736144\n",
      "Integrated Brier Score: 0.20678097684754265\n",
      "y_train breslow final [ 2.000e+00 -3.000e+00 -3.000e+00 -7.000e+00 -1.100e+01 -1.600e+01\n",
      " -1.600e+01 -1.800e+01  1.800e+01 -2.600e+01 -2.900e+01 -3.600e+01\n",
      " -3.800e+01  4.100e+01  4.300e+01  5.900e+01 -6.100e+01  6.200e+01\n",
      "  6.800e+01  7.300e+01  7.700e+01  9.300e+01  9.900e+01  1.010e+02\n",
      "  1.090e+02  1.100e+02 -1.180e+02 -1.190e+02  1.370e+02  1.390e+02\n",
      " -1.550e+02  1.640e+02 -1.660e+02  1.660e+02  1.680e+02 -1.750e+02\n",
      " -1.770e+02 -1.820e+02  1.830e+02 -1.940e+02  2.020e+02  2.040e+02\n",
      " -2.040e+02 -2.050e+02  2.060e+02 -2.060e+02  2.110e+02 -2.110e+02\n",
      "  2.220e+02  2.240e+02  2.450e+02 -2.560e+02 -2.930e+02  3.070e+02\n",
      "  3.110e+02  3.130e+02 -3.190e+02  3.200e+02 -3.230e+02  3.290e+02\n",
      "  3.300e+02  3.330e+02  3.340e+02  3.360e+02 -3.400e+02  3.420e+02\n",
      " -3.550e+02 -3.560e+02  3.620e+02 -3.620e+02 -3.650e+02 -3.690e+02\n",
      " -3.700e+02 -3.720e+02 -3.730e+02 -3.740e+02  3.750e+02 -3.850e+02\n",
      " -3.860e+02 -4.000e+02 -4.080e+02 -4.110e+02  4.310e+02 -4.310e+02\n",
      " -4.330e+02 -4.350e+02  4.460e+02  4.540e+02  4.590e+02 -4.680e+02\n",
      " -4.690e+02  4.750e+02  4.780e+02  4.800e+02  4.800e+02  4.850e+02\n",
      " -4.980e+02 -5.010e+02 -5.050e+02 -5.070e+02  5.100e+02 -5.110e+02\n",
      " -5.190e+02 -5.230e+02 -5.280e+02  5.520e+02  5.610e+02  5.610e+02\n",
      "  5.620e+02  5.630e+02 -5.630e+02 -5.670e+02  5.710e+02  5.720e+02\n",
      " -5.740e+02  5.740e+02  5.870e+02 -5.910e+02  6.000e+02 -6.170e+02\n",
      " -6.200e+02 -6.300e+02  6.370e+02  6.460e+02 -6.480e+02 -6.560e+02\n",
      " -6.620e+02 -6.650e+02 -6.850e+02 -6.930e+02 -6.930e+02 -7.000e+02\n",
      "  7.010e+02  7.090e+02  7.220e+02 -7.220e+02  7.270e+02  7.350e+02\n",
      " -7.350e+02 -7.370e+02 -7.500e+02  7.700e+02 -7.740e+02 -7.770e+02\n",
      "  7.820e+02 -7.880e+02  7.930e+02  8.190e+02  8.220e+02 -8.220e+02\n",
      "  8.280e+02 -8.300e+02  8.340e+02 -8.400e+02  8.410e+02  8.450e+02\n",
      " -8.610e+02 -8.730e+02 -8.740e+02  8.780e+02  8.830e+02  8.850e+02\n",
      "  8.850e+02 -9.190e+02  9.320e+02 -9.320e+02 -9.320e+02  9.460e+02\n",
      " -9.510e+02  9.520e+02 -9.520e+02  9.530e+02 -9.630e+02 -9.670e+02\n",
      " -9.700e+02 -9.720e+02 -9.920e+02 -1.011e+03 -1.014e+03 -1.018e+03\n",
      " -1.018e+03  1.034e+03  1.045e+03 -1.092e+03  1.097e+03 -1.106e+03\n",
      " -1.107e+03  1.111e+03  1.111e+03 -1.120e+03  1.121e+03 -1.124e+03\n",
      " -1.124e+03 -1.126e+03 -1.130e+03 -1.130e+03 -1.132e+03 -1.133e+03\n",
      "  1.133e+03 -1.133e+03 -1.137e+03 -1.140e+03 -1.143e+03 -1.165e+03\n",
      "  1.170e+03 -1.175e+03 -1.177e+03 -1.186e+03  1.191e+03  1.200e+03\n",
      "  1.200e+03 -1.217e+03 -1.218e+03  1.230e+03  1.238e+03  1.270e+03\n",
      " -1.291e+03 -1.299e+03 -1.307e+03 -1.308e+03  1.317e+03  1.337e+03\n",
      "  1.343e+03 -1.355e+03 -1.371e+03 -1.373e+03  1.378e+03 -1.384e+03\n",
      " -1.385e+03 -1.385e+03 -1.398e+03  1.404e+03 -1.411e+03 -1.413e+03\n",
      " -1.416e+03  1.417e+03 -1.423e+03  1.432e+03 -1.433e+03 -1.435e+03\n",
      " -1.436e+03 -1.450e+03 -1.459e+03 -1.462e+03  1.463e+03 -1.471e+03\n",
      " -1.476e+03 -1.485e+03 -1.485e+03 -1.489e+03 -1.491e+03 -1.493e+03\n",
      "  1.493e+03 -1.495e+03 -1.496e+03 -1.499e+03 -1.502e+03 -1.508e+03\n",
      " -1.516e+03 -1.520e+03 -1.520e+03 -1.521e+03 -1.525e+03 -1.525e+03\n",
      " -1.531e+03 -1.559e+03 -1.560e+03  1.567e+03  1.584e+03  1.588e+03\n",
      "  1.590e+03  1.598e+03 -1.604e+03  1.610e+03 -1.621e+03 -1.624e+03\n",
      "  1.625e+03  1.625e+03  1.626e+03 -1.632e+03  1.639e+03 -1.657e+03\n",
      "  1.661e+03 -1.666e+03 -1.670e+03  1.696e+03  1.714e+03  1.724e+03\n",
      " -1.729e+03 -1.731e+03 -1.733e+03 -1.746e+03 -1.755e+03 -1.759e+03\n",
      " -1.778e+03 -1.779e+03 -1.785e+03 -1.787e+03 -1.789e+03 -1.790e+03\n",
      " -1.793e+03 -1.794e+03 -1.834e+03 -1.843e+03 -1.854e+03 -1.862e+03\n",
      " -1.871e+03 -1.876e+03 -1.879e+03 -1.883e+03 -1.885e+03 -1.889e+03\n",
      " -1.893e+03 -1.905e+03 -1.906e+03  1.912e+03  1.912e+03  1.913e+03\n",
      " -1.924e+03 -1.928e+03 -1.929e+03 -1.935e+03 -1.946e+03 -1.952e+03\n",
      " -1.955e+03 -1.955e+03  1.964e+03  1.986e+03 -1.997e+03 -2.004e+03\n",
      " -2.014e+03 -2.016e+03 -2.017e+03  2.090e+03  2.105e+03 -2.128e+03\n",
      " -2.133e+03  2.145e+03 -2.150e+03 -2.172e+03 -2.172e+03 -2.184e+03\n",
      " -2.186e+03  2.190e+03 -2.208e+03 -2.217e+03 -2.226e+03 -2.231e+03\n",
      "  2.241e+03 -2.246e+03  2.256e+03 -2.257e+03 -2.259e+03 -2.263e+03\n",
      " -2.270e+03 -2.271e+03 -2.283e+03  2.299e+03 -2.361e+03 -2.372e+03\n",
      " -2.378e+03  2.386e+03 -2.392e+03  2.419e+03 -2.422e+03 -2.423e+03\n",
      " -2.439e+03 -2.461e+03 -2.470e+03 -2.489e+03 -2.504e+03 -2.508e+03\n",
      " -2.552e+03 -2.554e+03  2.564e+03  2.601e+03 -2.609e+03 -2.609e+03\n",
      " -2.630e+03 -2.660e+03 -2.688e+03 -2.722e+03 -2.746e+03  2.752e+03\n",
      " -2.782e+03 -2.799e+03  2.830e+03 -2.839e+03 -2.859e+03 -2.868e+03\n",
      " -2.873e+03 -2.964e+03 -3.037e+03 -3.074e+03 -3.146e+03 -3.205e+03\n",
      " -3.222e+03 -3.267e+03 -3.271e+03 -3.302e+03 -3.328e+03 -3.331e+03\n",
      " -3.341e+03 -3.343e+03 -3.377e+03 -3.392e+03 -3.431e+03 -3.451e+03\n",
      " -3.480e+03 -3.498e+03 -3.519e+03  3.554e+03 -3.583e+03  3.615e+03\n",
      " -3.631e+03 -3.639e+03 -3.728e+03 -3.736e+03 -3.834e+03 -3.936e+03\n",
      " -3.944e+03 -3.974e+03 -3.987e+03 -4.074e+03 -4.537e+03]\n",
      "durations 13.0 4067.0\n",
      "Concordance Index 0.5625276182059213\n",
      "Integrated Brier Score: 0.20663226237028023\n",
      "(425, 20531) <class 'pandas.core.frame.DataFrame'>\n",
      "(425,) <class 'pandas.core.series.Series'>\n",
      "(106, 20531) <class 'pandas.core.frame.DataFrame'>\n",
      "(106,) <class 'pandas.core.series.Series'>\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m2.8340\u001b[0m  0.0397\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.2471\u001b[0m  0.0329\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m2.9959\u001b[0m  0.0490\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.0618\u001b[0m  0.0349\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m2.3972\u001b[0m  0.0441\n",
      "      2        2.8340  0.0330\n",
      "      2        2.9959  0.0324\n",
      "      2        2.3972  0.0375\n",
      "      2        3.2471  0.0446\n",
      "      2        3.0618  0.0407\n",
      "      3        2.8340  0.0437\n",
      "      3        2.9959  0.0334\n",
      "      3        3.0618  0.0387\n",
      "      3        2.3972  0.0433\n",
      "      3        3.2471  0.0520\n",
      "      4        2.9959  0.0379\n",
      "      4        3.2471  0.0369\n",
      "      5        2.9959  0.0297\n",
      "      4        2.3972  0.0507\n",
      "      4        2.8340  0.0808\n",
      "      6        2.9959  0.0358\n",
      "      5        2.3972  0.0375\n",
      "      5        3.2471  0.0580\n",
      "      4        3.0618  0.1074\n",
      "      6        2.3972  0.0371\n",
      "      7        2.9959  0.0431\n",
      "      5        3.0618  0.0404\n",
      "      5        2.8340  0.0881\n",
      "      6        3.2471  0.0561\n",
      "      8        2.9959  0.0340\n",
      "      7        2.3972  0.0392\n",
      "      6        3.0618  0.0315\n",
      "      6        2.8340  0.0351\n",
      "      7        3.2471  0.0317\n",
      "      9        2.9959  0.0318\n",
      "      7        3.0618  0.0335\n",
      "      7        2.8340  0.0331\n",
      "      8        2.3972  0.0616\n",
      "      8        3.2471  0.0378\n",
      "     10        2.9959  0.0379\n",
      "Restoring best model from epoch 1.\n",
      "      8        3.0618  0.0365\n",
      "      8        2.8340  0.0337\n",
      "      9        3.2471  0.0336\n",
      "      9        2.3972  0.0381\n",
      "      9        3.0618  0.0475\n",
      "      9        2.8340  0.0438\n",
      "     10        3.2471  0.0332\n",
      "Restoring best model from epoch 1.\n",
      "     10        2.3972  0.0360\n",
      "Restoring best model from epoch 1.\n",
      "     10        2.8340  0.0376     10        3.0618  0.0376\n",
      "\n",
      "Restoring best model from epoch 1.\n",
      "Restoring best model from epoch 1.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.5141\u001b[0m        \u001b[32m2.3890\u001b[0m  0.8663\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.4061\u001b[0m        \u001b[32m2.8327\u001b[0m  0.9078\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.8935\u001b[0m        \u001b[32m3.0638\u001b[0m  0.9485\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.7591\u001b[0m        \u001b[32m3.2495\u001b[0m  1.0247\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.6099\u001b[0m        \u001b[32m2.9760\u001b[0m  1.0920\n",
      "      2        2.5155        \u001b[32m2.3854\u001b[0m  0.5823\n",
      "      2        2.4102        2.8381  0.5668\n",
      "      2        \u001b[36m2.8882\u001b[0m        \u001b[32m3.0603\u001b[0m  0.5605\n",
      "      2        \u001b[36m2.6082\u001b[0m        \u001b[32m2.9760\u001b[0m  0.5765\n",
      "      2        2.7749        \u001b[32m3.2427\u001b[0m  0.5933\n",
      "      3        \u001b[36m2.5004\u001b[0m        \u001b[32m2.3844\u001b[0m  0.5627\n",
      "      3        \u001b[36m2.3970\u001b[0m        2.8426  0.5557\n",
      "      3        2.8905        \u001b[32m3.0597\u001b[0m  0.5628\n",
      "      3        \u001b[36m2.7458\u001b[0m        \u001b[32m3.2420\u001b[0m  0.5681\n",
      "      3        2.6177        \u001b[32m2.9717\u001b[0m  0.5853\n",
      "      4        2.5117        \u001b[32m2.3827\u001b[0m  0.5657\n",
      "      4        2.3999        2.8461  0.5383\n",
      "      4        \u001b[36m2.8827\u001b[0m        3.0609  0.5803\n",
      "      4        2.7467        \u001b[32m3.2400\u001b[0m  0.5450\n",
      "      4        \u001b[36m2.5972\u001b[0m        \u001b[32m2.9584\u001b[0m  0.5920\n",
      "      5        2.4153        2.8477  0.6173\n",
      "      5        2.5041        \u001b[32m2.3747\u001b[0m  0.6457\n",
      "      5        \u001b[36m2.8781\u001b[0m        \u001b[32m3.0544\u001b[0m  0.6188\n",
      "      5        2.7653        \u001b[32m3.2310\u001b[0m  0.6131\n",
      "      5        \u001b[36m2.5656\u001b[0m        \u001b[32m2.9444\u001b[0m  0.6232\n",
      "      6        2.4045        2.8474  0.5861\n",
      "      6        \u001b[36m2.4611\u001b[0m        \u001b[32m2.3677\u001b[0m  0.5818\n",
      "      6        2.8840        \u001b[32m3.0540\u001b[0m  0.5727\n",
      "      6        2.7501        \u001b[32m3.2205\u001b[0m  0.5833\n",
      "      6        2.5893        \u001b[32m2.9300\u001b[0m  0.5961\n",
      "      7        2.4039        2.8506  0.5680\n",
      "      7        2.4859        \u001b[32m2.3628\u001b[0m  0.5730\n",
      "      7        \u001b[36m2.8722\u001b[0m        \u001b[32m3.0482\u001b[0m  0.5655\n",
      "      7        \u001b[36m2.7389\u001b[0m        \u001b[32m3.2074\u001b[0m  0.5661\n",
      "      7        \u001b[36m2.5538\u001b[0m        \u001b[32m2.9131\u001b[0m  0.5566\n",
      "      8        2.4036        2.8518  0.5581\n",
      "      8        2.4932        \u001b[32m2.3529\u001b[0m  0.5443\n",
      "      8        2.8805        \u001b[32m3.0434\u001b[0m  0.5605\n",
      "      8        \u001b[36m2.7187\u001b[0m        \u001b[32m3.1959\u001b[0m  0.5681\n",
      "      8        2.5608        \u001b[32m2.9057\u001b[0m  0.5485\n",
      "      9        2.4136        2.8525  0.6024\n",
      "      9        \u001b[36m2.4497\u001b[0m        \u001b[32m2.3353\u001b[0m  0.5897\n",
      "      9        2.8729        \u001b[32m3.0418\u001b[0m  0.6224\n",
      "      9        2.7310        \u001b[32m3.1789\u001b[0m  0.6219\n",
      "      9        \u001b[36m2.5501\u001b[0m        2.9066  0.6134\n",
      "     10        2.4004        2.8519  0.6116\n",
      "     10        \u001b[36m2.4332\u001b[0m        \u001b[32m2.3187\u001b[0m  0.6026\n",
      "Restoring best model from epoch 1.\n",
      "     10        \u001b[36m2.8672\u001b[0m        \u001b[32m3.0375\u001b[0m  0.6284\n",
      "     10        \u001b[36m2.5086\u001b[0m        2.9151  0.6530\n",
      "     10        \u001b[36m2.7106\u001b[0m        \u001b[32m3.1644\u001b[0m  0.6783\n",
      "Restoring best model from epoch 8.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.1978\u001b[0m  0.0732\n",
      "      2        3.1978  0.0258\n",
      "      3        3.1978  0.0283\n",
      "      4        3.1978  0.0286\n",
      "      5        3.1978  0.0477\n",
      "      6        3.1978  0.0217\n",
      "      7        3.1978  0.0186\n",
      "      8        3.1978  0.0354\n",
      "      9        3.1978  0.0192\n",
      "     10        3.1978  0.0485\n",
      "Restoring best model from epoch 1.\n",
      "y_train breslow final [-3.000e+00 -3.000e+00 -7.000e+00 -1.100e+01 -1.300e+01 -1.600e+01\n",
      " -1.600e+01 -1.600e+01 -1.800e+01 -2.900e+01 -3.600e+01 -3.800e+01\n",
      "  4.100e+01  4.200e+01 -4.300e+01  5.100e+01 -5.300e+01  5.900e+01\n",
      " -6.100e+01  6.200e+01  6.500e+01  6.800e+01  6.900e+01  7.300e+01\n",
      "  7.700e+01  9.300e+01  9.900e+01  1.010e+02  1.060e+02  1.090e+02\n",
      "  1.100e+02 -1.180e+02 -1.190e+02 -1.270e+02  1.390e+02 -1.410e+02\n",
      " -1.500e+02 -1.550e+02  1.620e+02  1.640e+02 -1.660e+02  1.660e+02\n",
      "  1.680e+02 -1.750e+02 -1.770e+02 -1.820e+02  1.820e+02  1.830e+02\n",
      " -1.930e+02 -1.940e+02  2.020e+02  2.040e+02 -2.040e+02 -2.050e+02\n",
      "  2.060e+02 -2.060e+02  2.110e+02 -2.110e+02  2.220e+02  2.240e+02\n",
      "  2.380e+02  2.420e+02 -2.560e+02 -2.930e+02  3.110e+02  3.130e+02\n",
      "  3.130e+02 -3.190e+02  3.200e+02 -3.230e+02  3.300e+02  3.330e+02\n",
      " -3.400e+02  3.420e+02  3.440e+02 -3.540e+02 -3.550e+02 -3.560e+02\n",
      "  3.620e+02 -3.650e+02 -3.650e+02 -3.690e+02 -3.720e+02 -3.730e+02\n",
      " -3.850e+02 -4.000e+02 -4.060e+02 -4.080e+02  4.310e+02 -4.310e+02\n",
      " -4.330e+02 -4.350e+02  4.450e+02  4.460e+02  4.540e+02 -4.540e+02\n",
      "  4.590e+02 -4.680e+02 -4.690e+02  4.750e+02  4.800e+02  4.800e+02\n",
      " -4.950e+02 -4.980e+02  5.100e+02 -5.110e+02 -5.230e+02 -5.280e+02\n",
      " -5.510e+02  5.520e+02  5.610e+02  5.610e+02  5.620e+02  5.630e+02\n",
      " -5.630e+02  5.710e+02  5.720e+02  5.740e+02  5.780e+02 -5.910e+02\n",
      "  6.000e+02 -6.030e+02 -6.070e+02 -6.170e+02 -6.200e+02 -6.300e+02\n",
      "  6.450e+02  6.460e+02 -6.480e+02 -6.560e+02 -6.650e+02  6.790e+02\n",
      "  6.830e+02 -6.850e+02 -6.890e+02 -6.930e+02 -6.930e+02 -7.000e+02\n",
      "  7.010e+02  7.090e+02 -7.140e+02  7.220e+02 -7.220e+02  7.270e+02\n",
      "  7.350e+02 -7.350e+02 -7.370e+02 -7.490e+02 -7.500e+02 -7.550e+02\n",
      " -7.620e+02  7.680e+02  7.700e+02 -7.740e+02 -7.770e+02  7.820e+02\n",
      " -7.850e+02 -7.880e+02  8.190e+02  8.220e+02 -8.220e+02  8.280e+02\n",
      " -8.300e+02  8.340e+02 -8.400e+02  8.410e+02  8.450e+02 -8.610e+02\n",
      "  8.660e+02 -8.730e+02 -8.740e+02  8.850e+02  8.850e+02 -9.090e+02\n",
      " -9.100e+02 -9.190e+02  9.270e+02 -9.320e+02 -9.320e+02 -9.450e+02\n",
      "  9.460e+02 -9.510e+02  9.520e+02 -9.520e+02  9.530e+02 -9.630e+02\n",
      " -9.700e+02 -9.720e+02 -9.920e+02  9.920e+02  1.003e+03 -1.011e+03\n",
      " -1.014e+03 -1.018e+03  1.019e+03  1.034e+03 -1.046e+03 -1.063e+03\n",
      " -1.071e+03  1.075e+03  1.091e+03 -1.092e+03  1.092e+03  1.097e+03\n",
      " -1.106e+03 -1.107e+03  1.111e+03 -1.120e+03  1.121e+03 -1.124e+03\n",
      " -1.130e+03 -1.130e+03 -1.132e+03 -1.133e+03 -1.143e+03 -1.165e+03\n",
      " -1.168e+03 -1.169e+03  1.170e+03 -1.175e+03 -1.177e+03  1.191e+03\n",
      "  1.200e+03  1.200e+03 -1.217e+03 -1.218e+03  1.230e+03 -1.238e+03\n",
      "  1.238e+03 -1.266e+03  1.270e+03 -1.274e+03 -1.290e+03 -1.291e+03\n",
      " -1.299e+03 -1.307e+03 -1.308e+03 -1.314e+03  1.317e+03 -1.329e+03\n",
      "  1.337e+03 -1.355e+03 -1.367e+03  1.371e+03 -1.373e+03  1.378e+03\n",
      " -1.380e+03 -1.384e+03 -1.385e+03  1.404e+03 -1.411e+03 -1.413e+03\n",
      " -1.413e+03 -1.423e+03 -1.433e+03 -1.436e+03 -1.450e+03 -1.459e+03\n",
      " -1.462e+03  1.463e+03 -1.471e+03 -1.476e+03 -1.478e+03 -1.485e+03\n",
      " -1.487e+03 -1.489e+03 -1.491e+03  1.493e+03 -1.495e+03 -1.498e+03\n",
      " -1.499e+03 -1.508e+03 -1.520e+03 -1.520e+03 -1.521e+03 -1.525e+03\n",
      " -1.525e+03 -1.528e+03 -1.552e+03 -1.559e+03 -1.560e+03  1.567e+03\n",
      "  1.584e+03  1.589e+03  1.590e+03  1.598e+03 -1.604e+03 -1.608e+03\n",
      "  1.620e+03 -1.624e+03  1.626e+03 -1.632e+03  1.639e+03  1.657e+03\n",
      "  1.661e+03 -1.666e+03 -1.670e+03 -1.683e+03  1.714e+03  1.724e+03\n",
      " -1.731e+03 -1.733e+03 -1.746e+03 -1.755e+03 -1.759e+03 -1.779e+03\n",
      " -1.782e+03 -1.787e+03 -1.789e+03 -1.790e+03 -1.793e+03 -1.794e+03\n",
      " -1.834e+03 -1.843e+03 -1.854e+03 -1.862e+03 -1.871e+03 -1.876e+03\n",
      " -1.883e+03 -1.885e+03 -1.886e+03 -1.888e+03 -1.889e+03 -1.893e+03\n",
      " -1.905e+03 -1.906e+03  1.912e+03  1.912e+03  1.913e+03 -1.929e+03\n",
      " -1.935e+03 -1.946e+03 -1.952e+03 -1.955e+03  1.980e+03  1.986e+03\n",
      " -1.993e+03 -1.997e+03 -2.004e+03 -2.009e+03 -2.014e+03 -2.016e+03\n",
      " -2.017e+03 -2.038e+03 -2.067e+03 -2.080e+03 -2.087e+03  2.105e+03\n",
      " -2.128e+03  2.145e+03 -2.150e+03 -2.172e+03 -2.184e+03 -2.217e+03\n",
      " -2.226e+03  2.227e+03  2.256e+03 -2.259e+03 -2.270e+03 -2.271e+03\n",
      " -2.274e+03  2.299e+03  2.343e+03 -2.361e+03 -2.372e+03  2.386e+03\n",
      " -2.412e+03  2.419e+03 -2.422e+03 -2.423e+03 -2.430e+03 -2.439e+03\n",
      "  2.454e+03 -2.461e+03 -2.470e+03 -2.489e+03 -2.508e+03 -2.531e+03\n",
      " -2.552e+03  2.601e+03 -2.609e+03 -2.630e+03 -2.660e+03 -2.688e+03\n",
      " -2.718e+03 -2.746e+03  2.752e+03 -2.754e+03  2.764e+03 -2.789e+03\n",
      " -2.799e+03  2.830e+03 -2.839e+03 -2.859e+03 -2.865e+03 -2.881e+03\n",
      " -3.037e+03 -3.146e+03 -3.205e+03 -3.222e+03 -3.229e+03 -3.267e+03\n",
      " -3.271e+03 -3.328e+03 -3.331e+03 -3.343e+03 -3.409e+03 -3.431e+03\n",
      " -3.480e+03 -3.498e+03  3.554e+03 -3.583e+03  3.615e+03 -3.631e+03\n",
      " -3.728e+03 -3.736e+03 -3.744e+03 -3.841e+03 -3.936e+03 -3.974e+03\n",
      " -3.987e+03 -3.989e+03 -4.067e+03 -4.074e+03 -4.537e+03]\n",
      "Concordance Index 0.5359991045694938\n",
      "Integrated Brier Score: 0.20783890501597688\n",
      "y_train breslow final [-3.000e+00 -3.000e+00 -7.000e+00 -1.100e+01 -1.300e+01 -1.600e+01\n",
      " -1.600e+01 -1.600e+01 -1.800e+01 -2.900e+01 -3.600e+01 -3.800e+01\n",
      "  4.100e+01  4.200e+01 -4.300e+01  5.100e+01 -5.300e+01  5.900e+01\n",
      " -6.100e+01  6.200e+01  6.500e+01  6.800e+01  6.900e+01  7.300e+01\n",
      "  7.700e+01  9.300e+01  9.900e+01  1.010e+02  1.060e+02  1.090e+02\n",
      "  1.100e+02 -1.180e+02 -1.190e+02 -1.270e+02  1.390e+02 -1.410e+02\n",
      " -1.500e+02 -1.550e+02  1.620e+02  1.640e+02 -1.660e+02  1.660e+02\n",
      "  1.680e+02 -1.750e+02 -1.770e+02 -1.820e+02  1.820e+02  1.830e+02\n",
      " -1.930e+02 -1.940e+02  2.020e+02  2.040e+02 -2.040e+02 -2.050e+02\n",
      "  2.060e+02 -2.060e+02  2.110e+02 -2.110e+02  2.220e+02  2.240e+02\n",
      "  2.380e+02  2.420e+02 -2.560e+02 -2.930e+02  3.110e+02  3.130e+02\n",
      "  3.130e+02 -3.190e+02  3.200e+02 -3.230e+02  3.300e+02  3.330e+02\n",
      " -3.400e+02  3.420e+02  3.440e+02 -3.540e+02 -3.550e+02 -3.560e+02\n",
      "  3.620e+02 -3.650e+02 -3.650e+02 -3.690e+02 -3.720e+02 -3.730e+02\n",
      " -3.850e+02 -4.000e+02 -4.060e+02 -4.080e+02  4.310e+02 -4.310e+02\n",
      " -4.330e+02 -4.350e+02  4.450e+02  4.460e+02  4.540e+02 -4.540e+02\n",
      "  4.590e+02 -4.680e+02 -4.690e+02  4.750e+02  4.800e+02  4.800e+02\n",
      " -4.950e+02 -4.980e+02  5.100e+02 -5.110e+02 -5.230e+02 -5.280e+02\n",
      " -5.510e+02  5.520e+02  5.610e+02  5.610e+02  5.620e+02  5.630e+02\n",
      " -5.630e+02  5.710e+02  5.720e+02  5.740e+02  5.780e+02 -5.910e+02\n",
      "  6.000e+02 -6.030e+02 -6.070e+02 -6.170e+02 -6.200e+02 -6.300e+02\n",
      "  6.450e+02  6.460e+02 -6.480e+02 -6.560e+02 -6.650e+02  6.790e+02\n",
      "  6.830e+02 -6.850e+02 -6.890e+02 -6.930e+02 -6.930e+02 -7.000e+02\n",
      "  7.010e+02  7.090e+02 -7.140e+02  7.220e+02 -7.220e+02  7.270e+02\n",
      "  7.350e+02 -7.350e+02 -7.370e+02 -7.490e+02 -7.500e+02 -7.550e+02\n",
      " -7.620e+02  7.680e+02  7.700e+02 -7.740e+02 -7.770e+02  7.820e+02\n",
      " -7.850e+02 -7.880e+02  8.190e+02  8.220e+02 -8.220e+02  8.280e+02\n",
      " -8.300e+02  8.340e+02 -8.400e+02  8.410e+02  8.450e+02 -8.610e+02\n",
      "  8.660e+02 -8.730e+02 -8.740e+02  8.850e+02  8.850e+02 -9.090e+02\n",
      " -9.100e+02 -9.190e+02  9.270e+02 -9.320e+02 -9.320e+02 -9.450e+02\n",
      "  9.460e+02 -9.510e+02  9.520e+02 -9.520e+02  9.530e+02 -9.630e+02\n",
      " -9.700e+02 -9.720e+02 -9.920e+02  9.920e+02  1.003e+03 -1.011e+03\n",
      " -1.014e+03 -1.018e+03  1.019e+03  1.034e+03 -1.046e+03 -1.063e+03\n",
      " -1.071e+03  1.075e+03  1.091e+03 -1.092e+03  1.092e+03  1.097e+03\n",
      " -1.106e+03 -1.107e+03  1.111e+03 -1.120e+03  1.121e+03 -1.124e+03\n",
      " -1.130e+03 -1.130e+03 -1.132e+03 -1.133e+03 -1.143e+03 -1.165e+03\n",
      " -1.168e+03 -1.169e+03  1.170e+03 -1.175e+03 -1.177e+03  1.191e+03\n",
      "  1.200e+03  1.200e+03 -1.217e+03 -1.218e+03  1.230e+03 -1.238e+03\n",
      "  1.238e+03 -1.266e+03  1.270e+03 -1.274e+03 -1.290e+03 -1.291e+03\n",
      " -1.299e+03 -1.307e+03 -1.308e+03 -1.314e+03  1.317e+03 -1.329e+03\n",
      "  1.337e+03 -1.355e+03 -1.367e+03  1.371e+03 -1.373e+03  1.378e+03\n",
      " -1.380e+03 -1.384e+03 -1.385e+03  1.404e+03 -1.411e+03 -1.413e+03\n",
      " -1.413e+03 -1.423e+03 -1.433e+03 -1.436e+03 -1.450e+03 -1.459e+03\n",
      " -1.462e+03  1.463e+03 -1.471e+03 -1.476e+03 -1.478e+03 -1.485e+03\n",
      " -1.487e+03 -1.489e+03 -1.491e+03  1.493e+03 -1.495e+03 -1.498e+03\n",
      " -1.499e+03 -1.508e+03 -1.520e+03 -1.520e+03 -1.521e+03 -1.525e+03\n",
      " -1.525e+03 -1.528e+03 -1.552e+03 -1.559e+03 -1.560e+03  1.567e+03\n",
      "  1.584e+03  1.589e+03  1.590e+03  1.598e+03 -1.604e+03 -1.608e+03\n",
      "  1.620e+03 -1.624e+03  1.626e+03 -1.632e+03  1.639e+03  1.657e+03\n",
      "  1.661e+03 -1.666e+03 -1.670e+03 -1.683e+03  1.714e+03  1.724e+03\n",
      " -1.731e+03 -1.733e+03 -1.746e+03 -1.755e+03 -1.759e+03 -1.779e+03\n",
      " -1.782e+03 -1.787e+03 -1.789e+03 -1.790e+03 -1.793e+03 -1.794e+03\n",
      " -1.834e+03 -1.843e+03 -1.854e+03 -1.862e+03 -1.871e+03 -1.876e+03\n",
      " -1.883e+03 -1.885e+03 -1.886e+03 -1.888e+03 -1.889e+03 -1.893e+03\n",
      " -1.905e+03 -1.906e+03  1.912e+03  1.912e+03  1.913e+03 -1.929e+03\n",
      " -1.935e+03 -1.946e+03 -1.952e+03 -1.955e+03  1.980e+03  1.986e+03\n",
      " -1.993e+03 -1.997e+03 -2.004e+03 -2.009e+03 -2.014e+03 -2.016e+03\n",
      " -2.017e+03 -2.038e+03 -2.067e+03 -2.080e+03 -2.087e+03  2.105e+03\n",
      " -2.128e+03  2.145e+03 -2.150e+03 -2.172e+03 -2.184e+03 -2.217e+03\n",
      " -2.226e+03  2.227e+03  2.256e+03 -2.259e+03 -2.270e+03 -2.271e+03\n",
      " -2.274e+03  2.299e+03  2.343e+03 -2.361e+03 -2.372e+03  2.386e+03\n",
      " -2.412e+03  2.419e+03 -2.422e+03 -2.423e+03 -2.430e+03 -2.439e+03\n",
      "  2.454e+03 -2.461e+03 -2.470e+03 -2.489e+03 -2.508e+03 -2.531e+03\n",
      " -2.552e+03  2.601e+03 -2.609e+03 -2.630e+03 -2.660e+03 -2.688e+03\n",
      " -2.718e+03 -2.746e+03  2.752e+03 -2.754e+03  2.764e+03 -2.789e+03\n",
      " -2.799e+03  2.830e+03 -2.839e+03 -2.859e+03 -2.865e+03 -2.881e+03\n",
      " -3.037e+03 -3.146e+03 -3.205e+03 -3.222e+03 -3.229e+03 -3.267e+03\n",
      " -3.271e+03 -3.328e+03 -3.331e+03 -3.343e+03 -3.409e+03 -3.431e+03\n",
      " -3.480e+03 -3.498e+03  3.554e+03 -3.583e+03  3.615e+03 -3.631e+03\n",
      " -3.728e+03 -3.736e+03 -3.744e+03 -3.841e+03 -3.936e+03 -3.974e+03\n",
      " -3.987e+03 -3.989e+03 -4.067e+03 -4.074e+03 -4.537e+03]\n",
      "durations 2.0 3944.0\n",
      "Concordance Index 0.5588830433023068\n",
      "Integrated Brier Score: 0.19577041336897816\n",
      "split gex_?|100130426      float32\n",
      "gex_?|100133144      float32\n",
      "gex_?|100134869      float32\n",
      "gex_?|10357          float32\n",
      "gex_?|10431          float32\n",
      "                      ...   \n",
      "gex_ZYG11A|440590    float32\n",
      "gex_ZYG11B|79699     float32\n",
      "gex_ZYX|7791         float32\n",
      "gex_ZZEF1|23140      float32\n",
      "gex_ZZZ3|26009       float32\n",
      "Length: 20531, dtype: object\n",
      "(408, 20531) <class 'pandas.core.frame.DataFrame'>\n",
      "(408,) <class 'pandas.core.series.Series'>\n",
      "(103, 20531) <class 'pandas.core.frame.DataFrame'>\n",
      "(103,) <class 'pandas.core.series.Series'>\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m2.1414\u001b[0m  0.0495\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      2        2.1414  0.0360\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      3        2.1414  0.0605\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      4        2.1414  0.0532\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m1.2560\u001b[0m  0.0554\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m2.5923\u001b[0m  0.0557\n",
      "      5        2.1414  0.0405\n",
      "      2        1.2560  0.0348\n",
      "      2        2.5923  0.0386\n",
      "      6        2.1414  0.0355\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      3        1.2560  0.0411\n",
      "      3        2.5923  0.0319\n",
      "      7        2.1414  0.0321\n",
      "      4        2.5923  0.0366\n",
      "      4        1.2560  0.0416\n",
      "      8        2.1414  0.0334\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      5        1.2560  0.0315\n",
      "      5        2.5923  0.0331\n",
      "      9        2.1414  0.0345\n",
      "      6        1.2560  0.0438\n",
      "     10        2.1414  0.0357\n",
      "Restoring best model from epoch 1.\n",
      "      6        2.5923  0.0481\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      7        1.2560  0.0348\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m1.2084\u001b[0m  0.0540\n",
      "      7        2.5923  0.0407\n",
      "      8        1.2560  0.0428\n",
      "      2        1.2084  0.0401\n",
      "      8        2.5923  0.0393\n",
      "      9        1.2560  0.0398\n",
      "      9        2.5923  0.0361\n",
      "      3        1.2084  0.0374\n",
      "     10        1.2560  0.0367\n",
      "      4        1.2084  0.0345\n",
      "Restoring best model from epoch 1.\n",
      "     10        2.5923  0.0412\n",
      "Restoring best model from epoch 1.\n",
      "      5        1.2084  0.0589\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m2.0326\u001b[0m  0.0400\n",
      "      6        1.2084  0.0381\n",
      "      2        2.0326  0.0486\n",
      "      7        1.2084  0.0354\n",
      "      3        2.0326  0.0301\n",
      "      8        1.2084  0.0300\n",
      "      4        2.0326  0.0379\n",
      "      9        1.2084  0.0308\n",
      "      5        2.0326  0.0301\n",
      "     10        1.2084  0.0441\n",
      "Restoring best model from epoch 1.\n",
      "      6        2.0326  0.0380\n",
      "      7        2.0326  0.0299\n",
      "      8        2.0326  0.0284\n",
      "      9        2.0326  0.0304\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.2196\u001b[0m        \u001b[32m1.2053\u001b[0m  0.8854\n",
      "     10        2.0326  0.0354\n",
      "Restoring best model from epoch 1.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.0265\u001b[0m        \u001b[32m2.1328\u001b[0m  0.9478\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m1.9454\u001b[0m        \u001b[32m2.5977\u001b[0m  0.9723\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m1.8753\u001b[0m        \u001b[32m1.2496\u001b[0m  0.8437\n",
      "      2        \u001b[36m2.2013\u001b[0m        \u001b[32m1.2009\u001b[0m  0.6616\n",
      "      2        2.0271        2.1356  0.6484\n",
      "      2        \u001b[36m1.8694\u001b[0m        1.2508  0.7517\n",
      "      2        1.9559        \u001b[32m2.5957\u001b[0m  0.7843\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.1909\u001b[0m        \u001b[32m2.0288\u001b[0m  1.6813\n",
      "      3        2.2121        \u001b[32m1.1985\u001b[0m  0.5614\n",
      "      3        \u001b[36m2.0260\u001b[0m        2.1372  0.5715\n",
      "      3        \u001b[36m1.8597\u001b[0m        1.2513  0.5775\n",
      "      3        \u001b[36m1.9326\u001b[0m        \u001b[32m2.5942\u001b[0m  0.5736\n",
      "      2        \u001b[36m2.1852\u001b[0m        \u001b[32m2.0253\u001b[0m  0.6367\n",
      "      4        2.2101        \u001b[32m1.1982\u001b[0m  0.5956\n",
      "      4        \u001b[36m2.0203\u001b[0m        2.1371  0.5866\n",
      "      4        \u001b[36m1.8412\u001b[0m        \u001b[32m1.2477\u001b[0m  0.5764\n",
      "      4        1.9422        2.5982  0.5987\n",
      "      3        \u001b[36m2.1831\u001b[0m        2.0267  0.5772\n",
      "      5        2.2112        1.1994  0.6183\n",
      "      5        \u001b[36m2.0174\u001b[0m        2.1388  0.5955\n",
      "      5        1.8538        \u001b[32m1.2451\u001b[0m  0.6428\n",
      "      5        1.9487        2.5967  0.6091\n",
      "      6        2.2036        1.1991  0.6704\n",
      "      4        \u001b[36m2.1741\u001b[0m        2.0277  0.7900\n",
      "      6        2.0253        2.1401  0.7223\n",
      "      6        1.8572        \u001b[32m1.2362\u001b[0m  0.6899\n",
      "      6        1.9538        \u001b[32m2.5900\u001b[0m  0.7569\n",
      "      7        \u001b[36m2.2002\u001b[0m        \u001b[32m1.1967\u001b[0m  0.5754\n",
      "      5        \u001b[36m2.1736\u001b[0m        2.0329  0.5871\n",
      "      7        2.0365        2.1407  0.5807\n",
      "      7        1.8572        \u001b[32m1.2230\u001b[0m  0.5759\n",
      "      7        1.9389        \u001b[32m2.5853\u001b[0m  0.6086\n",
      "      8        2.2081        \u001b[32m1.1841\u001b[0m  0.5911\n",
      "      6        2.1769        2.0415  0.5745\n",
      "      8        2.0275        2.1407  0.5461\n",
      "      8        1.8530        \u001b[32m1.2079\u001b[0m  0.5777\n",
      "      8        \u001b[36m1.9290\u001b[0m        \u001b[32m2.5830\u001b[0m  0.6104\n",
      "      9        \u001b[36m2.0141\u001b[0m        2.1405  0.6829\n",
      "      9        2.2046        \u001b[32m1.1665\u001b[0m  0.7293\n",
      "      7        2.1739        2.0474  0.8658\n",
      "      9        \u001b[36m1.8399\u001b[0m        \u001b[32m1.1918\u001b[0m  0.8386\n",
      "      9        1.9487        \u001b[32m2.5779\u001b[0m  0.8092\n",
      "     10        2.0148        2.1387  0.6145\n",
      "Restoring best model from epoch 1.\n",
      "     10        \u001b[36m2.1789\u001b[0m        \u001b[32m1.1431\u001b[0m  0.6255\n",
      "      8        2.1804        2.0490  0.5702\n",
      "     10        \u001b[36m1.8282\u001b[0m        \u001b[32m1.1664\u001b[0m  0.5477\n",
      "     10        \u001b[36m1.9232\u001b[0m        \u001b[32m2.5721\u001b[0m  0.5370\n",
      "      9        \u001b[36m2.1700\u001b[0m        2.0480  0.5591\n",
      "     10        2.1772        2.0502  0.5105\n",
      "Restoring best model from epoch 2.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m1.3812\u001b[0m  0.0500\n",
      "      2        1.3812  0.0175\n",
      "      3        1.3812  0.0164\n",
      "      4        1.3812  0.0160\n",
      "      5        1.3812  0.0208\n",
      "      6        1.3812  0.0409\n",
      "      7        1.3812  0.0146\n",
      "      8        1.3812  0.0445\n",
      "      9        1.3812  0.0195\n",
      "     10        1.3812  0.0206\n",
      "Restoring best model from epoch 1.\n",
      "y_train breslow final [-1.000e+00 -2.000e+00 -3.000e+00 -3.000e+00 -3.000e+00 -3.000e+00\n",
      " -3.000e+00 -3.000e+00 -3.000e+00 -4.000e+00 -4.000e+00 -5.000e+00\n",
      " -6.000e+00 -7.000e+00 -7.000e+00 -7.000e+00 -7.000e+00  7.000e+00\n",
      " -8.000e+00 -1.400e+01 -1.700e+01  2.300e+01 -2.300e+01  3.700e+01\n",
      " -3.900e+01 -5.500e+01 -6.200e+01 -6.300e+01 -7.000e+01 -7.200e+01\n",
      " -7.300e+01 -7.400e+01 -8.200e+01 -8.400e+01 -9.000e+01  9.600e+01\n",
      " -1.050e+02  1.110e+02 -1.120e+02  1.130e+02 -1.390e+02 -1.530e+02\n",
      " -1.620e+02 -1.660e+02 -1.740e+02  1.780e+02 -1.790e+02 -1.820e+02\n",
      " -1.840e+02 -1.900e+02 -1.940e+02 -1.940e+02  1.940e+02  1.990e+02\n",
      " -1.990e+02 -2.030e+02  2.050e+02 -2.100e+02  2.140e+02  2.280e+02\n",
      " -2.300e+02 -2.310e+02  2.340e+02  2.400e+02  2.410e+02 -2.420e+02\n",
      " -2.430e+02  2.450e+02  2.690e+02 -2.740e+02 -2.790e+02 -2.860e+02\n",
      " -2.920e+02 -3.010e+02 -3.130e+02  3.160e+02 -3.170e+02 -3.200e+02\n",
      " -3.260e+02 -3.280e+02 -3.330e+02 -3.360e+02 -3.370e+02 -3.420e+02\n",
      " -3.430e+02  3.470e+02  3.490e+02 -3.540e+02 -3.720e+02  3.720e+02\n",
      " -3.720e+02  3.780e+02  3.980e+02 -4.030e+02 -4.080e+02 -4.110e+02\n",
      " -4.140e+02 -4.140e+02 -4.160e+02 -4.170e+02 -4.180e+02 -4.190e+02\n",
      " -4.280e+02 -4.310e+02 -4.330e+02 -4.340e+02  4.350e+02  4.380e+02\n",
      " -4.380e+02 -4.380e+02 -4.420e+02 -4.430e+02 -4.490e+02 -4.510e+02\n",
      " -4.540e+02 -4.540e+02 -4.550e+02 -4.550e+02  4.560e+02 -4.570e+02\n",
      " -4.580e+02 -4.610e+02 -4.610e+02 -4.670e+02 -4.670e+02 -4.710e+02\n",
      " -4.780e+02 -4.820e+02 -4.860e+02 -4.870e+02 -4.910e+02  4.920e+02\n",
      " -4.920e+02 -4.940e+02 -4.940e+02 -5.000e+02 -5.020e+02 -5.080e+02\n",
      " -5.090e+02  5.120e+02 -5.150e+02 -5.160e+02 -5.220e+02 -5.230e+02\n",
      " -5.230e+02 -5.260e+02 -5.300e+02  5.310e+02 -5.310e+02 -5.320e+02\n",
      " -5.330e+02 -5.330e+02  5.370e+02  5.380e+02 -5.420e+02 -5.440e+02\n",
      " -5.440e+02  5.470e+02  5.590e+02  5.610e+02 -5.630e+02 -5.640e+02\n",
      " -5.660e+02 -5.660e+02 -5.660e+02 -5.670e+02 -5.710e+02 -5.710e+02\n",
      "  5.760e+02 -5.760e+02  5.780e+02 -5.820e+02 -5.840e+02 -5.850e+02\n",
      " -5.880e+02 -5.960e+02 -5.990e+02 -6.040e+02  6.050e+02  6.070e+02\n",
      " -6.080e+02 -6.090e+02 -6.090e+02 -6.110e+02 -6.110e+02 -6.150e+02\n",
      " -6.220e+02 -6.230e+02 -6.270e+02 -6.290e+02 -6.290e+02 -6.300e+02\n",
      " -6.330e+02  6.390e+02  6.480e+02  6.480e+02 -6.510e+02 -6.510e+02\n",
      " -6.550e+02 -6.560e+02 -6.710e+02 -6.770e+02 -6.780e+02  6.820e+02\n",
      " -6.850e+02 -6.860e+02 -7.050e+02 -7.060e+02 -7.060e+02 -7.140e+02\n",
      " -7.150e+02 -7.180e+02 -7.210e+02  7.220e+02 -7.240e+02  7.270e+02\n",
      " -7.350e+02 -7.360e+02 -7.380e+02  7.420e+02 -7.480e+02 -7.540e+02\n",
      "  7.580e+02 -7.580e+02 -7.600e+02  7.750e+02  7.750e+02 -7.770e+02\n",
      " -7.850e+02 -7.870e+02  7.880e+02 -7.920e+02 -7.950e+02 -7.960e+02\n",
      " -8.000e+02  8.140e+02  8.140e+02  8.190e+02  8.210e+02 -8.260e+02\n",
      " -8.350e+02 -8.370e+02 -8.460e+02 -8.460e+02 -8.540e+02 -8.540e+02\n",
      " -8.600e+02 -8.620e+02 -8.680e+02  8.860e+02 -8.890e+02 -9.000e+02\n",
      " -9.080e+02 -9.080e+02 -9.140e+02  9.190e+02 -9.260e+02  9.330e+02\n",
      " -9.330e+02 -9.350e+02 -9.390e+02 -9.460e+02 -9.490e+02  9.540e+02\n",
      " -9.560e+02  9.610e+02 -9.620e+02  9.620e+02 -9.640e+02 -9.640e+02\n",
      "  9.840e+02  9.870e+02 -9.920e+02 -9.920e+02 -9.930e+02 -1.004e+03\n",
      "  1.011e+03 -1.021e+03 -1.026e+03 -1.032e+03  1.033e+03 -1.040e+03\n",
      " -1.058e+03 -1.069e+03 -1.076e+03 -1.078e+03 -1.079e+03  1.106e+03\n",
      " -1.116e+03 -1.120e+03  1.120e+03  1.120e+03 -1.127e+03  1.137e+03\n",
      " -1.137e+03 -1.139e+03 -1.147e+03  1.152e+03 -1.164e+03 -1.173e+03\n",
      "  1.183e+03 -1.189e+03 -1.191e+03 -1.201e+03  1.209e+03 -1.210e+03\n",
      " -1.222e+03 -1.229e+03 -1.236e+03 -1.245e+03 -1.250e+03 -1.257e+03\n",
      " -1.277e+03 -1.279e+03 -1.294e+03 -1.294e+03 -1.300e+03 -1.301e+03\n",
      " -1.314e+03 -1.320e+03  1.335e+03 -1.337e+03  1.339e+03 -1.341e+03\n",
      "  1.351e+03 -1.354e+03 -1.359e+03 -1.387e+03 -1.393e+03 -1.397e+03\n",
      " -1.399e+03  1.401e+03 -1.401e+03 -1.426e+03 -1.428e+03 -1.458e+03\n",
      " -1.469e+03  1.481e+03 -1.494e+03 -1.500e+03 -1.540e+03  1.547e+03\n",
      " -1.553e+03 -1.567e+03 -1.568e+03 -1.588e+03 -1.631e+03 -1.650e+03\n",
      "  1.666e+03 -1.706e+03 -1.721e+03 -1.752e+03  1.762e+03 -1.796e+03\n",
      " -1.834e+03 -1.868e+03  1.886e+03  1.891e+03  1.915e+03  1.933e+03\n",
      " -1.943e+03 -1.989e+03  2.000e+03 -2.078e+03 -2.219e+03  2.235e+03\n",
      "  2.282e+03 -2.287e+03 -2.289e+03  2.379e+03 -2.381e+03  2.433e+03\n",
      " -2.493e+03 -2.565e+03 -2.585e+03 -2.602e+03 -2.650e+03 -2.702e+03\n",
      " -2.761e+03 -2.772e+03  2.835e+03 -2.860e+03 -2.869e+03  2.875e+03\n",
      " -2.893e+03  2.907e+03 -2.918e+03 -2.918e+03  2.988e+03 -3.000e+03\n",
      " -3.013e+03  3.200e+03 -3.253e+03  3.571e+03 -3.574e+03 -3.725e+03\n",
      " -3.761e+03  3.978e+03  4.068e+03  4.084e+03  4.229e+03  4.412e+03\n",
      "  4.445e+03 -4.752e+03  5.166e+03 -5.255e+03 -5.546e+03 -6.423e+03]\n",
      "Concordance Index 0.680024320141499\n",
      "Integrated Brier Score: 0.17514138932452922\n",
      "y_train breslow final [-1.000e+00 -2.000e+00 -3.000e+00 -3.000e+00 -3.000e+00 -3.000e+00\n",
      " -3.000e+00 -3.000e+00 -3.000e+00 -4.000e+00 -4.000e+00 -5.000e+00\n",
      " -6.000e+00 -7.000e+00 -7.000e+00 -7.000e+00 -7.000e+00  7.000e+00\n",
      " -8.000e+00 -1.400e+01 -1.700e+01  2.300e+01 -2.300e+01  3.700e+01\n",
      " -3.900e+01 -5.500e+01 -6.200e+01 -6.300e+01 -7.000e+01 -7.200e+01\n",
      " -7.300e+01 -7.400e+01 -8.200e+01 -8.400e+01 -9.000e+01  9.600e+01\n",
      " -1.050e+02  1.110e+02 -1.120e+02  1.130e+02 -1.390e+02 -1.530e+02\n",
      " -1.620e+02 -1.660e+02 -1.740e+02  1.780e+02 -1.790e+02 -1.820e+02\n",
      " -1.840e+02 -1.900e+02 -1.940e+02 -1.940e+02  1.940e+02  1.990e+02\n",
      " -1.990e+02 -2.030e+02  2.050e+02 -2.100e+02  2.140e+02  2.280e+02\n",
      " -2.300e+02 -2.310e+02  2.340e+02  2.400e+02  2.410e+02 -2.420e+02\n",
      " -2.430e+02  2.450e+02  2.690e+02 -2.740e+02 -2.790e+02 -2.860e+02\n",
      " -2.920e+02 -3.010e+02 -3.130e+02  3.160e+02 -3.170e+02 -3.200e+02\n",
      " -3.260e+02 -3.280e+02 -3.330e+02 -3.360e+02 -3.370e+02 -3.420e+02\n",
      " -3.430e+02  3.470e+02  3.490e+02 -3.540e+02 -3.720e+02  3.720e+02\n",
      " -3.720e+02  3.780e+02  3.980e+02 -4.030e+02 -4.080e+02 -4.110e+02\n",
      " -4.140e+02 -4.140e+02 -4.160e+02 -4.170e+02 -4.180e+02 -4.190e+02\n",
      " -4.280e+02 -4.310e+02 -4.330e+02 -4.340e+02  4.350e+02  4.380e+02\n",
      " -4.380e+02 -4.380e+02 -4.420e+02 -4.430e+02 -4.490e+02 -4.510e+02\n",
      " -4.540e+02 -4.540e+02 -4.550e+02 -4.550e+02  4.560e+02 -4.570e+02\n",
      " -4.580e+02 -4.610e+02 -4.610e+02 -4.670e+02 -4.670e+02 -4.710e+02\n",
      " -4.780e+02 -4.820e+02 -4.860e+02 -4.870e+02 -4.910e+02  4.920e+02\n",
      " -4.920e+02 -4.940e+02 -4.940e+02 -5.000e+02 -5.020e+02 -5.080e+02\n",
      " -5.090e+02  5.120e+02 -5.150e+02 -5.160e+02 -5.220e+02 -5.230e+02\n",
      " -5.230e+02 -5.260e+02 -5.300e+02  5.310e+02 -5.310e+02 -5.320e+02\n",
      " -5.330e+02 -5.330e+02  5.370e+02  5.380e+02 -5.420e+02 -5.440e+02\n",
      " -5.440e+02  5.470e+02  5.590e+02  5.610e+02 -5.630e+02 -5.640e+02\n",
      " -5.660e+02 -5.660e+02 -5.660e+02 -5.670e+02 -5.710e+02 -5.710e+02\n",
      "  5.760e+02 -5.760e+02  5.780e+02 -5.820e+02 -5.840e+02 -5.850e+02\n",
      " -5.880e+02 -5.960e+02 -5.990e+02 -6.040e+02  6.050e+02  6.070e+02\n",
      " -6.080e+02 -6.090e+02 -6.090e+02 -6.110e+02 -6.110e+02 -6.150e+02\n",
      " -6.220e+02 -6.230e+02 -6.270e+02 -6.290e+02 -6.290e+02 -6.300e+02\n",
      " -6.330e+02  6.390e+02  6.480e+02  6.480e+02 -6.510e+02 -6.510e+02\n",
      " -6.550e+02 -6.560e+02 -6.710e+02 -6.770e+02 -6.780e+02  6.820e+02\n",
      " -6.850e+02 -6.860e+02 -7.050e+02 -7.060e+02 -7.060e+02 -7.140e+02\n",
      " -7.150e+02 -7.180e+02 -7.210e+02  7.220e+02 -7.240e+02  7.270e+02\n",
      " -7.350e+02 -7.360e+02 -7.380e+02  7.420e+02 -7.480e+02 -7.540e+02\n",
      "  7.580e+02 -7.580e+02 -7.600e+02  7.750e+02  7.750e+02 -7.770e+02\n",
      " -7.850e+02 -7.870e+02  7.880e+02 -7.920e+02 -7.950e+02 -7.960e+02\n",
      " -8.000e+02  8.140e+02  8.140e+02  8.190e+02  8.210e+02 -8.260e+02\n",
      " -8.350e+02 -8.370e+02 -8.460e+02 -8.460e+02 -8.540e+02 -8.540e+02\n",
      " -8.600e+02 -8.620e+02 -8.680e+02  8.860e+02 -8.890e+02 -9.000e+02\n",
      " -9.080e+02 -9.080e+02 -9.140e+02  9.190e+02 -9.260e+02  9.330e+02\n",
      " -9.330e+02 -9.350e+02 -9.390e+02 -9.460e+02 -9.490e+02  9.540e+02\n",
      " -9.560e+02  9.610e+02 -9.620e+02  9.620e+02 -9.640e+02 -9.640e+02\n",
      "  9.840e+02  9.870e+02 -9.920e+02 -9.920e+02 -9.930e+02 -1.004e+03\n",
      "  1.011e+03 -1.021e+03 -1.026e+03 -1.032e+03  1.033e+03 -1.040e+03\n",
      " -1.058e+03 -1.069e+03 -1.076e+03 -1.078e+03 -1.079e+03  1.106e+03\n",
      " -1.116e+03 -1.120e+03  1.120e+03  1.120e+03 -1.127e+03  1.137e+03\n",
      " -1.137e+03 -1.139e+03 -1.147e+03  1.152e+03 -1.164e+03 -1.173e+03\n",
      "  1.183e+03 -1.189e+03 -1.191e+03 -1.201e+03  1.209e+03 -1.210e+03\n",
      " -1.222e+03 -1.229e+03 -1.236e+03 -1.245e+03 -1.250e+03 -1.257e+03\n",
      " -1.277e+03 -1.279e+03 -1.294e+03 -1.294e+03 -1.300e+03 -1.301e+03\n",
      " -1.314e+03 -1.320e+03  1.335e+03 -1.337e+03  1.339e+03 -1.341e+03\n",
      "  1.351e+03 -1.354e+03 -1.359e+03 -1.387e+03 -1.393e+03 -1.397e+03\n",
      " -1.399e+03  1.401e+03 -1.401e+03 -1.426e+03 -1.428e+03 -1.458e+03\n",
      " -1.469e+03  1.481e+03 -1.494e+03 -1.500e+03 -1.540e+03  1.547e+03\n",
      " -1.553e+03 -1.567e+03 -1.568e+03 -1.588e+03 -1.631e+03 -1.650e+03\n",
      "  1.666e+03 -1.706e+03 -1.721e+03 -1.752e+03  1.762e+03 -1.796e+03\n",
      " -1.834e+03 -1.868e+03  1.886e+03  1.891e+03  1.915e+03  1.933e+03\n",
      " -1.943e+03 -1.989e+03  2.000e+03 -2.078e+03 -2.219e+03  2.235e+03\n",
      "  2.282e+03 -2.287e+03 -2.289e+03  2.379e+03 -2.381e+03  2.433e+03\n",
      " -2.493e+03 -2.565e+03 -2.585e+03 -2.602e+03 -2.650e+03 -2.702e+03\n",
      " -2.761e+03 -2.772e+03  2.835e+03 -2.860e+03 -2.869e+03  2.875e+03\n",
      " -2.893e+03  2.907e+03 -2.918e+03 -2.918e+03  2.988e+03 -3.000e+03\n",
      " -3.013e+03  3.200e+03 -3.253e+03  3.571e+03 -3.574e+03 -3.725e+03\n",
      " -3.761e+03  3.978e+03  4.068e+03  4.084e+03  4.229e+03  4.412e+03\n",
      "  4.445e+03 -4.752e+03  5.166e+03 -5.255e+03 -5.546e+03 -6.423e+03]\n",
      "durations 3.0 4695.0\n",
      "Concordance Index 0.6801579466929911\n",
      "Integrated Brier Score: 0.19774841450440683\n",
      "(409, 20531) <class 'pandas.core.frame.DataFrame'>\n",
      "(409,) <class 'pandas.core.series.Series'>\n",
      "(102, 20531) <class 'pandas.core.frame.DataFrame'>\n",
      "(102,) <class 'pandas.core.series.Series'>\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m1.6185\u001b[0m        \u001b[32m1.8508\u001b[0m  0.2611\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.0651\u001b[0m        \u001b[32m2.2895\u001b[0m  0.6112\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m1.9500\u001b[0m        \u001b[32m2.4456\u001b[0m  0.6342\n",
      "      2        \u001b[36m1.5437\u001b[0m        \u001b[32m1.8469\u001b[0m  0.2384\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.1884\u001b[0m        \u001b[32m2.0585\u001b[0m  0.7269\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.1445\u001b[0m        \u001b[32m1.9358\u001b[0m  0.5799\n",
      "      2        \u001b[36m2.0353\u001b[0m        \u001b[32m2.2616\u001b[0m  0.2218\n",
      "      2        \u001b[36m1.8995\u001b[0m        \u001b[32m2.3779\u001b[0m  0.2901\n",
      "      2        \u001b[36m2.1249\u001b[0m        \u001b[32m2.0361\u001b[0m  0.2085\n",
      "      3        \u001b[36m1.5060\u001b[0m        1.8615  0.2442\n",
      "      3        \u001b[36m1.9919\u001b[0m        2.2832  0.2074\n",
      "      2        \u001b[36m2.0813\u001b[0m        \u001b[32m1.8777\u001b[0m  0.2947\n",
      "      3        \u001b[36m1.8582\u001b[0m        2.4227  0.1685\n",
      "      3        \u001b[36m2.0974\u001b[0m        2.0690  0.1597\n",
      "      4        \u001b[36m1.4885\u001b[0m        1.8729  0.1551\n",
      "      4        \u001b[36m1.9806\u001b[0m        2.3098  0.1552\n",
      "      3        \u001b[36m2.0437\u001b[0m        1.9077  0.1587\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.1311\u001b[0m        \u001b[32m1.9236\u001b[0m  1.1479\n",
      "      5        \u001b[36m1.4653\u001b[0m        1.8904  0.1819\n",
      "      4        \u001b[36m2.0819\u001b[0m        2.0872  0.1879\n",
      "      4        \u001b[36m1.8473\u001b[0m        2.4295  0.2018\n",
      "      5        \u001b[36m1.9585\u001b[0m        2.3209  0.1881\n",
      "      4        \u001b[36m2.0272\u001b[0m        1.9413  0.1899\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m1.5725\u001b[0m        \u001b[32m1.7174\u001b[0m  1.3814\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m1.8981\u001b[0m        \u001b[32m2.2587\u001b[0m  1.4073\n",
      "      6        1.4703        1.8905  0.1789\n",
      "      5        \u001b[36m2.0792\u001b[0m        2.0853  0.1934\n",
      "      5        \u001b[36m1.7927\u001b[0m        2.4087  0.1921\n",
      "      6        \u001b[36m1.9534\u001b[0m        2.3451  0.1746\n",
      "      5        \u001b[36m2.0139\u001b[0m        1.9570  0.1627\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.0348\u001b[0m        \u001b[32m2.1449\u001b[0m  1.6395\n",
      "      6        \u001b[36m2.0584\u001b[0m        2.0786  0.1498\n",
      "      7        \u001b[36m1.4527\u001b[0m        1.8887  0.1778\n",
      "      6        \u001b[36m1.7839\u001b[0m        2.4121  0.1550\n",
      "      7        \u001b[36m1.9463\u001b[0m        2.3673  0.1561\n",
      "      6        \u001b[36m1.9926\u001b[0m        1.9663  0.1608\n",
      "      7        \u001b[36m2.0278\u001b[0m        2.0760  0.1586\n",
      "      2        \u001b[36m2.0317\u001b[0m        1.9922  0.6082\n",
      "      7        1.7865        2.4177  0.1623\n",
      "      8        \u001b[36m1.4396\u001b[0m        1.8788  0.1787\n",
      "      8        1.9518        2.3713  0.1836\n",
      "      7        \u001b[36m1.9837\u001b[0m        1.9777  0.1923\n",
      "      8        \u001b[36m1.9896\u001b[0m        2.0695  0.1575\n",
      "      8        \u001b[36m1.7694\u001b[0m        2.4298  0.1533\n",
      "      9        \u001b[36m1.4166\u001b[0m        1.8724  0.1531\n",
      "      2        \u001b[36m1.8279\u001b[0m        2.3015  0.5888\n",
      "      9        \u001b[36m1.9246\u001b[0m        2.3682  0.1700\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.0883\u001b[0m        \u001b[32m1.8920\u001b[0m  2.0155\n",
      "      2        \u001b[36m1.4848\u001b[0m        1.7583  0.6479\n",
      "      8        \u001b[36m1.9713\u001b[0m        1.9766  0.1588\n",
      "      9        \u001b[36m1.9895\u001b[0m        2.0585  0.1588\n",
      "      9        \u001b[36m1.7602\u001b[0m        2.4347  0.1592\n",
      "     10        1.4174        1.8715  0.1521\n",
      "Restoring best model from epoch 2.\n",
      "     10        \u001b[36m1.9076\u001b[0m        2.3647  0.1503\n",
      "Restoring best model from epoch 2.\n",
      "      9        \u001b[36m1.9655\u001b[0m        1.9730  0.1419\n",
      "      2        \u001b[36m1.9565\u001b[0m        2.1787  0.5951\n",
      "     10        2.0010        2.0540  0.1608\n",
      "Restoring best model from epoch 2.\n",
      "     10        \u001b[36m1.7389\u001b[0m        2.4337  0.1503\n",
      "Restoring best model from epoch 2.\n",
      "     10        1.9665        1.9751  0.1499\n",
      "Restoring best model from epoch 2.\n",
      "      3        \u001b[36m2.0065\u001b[0m        1.9997  0.5571\n",
      "      3        \u001b[36m1.7794\u001b[0m        2.2876  0.5090\n",
      "      3        1.4913        1.7549  0.5072\n",
      "      2        \u001b[36m2.0839\u001b[0m        \u001b[32m1.8603\u001b[0m  0.6544\n",
      "      3        \u001b[36m1.9156\u001b[0m        2.2149  0.4815\n",
      "      4        \u001b[36m1.9960\u001b[0m        2.0283  0.4284\n",
      "      4        \u001b[36m1.7494\u001b[0m        2.2805  0.4176\n",
      "      4        \u001b[36m1.4785\u001b[0m        1.7643  0.4089\n",
      "      4        \u001b[36m1.8912\u001b[0m        2.2533  0.4162\n",
      "      3        \u001b[36m2.0758\u001b[0m        1.8835  0.4320\n",
      "      5        \u001b[36m1.9916\u001b[0m        2.0364  0.4223\n",
      "      5        \u001b[36m1.4586\u001b[0m        1.7788  0.4186\n",
      "      5        \u001b[36m1.7297\u001b[0m        2.2913  0.4426\n",
      "      5        \u001b[36m1.8553\u001b[0m        2.2226  0.4041\n",
      "      4        \u001b[36m2.0628\u001b[0m        1.8911  0.4340\n",
      "      6        \u001b[36m1.9590\u001b[0m        2.0325  0.4248\n",
      "      6        \u001b[36m1.4451\u001b[0m        1.7774  0.4013\n",
      "      6        \u001b[36m1.7184\u001b[0m        2.3004  0.4409\n",
      "      6        \u001b[36m1.8488\u001b[0m        2.2165  0.4031\n",
      "      5        \u001b[36m2.0561\u001b[0m        1.9352  0.4222\n",
      "      7        1.9689        2.0392  0.4127\n",
      "      7        \u001b[36m1.4234\u001b[0m        1.7888  0.4069\n",
      "      7        \u001b[36m1.7044\u001b[0m        2.3071  0.4239\n",
      "      7        \u001b[36m1.8386\u001b[0m        2.2144  0.4112\n",
      "      6        \u001b[36m2.0432\u001b[0m        1.9581  0.4201\n",
      "      8        \u001b[36m1.9376\u001b[0m        2.0346  0.4043\n",
      "      8        \u001b[36m1.4171\u001b[0m        1.7969  0.4149\n",
      "      8        \u001b[36m1.6963\u001b[0m        2.3062  0.4074\n",
      "      8        \u001b[36m1.8063\u001b[0m        2.2232  0.4183\n",
      "      7        \u001b[36m2.0197\u001b[0m        1.9678  0.4143\n",
      "      9        1.9381        2.0329  0.4201\n",
      "      9        1.4189        1.8023  0.4070\n",
      "      9        \u001b[36m1.6838\u001b[0m        2.2781  0.4144\n",
      "      9        1.8206        2.2392  0.4173\n",
      "      8        \u001b[36m2.0017\u001b[0m        1.9545  0.4231\n",
      "     10        \u001b[36m1.9217\u001b[0m        2.0353  0.4070\n",
      "Restoring best model from epoch 1.\n",
      "     10        \u001b[36m1.3978\u001b[0m        1.8009  0.4099\n",
      "Restoring best model from epoch 1.\n",
      "     10        \u001b[36m1.6706\u001b[0m        2.2792  0.4055\n",
      "Restoring best model from epoch 1.\n",
      "     10        \u001b[36m1.7921\u001b[0m        2.2568  0.4124\n",
      "Restoring best model from epoch 1.\n",
      "      9        \u001b[36m1.9965\u001b[0m        1.9467  0.4136\n",
      "     10        \u001b[36m1.9631\u001b[0m        1.9528  0.3948\n",
      "Restoring best model from epoch 2.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m1.9071\u001b[0m        \u001b[32m2.2984\u001b[0m  0.2331\n",
      "      2        \u001b[36m1.8215\u001b[0m        \u001b[32m2.2753\u001b[0m  0.1452\n",
      "      3        \u001b[36m1.7898\u001b[0m        2.3073  0.1457\n",
      "      4        \u001b[36m1.7673\u001b[0m        2.3036  0.1414\n",
      "      5        \u001b[36m1.7649\u001b[0m        2.3142  0.2002\n",
      "      6        \u001b[36m1.7592\u001b[0m        2.3299  0.1509\n",
      "      7        \u001b[36m1.7538\u001b[0m        2.3530  0.1427\n",
      "      8        \u001b[36m1.7367\u001b[0m        2.3586  0.1363\n",
      "      9        \u001b[36m1.7097\u001b[0m        2.3569  0.1846\n",
      "     10        \u001b[36m1.6968\u001b[0m        2.3481  0.1857\n",
      "Restoring best model from epoch 2.\n",
      "y_train breslow final [-1.000e+00 -2.000e+00 -3.000e+00 -3.000e+00 -3.000e+00 -3.000e+00\n",
      " -3.000e+00 -4.000e+00 -4.000e+00 -6.000e+00 -7.000e+00 -7.000e+00\n",
      " -7.000e+00 -7.000e+00 -7.000e+00 -7.000e+00  7.000e+00 -8.000e+00\n",
      " -1.000e+01 -1.400e+01 -1.500e+01 -1.700e+01  2.300e+01 -2.300e+01\n",
      "  3.700e+01 -3.900e+01 -5.000e+01 -5.500e+01 -5.800e+01 -6.200e+01\n",
      " -7.000e+01 -7.200e+01 -7.300e+01 -7.400e+01 -7.600e+01 -7.700e+01\n",
      " -8.200e+01  9.600e+01  1.110e+02 -1.120e+02 -1.140e+02 -1.220e+02\n",
      " -1.340e+02 -1.390e+02  1.550e+02  1.620e+02 -1.660e+02 -1.690e+02\n",
      " -1.740e+02  1.780e+02 -1.790e+02 -1.820e+02 -1.840e+02 -1.900e+02\n",
      " -1.940e+02 -1.940e+02  1.940e+02  1.990e+02 -2.030e+02 -2.040e+02\n",
      "  2.050e+02 -2.070e+02  2.140e+02  2.280e+02 -2.300e+02  2.340e+02\n",
      "  2.400e+02  2.410e+02 -2.420e+02  2.420e+02 -2.420e+02 -2.490e+02\n",
      " -2.570e+02 -2.790e+02 -2.860e+02 -2.870e+02 -2.920e+02 -3.010e+02\n",
      " -3.130e+02  3.150e+02 -3.200e+02 -3.260e+02 -3.330e+02 -3.360e+02\n",
      " -3.370e+02 -3.420e+02 -3.430e+02  3.490e+02  3.510e+02 -3.540e+02\n",
      "  3.540e+02 -3.680e+02 -3.720e+02  3.720e+02 -3.720e+02 -3.840e+02\n",
      "  3.880e+02 -3.950e+02 -4.030e+02 -4.050e+02 -4.070e+02 -4.110e+02\n",
      " -4.140e+02 -4.160e+02 -4.170e+02 -4.180e+02 -4.190e+02 -4.270e+02\n",
      " -4.310e+02 -4.340e+02  4.380e+02 -4.380e+02 -4.420e+02 -4.420e+02\n",
      " -4.420e+02 -4.430e+02  4.440e+02 -4.490e+02 -4.540e+02 -4.540e+02\n",
      " -4.550e+02 -4.570e+02 -4.580e+02 -4.610e+02  4.660e+02 -4.670e+02\n",
      " -4.710e+02 -4.780e+02 -4.820e+02 -4.860e+02 -4.870e+02 -4.870e+02\n",
      " -4.910e+02  4.920e+02 -4.920e+02  4.920e+02 -4.940e+02 -4.940e+02\n",
      " -4.970e+02 -5.020e+02 -5.030e+02 -5.080e+02 -5.090e+02  5.120e+02\n",
      " -5.120e+02 -5.150e+02 -5.230e+02 -5.230e+02 -5.260e+02 -5.300e+02\n",
      "  5.310e+02 -5.310e+02 -5.320e+02 -5.330e+02  5.380e+02 -5.440e+02\n",
      "  5.470e+02 -5.480e+02  5.610e+02 -5.660e+02 -5.670e+02 -5.670e+02\n",
      " -5.690e+02 -5.690e+02 -5.730e+02  5.760e+02 -5.760e+02  5.780e+02\n",
      " -5.820e+02 -5.840e+02 -5.850e+02 -5.880e+02  5.920e+02 -6.040e+02\n",
      "  6.050e+02 -6.090e+02 -6.090e+02 -6.110e+02 -6.110e+02 -6.150e+02\n",
      " -6.220e+02 -6.230e+02 -6.270e+02 -6.290e+02 -6.290e+02 -6.300e+02\n",
      " -6.380e+02  6.390e+02 -6.500e+02 -6.510e+02 -6.510e+02 -6.550e+02\n",
      " -6.710e+02 -6.770e+02 -6.780e+02 -6.860e+02 -7.050e+02 -7.060e+02\n",
      " -7.060e+02  7.090e+02 -7.140e+02 -7.150e+02 -7.180e+02  7.220e+02\n",
      "  7.270e+02 -7.350e+02 -7.360e+02 -7.380e+02  7.420e+02 -7.430e+02\n",
      " -7.480e+02  7.580e+02 -7.580e+02 -7.600e+02 -7.720e+02  7.750e+02\n",
      "  7.750e+02 -7.850e+02 -7.870e+02  7.880e+02 -7.920e+02 -7.950e+02\n",
      " -7.960e+02 -8.050e+02  8.140e+02  8.190e+02  8.210e+02 -8.260e+02\n",
      " -8.320e+02 -8.370e+02 -8.460e+02 -8.540e+02 -8.600e+02 -8.620e+02\n",
      " -8.630e+02 -8.680e+02 -8.780e+02 -8.890e+02 -9.080e+02 -9.080e+02\n",
      " -9.080e+02 -9.140e+02 -9.140e+02 -9.160e+02  9.190e+02 -9.190e+02\n",
      " -9.260e+02  9.330e+02 -9.350e+02 -9.390e+02 -9.460e+02  9.540e+02\n",
      " -9.550e+02 -9.560e+02 -9.620e+02  9.620e+02 -9.640e+02 -9.640e+02\n",
      " -9.680e+02  9.840e+02  9.870e+02 -9.920e+02 -9.920e+02 -9.930e+02\n",
      " -1.004e+03  1.011e+03 -1.012e+03 -1.021e+03 -1.026e+03  1.033e+03\n",
      " -1.040e+03 -1.058e+03 -1.069e+03 -1.076e+03 -1.079e+03 -1.099e+03\n",
      "  1.106e+03 -1.112e+03 -1.115e+03 -1.116e+03 -1.120e+03  1.120e+03\n",
      "  1.120e+03 -1.127e+03 -1.130e+03 -1.137e+03 -1.139e+03 -1.147e+03\n",
      " -1.164e+03 -1.173e+03  1.183e+03 -1.189e+03 -1.201e+03  1.209e+03\n",
      " -1.213e+03 -1.217e+03 -1.219e+03  1.220e+03 -1.222e+03 -1.227e+03\n",
      " -1.229e+03 -1.236e+03  1.242e+03 -1.245e+03 -1.250e+03  1.251e+03\n",
      " -1.257e+03  1.262e+03 -1.277e+03 -1.279e+03 -1.294e+03 -1.294e+03\n",
      " -1.300e+03 -1.301e+03 -1.314e+03 -1.320e+03  1.335e+03 -1.337e+03\n",
      "  1.339e+03 -1.341e+03  1.351e+03 -1.354e+03 -1.359e+03 -1.382e+03\n",
      " -1.387e+03 -1.393e+03 -1.397e+03 -1.399e+03  1.401e+03 -1.412e+03\n",
      " -1.421e+03 -1.426e+03 -1.428e+03 -1.453e+03 -1.458e+03 -1.470e+03\n",
      "  1.481e+03  1.491e+03 -1.494e+03 -1.500e+03 -1.519e+03  1.525e+03\n",
      " -1.540e+03 -1.567e+03 -1.568e+03  1.578e+03  1.585e+03 -1.588e+03\n",
      " -1.631e+03  1.666e+03 -1.706e+03 -1.721e+03 -1.752e+03 -1.796e+03\n",
      " -1.806e+03 -1.828e+03 -1.834e+03 -1.868e+03  1.886e+03  1.891e+03\n",
      "  1.933e+03 -1.943e+03 -1.989e+03  2.000e+03  2.052e+03 -2.078e+03\n",
      " -2.107e+03 -2.218e+03 -2.219e+03  2.235e+03  2.282e+03  2.286e+03\n",
      " -2.287e+03 -2.289e+03 -2.289e+03  2.379e+03 -2.381e+03  2.433e+03\n",
      " -2.493e+03 -2.565e+03 -2.585e+03 -2.602e+03 -2.650e+03  2.660e+03\n",
      " -2.702e+03 -2.761e+03  2.835e+03  2.875e+03 -2.893e+03  2.907e+03\n",
      " -2.918e+03  2.988e+03 -3.013e+03  3.200e+03 -3.253e+03  3.470e+03\n",
      "  3.571e+03 -3.725e+03 -3.733e+03 -3.761e+03  4.068e+03 -4.113e+03\n",
      "  4.229e+03  4.412e+03  4.445e+03  4.695e+03  5.166e+03 -5.255e+03\n",
      " -5.546e+03]\n",
      "Concordance Index 0.6982973893303065\n",
      "Integrated Brier Score: inf\n",
      "y_train breslow final [-1.000e+00 -2.000e+00 -3.000e+00 -3.000e+00 -3.000e+00 -3.000e+00\n",
      " -3.000e+00 -4.000e+00 -4.000e+00 -6.000e+00 -7.000e+00 -7.000e+00\n",
      " -7.000e+00 -7.000e+00 -7.000e+00 -7.000e+00  7.000e+00 -8.000e+00\n",
      " -1.000e+01 -1.400e+01 -1.500e+01 -1.700e+01  2.300e+01 -2.300e+01\n",
      "  3.700e+01 -3.900e+01 -5.000e+01 -5.500e+01 -5.800e+01 -6.200e+01\n",
      " -7.000e+01 -7.200e+01 -7.300e+01 -7.400e+01 -7.600e+01 -7.700e+01\n",
      " -8.200e+01  9.600e+01  1.110e+02 -1.120e+02 -1.140e+02 -1.220e+02\n",
      " -1.340e+02 -1.390e+02  1.550e+02  1.620e+02 -1.660e+02 -1.690e+02\n",
      " -1.740e+02  1.780e+02 -1.790e+02 -1.820e+02 -1.840e+02 -1.900e+02\n",
      " -1.940e+02 -1.940e+02  1.940e+02  1.990e+02 -2.030e+02 -2.040e+02\n",
      "  2.050e+02 -2.070e+02  2.140e+02  2.280e+02 -2.300e+02  2.340e+02\n",
      "  2.400e+02  2.410e+02 -2.420e+02  2.420e+02 -2.420e+02 -2.490e+02\n",
      " -2.570e+02 -2.790e+02 -2.860e+02 -2.870e+02 -2.920e+02 -3.010e+02\n",
      " -3.130e+02  3.150e+02 -3.200e+02 -3.260e+02 -3.330e+02 -3.360e+02\n",
      " -3.370e+02 -3.420e+02 -3.430e+02  3.490e+02  3.510e+02 -3.540e+02\n",
      "  3.540e+02 -3.680e+02 -3.720e+02  3.720e+02 -3.720e+02 -3.840e+02\n",
      "  3.880e+02 -3.950e+02 -4.030e+02 -4.050e+02 -4.070e+02 -4.110e+02\n",
      " -4.140e+02 -4.160e+02 -4.170e+02 -4.180e+02 -4.190e+02 -4.270e+02\n",
      " -4.310e+02 -4.340e+02  4.380e+02 -4.380e+02 -4.420e+02 -4.420e+02\n",
      " -4.420e+02 -4.430e+02  4.440e+02 -4.490e+02 -4.540e+02 -4.540e+02\n",
      " -4.550e+02 -4.570e+02 -4.580e+02 -4.610e+02  4.660e+02 -4.670e+02\n",
      " -4.710e+02 -4.780e+02 -4.820e+02 -4.860e+02 -4.870e+02 -4.870e+02\n",
      " -4.910e+02  4.920e+02 -4.920e+02  4.920e+02 -4.940e+02 -4.940e+02\n",
      " -4.970e+02 -5.020e+02 -5.030e+02 -5.080e+02 -5.090e+02  5.120e+02\n",
      " -5.120e+02 -5.150e+02 -5.230e+02 -5.230e+02 -5.260e+02 -5.300e+02\n",
      "  5.310e+02 -5.310e+02 -5.320e+02 -5.330e+02  5.380e+02 -5.440e+02\n",
      "  5.470e+02 -5.480e+02  5.610e+02 -5.660e+02 -5.670e+02 -5.670e+02\n",
      " -5.690e+02 -5.690e+02 -5.730e+02  5.760e+02 -5.760e+02  5.780e+02\n",
      " -5.820e+02 -5.840e+02 -5.850e+02 -5.880e+02  5.920e+02 -6.040e+02\n",
      "  6.050e+02 -6.090e+02 -6.090e+02 -6.110e+02 -6.110e+02 -6.150e+02\n",
      " -6.220e+02 -6.230e+02 -6.270e+02 -6.290e+02 -6.290e+02 -6.300e+02\n",
      " -6.380e+02  6.390e+02 -6.500e+02 -6.510e+02 -6.510e+02 -6.550e+02\n",
      " -6.710e+02 -6.770e+02 -6.780e+02 -6.860e+02 -7.050e+02 -7.060e+02\n",
      " -7.060e+02  7.090e+02 -7.140e+02 -7.150e+02 -7.180e+02  7.220e+02\n",
      "  7.270e+02 -7.350e+02 -7.360e+02 -7.380e+02  7.420e+02 -7.430e+02\n",
      " -7.480e+02  7.580e+02 -7.580e+02 -7.600e+02 -7.720e+02  7.750e+02\n",
      "  7.750e+02 -7.850e+02 -7.870e+02  7.880e+02 -7.920e+02 -7.950e+02\n",
      " -7.960e+02 -8.050e+02  8.140e+02  8.190e+02  8.210e+02 -8.260e+02\n",
      " -8.320e+02 -8.370e+02 -8.460e+02 -8.540e+02 -8.600e+02 -8.620e+02\n",
      " -8.630e+02 -8.680e+02 -8.780e+02 -8.890e+02 -9.080e+02 -9.080e+02\n",
      " -9.080e+02 -9.140e+02 -9.140e+02 -9.160e+02  9.190e+02 -9.190e+02\n",
      " -9.260e+02  9.330e+02 -9.350e+02 -9.390e+02 -9.460e+02  9.540e+02\n",
      " -9.550e+02 -9.560e+02 -9.620e+02  9.620e+02 -9.640e+02 -9.640e+02\n",
      " -9.680e+02  9.840e+02  9.870e+02 -9.920e+02 -9.920e+02 -9.930e+02\n",
      " -1.004e+03  1.011e+03 -1.012e+03 -1.021e+03 -1.026e+03  1.033e+03\n",
      " -1.040e+03 -1.058e+03 -1.069e+03 -1.076e+03 -1.079e+03 -1.099e+03\n",
      "  1.106e+03 -1.112e+03 -1.115e+03 -1.116e+03 -1.120e+03  1.120e+03\n",
      "  1.120e+03 -1.127e+03 -1.130e+03 -1.137e+03 -1.139e+03 -1.147e+03\n",
      " -1.164e+03 -1.173e+03  1.183e+03 -1.189e+03 -1.201e+03  1.209e+03\n",
      " -1.213e+03 -1.217e+03 -1.219e+03  1.220e+03 -1.222e+03 -1.227e+03\n",
      " -1.229e+03 -1.236e+03  1.242e+03 -1.245e+03 -1.250e+03  1.251e+03\n",
      " -1.257e+03  1.262e+03 -1.277e+03 -1.279e+03 -1.294e+03 -1.294e+03\n",
      " -1.300e+03 -1.301e+03 -1.314e+03 -1.320e+03  1.335e+03 -1.337e+03\n",
      "  1.339e+03 -1.341e+03  1.351e+03 -1.354e+03 -1.359e+03 -1.382e+03\n",
      " -1.387e+03 -1.393e+03 -1.397e+03 -1.399e+03  1.401e+03 -1.412e+03\n",
      " -1.421e+03 -1.426e+03 -1.428e+03 -1.453e+03 -1.458e+03 -1.470e+03\n",
      "  1.481e+03  1.491e+03 -1.494e+03 -1.500e+03 -1.519e+03  1.525e+03\n",
      " -1.540e+03 -1.567e+03 -1.568e+03  1.578e+03  1.585e+03 -1.588e+03\n",
      " -1.631e+03  1.666e+03 -1.706e+03 -1.721e+03 -1.752e+03 -1.796e+03\n",
      " -1.806e+03 -1.828e+03 -1.834e+03 -1.868e+03  1.886e+03  1.891e+03\n",
      "  1.933e+03 -1.943e+03 -1.989e+03  2.000e+03  2.052e+03 -2.078e+03\n",
      " -2.107e+03 -2.218e+03 -2.219e+03  2.235e+03  2.282e+03  2.286e+03\n",
      " -2.287e+03 -2.289e+03 -2.289e+03  2.379e+03 -2.381e+03  2.433e+03\n",
      " -2.493e+03 -2.565e+03 -2.585e+03 -2.602e+03 -2.650e+03  2.660e+03\n",
      " -2.702e+03 -2.761e+03  2.835e+03  2.875e+03 -2.893e+03  2.907e+03\n",
      " -2.918e+03  2.988e+03 -3.013e+03  3.200e+03 -3.253e+03  3.470e+03\n",
      "  3.571e+03 -3.725e+03 -3.733e+03 -3.761e+03  4.068e+03 -4.113e+03\n",
      "  4.229e+03  4.412e+03  4.445e+03  4.695e+03  5.166e+03 -5.255e+03\n",
      " -5.546e+03]\n",
      "durations 3.0 6423.0\n",
      "Concordance Index 0.7340332458442694\n",
      "Integrated Brier Score: 7.746701235826249e+46\n",
      "(409, 20531) <class 'pandas.core.frame.DataFrame'>\n",
      "(409,) <class 'pandas.core.series.Series'>\n",
      "(102, 20531) <class 'pandas.core.frame.DataFrame'>\n",
      "(102,) <class 'pandas.core.series.Series'>\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.0659\u001b[0m        \u001b[32m1.7978\u001b[0m  0.2176\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.0872\u001b[0m        \u001b[32m1.5754\u001b[0m  0.1892\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m1.8580\u001b[0m        \u001b[32m1.1373\u001b[0m  0.1729\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.1931\u001b[0m        \u001b[32m1.3528\u001b[0m  0.3317\n",
      "      2        \u001b[36m1.9789\u001b[0m        \u001b[32m1.7751\u001b[0m  0.1591\n",
      "      2        \u001b[36m2.0245\u001b[0m        \u001b[32m1.5627\u001b[0m  0.2026\n",
      "      2        \u001b[36m1.6905\u001b[0m        1.1383  0.1925\n",
      "      3        \u001b[36m1.6630\u001b[0m        1.1940  0.1400\n",
      "      2        \u001b[36m2.1634\u001b[0m        \u001b[32m1.2971\u001b[0m  0.2270\n",
      "      3        \u001b[36m1.9645\u001b[0m        \u001b[32m1.7690\u001b[0m  0.2366\n",
      "      3        \u001b[36m1.9422\u001b[0m        \u001b[32m1.5574\u001b[0m  0.1893\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.1141\u001b[0m        \u001b[32m1.9639\u001b[0m  0.5472\n",
      "      4        \u001b[36m1.6397\u001b[0m        1.1986  0.1544\n",
      "      3        \u001b[36m2.1036\u001b[0m        1.2994  0.1674\n",
      "      4        \u001b[36m1.9173\u001b[0m        \u001b[32m1.5393\u001b[0m  0.1571\n",
      "      4        \u001b[36m1.9378\u001b[0m        1.8004  0.1888\n",
      "      2        \u001b[36m2.0334\u001b[0m        \u001b[32m1.9368\u001b[0m  0.1650\n",
      "      5        \u001b[36m1.6340\u001b[0m        1.1963  0.1618\n",
      "      4        \u001b[36m2.0399\u001b[0m        \u001b[32m1.2930\u001b[0m  0.1710\n",
      "      5        \u001b[36m1.8721\u001b[0m        1.5709  0.1722\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.1718\u001b[0m        \u001b[32m1.3102\u001b[0m  0.7515\n",
      "      5        \u001b[36m1.9096\u001b[0m        1.8310  0.1565\n",
      "      3        \u001b[36m2.0258\u001b[0m        \u001b[32m1.9308\u001b[0m  0.1580\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.0605\u001b[0m        \u001b[32m1.4774\u001b[0m  0.8291\n",
      "      6        \u001b[36m1.6058\u001b[0m        1.1968  0.1649\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.0101\u001b[0m        \u001b[32m1.6557\u001b[0m  0.7935\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.0962\u001b[0m        \u001b[32m1.9314\u001b[0m  0.9287\n",
      "      6        \u001b[36m1.8886\u001b[0m        1.8424  0.1427\n",
      "      6        \u001b[36m1.8460\u001b[0m        1.5620  0.2009\n",
      "      5        \u001b[36m1.9982\u001b[0m        1.2990  0.2221\n",
      "      4        \u001b[36m1.9827\u001b[0m        1.9762  0.1269\n",
      "      7        1.6145        1.1984  0.1327\n",
      "      7        \u001b[36m1.8819\u001b[0m        1.8351  0.1513\n",
      "      7        \u001b[36m1.8396\u001b[0m        1.5688  0.1550\n",
      "      5        \u001b[36m1.9607\u001b[0m        1.9942  0.1527\n",
      "      6        \u001b[36m1.9923\u001b[0m        1.3069  0.1737\n",
      "      8        1.6067        1.2038  0.1905\n",
      "      8        \u001b[36m1.8736\u001b[0m        1.8425  0.1521\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m1.7815\u001b[0m        \u001b[32m1.1894\u001b[0m  1.2233\n",
      "      8        \u001b[36m1.8052\u001b[0m        1.5800  0.1605\n",
      "      7        \u001b[36m1.9538\u001b[0m        1.3127  0.1551\n",
      "      6        \u001b[36m1.9491\u001b[0m        2.0031  0.2017\n",
      "      2        \u001b[36m2.1242\u001b[0m        \u001b[32m1.2557\u001b[0m  0.5769\n",
      "      9        1.6086        1.2068  0.1582\n",
      "      9        \u001b[36m1.8558\u001b[0m        1.8464  0.1489\n",
      "      2        \u001b[36m1.9602\u001b[0m        \u001b[32m1.4554\u001b[0m  0.5350\n",
      "      9        \u001b[36m1.7924\u001b[0m        1.5885  0.1405\n",
      "      2        \u001b[36m1.9193\u001b[0m        1.7310  0.5589\n",
      "      8        \u001b[36m1.9126\u001b[0m        1.3235  0.1457\n",
      "      2        \u001b[36m2.0190\u001b[0m        1.9553  0.5775\n",
      "      7        \u001b[36m1.9461\u001b[0m        2.0106  0.1397\n",
      "     10        \u001b[36m1.5852\u001b[0m        1.2072  0.1379\n",
      "Restoring best model from epoch 1.\n",
      "     10        \u001b[36m1.8442\u001b[0m        1.8429  0.1463\n",
      "Restoring best model from epoch 3.\n",
      "     10        \u001b[36m1.7502\u001b[0m        1.6057  0.1365\n",
      "Restoring best model from epoch 4.\n",
      "      9        1.9367        1.3370  0.1513\n",
      "      8        \u001b[36m1.9292\u001b[0m        2.0039  0.1369\n",
      "     10        \u001b[36m1.9085\u001b[0m        1.3492  0.1315\n",
      "Restoring best model from epoch 4.\n",
      "      9        1.9361        1.9954  0.1345\n",
      "      2        \u001b[36m1.6954\u001b[0m        \u001b[32m1.1631\u001b[0m  0.5175\n",
      "      3        \u001b[36m2.0932\u001b[0m        \u001b[32m1.2458\u001b[0m  0.4756\n",
      "     10        \u001b[36m1.9157\u001b[0m        1.9892  0.1177\n",
      "Restoring best model from epoch 3.\n",
      "      3        \u001b[36m1.9146\u001b[0m        1.4789  0.4619\n",
      "      3        \u001b[36m1.8821\u001b[0m        1.7709  0.4549\n",
      "      3        \u001b[36m2.0003\u001b[0m        1.9695  0.4551\n",
      "      3        \u001b[36m1.6452\u001b[0m        1.2298  0.4137\n",
      "      4        \u001b[36m2.0738\u001b[0m        \u001b[32m1.2433\u001b[0m  0.4176\n",
      "      4        \u001b[36m1.8706\u001b[0m        1.4952  0.4143\n",
      "      4        \u001b[36m1.8698\u001b[0m        1.7817  0.4066\n",
      "      4        \u001b[36m1.9719\u001b[0m        1.9939  0.4054\n",
      "      4        \u001b[36m1.6305\u001b[0m        1.2359  0.4078\n",
      "      5        \u001b[36m2.0625\u001b[0m        \u001b[32m1.2418\u001b[0m  0.4167\n",
      "      5        \u001b[36m1.8373\u001b[0m        1.5027  0.4239\n",
      "      5        \u001b[36m1.8369\u001b[0m        1.7874  0.4177\n",
      "      5        \u001b[36m1.9465\u001b[0m        2.0325  0.4107\n",
      "      5        \u001b[36m1.6158\u001b[0m        1.2315  0.4121\n",
      "      6        \u001b[36m2.0599\u001b[0m        \u001b[32m1.2202\u001b[0m  0.4109\n",
      "      6        \u001b[36m1.8274\u001b[0m        1.5203  0.4173\n",
      "      6        \u001b[36m1.8051\u001b[0m        1.7858  0.4123\n",
      "      6        \u001b[36m1.9303\u001b[0m        2.0431  0.4115\n",
      "      6        \u001b[36m1.5866\u001b[0m        1.2294  0.4152\n",
      "      7        \u001b[36m2.0140\u001b[0m        \u001b[32m1.2152\u001b[0m  0.4142\n",
      "      7        \u001b[36m1.8028\u001b[0m        1.5025  0.4120\n",
      "      7        \u001b[36m1.8038\u001b[0m        1.7687  0.4186\n",
      "      7        \u001b[36m1.9132\u001b[0m        2.0501  0.4150\n",
      "      7        \u001b[36m1.5737\u001b[0m        1.2371  0.4048\n",
      "      8        \u001b[36m2.0133\u001b[0m        \u001b[32m1.2078\u001b[0m  0.4182\n",
      "      8        \u001b[36m1.7822\u001b[0m        1.7678  0.3997\n",
      "      8        1.8090        1.5057  0.4082\n",
      "      8        \u001b[36m1.9031\u001b[0m        2.0358  0.4051\n",
      "      8        1.5894        1.2322  0.4110\n",
      "      9        2.0137        1.2105  0.4058\n",
      "      9        \u001b[36m1.7953\u001b[0m        1.5269  0.4146\n",
      "      9        \u001b[36m1.7739\u001b[0m        1.7742  0.4160\n",
      "      9        \u001b[36m1.8840\u001b[0m        2.0205  0.4059\n",
      "      9        \u001b[36m1.5682\u001b[0m        1.2205  0.4085\n",
      "     10        \u001b[36m1.9770\u001b[0m        1.2145  0.4038\n",
      "Restoring best model from epoch 8.\n",
      "     10        \u001b[36m1.7670\u001b[0m        1.7881  0.4114\n",
      "Restoring best model from epoch 1.\n",
      "     10        \u001b[36m1.7724\u001b[0m        1.5224  0.4185\n",
      "Restoring best model from epoch 2.\n",
      "     10        \u001b[36m1.8518\u001b[0m        2.0381  0.4092\n",
      "Restoring best model from epoch 1.\n",
      "     10        \u001b[36m1.5512\u001b[0m        1.2278  0.4002\n",
      "Restoring best model from epoch 2.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m1.8726\u001b[0m        \u001b[32m1.6698\u001b[0m  0.1746\n",
      "      2        \u001b[36m1.7907\u001b[0m        \u001b[32m1.6551\u001b[0m  0.1438\n",
      "      3        \u001b[36m1.7294\u001b[0m        1.6996  0.1427\n",
      "      4        \u001b[36m1.7178\u001b[0m        1.6900  0.1295\n",
      "      5        \u001b[36m1.6748\u001b[0m        1.6665  0.1285\n",
      "      6        \u001b[36m1.6582\u001b[0m        \u001b[32m1.6525\u001b[0m  0.1322\n",
      "      7        \u001b[36m1.6346\u001b[0m        \u001b[32m1.6489\u001b[0m  0.1288\n",
      "      8        \u001b[36m1.6221\u001b[0m        1.6503  0.1763\n",
      "      9        \u001b[36m1.6051\u001b[0m        \u001b[32m1.6462\u001b[0m  0.1424\n",
      "     10        \u001b[36m1.5956\u001b[0m        \u001b[32m1.6387\u001b[0m  0.1377\n",
      "y_train breslow final [-1.000e+00 -2.000e+00 -3.000e+00 -3.000e+00 -3.000e+00 -3.000e+00\n",
      " -3.000e+00 -3.000e+00 -3.000e+00 -3.000e+00 -3.000e+00 -4.000e+00\n",
      " -4.000e+00 -5.000e+00 -6.000e+00 -7.000e+00 -7.000e+00 -7.000e+00\n",
      " -7.000e+00  7.000e+00 -1.000e+01 -1.500e+01  3.700e+01 -3.900e+01\n",
      " -5.000e+01 -5.500e+01 -5.800e+01 -6.200e+01 -6.200e+01 -6.300e+01\n",
      " -7.200e+01 -7.200e+01 -7.300e+01 -7.400e+01 -7.600e+01 -7.700e+01\n",
      " -8.200e+01 -8.400e+01 -9.000e+01 -1.050e+02  1.110e+02 -1.120e+02\n",
      "  1.130e+02 -1.140e+02 -1.220e+02 -1.340e+02 -1.390e+02 -1.530e+02\n",
      "  1.550e+02 -1.620e+02  1.620e+02 -1.660e+02 -1.690e+02  1.780e+02\n",
      " -1.790e+02 -1.820e+02 -1.840e+02 -1.900e+02 -1.940e+02 -1.940e+02\n",
      "  1.990e+02 -1.990e+02 -2.030e+02 -2.040e+02 -2.070e+02 -2.100e+02\n",
      "  2.140e+02  2.280e+02 -2.300e+02 -2.310e+02  2.340e+02  2.400e+02\n",
      "  2.410e+02 -2.420e+02  2.420e+02 -2.420e+02 -2.430e+02  2.450e+02\n",
      " -2.490e+02 -2.570e+02  2.690e+02 -2.740e+02 -2.790e+02 -2.870e+02\n",
      " -2.920e+02 -3.010e+02 -3.130e+02  3.150e+02  3.160e+02 -3.170e+02\n",
      " -3.260e+02 -3.280e+02 -3.360e+02 -3.370e+02 -3.420e+02 -3.430e+02\n",
      "  3.470e+02  3.490e+02  3.510e+02 -3.540e+02  3.540e+02 -3.680e+02\n",
      " -3.720e+02  3.780e+02 -3.840e+02  3.880e+02 -3.950e+02  3.980e+02\n",
      " -4.050e+02 -4.070e+02 -4.080e+02 -4.110e+02 -4.140e+02 -4.140e+02\n",
      " -4.170e+02 -4.180e+02 -4.190e+02 -4.270e+02 -4.280e+02 -4.310e+02\n",
      " -4.330e+02 -4.340e+02  4.350e+02  4.380e+02 -4.380e+02 -4.380e+02\n",
      " -4.420e+02 -4.420e+02 -4.420e+02  4.440e+02 -4.490e+02 -4.510e+02\n",
      " -4.540e+02 -4.540e+02 -4.550e+02 -4.550e+02  4.560e+02 -4.570e+02\n",
      " -4.580e+02 -4.610e+02 -4.610e+02  4.660e+02 -4.670e+02 -4.710e+02\n",
      " -4.820e+02 -4.870e+02 -4.870e+02 -4.910e+02  4.920e+02 -4.920e+02\n",
      "  4.920e+02 -4.940e+02 -4.970e+02 -5.000e+02 -5.020e+02 -5.030e+02\n",
      " -5.080e+02 -5.090e+02  5.120e+02 -5.120e+02 -5.160e+02 -5.220e+02\n",
      " -5.260e+02  5.310e+02 -5.330e+02  5.370e+02  5.380e+02 -5.420e+02\n",
      " -5.440e+02 -5.440e+02 -5.480e+02  5.590e+02  5.610e+02 -5.630e+02\n",
      " -5.640e+02 -5.660e+02 -5.660e+02 -5.660e+02 -5.670e+02 -5.690e+02\n",
      " -5.690e+02 -5.710e+02 -5.710e+02 -5.730e+02  5.760e+02  5.780e+02\n",
      " -5.850e+02 -5.880e+02  5.920e+02 -5.960e+02 -5.990e+02  6.050e+02\n",
      "  6.070e+02 -6.080e+02 -6.090e+02 -6.110e+02 -6.110e+02 -6.150e+02\n",
      " -6.220e+02 -6.290e+02 -6.290e+02 -6.300e+02 -6.330e+02 -6.380e+02\n",
      "  6.480e+02  6.480e+02 -6.500e+02 -6.510e+02 -6.560e+02 -6.770e+02\n",
      " -6.780e+02  6.820e+02 -6.850e+02 -6.860e+02 -7.050e+02 -7.060e+02\n",
      " -7.060e+02  7.090e+02 -7.140e+02 -7.180e+02 -7.210e+02  7.220e+02\n",
      " -7.240e+02 -7.380e+02  7.420e+02 -7.430e+02 -7.540e+02 -7.600e+02\n",
      " -7.720e+02  7.750e+02  7.750e+02 -7.770e+02 -7.850e+02  7.880e+02\n",
      " -7.950e+02 -7.960e+02 -8.000e+02 -8.050e+02  8.140e+02  8.140e+02\n",
      "  8.190e+02  8.210e+02 -8.260e+02 -8.320e+02 -8.350e+02 -8.370e+02\n",
      " -8.460e+02 -8.460e+02 -8.540e+02 -8.540e+02 -8.620e+02 -8.630e+02\n",
      " -8.680e+02 -8.780e+02  8.860e+02 -8.890e+02 -9.000e+02 -9.080e+02\n",
      " -9.080e+02 -9.140e+02 -9.160e+02 -9.190e+02 -9.330e+02 -9.350e+02\n",
      " -9.390e+02 -9.490e+02  9.540e+02 -9.550e+02 -9.560e+02  9.610e+02\n",
      " -9.620e+02 -9.640e+02 -9.680e+02  9.840e+02 -9.920e+02 -9.930e+02\n",
      " -1.004e+03  1.011e+03 -1.012e+03 -1.021e+03 -1.026e+03 -1.032e+03\n",
      " -1.058e+03 -1.078e+03 -1.099e+03 -1.112e+03 -1.115e+03 -1.116e+03\n",
      "  1.120e+03  1.120e+03 -1.127e+03 -1.130e+03  1.137e+03 -1.137e+03\n",
      " -1.147e+03  1.152e+03 -1.164e+03  1.183e+03 -1.189e+03 -1.191e+03\n",
      " -1.201e+03 -1.201e+03  1.209e+03 -1.210e+03 -1.213e+03 -1.217e+03\n",
      " -1.219e+03  1.220e+03 -1.227e+03 -1.229e+03 -1.236e+03  1.242e+03\n",
      " -1.250e+03  1.251e+03 -1.257e+03  1.262e+03 -1.277e+03 -1.294e+03\n",
      " -1.300e+03 -1.301e+03 -1.314e+03 -1.320e+03  1.335e+03 -1.337e+03\n",
      "  1.339e+03 -1.341e+03  1.351e+03 -1.382e+03 -1.397e+03 -1.399e+03\n",
      "  1.401e+03 -1.401e+03 -1.412e+03 -1.421e+03 -1.426e+03 -1.426e+03\n",
      " -1.453e+03 -1.458e+03 -1.469e+03 -1.470e+03  1.491e+03 -1.494e+03\n",
      " -1.500e+03 -1.519e+03  1.525e+03 -1.540e+03  1.547e+03 -1.553e+03\n",
      " -1.567e+03  1.578e+03  1.585e+03 -1.588e+03 -1.631e+03 -1.650e+03\n",
      "  1.666e+03 -1.706e+03 -1.721e+03 -1.752e+03  1.762e+03 -1.796e+03\n",
      " -1.806e+03 -1.828e+03  1.891e+03  1.915e+03  1.933e+03 -1.943e+03\n",
      "  2.000e+03  2.052e+03 -2.107e+03 -2.218e+03 -2.219e+03  2.282e+03\n",
      "  2.286e+03 -2.287e+03 -2.289e+03 -2.289e+03  2.379e+03 -2.381e+03\n",
      " -2.565e+03 -2.602e+03 -2.650e+03  2.660e+03 -2.702e+03 -2.761e+03\n",
      " -2.772e+03  2.835e+03 -2.860e+03 -2.869e+03  2.875e+03 -2.893e+03\n",
      " -2.918e+03 -2.918e+03  2.988e+03 -3.000e+03 -3.013e+03  3.470e+03\n",
      " -3.574e+03 -3.725e+03 -3.733e+03  3.978e+03  4.084e+03 -4.113e+03\n",
      "  4.229e+03  4.412e+03  4.695e+03 -4.752e+03 -5.255e+03 -5.546e+03\n",
      " -6.423e+03]\n",
      "Concordance Index 0.7904908312287252\n",
      "Integrated Brier Score: inf\n",
      "y_train breslow final [-1.000e+00 -2.000e+00 -3.000e+00 -3.000e+00 -3.000e+00 -3.000e+00\n",
      " -3.000e+00 -3.000e+00 -3.000e+00 -3.000e+00 -3.000e+00 -4.000e+00\n",
      " -4.000e+00 -5.000e+00 -6.000e+00 -7.000e+00 -7.000e+00 -7.000e+00\n",
      " -7.000e+00  7.000e+00 -1.000e+01 -1.500e+01  3.700e+01 -3.900e+01\n",
      " -5.000e+01 -5.500e+01 -5.800e+01 -6.200e+01 -6.200e+01 -6.300e+01\n",
      " -7.200e+01 -7.200e+01 -7.300e+01 -7.400e+01 -7.600e+01 -7.700e+01\n",
      " -8.200e+01 -8.400e+01 -9.000e+01 -1.050e+02  1.110e+02 -1.120e+02\n",
      "  1.130e+02 -1.140e+02 -1.220e+02 -1.340e+02 -1.390e+02 -1.530e+02\n",
      "  1.550e+02 -1.620e+02  1.620e+02 -1.660e+02 -1.690e+02  1.780e+02\n",
      " -1.790e+02 -1.820e+02 -1.840e+02 -1.900e+02 -1.940e+02 -1.940e+02\n",
      "  1.990e+02 -1.990e+02 -2.030e+02 -2.040e+02 -2.070e+02 -2.100e+02\n",
      "  2.140e+02  2.280e+02 -2.300e+02 -2.310e+02  2.340e+02  2.400e+02\n",
      "  2.410e+02 -2.420e+02  2.420e+02 -2.420e+02 -2.430e+02  2.450e+02\n",
      " -2.490e+02 -2.570e+02  2.690e+02 -2.740e+02 -2.790e+02 -2.870e+02\n",
      " -2.920e+02 -3.010e+02 -3.130e+02  3.150e+02  3.160e+02 -3.170e+02\n",
      " -3.260e+02 -3.280e+02 -3.360e+02 -3.370e+02 -3.420e+02 -3.430e+02\n",
      "  3.470e+02  3.490e+02  3.510e+02 -3.540e+02  3.540e+02 -3.680e+02\n",
      " -3.720e+02  3.780e+02 -3.840e+02  3.880e+02 -3.950e+02  3.980e+02\n",
      " -4.050e+02 -4.070e+02 -4.080e+02 -4.110e+02 -4.140e+02 -4.140e+02\n",
      " -4.170e+02 -4.180e+02 -4.190e+02 -4.270e+02 -4.280e+02 -4.310e+02\n",
      " -4.330e+02 -4.340e+02  4.350e+02  4.380e+02 -4.380e+02 -4.380e+02\n",
      " -4.420e+02 -4.420e+02 -4.420e+02  4.440e+02 -4.490e+02 -4.510e+02\n",
      " -4.540e+02 -4.540e+02 -4.550e+02 -4.550e+02  4.560e+02 -4.570e+02\n",
      " -4.580e+02 -4.610e+02 -4.610e+02  4.660e+02 -4.670e+02 -4.710e+02\n",
      " -4.820e+02 -4.870e+02 -4.870e+02 -4.910e+02  4.920e+02 -4.920e+02\n",
      "  4.920e+02 -4.940e+02 -4.970e+02 -5.000e+02 -5.020e+02 -5.030e+02\n",
      " -5.080e+02 -5.090e+02  5.120e+02 -5.120e+02 -5.160e+02 -5.220e+02\n",
      " -5.260e+02  5.310e+02 -5.330e+02  5.370e+02  5.380e+02 -5.420e+02\n",
      " -5.440e+02 -5.440e+02 -5.480e+02  5.590e+02  5.610e+02 -5.630e+02\n",
      " -5.640e+02 -5.660e+02 -5.660e+02 -5.660e+02 -5.670e+02 -5.690e+02\n",
      " -5.690e+02 -5.710e+02 -5.710e+02 -5.730e+02  5.760e+02  5.780e+02\n",
      " -5.850e+02 -5.880e+02  5.920e+02 -5.960e+02 -5.990e+02  6.050e+02\n",
      "  6.070e+02 -6.080e+02 -6.090e+02 -6.110e+02 -6.110e+02 -6.150e+02\n",
      " -6.220e+02 -6.290e+02 -6.290e+02 -6.300e+02 -6.330e+02 -6.380e+02\n",
      "  6.480e+02  6.480e+02 -6.500e+02 -6.510e+02 -6.560e+02 -6.770e+02\n",
      " -6.780e+02  6.820e+02 -6.850e+02 -6.860e+02 -7.050e+02 -7.060e+02\n",
      " -7.060e+02  7.090e+02 -7.140e+02 -7.180e+02 -7.210e+02  7.220e+02\n",
      " -7.240e+02 -7.380e+02  7.420e+02 -7.430e+02 -7.540e+02 -7.600e+02\n",
      " -7.720e+02  7.750e+02  7.750e+02 -7.770e+02 -7.850e+02  7.880e+02\n",
      " -7.950e+02 -7.960e+02 -8.000e+02 -8.050e+02  8.140e+02  8.140e+02\n",
      "  8.190e+02  8.210e+02 -8.260e+02 -8.320e+02 -8.350e+02 -8.370e+02\n",
      " -8.460e+02 -8.460e+02 -8.540e+02 -8.540e+02 -8.620e+02 -8.630e+02\n",
      " -8.680e+02 -8.780e+02  8.860e+02 -8.890e+02 -9.000e+02 -9.080e+02\n",
      " -9.080e+02 -9.140e+02 -9.160e+02 -9.190e+02 -9.330e+02 -9.350e+02\n",
      " -9.390e+02 -9.490e+02  9.540e+02 -9.550e+02 -9.560e+02  9.610e+02\n",
      " -9.620e+02 -9.640e+02 -9.680e+02  9.840e+02 -9.920e+02 -9.930e+02\n",
      " -1.004e+03  1.011e+03 -1.012e+03 -1.021e+03 -1.026e+03 -1.032e+03\n",
      " -1.058e+03 -1.078e+03 -1.099e+03 -1.112e+03 -1.115e+03 -1.116e+03\n",
      "  1.120e+03  1.120e+03 -1.127e+03 -1.130e+03  1.137e+03 -1.137e+03\n",
      " -1.147e+03  1.152e+03 -1.164e+03  1.183e+03 -1.189e+03 -1.191e+03\n",
      " -1.201e+03 -1.201e+03  1.209e+03 -1.210e+03 -1.213e+03 -1.217e+03\n",
      " -1.219e+03  1.220e+03 -1.227e+03 -1.229e+03 -1.236e+03  1.242e+03\n",
      " -1.250e+03  1.251e+03 -1.257e+03  1.262e+03 -1.277e+03 -1.294e+03\n",
      " -1.300e+03 -1.301e+03 -1.314e+03 -1.320e+03  1.335e+03 -1.337e+03\n",
      "  1.339e+03 -1.341e+03  1.351e+03 -1.382e+03 -1.397e+03 -1.399e+03\n",
      "  1.401e+03 -1.401e+03 -1.412e+03 -1.421e+03 -1.426e+03 -1.426e+03\n",
      " -1.453e+03 -1.458e+03 -1.469e+03 -1.470e+03  1.491e+03 -1.494e+03\n",
      " -1.500e+03 -1.519e+03  1.525e+03 -1.540e+03  1.547e+03 -1.553e+03\n",
      " -1.567e+03  1.578e+03  1.585e+03 -1.588e+03 -1.631e+03 -1.650e+03\n",
      "  1.666e+03 -1.706e+03 -1.721e+03 -1.752e+03  1.762e+03 -1.796e+03\n",
      " -1.806e+03 -1.828e+03  1.891e+03  1.915e+03  1.933e+03 -1.943e+03\n",
      "  2.000e+03  2.052e+03 -2.107e+03 -2.218e+03 -2.219e+03  2.282e+03\n",
      "  2.286e+03 -2.287e+03 -2.289e+03 -2.289e+03  2.379e+03 -2.381e+03\n",
      " -2.565e+03 -2.602e+03 -2.650e+03  2.660e+03 -2.702e+03 -2.761e+03\n",
      " -2.772e+03  2.835e+03 -2.860e+03 -2.869e+03  2.875e+03 -2.893e+03\n",
      " -2.918e+03 -2.918e+03  2.988e+03 -3.000e+03 -3.013e+03  3.470e+03\n",
      " -3.574e+03 -3.725e+03 -3.733e+03  3.978e+03  4.084e+03 -4.113e+03\n",
      "  4.229e+03  4.412e+03  4.695e+03 -4.752e+03 -5.255e+03 -5.546e+03\n",
      " -6.423e+03]\n",
      "durations 4.0 5166.0\n",
      "Concordance Index 0.7272727272727273\n",
      "Integrated Brier Score: inf\n",
      "(409, 20531) <class 'pandas.core.frame.DataFrame'>\n",
      "(409,) <class 'pandas.core.series.Series'>\n",
      "(102, 20531) <class 'pandas.core.frame.DataFrame'>\n",
      "(102,) <class 'pandas.core.series.Series'>\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.2239\u001b[0m        \u001b[32m1.8144\u001b[0m  0.1609\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.0956\u001b[0m        \u001b[32m1.7010\u001b[0m  0.1566\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.2507\u001b[0m        \u001b[32m1.8435\u001b[0m  0.1803\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m1.7476\u001b[0m        \u001b[32m1.1535\u001b[0m  0.1807\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.0418\u001b[0m        \u001b[32m1.9827\u001b[0m  0.3359\n",
      "      2        \u001b[36m2.1076\u001b[0m        \u001b[32m1.7633\u001b[0m  0.1783\n",
      "      2        \u001b[36m1.9802\u001b[0m        1.7332  0.1801\n",
      "      2        \u001b[36m2.1570\u001b[0m        \u001b[32m1.8367\u001b[0m  0.1435\n",
      "      2        \u001b[36m1.9396\u001b[0m        \u001b[32m1.9585\u001b[0m  0.1622\n",
      "      3        \u001b[36m2.0647\u001b[0m        1.7813  0.1577\n",
      "      3        \u001b[36m1.9146\u001b[0m        1.7321  0.1506\n",
      "      2        \u001b[36m1.5882\u001b[0m        \u001b[32m1.1313\u001b[0m  0.1921\n",
      "      3        \u001b[36m1.8978\u001b[0m        2.0152  0.1370\n",
      "      3        \u001b[36m2.1020\u001b[0m        1.8687  0.2483\n",
      "      4        \u001b[36m1.9070\u001b[0m        1.7256  0.1648\n",
      "      3        \u001b[36m1.5659\u001b[0m        1.1479  0.1736\n",
      "      4        \u001b[36m2.0323\u001b[0m        1.8041  0.2227\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m1.6943\u001b[0m        \u001b[32m1.1095\u001b[0m  0.5436\n",
      "      4        \u001b[36m1.8922\u001b[0m        2.0242  0.1356\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.0545\u001b[0m        \u001b[32m1.6606\u001b[0m  0.6504\n",
      "      5        \u001b[36m1.9016\u001b[0m        1.7261  0.1826\n",
      "      4        \u001b[36m2.0757\u001b[0m        1.8408  0.1876\n",
      "      4        \u001b[36m1.5619\u001b[0m        1.1708  0.1838\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.2092\u001b[0m        \u001b[32m1.7501\u001b[0m  0.6633\n",
      "      5        \u001b[36m2.0313\u001b[0m        1.8087  0.1674\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m1.9648\u001b[0m        \u001b[32m1.8608\u001b[0m  0.7069\n",
      "      5        \u001b[36m1.8771\u001b[0m        2.0078  0.2256\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.1564\u001b[0m        \u001b[32m1.7416\u001b[0m  0.8449\n",
      "      5        \u001b[36m1.5397\u001b[0m        1.1919  0.1981\n",
      "      6        \u001b[36m1.8901\u001b[0m        1.7258  0.2300\n",
      "      5        \u001b[36m2.0496\u001b[0m        1.8393  0.2619\n",
      "      6        \u001b[36m1.8680\u001b[0m        1.9934  0.1423\n",
      "      6        \u001b[36m2.0209\u001b[0m        1.8124  0.2503\n",
      "      7        1.9008        1.7302  0.1584\n",
      "      6        \u001b[36m1.5373\u001b[0m        1.2019  0.1788\n",
      "      6        \u001b[36m2.0240\u001b[0m        1.8429  0.2051\n",
      "      7        \u001b[36m1.8457\u001b[0m        1.9848  0.2181\n",
      "      7        \u001b[36m2.0203\u001b[0m        1.8182  0.2447\n",
      "      8        \u001b[36m1.8750\u001b[0m        1.7300  0.1480\n",
      "      7        \u001b[36m1.5201\u001b[0m        1.2057  0.1557\n",
      "      2        \u001b[36m1.6050\u001b[0m        \u001b[32m1.1018\u001b[0m  0.6811\n",
      "      2        \u001b[36m1.9051\u001b[0m        \u001b[32m1.6280\u001b[0m  0.7108\n",
      "      8        \u001b[36m1.5165\u001b[0m        1.2074  0.1389\n",
      "      2        \u001b[36m2.1830\u001b[0m        1.7765  0.6694\n",
      "      9        \u001b[36m1.8666\u001b[0m        1.7323  0.1691\n",
      "      8        1.8523        1.9839  0.2157\n",
      "      7        2.0337        1.8509  0.2452\n",
      "      8        \u001b[36m2.0104\u001b[0m        1.8222  0.2048\n",
      "      2        \u001b[36m2.0848\u001b[0m        1.7620  0.5815\n",
      "      2        \u001b[36m1.8938\u001b[0m        1.9507  0.6907\n",
      "      9        1.5296        1.2062  0.1436\n",
      "      9        1.8531        1.9869  0.1458\n",
      "      8        \u001b[36m2.0135\u001b[0m        1.8635  0.1660\n",
      "     10        \u001b[36m1.8515\u001b[0m        1.7326  0.1794\n",
      "Restoring best model from epoch 1.\n",
      "      9        \u001b[36m1.9836\u001b[0m        1.8229  0.1630\n",
      "     10        1.5216        1.2043  0.1646\n",
      "Restoring best model from epoch 2.\n",
      "     10        \u001b[36m1.8276\u001b[0m        1.9920  0.1618\n",
      "Restoring best model from epoch 2.\n",
      "      9        \u001b[36m1.9900\u001b[0m        1.8704  0.1959\n",
      "     10        \u001b[36m1.9668\u001b[0m        1.8236  0.1976\n",
      "Restoring best model from epoch 2.\n",
      "      3        \u001b[36m1.5925\u001b[0m        1.1226  0.5574\n",
      "     10        \u001b[36m1.9798\u001b[0m        1.8675  0.1320\n",
      "      3        \u001b[36m1.8915\u001b[0m        1.6664  0.5491\n",
      "Restoring best model from epoch 2.\n",
      "      3        \u001b[36m2.1651\u001b[0m        1.7906  0.5300\n",
      "      3        \u001b[36m2.0494\u001b[0m        1.7678  0.4889\n",
      "      3        \u001b[36m1.8595\u001b[0m        1.9529  0.4955\n",
      "      4        \u001b[36m1.5579\u001b[0m        1.1390  0.4211\n",
      "      4        \u001b[36m1.8843\u001b[0m        1.6946  0.4191\n",
      "      4        \u001b[36m2.1573\u001b[0m        1.7763  0.4120\n",
      "      4        \u001b[36m2.0430\u001b[0m        1.7735  0.4151\n",
      "      4        1.8675        1.9731  0.4121\n",
      "      5        \u001b[36m1.5485\u001b[0m        1.1484  0.4098\n",
      "      5        \u001b[36m1.8571\u001b[0m        1.7033  0.4276\n",
      "      5        \u001b[36m2.1385\u001b[0m        1.8009  0.4263\n",
      "      5        \u001b[36m1.9865\u001b[0m        1.8041  0.4195\n",
      "      5        \u001b[36m1.8221\u001b[0m        1.9877  0.4163\n",
      "      6        \u001b[36m1.5316\u001b[0m        1.1559  0.4086\n",
      "      6        \u001b[36m1.8390\u001b[0m        1.7058  0.4205\n",
      "      6        \u001b[36m2.1211\u001b[0m        1.7925  0.4215\n",
      "      6        \u001b[36m1.9753\u001b[0m        1.8137  0.4231\n",
      "      6        \u001b[36m1.8111\u001b[0m        1.9956  0.4186\n",
      "      7        \u001b[36m1.5198\u001b[0m        1.1572  0.4086\n",
      "      7        \u001b[36m2.0944\u001b[0m        1.7825  0.4069\n",
      "      7        \u001b[36m1.8168\u001b[0m        1.7092  0.4164\n",
      "      7        \u001b[36m1.9741\u001b[0m        1.8317  0.4218\n",
      "      7        \u001b[36m1.8035\u001b[0m        2.0054  0.4255\n",
      "      8        \u001b[36m1.5178\u001b[0m        1.1598  0.4094\n",
      "      8        \u001b[36m2.0737\u001b[0m        1.7830  0.4269\n",
      "      8        \u001b[36m1.8004\u001b[0m        1.7264  0.4316\n",
      "      8        \u001b[36m1.7623\u001b[0m        2.0078  0.4138\n",
      "      8        \u001b[36m1.9425\u001b[0m        1.8492  0.4219\n",
      "      9        \u001b[36m1.4924\u001b[0m        1.1641  0.4130\n",
      "      9        \u001b[36m2.0728\u001b[0m        1.7860  0.4090\n",
      "      9        1.8023        1.7454  0.4087\n",
      "      9        1.9625        1.8657  0.4219\n",
      "      9        \u001b[36m1.7407\u001b[0m        2.0060  0.4250\n",
      "     10        \u001b[36m1.4862\u001b[0m        1.1724  0.4157\n",
      "Restoring best model from epoch 2.\n",
      "     10        \u001b[36m2.0347\u001b[0m        1.7871  0.4252\n",
      "     10        1.8033        1.7427  0.4185\n",
      "Restoring best model from epoch 1.\n",
      "Restoring best model from epoch 2.\n",
      "     10        \u001b[36m1.9372\u001b[0m        1.8720  0.4111\n",
      "Restoring best model from epoch 1.\n",
      "     10        \u001b[36m1.7282\u001b[0m        2.0146  0.4178\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.0687\u001b[0m        \u001b[32m1.4227\u001b[0m  0.1704\n",
      "      2        \u001b[36m1.9594\u001b[0m        1.4555  0.1454\n",
      "      3        \u001b[36m1.9543\u001b[0m        1.4675  0.1306\n",
      "      4        \u001b[36m1.9321\u001b[0m        1.4735  0.1406\n",
      "      5        \u001b[36m1.9036\u001b[0m        1.4723  0.1389\n",
      "      6        \u001b[36m1.8948\u001b[0m        1.4760  0.1460\n",
      "      7        \u001b[36m1.8768\u001b[0m        1.4699  0.1396\n",
      "      8        \u001b[36m1.8509\u001b[0m        1.4658  0.1358\n",
      "      9        \u001b[36m1.8319\u001b[0m        1.4657  0.1441\n",
      "     10        \u001b[36m1.8296\u001b[0m        1.4722  0.1414\n",
      "Restoring best model from epoch 1.\n",
      "y_train breslow final [-1.000e+00 -3.000e+00 -3.000e+00 -3.000e+00 -3.000e+00 -3.000e+00\n",
      " -3.000e+00 -3.000e+00 -4.000e+00 -4.000e+00 -4.000e+00 -5.000e+00\n",
      " -7.000e+00 -7.000e+00 -7.000e+00 -7.000e+00 -7.000e+00  7.000e+00\n",
      " -8.000e+00 -1.000e+01 -1.400e+01 -1.500e+01 -1.700e+01  2.300e+01\n",
      " -2.300e+01  3.700e+01 -5.000e+01 -5.500e+01 -5.800e+01 -6.200e+01\n",
      " -6.200e+01 -6.300e+01 -7.000e+01 -7.200e+01 -7.200e+01 -7.300e+01\n",
      " -7.600e+01 -7.700e+01 -8.200e+01 -8.400e+01 -9.000e+01  9.600e+01\n",
      " -1.050e+02 -1.120e+02  1.130e+02 -1.140e+02 -1.220e+02 -1.340e+02\n",
      " -1.390e+02 -1.530e+02  1.550e+02 -1.620e+02  1.620e+02 -1.690e+02\n",
      " -1.740e+02 -1.790e+02 -1.820e+02 -1.840e+02 -1.940e+02  1.940e+02\n",
      "  1.990e+02 -1.990e+02 -2.030e+02 -2.040e+02  2.050e+02 -2.070e+02\n",
      " -2.100e+02  2.140e+02  2.280e+02 -2.310e+02  2.340e+02 -2.420e+02\n",
      "  2.420e+02 -2.420e+02 -2.430e+02  2.450e+02 -2.490e+02 -2.570e+02\n",
      "  2.690e+02 -2.740e+02 -2.790e+02 -2.860e+02 -2.870e+02 -3.010e+02\n",
      " -3.130e+02  3.150e+02  3.160e+02 -3.170e+02 -3.200e+02 -3.260e+02\n",
      " -3.280e+02 -3.330e+02 -3.420e+02  3.470e+02  3.490e+02  3.510e+02\n",
      " -3.540e+02  3.540e+02 -3.680e+02 -3.720e+02  3.720e+02 -3.720e+02\n",
      "  3.780e+02 -3.840e+02  3.880e+02 -3.950e+02  3.980e+02 -4.030e+02\n",
      " -4.050e+02 -4.070e+02 -4.080e+02 -4.110e+02 -4.140e+02 -4.140e+02\n",
      " -4.160e+02 -4.180e+02 -4.270e+02 -4.280e+02 -4.330e+02 -4.340e+02\n",
      "  4.350e+02 -4.380e+02 -4.380e+02 -4.420e+02 -4.420e+02 -4.420e+02\n",
      " -4.430e+02  4.440e+02 -4.490e+02 -4.510e+02 -4.550e+02  4.560e+02\n",
      " -4.580e+02 -4.610e+02 -4.610e+02  4.660e+02 -4.670e+02 -4.670e+02\n",
      " -4.710e+02 -4.780e+02 -4.820e+02 -4.860e+02 -4.870e+02  4.920e+02\n",
      " -4.940e+02 -4.970e+02 -5.000e+02 -5.030e+02 -5.090e+02 -5.120e+02\n",
      " -5.150e+02 -5.160e+02 -5.220e+02 -5.230e+02 -5.230e+02 -5.300e+02\n",
      " -5.310e+02 -5.320e+02 -5.330e+02 -5.330e+02  5.370e+02 -5.420e+02\n",
      " -5.440e+02  5.470e+02 -5.480e+02  5.590e+02 -5.630e+02 -5.640e+02\n",
      " -5.660e+02 -5.660e+02 -5.670e+02 -5.670e+02 -5.690e+02 -5.690e+02\n",
      " -5.710e+02 -5.710e+02 -5.730e+02 -5.760e+02  5.780e+02 -5.820e+02\n",
      " -5.840e+02  5.920e+02 -5.960e+02 -5.990e+02 -6.040e+02  6.050e+02\n",
      "  6.070e+02 -6.080e+02 -6.090e+02 -6.090e+02 -6.110e+02 -6.110e+02\n",
      " -6.230e+02 -6.270e+02 -6.330e+02 -6.380e+02  6.390e+02  6.480e+02\n",
      "  6.480e+02 -6.500e+02 -6.510e+02 -6.510e+02 -6.550e+02 -6.560e+02\n",
      " -6.710e+02  6.820e+02 -6.850e+02 -6.860e+02 -7.060e+02 -7.060e+02\n",
      "  7.090e+02 -7.140e+02 -7.150e+02 -7.210e+02 -7.240e+02  7.270e+02\n",
      " -7.350e+02 -7.360e+02 -7.380e+02  7.420e+02 -7.430e+02 -7.480e+02\n",
      " -7.540e+02  7.580e+02 -7.580e+02 -7.600e+02 -7.720e+02  7.750e+02\n",
      " -7.770e+02 -7.850e+02 -7.870e+02  7.880e+02 -7.920e+02 -7.950e+02\n",
      " -7.960e+02 -8.000e+02 -8.050e+02  8.140e+02  8.140e+02  8.190e+02\n",
      "  8.210e+02 -8.260e+02 -8.320e+02 -8.350e+02 -8.370e+02 -8.460e+02\n",
      " -8.540e+02 -8.600e+02 -8.630e+02 -8.680e+02 -8.780e+02  8.860e+02\n",
      " -9.000e+02 -9.080e+02 -9.080e+02 -9.140e+02 -9.140e+02 -9.160e+02\n",
      "  9.190e+02 -9.190e+02 -9.260e+02  9.330e+02 -9.330e+02 -9.350e+02\n",
      " -9.460e+02 -9.490e+02 -9.550e+02  9.610e+02  9.620e+02 -9.640e+02\n",
      " -9.640e+02 -9.680e+02  9.840e+02  9.870e+02 -9.920e+02 -1.004e+03\n",
      " -1.012e+03 -1.026e+03 -1.032e+03  1.033e+03 -1.040e+03 -1.058e+03\n",
      " -1.069e+03 -1.076e+03 -1.078e+03 -1.079e+03 -1.099e+03  1.106e+03\n",
      " -1.112e+03 -1.115e+03 -1.120e+03  1.120e+03 -1.127e+03 -1.130e+03\n",
      "  1.137e+03 -1.139e+03  1.152e+03 -1.173e+03 -1.189e+03 -1.191e+03\n",
      " -1.201e+03 -1.201e+03 -1.210e+03 -1.213e+03 -1.217e+03 -1.219e+03\n",
      "  1.220e+03 -1.222e+03 -1.227e+03 -1.229e+03 -1.236e+03  1.242e+03\n",
      " -1.245e+03  1.251e+03 -1.257e+03  1.262e+03 -1.279e+03 -1.294e+03\n",
      " -1.300e+03 -1.314e+03 -1.320e+03  1.335e+03  1.351e+03 -1.354e+03\n",
      " -1.359e+03 -1.382e+03 -1.387e+03 -1.393e+03 -1.397e+03 -1.401e+03\n",
      " -1.412e+03 -1.421e+03 -1.426e+03 -1.426e+03 -1.428e+03 -1.453e+03\n",
      " -1.469e+03 -1.470e+03  1.481e+03  1.491e+03 -1.519e+03  1.525e+03\n",
      " -1.540e+03  1.547e+03 -1.553e+03 -1.568e+03  1.578e+03  1.585e+03\n",
      " -1.588e+03 -1.631e+03 -1.650e+03  1.666e+03 -1.721e+03 -1.752e+03\n",
      "  1.762e+03 -1.806e+03 -1.828e+03 -1.834e+03 -1.868e+03  1.886e+03\n",
      "  1.915e+03 -1.943e+03 -1.989e+03  2.052e+03 -2.078e+03 -2.107e+03\n",
      " -2.218e+03 -2.219e+03  2.235e+03  2.282e+03  2.286e+03 -2.289e+03\n",
      "  2.379e+03 -2.381e+03  2.433e+03 -2.493e+03 -2.585e+03 -2.650e+03\n",
      "  2.660e+03 -2.772e+03 -2.860e+03 -2.869e+03  2.875e+03 -2.893e+03\n",
      "  2.907e+03 -2.918e+03 -2.918e+03  2.988e+03 -3.000e+03 -3.013e+03\n",
      "  3.200e+03 -3.253e+03  3.470e+03  3.571e+03 -3.574e+03 -3.725e+03\n",
      " -3.733e+03 -3.761e+03  3.978e+03  4.068e+03  4.084e+03 -4.113e+03\n",
      "  4.412e+03  4.445e+03  4.695e+03 -4.752e+03  5.166e+03 -5.546e+03\n",
      " -6.423e+03]\n",
      "Concordance Index 0.1631322295305058\n",
      "Integrated Brier Score: inf\n",
      "y_train breslow final [-1.000e+00 -3.000e+00 -3.000e+00 -3.000e+00 -3.000e+00 -3.000e+00\n",
      " -3.000e+00 -3.000e+00 -4.000e+00 -4.000e+00 -4.000e+00 -5.000e+00\n",
      " -7.000e+00 -7.000e+00 -7.000e+00 -7.000e+00 -7.000e+00  7.000e+00\n",
      " -8.000e+00 -1.000e+01 -1.400e+01 -1.500e+01 -1.700e+01  2.300e+01\n",
      " -2.300e+01  3.700e+01 -5.000e+01 -5.500e+01 -5.800e+01 -6.200e+01\n",
      " -6.200e+01 -6.300e+01 -7.000e+01 -7.200e+01 -7.200e+01 -7.300e+01\n",
      " -7.600e+01 -7.700e+01 -8.200e+01 -8.400e+01 -9.000e+01  9.600e+01\n",
      " -1.050e+02 -1.120e+02  1.130e+02 -1.140e+02 -1.220e+02 -1.340e+02\n",
      " -1.390e+02 -1.530e+02  1.550e+02 -1.620e+02  1.620e+02 -1.690e+02\n",
      " -1.740e+02 -1.790e+02 -1.820e+02 -1.840e+02 -1.940e+02  1.940e+02\n",
      "  1.990e+02 -1.990e+02 -2.030e+02 -2.040e+02  2.050e+02 -2.070e+02\n",
      " -2.100e+02  2.140e+02  2.280e+02 -2.310e+02  2.340e+02 -2.420e+02\n",
      "  2.420e+02 -2.420e+02 -2.430e+02  2.450e+02 -2.490e+02 -2.570e+02\n",
      "  2.690e+02 -2.740e+02 -2.790e+02 -2.860e+02 -2.870e+02 -3.010e+02\n",
      " -3.130e+02  3.150e+02  3.160e+02 -3.170e+02 -3.200e+02 -3.260e+02\n",
      " -3.280e+02 -3.330e+02 -3.420e+02  3.470e+02  3.490e+02  3.510e+02\n",
      " -3.540e+02  3.540e+02 -3.680e+02 -3.720e+02  3.720e+02 -3.720e+02\n",
      "  3.780e+02 -3.840e+02  3.880e+02 -3.950e+02  3.980e+02 -4.030e+02\n",
      " -4.050e+02 -4.070e+02 -4.080e+02 -4.110e+02 -4.140e+02 -4.140e+02\n",
      " -4.160e+02 -4.180e+02 -4.270e+02 -4.280e+02 -4.330e+02 -4.340e+02\n",
      "  4.350e+02 -4.380e+02 -4.380e+02 -4.420e+02 -4.420e+02 -4.420e+02\n",
      " -4.430e+02  4.440e+02 -4.490e+02 -4.510e+02 -4.550e+02  4.560e+02\n",
      " -4.580e+02 -4.610e+02 -4.610e+02  4.660e+02 -4.670e+02 -4.670e+02\n",
      " -4.710e+02 -4.780e+02 -4.820e+02 -4.860e+02 -4.870e+02  4.920e+02\n",
      " -4.940e+02 -4.970e+02 -5.000e+02 -5.030e+02 -5.090e+02 -5.120e+02\n",
      " -5.150e+02 -5.160e+02 -5.220e+02 -5.230e+02 -5.230e+02 -5.300e+02\n",
      " -5.310e+02 -5.320e+02 -5.330e+02 -5.330e+02  5.370e+02 -5.420e+02\n",
      " -5.440e+02  5.470e+02 -5.480e+02  5.590e+02 -5.630e+02 -5.640e+02\n",
      " -5.660e+02 -5.660e+02 -5.670e+02 -5.670e+02 -5.690e+02 -5.690e+02\n",
      " -5.710e+02 -5.710e+02 -5.730e+02 -5.760e+02  5.780e+02 -5.820e+02\n",
      " -5.840e+02  5.920e+02 -5.960e+02 -5.990e+02 -6.040e+02  6.050e+02\n",
      "  6.070e+02 -6.080e+02 -6.090e+02 -6.090e+02 -6.110e+02 -6.110e+02\n",
      " -6.230e+02 -6.270e+02 -6.330e+02 -6.380e+02  6.390e+02  6.480e+02\n",
      "  6.480e+02 -6.500e+02 -6.510e+02 -6.510e+02 -6.550e+02 -6.560e+02\n",
      " -6.710e+02  6.820e+02 -6.850e+02 -6.860e+02 -7.060e+02 -7.060e+02\n",
      "  7.090e+02 -7.140e+02 -7.150e+02 -7.210e+02 -7.240e+02  7.270e+02\n",
      " -7.350e+02 -7.360e+02 -7.380e+02  7.420e+02 -7.430e+02 -7.480e+02\n",
      " -7.540e+02  7.580e+02 -7.580e+02 -7.600e+02 -7.720e+02  7.750e+02\n",
      " -7.770e+02 -7.850e+02 -7.870e+02  7.880e+02 -7.920e+02 -7.950e+02\n",
      " -7.960e+02 -8.000e+02 -8.050e+02  8.140e+02  8.140e+02  8.190e+02\n",
      "  8.210e+02 -8.260e+02 -8.320e+02 -8.350e+02 -8.370e+02 -8.460e+02\n",
      " -8.540e+02 -8.600e+02 -8.630e+02 -8.680e+02 -8.780e+02  8.860e+02\n",
      " -9.000e+02 -9.080e+02 -9.080e+02 -9.140e+02 -9.140e+02 -9.160e+02\n",
      "  9.190e+02 -9.190e+02 -9.260e+02  9.330e+02 -9.330e+02 -9.350e+02\n",
      " -9.460e+02 -9.490e+02 -9.550e+02  9.610e+02  9.620e+02 -9.640e+02\n",
      " -9.640e+02 -9.680e+02  9.840e+02  9.870e+02 -9.920e+02 -1.004e+03\n",
      " -1.012e+03 -1.026e+03 -1.032e+03  1.033e+03 -1.040e+03 -1.058e+03\n",
      " -1.069e+03 -1.076e+03 -1.078e+03 -1.079e+03 -1.099e+03  1.106e+03\n",
      " -1.112e+03 -1.115e+03 -1.120e+03  1.120e+03 -1.127e+03 -1.130e+03\n",
      "  1.137e+03 -1.139e+03  1.152e+03 -1.173e+03 -1.189e+03 -1.191e+03\n",
      " -1.201e+03 -1.201e+03 -1.210e+03 -1.213e+03 -1.217e+03 -1.219e+03\n",
      "  1.220e+03 -1.222e+03 -1.227e+03 -1.229e+03 -1.236e+03  1.242e+03\n",
      " -1.245e+03  1.251e+03 -1.257e+03  1.262e+03 -1.279e+03 -1.294e+03\n",
      " -1.300e+03 -1.314e+03 -1.320e+03  1.335e+03  1.351e+03 -1.354e+03\n",
      " -1.359e+03 -1.382e+03 -1.387e+03 -1.393e+03 -1.397e+03 -1.401e+03\n",
      " -1.412e+03 -1.421e+03 -1.426e+03 -1.426e+03 -1.428e+03 -1.453e+03\n",
      " -1.469e+03 -1.470e+03  1.481e+03  1.491e+03 -1.519e+03  1.525e+03\n",
      " -1.540e+03  1.547e+03 -1.553e+03 -1.568e+03  1.578e+03  1.585e+03\n",
      " -1.588e+03 -1.631e+03 -1.650e+03  1.666e+03 -1.721e+03 -1.752e+03\n",
      "  1.762e+03 -1.806e+03 -1.828e+03 -1.834e+03 -1.868e+03  1.886e+03\n",
      "  1.915e+03 -1.943e+03 -1.989e+03  2.052e+03 -2.078e+03 -2.107e+03\n",
      " -2.218e+03 -2.219e+03  2.235e+03  2.282e+03  2.286e+03 -2.289e+03\n",
      "  2.379e+03 -2.381e+03  2.433e+03 -2.493e+03 -2.585e+03 -2.650e+03\n",
      "  2.660e+03 -2.772e+03 -2.860e+03 -2.869e+03  2.875e+03 -2.893e+03\n",
      "  2.907e+03 -2.918e+03 -2.918e+03  2.988e+03 -3.000e+03 -3.013e+03\n",
      "  3.200e+03 -3.253e+03  3.470e+03  3.571e+03 -3.574e+03 -3.725e+03\n",
      " -3.733e+03 -3.761e+03  3.978e+03  4.068e+03  4.084e+03 -4.113e+03\n",
      "  4.412e+03  4.445e+03  4.695e+03 -4.752e+03  5.166e+03 -5.546e+03\n",
      " -6.423e+03]\n",
      "durations 2.0 5255.0\n",
      "Concordance Index 0.14236410698878343\n",
      "Integrated Brier Score: 0.5172522980415964\n",
      "(409, 20531) <class 'pandas.core.frame.DataFrame'>\n",
      "(409,) <class 'pandas.core.series.Series'>\n",
      "(102, 20531) <class 'pandas.core.frame.DataFrame'>\n",
      "(102,) <class 'pandas.core.series.Series'>\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m1.7524\u001b[0m        \u001b[32m1.0861\u001b[0m  0.1374\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.0921\u001b[0m        \u001b[32m1.7630\u001b[0m  0.1890\n",
      "      2        \u001b[36m1.6597\u001b[0m        1.0878  0.1410\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.1519\u001b[0m        \u001b[32m1.9581\u001b[0m  0.3163\n",
      "      2        \u001b[36m2.0648\u001b[0m        \u001b[32m1.7073\u001b[0m  0.1424\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      3        \u001b[36m1.6321\u001b[0m        1.1079  0.1688\n",
      "      2        \u001b[36m2.1448\u001b[0m        \u001b[32m1.9241\u001b[0m  0.1701\n",
      "      3        \u001b[36m2.0428\u001b[0m        \u001b[32m1.7071\u001b[0m  0.1604\n",
      "      3        \u001b[36m2.1259\u001b[0m        1.9396  0.1534\n",
      "      4        \u001b[36m1.6297\u001b[0m        1.1074  0.1793\n",
      "      4        \u001b[36m2.0296\u001b[0m        1.7130  0.1520\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m1.8779\u001b[0m        \u001b[32m2.0904\u001b[0m  0.5680\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.0258\u001b[0m        \u001b[32m2.0684\u001b[0m  0.6095\n",
      "      4        \u001b[36m2.1133\u001b[0m        1.9381  0.1279\n",
      "      5        \u001b[36m1.6075\u001b[0m        1.1064  0.1486\n",
      "      5        \u001b[36m1.9864\u001b[0m        1.7093  0.1335\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m1.7374\u001b[0m        \u001b[32m1.2188\u001b[0m  0.6885\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.1259\u001b[0m        \u001b[32m1.9325\u001b[0m  0.7529\n",
      "      6        \u001b[36m1.5860\u001b[0m        1.1094  0.1431\n",
      "      6        1.9907        \u001b[32m1.6944\u001b[0m  0.1402\n",
      "      5        2.1172        1.9393  0.1822\n",
      "      7        1.5969        1.1154  0.1536\n",
      "      6        2.1175        1.9477  0.1520\n",
      "      7        \u001b[36m1.9709\u001b[0m        \u001b[32m1.6821\u001b[0m  0.1632\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.0244\u001b[0m        \u001b[32m1.5021\u001b[0m  0.6907\n",
      "      2        \u001b[36m1.8052\u001b[0m        2.1144  0.4907\n",
      "      8        \u001b[36m1.5760\u001b[0m        1.1197  0.1397\n",
      "      8        1.9732        \u001b[32m1.6719\u001b[0m  0.1388\n",
      "      2        \u001b[36m1.9710\u001b[0m        \u001b[32m2.0676\u001b[0m  0.5197\n",
      "      7        \u001b[36m2.1119\u001b[0m        1.9597  0.1541\n",
      "      9        \u001b[36m1.9442\u001b[0m        \u001b[32m1.6660\u001b[0m  0.1350\n",
      "      9        \u001b[36m1.5735\u001b[0m        1.1267  0.1724\n",
      "      2        \u001b[36m1.6900\u001b[0m        \u001b[32m1.1911\u001b[0m  0.5497\n",
      "      8        \u001b[36m2.0850\u001b[0m        1.9715  0.1567\n",
      "      2        \u001b[36m2.0882\u001b[0m        \u001b[32m1.9181\u001b[0m  0.5952\n",
      "     10        \u001b[36m1.9262\u001b[0m        \u001b[32m1.6614\u001b[0m  0.1403\n",
      "     10        1.5781        1.1307  0.1304\n",
      "Restoring best model from epoch 1.\n",
      "      9        \u001b[36m2.0848\u001b[0m        1.9807  0.1342\n",
      "      3        \u001b[36m1.7937\u001b[0m        \u001b[32m2.0790\u001b[0m  0.4922\n",
      "      2        \u001b[36m1.9527\u001b[0m        1.5313  0.5390\n",
      "     10        2.1072        1.9845  0.1464\n",
      "Restoring best model from epoch 2.\n",
      "      3        \u001b[36m1.9546\u001b[0m        \u001b[32m2.0640\u001b[0m  0.4797\n",
      "      3        \u001b[36m1.6776\u001b[0m        1.2190  0.4539\n",
      "      3        \u001b[36m2.0690\u001b[0m        1.9464  0.4878\n",
      "      4        \u001b[36m1.7791\u001b[0m        2.0930  0.4292\n",
      "      3        \u001b[36m1.9165\u001b[0m        1.5475  0.4344\n",
      "      4        \u001b[36m1.9225\u001b[0m        2.0741  0.4145\n",
      "      4        \u001b[36m1.6710\u001b[0m        1.2110  0.4141\n",
      "      4        \u001b[36m2.0525\u001b[0m        1.9813  0.4231\n",
      "      5        \u001b[36m1.7702\u001b[0m        2.0989  0.4298\n",
      "      4        1.9209        1.5660  0.4299\n",
      "      5        \u001b[36m1.8896\u001b[0m        2.0843  0.4202\n",
      "      5        \u001b[36m1.6433\u001b[0m        1.1992  0.4297\n",
      "      5        \u001b[36m2.0350\u001b[0m        2.0102  0.4226\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      6        \u001b[36m1.7399\u001b[0m        2.1033  0.4305\n",
      "      5        \u001b[36m1.8679\u001b[0m        1.5834  0.4241\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      6        \u001b[36m1.8650\u001b[0m        2.0947  0.4216\n",
      "      6        \u001b[36m1.6260\u001b[0m        1.2012  0.4304\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m1.9258\u001b[0m        \u001b[32m2.2083\u001b[0m  0.2186\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.0359\u001b[0m        \u001b[32m2.0482\u001b[0m  0.2180\n",
      "      6        \u001b[36m2.0207\u001b[0m        2.0093  0.4492\n",
      "      2        \u001b[36m1.8832\u001b[0m        \u001b[32m2.1689\u001b[0m  0.1450\n",
      "      2        \u001b[36m1.9463\u001b[0m        2.0636  0.1392\n",
      "      7        \u001b[36m1.7384\u001b[0m        2.1207  0.4587\n",
      "      6        1.8710        1.6002  0.4574\n",
      "      7        \u001b[36m1.8487\u001b[0m        2.1002  0.4488\n",
      "      3        1.9605        \u001b[32m2.0466\u001b[0m  0.1319\n",
      "      3        \u001b[36m1.8378\u001b[0m        \u001b[32m2.1645\u001b[0m  0.1354\n",
      "      4        \u001b[36m1.9120\u001b[0m        2.0572  0.1183\n",
      "      4        \u001b[36m1.8200\u001b[0m        \u001b[32m2.1637\u001b[0m  0.1169\n",
      "      7        \u001b[36m1.6154\u001b[0m        1.1990  0.4472\n",
      "      7        \u001b[36m1.9878\u001b[0m        2.0194  0.4275\n",
      "      5        \u001b[36m1.7915\u001b[0m        2.1745  0.1283\n",
      "      5        \u001b[36m1.9089\u001b[0m        2.0845  0.1292\n",
      "      6        \u001b[36m1.8909\u001b[0m        2.1151  0.1224\n",
      "      6        \u001b[36m1.7761\u001b[0m        \u001b[32m2.1569\u001b[0m  0.1258\n",
      "      7        1.8680        1.5977  0.4262\n",
      "      8        1.7422        2.1294  0.4405\n",
      "      8        \u001b[36m1.8331\u001b[0m        2.1058  0.4232\n",
      "      7        \u001b[36m1.8896\u001b[0m        2.1427  0.1207\n",
      "      7        \u001b[36m1.7648\u001b[0m        \u001b[32m2.1468\u001b[0m  0.1201\n",
      "      8        \u001b[36m1.5783\u001b[0m        1.2115  0.4386\n",
      "      8        \u001b[36m1.8711\u001b[0m        2.1612  0.1273\n",
      "      8        \u001b[36m1.7464\u001b[0m        2.1523  0.1260\n",
      "      8        \u001b[36m1.9861\u001b[0m        2.0270  0.4417\n",
      "      9        1.8715        2.1643  0.1152\n",
      "      9        \u001b[36m1.7242\u001b[0m        2.1527  0.1159\n",
      "      8        \u001b[36m1.8446\u001b[0m        1.5930  0.4417\n",
      "      9        \u001b[36m1.7237\u001b[0m        2.1085  0.4508\n",
      "      9        \u001b[36m1.8187\u001b[0m        2.1071  0.4415\n",
      "     10        1.8721        2.1587  0.1264\n",
      "Restoring best model from epoch 3.\n",
      "     10        \u001b[36m1.7152\u001b[0m        2.1512  0.1278\n",
      "Restoring best model from epoch 7.\n",
      "      9        \u001b[36m1.5745\u001b[0m        1.2432  0.4540\n",
      "      9        \u001b[36m1.9629\u001b[0m        2.0056  0.4464\n",
      "      9        1.8479        1.5991  0.4076\n",
      "     10        \u001b[36m1.7064\u001b[0m        2.1063  0.4501\n",
      "     10        \u001b[36m1.8089\u001b[0m        2.1089  0.4483\n",
      "Restoring best model from epoch 3.\n",
      "Restoring best model from epoch 3.\n",
      "     10        \u001b[36m1.5641\u001b[0m        1.2637  0.4034\n",
      "Restoring best model from epoch 2.\n",
      "     10        \u001b[36m1.9233\u001b[0m        2.0101  0.4065\n",
      "Restoring best model from epoch 2.\n",
      "     10        \u001b[36m1.8391\u001b[0m        1.6117  0.4001\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.0240\u001b[0m        \u001b[32m1.7267\u001b[0m  0.2319\n",
      "      2        \u001b[36m1.9460\u001b[0m        \u001b[32m1.6982\u001b[0m  0.1382\n",
      "      3        \u001b[36m1.9365\u001b[0m        \u001b[32m1.6840\u001b[0m  0.1426\n",
      "      4        \u001b[36m1.9363\u001b[0m        \u001b[32m1.6648\u001b[0m  0.1359\n",
      "      5        1.9492        1.7074  0.1543\n",
      "      6        1.9450        1.6807  0.1320\n",
      "      7        \u001b[36m1.9160\u001b[0m        \u001b[32m1.6561\u001b[0m  0.1347\n",
      "      8        \u001b[36m1.8786\u001b[0m        \u001b[32m1.6552\u001b[0m  0.1385\n",
      "      9        \u001b[36m1.8685\u001b[0m        1.6701  0.1395\n",
      "     10        \u001b[36m1.8381\u001b[0m        1.6973  0.1330\n",
      "Restoring best model from epoch 8.\n",
      "y_train breslow final [-2.000e+00 -3.000e+00 -3.000e+00 -3.000e+00 -3.000e+00 -3.000e+00\n",
      " -3.000e+00 -3.000e+00 -3.000e+00 -4.000e+00 -4.000e+00 -4.000e+00\n",
      " -5.000e+00 -6.000e+00 -7.000e+00 -7.000e+00 -7.000e+00 -7.000e+00\n",
      " -7.000e+00 -8.000e+00 -1.000e+01 -1.400e+01 -1.500e+01 -1.700e+01\n",
      "  2.300e+01 -2.300e+01 -3.900e+01 -5.000e+01 -5.800e+01 -6.200e+01\n",
      " -6.200e+01 -6.300e+01 -7.000e+01 -7.200e+01 -7.200e+01 -7.400e+01\n",
      " -7.600e+01 -7.700e+01 -8.400e+01 -9.000e+01  9.600e+01 -1.050e+02\n",
      "  1.110e+02  1.130e+02 -1.140e+02 -1.220e+02 -1.340e+02 -1.530e+02\n",
      "  1.550e+02 -1.620e+02  1.620e+02 -1.660e+02 -1.690e+02 -1.740e+02\n",
      "  1.780e+02 -1.900e+02 -1.940e+02  1.940e+02 -1.990e+02 -2.040e+02\n",
      "  2.050e+02 -2.070e+02 -2.100e+02 -2.300e+02 -2.310e+02  2.400e+02\n",
      "  2.410e+02  2.420e+02 -2.420e+02 -2.430e+02  2.450e+02 -2.490e+02\n",
      " -2.570e+02  2.690e+02 -2.740e+02 -2.860e+02 -2.870e+02 -2.920e+02\n",
      "  3.150e+02  3.160e+02 -3.170e+02 -3.200e+02 -3.280e+02 -3.330e+02\n",
      " -3.360e+02 -3.370e+02 -3.430e+02  3.470e+02  3.510e+02  3.540e+02\n",
      " -3.680e+02  3.720e+02 -3.720e+02  3.780e+02 -3.840e+02  3.880e+02\n",
      " -3.950e+02  3.980e+02 -4.030e+02 -4.050e+02 -4.070e+02 -4.080e+02\n",
      " -4.140e+02 -4.160e+02 -4.170e+02 -4.190e+02 -4.270e+02 -4.280e+02\n",
      " -4.310e+02 -4.330e+02  4.350e+02  4.380e+02 -4.380e+02 -4.420e+02\n",
      " -4.420e+02 -4.430e+02  4.440e+02 -4.510e+02 -4.540e+02 -4.540e+02\n",
      " -4.550e+02 -4.550e+02  4.560e+02 -4.570e+02 -4.610e+02  4.660e+02\n",
      " -4.670e+02 -4.670e+02 -4.780e+02 -4.860e+02 -4.870e+02 -4.870e+02\n",
      " -4.910e+02  4.920e+02 -4.920e+02  4.920e+02 -4.940e+02 -4.940e+02\n",
      " -4.970e+02 -5.000e+02 -5.020e+02 -5.030e+02 -5.080e+02  5.120e+02\n",
      " -5.120e+02 -5.150e+02 -5.160e+02 -5.220e+02 -5.230e+02 -5.230e+02\n",
      " -5.260e+02 -5.300e+02  5.310e+02 -5.310e+02 -5.320e+02 -5.330e+02\n",
      " -5.330e+02  5.370e+02  5.380e+02 -5.420e+02 -5.440e+02 -5.440e+02\n",
      "  5.470e+02 -5.480e+02  5.590e+02  5.610e+02 -5.630e+02 -5.640e+02\n",
      " -5.660e+02 -5.660e+02 -5.660e+02 -5.670e+02 -5.670e+02 -5.690e+02\n",
      " -5.690e+02 -5.710e+02 -5.710e+02 -5.730e+02  5.760e+02 -5.760e+02\n",
      " -5.820e+02 -5.840e+02 -5.850e+02 -5.880e+02  5.920e+02 -5.960e+02\n",
      " -5.990e+02 -6.040e+02  6.070e+02 -6.080e+02 -6.090e+02 -6.150e+02\n",
      " -6.220e+02 -6.230e+02 -6.270e+02 -6.290e+02 -6.290e+02 -6.300e+02\n",
      " -6.330e+02 -6.380e+02  6.390e+02  6.480e+02  6.480e+02 -6.500e+02\n",
      " -6.510e+02 -6.550e+02 -6.560e+02 -6.710e+02 -6.770e+02 -6.780e+02\n",
      "  6.820e+02 -6.850e+02 -7.050e+02  7.090e+02 -7.150e+02 -7.180e+02\n",
      " -7.210e+02  7.220e+02 -7.240e+02  7.270e+02 -7.350e+02 -7.360e+02\n",
      " -7.430e+02 -7.480e+02 -7.540e+02  7.580e+02 -7.580e+02 -7.720e+02\n",
      "  7.750e+02 -7.770e+02 -7.870e+02 -7.920e+02 -8.000e+02 -8.050e+02\n",
      "  8.140e+02 -8.320e+02 -8.350e+02 -8.460e+02 -8.460e+02 -8.540e+02\n",
      " -8.540e+02 -8.600e+02 -8.620e+02 -8.630e+02 -8.780e+02  8.860e+02\n",
      " -8.890e+02 -9.000e+02 -9.080e+02 -9.080e+02 -9.080e+02 -9.140e+02\n",
      " -9.140e+02 -9.160e+02  9.190e+02 -9.190e+02 -9.260e+02  9.330e+02\n",
      " -9.330e+02 -9.390e+02 -9.460e+02 -9.490e+02  9.540e+02 -9.550e+02\n",
      " -9.560e+02  9.610e+02 -9.620e+02  9.620e+02 -9.640e+02 -9.680e+02\n",
      "  9.870e+02 -9.920e+02 -9.920e+02 -9.930e+02  1.011e+03 -1.012e+03\n",
      " -1.021e+03 -1.032e+03  1.033e+03 -1.040e+03 -1.069e+03 -1.076e+03\n",
      " -1.078e+03 -1.079e+03 -1.099e+03  1.106e+03 -1.112e+03 -1.115e+03\n",
      " -1.116e+03 -1.120e+03  1.120e+03 -1.130e+03  1.137e+03 -1.137e+03\n",
      " -1.139e+03 -1.147e+03  1.152e+03 -1.164e+03 -1.173e+03  1.183e+03\n",
      " -1.191e+03 -1.201e+03 -1.201e+03  1.209e+03 -1.210e+03 -1.213e+03\n",
      " -1.217e+03 -1.219e+03  1.220e+03 -1.222e+03 -1.227e+03  1.242e+03\n",
      " -1.245e+03 -1.250e+03  1.251e+03  1.262e+03 -1.277e+03 -1.279e+03\n",
      " -1.294e+03 -1.294e+03 -1.301e+03 -1.337e+03  1.339e+03 -1.341e+03\n",
      " -1.354e+03 -1.359e+03 -1.382e+03 -1.387e+03 -1.393e+03 -1.399e+03\n",
      "  1.401e+03 -1.401e+03 -1.412e+03 -1.421e+03 -1.426e+03 -1.426e+03\n",
      " -1.428e+03 -1.453e+03 -1.458e+03 -1.469e+03 -1.470e+03  1.481e+03\n",
      "  1.491e+03 -1.494e+03 -1.500e+03 -1.519e+03  1.525e+03  1.547e+03\n",
      " -1.553e+03 -1.567e+03 -1.568e+03  1.578e+03  1.585e+03 -1.650e+03\n",
      " -1.706e+03  1.762e+03 -1.796e+03 -1.806e+03 -1.828e+03 -1.834e+03\n",
      " -1.868e+03  1.886e+03  1.891e+03  1.915e+03  1.933e+03 -1.989e+03\n",
      "  2.000e+03  2.052e+03 -2.078e+03 -2.107e+03 -2.218e+03  2.235e+03\n",
      "  2.286e+03 -2.287e+03 -2.289e+03 -2.289e+03  2.433e+03 -2.493e+03\n",
      " -2.565e+03 -2.585e+03 -2.602e+03  2.660e+03 -2.702e+03 -2.761e+03\n",
      " -2.772e+03  2.835e+03 -2.860e+03 -2.869e+03  2.907e+03 -2.918e+03\n",
      " -3.000e+03  3.200e+03 -3.253e+03  3.470e+03  3.571e+03 -3.574e+03\n",
      " -3.733e+03 -3.761e+03  3.978e+03  4.068e+03  4.084e+03 -4.113e+03\n",
      "  4.229e+03  4.445e+03  4.695e+03 -4.752e+03  5.166e+03 -5.255e+03\n",
      " -6.423e+03]\n",
      "Concordance Index 0.3374816155673719\n",
      "Integrated Brier Score: 0.5257463542895678\n",
      "y_train breslow final [-2.000e+00 -3.000e+00 -3.000e+00 -3.000e+00 -3.000e+00 -3.000e+00\n",
      " -3.000e+00 -3.000e+00 -3.000e+00 -4.000e+00 -4.000e+00 -4.000e+00\n",
      " -5.000e+00 -6.000e+00 -7.000e+00 -7.000e+00 -7.000e+00 -7.000e+00\n",
      " -7.000e+00 -8.000e+00 -1.000e+01 -1.400e+01 -1.500e+01 -1.700e+01\n",
      "  2.300e+01 -2.300e+01 -3.900e+01 -5.000e+01 -5.800e+01 -6.200e+01\n",
      " -6.200e+01 -6.300e+01 -7.000e+01 -7.200e+01 -7.200e+01 -7.400e+01\n",
      " -7.600e+01 -7.700e+01 -8.400e+01 -9.000e+01  9.600e+01 -1.050e+02\n",
      "  1.110e+02  1.130e+02 -1.140e+02 -1.220e+02 -1.340e+02 -1.530e+02\n",
      "  1.550e+02 -1.620e+02  1.620e+02 -1.660e+02 -1.690e+02 -1.740e+02\n",
      "  1.780e+02 -1.900e+02 -1.940e+02  1.940e+02 -1.990e+02 -2.040e+02\n",
      "  2.050e+02 -2.070e+02 -2.100e+02 -2.300e+02 -2.310e+02  2.400e+02\n",
      "  2.410e+02  2.420e+02 -2.420e+02 -2.430e+02  2.450e+02 -2.490e+02\n",
      " -2.570e+02  2.690e+02 -2.740e+02 -2.860e+02 -2.870e+02 -2.920e+02\n",
      "  3.150e+02  3.160e+02 -3.170e+02 -3.200e+02 -3.280e+02 -3.330e+02\n",
      " -3.360e+02 -3.370e+02 -3.430e+02  3.470e+02  3.510e+02  3.540e+02\n",
      " -3.680e+02  3.720e+02 -3.720e+02  3.780e+02 -3.840e+02  3.880e+02\n",
      " -3.950e+02  3.980e+02 -4.030e+02 -4.050e+02 -4.070e+02 -4.080e+02\n",
      " -4.140e+02 -4.160e+02 -4.170e+02 -4.190e+02 -4.270e+02 -4.280e+02\n",
      " -4.310e+02 -4.330e+02  4.350e+02  4.380e+02 -4.380e+02 -4.420e+02\n",
      " -4.420e+02 -4.430e+02  4.440e+02 -4.510e+02 -4.540e+02 -4.540e+02\n",
      " -4.550e+02 -4.550e+02  4.560e+02 -4.570e+02 -4.610e+02  4.660e+02\n",
      " -4.670e+02 -4.670e+02 -4.780e+02 -4.860e+02 -4.870e+02 -4.870e+02\n",
      " -4.910e+02  4.920e+02 -4.920e+02  4.920e+02 -4.940e+02 -4.940e+02\n",
      " -4.970e+02 -5.000e+02 -5.020e+02 -5.030e+02 -5.080e+02  5.120e+02\n",
      " -5.120e+02 -5.150e+02 -5.160e+02 -5.220e+02 -5.230e+02 -5.230e+02\n",
      " -5.260e+02 -5.300e+02  5.310e+02 -5.310e+02 -5.320e+02 -5.330e+02\n",
      " -5.330e+02  5.370e+02  5.380e+02 -5.420e+02 -5.440e+02 -5.440e+02\n",
      "  5.470e+02 -5.480e+02  5.590e+02  5.610e+02 -5.630e+02 -5.640e+02\n",
      " -5.660e+02 -5.660e+02 -5.660e+02 -5.670e+02 -5.670e+02 -5.690e+02\n",
      " -5.690e+02 -5.710e+02 -5.710e+02 -5.730e+02  5.760e+02 -5.760e+02\n",
      " -5.820e+02 -5.840e+02 -5.850e+02 -5.880e+02  5.920e+02 -5.960e+02\n",
      " -5.990e+02 -6.040e+02  6.070e+02 -6.080e+02 -6.090e+02 -6.150e+02\n",
      " -6.220e+02 -6.230e+02 -6.270e+02 -6.290e+02 -6.290e+02 -6.300e+02\n",
      " -6.330e+02 -6.380e+02  6.390e+02  6.480e+02  6.480e+02 -6.500e+02\n",
      " -6.510e+02 -6.550e+02 -6.560e+02 -6.710e+02 -6.770e+02 -6.780e+02\n",
      "  6.820e+02 -6.850e+02 -7.050e+02  7.090e+02 -7.150e+02 -7.180e+02\n",
      " -7.210e+02  7.220e+02 -7.240e+02  7.270e+02 -7.350e+02 -7.360e+02\n",
      " -7.430e+02 -7.480e+02 -7.540e+02  7.580e+02 -7.580e+02 -7.720e+02\n",
      "  7.750e+02 -7.770e+02 -7.870e+02 -7.920e+02 -8.000e+02 -8.050e+02\n",
      "  8.140e+02 -8.320e+02 -8.350e+02 -8.460e+02 -8.460e+02 -8.540e+02\n",
      " -8.540e+02 -8.600e+02 -8.620e+02 -8.630e+02 -8.780e+02  8.860e+02\n",
      " -8.890e+02 -9.000e+02 -9.080e+02 -9.080e+02 -9.080e+02 -9.140e+02\n",
      " -9.140e+02 -9.160e+02  9.190e+02 -9.190e+02 -9.260e+02  9.330e+02\n",
      " -9.330e+02 -9.390e+02 -9.460e+02 -9.490e+02  9.540e+02 -9.550e+02\n",
      " -9.560e+02  9.610e+02 -9.620e+02  9.620e+02 -9.640e+02 -9.680e+02\n",
      "  9.870e+02 -9.920e+02 -9.920e+02 -9.930e+02  1.011e+03 -1.012e+03\n",
      " -1.021e+03 -1.032e+03  1.033e+03 -1.040e+03 -1.069e+03 -1.076e+03\n",
      " -1.078e+03 -1.079e+03 -1.099e+03  1.106e+03 -1.112e+03 -1.115e+03\n",
      " -1.116e+03 -1.120e+03  1.120e+03 -1.130e+03  1.137e+03 -1.137e+03\n",
      " -1.139e+03 -1.147e+03  1.152e+03 -1.164e+03 -1.173e+03  1.183e+03\n",
      " -1.191e+03 -1.201e+03 -1.201e+03  1.209e+03 -1.210e+03 -1.213e+03\n",
      " -1.217e+03 -1.219e+03  1.220e+03 -1.222e+03 -1.227e+03  1.242e+03\n",
      " -1.245e+03 -1.250e+03  1.251e+03  1.262e+03 -1.277e+03 -1.279e+03\n",
      " -1.294e+03 -1.294e+03 -1.301e+03 -1.337e+03  1.339e+03 -1.341e+03\n",
      " -1.354e+03 -1.359e+03 -1.382e+03 -1.387e+03 -1.393e+03 -1.399e+03\n",
      "  1.401e+03 -1.401e+03 -1.412e+03 -1.421e+03 -1.426e+03 -1.426e+03\n",
      " -1.428e+03 -1.453e+03 -1.458e+03 -1.469e+03 -1.470e+03  1.481e+03\n",
      "  1.491e+03 -1.494e+03 -1.500e+03 -1.519e+03  1.525e+03  1.547e+03\n",
      " -1.553e+03 -1.567e+03 -1.568e+03  1.578e+03  1.585e+03 -1.650e+03\n",
      " -1.706e+03  1.762e+03 -1.796e+03 -1.806e+03 -1.828e+03 -1.834e+03\n",
      " -1.868e+03  1.886e+03  1.891e+03  1.915e+03  1.933e+03 -1.989e+03\n",
      "  2.000e+03  2.052e+03 -2.078e+03 -2.107e+03 -2.218e+03  2.235e+03\n",
      "  2.286e+03 -2.287e+03 -2.289e+03 -2.289e+03  2.433e+03 -2.493e+03\n",
      " -2.565e+03 -2.585e+03 -2.602e+03  2.660e+03 -2.702e+03 -2.761e+03\n",
      " -2.772e+03  2.835e+03 -2.860e+03 -2.869e+03  2.907e+03 -2.918e+03\n",
      " -3.000e+03  3.200e+03 -3.253e+03  3.470e+03  3.571e+03 -3.574e+03\n",
      " -3.733e+03 -3.761e+03  3.978e+03  4.068e+03  4.084e+03 -4.113e+03\n",
      "  4.229e+03  4.445e+03  4.695e+03 -4.752e+03  5.166e+03 -5.255e+03\n",
      " -6.423e+03]\n",
      "durations 1.0 5546.0\n",
      "Concordance Index 0.23879310344827587\n",
      "Integrated Brier Score: 0.4710354711771211\n",
      "split gex_?|100130426      float32\n",
      "gex_?|100133144      float32\n",
      "gex_?|100134869      float32\n",
      "gex_?|10357          float32\n",
      "gex_?|10431          float32\n",
      "                      ...   \n",
      "gex_ZYG11A|440590    float32\n",
      "gex_ZYG11B|79699     float32\n",
      "gex_ZYX|7791         float32\n",
      "gex_ZZEF1|23140      float32\n",
      "gex_ZZZ3|26009       float32\n",
      "Length: 20531, dtype: object\n",
      "(292, 20531) <class 'pandas.core.frame.DataFrame'>\n",
      "(292,) <class 'pandas.core.series.Series'>\n",
      "(73, 20531) <class 'pandas.core.frame.DataFrame'>\n",
      "(73,) <class 'pandas.core.series.Series'>\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m2.0993\u001b[0m  0.0211\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2        2.0993  0.0186\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m2.5368\u001b[0m  0.0180\n",
      "      3        2.0993  0.0139\n",
      "      2        2.5368  0.0222\n",
      "      4        2.0993  0.0188\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m1.7375\u001b[0m  0.0179\n",
      "      3        2.5368  0.0167\n",
      "      5        2.0993  0.0133\n",
      "      2        1.7375  0.0243\n",
      "      4        2.5368  0.0172\n",
      "      6        2.0993  0.0170\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      5        2.5368  0.0201\n",
      "      7        2.0993  0.0166\n",
      "      3        1.7375  0.0274\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      8        2.0993  0.0140\n",
      "      4        1.7375  0.0142\n",
      "      6        2.5368  0.0203\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      5        1.7375  0.0158\n",
      "      9        2.0993  0.0205\n",
      "      7        2.5368  0.0181\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "     10        2.0993  0.0167\n",
      "Restoring best model from epoch 1.\n",
      "      6        1.7375  0.0179\n",
      "      8        2.5368  0.0169\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m2.2255\u001b[0m  0.0190\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      7        1.7375  0.0142\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      9        2.5368  0.0166\n",
      "      2        2.2255  0.0192\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      8        1.7375  0.0210\n",
      "     10        2.5368  0.0211\n",
      "Restoring best model from epoch 1.\n",
      "      9        1.7375  0.0156\n",
      "      3        2.2255  0.0328\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m2.5744\u001b[0m  0.0346\n",
      "      4        2.2255  0.0143\n",
      "     10        1.7375  0.0264\n",
      "Restoring best model from epoch 1.\n",
      "      2        2.5744  0.0172\n",
      "      5        2.2255  0.0150\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      3        2.5744  0.0226\n",
      "      6        2.2255  0.0252\n",
      "      4        2.5744  0.0223\n",
      "      7        2.2255  0.0147\n",
      "      5        2.5744  0.0141\n",
      "      8        2.2255  0.0129\n",
      "      6        2.5744  0.0129\n",
      "      9        2.2255  0.0131\n",
      "      7        2.5744  0.0129\n",
      "     10        2.2255  0.0127\n",
      "Restoring best model from epoch 1.\n",
      "      8        2.5744  0.0180\n",
      "      9        2.5744  0.0144\n",
      "     10        2.5744  0.0130\n",
      "Restoring best model from epoch 1.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.8510\u001b[0m        \u001b[32m2.3679\u001b[0m  0.3863\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.7713\u001b[0m        \u001b[32m2.2456\u001b[0m  0.4496\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.5315\u001b[0m        \u001b[32m2.0386\u001b[0m  0.4526\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.1508\u001b[0m        \u001b[32m1.5484\u001b[0m  0.4615\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.9584\u001b[0m        \u001b[32m1.7894\u001b[0m  0.4703\n",
      "      2        \u001b[36m2.6297\u001b[0m        \u001b[32m2.2954\u001b[0m  0.2392\n",
      "      2        \u001b[36m2.4942\u001b[0m        \u001b[32m2.0068\u001b[0m  0.2604\n",
      "      2        \u001b[36m2.7052\u001b[0m        2.2932  0.2826\n",
      "      2        \u001b[36m3.0062\u001b[0m        1.5855  0.2512\n",
      "      2        \u001b[36m2.8425\u001b[0m        \u001b[32m1.7008\u001b[0m  0.2790\n",
      "      3        \u001b[36m2.5884\u001b[0m        2.3018  0.2315\n",
      "      3        \u001b[36m2.7003\u001b[0m        2.3066  0.2307\n",
      "      3        \u001b[36m2.4547\u001b[0m        \u001b[32m1.9994\u001b[0m  0.2322\n",
      "      3        \u001b[36m2.9960\u001b[0m        \u001b[32m1.5249\u001b[0m  0.2215\n",
      "      3        \u001b[36m2.8302\u001b[0m        1.7427  0.2183\n",
      "      4        \u001b[36m2.5226\u001b[0m        2.3085  0.2114\n",
      "      4        \u001b[36m2.6163\u001b[0m        2.2543  0.2223\n",
      "      4        \u001b[36m2.9598\u001b[0m        1.5772  0.2253\n",
      "      4        2.4556        2.0286  0.2285\n",
      "      4        \u001b[36m2.7985\u001b[0m        1.7574  0.2252\n",
      "      5        \u001b[36m2.5131\u001b[0m        2.3448  0.2274\n",
      "      5        \u001b[36m2.5607\u001b[0m        2.2782  0.2240\n",
      "      5        \u001b[36m2.9157\u001b[0m        1.5778  0.2237\n",
      "      5        \u001b[36m2.4183\u001b[0m        2.0868  0.2230\n",
      "      5        \u001b[36m2.7143\u001b[0m        1.7979  0.2201\n",
      "      6        \u001b[36m2.4312\u001b[0m        2.3623  0.2173\n",
      "      6        \u001b[36m2.4868\u001b[0m        2.2741  0.2211\n",
      "      6        \u001b[36m2.9009\u001b[0m        1.5734  0.2220\n",
      "      6        \u001b[36m2.3999\u001b[0m        2.1122  0.2280\n",
      "      6        \u001b[36m2.6567\u001b[0m        1.8507  0.2157\n",
      "      7        \u001b[36m2.3972\u001b[0m        2.3219  0.2164\n",
      "      7        \u001b[36m2.4630\u001b[0m        2.3103  0.2262\n",
      "      7        \u001b[36m2.8740\u001b[0m        1.5585  0.2247\n",
      "      7        \u001b[36m2.3824\u001b[0m        2.1339  0.2187\n",
      "      7        \u001b[36m2.6441\u001b[0m        1.8881  0.2252\n",
      "      8        \u001b[36m2.3445\u001b[0m        \u001b[32m2.2929\u001b[0m  0.2196\n",
      "      8        \u001b[36m2.8189\u001b[0m        1.5707  0.2193\n",
      "      8        \u001b[36m2.4499\u001b[0m        2.3665  0.2299\n",
      "      8        \u001b[36m2.3465\u001b[0m        2.1443  0.2246\n",
      "      8        \u001b[36m2.6208\u001b[0m        1.9033  0.2206\n",
      "      9        \u001b[36m2.3287\u001b[0m        2.2996  0.2169\n",
      "      9        \u001b[36m2.7930\u001b[0m        1.5957  0.2284\n",
      "      9        \u001b[36m2.3952\u001b[0m        2.3753  0.2321\n",
      "      9        \u001b[36m2.3222\u001b[0m        2.1502  0.2334\n",
      "      9        \u001b[36m2.5510\u001b[0m        1.9103  0.2378\n",
      "     10        \u001b[36m2.3140\u001b[0m        2.3119  0.2440\n",
      "Restoring best model from epoch 8.\n",
      "     10        \u001b[36m2.7831\u001b[0m        1.6009  0.2465\n",
      "Restoring best model from epoch 3.\n",
      "     10        2.4319        2.3706  0.2460\n",
      "     10        \u001b[36m2.2955\u001b[0m        2.1250  0.2454\n",
      "Restoring best model from epoch 1.\n",
      "Restoring best model from epoch 3.\n",
      "     10        2.5742        1.9209  0.2255\n",
      "Restoring best model from epoch 2.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.7760\u001b[0m        \u001b[32m2.9256\u001b[0m  0.4060\n",
      "      2        \u001b[36m2.6255\u001b[0m        \u001b[32m2.8110\u001b[0m  0.1146\n",
      "      3        \u001b[36m2.5909\u001b[0m        \u001b[32m2.7620\u001b[0m  0.0814\n",
      "      4        \u001b[36m2.5445\u001b[0m        2.7832  0.0784\n",
      "      5        \u001b[36m2.4950\u001b[0m        \u001b[32m2.7563\u001b[0m  0.0856\n",
      "      6        \u001b[36m2.4705\u001b[0m        2.8191  0.0710\n",
      "      7        \u001b[36m2.4132\u001b[0m        2.8193  0.0748\n",
      "      8        2.4139        2.8017  0.0903\n",
      "      9        \u001b[36m2.3714\u001b[0m        2.8320  0.1158\n",
      "     10        \u001b[36m2.3279\u001b[0m        2.8607  0.1194\n",
      "Restoring best model from epoch 5.\n",
      "y_train breslow final [-1.000e+00 -6.000e+00 -6.000e+00 -8.000e+00 -9.000e+00 -9.000e+00\n",
      "  9.000e+00 -1.000e+01  1.100e+01  1.200e+01 -1.200e+01  1.400e+01\n",
      "  1.600e+01 -2.000e+01 -2.000e+01 -2.100e+01 -2.300e+01  2.700e+01\n",
      " -3.000e+01  3.100e+01  3.400e+01  4.600e+01  5.600e+01  5.600e+01\n",
      "  6.500e+01  6.500e+01  6.700e+01 -7.900e+01  8.700e+01  9.100e+01\n",
      " -9.400e+01  9.700e+01  1.010e+02  1.020e+02  1.030e+02  1.070e+02\n",
      "  1.150e+02  1.290e+02  1.290e+02  1.400e+02 -1.410e+02  1.530e+02\n",
      "  1.710e+02  1.710e+02 -1.800e+02 -1.830e+02  1.940e+02 -2.110e+02\n",
      "  2.140e+02  2.170e+02  2.230e+02 -2.290e+02  2.330e+02  2.470e+02\n",
      " -2.480e+02 -2.500e+02  2.620e+02  2.780e+02  2.790e+02  2.830e+02\n",
      " -2.910e+02  2.960e+02  2.990e+02  3.000e+02  3.000e+02 -3.030e+02\n",
      "  3.030e+02  3.040e+02  3.080e+02  3.150e+02 -3.220e+02 -3.270e+02\n",
      " -3.280e+02 -3.300e+02  3.440e+02 -3.450e+02 -3.470e+02  3.470e+02\n",
      "  3.490e+02 -3.540e+02 -3.570e+02 -3.590e+02  3.590e+02 -3.610e+02\n",
      " -3.610e+02 -3.630e+02 -3.630e+02 -3.650e+02  3.650e+02  3.660e+02\n",
      " -3.660e+02 -3.720e+02  3.810e+02 -3.820e+02 -3.870e+02 -3.870e+02\n",
      " -3.900e+02  3.940e+02 -3.950e+02 -3.950e+02 -3.960e+02 -3.990e+02\n",
      " -4.060e+02 -4.060e+02 -4.080e+02 -4.090e+02  4.100e+02  4.120e+02\n",
      " -4.120e+02  4.150e+02 -4.150e+02  4.190e+02 -4.230e+02 -4.250e+02\n",
      "  4.250e+02 -4.270e+02 -4.300e+02  4.380e+02 -4.440e+02 -4.470e+02\n",
      " -4.490e+02  4.520e+02 -4.580e+02 -4.680e+02 -4.720e+02 -4.740e+02\n",
      " -4.760e+02 -4.780e+02 -4.800e+02 -4.860e+02 -5.200e+02 -5.380e+02\n",
      "  5.470e+02 -5.520e+02 -5.540e+02 -5.550e+02 -5.550e+02  5.550e+02\n",
      "  5.560e+02  5.580e+02 -5.620e+02 -5.660e+02 -5.700e+02 -5.740e+02\n",
      " -5.750e+02  5.810e+02 -5.850e+02 -5.870e+02 -5.880e+02  5.960e+02\n",
      "  6.010e+02 -6.020e+02 -6.080e+02  6.120e+02 -6.150e+02 -6.210e+02\n",
      "  6.270e+02 -6.300e+02 -6.310e+02 -6.320e+02 -6.360e+02 -6.380e+02\n",
      " -6.400e+02  6.430e+02  6.490e+02 -6.550e+02 -6.580e+02  6.600e+02\n",
      " -6.710e+02 -6.720e+02 -6.730e+02  6.880e+02  6.930e+02 -6.970e+02\n",
      " -6.980e+02 -7.060e+02  7.110e+02 -7.220e+02  7.240e+02 -7.290e+02\n",
      " -7.440e+02 -7.470e+02 -7.560e+02  7.570e+02 -7.630e+02 -7.800e+02\n",
      " -7.820e+02 -7.840e+02  8.020e+02 -8.100e+02 -8.190e+02  8.270e+02\n",
      "  8.370e+02 -8.480e+02 -8.480e+02 -8.490e+02 -8.600e+02 -8.790e+02\n",
      " -8.980e+02 -8.980e+02  8.990e+02 -9.100e+02 -9.170e+02 -9.280e+02\n",
      "  9.310e+02 -9.420e+02 -9.510e+02 -9.890e+02  1.005e+03 -1.008e+03\n",
      " -1.030e+03 -1.049e+03 -1.066e+03 -1.067e+03 -1.091e+03 -1.098e+03\n",
      " -1.115e+03  1.135e+03 -1.145e+03  1.147e+03 -1.168e+03  1.210e+03\n",
      " -1.219e+03  1.229e+03 -1.241e+03 -1.242e+03 -1.295e+03 -1.339e+03\n",
      " -1.345e+03 -1.351e+03 -1.363e+03  1.386e+03  1.397e+03  1.423e+03\n",
      " -1.424e+03 -1.450e+03 -1.452e+03  1.490e+03 -1.495e+03 -1.516e+03\n",
      " -1.531e+03 -1.531e+03 -1.553e+03 -1.562e+03 -1.567e+03 -1.618e+03\n",
      "  1.622e+03  1.624e+03 -1.633e+03 -1.636e+03  1.685e+03  1.694e+03\n",
      " -1.711e+03 -1.718e+03 -1.731e+03 -1.804e+03 -1.823e+03 -1.855e+03\n",
      " -1.876e+03 -1.900e+03 -1.939e+03 -1.970e+03 -2.015e+03 -2.017e+03\n",
      " -2.102e+03  2.116e+03  2.131e+03 -2.184e+03 -2.202e+03 -2.245e+03\n",
      " -2.301e+03 -2.301e+03 -2.317e+03 -2.324e+03 -2.398e+03 -2.412e+03\n",
      " -2.415e+03 -2.425e+03 -2.442e+03 -2.455e+03  2.456e+03  2.486e+03\n",
      "  2.532e+03  2.542e+03 -2.728e+03 -2.746e+03 -2.752e+03  3.125e+03\n",
      "  3.258e+03 -3.437e+03 -3.478e+03 -3.675e+03]\n",
      "Concordance Index 0.29555236728837875\n",
      "Integrated Brier Score: inf\n",
      "y_train breslow final [-1.000e+00 -6.000e+00 -6.000e+00 -8.000e+00 -9.000e+00 -9.000e+00\n",
      "  9.000e+00 -1.000e+01  1.100e+01  1.200e+01 -1.200e+01  1.400e+01\n",
      "  1.600e+01 -2.000e+01 -2.000e+01 -2.100e+01 -2.300e+01  2.700e+01\n",
      " -3.000e+01  3.100e+01  3.400e+01  4.600e+01  5.600e+01  5.600e+01\n",
      "  6.500e+01  6.500e+01  6.700e+01 -7.900e+01  8.700e+01  9.100e+01\n",
      " -9.400e+01  9.700e+01  1.010e+02  1.020e+02  1.030e+02  1.070e+02\n",
      "  1.150e+02  1.290e+02  1.290e+02  1.400e+02 -1.410e+02  1.530e+02\n",
      "  1.710e+02  1.710e+02 -1.800e+02 -1.830e+02  1.940e+02 -2.110e+02\n",
      "  2.140e+02  2.170e+02  2.230e+02 -2.290e+02  2.330e+02  2.470e+02\n",
      " -2.480e+02 -2.500e+02  2.620e+02  2.780e+02  2.790e+02  2.830e+02\n",
      " -2.910e+02  2.960e+02  2.990e+02  3.000e+02  3.000e+02 -3.030e+02\n",
      "  3.030e+02  3.040e+02  3.080e+02  3.150e+02 -3.220e+02 -3.270e+02\n",
      " -3.280e+02 -3.300e+02  3.440e+02 -3.450e+02 -3.470e+02  3.470e+02\n",
      "  3.490e+02 -3.540e+02 -3.570e+02 -3.590e+02  3.590e+02 -3.610e+02\n",
      " -3.610e+02 -3.630e+02 -3.630e+02 -3.650e+02  3.650e+02  3.660e+02\n",
      " -3.660e+02 -3.720e+02  3.810e+02 -3.820e+02 -3.870e+02 -3.870e+02\n",
      " -3.900e+02  3.940e+02 -3.950e+02 -3.950e+02 -3.960e+02 -3.990e+02\n",
      " -4.060e+02 -4.060e+02 -4.080e+02 -4.090e+02  4.100e+02  4.120e+02\n",
      " -4.120e+02  4.150e+02 -4.150e+02  4.190e+02 -4.230e+02 -4.250e+02\n",
      "  4.250e+02 -4.270e+02 -4.300e+02  4.380e+02 -4.440e+02 -4.470e+02\n",
      " -4.490e+02  4.520e+02 -4.580e+02 -4.680e+02 -4.720e+02 -4.740e+02\n",
      " -4.760e+02 -4.780e+02 -4.800e+02 -4.860e+02 -5.200e+02 -5.380e+02\n",
      "  5.470e+02 -5.520e+02 -5.540e+02 -5.550e+02 -5.550e+02  5.550e+02\n",
      "  5.560e+02  5.580e+02 -5.620e+02 -5.660e+02 -5.700e+02 -5.740e+02\n",
      " -5.750e+02  5.810e+02 -5.850e+02 -5.870e+02 -5.880e+02  5.960e+02\n",
      "  6.010e+02 -6.020e+02 -6.080e+02  6.120e+02 -6.150e+02 -6.210e+02\n",
      "  6.270e+02 -6.300e+02 -6.310e+02 -6.320e+02 -6.360e+02 -6.380e+02\n",
      " -6.400e+02  6.430e+02  6.490e+02 -6.550e+02 -6.580e+02  6.600e+02\n",
      " -6.710e+02 -6.720e+02 -6.730e+02  6.880e+02  6.930e+02 -6.970e+02\n",
      " -6.980e+02 -7.060e+02  7.110e+02 -7.220e+02  7.240e+02 -7.290e+02\n",
      " -7.440e+02 -7.470e+02 -7.560e+02  7.570e+02 -7.630e+02 -7.800e+02\n",
      " -7.820e+02 -7.840e+02  8.020e+02 -8.100e+02 -8.190e+02  8.270e+02\n",
      "  8.370e+02 -8.480e+02 -8.480e+02 -8.490e+02 -8.600e+02 -8.790e+02\n",
      " -8.980e+02 -8.980e+02  8.990e+02 -9.100e+02 -9.170e+02 -9.280e+02\n",
      "  9.310e+02 -9.420e+02 -9.510e+02 -9.890e+02  1.005e+03 -1.008e+03\n",
      " -1.030e+03 -1.049e+03 -1.066e+03 -1.067e+03 -1.091e+03 -1.098e+03\n",
      " -1.115e+03  1.135e+03 -1.145e+03  1.147e+03 -1.168e+03  1.210e+03\n",
      " -1.219e+03  1.229e+03 -1.241e+03 -1.242e+03 -1.295e+03 -1.339e+03\n",
      " -1.345e+03 -1.351e+03 -1.363e+03  1.386e+03  1.397e+03  1.423e+03\n",
      " -1.424e+03 -1.450e+03 -1.452e+03  1.490e+03 -1.495e+03 -1.516e+03\n",
      " -1.531e+03 -1.531e+03 -1.553e+03 -1.562e+03 -1.567e+03 -1.618e+03\n",
      "  1.622e+03  1.624e+03 -1.633e+03 -1.636e+03  1.685e+03  1.694e+03\n",
      " -1.711e+03 -1.718e+03 -1.731e+03 -1.804e+03 -1.823e+03 -1.855e+03\n",
      " -1.876e+03 -1.900e+03 -1.939e+03 -1.970e+03 -2.015e+03 -2.017e+03\n",
      " -2.102e+03  2.116e+03  2.131e+03 -2.184e+03 -2.202e+03 -2.245e+03\n",
      " -2.301e+03 -2.301e+03 -2.317e+03 -2.324e+03 -2.398e+03 -2.412e+03\n",
      " -2.415e+03 -2.425e+03 -2.442e+03 -2.455e+03  2.456e+03  2.486e+03\n",
      "  2.532e+03  2.542e+03 -2.728e+03 -2.746e+03 -2.752e+03  3.125e+03\n",
      "  3.258e+03 -3.437e+03 -3.478e+03 -3.675e+03]\n",
      "durations 6.0 3308.0\n",
      "Concordance Index 0.3620146904512067\n",
      "Integrated Brier Score: 8.407837098626072e+297\n",
      "(292, 20531) <class 'pandas.core.frame.DataFrame'>\n",
      "(292,) <class 'pandas.core.series.Series'>\n",
      "(73, 20531) <class 'pandas.core.frame.DataFrame'>\n",
      "(73,) <class 'pandas.core.series.Series'>\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.4271\u001b[0m        \u001b[32m2.1020\u001b[0m  0.3663\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.9212\u001b[0m        \u001b[32m2.9273\u001b[0m  0.5016\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.8494\u001b[0m        \u001b[32m2.3420\u001b[0m  0.6180\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.8642\u001b[0m        \u001b[32m3.4108\u001b[0m  0.4200\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.9191\u001b[0m        \u001b[32m2.9149\u001b[0m  0.5074\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.8489\u001b[0m        \u001b[32m2.3360\u001b[0m  0.4341\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.8970\u001b[0m        \u001b[32m3.3716\u001b[0m  0.6091\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.4246\u001b[0m        \u001b[32m2.1060\u001b[0m  0.6277\n",
      "      2        \u001b[36m2.4030\u001b[0m        2.1040  0.3320\n",
      "      2        \u001b[36m2.9094\u001b[0m        \u001b[32m2.9273\u001b[0m  0.2824\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.8421\u001b[0m        \u001b[32m3.4026\u001b[0m  0.5130\n",
      "      2        \u001b[36m2.8152\u001b[0m        \u001b[32m2.3406\u001b[0m  0.2441\n",
      "      2        \u001b[36m2.8312\u001b[0m        \u001b[32m2.3287\u001b[0m  0.2742\n",
      "      2        \u001b[36m2.8603\u001b[0m        \u001b[32m3.3966\u001b[0m  0.2969\n",
      "      2        \u001b[36m2.9083\u001b[0m        \u001b[32m2.9133\u001b[0m  0.3150\n",
      "      3        2.4163        2.1037  0.2794\n",
      "      2        \u001b[36m2.8546\u001b[0m        3.3733  0.3905\n",
      "      2        2.4272        \u001b[32m2.1055\u001b[0m  0.3596\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.8638\u001b[0m        \u001b[32m3.3642\u001b[0m  0.8078\n",
      "      3        \u001b[36m2.9019\u001b[0m        \u001b[32m2.9166\u001b[0m  0.3040\n",
      "      2        2.8768        \u001b[32m3.3925\u001b[0m  0.3050\n",
      "      3        \u001b[36m2.8033\u001b[0m        \u001b[32m2.3329\u001b[0m  0.2817\n",
      "      3        \u001b[36m2.7846\u001b[0m        2.3369  0.3000\n",
      "      3        \u001b[36m2.7993\u001b[0m        \u001b[32m3.3828\u001b[0m  0.3054\n",
      "      3        \u001b[36m2.8937\u001b[0m        \u001b[32m2.9065\u001b[0m  0.3117\n",
      "      4        2.4084        2.1021  0.2918\n",
      "      3        \u001b[36m2.8512\u001b[0m        \u001b[32m3.3676\u001b[0m  0.2769\n",
      "      3        \u001b[36m2.4131\u001b[0m        \u001b[32m2.0997\u001b[0m  0.2790\n",
      "      4        \u001b[36m2.8821\u001b[0m        \u001b[32m2.9098\u001b[0m  0.2913\n",
      "      2        \u001b[36m2.8458\u001b[0m        \u001b[32m3.3530\u001b[0m  0.3018\n",
      "      3        \u001b[36m2.8192\u001b[0m        \u001b[32m3.3879\u001b[0m  0.2362\n",
      "      4        \u001b[36m2.7226\u001b[0m        \u001b[32m2.3257\u001b[0m  0.2369\n",
      "      4        2.8060        \u001b[32m3.3821\u001b[0m  0.2347\n",
      "      4        \u001b[36m2.8882\u001b[0m        \u001b[32m2.9053\u001b[0m  0.2632\n",
      "      4        \u001b[36m2.3944\u001b[0m        \u001b[32m2.0943\u001b[0m  0.2278\n",
      "      4        \u001b[36m2.8433\u001b[0m        \u001b[32m3.3549\u001b[0m  0.2433\n",
      "      4        \u001b[36m2.7467\u001b[0m        2.3452  0.3758\n",
      "      5        \u001b[36m2.3739\u001b[0m        \u001b[32m2.1009\u001b[0m  0.3021\n",
      "      3        \u001b[36m2.8367\u001b[0m        3.3541  0.2402\n",
      "      4        \u001b[36m2.7845\u001b[0m        3.3891  0.2417\n",
      "      5        \u001b[36m2.8613\u001b[0m        \u001b[32m2.9052\u001b[0m  0.2640\n",
      "      5        2.8065        3.3941  0.2567\n",
      "      5        2.7436        2.3280  0.3680\n",
      "      5        \u001b[36m2.8464\u001b[0m        \u001b[32m2.9035\u001b[0m  0.2284\n",
      "      5        \u001b[36m2.7652\u001b[0m        3.3574  0.2016\n",
      "      5        \u001b[36m2.3659\u001b[0m        \u001b[32m2.0928\u001b[0m  0.2156\n",
      "      6        2.4166        2.1026  0.2118\n",
      "      4        \u001b[36m2.8202\u001b[0m        3.3617  0.2039\n",
      "      5        2.8222        3.3963  0.2119\n",
      "      5        \u001b[36m2.6833\u001b[0m        2.3501  0.2449\n",
      "      6        2.8752        2.9083  0.2164\n",
      "      6        \u001b[36m2.7864\u001b[0m        3.4079  0.2329\n",
      "      6        2.7489        2.3506  0.2267\n",
      "      6        2.8097        3.3661  0.2379\n",
      "      6        2.8547        \u001b[32m2.9029\u001b[0m  0.2558\n",
      "      5        2.8241        3.3744  0.2389\n",
      "      6        2.4083        \u001b[32m2.0922\u001b[0m  0.2602\n",
      "      7        2.3944        2.1037  0.2503\n",
      "      6        2.7906        3.4065  0.2443\n",
      "      6        2.7778        2.3465  0.2402\n",
      "      7        \u001b[36m2.8491\u001b[0m        2.9144  0.2683\n",
      "      7        \u001b[36m2.7522\u001b[0m        3.4159  0.2325\n",
      "      7        \u001b[36m2.6902\u001b[0m        2.3633  0.2315\n",
      "      7        \u001b[36m2.3549\u001b[0m        2.0941  0.2059\n",
      "      6        \u001b[36m2.7841\u001b[0m        3.3765  0.2183\n",
      "      7        \u001b[36m2.8084\u001b[0m        2.9033  0.2399\n",
      "      7        2.7160        2.3347  0.2256\n",
      "      7        \u001b[36m2.7321\u001b[0m        3.4088  0.3224\n",
      "      7        \u001b[36m2.7381\u001b[0m        3.3657  0.3690\n",
      "      8        \u001b[36m2.6670\u001b[0m        2.3637  0.1917\n",
      "      8        \u001b[36m2.3569\u001b[0m        \u001b[32m2.1002\u001b[0m  0.3740\n",
      "      8        \u001b[36m2.7178\u001b[0m        3.4141  0.2844\n",
      "      8        2.8556        2.9229  0.3873\n",
      "      8        2.8449        2.9129  0.2886\n",
      "      8        2.4072        2.0964  0.3228\n",
      "      7        \u001b[36m2.7302\u001b[0m        3.3653  0.3588\n",
      "      8        2.6838        \u001b[32m2.3223\u001b[0m  0.3551\n",
      "      9        \u001b[36m2.6263\u001b[0m        2.3571  0.3304\n",
      "      8        \u001b[36m2.7188\u001b[0m        3.3614  0.3407\n",
      "      8        2.7682        3.3915  0.3454\n",
      "      9        2.3801        \u001b[32m2.0992\u001b[0m  0.4147\n",
      "      9        \u001b[36m2.8443\u001b[0m        2.9280  0.3945\n",
      "      9        \u001b[36m2.7143\u001b[0m        3.4039  0.4125\n",
      "      9        2.8434        2.9181  0.3494\n",
      "      9        2.3896        2.0969  0.3585\n",
      "      8        \u001b[36m2.7057\u001b[0m        3.3573  0.3787\n",
      "      9        \u001b[36m2.6369\u001b[0m        \u001b[32m2.3111\u001b[0m  0.3867\n",
      "      9        2.7298        \u001b[32m3.3540\u001b[0m  0.4129\n",
      "     10        2.6721        2.3517  0.4403\n",
      "      9        \u001b[36m2.6775\u001b[0m        \u001b[32m3.3591\u001b[0m  0.4625\n",
      "Restoring best model from epoch 4.\n",
      "     10        \u001b[36m2.6301\u001b[0m        3.4002  0.4214\n",
      "     10        \u001b[36m2.8089\u001b[0m        2.9252  0.4283\n",
      "Restoring best model from epoch 5.\n",
      "Restoring best model from epoch 4.\n",
      "     10        2.3633        2.0971  0.4212\n",
      "Restoring best model from epoch 6.\n",
      "     10        2.8270        2.9115  0.4899\n",
      "Restoring best model from epoch 6.\n",
      "     10        \u001b[36m2.3342\u001b[0m        2.1042  0.5903\n",
      "Restoring best model from epoch 9.\n",
      "     10        \u001b[36m2.6024\u001b[0m        \u001b[32m2.3075\u001b[0m  0.4232\n",
      "      9        2.7495        3.3568  0.5504\n",
      "     10        \u001b[36m2.6572\u001b[0m        \u001b[32m3.3308\u001b[0m  0.4010\n",
      "     10        \u001b[36m2.6906\u001b[0m        \u001b[32m3.3384\u001b[0m  0.5014\n",
      "     10        \u001b[36m2.6538\u001b[0m        3.3590  0.2517\n",
      "Restoring best model from epoch 2.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.6579\u001b[0m        \u001b[32m2.8212\u001b[0m  0.5540\n",
      "      2        \u001b[36m2.6577\u001b[0m        2.8220  0.1460\n",
      "      3        \u001b[36m2.6244\u001b[0m        \u001b[32m2.8050\u001b[0m  0.1470\n",
      "      4        \u001b[36m2.6102\u001b[0m        \u001b[32m2.7955\u001b[0m  0.1774\n",
      "      5        \u001b[36m2.5553\u001b[0m        \u001b[32m2.7846\u001b[0m  0.1929\n",
      "      6        \u001b[36m2.5376\u001b[0m        \u001b[32m2.7765\u001b[0m  0.4412\n",
      "      7        2.5382        2.7807  0.1739\n",
      "      8        \u001b[36m2.5019\u001b[0m        \u001b[32m2.7667\u001b[0m  0.1700\n",
      "      9        \u001b[36m2.4484\u001b[0m        \u001b[32m2.7410\u001b[0m  0.1918\n",
      "     10        \u001b[36m2.4472\u001b[0m        \u001b[32m2.7353\u001b[0m  0.1556\n",
      "y_train breslow final [   -6.    -6.    -6.    -8.    -9.    -9.     9.    11.    12.   -12.\n",
      "    14.   -15.    16.    19.   -20.   -21.   -22.   -23.   -30.    31.\n",
      "    34.    36.   -44.    46.    52.    56.    56.    65.    65.    67.\n",
      "   -79.    87.    91.    91.   -94.    97.   101.   103.   107.   115.\n",
      "   129.   129.  -137.  -141.   153.  -170.   171.   171.  -180.  -183.\n",
      "   194.   195.  -211.   217.  -219.   233.   247.  -248.  -260.   262.\n",
      "   262.   272.   278.   283.  -291.   296.   299.   300.   304.   308.\n",
      "  -314.  -322.  -327.  -328.  -330.  -341.   344.  -345.  -347.   347.\n",
      "  -354.  -357.  -359.  -361.  -363.  -365.   365.   366.  -366.  -372.\n",
      "   373.  -382.  -387.  -387.  -390.   394.  -395.  -395.  -396.  -400.\n",
      "  -406.  -406.  -408.  -409.   410.   412.  -412.   415.   416.   419.\n",
      "  -423.  -425.   425.   432.  -436.   438.  -444.  -447.   452.  -453.\n",
      "  -468.   469.  -472.  -474.  -478.  -480.  -486.  -500.  -507.  -512.\n",
      "  -519.  -520.  -520.   535.  -538.   547.  -552.  -555.  -555.   555.\n",
      "   556.  -564.  -566.  -570.  -575.  -579.   581.  -585.  -587.  -588.\n",
      "  -594.  -601.   601.  -608.  -608.   612.  -615.  -621.   627.  -630.\n",
      "   633.  -636.  -638.   639.  -640.   643.  -644.  -655.  -662.  -671.\n",
      "  -672.  -673.  -680.   688.  -693.   693.  -697.  -698.  -698.  -701.\n",
      "  -706.   711.  -719.  -722.   724.  -728.  -729.  -747.  -756.   757.\n",
      "  -763.   765.   768.   770.  -782.  -799.   802.  -810.  -816.  -819.\n",
      "   827.  -829.   837.   837.  -848.  -848.   848.  -854.  -860.  -898.\n",
      "  -898.  -906.  -910.  -917.  -925.  -928.  -951.  -989.  1005. -1008.\n",
      " -1030. -1066. -1085. -1085.  1088. -1091. -1145.  1147.  1149. -1168.\n",
      "  1210.  1229. -1231. -1233. -1242.  1271. -1295. -1302. -1339. -1351.\n",
      " -1363.  1372.  1386.  1423. -1424. -1450. -1516. -1531. -1553.  1560.\n",
      " -1562. -1567. -1618.  1624. -1633. -1636.  1685.  1694. -1779.  1791.\n",
      " -1823.  1852. -1855. -1876. -1900. -1939. -1970. -1989. -2015. -2017.\n",
      " -2028.  2116.  2131. -2184. -2202. -2245. -2301. -2301. -2317. -2398.\n",
      " -2415. -2425. -2442. -2455. -2513.  2542. -2728. -2752.  3258. -3308.\n",
      " -3478. -3675.]\n",
      "Concordance Index 0.2288677130044843\n",
      "Integrated Brier Score: 0.49581659376335546\n",
      "y_train breslow final [   -6.    -6.    -6.    -8.    -9.    -9.     9.    11.    12.   -12.\n",
      "    14.   -15.    16.    19.   -20.   -21.   -22.   -23.   -30.    31.\n",
      "    34.    36.   -44.    46.    52.    56.    56.    65.    65.    67.\n",
      "   -79.    87.    91.    91.   -94.    97.   101.   103.   107.   115.\n",
      "   129.   129.  -137.  -141.   153.  -170.   171.   171.  -180.  -183.\n",
      "   194.   195.  -211.   217.  -219.   233.   247.  -248.  -260.   262.\n",
      "   262.   272.   278.   283.  -291.   296.   299.   300.   304.   308.\n",
      "  -314.  -322.  -327.  -328.  -330.  -341.   344.  -345.  -347.   347.\n",
      "  -354.  -357.  -359.  -361.  -363.  -365.   365.   366.  -366.  -372.\n",
      "   373.  -382.  -387.  -387.  -390.   394.  -395.  -395.  -396.  -400.\n",
      "  -406.  -406.  -408.  -409.   410.   412.  -412.   415.   416.   419.\n",
      "  -423.  -425.   425.   432.  -436.   438.  -444.  -447.   452.  -453.\n",
      "  -468.   469.  -472.  -474.  -478.  -480.  -486.  -500.  -507.  -512.\n",
      "  -519.  -520.  -520.   535.  -538.   547.  -552.  -555.  -555.   555.\n",
      "   556.  -564.  -566.  -570.  -575.  -579.   581.  -585.  -587.  -588.\n",
      "  -594.  -601.   601.  -608.  -608.   612.  -615.  -621.   627.  -630.\n",
      "   633.  -636.  -638.   639.  -640.   643.  -644.  -655.  -662.  -671.\n",
      "  -672.  -673.  -680.   688.  -693.   693.  -697.  -698.  -698.  -701.\n",
      "  -706.   711.  -719.  -722.   724.  -728.  -729.  -747.  -756.   757.\n",
      "  -763.   765.   768.   770.  -782.  -799.   802.  -810.  -816.  -819.\n",
      "   827.  -829.   837.   837.  -848.  -848.   848.  -854.  -860.  -898.\n",
      "  -898.  -906.  -910.  -917.  -925.  -928.  -951.  -989.  1005. -1008.\n",
      " -1030. -1066. -1085. -1085.  1088. -1091. -1145.  1147.  1149. -1168.\n",
      "  1210.  1229. -1231. -1233. -1242.  1271. -1295. -1302. -1339. -1351.\n",
      " -1363.  1372.  1386.  1423. -1424. -1450. -1516. -1531. -1553.  1560.\n",
      " -1562. -1567. -1618.  1624. -1633. -1636.  1685.  1694. -1779.  1791.\n",
      " -1823.  1852. -1855. -1876. -1900. -1939. -1970. -1989. -2015. -2017.\n",
      " -2028.  2116.  2131. -2184. -2202. -2245. -2301. -2301. -2317. -2398.\n",
      " -2415. -2425. -2442. -2455. -2513.  2542. -2728. -2752.  3258. -3308.\n",
      " -3478. -3675.]\n",
      "durations 1.0 3437.0\n",
      "Concordance Index 0.38246628131021193\n",
      "Integrated Brier Score: 0.4344893923371318\n",
      "(292, 20531) <class 'pandas.core.frame.DataFrame'>\n",
      "(292,) <class 'pandas.core.series.Series'>\n",
      "(73, 20531) <class 'pandas.core.frame.DataFrame'>\n",
      "(73,) <class 'pandas.core.series.Series'>\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.8681\u001b[0m        \u001b[32m2.2327\u001b[0m  0.2715\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.9011\u001b[0m        \u001b[32m3.0095\u001b[0m  0.2774\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.9031\u001b[0m        \u001b[32m3.0069\u001b[0m  0.2625\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.6712\u001b[0m        \u001b[32m2.2823\u001b[0m  0.2492\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.8506\u001b[0m        \u001b[32m2.2309\u001b[0m  0.3530\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.8199\u001b[0m        \u001b[32m2.9543\u001b[0m  0.3015\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.7594\u001b[0m        \u001b[32m3.1198\u001b[0m  0.3042\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.8402\u001b[0m        \u001b[32m2.9639\u001b[0m  0.3070\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.6643\u001b[0m        \u001b[32m2.2743\u001b[0m  0.3655\n",
      "      2        \u001b[36m2.8607\u001b[0m        \u001b[32m2.2319\u001b[0m  0.2095\n",
      "      2        \u001b[36m2.8982\u001b[0m        \u001b[32m2.9857\u001b[0m  0.2172\n",
      "      2        2.9059        \u001b[32m2.9898\u001b[0m  0.1989\n",
      "      2        2.8797        \u001b[32m2.2232\u001b[0m  0.2105\n",
      "      2        \u001b[36m2.6623\u001b[0m        2.2826  0.2321\n",
      "      2        2.8447        \u001b[32m2.9497\u001b[0m  0.2088\n",
      "      2        \u001b[36m2.7563\u001b[0m        \u001b[32m3.1019\u001b[0m  0.1906\n",
      "      2        2.8562        \u001b[32m2.9630\u001b[0m  0.2027\n",
      "      2        \u001b[36m2.6624\u001b[0m        \u001b[32m2.2690\u001b[0m  0.2030\n",
      "      3        \u001b[36m2.8305\u001b[0m        2.2350  0.1860\n",
      "      3        \u001b[36m2.8852\u001b[0m        \u001b[32m2.9568\u001b[0m  0.1949\n",
      "      3        \u001b[36m2.8178\u001b[0m        \u001b[32m2.2179\u001b[0m  0.1830\n",
      "      3        \u001b[36m2.8677\u001b[0m        \u001b[32m2.9747\u001b[0m  0.2275\n",
      "      3        2.8379        2.9513  0.1829\n",
      "      3        2.6891        \u001b[32m2.2801\u001b[0m  0.1961\n",
      "      3        \u001b[36m2.7490\u001b[0m        \u001b[32m3.0855\u001b[0m  0.2010\n",
      "      3        2.6652        \u001b[32m2.2650\u001b[0m  0.2080\n",
      "      4        \u001b[36m2.7721\u001b[0m        2.2438  0.2203\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.7634\u001b[0m        \u001b[32m3.1225\u001b[0m  0.7211\n",
      "      4        \u001b[36m2.8739\u001b[0m        \u001b[32m2.9190\u001b[0m  0.2242\n",
      "      3        2.8503        \u001b[32m2.9622\u001b[0m  0.2725\n",
      "      4        \u001b[36m2.6567\u001b[0m        \u001b[32m2.2785\u001b[0m  0.1988\n",
      "      4        2.9064        \u001b[32m2.9466\u001b[0m  0.2304\n",
      "      4        \u001b[36m2.8058\u001b[0m        2.9566  0.2306\n",
      "      4        2.8300        2.2274  0.2829\n",
      "      4        \u001b[36m2.7079\u001b[0m        \u001b[32m3.0598\u001b[0m  0.2323\n",
      "      4        2.6673        \u001b[32m2.2600\u001b[0m  0.2274\n",
      "      5        2.8125        2.2419  0.2273\n",
      "      4        \u001b[36m2.7995\u001b[0m        \u001b[32m2.9600\u001b[0m  0.2031\n",
      "      5        2.6656        \u001b[32m2.2716\u001b[0m  0.2110\n",
      "      5        \u001b[36m2.8349\u001b[0m        \u001b[32m2.8804\u001b[0m  0.2485\n",
      "      5        \u001b[36m2.8333\u001b[0m        \u001b[32m2.9202\u001b[0m  0.2048\n",
      "      5        \u001b[36m2.7495\u001b[0m        2.9598  0.2368\n",
      "      5        2.8401        2.2373  0.2090\n",
      "      2        \u001b[36m2.7577\u001b[0m        \u001b[32m3.1020\u001b[0m  0.3175\n",
      "      5        \u001b[36m2.6757\u001b[0m        \u001b[32m3.0349\u001b[0m  0.2141\n",
      "      6        2.8093        2.2320  0.2074\n",
      "      5        \u001b[36m2.6287\u001b[0m        \u001b[32m2.2547\u001b[0m  0.2310\n",
      "      6        \u001b[36m2.6082\u001b[0m        \u001b[32m2.2584\u001b[0m  0.1968\n",
      "      5        2.8080        \u001b[32m2.9573\u001b[0m  0.2287\n",
      "      6        \u001b[36m2.8046\u001b[0m        \u001b[32m2.8981\u001b[0m  0.1976\n",
      "      6        \u001b[36m2.8034\u001b[0m        2.2392  0.1950\n",
      "      6        \u001b[36m2.8267\u001b[0m        \u001b[32m2.8563\u001b[0m  0.2519\n",
      "      6        2.7533        2.9581  0.2267\n",
      "      3        \u001b[36m2.7144\u001b[0m        \u001b[32m3.0846\u001b[0m  0.2308\n",
      "      6        2.7029        \u001b[32m3.0191\u001b[0m  0.2234\n",
      "      6        2.6492        \u001b[32m2.2534\u001b[0m  0.1918\n",
      "      7        2.6124        \u001b[32m2.2374\u001b[0m  0.1767\n",
      "      7        2.7732        \u001b[32m2.2210\u001b[0m  0.2372\n",
      "      6        2.8141        \u001b[32m2.9538\u001b[0m  0.2349\n",
      "      7        2.8232        \u001b[32m2.8800\u001b[0m  0.2160\n",
      "      7        2.8091        2.2354  0.2126\n",
      "      7        \u001b[36m2.7976\u001b[0m        \u001b[32m2.8387\u001b[0m  0.1985\n",
      "      7        \u001b[36m2.6566\u001b[0m        \u001b[32m3.0111\u001b[0m  0.1867\n",
      "      4        \u001b[36m2.6790\u001b[0m        \u001b[32m3.0633\u001b[0m  0.2480\n",
      "      8        2.6386        \u001b[32m2.2263\u001b[0m  0.1767\n",
      "      7        \u001b[36m2.7265\u001b[0m        \u001b[32m2.9378\u001b[0m  0.2750\n",
      "      7        2.6508        \u001b[32m2.2475\u001b[0m  0.1967\n",
      "      8        \u001b[36m2.7374\u001b[0m        \u001b[32m2.2065\u001b[0m  0.1809\n",
      "      8        2.8456        \u001b[32m2.8281\u001b[0m  0.1845\n",
      "      8        \u001b[36m2.7503\u001b[0m        2.2299  0.1899\n",
      "      8        \u001b[36m2.8001\u001b[0m        \u001b[32m2.8724\u001b[0m  0.2168\n",
      "      7        \u001b[36m2.7705\u001b[0m        \u001b[32m2.9475\u001b[0m  0.2583\n",
      "      8        \u001b[36m2.7107\u001b[0m        \u001b[32m2.9266\u001b[0m  0.1907\n",
      "      9        2.6351        \u001b[32m2.2199\u001b[0m  0.2073\n",
      "      8        2.6502        \u001b[32m2.2410\u001b[0m  0.2077\n",
      "      8        2.6656        \u001b[32m3.0025\u001b[0m  0.2654\n",
      "      9        2.7468        \u001b[32m2.1933\u001b[0m  0.1984\n",
      "      5        \u001b[36m2.6367\u001b[0m        \u001b[32m3.0424\u001b[0m  0.2438\n",
      "      9        \u001b[36m2.7749\u001b[0m        \u001b[32m2.8214\u001b[0m  0.2084\n",
      "      9        2.8164        2.8752  0.2219\n",
      "      8        \u001b[36m2.7539\u001b[0m        \u001b[32m2.9440\u001b[0m  0.1869\n",
      "      9        \u001b[36m2.7390\u001b[0m        2.2239  0.2397\n",
      "      9        \u001b[36m2.7030\u001b[0m        \u001b[32m2.9171\u001b[0m  0.1811\n",
      "      9        \u001b[36m2.5827\u001b[0m        \u001b[32m2.9951\u001b[0m  0.1836\n",
      "     10        \u001b[36m2.6051\u001b[0m        \u001b[32m2.2135\u001b[0m  0.2131\n",
      "     10        2.7646        \u001b[32m2.1879\u001b[0m  0.2032\n",
      "      6        2.6394        \u001b[32m3.0313\u001b[0m  0.1875\n",
      "     10        \u001b[36m2.7316\u001b[0m        \u001b[32m2.8141\u001b[0m  0.1833\n",
      "      9        2.6566        \u001b[32m2.2351\u001b[0m  0.2898\n",
      "     10        \u001b[36m2.7762\u001b[0m        2.8811  0.1952\n",
      "Restoring best model from epoch 8.\n",
      "      9        \u001b[36m2.7405\u001b[0m        2.9442  0.2011\n",
      "     10        \u001b[36m2.7309\u001b[0m        \u001b[32m2.2158\u001b[0m  0.2264\n",
      "     10        \u001b[36m2.6520\u001b[0m        \u001b[32m2.9069\u001b[0m  0.2189\n",
      "     10        2.6132        \u001b[32m2.9904\u001b[0m  0.1863\n",
      "      7        \u001b[36m2.5728\u001b[0m        \u001b[32m3.0031\u001b[0m  0.1927\n",
      "     10        2.6307        \u001b[32m2.2295\u001b[0m  0.1990\n",
      "     10        \u001b[36m2.6818\u001b[0m        \u001b[32m2.9440\u001b[0m  0.1772\n",
      "Restoring best model from epoch 8.\n",
      "      8        2.6548        \u001b[32m2.9756\u001b[0m  0.1560\n",
      "      9        2.6003        \u001b[32m2.9626\u001b[0m  0.1465\n",
      "     10        2.5773        2.9670  0.1505\n",
      "Restoring best model from epoch 9.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.4977\u001b[0m        \u001b[32m3.3836\u001b[0m  0.1334\n",
      "      2        2.5074        \u001b[32m3.3780\u001b[0m  0.1177\n",
      "      3        \u001b[36m2.4702\u001b[0m        \u001b[32m3.3744\u001b[0m  0.0971\n",
      "      4        \u001b[36m2.4114\u001b[0m        \u001b[32m3.3680\u001b[0m  0.0984\n",
      "      5        2.4436        \u001b[32m3.3665\u001b[0m  0.0842\n",
      "      6        \u001b[36m2.3911\u001b[0m        3.3679  0.0877\n",
      "      7        2.3959        \u001b[32m3.3661\u001b[0m  0.0847\n",
      "      8        \u001b[36m2.3645\u001b[0m        \u001b[32m3.3635\u001b[0m  0.0890\n",
      "      9        \u001b[36m2.3260\u001b[0m        \u001b[32m3.3521\u001b[0m  0.0838\n",
      "     10        \u001b[36m2.3153\u001b[0m        \u001b[32m3.3465\u001b[0m  0.0942\n",
      "y_train breslow final [-1.000e+00 -6.000e+00 -6.000e+00 -6.000e+00 -9.000e+00  9.000e+00\n",
      " -1.000e+01  1.100e+01  1.200e+01 -1.200e+01 -1.500e+01  1.600e+01\n",
      "  1.900e+01 -2.000e+01 -2.000e+01 -2.100e+01 -2.200e+01 -2.300e+01\n",
      "  2.700e+01 -3.000e+01  3.100e+01  3.600e+01 -4.400e+01  4.600e+01\n",
      "  5.200e+01  5.600e+01  6.500e+01  6.500e+01  6.700e+01  8.700e+01\n",
      "  9.100e+01 -9.400e+01  9.700e+01  1.010e+02  1.020e+02  1.030e+02\n",
      "  1.070e+02  1.290e+02  1.290e+02 -1.370e+02  1.400e+02 -1.410e+02\n",
      " -1.700e+02  1.710e+02  1.710e+02 -1.830e+02  1.940e+02  1.950e+02\n",
      " -2.110e+02  2.140e+02  2.170e+02 -2.190e+02  2.230e+02 -2.290e+02\n",
      "  2.330e+02  2.470e+02 -2.480e+02 -2.500e+02 -2.600e+02  2.620e+02\n",
      "  2.620e+02  2.720e+02  2.790e+02 -2.910e+02  2.990e+02  3.000e+02\n",
      "  3.000e+02 -3.030e+02  3.030e+02  3.040e+02 -3.140e+02  3.150e+02\n",
      " -3.280e+02 -3.300e+02 -3.410e+02 -3.450e+02 -3.470e+02  3.470e+02\n",
      "  3.490e+02 -3.540e+02 -3.570e+02  3.590e+02 -3.610e+02 -3.610e+02\n",
      " -3.630e+02 -3.630e+02  3.650e+02 -3.660e+02 -3.720e+02  3.730e+02\n",
      "  3.810e+02 -3.820e+02 -3.870e+02 -3.870e+02 -3.900e+02 -3.960e+02\n",
      " -3.990e+02 -4.000e+02 -4.060e+02 -4.080e+02  4.100e+02 -4.120e+02\n",
      " -4.150e+02  4.160e+02 -4.230e+02  4.250e+02 -4.270e+02 -4.300e+02\n",
      "  4.320e+02 -4.360e+02  4.380e+02 -4.490e+02  4.520e+02 -4.530e+02\n",
      " -4.580e+02  4.690e+02 -4.720e+02 -4.740e+02 -4.760e+02 -4.800e+02\n",
      " -5.000e+02 -5.070e+02 -5.120e+02 -5.190e+02 -5.200e+02  5.350e+02\n",
      " -5.520e+02 -5.540e+02 -5.550e+02 -5.550e+02  5.560e+02  5.580e+02\n",
      " -5.620e+02 -5.640e+02 -5.700e+02 -5.740e+02 -5.790e+02 -5.850e+02\n",
      " -5.870e+02 -5.880e+02 -5.940e+02  5.960e+02 -6.010e+02  6.010e+02\n",
      " -6.020e+02 -6.080e+02  6.270e+02 -6.300e+02 -6.310e+02 -6.320e+02\n",
      "  6.330e+02 -6.360e+02 -6.380e+02  6.390e+02  6.430e+02 -6.440e+02\n",
      "  6.490e+02 -6.580e+02  6.600e+02 -6.620e+02 -6.720e+02 -6.800e+02\n",
      " -6.930e+02 -6.970e+02 -6.980e+02 -6.980e+02 -7.010e+02 -7.060e+02\n",
      "  7.110e+02 -7.190e+02  7.240e+02 -7.280e+02 -7.290e+02 -7.440e+02\n",
      " -7.560e+02 -7.630e+02  7.650e+02  7.680e+02  7.700e+02 -7.800e+02\n",
      " -7.820e+02 -7.840e+02 -7.990e+02  8.020e+02 -8.100e+02 -8.160e+02\n",
      "  8.270e+02 -8.290e+02  8.370e+02  8.370e+02 -8.480e+02 -8.480e+02\n",
      "  8.480e+02 -8.490e+02 -8.540e+02 -8.600e+02 -8.790e+02 -8.980e+02\n",
      "  8.990e+02 -9.060e+02 -9.100e+02 -9.170e+02 -9.250e+02  9.310e+02\n",
      " -9.420e+02 -9.890e+02 -1.008e+03 -1.030e+03 -1.049e+03 -1.066e+03\n",
      " -1.067e+03 -1.085e+03 -1.085e+03  1.088e+03 -1.098e+03 -1.115e+03\n",
      "  1.135e+03 -1.145e+03  1.147e+03  1.149e+03 -1.168e+03 -1.219e+03\n",
      "  1.229e+03 -1.231e+03 -1.233e+03 -1.241e+03 -1.242e+03  1.271e+03\n",
      " -1.302e+03 -1.339e+03 -1.345e+03 -1.351e+03  1.372e+03  1.386e+03\n",
      "  1.397e+03  1.423e+03 -1.424e+03 -1.450e+03 -1.452e+03  1.490e+03\n",
      " -1.495e+03 -1.531e+03 -1.531e+03 -1.553e+03  1.560e+03 -1.562e+03\n",
      " -1.567e+03 -1.618e+03  1.622e+03  1.624e+03 -1.633e+03 -1.636e+03\n",
      "  1.694e+03 -1.711e+03 -1.718e+03 -1.731e+03 -1.779e+03  1.791e+03\n",
      " -1.804e+03 -1.823e+03  1.852e+03 -1.855e+03 -1.876e+03 -1.900e+03\n",
      " -1.939e+03 -1.989e+03 -2.015e+03 -2.017e+03 -2.028e+03 -2.102e+03\n",
      "  2.116e+03  2.131e+03 -2.184e+03 -2.245e+03 -2.324e+03 -2.398e+03\n",
      " -2.412e+03 -2.415e+03 -2.425e+03 -2.455e+03  2.456e+03  2.486e+03\n",
      " -2.513e+03  2.532e+03  2.542e+03 -2.728e+03 -2.746e+03  3.125e+03\n",
      "  3.258e+03 -3.308e+03 -3.437e+03 -3.478e+03]\n",
      "Concordance Index 0.2348313956851177\n",
      "Integrated Brier Score: 0.43954641890354695\n",
      "y_train breslow final [-1.000e+00 -6.000e+00 -6.000e+00 -6.000e+00 -9.000e+00  9.000e+00\n",
      " -1.000e+01  1.100e+01  1.200e+01 -1.200e+01 -1.500e+01  1.600e+01\n",
      "  1.900e+01 -2.000e+01 -2.000e+01 -2.100e+01 -2.200e+01 -2.300e+01\n",
      "  2.700e+01 -3.000e+01  3.100e+01  3.600e+01 -4.400e+01  4.600e+01\n",
      "  5.200e+01  5.600e+01  6.500e+01  6.500e+01  6.700e+01  8.700e+01\n",
      "  9.100e+01 -9.400e+01  9.700e+01  1.010e+02  1.020e+02  1.030e+02\n",
      "  1.070e+02  1.290e+02  1.290e+02 -1.370e+02  1.400e+02 -1.410e+02\n",
      " -1.700e+02  1.710e+02  1.710e+02 -1.830e+02  1.940e+02  1.950e+02\n",
      " -2.110e+02  2.140e+02  2.170e+02 -2.190e+02  2.230e+02 -2.290e+02\n",
      "  2.330e+02  2.470e+02 -2.480e+02 -2.500e+02 -2.600e+02  2.620e+02\n",
      "  2.620e+02  2.720e+02  2.790e+02 -2.910e+02  2.990e+02  3.000e+02\n",
      "  3.000e+02 -3.030e+02  3.030e+02  3.040e+02 -3.140e+02  3.150e+02\n",
      " -3.280e+02 -3.300e+02 -3.410e+02 -3.450e+02 -3.470e+02  3.470e+02\n",
      "  3.490e+02 -3.540e+02 -3.570e+02  3.590e+02 -3.610e+02 -3.610e+02\n",
      " -3.630e+02 -3.630e+02  3.650e+02 -3.660e+02 -3.720e+02  3.730e+02\n",
      "  3.810e+02 -3.820e+02 -3.870e+02 -3.870e+02 -3.900e+02 -3.960e+02\n",
      " -3.990e+02 -4.000e+02 -4.060e+02 -4.080e+02  4.100e+02 -4.120e+02\n",
      " -4.150e+02  4.160e+02 -4.230e+02  4.250e+02 -4.270e+02 -4.300e+02\n",
      "  4.320e+02 -4.360e+02  4.380e+02 -4.490e+02  4.520e+02 -4.530e+02\n",
      " -4.580e+02  4.690e+02 -4.720e+02 -4.740e+02 -4.760e+02 -4.800e+02\n",
      " -5.000e+02 -5.070e+02 -5.120e+02 -5.190e+02 -5.200e+02  5.350e+02\n",
      " -5.520e+02 -5.540e+02 -5.550e+02 -5.550e+02  5.560e+02  5.580e+02\n",
      " -5.620e+02 -5.640e+02 -5.700e+02 -5.740e+02 -5.790e+02 -5.850e+02\n",
      " -5.870e+02 -5.880e+02 -5.940e+02  5.960e+02 -6.010e+02  6.010e+02\n",
      " -6.020e+02 -6.080e+02  6.270e+02 -6.300e+02 -6.310e+02 -6.320e+02\n",
      "  6.330e+02 -6.360e+02 -6.380e+02  6.390e+02  6.430e+02 -6.440e+02\n",
      "  6.490e+02 -6.580e+02  6.600e+02 -6.620e+02 -6.720e+02 -6.800e+02\n",
      " -6.930e+02 -6.970e+02 -6.980e+02 -6.980e+02 -7.010e+02 -7.060e+02\n",
      "  7.110e+02 -7.190e+02  7.240e+02 -7.280e+02 -7.290e+02 -7.440e+02\n",
      " -7.560e+02 -7.630e+02  7.650e+02  7.680e+02  7.700e+02 -7.800e+02\n",
      " -7.820e+02 -7.840e+02 -7.990e+02  8.020e+02 -8.100e+02 -8.160e+02\n",
      "  8.270e+02 -8.290e+02  8.370e+02  8.370e+02 -8.480e+02 -8.480e+02\n",
      "  8.480e+02 -8.490e+02 -8.540e+02 -8.600e+02 -8.790e+02 -8.980e+02\n",
      "  8.990e+02 -9.060e+02 -9.100e+02 -9.170e+02 -9.250e+02  9.310e+02\n",
      " -9.420e+02 -9.890e+02 -1.008e+03 -1.030e+03 -1.049e+03 -1.066e+03\n",
      " -1.067e+03 -1.085e+03 -1.085e+03  1.088e+03 -1.098e+03 -1.115e+03\n",
      "  1.135e+03 -1.145e+03  1.147e+03  1.149e+03 -1.168e+03 -1.219e+03\n",
      "  1.229e+03 -1.231e+03 -1.233e+03 -1.241e+03 -1.242e+03  1.271e+03\n",
      " -1.302e+03 -1.339e+03 -1.345e+03 -1.351e+03  1.372e+03  1.386e+03\n",
      "  1.397e+03  1.423e+03 -1.424e+03 -1.450e+03 -1.452e+03  1.490e+03\n",
      " -1.495e+03 -1.531e+03 -1.531e+03 -1.553e+03  1.560e+03 -1.562e+03\n",
      " -1.567e+03 -1.618e+03  1.622e+03  1.624e+03 -1.633e+03 -1.636e+03\n",
      "  1.694e+03 -1.711e+03 -1.718e+03 -1.731e+03 -1.779e+03  1.791e+03\n",
      " -1.804e+03 -1.823e+03  1.852e+03 -1.855e+03 -1.876e+03 -1.900e+03\n",
      " -1.939e+03 -1.989e+03 -2.015e+03 -2.017e+03 -2.028e+03 -2.102e+03\n",
      "  2.116e+03  2.131e+03 -2.184e+03 -2.245e+03 -2.324e+03 -2.398e+03\n",
      " -2.412e+03 -2.415e+03 -2.425e+03 -2.455e+03  2.456e+03  2.486e+03\n",
      " -2.513e+03  2.532e+03  2.542e+03 -2.728e+03 -2.746e+03  3.125e+03\n",
      "  3.258e+03 -3.308e+03 -3.437e+03 -3.478e+03]\n",
      "durations 8.0 3675.0\n",
      "Concordance Index 0.4940374787052811\n",
      "Integrated Brier Score: 0.31797673689750533\n",
      "(292, 20531) <class 'pandas.core.frame.DataFrame'>\n",
      "(292,) <class 'pandas.core.series.Series'>\n",
      "(73, 20531) <class 'pandas.core.frame.DataFrame'>\n",
      "(73,) <class 'pandas.core.series.Series'>\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.7673\u001b[0m        \u001b[32m1.8721\u001b[0m  0.2343\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.8955\u001b[0m        \u001b[32m2.6798\u001b[0m  0.2252\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.7544\u001b[0m        \u001b[32m1.8622\u001b[0m  0.2431\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.5815\u001b[0m        \u001b[32m2.6429\u001b[0m  0.2697\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.7774\u001b[0m        \u001b[32m2.8400\u001b[0m  0.2375\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.5860\u001b[0m        \u001b[32m2.6480\u001b[0m  0.2612\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.8714\u001b[0m        \u001b[32m2.6834\u001b[0m  0.2829\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.4372\u001b[0m        \u001b[32m1.9168\u001b[0m  0.3966\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.7664\u001b[0m        \u001b[32m2.8332\u001b[0m  0.4541\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.4693\u001b[0m        \u001b[32m1.9286\u001b[0m  0.5900\n",
      "      2        2.9103        \u001b[32m2.6790\u001b[0m  0.4499\n",
      "      2        \u001b[36m2.7655\u001b[0m        2.8421  0.4860\n",
      "      2        2.5998        \u001b[32m2.6398\u001b[0m  0.5214\n",
      "      2        \u001b[36m2.7491\u001b[0m        1.8780  0.5300\n",
      "      2        2.6007        \u001b[32m2.6450\u001b[0m  0.4480\n",
      "      2        \u001b[36m2.4367\u001b[0m        \u001b[32m1.9014\u001b[0m  0.3443\n",
      "      2        2.8966        2.6838  0.4367\n",
      "      2        \u001b[36m2.7542\u001b[0m        1.8856  0.5876\n",
      "      2        \u001b[36m2.7416\u001b[0m        \u001b[32m2.8321\u001b[0m  0.3210\n",
      "      2        \u001b[36m2.4221\u001b[0m        \u001b[32m1.9238\u001b[0m  0.2643\n",
      "      3        2.7754        2.8423  0.2260\n",
      "      3        \u001b[36m2.8929\u001b[0m        \u001b[32m2.6770\u001b[0m  0.2888\n",
      "      3        \u001b[36m2.7358\u001b[0m        1.8769  0.2247\n",
      "      3        2.6171        \u001b[32m2.6357\u001b[0m  0.2276\n",
      "      3        \u001b[36m2.4150\u001b[0m        \u001b[32m1.8957\u001b[0m  0.2238\n",
      "      3        2.8971        \u001b[32m2.6801\u001b[0m  0.2399\n",
      "      3        2.7580        1.8875  0.2304\n",
      "      3        2.7587        2.8333  0.2394\n",
      "      3        \u001b[36m2.4219\u001b[0m        1.9342  0.2160\n",
      "      3        2.6180        \u001b[32m2.6430\u001b[0m  0.3731\n",
      "      4        \u001b[36m2.7264\u001b[0m        2.8416  0.2062\n",
      "      4        \u001b[36m2.8856\u001b[0m        \u001b[32m2.6733\u001b[0m  0.2140\n",
      "      4        2.7521        1.8783  0.2100\n",
      "      4        \u001b[36m2.5746\u001b[0m        \u001b[32m2.6303\u001b[0m  0.2143\n",
      "      4        \u001b[36m2.7499\u001b[0m        1.8890  0.2108\n",
      "      4        2.4157        1.9014  0.2346\n",
      "      4        2.8956        \u001b[32m2.6764\u001b[0m  0.2248\n",
      "      4        2.7471        2.8339  0.2142\n",
      "      4        \u001b[36m2.4168\u001b[0m        1.9554  0.2047\n",
      "      4        2.6036        \u001b[32m2.6413\u001b[0m  0.1956\n",
      "      5        2.7321        \u001b[32m2.8398\u001b[0m  0.2131\n",
      "      5        \u001b[36m2.7240\u001b[0m        1.8789  0.1909\n",
      "      5        \u001b[36m2.8580\u001b[0m        \u001b[32m2.6701\u001b[0m  0.2048\n",
      "      5        \u001b[36m2.5634\u001b[0m        \u001b[32m2.6270\u001b[0m  0.2015\n",
      "      5        \u001b[36m2.7144\u001b[0m        1.8884  0.1844\n",
      "      5        2.4344        1.9120  0.2030\n",
      "      5        2.9005        \u001b[32m2.6717\u001b[0m  0.2003\n",
      "      5        \u001b[36m2.6828\u001b[0m        2.8328  0.2467\n",
      "      5        2.5918        \u001b[32m2.6340\u001b[0m  0.1914\n",
      "      5        2.4250        1.9621  0.2594\n",
      "      6        \u001b[36m2.7239\u001b[0m        1.8748  0.1976\n",
      "      6        2.7303        2.8410  0.2105\n",
      "      6        \u001b[36m2.5369\u001b[0m        \u001b[32m2.6266\u001b[0m  0.1816\n",
      "      6        2.7400        1.8813  0.1704\n",
      "      6        2.8830        \u001b[32m2.6620\u001b[0m  0.2057\n",
      "      6        \u001b[36m2.8660\u001b[0m        \u001b[32m2.6673\u001b[0m  0.2034\n",
      "      6        \u001b[36m2.6540\u001b[0m        \u001b[32m2.8210\u001b[0m  0.1903\n",
      "      6        2.4226        1.9276  0.2677\n",
      "      6        2.6024        \u001b[32m2.6310\u001b[0m  0.2052\n",
      "      7        \u001b[36m2.8477\u001b[0m        \u001b[32m2.6511\u001b[0m  0.2256\n",
      "      6        \u001b[36m2.4001\u001b[0m        1.9624  0.2526\n",
      "      7        2.7148        \u001b[32m1.8639\u001b[0m  0.2345\n",
      "      7        \u001b[36m2.5190\u001b[0m        2.6298  0.2464\n",
      "      7        \u001b[36m2.7048\u001b[0m        \u001b[32m2.8384\u001b[0m  0.2609\n",
      "      7        \u001b[36m2.7071\u001b[0m        1.8741  0.2978\n",
      "      7        2.6912        \u001b[32m2.7968\u001b[0m  0.1773\n",
      "      7        \u001b[36m2.8560\u001b[0m        \u001b[32m2.6608\u001b[0m  0.2411\n",
      "      7        2.4154        1.9389  0.2332\n",
      "      7        2.4161        1.9568  0.1872\n",
      "      7        \u001b[36m2.5358\u001b[0m        2.6393  0.2718\n",
      "      8        \u001b[36m2.5020\u001b[0m        2.6364  0.1866\n",
      "      8        2.8616        \u001b[32m2.6335\u001b[0m  0.1963\n",
      "      8        \u001b[36m2.7027\u001b[0m        \u001b[32m1.8445\u001b[0m  0.2039\n",
      "      8        \u001b[36m2.6652\u001b[0m        1.8815  0.1969\n",
      "      8        \u001b[36m2.6340\u001b[0m        \u001b[32m2.7749\u001b[0m  0.1894\n",
      "      8        \u001b[36m2.6635\u001b[0m        \u001b[32m2.8255\u001b[0m  0.2310\n",
      "      8        2.4197        1.9455  0.1720\n",
      "      8        2.8742        \u001b[32m2.6564\u001b[0m  0.2290\n",
      "      8        \u001b[36m2.5231\u001b[0m        2.6425  0.1933\n",
      "      9        2.5164        2.6420  0.1980\n",
      "      9        \u001b[36m2.6357\u001b[0m        \u001b[32m1.8416\u001b[0m  0.1830\n",
      "      8        2.4168        1.9583  0.2230\n",
      "      9        \u001b[36m2.8418\u001b[0m        \u001b[32m2.6077\u001b[0m  0.2131\n",
      "      9        2.6939        1.8921  0.2068\n",
      "      9        \u001b[36m2.6291\u001b[0m        \u001b[32m2.7619\u001b[0m  0.2036\n",
      "      9        \u001b[36m2.6573\u001b[0m        \u001b[32m2.8115\u001b[0m  0.2058\n",
      "      9        \u001b[36m2.3983\u001b[0m        1.9567  0.1870\n",
      "      9        2.8726        \u001b[32m2.6492\u001b[0m  0.1788\n",
      "      9        \u001b[36m2.5172\u001b[0m        \u001b[32m2.6285\u001b[0m  0.2005\n",
      "     10        \u001b[36m2.4969\u001b[0m        2.6457  0.1995\n",
      "Restoring best model from epoch 6.\n",
      "     10        2.6683        \u001b[32m1.8364\u001b[0m  0.2033\n",
      "     10        \u001b[36m2.8215\u001b[0m        \u001b[32m2.5897\u001b[0m  0.1848\n",
      "      9        2.4103        1.9568  0.2407\n",
      "     10        2.6686        \u001b[32m2.7961\u001b[0m  0.2004\n",
      "     10        2.6708        1.9035  0.2215\n",
      "Restoring best model from epoch 1.\n",
      "     10        \u001b[36m2.3768\u001b[0m        1.9626  0.2123\n",
      "Restoring best model from epoch 3.\n",
      "     10        2.6395        2.7650  0.2351\n",
      "Restoring best model from epoch 9.\n",
      "     10        2.8642        \u001b[32m2.6408\u001b[0m  0.2023\n",
      "     10        \u001b[36m2.5041\u001b[0m        \u001b[32m2.6075\u001b[0m  0.1865\n",
      "     10        \u001b[36m2.3716\u001b[0m        1.9503  0.1634\n",
      "Restoring best model from epoch 2.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.7386\u001b[0m        \u001b[32m3.2082\u001b[0m  0.1493\n",
      "      2        2.7804        \u001b[32m3.2069\u001b[0m  0.1238\n",
      "      3        2.7422        \u001b[32m3.1942\u001b[0m  0.0989\n",
      "      4        2.7583        \u001b[32m3.1916\u001b[0m  0.0885\n",
      "      5        2.7419        3.2061  0.0833\n",
      "      6        \u001b[36m2.7289\u001b[0m        \u001b[32m3.1890\u001b[0m  0.0846\n",
      "      7        \u001b[36m2.6992\u001b[0m        \u001b[32m3.1841\u001b[0m  0.0905\n",
      "      8        \u001b[36m2.6707\u001b[0m        \u001b[32m3.1816\u001b[0m  0.0988\n",
      "      9        2.6758        \u001b[32m3.1706\u001b[0m  0.0999\n",
      "     10        \u001b[36m2.6489\u001b[0m        \u001b[32m3.0969\u001b[0m  0.0886\n",
      "y_train breslow final [-1.000e+00 -6.000e+00 -6.000e+00 -8.000e+00 -9.000e+00 -9.000e+00\n",
      "  9.000e+00 -1.000e+01  1.100e+01  1.200e+01 -1.200e+01  1.400e+01\n",
      " -1.500e+01  1.600e+01  1.900e+01 -2.000e+01 -2.000e+01 -2.100e+01\n",
      " -2.200e+01 -2.300e+01  2.700e+01  3.400e+01  3.600e+01 -4.400e+01\n",
      "  4.600e+01  5.200e+01  5.600e+01  5.600e+01  6.500e+01 -7.900e+01\n",
      "  8.700e+01  9.100e+01  9.100e+01 -9.400e+01  1.020e+02  1.030e+02\n",
      "  1.150e+02  1.290e+02 -1.370e+02  1.400e+02 -1.410e+02  1.530e+02\n",
      " -1.700e+02  1.710e+02  1.710e+02 -1.800e+02  1.940e+02  1.950e+02\n",
      " -2.110e+02  2.140e+02  2.170e+02 -2.190e+02  2.230e+02 -2.290e+02\n",
      " -2.500e+02 -2.600e+02  2.620e+02  2.720e+02  2.780e+02  2.790e+02\n",
      "  2.830e+02  2.960e+02  3.000e+02  3.000e+02 -3.030e+02  3.030e+02\n",
      "  3.080e+02 -3.140e+02  3.150e+02 -3.220e+02 -3.270e+02 -3.410e+02\n",
      "  3.440e+02  3.490e+02 -3.570e+02 -3.590e+02  3.590e+02 -3.610e+02\n",
      " -3.610e+02 -3.630e+02 -3.630e+02 -3.650e+02  3.650e+02  3.660e+02\n",
      "  3.730e+02  3.810e+02 -3.870e+02 -3.900e+02  3.940e+02 -3.950e+02\n",
      " -3.950e+02 -3.990e+02 -4.000e+02 -4.060e+02 -4.090e+02  4.100e+02\n",
      "  4.120e+02 -4.120e+02  4.150e+02 -4.150e+02  4.160e+02  4.190e+02\n",
      " -4.230e+02 -4.250e+02  4.250e+02 -4.270e+02 -4.300e+02  4.320e+02\n",
      " -4.360e+02  4.380e+02 -4.440e+02 -4.470e+02 -4.490e+02 -4.530e+02\n",
      " -4.580e+02 -4.680e+02  4.690e+02 -4.760e+02 -4.780e+02 -4.800e+02\n",
      " -4.860e+02 -5.000e+02 -5.070e+02 -5.120e+02 -5.190e+02 -5.200e+02\n",
      " -5.200e+02  5.350e+02 -5.380e+02  5.470e+02 -5.520e+02 -5.540e+02\n",
      "  5.550e+02  5.580e+02 -5.620e+02 -5.640e+02 -5.660e+02 -5.740e+02\n",
      " -5.750e+02 -5.790e+02  5.810e+02 -5.870e+02 -5.940e+02  5.960e+02\n",
      " -6.010e+02  6.010e+02 -6.020e+02 -6.080e+02 -6.080e+02  6.120e+02\n",
      " -6.150e+02 -6.210e+02  6.270e+02 -6.310e+02 -6.320e+02  6.330e+02\n",
      "  6.390e+02 -6.400e+02 -6.440e+02  6.490e+02 -6.550e+02 -6.580e+02\n",
      "  6.600e+02 -6.620e+02 -6.710e+02 -6.720e+02 -6.730e+02 -6.800e+02\n",
      "  6.880e+02 -6.930e+02  6.930e+02 -6.970e+02 -6.980e+02 -7.010e+02\n",
      " -7.190e+02 -7.220e+02  7.240e+02 -7.280e+02 -7.440e+02 -7.470e+02\n",
      " -7.560e+02  7.570e+02  7.650e+02  7.680e+02  7.700e+02 -7.800e+02\n",
      " -7.840e+02 -7.990e+02 -8.160e+02 -8.190e+02  8.270e+02 -8.290e+02\n",
      "  8.370e+02  8.370e+02 -8.480e+02  8.480e+02 -8.490e+02 -8.540e+02\n",
      " -8.600e+02 -8.790e+02 -8.980e+02  8.990e+02 -9.060e+02 -9.250e+02\n",
      " -9.280e+02  9.310e+02 -9.420e+02 -9.510e+02 -9.890e+02  1.005e+03\n",
      " -1.008e+03 -1.030e+03 -1.049e+03 -1.067e+03 -1.085e+03 -1.085e+03\n",
      "  1.088e+03 -1.091e+03 -1.098e+03 -1.115e+03  1.135e+03 -1.145e+03\n",
      "  1.147e+03  1.149e+03 -1.168e+03  1.210e+03 -1.219e+03 -1.231e+03\n",
      " -1.233e+03 -1.241e+03 -1.242e+03  1.271e+03 -1.295e+03 -1.302e+03\n",
      " -1.345e+03 -1.351e+03 -1.363e+03  1.372e+03  1.397e+03 -1.424e+03\n",
      " -1.450e+03 -1.452e+03  1.490e+03 -1.495e+03 -1.516e+03 -1.531e+03\n",
      " -1.531e+03  1.560e+03 -1.562e+03 -1.567e+03 -1.618e+03  1.622e+03\n",
      " -1.633e+03  1.685e+03 -1.711e+03 -1.718e+03 -1.731e+03 -1.779e+03\n",
      "  1.791e+03 -1.804e+03 -1.823e+03  1.852e+03 -1.876e+03 -1.939e+03\n",
      " -1.970e+03 -1.989e+03 -2.017e+03 -2.028e+03 -2.102e+03 -2.202e+03\n",
      " -2.301e+03 -2.301e+03 -2.317e+03 -2.324e+03 -2.398e+03 -2.412e+03\n",
      " -2.415e+03 -2.425e+03 -2.442e+03 -2.455e+03  2.456e+03  2.486e+03\n",
      " -2.513e+03  2.532e+03  2.542e+03 -2.728e+03 -2.746e+03 -2.752e+03\n",
      "  3.125e+03 -3.308e+03 -3.437e+03 -3.675e+03]\n",
      "Concordance Index 0.3229452246405192\n",
      "Integrated Brier Score: 0.4502328535999867\n",
      "y_train breslow final [-1.000e+00 -6.000e+00 -6.000e+00 -8.000e+00 -9.000e+00 -9.000e+00\n",
      "  9.000e+00 -1.000e+01  1.100e+01  1.200e+01 -1.200e+01  1.400e+01\n",
      " -1.500e+01  1.600e+01  1.900e+01 -2.000e+01 -2.000e+01 -2.100e+01\n",
      " -2.200e+01 -2.300e+01  2.700e+01  3.400e+01  3.600e+01 -4.400e+01\n",
      "  4.600e+01  5.200e+01  5.600e+01  5.600e+01  6.500e+01 -7.900e+01\n",
      "  8.700e+01  9.100e+01  9.100e+01 -9.400e+01  1.020e+02  1.030e+02\n",
      "  1.150e+02  1.290e+02 -1.370e+02  1.400e+02 -1.410e+02  1.530e+02\n",
      " -1.700e+02  1.710e+02  1.710e+02 -1.800e+02  1.940e+02  1.950e+02\n",
      " -2.110e+02  2.140e+02  2.170e+02 -2.190e+02  2.230e+02 -2.290e+02\n",
      " -2.500e+02 -2.600e+02  2.620e+02  2.720e+02  2.780e+02  2.790e+02\n",
      "  2.830e+02  2.960e+02  3.000e+02  3.000e+02 -3.030e+02  3.030e+02\n",
      "  3.080e+02 -3.140e+02  3.150e+02 -3.220e+02 -3.270e+02 -3.410e+02\n",
      "  3.440e+02  3.490e+02 -3.570e+02 -3.590e+02  3.590e+02 -3.610e+02\n",
      " -3.610e+02 -3.630e+02 -3.630e+02 -3.650e+02  3.650e+02  3.660e+02\n",
      "  3.730e+02  3.810e+02 -3.870e+02 -3.900e+02  3.940e+02 -3.950e+02\n",
      " -3.950e+02 -3.990e+02 -4.000e+02 -4.060e+02 -4.090e+02  4.100e+02\n",
      "  4.120e+02 -4.120e+02  4.150e+02 -4.150e+02  4.160e+02  4.190e+02\n",
      " -4.230e+02 -4.250e+02  4.250e+02 -4.270e+02 -4.300e+02  4.320e+02\n",
      " -4.360e+02  4.380e+02 -4.440e+02 -4.470e+02 -4.490e+02 -4.530e+02\n",
      " -4.580e+02 -4.680e+02  4.690e+02 -4.760e+02 -4.780e+02 -4.800e+02\n",
      " -4.860e+02 -5.000e+02 -5.070e+02 -5.120e+02 -5.190e+02 -5.200e+02\n",
      " -5.200e+02  5.350e+02 -5.380e+02  5.470e+02 -5.520e+02 -5.540e+02\n",
      "  5.550e+02  5.580e+02 -5.620e+02 -5.640e+02 -5.660e+02 -5.740e+02\n",
      " -5.750e+02 -5.790e+02  5.810e+02 -5.870e+02 -5.940e+02  5.960e+02\n",
      " -6.010e+02  6.010e+02 -6.020e+02 -6.080e+02 -6.080e+02  6.120e+02\n",
      " -6.150e+02 -6.210e+02  6.270e+02 -6.310e+02 -6.320e+02  6.330e+02\n",
      "  6.390e+02 -6.400e+02 -6.440e+02  6.490e+02 -6.550e+02 -6.580e+02\n",
      "  6.600e+02 -6.620e+02 -6.710e+02 -6.720e+02 -6.730e+02 -6.800e+02\n",
      "  6.880e+02 -6.930e+02  6.930e+02 -6.970e+02 -6.980e+02 -7.010e+02\n",
      " -7.190e+02 -7.220e+02  7.240e+02 -7.280e+02 -7.440e+02 -7.470e+02\n",
      " -7.560e+02  7.570e+02  7.650e+02  7.680e+02  7.700e+02 -7.800e+02\n",
      " -7.840e+02 -7.990e+02 -8.160e+02 -8.190e+02  8.270e+02 -8.290e+02\n",
      "  8.370e+02  8.370e+02 -8.480e+02  8.480e+02 -8.490e+02 -8.540e+02\n",
      " -8.600e+02 -8.790e+02 -8.980e+02  8.990e+02 -9.060e+02 -9.250e+02\n",
      " -9.280e+02  9.310e+02 -9.420e+02 -9.510e+02 -9.890e+02  1.005e+03\n",
      " -1.008e+03 -1.030e+03 -1.049e+03 -1.067e+03 -1.085e+03 -1.085e+03\n",
      "  1.088e+03 -1.091e+03 -1.098e+03 -1.115e+03  1.135e+03 -1.145e+03\n",
      "  1.147e+03  1.149e+03 -1.168e+03  1.210e+03 -1.219e+03 -1.231e+03\n",
      " -1.233e+03 -1.241e+03 -1.242e+03  1.271e+03 -1.295e+03 -1.302e+03\n",
      " -1.345e+03 -1.351e+03 -1.363e+03  1.372e+03  1.397e+03 -1.424e+03\n",
      " -1.450e+03 -1.452e+03  1.490e+03 -1.495e+03 -1.516e+03 -1.531e+03\n",
      " -1.531e+03  1.560e+03 -1.562e+03 -1.567e+03 -1.618e+03  1.622e+03\n",
      " -1.633e+03  1.685e+03 -1.711e+03 -1.718e+03 -1.731e+03 -1.779e+03\n",
      "  1.791e+03 -1.804e+03 -1.823e+03  1.852e+03 -1.876e+03 -1.939e+03\n",
      " -1.970e+03 -1.989e+03 -2.017e+03 -2.028e+03 -2.102e+03 -2.202e+03\n",
      " -2.301e+03 -2.301e+03 -2.317e+03 -2.324e+03 -2.398e+03 -2.412e+03\n",
      " -2.415e+03 -2.425e+03 -2.442e+03 -2.455e+03  2.456e+03  2.486e+03\n",
      " -2.513e+03  2.532e+03  2.542e+03 -2.728e+03 -2.746e+03 -2.752e+03\n",
      "  3.125e+03 -3.308e+03 -3.437e+03 -3.675e+03]\n",
      "durations 6.0 3478.0\n",
      "Concordance Index 0.25763358778625955\n",
      "Integrated Brier Score: 0.3669492591751186\n",
      "(292, 20531) <class 'pandas.core.frame.DataFrame'>\n",
      "(292,) <class 'pandas.core.series.Series'>\n",
      "(73, 20531) <class 'pandas.core.frame.DataFrame'>\n",
      "(73,) <class 'pandas.core.series.Series'>\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.9636\u001b[0m        \u001b[32m2.3680\u001b[0m  0.2453\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.9640\u001b[0m        \u001b[32m2.3774\u001b[0m  0.3137\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.0450\u001b[0m        \u001b[32m2.5393\u001b[0m  0.3053\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.0232\u001b[0m        \u001b[32m2.5369\u001b[0m  0.2855\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.7232\u001b[0m        \u001b[32m2.9913\u001b[0m  0.2924\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.7313\u001b[0m        \u001b[32m2.9889\u001b[0m  0.2953\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.4209\u001b[0m        \u001b[32m2.6867\u001b[0m  0.2634\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.6599\u001b[0m        \u001b[32m1.7906\u001b[0m  0.4142\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.4043\u001b[0m        \u001b[32m2.6938\u001b[0m  0.2747\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.6500\u001b[0m        \u001b[32m1.7885\u001b[0m  0.3699\n",
      "      2        2.9682        2.3775  0.2692\n",
      "      2        \u001b[36m2.7220\u001b[0m        \u001b[32m2.9846\u001b[0m  0.2467\n",
      "      2        2.9737        2.3704  0.3075\n",
      "      2        \u001b[36m3.0029\u001b[0m        \u001b[32m2.5309\u001b[0m  0.2798\n",
      "      2        3.0319        \u001b[32m2.5173\u001b[0m  0.2798\n",
      "      2        \u001b[36m2.7118\u001b[0m        \u001b[32m2.9842\u001b[0m  0.2285\n",
      "      2        \u001b[36m2.4192\u001b[0m        2.6888  0.2403\n",
      "      2        2.4170        2.6976  0.2287\n",
      "      2        \u001b[36m2.6581\u001b[0m        1.7906  0.2777\n",
      "      2        2.6586        \u001b[32m1.7872\u001b[0m  0.2584\n",
      "      3        3.0087        \u001b[32m2.5270\u001b[0m  0.1873\n",
      "      3        \u001b[36m3.0229\u001b[0m        \u001b[32m2.5087\u001b[0m  0.2303\n",
      "      3        \u001b[36m2.6960\u001b[0m        \u001b[32m2.9839\u001b[0m  0.2279\n",
      "      3        \u001b[36m2.9444\u001b[0m        \u001b[32m2.3721\u001b[0m  0.2598\n",
      "      3        \u001b[36m2.9573\u001b[0m        \u001b[32m2.3560\u001b[0m  0.2595\n",
      "      3        \u001b[36m2.6879\u001b[0m        \u001b[32m2.9811\u001b[0m  0.2591\n",
      "      3        \u001b[36m2.4040\u001b[0m        2.7012  0.2454\n",
      "      3        2.6725        \u001b[32m1.7853\u001b[0m  0.2079\n",
      "      3        \u001b[36m2.3913\u001b[0m        2.6898  0.3155\n",
      "      3        \u001b[36m2.6506\u001b[0m        \u001b[32m1.7868\u001b[0m  0.2595\n",
      "      4        3.0321        \u001b[32m2.5190\u001b[0m  0.2497\n",
      "      4        \u001b[36m2.6934\u001b[0m        2.9859  0.2495\n",
      "      4        \u001b[36m2.9951\u001b[0m        \u001b[32m2.4985\u001b[0m  0.2505\n",
      "      4        2.9454        \u001b[32m2.3667\u001b[0m  0.2422\n",
      "      4        2.4065        2.7037  0.2176\n",
      "      4        \u001b[36m2.6724\u001b[0m        \u001b[32m2.9742\u001b[0m  0.2845\n",
      "      4        \u001b[36m2.9517\u001b[0m        \u001b[32m2.3473\u001b[0m  0.3046\n",
      "      4        \u001b[36m2.6141\u001b[0m        \u001b[32m1.7823\u001b[0m  0.2543\n",
      "      4        \u001b[36m2.6203\u001b[0m        \u001b[32m1.7822\u001b[0m  0.2318\n",
      "      4        2.4003        2.6912  0.2694\n",
      "      5        3.0174        \u001b[32m2.5078\u001b[0m  0.2252\n",
      "      5        \u001b[36m2.6867\u001b[0m        \u001b[32m2.9831\u001b[0m  0.2043\n",
      "      5        \u001b[36m2.9759\u001b[0m        \u001b[32m2.4920\u001b[0m  0.2331\n",
      "      5        2.9557        \u001b[32m2.3514\u001b[0m  0.2260\n",
      "      5        2.9699        \u001b[32m2.3404\u001b[0m  0.1918\n",
      "      5        2.6763        \u001b[32m2.9672\u001b[0m  0.2121\n",
      "      5        2.6355        \u001b[32m1.7791\u001b[0m  0.1956\n",
      "      5        \u001b[36m2.3893\u001b[0m        2.7006  0.3094\n",
      "      5        2.6320        \u001b[32m1.7753\u001b[0m  0.1981\n",
      "      6        3.0151        \u001b[32m2.4922\u001b[0m  0.1929\n",
      "      6        2.7054        \u001b[32m2.9765\u001b[0m  0.2053\n",
      "      5        2.3933        2.6943  0.2469\n",
      "      6        2.9483        \u001b[32m2.3351\u001b[0m  0.2294\n",
      "      6        3.0212        \u001b[32m2.4869\u001b[0m  0.2489\n",
      "      6        \u001b[36m2.9311\u001b[0m        \u001b[32m2.3347\u001b[0m  0.2303\n",
      "      6        \u001b[36m2.6252\u001b[0m        \u001b[32m2.9624\u001b[0m  0.2348\n",
      "      6        \u001b[36m2.3761\u001b[0m        \u001b[32m2.6910\u001b[0m  0.1969\n",
      "      6        \u001b[36m2.6082\u001b[0m        \u001b[32m1.7731\u001b[0m  0.2486\n",
      "      7        \u001b[36m2.9808\u001b[0m        \u001b[32m2.4812\u001b[0m  0.2202\n",
      "      6        \u001b[36m2.3783\u001b[0m        2.6938  0.2024\n",
      "      6        \u001b[36m2.5930\u001b[0m        \u001b[32m1.7699\u001b[0m  0.2584\n",
      "      7        \u001b[36m2.6501\u001b[0m        \u001b[32m2.9649\u001b[0m  0.2110\n",
      "      7        \u001b[36m2.9266\u001b[0m        \u001b[32m2.3144\u001b[0m  0.1933\n",
      "      7        \u001b[36m2.9715\u001b[0m        \u001b[32m2.4787\u001b[0m  0.2033\n",
      "      7        \u001b[36m2.5910\u001b[0m        \u001b[32m2.9558\u001b[0m  0.2005\n",
      "      7        \u001b[36m2.9117\u001b[0m        \u001b[32m2.3332\u001b[0m  0.2332\n",
      "      7        2.3809        \u001b[32m2.6811\u001b[0m  0.2045\n",
      "      7        2.3897        2.6910  0.2055\n",
      "      7        2.6017        \u001b[32m1.7646\u001b[0m  0.2098\n",
      "      8        2.6854        \u001b[32m2.9542\u001b[0m  0.2102\n",
      "      7        2.6213        \u001b[32m1.7706\u001b[0m  0.2821\n",
      "      8        \u001b[36m2.9408\u001b[0m        \u001b[32m2.4719\u001b[0m  0.2486\n",
      "      8        \u001b[36m2.8656\u001b[0m        \u001b[32m2.3016\u001b[0m  0.2054\n",
      "      8        3.0133        \u001b[32m2.4704\u001b[0m  0.1974\n",
      "      8        2.6485        \u001b[32m2.9456\u001b[0m  0.1970\n",
      "      8        \u001b[36m2.8996\u001b[0m        2.3343  0.2003\n",
      "      8        \u001b[36m2.3461\u001b[0m        \u001b[32m2.6644\u001b[0m  0.1931\n",
      "      8        2.6300        \u001b[32m1.7561\u001b[0m  0.1910\n",
      "      8        \u001b[36m2.3607\u001b[0m        \u001b[32m2.6855\u001b[0m  0.2102\n",
      "      9        \u001b[36m2.6379\u001b[0m        \u001b[32m2.9444\u001b[0m  0.1943\n",
      "      9        2.9516        \u001b[32m2.4548\u001b[0m  0.2058\n",
      "      9        \u001b[36m2.9625\u001b[0m        \u001b[32m2.4644\u001b[0m  0.1972\n",
      "      9        2.8828        \u001b[32m2.2971\u001b[0m  0.2066\n",
      "      8        2.6090        \u001b[32m1.7647\u001b[0m  0.2398\n",
      "      9        \u001b[36m2.5688\u001b[0m        \u001b[32m2.9327\u001b[0m  0.2112\n",
      "      9        2.9259        2.3366  0.1904\n",
      "      9        \u001b[36m2.3112\u001b[0m        \u001b[32m2.6417\u001b[0m  0.1888\n",
      "     10        2.6597        \u001b[32m2.9355\u001b[0m  0.1768\n",
      "      9        \u001b[36m2.5764\u001b[0m        \u001b[32m1.7439\u001b[0m  0.1934\n",
      "     10        \u001b[36m2.9244\u001b[0m        \u001b[32m2.4478\u001b[0m  0.2049\n",
      "     10        \u001b[36m2.9382\u001b[0m        \u001b[32m2.4575\u001b[0m  0.1926\n",
      "     10        2.9089        \u001b[32m2.2927\u001b[0m  0.1862\n",
      "      9        \u001b[36m2.6056\u001b[0m        \u001b[32m1.7626\u001b[0m  0.1825\n",
      "     10        \u001b[36m2.8695\u001b[0m        2.3404  0.1830\n",
      "Restoring best model from epoch 7.\n",
      "     10        2.6027        \u001b[32m2.9213\u001b[0m  0.1897\n",
      "      9        \u001b[36m2.3557\u001b[0m        \u001b[32m2.6845\u001b[0m  0.2806\n",
      "     10        2.3579        \u001b[32m2.6186\u001b[0m  0.1737\n",
      "     10        \u001b[36m2.5495\u001b[0m        \u001b[32m1.7296\u001b[0m  0.2350\n",
      "     10        \u001b[36m2.5804\u001b[0m        1.7675  0.2245\n",
      "Restoring best model from epoch 9.\n",
      "     10        2.3716        \u001b[32m2.6839\u001b[0m  0.2453\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.7347\u001b[0m        \u001b[32m2.4529\u001b[0m  0.1814\n",
      "      2        \u001b[36m2.7325\u001b[0m        \u001b[32m2.4495\u001b[0m  0.0964\n",
      "      3        2.7497        2.4496  0.0920\n",
      "      4        2.7499        \u001b[32m2.4487\u001b[0m  0.0896\n",
      "      5        \u001b[36m2.7319\u001b[0m        \u001b[32m2.4413\u001b[0m  0.0871\n",
      "      6        \u001b[36m2.7170\u001b[0m        \u001b[32m2.4223\u001b[0m  0.1115\n",
      "      7        \u001b[36m2.7157\u001b[0m        \u001b[32m2.3970\u001b[0m  0.1189\n",
      "      8        \u001b[36m2.6756\u001b[0m        2.4106  0.1323\n",
      "      9        \u001b[36m2.6697\u001b[0m        2.4200  0.1118\n",
      "     10        \u001b[36m2.6608\u001b[0m        2.4148  0.0914\n",
      "Restoring best model from epoch 7.\n",
      "y_train breslow final [-1.000e+00 -6.000e+00 -6.000e+00 -8.000e+00 -9.000e+00 -1.000e+01\n",
      "  1.400e+01 -1.500e+01  1.900e+01 -2.000e+01 -2.200e+01  2.700e+01\n",
      " -3.000e+01  3.100e+01  3.400e+01  3.600e+01 -4.400e+01  5.200e+01\n",
      "  5.600e+01  6.500e+01  6.700e+01 -7.900e+01  9.100e+01  9.100e+01\n",
      "  9.700e+01  1.010e+02  1.020e+02  1.070e+02  1.150e+02  1.290e+02\n",
      " -1.370e+02  1.400e+02  1.530e+02 -1.700e+02 -1.800e+02 -1.830e+02\n",
      "  1.950e+02  2.140e+02 -2.190e+02  2.230e+02 -2.290e+02  2.330e+02\n",
      "  2.470e+02 -2.480e+02 -2.500e+02 -2.600e+02  2.620e+02  2.620e+02\n",
      "  2.720e+02  2.780e+02  2.790e+02  2.830e+02 -2.910e+02  2.960e+02\n",
      "  2.990e+02  3.000e+02 -3.030e+02  3.030e+02  3.040e+02  3.080e+02\n",
      " -3.140e+02  3.150e+02 -3.220e+02 -3.270e+02 -3.280e+02 -3.300e+02\n",
      " -3.410e+02  3.440e+02 -3.450e+02 -3.470e+02  3.470e+02  3.490e+02\n",
      " -3.540e+02 -3.590e+02  3.590e+02 -3.610e+02 -3.630e+02 -3.650e+02\n",
      "  3.660e+02 -3.660e+02 -3.720e+02  3.730e+02  3.810e+02 -3.820e+02\n",
      " -3.870e+02  3.940e+02 -3.950e+02 -3.950e+02 -3.960e+02 -3.990e+02\n",
      " -4.000e+02 -4.060e+02 -4.060e+02 -4.080e+02 -4.090e+02  4.120e+02\n",
      "  4.150e+02 -4.150e+02  4.160e+02  4.190e+02 -4.250e+02 -4.270e+02\n",
      " -4.300e+02  4.320e+02 -4.360e+02 -4.440e+02 -4.470e+02 -4.490e+02\n",
      "  4.520e+02 -4.530e+02 -4.580e+02 -4.680e+02  4.690e+02 -4.720e+02\n",
      " -4.740e+02 -4.760e+02 -4.780e+02 -4.860e+02 -5.000e+02 -5.070e+02\n",
      " -5.120e+02 -5.190e+02 -5.200e+02 -5.200e+02  5.350e+02 -5.380e+02\n",
      "  5.470e+02 -5.540e+02 -5.550e+02 -5.550e+02  5.550e+02  5.560e+02\n",
      "  5.580e+02 -5.620e+02 -5.640e+02 -5.660e+02 -5.700e+02 -5.740e+02\n",
      " -5.750e+02 -5.790e+02  5.810e+02 -5.850e+02 -5.880e+02 -5.940e+02\n",
      "  5.960e+02 -6.010e+02 -6.020e+02 -6.080e+02 -6.080e+02  6.120e+02\n",
      " -6.150e+02 -6.210e+02 -6.300e+02 -6.310e+02 -6.320e+02  6.330e+02\n",
      " -6.360e+02 -6.380e+02  6.390e+02 -6.400e+02  6.430e+02 -6.440e+02\n",
      "  6.490e+02 -6.550e+02 -6.580e+02  6.600e+02 -6.620e+02 -6.710e+02\n",
      " -6.730e+02 -6.800e+02  6.880e+02 -6.930e+02  6.930e+02 -6.980e+02\n",
      " -6.980e+02 -7.010e+02 -7.060e+02  7.110e+02 -7.190e+02 -7.220e+02\n",
      " -7.280e+02 -7.290e+02 -7.440e+02 -7.470e+02  7.570e+02 -7.630e+02\n",
      "  7.650e+02  7.680e+02  7.700e+02 -7.800e+02 -7.820e+02 -7.840e+02\n",
      " -7.990e+02  8.020e+02 -8.100e+02 -8.160e+02 -8.190e+02 -8.290e+02\n",
      "  8.370e+02 -8.480e+02  8.480e+02 -8.490e+02 -8.540e+02 -8.790e+02\n",
      " -8.980e+02 -8.980e+02  8.990e+02 -9.060e+02 -9.100e+02 -9.170e+02\n",
      " -9.250e+02 -9.280e+02  9.310e+02 -9.420e+02 -9.510e+02  1.005e+03\n",
      " -1.049e+03 -1.066e+03 -1.067e+03 -1.085e+03 -1.085e+03  1.088e+03\n",
      " -1.091e+03 -1.098e+03 -1.115e+03  1.135e+03  1.149e+03  1.210e+03\n",
      " -1.219e+03  1.229e+03 -1.231e+03 -1.233e+03 -1.241e+03  1.271e+03\n",
      " -1.295e+03 -1.302e+03 -1.339e+03 -1.345e+03 -1.363e+03  1.372e+03\n",
      "  1.386e+03  1.397e+03  1.423e+03 -1.452e+03  1.490e+03 -1.495e+03\n",
      " -1.516e+03 -1.531e+03 -1.553e+03  1.560e+03  1.622e+03  1.624e+03\n",
      " -1.636e+03  1.685e+03  1.694e+03 -1.711e+03 -1.718e+03 -1.731e+03\n",
      " -1.779e+03  1.791e+03 -1.804e+03  1.852e+03 -1.855e+03 -1.900e+03\n",
      " -1.970e+03 -1.989e+03 -2.015e+03 -2.028e+03 -2.102e+03  2.116e+03\n",
      "  2.131e+03 -2.184e+03 -2.202e+03 -2.245e+03 -2.301e+03 -2.301e+03\n",
      " -2.317e+03 -2.324e+03 -2.412e+03 -2.442e+03  2.456e+03  2.486e+03\n",
      " -2.513e+03  2.532e+03 -2.746e+03 -2.752e+03  3.125e+03  3.258e+03\n",
      " -3.308e+03 -3.437e+03 -3.478e+03 -3.675e+03]\n",
      "Concordance Index 0.4208296557811121\n",
      "Integrated Brier Score: 0.33994677218734953\n",
      "y_train breslow final [-1.000e+00 -6.000e+00 -6.000e+00 -8.000e+00 -9.000e+00 -1.000e+01\n",
      "  1.400e+01 -1.500e+01  1.900e+01 -2.000e+01 -2.200e+01  2.700e+01\n",
      " -3.000e+01  3.100e+01  3.400e+01  3.600e+01 -4.400e+01  5.200e+01\n",
      "  5.600e+01  6.500e+01  6.700e+01 -7.900e+01  9.100e+01  9.100e+01\n",
      "  9.700e+01  1.010e+02  1.020e+02  1.070e+02  1.150e+02  1.290e+02\n",
      " -1.370e+02  1.400e+02  1.530e+02 -1.700e+02 -1.800e+02 -1.830e+02\n",
      "  1.950e+02  2.140e+02 -2.190e+02  2.230e+02 -2.290e+02  2.330e+02\n",
      "  2.470e+02 -2.480e+02 -2.500e+02 -2.600e+02  2.620e+02  2.620e+02\n",
      "  2.720e+02  2.780e+02  2.790e+02  2.830e+02 -2.910e+02  2.960e+02\n",
      "  2.990e+02  3.000e+02 -3.030e+02  3.030e+02  3.040e+02  3.080e+02\n",
      " -3.140e+02  3.150e+02 -3.220e+02 -3.270e+02 -3.280e+02 -3.300e+02\n",
      " -3.410e+02  3.440e+02 -3.450e+02 -3.470e+02  3.470e+02  3.490e+02\n",
      " -3.540e+02 -3.590e+02  3.590e+02 -3.610e+02 -3.630e+02 -3.650e+02\n",
      "  3.660e+02 -3.660e+02 -3.720e+02  3.730e+02  3.810e+02 -3.820e+02\n",
      " -3.870e+02  3.940e+02 -3.950e+02 -3.950e+02 -3.960e+02 -3.990e+02\n",
      " -4.000e+02 -4.060e+02 -4.060e+02 -4.080e+02 -4.090e+02  4.120e+02\n",
      "  4.150e+02 -4.150e+02  4.160e+02  4.190e+02 -4.250e+02 -4.270e+02\n",
      " -4.300e+02  4.320e+02 -4.360e+02 -4.440e+02 -4.470e+02 -4.490e+02\n",
      "  4.520e+02 -4.530e+02 -4.580e+02 -4.680e+02  4.690e+02 -4.720e+02\n",
      " -4.740e+02 -4.760e+02 -4.780e+02 -4.860e+02 -5.000e+02 -5.070e+02\n",
      " -5.120e+02 -5.190e+02 -5.200e+02 -5.200e+02  5.350e+02 -5.380e+02\n",
      "  5.470e+02 -5.540e+02 -5.550e+02 -5.550e+02  5.550e+02  5.560e+02\n",
      "  5.580e+02 -5.620e+02 -5.640e+02 -5.660e+02 -5.700e+02 -5.740e+02\n",
      " -5.750e+02 -5.790e+02  5.810e+02 -5.850e+02 -5.880e+02 -5.940e+02\n",
      "  5.960e+02 -6.010e+02 -6.020e+02 -6.080e+02 -6.080e+02  6.120e+02\n",
      " -6.150e+02 -6.210e+02 -6.300e+02 -6.310e+02 -6.320e+02  6.330e+02\n",
      " -6.360e+02 -6.380e+02  6.390e+02 -6.400e+02  6.430e+02 -6.440e+02\n",
      "  6.490e+02 -6.550e+02 -6.580e+02  6.600e+02 -6.620e+02 -6.710e+02\n",
      " -6.730e+02 -6.800e+02  6.880e+02 -6.930e+02  6.930e+02 -6.980e+02\n",
      " -6.980e+02 -7.010e+02 -7.060e+02  7.110e+02 -7.190e+02 -7.220e+02\n",
      " -7.280e+02 -7.290e+02 -7.440e+02 -7.470e+02  7.570e+02 -7.630e+02\n",
      "  7.650e+02  7.680e+02  7.700e+02 -7.800e+02 -7.820e+02 -7.840e+02\n",
      " -7.990e+02  8.020e+02 -8.100e+02 -8.160e+02 -8.190e+02 -8.290e+02\n",
      "  8.370e+02 -8.480e+02  8.480e+02 -8.490e+02 -8.540e+02 -8.790e+02\n",
      " -8.980e+02 -8.980e+02  8.990e+02 -9.060e+02 -9.100e+02 -9.170e+02\n",
      " -9.250e+02 -9.280e+02  9.310e+02 -9.420e+02 -9.510e+02  1.005e+03\n",
      " -1.049e+03 -1.066e+03 -1.067e+03 -1.085e+03 -1.085e+03  1.088e+03\n",
      " -1.091e+03 -1.098e+03 -1.115e+03  1.135e+03  1.149e+03  1.210e+03\n",
      " -1.219e+03  1.229e+03 -1.231e+03 -1.233e+03 -1.241e+03  1.271e+03\n",
      " -1.295e+03 -1.302e+03 -1.339e+03 -1.345e+03 -1.363e+03  1.372e+03\n",
      "  1.386e+03  1.397e+03  1.423e+03 -1.452e+03  1.490e+03 -1.495e+03\n",
      " -1.516e+03 -1.531e+03 -1.553e+03  1.560e+03  1.622e+03  1.624e+03\n",
      " -1.636e+03  1.685e+03  1.694e+03 -1.711e+03 -1.718e+03 -1.731e+03\n",
      " -1.779e+03  1.791e+03 -1.804e+03  1.852e+03 -1.855e+03 -1.900e+03\n",
      " -1.970e+03 -1.989e+03 -2.015e+03 -2.028e+03 -2.102e+03  2.116e+03\n",
      "  2.131e+03 -2.184e+03 -2.202e+03 -2.245e+03 -2.301e+03 -2.301e+03\n",
      " -2.317e+03 -2.324e+03 -2.412e+03 -2.442e+03  2.456e+03  2.486e+03\n",
      " -2.513e+03  2.532e+03 -2.746e+03 -2.752e+03  3.125e+03  3.258e+03\n",
      " -3.308e+03 -3.437e+03 -3.478e+03 -3.675e+03]\n",
      "durations 6.0 2728.0\n",
      "Concordance Index 0.3605887162714636\n",
      "Integrated Brier Score: 0.30305063811872346\n",
      "split gex_?|100130426      float32\n",
      "gex_?|100133144      float32\n",
      "gex_?|100134869      float32\n",
      "gex_?|10357          float32\n",
      "gex_?|10431          float32\n",
      "                      ...   \n",
      "gex_ZYG11A|440590    float32\n",
      "gex_ZYG11B|79699     float32\n",
      "gex_ZYX|7791         float32\n",
      "gex_ZZEF1|23140      float32\n",
      "gex_ZZZ3|26009       float32\n",
      "Length: 20531, dtype: object\n",
      "(401, 20531) <class 'pandas.core.frame.DataFrame'>\n",
      "(401,) <class 'pandas.core.series.Series'>\n",
      "(101, 20531) <class 'pandas.core.frame.DataFrame'>\n",
      "(101,) <class 'pandas.core.series.Series'>\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.5890\u001b[0m        \u001b[32m2.3566\u001b[0m  0.5169\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.9905\u001b[0m        \u001b[32m3.5054\u001b[0m  0.5478\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.7286\u001b[0m        \u001b[32m3.1858\u001b[0m  0.5171\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.6625\u001b[0m        \u001b[32m3.1513\u001b[0m  0.6340\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.9842\u001b[0m        \u001b[32m3.4995\u001b[0m  0.5576\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.7957\u001b[0m        \u001b[32m3.5591\u001b[0m  0.5682\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.5952\u001b[0m        \u001b[32m2.3596\u001b[0m  0.5623\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.7117\u001b[0m        \u001b[32m3.1850\u001b[0m  0.5046\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.6689\u001b[0m        \u001b[32m3.1473\u001b[0m  0.6003\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.7932\u001b[0m        \u001b[32m3.5580\u001b[0m  0.6058\n",
      "      2        \u001b[36m2.5701\u001b[0m        \u001b[32m2.3531\u001b[0m  0.4898\n",
      "      2        \u001b[36m2.9638\u001b[0m        \u001b[32m3.5024\u001b[0m  0.4445\n",
      "      2        \u001b[36m2.7229\u001b[0m        3.1940  0.3934\n",
      "      2        \u001b[36m2.9518\u001b[0m        \u001b[32m3.4943\u001b[0m  0.3756\n",
      "      2        2.8002        3.5627  0.4378\n",
      "      2        \u001b[36m2.6477\u001b[0m        \u001b[32m3.1498\u001b[0m  0.4678\n",
      "      2        \u001b[36m2.5631\u001b[0m        \u001b[32m2.3573\u001b[0m  0.4407\n",
      "      2        \u001b[36m2.6579\u001b[0m        3.1505  0.3925\n",
      "      2        2.7374        \u001b[32m3.1839\u001b[0m  0.4251\n",
      "      2        \u001b[36m2.7910\u001b[0m        3.5593  0.4241\n",
      "      3        \u001b[36m2.5658\u001b[0m        \u001b[32m2.3442\u001b[0m  0.4133\n",
      "      3        \u001b[36m2.9389\u001b[0m        3.5038  0.3575\n",
      "      3        \u001b[36m2.7144\u001b[0m        3.1872  0.4094\n",
      "      3        2.9847        \u001b[32m3.4861\u001b[0m  0.4340\n",
      "      3        2.6491        \u001b[32m3.1449\u001b[0m  0.4042\n",
      "      3        2.5687        \u001b[32m2.3547\u001b[0m  0.4016\n",
      "      3        \u001b[36m2.7809\u001b[0m        3.5626  0.4209\n",
      "      3        2.7361        3.1890  0.3782\n",
      "      3        \u001b[36m2.6401\u001b[0m        3.1607  0.4108\n",
      "      3        \u001b[36m2.7728\u001b[0m        3.5616  0.4455\n",
      "      4        \u001b[36m2.5654\u001b[0m        \u001b[32m2.3379\u001b[0m  0.4191\n",
      "      4        2.9612        3.5052  0.4244\n",
      "      4        \u001b[36m2.6585\u001b[0m        \u001b[32m3.1728\u001b[0m  0.4260\n",
      "      4        2.9734        \u001b[32m3.4855\u001b[0m  0.4420\n",
      "      4        2.6479        3.1449  0.4571\n",
      "      4        \u001b[36m2.5576\u001b[0m        \u001b[32m2.3477\u001b[0m  0.4554\n",
      "      4        2.7946        3.5636  0.4955\n",
      "      4        \u001b[36m2.6940\u001b[0m        3.1861  0.4262\n",
      "      4        \u001b[36m2.6047\u001b[0m        3.1695  0.4567\n",
      "      4        2.7728        3.5655  0.4411\n",
      "      5        \u001b[36m2.5560\u001b[0m        \u001b[32m2.3298\u001b[0m  0.5792\n",
      "      5        \u001b[36m2.9150\u001b[0m        \u001b[32m3.4967\u001b[0m  0.5854\n",
      "      5        \u001b[36m2.6668\u001b[0m        \u001b[32m3.1634\u001b[0m  0.4111\n",
      "      5        2.9649        3.4902  0.5311\n",
      "      5        \u001b[36m2.6523\u001b[0m        3.1832  0.6082\n",
      "      5        2.5746        \u001b[32m2.3389\u001b[0m  0.4978\n",
      "      5        \u001b[36m2.5979\u001b[0m        3.1556  0.4354\n",
      "      5        \u001b[36m2.6192\u001b[0m        \u001b[32m3.1433\u001b[0m  0.5249\n",
      "      5        \u001b[36m2.7412\u001b[0m        3.5731  0.4046\n",
      "      5        \u001b[36m2.7572\u001b[0m        3.5622  0.5996\n",
      "      6        \u001b[36m2.8843\u001b[0m        \u001b[32m3.4758\u001b[0m  0.3880\n",
      "      6        \u001b[36m2.5520\u001b[0m        \u001b[32m2.3236\u001b[0m  0.4301\n",
      "      6        \u001b[36m2.9414\u001b[0m        3.4887  0.3700\n",
      "      6        2.6702        \u001b[32m3.1570\u001b[0m  0.3856\n",
      "      6        \u001b[36m2.6365\u001b[0m        \u001b[32m3.1687\u001b[0m  0.3713\n",
      "      6        2.6324        \u001b[32m3.1273\u001b[0m  0.4044\n",
      "      6        \u001b[36m2.5742\u001b[0m        3.1523  0.4465\n",
      "      6        \u001b[36m2.5492\u001b[0m        2.3416  0.4626\n",
      "      6        2.7539        3.5760  0.4170\n",
      "      6        \u001b[36m2.7534\u001b[0m        \u001b[32m3.5574\u001b[0m  0.3945\n",
      "      7        \u001b[36m2.5288\u001b[0m        \u001b[32m2.3148\u001b[0m  0.4140\n",
      "      7        \u001b[36m2.9181\u001b[0m        \u001b[32m3.4728\u001b[0m  0.3949\n",
      "      7        \u001b[36m2.6417\u001b[0m        \u001b[32m3.1537\u001b[0m  0.3975\n",
      "      7        \u001b[36m2.6146\u001b[0m        \u001b[32m3.1343\u001b[0m  0.3982\n",
      "      7        \u001b[36m2.6127\u001b[0m        3.1438  0.3534\n",
      "      7        \u001b[36m2.5609\u001b[0m        \u001b[32m3.1441\u001b[0m  0.3587\n",
      "      7        \u001b[36m2.5262\u001b[0m        2.3479  0.3731\n",
      "      7        \u001b[36m2.7169\u001b[0m        3.5843  0.4134\n",
      "      7        \u001b[36m2.7468\u001b[0m        \u001b[32m3.5404\u001b[0m  0.3964\n",
      "      7        2.9086        \u001b[32m3.4432\u001b[0m  0.6460\n",
      "      8        2.9277        \u001b[32m3.4460\u001b[0m  0.4173\n",
      "      8        \u001b[36m2.6270\u001b[0m        \u001b[32m3.1402\u001b[0m  0.4379\n",
      "      8        2.5394        \u001b[32m2.3100\u001b[0m  0.4666\n",
      "      8        \u001b[36m2.6131\u001b[0m        \u001b[32m3.1208\u001b[0m  0.4428\n",
      "      8        \u001b[36m2.5077\u001b[0m        3.1537  0.4003\n",
      "      8        \u001b[36m2.6015\u001b[0m        3.1524  0.4472\n",
      "      8        2.5352        2.3423  0.4550\n",
      "      8        \u001b[36m2.7455\u001b[0m        \u001b[32m3.5151\u001b[0m  0.4051\n",
      "      8        \u001b[36m2.7117\u001b[0m        3.5774  0.4732\n",
      "      8        2.9123        3.4467  0.4389\n",
      "      9        2.9327        3.4543  0.3776\n",
      "      9        \u001b[36m2.5955\u001b[0m        3.1413  0.3764\n",
      "      9        2.5332        2.3132  0.3687\n",
      "      9        \u001b[36m2.4963\u001b[0m        3.1566  0.3445\n",
      "      9        \u001b[36m2.5870\u001b[0m        3.1274  0.3864\n",
      "      9        \u001b[36m2.5698\u001b[0m        3.1457  0.3570\n",
      "      9        \u001b[36m2.5066\u001b[0m        2.3443  0.3883\n",
      "      9        \u001b[36m2.7329\u001b[0m        \u001b[32m3.4686\u001b[0m  0.3730\n",
      "      9        \u001b[36m2.8636\u001b[0m        3.4448  0.3683\n",
      "      9        \u001b[36m2.6672\u001b[0m        3.5588  0.4034\n",
      "     10        \u001b[36m2.5025\u001b[0m        2.3148  0.3319\n",
      "Restoring best model from epoch 8.\n",
      "     10        \u001b[36m2.9037\u001b[0m        3.4606  0.3980\n",
      "Restoring best model from epoch 8.\n",
      "     10        \u001b[36m2.5603\u001b[0m        \u001b[32m3.1376\u001b[0m  0.3883\n",
      "     10        \u001b[36m2.4486\u001b[0m        \u001b[32m3.1378\u001b[0m  0.4031\n",
      "     10        2.5699        3.1509  0.3914\n",
      "Restoring best model from epoch 6.\n",
      "     10        \u001b[36m2.4818\u001b[0m        2.3530  0.3272\n",
      "     10        \u001b[36m2.5796\u001b[0m        3.1260  0.4278\n",
      "Restoring best model from epoch 8.\n",
      "Restoring best model from epoch 5.\n",
      "     10        \u001b[36m2.8562\u001b[0m        3.4461  0.3372\n",
      "Restoring best model from epoch 7.\n",
      "     10        2.6673        3.5669  0.3971\n",
      "     10        \u001b[36m2.6984\u001b[0m        \u001b[32m3.4461\u001b[0m  0.4552\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.8410\u001b[0m        \u001b[32m2.8984\u001b[0m  0.2157\n",
      "      2        \u001b[36m2.8026\u001b[0m        2.9050  0.1734\n",
      "      3        2.8137        2.9149  0.1727\n",
      "      4        2.8210        2.9328  0.1438\n",
      "      5        \u001b[36m2.7842\u001b[0m        2.9333  0.1642\n",
      "      6        2.8056        2.9339  0.1736\n",
      "      7        \u001b[36m2.7770\u001b[0m        2.9453  0.2165\n",
      "      8        \u001b[36m2.7535\u001b[0m        2.9566  0.1503\n",
      "      9        \u001b[36m2.7369\u001b[0m        2.9337  0.1594\n",
      "     10        \u001b[36m2.7304\u001b[0m        2.9243  0.1618\n",
      "Restoring best model from epoch 1.\n",
      "y_train breslow final [  -11.   -15.    18.    19.    22.   -24.   -28.    33.   -35.   -44.\n",
      "   -44.   -50.   -60.   -62.    62.    74.   -79.   -84.    87.    91.\n",
      "    97.    99.   116.   118.  -119.   121.   124.  -129.  -131.  -133.\n",
      "  -134.   139.  -139.  -141.  -151.   154.   161.   164.  -165.   167.\n",
      "  -174.   176.   179.  -179.  -182.  -184.  -186.   189.   193.  -202.\n",
      "   210.  -225.   237.   243.   244.   244.  -256.   257.   258.   260.\n",
      "  -264.   275.   281.   282.  -285.  -287.   291.   300.   303.  -307.\n",
      "   307.   308.  -310.   321.   321.  -323.   336.   339.   340.   343.\n",
      "  -351.  -353.   354.  -354.  -365.   370.  -372.   375.  -377.  -385.\n",
      "   385.  -385.  -408.   409.  -409.  -411.   414.  -414.  -415.  -417.\n",
      "  -418.  -422.  -423.  -424.  -426.  -426.  -427.   428.   434.  -435.\n",
      "  -435.  -440.   442.  -442.   444.   444.  -446.  -448.  -455.   460.\n",
      "  -466.   468.  -476.  -476.  -476.  -477.  -478.  -481.  -486.  -487.\n",
      "   488.  -499.   500.   503.  -515.  -520.  -522.  -526.  -534.  -536.\n",
      "  -537.  -539.  -540.  -541.  -545.  -546.   550.  -551.  -553.   561.\n",
      "  -564.  -565.  -567.  -568.  -568.  -573.   574.  -578.  -578.  -578.\n",
      "   586.  -591.   593.   594.  -595.  -596.   598.  -599.  -600.  -601.\n",
      "  -603.  -603.  -603.   607.  -608.  -609.  -610.  -617.  -624.   624.\n",
      "   625.  -626.   626.  -626.  -629.  -630.  -634.  -651.  -652.  -652.\n",
      "  -653.   656.  -658.   666.  -669.  -670.  -670.  -674.   677.  -677.\n",
      "  -683.  -689.  -690.   694.   697.  -701.   701.   702.  -704.  -704.\n",
      "  -705.   711.  -719.   719.  -724.   731.   737.  -741.  -747.   760.\n",
      "   761.  -761.  -775.   777.  -791.  -791.   800.  -805.   807.  -813.\n",
      "  -824.   826.  -829.  -830.  -839.  -842.  -845.  -852.   855.  -862.\n",
      "  -863.  -864.   864.  -866.   869.  -872.   879.  -882.  -882.  -889.\n",
      "   896.  -896.   896.  -904.   905.  -910.  -912.  -913.   922.  -930.\n",
      "  -938.  -944.  -947.  -949.   949.   952.   976.  -977.  -988.  -993.\n",
      "   995.  -997. -1013.  1026. -1036. -1040.  1043.  1046. -1060. -1071.\n",
      " -1072.  1073. -1079.  1081. -1097.  1115. -1118. -1119. -1125. -1126.\n",
      " -1126. -1130. -1148. -1157. -1159. -1163.  1167.  1171. -1175. -1178.\n",
      " -1189.  1194.  1209.  1215. -1216.  1229. -1233.  1235. -1246.  1258.\n",
      "  1265.  1268. -1272. -1280.  1288. -1289.  1293. -1301. -1305. -1324.\n",
      " -1351.  1357. -1367. -1369.  1379. -1400.  1421. -1429. -1431. -1432.\n",
      " -1442.  1454. -1479.  1498.  1499.  1501.  1516. -1523.  1528.  1531.\n",
      " -1559. -1617. -1621.  1622.  1632.  1653. -1683. -1700.  1725. -1728.\n",
      " -1750.  1778.  1790.  1798.  1830. -1847. -1864. -1870. -1893. -1932.\n",
      " -1965. -1974.  2027. -2065. -2067. -2109. -2137. -2161.  2174. -2199.\n",
      " -2224. -2248. -2261.  2318. -2368. -2449. -2488. -2515. -2595. -2616.\n",
      "  2617.  2620. -2696. -2823. -2832. -2973. -3059. -3094.  3169. -3261.\n",
      " -3305.  3361. -3674. -3759. -3940. -4765.  4961. -4992. -6732. -7062.\n",
      " -7248.]\n",
      "Concordance Index 0.5535548957634965\n",
      "Integrated Brier Score: 0.2019559578707649\n",
      "y_train breslow final [  -11.   -15.    18.    19.    22.   -24.   -28.    33.   -35.   -44.\n",
      "   -44.   -50.   -60.   -62.    62.    74.   -79.   -84.    87.    91.\n",
      "    97.    99.   116.   118.  -119.   121.   124.  -129.  -131.  -133.\n",
      "  -134.   139.  -139.  -141.  -151.   154.   161.   164.  -165.   167.\n",
      "  -174.   176.   179.  -179.  -182.  -184.  -186.   189.   193.  -202.\n",
      "   210.  -225.   237.   243.   244.   244.  -256.   257.   258.   260.\n",
      "  -264.   275.   281.   282.  -285.  -287.   291.   300.   303.  -307.\n",
      "   307.   308.  -310.   321.   321.  -323.   336.   339.   340.   343.\n",
      "  -351.  -353.   354.  -354.  -365.   370.  -372.   375.  -377.  -385.\n",
      "   385.  -385.  -408.   409.  -409.  -411.   414.  -414.  -415.  -417.\n",
      "  -418.  -422.  -423.  -424.  -426.  -426.  -427.   428.   434.  -435.\n",
      "  -435.  -440.   442.  -442.   444.   444.  -446.  -448.  -455.   460.\n",
      "  -466.   468.  -476.  -476.  -476.  -477.  -478.  -481.  -486.  -487.\n",
      "   488.  -499.   500.   503.  -515.  -520.  -522.  -526.  -534.  -536.\n",
      "  -537.  -539.  -540.  -541.  -545.  -546.   550.  -551.  -553.   561.\n",
      "  -564.  -565.  -567.  -568.  -568.  -573.   574.  -578.  -578.  -578.\n",
      "   586.  -591.   593.   594.  -595.  -596.   598.  -599.  -600.  -601.\n",
      "  -603.  -603.  -603.   607.  -608.  -609.  -610.  -617.  -624.   624.\n",
      "   625.  -626.   626.  -626.  -629.  -630.  -634.  -651.  -652.  -652.\n",
      "  -653.   656.  -658.   666.  -669.  -670.  -670.  -674.   677.  -677.\n",
      "  -683.  -689.  -690.   694.   697.  -701.   701.   702.  -704.  -704.\n",
      "  -705.   711.  -719.   719.  -724.   731.   737.  -741.  -747.   760.\n",
      "   761.  -761.  -775.   777.  -791.  -791.   800.  -805.   807.  -813.\n",
      "  -824.   826.  -829.  -830.  -839.  -842.  -845.  -852.   855.  -862.\n",
      "  -863.  -864.   864.  -866.   869.  -872.   879.  -882.  -882.  -889.\n",
      "   896.  -896.   896.  -904.   905.  -910.  -912.  -913.   922.  -930.\n",
      "  -938.  -944.  -947.  -949.   949.   952.   976.  -977.  -988.  -993.\n",
      "   995.  -997. -1013.  1026. -1036. -1040.  1043.  1046. -1060. -1071.\n",
      " -1072.  1073. -1079.  1081. -1097.  1115. -1118. -1119. -1125. -1126.\n",
      " -1126. -1130. -1148. -1157. -1159. -1163.  1167.  1171. -1175. -1178.\n",
      " -1189.  1194.  1209.  1215. -1216.  1229. -1233.  1235. -1246.  1258.\n",
      "  1265.  1268. -1272. -1280.  1288. -1289.  1293. -1301. -1305. -1324.\n",
      " -1351.  1357. -1367. -1369.  1379. -1400.  1421. -1429. -1431. -1432.\n",
      " -1442.  1454. -1479.  1498.  1499.  1501.  1516. -1523.  1528.  1531.\n",
      " -1559. -1617. -1621.  1622.  1632.  1653. -1683. -1700.  1725. -1728.\n",
      " -1750.  1778.  1790.  1798.  1830. -1847. -1864. -1870. -1893. -1932.\n",
      " -1965. -1974.  2027. -2065. -2067. -2109. -2137. -2161.  2174. -2199.\n",
      " -2224. -2248. -2261.  2318. -2368. -2449. -2488. -2515. -2595. -2616.\n",
      "  2617.  2620. -2696. -2823. -2832. -2973. -3059. -3094.  3169. -3261.\n",
      " -3305.  3361. -3674. -3759. -3940. -4765.  4961. -4992. -6732. -7062.\n",
      " -7248.]\n",
      "durations 4.0 3635.0\n",
      "Concordance Index 0.5239307535641547\n",
      "Integrated Brier Score: 0.2073462003309286\n",
      "(401, 20531) <class 'pandas.core.frame.DataFrame'>\n",
      "(401,) <class 'pandas.core.series.Series'>\n",
      "(101, 20531) <class 'pandas.core.frame.DataFrame'>\n",
      "(101,) <class 'pandas.core.series.Series'>\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.0684\u001b[0m  0.0353\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.0268\u001b[0m  0.0309\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.3507\u001b[0m  0.0342\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      2        3.0684  0.0311\n",
      "      2        3.0268  0.0361\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      3        3.0684  0.0360\n",
      "      2        3.3507  0.0401\n",
      "      3        3.0268  0.0324\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing optimizer.\n",
      "      3        3.3507  0.0310\n",
      "      4        3.0684  0.0346\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      4        3.0268  0.0326\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.3419\u001b[0m  0.0455\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.2833\u001b[0m  0.0434\n",
      "      5        3.0684  0.0469\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.0204\u001b[0m  0.0429\n",
      "      4        3.3507  0.0504\n",
      "      5        3.0268  0.0509\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m2.8571\u001b[0m  0.0468\n",
      "      2        3.2833  0.0374\n",
      "      6        3.0684  0.0436\n",
      "      5        3.3507  0.0415\n",
      "      2        3.0204  0.0420\n",
      "      2        2.8571  0.0302\n",
      "      2        3.3419  0.0606\n",
      "      6        3.0268  0.0487\n",
      "      3        3.0204  0.0368\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m2.8742\u001b[0m  0.0605\n",
      "      6        3.3507  0.0392\n",
      "      7        3.0684  0.0404\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.0698\u001b[0m  0.0585\n",
      "      3        3.2833  0.0471\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      7        3.0268  0.0360\n",
      "      3        3.3419  0.0476\n",
      "      3        2.8571  0.0509\n",
      "      4        3.2833  0.0341\n",
      "      2        2.8742  0.0387\n",
      "      8        3.0684  0.0369\n",
      "      4        3.0204  0.0411\n",
      "      4        3.3419  0.0286\n",
      "      7        3.3507  0.0452\n",
      "      2        3.0698  0.0445\n",
      "      8        3.0268  0.0411\n",
      "      4        2.8571  0.0333\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      9        3.0684  0.0313\n",
      "      3        2.8742  0.0317\n",
      "      5        3.2833  0.0357\n",
      "      5        3.0204  0.0344\n",
      "      3        3.0698  0.0348\n",
      "      5        3.3419  0.0407\n",
      "      5        2.8571  0.0333\n",
      "      8        3.3507  0.0443\n",
      "      4        2.8742  0.0329\n",
      "      6        3.2833  0.0308\n",
      "      6        3.0204  0.0312\n",
      "      6        2.8571  0.0333\n",
      "      6        3.3419  0.0349\n",
      "      9        3.3507  0.0353\n",
      "      7        3.2833  0.0349\n",
      "      5        2.8742  0.0387\n",
      "     10        3.0684  0.0802\n",
      "      7        2.8571  0.0303\n",
      "Restoring best model from epoch 1.\n",
      "      4        3.0698  0.0707\n",
      "     10        3.3507  0.0334\n",
      "Restoring best model from epoch 1.\n",
      "      7        3.3419  0.0403\n",
      "      9        3.0268  0.1161\n",
      "      8        3.2833  0.0378\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.2961\u001b[0m  0.0967\n",
      "      5        3.0698  0.0311\n",
      "      6        2.8742  0.0481\n",
      "      8        2.8571  0.0442\n",
      "      7        3.0204  0.0887\n",
      "      8        3.3419  0.0386\n",
      "     10        3.0268  0.0406\n",
      "      2        3.2961  0.0298\n",
      "Restoring best model from epoch 1.\n",
      "      9        3.2833  0.0341\n",
      "      6        3.0698  0.0305\n",
      "      7        2.8742  0.0375\n",
      "      9        2.8571  0.0347\n",
      "      9        3.3419  0.0304\n",
      "      8        3.0204  0.0381\n",
      "      3        3.2961  0.0500\n",
      "      8        2.8742  0.0343\n",
      "     10        3.3419  0.0317\n",
      "Restoring best model from epoch 1.\n",
      "      9        3.0204  0.0342\n",
      "     10        3.2833  0.0690\n",
      "Restoring best model from epoch 1.\n",
      "     10        2.8571  0.0537\n",
      "Restoring best model from epoch 1.\n",
      "      7        3.0698  0.0732\n",
      "      4        3.2961  0.0351\n",
      "      9        2.8742  0.0320\n",
      "     10        3.0204  0.0325\n",
      "Restoring best model from epoch 1.\n",
      "      5        3.2961  0.0298\n",
      "     10        2.8742  0.0294\n",
      "Restoring best model from epoch 1.\n",
      "      8        3.0698  0.0395\n",
      "      9        3.0698  0.0343\n",
      "      6        3.2961  0.0503\n",
      "     10        3.0698  0.0275\n",
      "Restoring best model from epoch 1.\n",
      "      7        3.2961  0.0277\n",
      "      8        3.2961  0.0285\n",
      "      9        3.2961  0.0282\n",
      "     10        3.2961  0.0270\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m2.3658\u001b[0m  0.0307\n",
      "      2        2.3658  0.0173\n",
      "      3        2.3658  0.0206\n",
      "      4        2.3658  0.0180\n",
      "      5        2.3658  0.0138\n",
      "      6        2.3658  0.0327\n",
      "      7        2.3658  0.0200\n",
      "      8        2.3658  0.0188\n",
      "      9        2.3658  0.0197\n",
      "     10        2.3658  0.0148\n",
      "Restoring best model from epoch 1.\n",
      "y_train breslow final [ 4.000e+00 -1.100e+01 -1.300e+01 -1.400e+01 -1.500e+01  1.900e+01\n",
      "  2.200e+01 -2.800e+01  3.300e+01 -3.600e+01  3.800e+01 -4.800e+01\n",
      "  5.800e+01 -6.200e+01  7.400e+01 -7.900e+01 -8.400e+01 -8.400e+01\n",
      "  8.700e+01  9.100e+01  9.700e+01  1.160e+02  1.190e+02 -1.190e+02\n",
      "  1.240e+02 -1.290e+02 -1.310e+02 -1.340e+02  1.390e+02 -1.390e+02\n",
      " -1.410e+02 -1.510e+02  1.610e+02  1.640e+02  1.670e+02  1.710e+02\n",
      "  1.730e+02 -1.740e+02  1.760e+02 -1.820e+02 -1.840e+02 -1.860e+02\n",
      "  1.870e+02  1.890e+02  1.930e+02  2.100e+02 -2.180e+02 -2.240e+02\n",
      " -2.250e+02 -2.300e+02  2.430e+02  2.440e+02  2.500e+02 -2.560e+02\n",
      "  2.570e+02 -2.600e+02 -2.640e+02  2.680e+02  2.740e+02  2.810e+02\n",
      "  2.820e+02  2.910e+02  3.000e+02  3.030e+02 -3.070e+02  3.080e+02\n",
      " -3.100e+02  3.210e+02  3.210e+02 -3.230e+02  3.360e+02  3.360e+02\n",
      "  3.390e+02  3.430e+02 -3.510e+02  3.540e+02 -3.540e+02 -3.650e+02\n",
      "  3.700e+02  3.750e+02  3.760e+02 -3.850e+02  3.850e+02 -3.850e+02\n",
      "  4.090e+02 -4.090e+02 -4.110e+02  4.140e+02 -4.150e+02 -4.150e+02\n",
      " -4.160e+02 -4.170e+02 -4.180e+02 -4.220e+02 -4.260e+02 -4.260e+02\n",
      " -4.270e+02  4.280e+02  4.290e+02  4.340e+02  4.340e+02 -4.350e+02\n",
      " -4.350e+02 -4.350e+02  4.420e+02  4.440e+02  4.440e+02 -4.460e+02\n",
      " -4.480e+02 -4.550e+02 -4.570e+02  4.570e+02  4.600e+02 -4.620e+02\n",
      "  4.640e+02 -4.660e+02 -4.670e+02  4.680e+02  4.680e+02  4.690e+02\n",
      " -4.760e+02 -4.760e+02 -4.780e+02  4.780e+02 -4.810e+02 -4.840e+02\n",
      " -4.860e+02 -4.870e+02  4.880e+02 -4.920e+02 -4.990e+02  5.000e+02\n",
      "  5.030e+02 -5.050e+02 -5.130e+02 -5.150e+02 -5.200e+02 -5.220e+02\n",
      " -5.260e+02 -5.310e+02 -5.360e+02 -5.370e+02 -5.390e+02 -5.400e+02\n",
      " -5.410e+02 -5.460e+02 -5.470e+02  5.500e+02 -5.520e+02 -5.530e+02\n",
      "  5.570e+02  5.610e+02 -5.640e+02 -5.650e+02 -5.670e+02 -5.680e+02\n",
      " -5.680e+02 -5.680e+02 -5.730e+02  5.740e+02 -5.780e+02 -5.780e+02\n",
      " -5.780e+02  5.820e+02  5.860e+02 -5.910e+02 -5.920e+02 -5.960e+02\n",
      " -6.000e+02 -6.030e+02 -6.030e+02 -6.050e+02 -6.050e+02 -6.070e+02\n",
      "  6.070e+02 -6.080e+02 -6.090e+02 -6.100e+02 -6.100e+02 -6.100e+02\n",
      " -6.140e+02  6.140e+02 -6.170e+02 -6.170e+02 -6.240e+02  6.250e+02\n",
      " -6.260e+02  6.260e+02 -6.260e+02  6.280e+02 -6.290e+02 -6.510e+02\n",
      " -6.520e+02 -6.520e+02 -6.530e+02  6.530e+02  6.560e+02 -6.570e+02\n",
      " -6.580e+02 -6.580e+02 -6.640e+02  6.660e+02 -6.690e+02 -6.700e+02\n",
      " -6.740e+02  6.770e+02 -6.770e+02 -6.890e+02 -6.900e+02 -6.910e+02\n",
      "  6.940e+02 -7.010e+02  7.010e+02  7.020e+02 -7.040e+02 -7.040e+02\n",
      " -7.050e+02  7.110e+02 -7.180e+02 -7.190e+02  7.190e+02 -7.240e+02\n",
      " -7.280e+02 -7.300e+02  7.310e+02 -7.390e+02 -7.400e+02 -7.410e+02\n",
      " -7.470e+02  7.600e+02 -7.610e+02  7.610e+02 -7.730e+02 -7.750e+02\n",
      "  7.770e+02 -7.910e+02 -7.910e+02 -7.910e+02 -7.910e+02 -8.000e+02\n",
      " -8.050e+02 -8.060e+02  8.080e+02 -8.240e+02 -8.240e+02 -8.270e+02\n",
      " -8.390e+02 -8.420e+02 -8.450e+02 -8.520e+02  8.550e+02 -8.620e+02\n",
      " -8.630e+02 -8.640e+02  8.640e+02 -8.660e+02 -8.720e+02  8.790e+02\n",
      " -8.820e+02 -8.820e+02 -8.880e+02 -8.890e+02 -8.890e+02  8.960e+02\n",
      "  8.960e+02 -9.040e+02  9.050e+02 -9.100e+02 -9.120e+02 -9.130e+02\n",
      "  9.290e+02 -9.300e+02 -9.380e+02 -9.440e+02  9.490e+02  9.500e+02\n",
      "  9.520e+02 -9.770e+02  9.870e+02 -9.880e+02  9.950e+02  9.950e+02\n",
      " -9.970e+02  9.990e+02 -1.013e+03  1.026e+03 -1.036e+03 -1.040e+03\n",
      "  1.043e+03  1.046e+03 -1.060e+03  1.073e+03 -1.079e+03  1.081e+03\n",
      " -1.097e+03  1.115e+03 -1.118e+03 -1.119e+03 -1.126e+03 -1.130e+03\n",
      "  1.135e+03  1.147e+03 -1.157e+03 -1.159e+03 -1.163e+03  1.171e+03\n",
      " -1.175e+03 -1.178e+03 -1.189e+03 -1.189e+03  1.194e+03  1.197e+03\n",
      "  1.215e+03 -1.216e+03  1.229e+03 -1.233e+03  1.235e+03 -1.239e+03\n",
      " -1.246e+03  1.258e+03 -1.272e+03 -1.280e+03 -1.285e+03  1.288e+03\n",
      " -1.289e+03 -1.301e+03 -1.305e+03 -1.324e+03 -1.333e+03  1.357e+03\n",
      " -1.367e+03 -1.369e+03 -1.400e+03 -1.431e+03 -1.432e+03 -1.442e+03\n",
      "  1.454e+03 -1.474e+03  1.492e+03  1.499e+03  1.501e+03 -1.523e+03\n",
      "  1.528e+03  1.531e+03 -1.559e+03  1.600e+03 -1.617e+03 -1.621e+03\n",
      "  1.632e+03  1.653e+03 -1.683e+03 -1.700e+03  1.725e+03 -1.728e+03\n",
      "  1.778e+03  1.790e+03  1.798e+03  1.830e+03 -1.847e+03 -1.870e+03\n",
      " -1.893e+03 -1.932e+03 -1.974e+03  2.027e+03 -2.065e+03 -2.109e+03\n",
      " -2.137e+03 -2.161e+03  2.174e+03 -2.199e+03 -2.224e+03 -2.248e+03\n",
      " -2.261e+03  2.318e+03 -2.360e+03 -2.368e+03  2.393e+03 -2.488e+03\n",
      " -2.515e+03 -2.590e+03 -2.595e+03 -2.616e+03  2.620e+03 -2.676e+03\n",
      "  2.681e+03 -2.696e+03 -2.823e+03 -2.973e+03 -3.094e+03  3.169e+03\n",
      " -3.261e+03 -3.305e+03  3.361e+03 -3.635e+03 -3.759e+03 -3.940e+03\n",
      "  4.961e+03 -4.992e+03 -6.732e+03 -7.062e+03 -7.248e+03]\n",
      "Concordance Index 0.4884754847863787\n",
      "Integrated Brier Score: 0.18947511214568788\n",
      "y_train breslow final [ 4.000e+00 -1.100e+01 -1.300e+01 -1.400e+01 -1.500e+01  1.900e+01\n",
      "  2.200e+01 -2.800e+01  3.300e+01 -3.600e+01  3.800e+01 -4.800e+01\n",
      "  5.800e+01 -6.200e+01  7.400e+01 -7.900e+01 -8.400e+01 -8.400e+01\n",
      "  8.700e+01  9.100e+01  9.700e+01  1.160e+02  1.190e+02 -1.190e+02\n",
      "  1.240e+02 -1.290e+02 -1.310e+02 -1.340e+02  1.390e+02 -1.390e+02\n",
      " -1.410e+02 -1.510e+02  1.610e+02  1.640e+02  1.670e+02  1.710e+02\n",
      "  1.730e+02 -1.740e+02  1.760e+02 -1.820e+02 -1.840e+02 -1.860e+02\n",
      "  1.870e+02  1.890e+02  1.930e+02  2.100e+02 -2.180e+02 -2.240e+02\n",
      " -2.250e+02 -2.300e+02  2.430e+02  2.440e+02  2.500e+02 -2.560e+02\n",
      "  2.570e+02 -2.600e+02 -2.640e+02  2.680e+02  2.740e+02  2.810e+02\n",
      "  2.820e+02  2.910e+02  3.000e+02  3.030e+02 -3.070e+02  3.080e+02\n",
      " -3.100e+02  3.210e+02  3.210e+02 -3.230e+02  3.360e+02  3.360e+02\n",
      "  3.390e+02  3.430e+02 -3.510e+02  3.540e+02 -3.540e+02 -3.650e+02\n",
      "  3.700e+02  3.750e+02  3.760e+02 -3.850e+02  3.850e+02 -3.850e+02\n",
      "  4.090e+02 -4.090e+02 -4.110e+02  4.140e+02 -4.150e+02 -4.150e+02\n",
      " -4.160e+02 -4.170e+02 -4.180e+02 -4.220e+02 -4.260e+02 -4.260e+02\n",
      " -4.270e+02  4.280e+02  4.290e+02  4.340e+02  4.340e+02 -4.350e+02\n",
      " -4.350e+02 -4.350e+02  4.420e+02  4.440e+02  4.440e+02 -4.460e+02\n",
      " -4.480e+02 -4.550e+02 -4.570e+02  4.570e+02  4.600e+02 -4.620e+02\n",
      "  4.640e+02 -4.660e+02 -4.670e+02  4.680e+02  4.680e+02  4.690e+02\n",
      " -4.760e+02 -4.760e+02 -4.780e+02  4.780e+02 -4.810e+02 -4.840e+02\n",
      " -4.860e+02 -4.870e+02  4.880e+02 -4.920e+02 -4.990e+02  5.000e+02\n",
      "  5.030e+02 -5.050e+02 -5.130e+02 -5.150e+02 -5.200e+02 -5.220e+02\n",
      " -5.260e+02 -5.310e+02 -5.360e+02 -5.370e+02 -5.390e+02 -5.400e+02\n",
      " -5.410e+02 -5.460e+02 -5.470e+02  5.500e+02 -5.520e+02 -5.530e+02\n",
      "  5.570e+02  5.610e+02 -5.640e+02 -5.650e+02 -5.670e+02 -5.680e+02\n",
      " -5.680e+02 -5.680e+02 -5.730e+02  5.740e+02 -5.780e+02 -5.780e+02\n",
      " -5.780e+02  5.820e+02  5.860e+02 -5.910e+02 -5.920e+02 -5.960e+02\n",
      " -6.000e+02 -6.030e+02 -6.030e+02 -6.050e+02 -6.050e+02 -6.070e+02\n",
      "  6.070e+02 -6.080e+02 -6.090e+02 -6.100e+02 -6.100e+02 -6.100e+02\n",
      " -6.140e+02  6.140e+02 -6.170e+02 -6.170e+02 -6.240e+02  6.250e+02\n",
      " -6.260e+02  6.260e+02 -6.260e+02  6.280e+02 -6.290e+02 -6.510e+02\n",
      " -6.520e+02 -6.520e+02 -6.530e+02  6.530e+02  6.560e+02 -6.570e+02\n",
      " -6.580e+02 -6.580e+02 -6.640e+02  6.660e+02 -6.690e+02 -6.700e+02\n",
      " -6.740e+02  6.770e+02 -6.770e+02 -6.890e+02 -6.900e+02 -6.910e+02\n",
      "  6.940e+02 -7.010e+02  7.010e+02  7.020e+02 -7.040e+02 -7.040e+02\n",
      " -7.050e+02  7.110e+02 -7.180e+02 -7.190e+02  7.190e+02 -7.240e+02\n",
      " -7.280e+02 -7.300e+02  7.310e+02 -7.390e+02 -7.400e+02 -7.410e+02\n",
      " -7.470e+02  7.600e+02 -7.610e+02  7.610e+02 -7.730e+02 -7.750e+02\n",
      "  7.770e+02 -7.910e+02 -7.910e+02 -7.910e+02 -7.910e+02 -8.000e+02\n",
      " -8.050e+02 -8.060e+02  8.080e+02 -8.240e+02 -8.240e+02 -8.270e+02\n",
      " -8.390e+02 -8.420e+02 -8.450e+02 -8.520e+02  8.550e+02 -8.620e+02\n",
      " -8.630e+02 -8.640e+02  8.640e+02 -8.660e+02 -8.720e+02  8.790e+02\n",
      " -8.820e+02 -8.820e+02 -8.880e+02 -8.890e+02 -8.890e+02  8.960e+02\n",
      "  8.960e+02 -9.040e+02  9.050e+02 -9.100e+02 -9.120e+02 -9.130e+02\n",
      "  9.290e+02 -9.300e+02 -9.380e+02 -9.440e+02  9.490e+02  9.500e+02\n",
      "  9.520e+02 -9.770e+02  9.870e+02 -9.880e+02  9.950e+02  9.950e+02\n",
      " -9.970e+02  9.990e+02 -1.013e+03  1.026e+03 -1.036e+03 -1.040e+03\n",
      "  1.043e+03  1.046e+03 -1.060e+03  1.073e+03 -1.079e+03  1.081e+03\n",
      " -1.097e+03  1.115e+03 -1.118e+03 -1.119e+03 -1.126e+03 -1.130e+03\n",
      "  1.135e+03  1.147e+03 -1.157e+03 -1.159e+03 -1.163e+03  1.171e+03\n",
      " -1.175e+03 -1.178e+03 -1.189e+03 -1.189e+03  1.194e+03  1.197e+03\n",
      "  1.215e+03 -1.216e+03  1.229e+03 -1.233e+03  1.235e+03 -1.239e+03\n",
      " -1.246e+03  1.258e+03 -1.272e+03 -1.280e+03 -1.285e+03  1.288e+03\n",
      " -1.289e+03 -1.301e+03 -1.305e+03 -1.324e+03 -1.333e+03  1.357e+03\n",
      " -1.367e+03 -1.369e+03 -1.400e+03 -1.431e+03 -1.432e+03 -1.442e+03\n",
      "  1.454e+03 -1.474e+03  1.492e+03  1.499e+03  1.501e+03 -1.523e+03\n",
      "  1.528e+03  1.531e+03 -1.559e+03  1.600e+03 -1.617e+03 -1.621e+03\n",
      "  1.632e+03  1.653e+03 -1.683e+03 -1.700e+03  1.725e+03 -1.728e+03\n",
      "  1.778e+03  1.790e+03  1.798e+03  1.830e+03 -1.847e+03 -1.870e+03\n",
      " -1.893e+03 -1.932e+03 -1.974e+03  2.027e+03 -2.065e+03 -2.109e+03\n",
      " -2.137e+03 -2.161e+03  2.174e+03 -2.199e+03 -2.224e+03 -2.248e+03\n",
      " -2.261e+03  2.318e+03 -2.360e+03 -2.368e+03  2.393e+03 -2.488e+03\n",
      " -2.515e+03 -2.590e+03 -2.595e+03 -2.616e+03  2.620e+03 -2.676e+03\n",
      "  2.681e+03 -2.696e+03 -2.823e+03 -2.973e+03 -3.094e+03  3.169e+03\n",
      " -3.261e+03 -3.305e+03  3.361e+03 -3.635e+03 -3.759e+03 -3.940e+03\n",
      "  4.961e+03 -4.992e+03 -6.732e+03 -7.062e+03 -7.248e+03]\n",
      "durations 18.0 4765.0\n",
      "Concordance Index 0.4067343666488509\n",
      "Integrated Brier Score: 0.2005198409829566\n",
      "(402, 20531) <class 'pandas.core.frame.DataFrame'>\n",
      "(402,) <class 'pandas.core.series.Series'>\n",
      "(100, 20531) <class 'pandas.core.frame.DataFrame'>\n",
      "(100,) <class 'pandas.core.series.Series'>\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m2.8641\u001b[0m  0.0367\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m2.1188\u001b[0m  0.0632\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m2.5087\u001b[0m  0.0382\n",
      "      2        2.1188  0.0355\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2        2.8641  0.0662\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m2.8728\u001b[0m  0.0367\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.0242\u001b[0m  0.0386\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.0052\u001b[0m  0.0565\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m2.6374\u001b[0m  0.0477\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m2.5026\u001b[0m  0.0507\n",
      "      3        2.8641  0.0339\n",
      "      3        2.1188  0.0469\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m2.6523\u001b[0m  0.0442\n",
      "      2        2.5087  0.0543\n",
      "      2        3.0242  0.0351\n",
      "      2        2.8728  0.0433\n",
      "      2        3.0052  0.0380\n",
      "      2        2.5026  0.0380\n",
      "      2        2.6374  0.0486\n",
      "      4        2.8641  0.0448\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m2.1494\u001b[0m  0.0344\n",
      "      3        2.8728  0.0336\n",
      "      3        2.5087  0.0491\n",
      "      4        2.1188  0.0533\n",
      "      3        2.5026  0.0390\n",
      "      3        3.0242  0.0627\n",
      "      2        2.6523  0.0694\n",
      "      2        2.1494  0.0402\n",
      "      4        2.8728  0.0345\n",
      "      4        2.5087  0.0372\n",
      "      5        2.1188  0.0372\n",
      "      3        2.6374  0.0611\n",
      "      5        2.8641  0.0552\n",
      "      3        3.0052  0.0783\n",
      "      4        2.5026  0.0481\n",
      "      5        2.8728  0.0386\n",
      "      6        2.1188  0.0394\n",
      "      4        3.0242  0.0654\n",
      "      5        2.5087  0.0495\n",
      "      3        2.6523  0.0710\n",
      "      3        2.1494  0.0699\n",
      "      4        2.6374  0.0569\n",
      "      6        2.8641  0.0578\n",
      "      5        2.5026  0.0494\n",
      "      4        3.0052  0.0676\n",
      "      7        2.1188  0.0421\n",
      "      6        2.5087  0.0345\n",
      "      5        3.0242  0.0398\n",
      "      4        2.1494  0.0312\n",
      "      6        2.8728  0.0624\n",
      "      4        2.6523  0.0414\n",
      "      5        3.0052  0.0282\n",
      "      6        2.5026  0.0368\n",
      "      7        2.8641  0.0467\n",
      "      5        2.6374  0.0508\n",
      "      8        2.1188  0.0308\n",
      "      7        2.5087  0.0357\n",
      "      7        2.8728  0.0294\n",
      "      6        3.0242  0.0365\n",
      "      5        2.1494  0.0305\n",
      "      5        2.6523  0.0382\n",
      "      6        2.6374  0.0291\n",
      "      8        2.8641  0.0313\n",
      "      6        3.0052  0.0358\n",
      "      7        2.5026  0.0370\n",
      "      9        2.1188  0.0424\n",
      "      8        2.8728  0.0363\n",
      "      7        3.0242  0.0370\n",
      "      6        2.1494  0.0365\n",
      "      8        2.5087  0.0408\n",
      "      7        2.6374  0.0339\n",
      "      8        2.5026  0.0328\n",
      "      6        2.6523  0.0529\n",
      "      7        3.0052  0.0452\n",
      "      7        2.1494  0.0305\n",
      "      9        2.8641  0.0491\n",
      "     10        2.1188  0.0346\n",
      "Restoring best model from epoch 1.\n",
      "      8        3.0242  0.0334\n",
      "      9        2.8728  0.0391\n",
      "      9        2.5087  0.0493\n",
      "      9        2.5026  0.0340\n",
      "      8        2.6374  0.0391\n",
      "      9        3.0242  0.0292\n",
      "      8        2.1494  0.0350\n",
      "      8        3.0052  0.0403\n",
      "     10        2.8641  0.0423\n",
      "Restoring best model from epoch 1.\n",
      "     10        2.8728  0.0373\n",
      "Restoring best model from epoch 1.\n",
      "     10        2.5087  0.0306\n",
      "     10        2.5026  0.0285\n",
      "Restoring best model from epoch 1.\n",
      "Restoring best model from epoch 1.\n",
      "      7        2.6523  0.0585\n",
      "      9        2.6374  0.0396\n",
      "     10        3.0242  0.0321\n",
      "Restoring best model from epoch 1.\n",
      "      9        3.0052  0.0315\n",
      "     10        2.6374  0.0272\n",
      "      8        2.6523  0.0384\n",
      "Restoring best model from epoch 1.\n",
      "      9        2.1494  0.0630\n",
      "     10        3.0052  0.0371\n",
      "Restoring best model from epoch 1.\n",
      "      9        2.6523  0.0299\n",
      "     10        2.1494  0.0286\n",
      "Restoring best model from epoch 1.\n",
      "     10        2.6523  0.0361\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m2.9482\u001b[0m  0.0204\n",
      "      2        2.9482  0.0156\n",
      "      3        2.9482  0.0201\n",
      "      4        2.9482  0.0142\n",
      "      5        2.9482  0.0145\n",
      "      6        2.9482  0.0190\n",
      "      7        2.9482  0.0178\n",
      "      8        2.9482  0.0155\n",
      "      9        2.9482  0.0141\n",
      "     10        2.9482  0.0554\n",
      "Restoring best model from epoch 1.\n",
      "y_train breslow final [ 4.000e+00 -1.300e+01 -1.400e+01  1.800e+01  1.900e+01 -2.400e+01\n",
      " -2.800e+01  3.300e+01 -3.500e+01 -3.600e+01  3.800e+01 -4.400e+01\n",
      " -4.400e+01 -4.800e+01 -5.000e+01  5.800e+01 -6.000e+01 -6.200e+01\n",
      "  6.200e+01  7.400e+01 -8.400e+01  9.700e+01  9.900e+01  1.160e+02\n",
      "  1.180e+02  1.190e+02  1.210e+02  1.240e+02 -1.330e+02 -1.340e+02\n",
      " -1.390e+02 -1.410e+02  1.540e+02  1.610e+02  1.640e+02 -1.650e+02\n",
      "  1.670e+02  1.710e+02  1.730e+02  1.760e+02  1.790e+02 -1.790e+02\n",
      " -1.820e+02 -1.840e+02 -1.860e+02 -1.860e+02  1.870e+02  1.890e+02\n",
      "  1.930e+02 -2.020e+02  2.100e+02 -2.180e+02 -2.240e+02 -2.250e+02\n",
      " -2.300e+02  2.370e+02  2.440e+02  2.500e+02 -2.560e+02  2.570e+02\n",
      "  2.580e+02  2.600e+02 -2.600e+02 -2.640e+02  2.680e+02  2.740e+02\n",
      "  2.750e+02  2.810e+02 -2.850e+02 -2.870e+02  2.910e+02  3.030e+02\n",
      " -3.070e+02  3.070e+02  3.080e+02  3.210e+02 -3.230e+02  3.360e+02\n",
      "  3.360e+02  3.400e+02  3.430e+02 -3.510e+02 -3.530e+02  3.540e+02\n",
      " -3.540e+02 -3.720e+02  3.750e+02  3.760e+02 -3.770e+02 -3.850e+02\n",
      "  3.850e+02 -4.080e+02 -4.090e+02 -4.110e+02  4.140e+02 -4.140e+02\n",
      " -4.150e+02 -4.150e+02 -4.160e+02 -4.170e+02 -4.180e+02 -4.230e+02\n",
      " -4.240e+02  4.280e+02  4.290e+02  4.340e+02 -4.350e+02 -4.350e+02\n",
      " -4.400e+02  4.420e+02 -4.420e+02  4.440e+02 -4.480e+02 -4.480e+02\n",
      " -4.550e+02 -4.570e+02  4.570e+02 -4.620e+02  4.640e+02 -4.660e+02\n",
      " -4.670e+02  4.680e+02  4.680e+02  4.690e+02 -4.760e+02 -4.760e+02\n",
      " -4.760e+02 -4.770e+02 -4.780e+02  4.780e+02 -4.810e+02 -4.840e+02\n",
      " -4.920e+02 -4.990e+02  5.000e+02  5.030e+02 -5.050e+02 -5.130e+02\n",
      " -5.150e+02 -5.150e+02 -5.200e+02 -5.260e+02 -5.310e+02 -5.340e+02\n",
      " -5.360e+02 -5.370e+02 -5.400e+02 -5.410e+02 -5.450e+02 -5.470e+02\n",
      "  5.500e+02 -5.510e+02 -5.520e+02 -5.530e+02  5.570e+02  5.610e+02\n",
      " -5.650e+02 -5.680e+02 -5.680e+02 -5.680e+02 -5.730e+02  5.740e+02\n",
      "  5.820e+02  5.860e+02 -5.920e+02  5.930e+02  5.940e+02 -5.950e+02\n",
      "  5.980e+02 -5.990e+02 -6.000e+02 -6.010e+02 -6.030e+02 -6.030e+02\n",
      " -6.050e+02 -6.050e+02 -6.070e+02  6.070e+02 -6.080e+02 -6.090e+02\n",
      " -6.100e+02 -6.100e+02 -6.100e+02 -6.140e+02  6.140e+02 -6.170e+02\n",
      " -6.170e+02  6.240e+02  6.250e+02 -6.260e+02  6.260e+02 -6.260e+02\n",
      "  6.280e+02 -6.290e+02 -6.300e+02 -6.340e+02 -6.510e+02 -6.510e+02\n",
      " -6.520e+02 -6.530e+02  6.530e+02  6.560e+02 -6.570e+02 -6.580e+02\n",
      " -6.580e+02 -6.640e+02 -6.690e+02 -6.700e+02 -6.700e+02 -6.740e+02\n",
      " -6.830e+02 -6.900e+02 -6.910e+02  6.970e+02 -7.010e+02  7.010e+02\n",
      " -7.040e+02 -7.040e+02 -7.050e+02 -7.050e+02 -7.180e+02 -7.190e+02\n",
      "  7.190e+02 -7.240e+02 -7.280e+02 -7.300e+02  7.310e+02  7.370e+02\n",
      " -7.390e+02 -7.400e+02 -7.410e+02 -7.470e+02 -7.610e+02  7.610e+02\n",
      " -7.610e+02 -7.730e+02 -7.750e+02  7.770e+02 -7.910e+02 -7.910e+02\n",
      " -7.910e+02 -7.910e+02  8.000e+02 -8.000e+02 -8.060e+02  8.070e+02\n",
      "  8.080e+02 -8.130e+02 -8.240e+02  8.260e+02 -8.270e+02 -8.290e+02\n",
      " -8.300e+02 -8.390e+02 -8.450e+02 -8.520e+02  8.550e+02 -8.630e+02\n",
      "  8.640e+02  8.690e+02 -8.720e+02  8.790e+02 -8.820e+02 -8.820e+02\n",
      " -8.880e+02 -8.890e+02  8.960e+02 -8.960e+02  8.960e+02 -9.040e+02\n",
      " -9.130e+02  9.220e+02  9.290e+02 -9.380e+02 -9.440e+02 -9.470e+02\n",
      " -9.490e+02  9.490e+02  9.500e+02  9.520e+02  9.760e+02 -9.770e+02\n",
      "  9.870e+02 -9.930e+02  9.950e+02  9.990e+02 -1.013e+03  1.026e+03\n",
      " -1.036e+03 -1.040e+03 -1.060e+03 -1.071e+03 -1.072e+03  1.073e+03\n",
      "  1.081e+03 -1.118e+03 -1.119e+03 -1.125e+03 -1.126e+03 -1.126e+03\n",
      "  1.135e+03  1.147e+03 -1.148e+03 -1.157e+03 -1.159e+03 -1.163e+03\n",
      "  1.167e+03  1.171e+03 -1.178e+03 -1.189e+03 -1.189e+03  1.194e+03\n",
      "  1.197e+03  1.209e+03  1.215e+03 -1.216e+03  1.229e+03 -1.233e+03\n",
      "  1.235e+03 -1.239e+03 -1.246e+03  1.258e+03  1.265e+03  1.268e+03\n",
      " -1.272e+03 -1.280e+03 -1.285e+03 -1.289e+03  1.293e+03 -1.305e+03\n",
      " -1.324e+03 -1.333e+03 -1.351e+03  1.357e+03 -1.367e+03 -1.369e+03\n",
      "  1.379e+03 -1.400e+03  1.421e+03 -1.429e+03 -1.432e+03 -1.474e+03\n",
      " -1.479e+03  1.492e+03  1.498e+03  1.499e+03  1.501e+03  1.516e+03\n",
      " -1.523e+03  1.528e+03  1.531e+03 -1.559e+03  1.600e+03 -1.621e+03\n",
      "  1.622e+03  1.632e+03  1.653e+03 -1.700e+03  1.725e+03 -1.728e+03\n",
      " -1.750e+03  1.798e+03 -1.847e+03 -1.864e+03 -1.893e+03 -1.932e+03\n",
      " -1.965e+03 -1.974e+03 -2.065e+03 -2.067e+03 -2.137e+03 -2.161e+03\n",
      "  2.174e+03 -2.199e+03 -2.224e+03 -2.248e+03 -2.261e+03  2.318e+03\n",
      " -2.360e+03 -2.368e+03  2.393e+03 -2.449e+03 -2.488e+03 -2.515e+03\n",
      " -2.590e+03 -2.595e+03  2.617e+03 -2.676e+03  2.681e+03 -2.696e+03\n",
      " -2.832e+03 -2.973e+03 -3.059e+03 -3.094e+03 -3.261e+03 -3.305e+03\n",
      "  3.361e+03 -3.635e+03 -3.674e+03 -3.940e+03 -4.765e+03 -6.732e+03]\n",
      "Concordance Index 0.46433281004709576\n",
      "Integrated Brier Score: 0.20123628562382373\n",
      "y_train breslow final [ 4.000e+00 -1.300e+01 -1.400e+01  1.800e+01  1.900e+01 -2.400e+01\n",
      " -2.800e+01  3.300e+01 -3.500e+01 -3.600e+01  3.800e+01 -4.400e+01\n",
      " -4.400e+01 -4.800e+01 -5.000e+01  5.800e+01 -6.000e+01 -6.200e+01\n",
      "  6.200e+01  7.400e+01 -8.400e+01  9.700e+01  9.900e+01  1.160e+02\n",
      "  1.180e+02  1.190e+02  1.210e+02  1.240e+02 -1.330e+02 -1.340e+02\n",
      " -1.390e+02 -1.410e+02  1.540e+02  1.610e+02  1.640e+02 -1.650e+02\n",
      "  1.670e+02  1.710e+02  1.730e+02  1.760e+02  1.790e+02 -1.790e+02\n",
      " -1.820e+02 -1.840e+02 -1.860e+02 -1.860e+02  1.870e+02  1.890e+02\n",
      "  1.930e+02 -2.020e+02  2.100e+02 -2.180e+02 -2.240e+02 -2.250e+02\n",
      " -2.300e+02  2.370e+02  2.440e+02  2.500e+02 -2.560e+02  2.570e+02\n",
      "  2.580e+02  2.600e+02 -2.600e+02 -2.640e+02  2.680e+02  2.740e+02\n",
      "  2.750e+02  2.810e+02 -2.850e+02 -2.870e+02  2.910e+02  3.030e+02\n",
      " -3.070e+02  3.070e+02  3.080e+02  3.210e+02 -3.230e+02  3.360e+02\n",
      "  3.360e+02  3.400e+02  3.430e+02 -3.510e+02 -3.530e+02  3.540e+02\n",
      " -3.540e+02 -3.720e+02  3.750e+02  3.760e+02 -3.770e+02 -3.850e+02\n",
      "  3.850e+02 -4.080e+02 -4.090e+02 -4.110e+02  4.140e+02 -4.140e+02\n",
      " -4.150e+02 -4.150e+02 -4.160e+02 -4.170e+02 -4.180e+02 -4.230e+02\n",
      " -4.240e+02  4.280e+02  4.290e+02  4.340e+02 -4.350e+02 -4.350e+02\n",
      " -4.400e+02  4.420e+02 -4.420e+02  4.440e+02 -4.480e+02 -4.480e+02\n",
      " -4.550e+02 -4.570e+02  4.570e+02 -4.620e+02  4.640e+02 -4.660e+02\n",
      " -4.670e+02  4.680e+02  4.680e+02  4.690e+02 -4.760e+02 -4.760e+02\n",
      " -4.760e+02 -4.770e+02 -4.780e+02  4.780e+02 -4.810e+02 -4.840e+02\n",
      " -4.920e+02 -4.990e+02  5.000e+02  5.030e+02 -5.050e+02 -5.130e+02\n",
      " -5.150e+02 -5.150e+02 -5.200e+02 -5.260e+02 -5.310e+02 -5.340e+02\n",
      " -5.360e+02 -5.370e+02 -5.400e+02 -5.410e+02 -5.450e+02 -5.470e+02\n",
      "  5.500e+02 -5.510e+02 -5.520e+02 -5.530e+02  5.570e+02  5.610e+02\n",
      " -5.650e+02 -5.680e+02 -5.680e+02 -5.680e+02 -5.730e+02  5.740e+02\n",
      "  5.820e+02  5.860e+02 -5.920e+02  5.930e+02  5.940e+02 -5.950e+02\n",
      "  5.980e+02 -5.990e+02 -6.000e+02 -6.010e+02 -6.030e+02 -6.030e+02\n",
      " -6.050e+02 -6.050e+02 -6.070e+02  6.070e+02 -6.080e+02 -6.090e+02\n",
      " -6.100e+02 -6.100e+02 -6.100e+02 -6.140e+02  6.140e+02 -6.170e+02\n",
      " -6.170e+02  6.240e+02  6.250e+02 -6.260e+02  6.260e+02 -6.260e+02\n",
      "  6.280e+02 -6.290e+02 -6.300e+02 -6.340e+02 -6.510e+02 -6.510e+02\n",
      " -6.520e+02 -6.530e+02  6.530e+02  6.560e+02 -6.570e+02 -6.580e+02\n",
      " -6.580e+02 -6.640e+02 -6.690e+02 -6.700e+02 -6.700e+02 -6.740e+02\n",
      " -6.830e+02 -6.900e+02 -6.910e+02  6.970e+02 -7.010e+02  7.010e+02\n",
      " -7.040e+02 -7.040e+02 -7.050e+02 -7.050e+02 -7.180e+02 -7.190e+02\n",
      "  7.190e+02 -7.240e+02 -7.280e+02 -7.300e+02  7.310e+02  7.370e+02\n",
      " -7.390e+02 -7.400e+02 -7.410e+02 -7.470e+02 -7.610e+02  7.610e+02\n",
      " -7.610e+02 -7.730e+02 -7.750e+02  7.770e+02 -7.910e+02 -7.910e+02\n",
      " -7.910e+02 -7.910e+02  8.000e+02 -8.000e+02 -8.060e+02  8.070e+02\n",
      "  8.080e+02 -8.130e+02 -8.240e+02  8.260e+02 -8.270e+02 -8.290e+02\n",
      " -8.300e+02 -8.390e+02 -8.450e+02 -8.520e+02  8.550e+02 -8.630e+02\n",
      "  8.640e+02  8.690e+02 -8.720e+02  8.790e+02 -8.820e+02 -8.820e+02\n",
      " -8.880e+02 -8.890e+02  8.960e+02 -8.960e+02  8.960e+02 -9.040e+02\n",
      " -9.130e+02  9.220e+02  9.290e+02 -9.380e+02 -9.440e+02 -9.470e+02\n",
      " -9.490e+02  9.490e+02  9.500e+02  9.520e+02  9.760e+02 -9.770e+02\n",
      "  9.870e+02 -9.930e+02  9.950e+02  9.990e+02 -1.013e+03  1.026e+03\n",
      " -1.036e+03 -1.040e+03 -1.060e+03 -1.071e+03 -1.072e+03  1.073e+03\n",
      "  1.081e+03 -1.118e+03 -1.119e+03 -1.125e+03 -1.126e+03 -1.126e+03\n",
      "  1.135e+03  1.147e+03 -1.148e+03 -1.157e+03 -1.159e+03 -1.163e+03\n",
      "  1.167e+03  1.171e+03 -1.178e+03 -1.189e+03 -1.189e+03  1.194e+03\n",
      "  1.197e+03  1.209e+03  1.215e+03 -1.216e+03  1.229e+03 -1.233e+03\n",
      "  1.235e+03 -1.239e+03 -1.246e+03  1.258e+03  1.265e+03  1.268e+03\n",
      " -1.272e+03 -1.280e+03 -1.285e+03 -1.289e+03  1.293e+03 -1.305e+03\n",
      " -1.324e+03 -1.333e+03 -1.351e+03  1.357e+03 -1.367e+03 -1.369e+03\n",
      "  1.379e+03 -1.400e+03  1.421e+03 -1.429e+03 -1.432e+03 -1.474e+03\n",
      " -1.479e+03  1.492e+03  1.498e+03  1.499e+03  1.501e+03  1.516e+03\n",
      " -1.523e+03  1.528e+03  1.531e+03 -1.559e+03  1.600e+03 -1.621e+03\n",
      "  1.622e+03  1.632e+03  1.653e+03 -1.700e+03  1.725e+03 -1.728e+03\n",
      " -1.750e+03  1.798e+03 -1.847e+03 -1.864e+03 -1.893e+03 -1.932e+03\n",
      " -1.965e+03 -1.974e+03 -2.065e+03 -2.067e+03 -2.137e+03 -2.161e+03\n",
      "  2.174e+03 -2.199e+03 -2.224e+03 -2.248e+03 -2.261e+03  2.318e+03\n",
      " -2.360e+03 -2.368e+03  2.393e+03 -2.449e+03 -2.488e+03 -2.515e+03\n",
      " -2.590e+03 -2.595e+03  2.617e+03 -2.676e+03  2.681e+03 -2.696e+03\n",
      " -2.832e+03 -2.973e+03 -3.059e+03 -3.094e+03 -3.261e+03 -3.305e+03\n",
      "  3.361e+03 -3.635e+03 -3.674e+03 -3.940e+03 -4.765e+03 -6.732e+03]\n",
      "durations 11.0 7248.0\n",
      "Concordance Index 0.5049450549450549\n",
      "Integrated Brier Score: 0.19139275581165024\n",
      "(402, 20531) <class 'pandas.core.frame.DataFrame'>\n",
      "(402,) <class 'pandas.core.series.Series'>\n",
      "(100, 20531) <class 'pandas.core.frame.DataFrame'>\n",
      "(100,) <class 'pandas.core.series.Series'>\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.2746\u001b[0m  0.0280\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2        3.2746  0.0392\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.0392\u001b[0m  0.0344\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.6595\u001b[0m  0.0496\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.0924\u001b[0m  0.0331\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.6489\u001b[0m  0.0388\n",
      "      3        3.2746  0.0353\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m2.9338\u001b[0m  0.0412\n",
      "      2        3.0392  0.0380\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      4        3.2746  0.0346\n",
      "      2        3.6595  0.0487\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2        3.0924  0.0536\n",
      "      2        3.6489  0.0504\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.0721\u001b[0m  0.0363\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      3        3.0392  0.0403\n",
      "      5        3.2746  0.0284\n",
      "      3        3.6595  0.0334\n",
      "      3        3.0924  0.0345\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      3        3.6489  0.0377\n",
      "      2        2.9338  0.0792\n",
      "      4        3.0392  0.0298\n",
      "      2        3.0721  0.0394\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.0302\u001b[0m  0.0464\n",
      "      6        3.2746  0.0437      4        3.6595  0.0356\n",
      "\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m2.9367\u001b[0m  0.0367\n",
      "      5        3.0392  0.0315\n",
      "      4        3.6489  0.0351\n",
      "      4        3.0924  0.0417\n",
      "      3        3.0721  0.0352\n",
      "      3        2.9338  0.0396\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.2639\u001b[0m  0.0395\n",
      "      2        3.0302  0.0422\n",
      "      5        3.6595  0.0380\n",
      "      6        3.0392  0.0295\n",
      "      7        3.2746  0.0391\n",
      "      5        3.0924  0.0304\n",
      "      4        2.9338  0.0319\n",
      "      4        3.0721  0.0361\n",
      "      2        2.9367  0.0486\n",
      "      5        3.6489  0.0539\n",
      "      2        3.2639  0.0342\n",
      "      8        3.2746  0.0325\n",
      "      7        3.0392  0.0372\n",
      "      6        3.6595  0.0375\n",
      "      3        3.0302  0.0434\n",
      "      6        3.0924  0.0323\n",
      "      3        2.9367  0.0302\n",
      "      5        3.0721  0.0357\n",
      "      5        2.9338  0.0387\n",
      "      6        3.6489  0.0291\n",
      "      3        3.2639  0.0292\n",
      "      9        3.2746  0.0351\n",
      "      7        3.0924  0.0319\n",
      "      7        3.6595  0.0344\n",
      "      8        3.0392  0.0379\n",
      "      4        3.0302  0.0378\n",
      "      6        3.0721  0.0299\n",
      "      6        2.9338  0.0342\n",
      "      4        2.9367  0.0413\n",
      "      7        3.6489  0.0366\n",
      "      4        3.2639  0.0356\n",
      "     10        3.2746  0.0304\n",
      "Restoring best model from epoch 1.\n",
      "      8        3.0924  0.0311\n",
      "      9        3.0392  0.0327\n",
      "      8        3.6595  0.0359\n",
      "      5        3.0302  0.0356\n",
      "      7        3.0721  0.0355\n",
      "      7        2.9338  0.0350\n",
      "      8        3.6489  0.0288\n",
      "      5        2.9367  0.0364\n",
      "      5        3.2639  0.0363\n",
      "      9        3.0924  0.0339\n",
      "     10        3.0392  0.0341\n",
      "Restoring best model from epoch 1.\n",
      "      9        3.6595  0.0369\n",
      "      8        3.0721  0.0352\n",
      "      8        2.9338  0.0320\n",
      "      6        3.0302  0.0412\n",
      "      6        2.9367  0.0303\n",
      "      6        3.2639  0.0279\n",
      "      9        3.6489  0.0391\n",
      "     10        3.0924  0.0277\n",
      "Restoring best model from epoch 1.\n",
      "     10        3.6595  0.0300\n",
      "Restoring best model from epoch 1.\n",
      "      9        2.9338  0.0311\n",
      "      9        3.0721  0.0377\n",
      "     10        3.6489  0.0282\n",
      "      7        3.0302  0.0357\n",
      "      7        3.2639  0.0358\n",
      "      7        2.9367  0.0369\n",
      "Restoring best model from epoch 1.\n",
      "     10        2.9338  0.0275\n",
      "Restoring best model from epoch 1.\n",
      "     10        3.0721  0.0355\n",
      "Restoring best model from epoch 1.\n",
      "      8        3.0302  0.0375\n",
      "      8        3.2639  0.0343\n",
      "      8        2.9367  0.0351\n",
      "      9        2.9367  0.0306\n",
      "      9        3.0302  0.0348\n",
      "      9        3.2639  0.0370\n",
      "     10        2.9367  0.0337\n",
      "Restoring best model from epoch 1.\n",
      "     10        3.0302  0.0343\n",
      "Restoring best model from epoch 1.\n",
      "     10        3.2639  0.0326\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m2.9358\u001b[0m  0.0222\n",
      "      2        2.9358  0.0152\n",
      "      3        2.9358  0.0157\n",
      "      4        2.9358  0.0177\n",
      "      5        2.9358  0.0177\n",
      "      6        2.9358  0.0354\n",
      "      7        2.9358  0.0171\n",
      "      8        2.9358  0.0155\n",
      "      9        2.9358  0.0166\n",
      "     10        2.9358  0.0166\n",
      "Restoring best model from epoch 1.\n",
      "y_train breslow final [ 4.000e+00 -1.100e+01 -1.300e+01 -1.400e+01 -1.500e+01  1.800e+01\n",
      "  1.900e+01  2.200e+01 -2.400e+01  3.300e+01 -3.500e+01 -3.600e+01\n",
      "  3.800e+01 -4.400e+01 -4.400e+01 -4.800e+01 -5.000e+01  5.800e+01\n",
      " -6.000e+01  6.200e+01 -7.900e+01 -8.400e+01 -8.400e+01  8.700e+01\n",
      "  9.100e+01  9.700e+01  9.900e+01  1.180e+02  1.190e+02 -1.190e+02\n",
      "  1.210e+02 -1.290e+02 -1.310e+02 -1.330e+02 -1.340e+02  1.390e+02\n",
      " -1.410e+02 -1.510e+02  1.540e+02  1.610e+02 -1.650e+02  1.710e+02\n",
      "  1.730e+02 -1.740e+02  1.790e+02 -1.790e+02 -1.820e+02 -1.860e+02\n",
      " -1.860e+02  1.870e+02  1.890e+02 -2.020e+02  2.100e+02 -2.180e+02\n",
      " -2.240e+02 -2.300e+02  2.370e+02  2.430e+02  2.440e+02  2.440e+02\n",
      "  2.500e+02  2.580e+02  2.600e+02 -2.600e+02 -2.640e+02  2.680e+02\n",
      "  2.740e+02  2.750e+02  2.820e+02 -2.850e+02 -2.870e+02  3.000e+02\n",
      " -3.070e+02  3.070e+02 -3.100e+02  3.210e+02  3.210e+02  3.360e+02\n",
      "  3.360e+02  3.390e+02  3.400e+02  3.430e+02 -3.510e+02 -3.530e+02\n",
      "  3.540e+02 -3.650e+02  3.700e+02 -3.720e+02  3.750e+02  3.760e+02\n",
      " -3.770e+02 -3.850e+02  3.850e+02 -3.850e+02 -4.080e+02  4.090e+02\n",
      " -4.140e+02 -4.150e+02 -4.150e+02 -4.160e+02 -4.180e+02 -4.220e+02\n",
      " -4.230e+02 -4.240e+02 -4.260e+02 -4.260e+02 -4.270e+02  4.290e+02\n",
      "  4.340e+02  4.340e+02 -4.350e+02 -4.350e+02 -4.350e+02 -4.400e+02\n",
      "  4.420e+02 -4.420e+02  4.440e+02 -4.460e+02 -4.480e+02 -4.480e+02\n",
      " -4.570e+02  4.570e+02  4.600e+02 -4.620e+02  4.640e+02 -4.670e+02\n",
      "  4.680e+02  4.690e+02 -4.760e+02 -4.760e+02 -4.760e+02 -4.770e+02\n",
      "  4.780e+02 -4.810e+02 -4.840e+02 -4.860e+02 -4.870e+02  4.880e+02\n",
      " -4.920e+02 -5.050e+02 -5.130e+02 -5.150e+02 -5.150e+02 -5.200e+02\n",
      " -5.220e+02 -5.310e+02 -5.340e+02 -5.360e+02 -5.390e+02 -5.400e+02\n",
      " -5.410e+02 -5.450e+02 -5.460e+02 -5.470e+02 -5.510e+02 -5.520e+02\n",
      " -5.530e+02  5.570e+02  5.610e+02 -5.640e+02 -5.650e+02 -5.670e+02\n",
      " -5.680e+02  5.740e+02 -5.780e+02 -5.780e+02 -5.780e+02  5.820e+02\n",
      " -5.910e+02 -5.920e+02  5.930e+02  5.940e+02 -5.950e+02 -5.960e+02\n",
      "  5.980e+02 -5.990e+02 -6.000e+02 -6.010e+02 -6.030e+02 -6.030e+02\n",
      " -6.030e+02 -6.050e+02 -6.050e+02 -6.070e+02 -6.090e+02 -6.100e+02\n",
      " -6.100e+02 -6.100e+02 -6.140e+02  6.140e+02 -6.170e+02 -6.170e+02\n",
      " -6.240e+02  6.240e+02  6.280e+02 -6.290e+02 -6.300e+02 -6.340e+02\n",
      " -6.510e+02 -6.510e+02 -6.520e+02  6.530e+02 -6.570e+02 -6.580e+02\n",
      " -6.640e+02  6.660e+02 -6.700e+02 -6.700e+02  6.770e+02 -6.770e+02\n",
      " -6.830e+02 -6.890e+02 -6.900e+02 -6.910e+02  6.940e+02  6.970e+02\n",
      " -7.010e+02  7.020e+02 -7.040e+02 -7.050e+02 -7.050e+02  7.110e+02\n",
      " -7.180e+02  7.190e+02 -7.240e+02 -7.280e+02 -7.300e+02  7.310e+02\n",
      "  7.370e+02 -7.390e+02 -7.400e+02  7.600e+02 -7.610e+02 -7.610e+02\n",
      " -7.730e+02 -7.750e+02  7.770e+02 -7.910e+02 -7.910e+02  8.000e+02\n",
      " -8.000e+02 -8.050e+02 -8.060e+02  8.070e+02  8.080e+02 -8.130e+02\n",
      " -8.240e+02 -8.240e+02  8.260e+02 -8.270e+02 -8.290e+02 -8.300e+02\n",
      " -8.390e+02 -8.420e+02 -8.620e+02 -8.640e+02 -8.660e+02  8.690e+02\n",
      "  8.790e+02 -8.820e+02 -8.880e+02 -8.890e+02 -8.890e+02  8.960e+02\n",
      " -8.960e+02 -9.040e+02  9.050e+02 -9.100e+02 -9.120e+02  9.220e+02\n",
      "  9.290e+02 -9.300e+02 -9.470e+02 -9.490e+02  9.490e+02  9.500e+02\n",
      "  9.520e+02  9.760e+02 -9.770e+02  9.870e+02 -9.880e+02 -9.930e+02\n",
      "  9.950e+02  9.950e+02 -9.970e+02  9.990e+02 -1.040e+03  1.043e+03\n",
      "  1.046e+03 -1.071e+03 -1.072e+03  1.073e+03 -1.079e+03  1.081e+03\n",
      " -1.097e+03  1.115e+03 -1.119e+03 -1.125e+03 -1.126e+03 -1.130e+03\n",
      "  1.135e+03  1.147e+03 -1.148e+03 -1.157e+03 -1.159e+03 -1.163e+03\n",
      "  1.167e+03  1.171e+03 -1.175e+03 -1.189e+03 -1.189e+03  1.197e+03\n",
      "  1.209e+03  1.215e+03 -1.216e+03 -1.233e+03  1.235e+03 -1.239e+03\n",
      " -1.246e+03  1.258e+03  1.265e+03  1.268e+03 -1.280e+03 -1.285e+03\n",
      "  1.288e+03  1.293e+03 -1.301e+03 -1.324e+03 -1.333e+03 -1.351e+03\n",
      "  1.357e+03 -1.367e+03 -1.369e+03  1.379e+03 -1.400e+03  1.421e+03\n",
      " -1.429e+03 -1.431e+03 -1.432e+03 -1.442e+03  1.454e+03 -1.474e+03\n",
      " -1.479e+03  1.492e+03  1.498e+03  1.499e+03  1.516e+03  1.528e+03\n",
      "  1.531e+03 -1.559e+03  1.600e+03 -1.617e+03  1.622e+03  1.632e+03\n",
      " -1.683e+03 -1.700e+03  1.725e+03 -1.728e+03 -1.750e+03  1.778e+03\n",
      "  1.790e+03  1.830e+03 -1.864e+03 -1.870e+03 -1.932e+03 -1.965e+03\n",
      " -1.974e+03  2.027e+03 -2.067e+03 -2.109e+03 -2.224e+03 -2.261e+03\n",
      "  2.318e+03 -2.360e+03 -2.368e+03  2.393e+03 -2.449e+03 -2.590e+03\n",
      " -2.616e+03  2.617e+03  2.620e+03 -2.676e+03  2.681e+03 -2.696e+03\n",
      " -2.823e+03 -2.832e+03 -2.973e+03 -3.059e+03 -3.094e+03  3.169e+03\n",
      " -3.261e+03 -3.305e+03  3.361e+03 -3.635e+03 -3.674e+03 -3.759e+03\n",
      " -4.765e+03  4.961e+03 -4.992e+03 -6.732e+03 -7.062e+03 -7.248e+03]\n",
      "Concordance Index 0.4604603949048757\n",
      "Integrated Brier Score: 0.18529172302647648\n",
      "y_train breslow final [ 4.000e+00 -1.100e+01 -1.300e+01 -1.400e+01 -1.500e+01  1.800e+01\n",
      "  1.900e+01  2.200e+01 -2.400e+01  3.300e+01 -3.500e+01 -3.600e+01\n",
      "  3.800e+01 -4.400e+01 -4.400e+01 -4.800e+01 -5.000e+01  5.800e+01\n",
      " -6.000e+01  6.200e+01 -7.900e+01 -8.400e+01 -8.400e+01  8.700e+01\n",
      "  9.100e+01  9.700e+01  9.900e+01  1.180e+02  1.190e+02 -1.190e+02\n",
      "  1.210e+02 -1.290e+02 -1.310e+02 -1.330e+02 -1.340e+02  1.390e+02\n",
      " -1.410e+02 -1.510e+02  1.540e+02  1.610e+02 -1.650e+02  1.710e+02\n",
      "  1.730e+02 -1.740e+02  1.790e+02 -1.790e+02 -1.820e+02 -1.860e+02\n",
      " -1.860e+02  1.870e+02  1.890e+02 -2.020e+02  2.100e+02 -2.180e+02\n",
      " -2.240e+02 -2.300e+02  2.370e+02  2.430e+02  2.440e+02  2.440e+02\n",
      "  2.500e+02  2.580e+02  2.600e+02 -2.600e+02 -2.640e+02  2.680e+02\n",
      "  2.740e+02  2.750e+02  2.820e+02 -2.850e+02 -2.870e+02  3.000e+02\n",
      " -3.070e+02  3.070e+02 -3.100e+02  3.210e+02  3.210e+02  3.360e+02\n",
      "  3.360e+02  3.390e+02  3.400e+02  3.430e+02 -3.510e+02 -3.530e+02\n",
      "  3.540e+02 -3.650e+02  3.700e+02 -3.720e+02  3.750e+02  3.760e+02\n",
      " -3.770e+02 -3.850e+02  3.850e+02 -3.850e+02 -4.080e+02  4.090e+02\n",
      " -4.140e+02 -4.150e+02 -4.150e+02 -4.160e+02 -4.180e+02 -4.220e+02\n",
      " -4.230e+02 -4.240e+02 -4.260e+02 -4.260e+02 -4.270e+02  4.290e+02\n",
      "  4.340e+02  4.340e+02 -4.350e+02 -4.350e+02 -4.350e+02 -4.400e+02\n",
      "  4.420e+02 -4.420e+02  4.440e+02 -4.460e+02 -4.480e+02 -4.480e+02\n",
      " -4.570e+02  4.570e+02  4.600e+02 -4.620e+02  4.640e+02 -4.670e+02\n",
      "  4.680e+02  4.690e+02 -4.760e+02 -4.760e+02 -4.760e+02 -4.770e+02\n",
      "  4.780e+02 -4.810e+02 -4.840e+02 -4.860e+02 -4.870e+02  4.880e+02\n",
      " -4.920e+02 -5.050e+02 -5.130e+02 -5.150e+02 -5.150e+02 -5.200e+02\n",
      " -5.220e+02 -5.310e+02 -5.340e+02 -5.360e+02 -5.390e+02 -5.400e+02\n",
      " -5.410e+02 -5.450e+02 -5.460e+02 -5.470e+02 -5.510e+02 -5.520e+02\n",
      " -5.530e+02  5.570e+02  5.610e+02 -5.640e+02 -5.650e+02 -5.670e+02\n",
      " -5.680e+02  5.740e+02 -5.780e+02 -5.780e+02 -5.780e+02  5.820e+02\n",
      " -5.910e+02 -5.920e+02  5.930e+02  5.940e+02 -5.950e+02 -5.960e+02\n",
      "  5.980e+02 -5.990e+02 -6.000e+02 -6.010e+02 -6.030e+02 -6.030e+02\n",
      " -6.030e+02 -6.050e+02 -6.050e+02 -6.070e+02 -6.090e+02 -6.100e+02\n",
      " -6.100e+02 -6.100e+02 -6.140e+02  6.140e+02 -6.170e+02 -6.170e+02\n",
      " -6.240e+02  6.240e+02  6.280e+02 -6.290e+02 -6.300e+02 -6.340e+02\n",
      " -6.510e+02 -6.510e+02 -6.520e+02  6.530e+02 -6.570e+02 -6.580e+02\n",
      " -6.640e+02  6.660e+02 -6.700e+02 -6.700e+02  6.770e+02 -6.770e+02\n",
      " -6.830e+02 -6.890e+02 -6.900e+02 -6.910e+02  6.940e+02  6.970e+02\n",
      " -7.010e+02  7.020e+02 -7.040e+02 -7.050e+02 -7.050e+02  7.110e+02\n",
      " -7.180e+02  7.190e+02 -7.240e+02 -7.280e+02 -7.300e+02  7.310e+02\n",
      "  7.370e+02 -7.390e+02 -7.400e+02  7.600e+02 -7.610e+02 -7.610e+02\n",
      " -7.730e+02 -7.750e+02  7.770e+02 -7.910e+02 -7.910e+02  8.000e+02\n",
      " -8.000e+02 -8.050e+02 -8.060e+02  8.070e+02  8.080e+02 -8.130e+02\n",
      " -8.240e+02 -8.240e+02  8.260e+02 -8.270e+02 -8.290e+02 -8.300e+02\n",
      " -8.390e+02 -8.420e+02 -8.620e+02 -8.640e+02 -8.660e+02  8.690e+02\n",
      "  8.790e+02 -8.820e+02 -8.880e+02 -8.890e+02 -8.890e+02  8.960e+02\n",
      " -8.960e+02 -9.040e+02  9.050e+02 -9.100e+02 -9.120e+02  9.220e+02\n",
      "  9.290e+02 -9.300e+02 -9.470e+02 -9.490e+02  9.490e+02  9.500e+02\n",
      "  9.520e+02  9.760e+02 -9.770e+02  9.870e+02 -9.880e+02 -9.930e+02\n",
      "  9.950e+02  9.950e+02 -9.970e+02  9.990e+02 -1.040e+03  1.043e+03\n",
      "  1.046e+03 -1.071e+03 -1.072e+03  1.073e+03 -1.079e+03  1.081e+03\n",
      " -1.097e+03  1.115e+03 -1.119e+03 -1.125e+03 -1.126e+03 -1.130e+03\n",
      "  1.135e+03  1.147e+03 -1.148e+03 -1.157e+03 -1.159e+03 -1.163e+03\n",
      "  1.167e+03  1.171e+03 -1.175e+03 -1.189e+03 -1.189e+03  1.197e+03\n",
      "  1.209e+03  1.215e+03 -1.216e+03 -1.233e+03  1.235e+03 -1.239e+03\n",
      " -1.246e+03  1.258e+03  1.265e+03  1.268e+03 -1.280e+03 -1.285e+03\n",
      "  1.288e+03  1.293e+03 -1.301e+03 -1.324e+03 -1.333e+03 -1.351e+03\n",
      "  1.357e+03 -1.367e+03 -1.369e+03  1.379e+03 -1.400e+03  1.421e+03\n",
      " -1.429e+03 -1.431e+03 -1.432e+03 -1.442e+03  1.454e+03 -1.474e+03\n",
      " -1.479e+03  1.492e+03  1.498e+03  1.499e+03  1.516e+03  1.528e+03\n",
      "  1.531e+03 -1.559e+03  1.600e+03 -1.617e+03  1.622e+03  1.632e+03\n",
      " -1.683e+03 -1.700e+03  1.725e+03 -1.728e+03 -1.750e+03  1.778e+03\n",
      "  1.790e+03  1.830e+03 -1.864e+03 -1.870e+03 -1.932e+03 -1.965e+03\n",
      " -1.974e+03  2.027e+03 -2.067e+03 -2.109e+03 -2.224e+03 -2.261e+03\n",
      "  2.318e+03 -2.360e+03 -2.368e+03  2.393e+03 -2.449e+03 -2.590e+03\n",
      " -2.616e+03  2.617e+03  2.620e+03 -2.676e+03  2.681e+03 -2.696e+03\n",
      " -2.823e+03 -2.832e+03 -2.973e+03 -3.059e+03 -3.094e+03  3.169e+03\n",
      " -3.261e+03 -3.305e+03  3.361e+03 -3.635e+03 -3.674e+03 -3.759e+03\n",
      " -4.765e+03  4.961e+03 -4.992e+03 -6.732e+03 -7.062e+03 -7.248e+03]\n",
      "durations 28.0 3940.0\n",
      "Concordance Index 0.5310185185185186\n",
      "Integrated Brier Score: 0.2156040232233212\n",
      "(402, 20531) <class 'pandas.core.frame.DataFrame'>\n",
      "(402,) <class 'pandas.core.series.Series'>\n",
      "(100, 20531) <class 'pandas.core.frame.DataFrame'>\n",
      "(100,) <class 'pandas.core.series.Series'>\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m2.8517\u001b[0m  0.0749\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2        2.8517  0.0452\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.3228\u001b[0m  0.0517\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.2010\u001b[0m  0.0420\n",
      "      3        2.8517  0.0615\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m2.1453\u001b[0m  0.0544\n",
      "      2        3.3228  0.0571\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2        3.2010  0.0452\n",
      "      4        2.8517  0.0471\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m2.8780\u001b[0m  0.0442\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m2.4714\u001b[0m  0.0453\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.2055\u001b[0m  0.0454\n",
      "      2        2.1453  0.0354\n",
      "      3        3.3228  0.0377\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.3525\u001b[0m  0.0479\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m2.1272\u001b[0m  0.0415\n",
      "      2        2.8780  0.0341\n",
      "      5        2.8517  0.0361\n",
      "      2        3.2055  0.0417\n",
      "      2        2.4714  0.0428\n",
      "      3        3.2010  0.0605\n",
      "      3        2.1453  0.0382\n",
      "      2        2.1272  0.0311\n",
      "      6        2.8517  0.0310\n",
      "      2        3.3525  0.0385\n",
      "      3        2.8780  0.0385\n",
      "      4        3.3228  0.0499\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m2.4807\u001b[0m  0.0527\n",
      "      3        3.2055  0.0368\n",
      "      3        2.1272  0.0342\n",
      "      4        3.2010  0.0367\n",
      "      7        2.8517  0.0314\n",
      "      3        2.4714  0.0536\n",
      "      3        3.3525  0.0399\n",
      "      4        2.1453  0.0535\n",
      "      5        3.3228  0.0442\n",
      "      4        2.8780  0.0468\n",
      "      2        2.4807  0.0425\n",
      "      4        2.1272  0.0327\n",
      "      5        3.2010  0.0328\n",
      "      4        3.2055  0.0468\n",
      "      4        2.4714  0.0378\n",
      "      8        2.8517  0.0402\n",
      "      6        3.3228  0.0385\n",
      "      4        3.3525  0.0564\n",
      "      6        3.2010  0.0357\n",
      "      5        3.2055  0.0345\n",
      "      5        2.1272  0.0366\n",
      "      9        2.8517  0.0344\n",
      "      5        2.1453  0.0616\n",
      "      5        2.4714  0.0415\n",
      "      3        2.4807  0.0624\n",
      "      5        2.8780  0.0653\n",
      "      7        3.3228  0.0351\n",
      "      6        2.1272  0.0411\n",
      "      7        3.2010  0.0421\n",
      "      6        3.2055  0.0428\n",
      "      6        2.1453  0.0431\n",
      "     10        2.8517  0.0448\n",
      "      6        2.4714  0.0388\n",
      "Restoring best model from epoch 1.\n",
      "      4        2.4807  0.0365\n",
      "      5        3.3525  0.0633\n",
      "      6        2.8780  0.0404\n",
      "      8        3.3228  0.0358\n",
      "      8        3.2010  0.0285\n",
      "      7        2.1272  0.0369\n",
      "      7        3.2055  0.0369\n",
      "      5        2.4807  0.0349\n",
      "      7        2.1453  0.0418\n",
      "      6        3.3525  0.0361\n",
      "      9        3.2010  0.0288\n",
      "      9        3.3228  0.0352\n",
      "      7        2.8780  0.0399\n",
      "      7        2.4714  0.0522\n",
      "      8        3.2055  0.0357\n",
      "      8        2.1272  0.0381\n",
      "      6        2.4807  0.0339\n",
      "     10        3.2010  0.0316\n",
      "      8        2.1453  0.0354\n",
      "      8        2.8780  0.0299\n",
      "Restoring best model from epoch 1.\n",
      "      7        3.3525  0.0366\n",
      "     10        3.3228  0.0379\n",
      "      8        2.4714  0.0321\n",
      "Restoring best model from epoch 1.\n",
      "      9        3.2055  0.0304\n",
      "      9        2.1272  0.0338\n",
      "      9        2.1453  0.0291\n",
      "      7        2.4807  0.0391\n",
      "      8        3.3525  0.0353\n",
      "      9        2.4714  0.0351\n",
      "      9        2.8780  0.0447\n",
      "     10        2.1272  0.0323\n",
      "Restoring best model from epoch 1.\n",
      "     10        3.2055  0.0408\n",
      "Restoring best model from epoch 1.\n",
      "     10        2.1453  0.0323\n",
      "Restoring best model from epoch 1.\n",
      "      9        3.3525  0.0330\n",
      "      8        2.4807  0.0424\n",
      "     10        2.4714  0.0368\n",
      "Restoring best model from epoch 1.\n",
      "     10        2.8780  0.0475\n",
      "Restoring best model from epoch 1.\n",
      "     10        3.3525  0.0302\n",
      "Restoring best model from epoch 1.\n",
      "      9        2.4807  0.0452\n",
      "     10        2.4807  0.0396\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.0122\u001b[0m  0.0253\n",
      "      2        3.0122  0.0362\n",
      "      3        3.0122  0.0312\n",
      "      4        3.0122  0.0169\n",
      "      5        3.0122  0.0259\n",
      "      6        3.0122  0.0472\n",
      "      7        3.0122  0.0229\n",
      "      8        3.0122  0.0158\n",
      "      9        3.0122  0.0224\n",
      "     10        3.0122  0.0162\n",
      "Restoring best model from epoch 1.\n",
      "y_train breslow final [ 4.000e+00 -1.100e+01 -1.300e+01 -1.400e+01 -1.500e+01  1.800e+01\n",
      "  2.200e+01 -2.400e+01 -2.800e+01 -3.500e+01 -3.600e+01  3.800e+01\n",
      " -4.400e+01 -4.400e+01 -4.800e+01 -5.000e+01  5.800e+01 -6.000e+01\n",
      " -6.200e+01  6.200e+01  7.400e+01 -7.900e+01 -8.400e+01 -8.400e+01\n",
      "  8.700e+01  9.100e+01  9.900e+01  1.160e+02  1.180e+02  1.190e+02\n",
      " -1.190e+02  1.210e+02  1.240e+02 -1.290e+02 -1.310e+02 -1.330e+02\n",
      "  1.390e+02 -1.390e+02 -1.510e+02  1.540e+02  1.640e+02 -1.650e+02\n",
      "  1.670e+02  1.710e+02  1.730e+02 -1.740e+02  1.760e+02  1.790e+02\n",
      " -1.790e+02 -1.840e+02 -1.860e+02 -1.860e+02  1.870e+02  1.930e+02\n",
      " -2.020e+02 -2.180e+02 -2.240e+02 -2.250e+02 -2.300e+02  2.370e+02\n",
      "  2.430e+02  2.440e+02  2.440e+02  2.500e+02 -2.560e+02  2.570e+02\n",
      "  2.580e+02  2.600e+02 -2.600e+02  2.680e+02  2.740e+02  2.750e+02\n",
      "  2.810e+02  2.820e+02 -2.850e+02 -2.870e+02  2.910e+02  3.000e+02\n",
      "  3.030e+02  3.070e+02  3.080e+02 -3.100e+02  3.210e+02 -3.230e+02\n",
      "  3.360e+02  3.390e+02  3.400e+02 -3.530e+02 -3.540e+02 -3.650e+02\n",
      "  3.700e+02 -3.720e+02  3.760e+02 -3.770e+02 -3.850e+02 -4.080e+02\n",
      "  4.090e+02 -4.090e+02 -4.110e+02  4.140e+02 -4.140e+02 -4.150e+02\n",
      " -4.160e+02 -4.170e+02 -4.220e+02 -4.230e+02 -4.240e+02 -4.260e+02\n",
      " -4.260e+02 -4.270e+02  4.280e+02  4.290e+02  4.340e+02  4.340e+02\n",
      " -4.350e+02 -4.350e+02 -4.400e+02 -4.420e+02  4.440e+02  4.440e+02\n",
      " -4.460e+02 -4.480e+02 -4.480e+02 -4.550e+02 -4.570e+02  4.570e+02\n",
      "  4.600e+02 -4.620e+02  4.640e+02 -4.660e+02 -4.670e+02  4.680e+02\n",
      "  4.680e+02  4.690e+02 -4.760e+02 -4.770e+02 -4.780e+02  4.780e+02\n",
      " -4.840e+02 -4.860e+02 -4.870e+02  4.880e+02 -4.920e+02 -4.990e+02\n",
      "  5.000e+02  5.030e+02 -5.050e+02 -5.130e+02 -5.150e+02 -5.150e+02\n",
      " -5.220e+02 -5.260e+02 -5.310e+02 -5.340e+02 -5.370e+02 -5.390e+02\n",
      " -5.450e+02 -5.460e+02 -5.470e+02  5.500e+02 -5.510e+02 -5.520e+02\n",
      "  5.570e+02 -5.640e+02 -5.670e+02 -5.680e+02 -5.680e+02 -5.680e+02\n",
      " -5.730e+02 -5.780e+02 -5.780e+02 -5.780e+02  5.820e+02  5.860e+02\n",
      " -5.910e+02 -5.920e+02  5.930e+02  5.940e+02 -5.950e+02 -5.960e+02\n",
      "  5.980e+02 -5.990e+02 -6.010e+02 -6.030e+02 -6.030e+02 -6.050e+02\n",
      " -6.050e+02 -6.070e+02  6.070e+02 -6.080e+02 -6.100e+02 -6.100e+02\n",
      " -6.140e+02  6.140e+02 -6.170e+02 -6.240e+02  6.240e+02  6.250e+02\n",
      " -6.260e+02  6.260e+02 -6.260e+02  6.280e+02 -6.300e+02 -6.340e+02\n",
      " -6.510e+02 -6.510e+02 -6.520e+02 -6.520e+02 -6.530e+02  6.530e+02\n",
      "  6.560e+02 -6.570e+02 -6.580e+02 -6.580e+02 -6.640e+02  6.660e+02\n",
      " -6.690e+02 -6.700e+02 -6.740e+02  6.770e+02 -6.770e+02 -6.830e+02\n",
      " -6.890e+02 -6.910e+02  6.940e+02  6.970e+02  7.010e+02  7.020e+02\n",
      " -7.040e+02 -7.050e+02 -7.050e+02  7.110e+02 -7.180e+02 -7.190e+02\n",
      " -7.280e+02 -7.300e+02  7.370e+02 -7.390e+02 -7.400e+02 -7.410e+02\n",
      " -7.470e+02  7.600e+02 -7.610e+02  7.610e+02 -7.610e+02 -7.730e+02\n",
      " -7.910e+02 -7.910e+02 -7.910e+02 -7.910e+02  8.000e+02 -8.000e+02\n",
      " -8.050e+02 -8.060e+02  8.070e+02  8.080e+02 -8.130e+02 -8.240e+02\n",
      " -8.240e+02  8.260e+02 -8.270e+02 -8.290e+02 -8.300e+02 -8.420e+02\n",
      " -8.450e+02 -8.520e+02  8.550e+02 -8.620e+02 -8.630e+02 -8.640e+02\n",
      "  8.640e+02 -8.660e+02  8.690e+02 -8.720e+02 -8.820e+02 -8.880e+02\n",
      " -8.890e+02 -8.890e+02 -8.960e+02  8.960e+02  9.050e+02 -9.100e+02\n",
      " -9.120e+02 -9.130e+02  9.220e+02  9.290e+02 -9.300e+02 -9.380e+02\n",
      " -9.440e+02 -9.470e+02 -9.490e+02  9.500e+02  9.760e+02  9.870e+02\n",
      " -9.880e+02 -9.930e+02  9.950e+02  9.950e+02 -9.970e+02  9.990e+02\n",
      " -1.013e+03  1.026e+03 -1.036e+03  1.043e+03  1.046e+03 -1.060e+03\n",
      " -1.071e+03 -1.072e+03 -1.079e+03 -1.097e+03  1.115e+03 -1.118e+03\n",
      " -1.125e+03 -1.126e+03 -1.126e+03 -1.130e+03  1.135e+03  1.147e+03\n",
      " -1.148e+03  1.167e+03 -1.175e+03 -1.178e+03 -1.189e+03  1.194e+03\n",
      "  1.197e+03  1.209e+03  1.229e+03 -1.239e+03  1.265e+03  1.268e+03\n",
      " -1.272e+03 -1.285e+03  1.288e+03 -1.289e+03  1.293e+03 -1.301e+03\n",
      " -1.305e+03 -1.333e+03 -1.351e+03  1.379e+03  1.421e+03 -1.429e+03\n",
      " -1.431e+03 -1.442e+03  1.454e+03 -1.474e+03 -1.479e+03  1.492e+03\n",
      "  1.498e+03  1.501e+03  1.516e+03 -1.523e+03  1.600e+03 -1.617e+03\n",
      " -1.621e+03  1.622e+03  1.653e+03 -1.683e+03 -1.750e+03  1.778e+03\n",
      "  1.790e+03  1.798e+03  1.830e+03 -1.847e+03 -1.864e+03 -1.870e+03\n",
      " -1.893e+03 -1.965e+03  2.027e+03 -2.065e+03 -2.067e+03 -2.109e+03\n",
      " -2.137e+03 -2.161e+03  2.174e+03 -2.199e+03 -2.248e+03 -2.360e+03\n",
      "  2.393e+03 -2.449e+03 -2.488e+03 -2.515e+03 -2.590e+03 -2.595e+03\n",
      " -2.616e+03  2.617e+03  2.620e+03 -2.676e+03  2.681e+03 -2.823e+03\n",
      " -2.832e+03 -3.059e+03  3.169e+03 -3.635e+03 -3.674e+03 -3.759e+03\n",
      " -3.940e+03 -4.765e+03  4.961e+03 -4.992e+03 -7.062e+03 -7.248e+03]\n",
      "Concordance Index 0.48753240933388814\n",
      "Integrated Brier Score: 0.1827280634030793\n",
      "y_train breslow final [ 4.000e+00 -1.100e+01 -1.300e+01 -1.400e+01 -1.500e+01  1.800e+01\n",
      "  2.200e+01 -2.400e+01 -2.800e+01 -3.500e+01 -3.600e+01  3.800e+01\n",
      " -4.400e+01 -4.400e+01 -4.800e+01 -5.000e+01  5.800e+01 -6.000e+01\n",
      " -6.200e+01  6.200e+01  7.400e+01 -7.900e+01 -8.400e+01 -8.400e+01\n",
      "  8.700e+01  9.100e+01  9.900e+01  1.160e+02  1.180e+02  1.190e+02\n",
      " -1.190e+02  1.210e+02  1.240e+02 -1.290e+02 -1.310e+02 -1.330e+02\n",
      "  1.390e+02 -1.390e+02 -1.510e+02  1.540e+02  1.640e+02 -1.650e+02\n",
      "  1.670e+02  1.710e+02  1.730e+02 -1.740e+02  1.760e+02  1.790e+02\n",
      " -1.790e+02 -1.840e+02 -1.860e+02 -1.860e+02  1.870e+02  1.930e+02\n",
      " -2.020e+02 -2.180e+02 -2.240e+02 -2.250e+02 -2.300e+02  2.370e+02\n",
      "  2.430e+02  2.440e+02  2.440e+02  2.500e+02 -2.560e+02  2.570e+02\n",
      "  2.580e+02  2.600e+02 -2.600e+02  2.680e+02  2.740e+02  2.750e+02\n",
      "  2.810e+02  2.820e+02 -2.850e+02 -2.870e+02  2.910e+02  3.000e+02\n",
      "  3.030e+02  3.070e+02  3.080e+02 -3.100e+02  3.210e+02 -3.230e+02\n",
      "  3.360e+02  3.390e+02  3.400e+02 -3.530e+02 -3.540e+02 -3.650e+02\n",
      "  3.700e+02 -3.720e+02  3.760e+02 -3.770e+02 -3.850e+02 -4.080e+02\n",
      "  4.090e+02 -4.090e+02 -4.110e+02  4.140e+02 -4.140e+02 -4.150e+02\n",
      " -4.160e+02 -4.170e+02 -4.220e+02 -4.230e+02 -4.240e+02 -4.260e+02\n",
      " -4.260e+02 -4.270e+02  4.280e+02  4.290e+02  4.340e+02  4.340e+02\n",
      " -4.350e+02 -4.350e+02 -4.400e+02 -4.420e+02  4.440e+02  4.440e+02\n",
      " -4.460e+02 -4.480e+02 -4.480e+02 -4.550e+02 -4.570e+02  4.570e+02\n",
      "  4.600e+02 -4.620e+02  4.640e+02 -4.660e+02 -4.670e+02  4.680e+02\n",
      "  4.680e+02  4.690e+02 -4.760e+02 -4.770e+02 -4.780e+02  4.780e+02\n",
      " -4.840e+02 -4.860e+02 -4.870e+02  4.880e+02 -4.920e+02 -4.990e+02\n",
      "  5.000e+02  5.030e+02 -5.050e+02 -5.130e+02 -5.150e+02 -5.150e+02\n",
      " -5.220e+02 -5.260e+02 -5.310e+02 -5.340e+02 -5.370e+02 -5.390e+02\n",
      " -5.450e+02 -5.460e+02 -5.470e+02  5.500e+02 -5.510e+02 -5.520e+02\n",
      "  5.570e+02 -5.640e+02 -5.670e+02 -5.680e+02 -5.680e+02 -5.680e+02\n",
      " -5.730e+02 -5.780e+02 -5.780e+02 -5.780e+02  5.820e+02  5.860e+02\n",
      " -5.910e+02 -5.920e+02  5.930e+02  5.940e+02 -5.950e+02 -5.960e+02\n",
      "  5.980e+02 -5.990e+02 -6.010e+02 -6.030e+02 -6.030e+02 -6.050e+02\n",
      " -6.050e+02 -6.070e+02  6.070e+02 -6.080e+02 -6.100e+02 -6.100e+02\n",
      " -6.140e+02  6.140e+02 -6.170e+02 -6.240e+02  6.240e+02  6.250e+02\n",
      " -6.260e+02  6.260e+02 -6.260e+02  6.280e+02 -6.300e+02 -6.340e+02\n",
      " -6.510e+02 -6.510e+02 -6.520e+02 -6.520e+02 -6.530e+02  6.530e+02\n",
      "  6.560e+02 -6.570e+02 -6.580e+02 -6.580e+02 -6.640e+02  6.660e+02\n",
      " -6.690e+02 -6.700e+02 -6.740e+02  6.770e+02 -6.770e+02 -6.830e+02\n",
      " -6.890e+02 -6.910e+02  6.940e+02  6.970e+02  7.010e+02  7.020e+02\n",
      " -7.040e+02 -7.050e+02 -7.050e+02  7.110e+02 -7.180e+02 -7.190e+02\n",
      " -7.280e+02 -7.300e+02  7.370e+02 -7.390e+02 -7.400e+02 -7.410e+02\n",
      " -7.470e+02  7.600e+02 -7.610e+02  7.610e+02 -7.610e+02 -7.730e+02\n",
      " -7.910e+02 -7.910e+02 -7.910e+02 -7.910e+02  8.000e+02 -8.000e+02\n",
      " -8.050e+02 -8.060e+02  8.070e+02  8.080e+02 -8.130e+02 -8.240e+02\n",
      " -8.240e+02  8.260e+02 -8.270e+02 -8.290e+02 -8.300e+02 -8.420e+02\n",
      " -8.450e+02 -8.520e+02  8.550e+02 -8.620e+02 -8.630e+02 -8.640e+02\n",
      "  8.640e+02 -8.660e+02  8.690e+02 -8.720e+02 -8.820e+02 -8.880e+02\n",
      " -8.890e+02 -8.890e+02 -8.960e+02  8.960e+02  9.050e+02 -9.100e+02\n",
      " -9.120e+02 -9.130e+02  9.220e+02  9.290e+02 -9.300e+02 -9.380e+02\n",
      " -9.440e+02 -9.470e+02 -9.490e+02  9.500e+02  9.760e+02  9.870e+02\n",
      " -9.880e+02 -9.930e+02  9.950e+02  9.950e+02 -9.970e+02  9.990e+02\n",
      " -1.013e+03  1.026e+03 -1.036e+03  1.043e+03  1.046e+03 -1.060e+03\n",
      " -1.071e+03 -1.072e+03 -1.079e+03 -1.097e+03  1.115e+03 -1.118e+03\n",
      " -1.125e+03 -1.126e+03 -1.126e+03 -1.130e+03  1.135e+03  1.147e+03\n",
      " -1.148e+03  1.167e+03 -1.175e+03 -1.178e+03 -1.189e+03  1.194e+03\n",
      "  1.197e+03  1.209e+03  1.229e+03 -1.239e+03  1.265e+03  1.268e+03\n",
      " -1.272e+03 -1.285e+03  1.288e+03 -1.289e+03  1.293e+03 -1.301e+03\n",
      " -1.305e+03 -1.333e+03 -1.351e+03  1.379e+03  1.421e+03 -1.429e+03\n",
      " -1.431e+03 -1.442e+03  1.454e+03 -1.474e+03 -1.479e+03  1.492e+03\n",
      "  1.498e+03  1.501e+03  1.516e+03 -1.523e+03  1.600e+03 -1.617e+03\n",
      " -1.621e+03  1.622e+03  1.653e+03 -1.683e+03 -1.750e+03  1.778e+03\n",
      "  1.790e+03  1.798e+03  1.830e+03 -1.847e+03 -1.864e+03 -1.870e+03\n",
      " -1.893e+03 -1.965e+03  2.027e+03 -2.065e+03 -2.067e+03 -2.109e+03\n",
      " -2.137e+03 -2.161e+03  2.174e+03 -2.199e+03 -2.248e+03 -2.360e+03\n",
      "  2.393e+03 -2.449e+03 -2.488e+03 -2.515e+03 -2.590e+03 -2.595e+03\n",
      " -2.616e+03  2.617e+03  2.620e+03 -2.676e+03  2.681e+03 -2.823e+03\n",
      " -2.832e+03 -3.059e+03  3.169e+03 -3.635e+03 -3.674e+03 -3.759e+03\n",
      " -3.940e+03 -4.765e+03  4.961e+03 -4.992e+03 -7.062e+03 -7.248e+03]\n",
      "durations 19.0 6732.0\n",
      "Concordance Index 0.4332993890020367\n",
      "Integrated Brier Score: 0.1902881231346626\n",
      "split gex_?|100130426      float32\n",
      "gex_?|100133144      float32\n",
      "gex_?|100134869      float32\n",
      "gex_?|10357          float32\n",
      "gex_?|10431          float32\n",
      "                      ...   \n",
      "gex_ZYG11A|440590    float32\n",
      "gex_ZYG11B|79699     float32\n",
      "gex_ZYX|7791         float32\n",
      "gex_ZZEF1|23140      float32\n",
      "gex_ZZZ3|26009       float32\n",
      "Length: 20531, dtype: object\n",
      "(394, 20531) <class 'pandas.core.frame.DataFrame'>\n",
      "(394,) <class 'pandas.core.series.Series'>\n",
      "(99, 20531) <class 'pandas.core.frame.DataFrame'>\n",
      "(99,) <class 'pandas.core.series.Series'>\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.7677\u001b[0m  0.0344\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.9420\u001b[0m  0.0394\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.5920\u001b[0m  0.0337\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.6414\u001b[0m  0.0379\n",
      "      2        3.7677  0.0420\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.6333\u001b[0m  0.0444\n",
      "      2        3.9420  0.0377\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.3590\u001b[0m  0.0335\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2        3.5920  0.0389\n",
      "      2        3.6414  0.0370\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.7705\u001b[0m  0.0427\n",
      "      3        3.7677  0.0390\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.9297\u001b[0m  0.0328\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.3506\u001b[0m  0.0487\n",
      "      3        3.9420  0.0363\n",
      "      2        3.6333  0.0494\n",
      "      2        3.3590  0.0469\n",
      "      3        3.5920  0.0380\n",
      "      3        3.6414  0.0349\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.5879\u001b[0m  0.0325\n",
      "      4        3.7677  0.0388\n",
      "      2        3.9297  0.0383\n",
      "      2        3.3506  0.0415\n",
      "      2        3.7705  0.0538\n",
      "      4        3.9420  0.0413\n",
      "      3        3.6333  0.0326\n",
      "      2        3.5879  0.0306\n",
      "      3        3.3590  0.0383\n",
      "      4        3.6414  0.0436\n",
      "      4        3.5920  0.0514\n",
      "      3        3.3506  0.0432\n",
      "      5        3.7677  0.0497\n",
      "      3        3.9297  0.0527\n",
      "      4        3.6333  0.0435\n",
      "      3        3.7705  0.0479\n",
      "      4        3.3590  0.0438\n",
      "      3        3.5879  0.0469\n",
      "      5        3.9420  0.0532\n",
      "      5        3.6414  0.0396\n",
      "      5        3.5920  0.0383\n",
      "      6        3.7677  0.0372\n",
      "      4        3.9297  0.0339\n",
      "      4        3.3506  0.0448\n",
      "      4        3.5879  0.0343\n",
      "      6        3.5920  0.0290\n",
      "      6        3.6414  0.0364\n",
      "      6        3.9420  0.0453\n",
      "      5        3.6333  0.0572\n",
      "      4        3.7705  0.0581\n",
      "      7        3.7677  0.0325\n",
      "      5        3.5879  0.0288\n",
      "      7        3.5920  0.0283\n",
      "      5        3.9297  0.0429\n",
      "      5        3.3506  0.0391\n",
      "      5        3.3590  0.0774\n",
      "      7        3.6414  0.0353\n",
      "      7        3.9420  0.0379\n",
      "      8        3.7677  0.0377\n",
      "      8        3.5920  0.0283\n",
      "      6        3.5879  0.0353\n",
      "      6        3.9297  0.0371\n",
      "      6        3.3590  0.0323\n",
      "      6        3.3506  0.0469\n",
      "      8        3.6414  0.0402\n",
      "      9        3.5920  0.0277\n",
      "      7        3.5879  0.0271\n",
      "      9        3.7677  0.0393\n",
      "      7        3.9297  0.0343\n",
      "      7        3.3590  0.0344\n",
      "      7        3.3506  0.0289\n",
      "      9        3.6414  0.0287\n",
      "     10        3.5920  0.0305\n",
      "Restoring best model from epoch 1.\n",
      "      6        3.6333  0.1115\n",
      "     10        3.7677  0.0330\n",
      "Restoring best model from epoch 1.\n",
      "      8        3.5879  0.0383\n",
      "      8        3.3506  0.0354\n",
      "      5        3.7705  0.1294\n",
      "      8        3.9297  0.0475\n",
      "      8        3.3590  0.0457\n",
      "     10        3.6414  0.0449\n",
      "Restoring best model from epoch 1.\n",
      "      9        3.5879  0.0370\n",
      "      9        3.3506  0.0321\n",
      "      8        3.9420  0.1266\n",
      "      9        3.9297  0.0365\n",
      "      7        3.6333  0.0690\n",
      "      9        3.3590  0.0431\n",
      "     10        3.5879  0.0344\n",
      "      6        3.7705  0.0561\n",
      "Restoring best model from epoch 1.\n",
      "     10        3.3506  0.0336\n",
      "Restoring best model from epoch 1.\n",
      "     10        3.9297  0.0392\n",
      "      9        3.9420  0.0503\n",
      "Restoring best model from epoch 1.\n",
      "      7        3.7705  0.0366\n",
      "     10        3.3590  0.0435\n",
      "Restoring best model from epoch 1.\n",
      "      8        3.6333  0.0572\n",
      "     10        3.9420  0.0301\n",
      "Restoring best model from epoch 1.\n",
      "      8        3.7705  0.0367\n",
      "      9        3.6333  0.0352\n",
      "      9        3.7705  0.0394\n",
      "     10        3.6333  0.0390\n",
      "Restoring best model from epoch 1.\n",
      "     10        3.7705  0.0395\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m4.0198\u001b[0m  0.0151\n",
      "      2        4.0198  0.0155\n",
      "      3        4.0198  0.0157\n",
      "      4        4.0198  0.0146\n",
      "      5        4.0198  0.0139\n",
      "      6        4.0198  0.0323\n",
      "      7        4.0198  0.0162\n",
      "      8        4.0198  0.0192\n",
      "      9        4.0198  0.0162\n",
      "     10        4.0198  0.0167\n",
      "Restoring best model from epoch 1.\n",
      "y_train breslow final [ 2.000e+00  3.000e+00 -4.000e+00  5.000e+00  5.000e+00  6.000e+00\n",
      " -8.000e+00  9.000e+00 -1.200e+01  1.200e+01 -1.200e+01 -1.300e+01\n",
      " -1.300e+01 -1.500e+01  1.700e+01  2.300e+01  2.400e+01 -2.800e+01\n",
      " -3.000e+01 -3.000e+01 -3.000e+01 -3.100e+01 -3.100e+01  3.400e+01\n",
      " -3.500e+01 -4.200e+01  4.700e+01  5.300e+01  5.900e+01 -6.000e+01\n",
      " -6.100e+01 -6.100e+01 -6.100e+01 -6.100e+01 -6.100e+01 -6.100e+01\n",
      " -6.100e+01 -7.000e+01  8.000e+01 -8.300e+01  8.400e+01  8.500e+01\n",
      "  8.800e+01 -8.900e+01  8.900e+01  9.200e+01  9.400e+01 -9.700e+01\n",
      " -1.030e+02 -1.050e+02 -1.060e+02  1.090e+02 -1.110e+02 -1.110e+02\n",
      "  1.160e+02 -1.210e+02 -1.220e+02 -1.220e+02 -1.220e+02 -1.220e+02\n",
      "  1.230e+02 -1.280e+02  1.310e+02  1.360e+02  1.380e+02  1.430e+02\n",
      "  1.460e+02 -1.500e+02  1.510e+02 -1.530e+02  1.530e+02 -1.600e+02\n",
      "  1.610e+02  1.660e+02  1.660e+02 -1.730e+02  1.800e+02  1.880e+02\n",
      "  1.950e+02  1.980e+02  1.980e+02 -2.020e+02 -2.070e+02 -2.110e+02\n",
      "  2.110e+02  2.120e+02  2.150e+02  2.230e+02 -2.270e+02 -2.360e+02\n",
      " -2.380e+02 -2.420e+02 -2.440e+02  2.660e+02  2.740e+02 -2.890e+02\n",
      "  2.910e+02  2.940e+02  2.990e+02  3.020e+02  3.070e+02  3.110e+02\n",
      "  3.150e+02  3.220e+02 -3.240e+02  3.290e+02  3.400e+02  3.400e+02\n",
      "  3.450e+02 -3.530e+02 -3.580e+02 -3.580e+02  3.600e+02  3.650e+02\n",
      "  3.660e+02 -3.660e+02 -3.670e+02 -3.700e+02  3.710e+02  3.720e+02\n",
      " -3.770e+02 -3.780e+02  3.830e+02  3.830e+02  3.970e+02  3.990e+02\n",
      " -4.010e+02  4.020e+02 -4.050e+02 -4.060e+02 -4.070e+02  4.080e+02\n",
      "  4.120e+02  4.180e+02 -4.200e+02 -4.230e+02 -4.280e+02  4.290e+02\n",
      " -4.300e+02 -4.320e+02 -4.400e+02  4.420e+02  4.450e+02 -4.480e+02\n",
      "  4.480e+02  4.540e+02  4.550e+02  4.560e+02 -4.810e+02 -4.820e+02\n",
      "  4.900e+02 -4.920e+02 -4.980e+02  5.010e+02 -5.100e+02 -5.100e+02\n",
      " -5.110e+02  5.150e+02  5.160e+02 -5.170e+02  5.390e+02  5.430e+02\n",
      "  5.520e+02 -5.550e+02 -5.570e+02  5.590e+02 -5.590e+02  5.620e+02\n",
      " -5.650e+02 -5.700e+02 -5.760e+02 -5.780e+02 -5.780e+02 -5.790e+02\n",
      " -5.810e+02 -5.850e+02 -5.890e+02 -5.960e+02 -6.000e+02 -6.010e+02\n",
      "  6.010e+02  6.040e+02 -6.080e+02 -6.150e+02 -6.180e+02  6.230e+02\n",
      " -6.300e+02  6.320e+02 -6.370e+02 -6.400e+02 -6.410e+02 -6.420e+02\n",
      "  6.450e+02 -6.600e+02 -6.660e+02  6.690e+02  6.780e+02  6.840e+02\n",
      "  6.870e+02 -6.880e+02 -6.990e+02 -7.000e+02  7.080e+02  7.160e+02\n",
      " -7.170e+02  7.340e+02 -7.340e+02  7.400e+02 -7.410e+02 -7.570e+02\n",
      " -7.580e+02 -7.620e+02 -7.650e+02 -7.690e+02  7.700e+02 -7.900e+02\n",
      "  8.030e+02 -8.040e+02 -8.040e+02 -8.160e+02 -8.180e+02 -8.220e+02\n",
      "  8.220e+02  8.260e+02 -8.260e+02 -8.330e+02  8.350e+02  8.400e+02\n",
      " -8.420e+02 -8.490e+02 -8.670e+02  8.810e+02 -9.080e+02 -9.100e+02\n",
      " -9.110e+02 -9.110e+02  9.160e+02  9.210e+02  9.270e+02  9.330e+02\n",
      "  9.370e+02 -9.370e+02 -9.420e+02  9.510e+02 -9.520e+02 -9.600e+02\n",
      "  9.650e+02  9.730e+02  9.740e+02 -9.830e+02 -9.950e+02  1.000e+03\n",
      "  1.001e+03  1.006e+03 -1.007e+03 -1.032e+03  1.045e+03 -1.050e+03\n",
      "  1.057e+03 -1.058e+03 -1.072e+03  1.075e+03 -1.092e+03  1.097e+03\n",
      " -1.100e+03 -1.106e+03  1.107e+03 -1.111e+03 -1.113e+03  1.114e+03\n",
      "  1.143e+03  1.150e+03 -1.160e+03 -1.176e+03 -1.180e+03 -1.182e+03\n",
      "  1.189e+03  1.190e+03 -1.196e+03 -1.212e+03 -1.217e+03 -1.224e+03\n",
      " -1.259e+03 -1.260e+03 -1.268e+03 -1.280e+03 -1.297e+03 -1.311e+03\n",
      "  1.315e+03 -1.336e+03  1.338e+03  1.344e+03  1.346e+03 -1.361e+03\n",
      " -1.386e+03  1.423e+03  1.426e+03  1.470e+03 -1.475e+03  1.485e+03\n",
      " -1.492e+03 -1.499e+03 -1.505e+03 -1.519e+03 -1.531e+03 -1.541e+03\n",
      " -1.602e+03 -1.639e+03  1.640e+03 -1.645e+03  1.655e+03  1.656e+03\n",
      "  1.679e+03 -1.682e+03 -1.690e+03  1.695e+03  1.713e+03 -1.723e+03\n",
      " -1.731e+03  1.736e+03 -1.743e+03 -1.784e+03 -1.824e+03 -1.834e+03\n",
      "  1.841e+03 -1.845e+03  1.856e+03 -1.884e+03  1.912e+03 -1.927e+03\n",
      "  1.933e+03  1.953e+03 -1.963e+03  1.975e+03  1.984e+03 -1.992e+03\n",
      " -1.997e+03 -2.023e+03 -2.024e+03 -2.073e+03 -2.133e+03  2.133e+03\n",
      " -2.134e+03 -2.148e+03  2.160e+03 -2.167e+03  2.224e+03 -2.227e+03\n",
      " -2.271e+03  2.284e+03  2.304e+03 -2.336e+03  2.378e+03 -2.381e+03\n",
      "  2.409e+03 -2.419e+03 -2.447e+03 -2.471e+03 -2.542e+03 -2.589e+03\n",
      "  2.625e+03  2.639e+03 -2.645e+03  2.680e+03  2.803e+03 -2.811e+03\n",
      " -2.820e+03  2.945e+03  2.979e+03 -3.016e+03 -3.108e+03 -3.123e+03\n",
      " -3.166e+03 -3.189e+03  3.253e+03  3.376e+03 -3.387e+03 -3.576e+03\n",
      "  3.600e+03 -3.636e+03 -3.644e+03 -3.724e+03 -3.747e+03 -3.850e+03\n",
      "  3.924e+03 -4.026e+03 -4.053e+03 -4.068e+03 -4.261e+03 -4.570e+03\n",
      "  4.601e+03 -4.694e+03 -4.765e+03  5.287e+03]\n",
      "Concordance Index 0.5064660637308765\n",
      "Integrated Brier Score: 0.19456900948843972\n",
      "y_train breslow final [ 2.000e+00  3.000e+00 -4.000e+00  5.000e+00  5.000e+00  6.000e+00\n",
      " -8.000e+00  9.000e+00 -1.200e+01  1.200e+01 -1.200e+01 -1.300e+01\n",
      " -1.300e+01 -1.500e+01  1.700e+01  2.300e+01  2.400e+01 -2.800e+01\n",
      " -3.000e+01 -3.000e+01 -3.000e+01 -3.100e+01 -3.100e+01  3.400e+01\n",
      " -3.500e+01 -4.200e+01  4.700e+01  5.300e+01  5.900e+01 -6.000e+01\n",
      " -6.100e+01 -6.100e+01 -6.100e+01 -6.100e+01 -6.100e+01 -6.100e+01\n",
      " -6.100e+01 -7.000e+01  8.000e+01 -8.300e+01  8.400e+01  8.500e+01\n",
      "  8.800e+01 -8.900e+01  8.900e+01  9.200e+01  9.400e+01 -9.700e+01\n",
      " -1.030e+02 -1.050e+02 -1.060e+02  1.090e+02 -1.110e+02 -1.110e+02\n",
      "  1.160e+02 -1.210e+02 -1.220e+02 -1.220e+02 -1.220e+02 -1.220e+02\n",
      "  1.230e+02 -1.280e+02  1.310e+02  1.360e+02  1.380e+02  1.430e+02\n",
      "  1.460e+02 -1.500e+02  1.510e+02 -1.530e+02  1.530e+02 -1.600e+02\n",
      "  1.610e+02  1.660e+02  1.660e+02 -1.730e+02  1.800e+02  1.880e+02\n",
      "  1.950e+02  1.980e+02  1.980e+02 -2.020e+02 -2.070e+02 -2.110e+02\n",
      "  2.110e+02  2.120e+02  2.150e+02  2.230e+02 -2.270e+02 -2.360e+02\n",
      " -2.380e+02 -2.420e+02 -2.440e+02  2.660e+02  2.740e+02 -2.890e+02\n",
      "  2.910e+02  2.940e+02  2.990e+02  3.020e+02  3.070e+02  3.110e+02\n",
      "  3.150e+02  3.220e+02 -3.240e+02  3.290e+02  3.400e+02  3.400e+02\n",
      "  3.450e+02 -3.530e+02 -3.580e+02 -3.580e+02  3.600e+02  3.650e+02\n",
      "  3.660e+02 -3.660e+02 -3.670e+02 -3.700e+02  3.710e+02  3.720e+02\n",
      " -3.770e+02 -3.780e+02  3.830e+02  3.830e+02  3.970e+02  3.990e+02\n",
      " -4.010e+02  4.020e+02 -4.050e+02 -4.060e+02 -4.070e+02  4.080e+02\n",
      "  4.120e+02  4.180e+02 -4.200e+02 -4.230e+02 -4.280e+02  4.290e+02\n",
      " -4.300e+02 -4.320e+02 -4.400e+02  4.420e+02  4.450e+02 -4.480e+02\n",
      "  4.480e+02  4.540e+02  4.550e+02  4.560e+02 -4.810e+02 -4.820e+02\n",
      "  4.900e+02 -4.920e+02 -4.980e+02  5.010e+02 -5.100e+02 -5.100e+02\n",
      " -5.110e+02  5.150e+02  5.160e+02 -5.170e+02  5.390e+02  5.430e+02\n",
      "  5.520e+02 -5.550e+02 -5.570e+02  5.590e+02 -5.590e+02  5.620e+02\n",
      " -5.650e+02 -5.700e+02 -5.760e+02 -5.780e+02 -5.780e+02 -5.790e+02\n",
      " -5.810e+02 -5.850e+02 -5.890e+02 -5.960e+02 -6.000e+02 -6.010e+02\n",
      "  6.010e+02  6.040e+02 -6.080e+02 -6.150e+02 -6.180e+02  6.230e+02\n",
      " -6.300e+02  6.320e+02 -6.370e+02 -6.400e+02 -6.410e+02 -6.420e+02\n",
      "  6.450e+02 -6.600e+02 -6.660e+02  6.690e+02  6.780e+02  6.840e+02\n",
      "  6.870e+02 -6.880e+02 -6.990e+02 -7.000e+02  7.080e+02  7.160e+02\n",
      " -7.170e+02  7.340e+02 -7.340e+02  7.400e+02 -7.410e+02 -7.570e+02\n",
      " -7.580e+02 -7.620e+02 -7.650e+02 -7.690e+02  7.700e+02 -7.900e+02\n",
      "  8.030e+02 -8.040e+02 -8.040e+02 -8.160e+02 -8.180e+02 -8.220e+02\n",
      "  8.220e+02  8.260e+02 -8.260e+02 -8.330e+02  8.350e+02  8.400e+02\n",
      " -8.420e+02 -8.490e+02 -8.670e+02  8.810e+02 -9.080e+02 -9.100e+02\n",
      " -9.110e+02 -9.110e+02  9.160e+02  9.210e+02  9.270e+02  9.330e+02\n",
      "  9.370e+02 -9.370e+02 -9.420e+02  9.510e+02 -9.520e+02 -9.600e+02\n",
      "  9.650e+02  9.730e+02  9.740e+02 -9.830e+02 -9.950e+02  1.000e+03\n",
      "  1.001e+03  1.006e+03 -1.007e+03 -1.032e+03  1.045e+03 -1.050e+03\n",
      "  1.057e+03 -1.058e+03 -1.072e+03  1.075e+03 -1.092e+03  1.097e+03\n",
      " -1.100e+03 -1.106e+03  1.107e+03 -1.111e+03 -1.113e+03  1.114e+03\n",
      "  1.143e+03  1.150e+03 -1.160e+03 -1.176e+03 -1.180e+03 -1.182e+03\n",
      "  1.189e+03  1.190e+03 -1.196e+03 -1.212e+03 -1.217e+03 -1.224e+03\n",
      " -1.259e+03 -1.260e+03 -1.268e+03 -1.280e+03 -1.297e+03 -1.311e+03\n",
      "  1.315e+03 -1.336e+03  1.338e+03  1.344e+03  1.346e+03 -1.361e+03\n",
      " -1.386e+03  1.423e+03  1.426e+03  1.470e+03 -1.475e+03  1.485e+03\n",
      " -1.492e+03 -1.499e+03 -1.505e+03 -1.519e+03 -1.531e+03 -1.541e+03\n",
      " -1.602e+03 -1.639e+03  1.640e+03 -1.645e+03  1.655e+03  1.656e+03\n",
      "  1.679e+03 -1.682e+03 -1.690e+03  1.695e+03  1.713e+03 -1.723e+03\n",
      " -1.731e+03  1.736e+03 -1.743e+03 -1.784e+03 -1.824e+03 -1.834e+03\n",
      "  1.841e+03 -1.845e+03  1.856e+03 -1.884e+03  1.912e+03 -1.927e+03\n",
      "  1.933e+03  1.953e+03 -1.963e+03  1.975e+03  1.984e+03 -1.992e+03\n",
      " -1.997e+03 -2.023e+03 -2.024e+03 -2.073e+03 -2.133e+03  2.133e+03\n",
      " -2.134e+03 -2.148e+03  2.160e+03 -2.167e+03  2.224e+03 -2.227e+03\n",
      " -2.271e+03  2.284e+03  2.304e+03 -2.336e+03  2.378e+03 -2.381e+03\n",
      "  2.409e+03 -2.419e+03 -2.447e+03 -2.471e+03 -2.542e+03 -2.589e+03\n",
      "  2.625e+03  2.639e+03 -2.645e+03  2.680e+03  2.803e+03 -2.811e+03\n",
      " -2.820e+03  2.945e+03  2.979e+03 -3.016e+03 -3.108e+03 -3.123e+03\n",
      " -3.166e+03 -3.189e+03  3.253e+03  3.376e+03 -3.387e+03 -3.576e+03\n",
      "  3.600e+03 -3.636e+03 -3.644e+03 -3.724e+03 -3.747e+03 -3.850e+03\n",
      "  3.924e+03 -4.026e+03 -4.053e+03 -4.068e+03 -4.261e+03 -4.570e+03\n",
      "  4.601e+03 -4.694e+03 -4.765e+03  5.287e+03]\n",
      "durations 1.0 3838.0\n",
      "Concordance Index 0.4788371098760154\n",
      "Integrated Brier Score: 0.20206647003085834\n",
      "(394, 20531) <class 'pandas.core.frame.DataFrame'>\n",
      "(394,) <class 'pandas.core.series.Series'>\n",
      "(99, 20531) <class 'pandas.core.frame.DataFrame'>\n",
      "(99,) <class 'pandas.core.series.Series'>\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.0701\u001b[0m  0.0433\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.1274\u001b[0m  0.0471\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.4473\u001b[0m  0.0340\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m2.8915\u001b[0m  0.0586\n",
      "      2        3.0701  0.0322\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.3675\u001b[0m  0.0347\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2        3.1274  0.0370\n",
      "      2        3.4473  0.0480\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2        2.8915  0.0376\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.0688\u001b[0m  0.0438\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.3621\u001b[0m  0.0478\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m2.8997\u001b[0m  0.0587\n",
      "      3        3.0701  0.0504\n",
      "      3        3.4473  0.0328\n",
      "      2        3.3675  0.0511\n",
      "      3        3.1274  0.0516\n",
      "      3        2.8915  0.0403\n",
      "      4        3.0701  0.0313\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.4565\u001b[0m  0.0598\n",
      "      2        3.3621  0.0388\n",
      "      4        3.4473  0.0350\n",
      "      2        3.0688  0.0456\n",
      "      3        3.3675  0.0402\n",
      "      2        2.8997  0.0446\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.0991\u001b[0m  0.0416\n",
      "      4        3.1274  0.0389\n",
      "      4        2.8915  0.0377\n",
      "      5        3.0701  0.0337\n",
      "      3        3.3621  0.0397\n",
      "      2        3.4565  0.0414\n",
      "      4        3.3675  0.0312\n",
      "      5        3.4473  0.0469\n",
      "      5        3.1274  0.0369\n",
      "      2        3.0991  0.0409\n",
      "      3        2.8997  0.0446\n",
      "      5        2.8915  0.0370\n",
      "      3        3.0688  0.0601\n",
      "      3        3.4565  0.0294\n",
      "      5        3.3675  0.0369\n",
      "      4        3.3621  0.0410\n",
      "      6        3.4473  0.0331\n",
      "      4        2.8997  0.0296\n",
      "      3        3.0991  0.0345\n",
      "      6        2.8915  0.0337\n",
      "      6        3.1274  0.0456\n",
      "      6        3.0701  0.0640\n",
      "      4        3.4565  0.0357\n",
      "      4        3.0688  0.0416\n",
      "      6        3.3675  0.0339\n",
      "      7        3.4473  0.0374\n",
      "      5        2.8997  0.0378\n",
      "      4        3.0991  0.0343\n",
      "      5        3.3621  0.0439\n",
      "      7        2.8915  0.0340\n",
      "      7        3.0701  0.0397\n",
      "      7        3.1274  0.0421\n",
      "      5        3.4565  0.0342\n",
      "      7        3.3675  0.0382\n",
      "      5        3.0688  0.0496\n",
      "      6        2.8997  0.0409      8        2.8915  0.0339\n",
      "\n",
      "      5        3.0991  0.0410\n",
      "      6        3.3621  0.0374\n",
      "      8        3.0701  0.0357\n",
      "      6        3.4565  0.0329\n",
      "      8        3.1274  0.0415\n",
      "      8        3.4473  0.0635\n",
      "      8        3.3675  0.0389\n",
      "      6        3.0688  0.0374\n",
      "      6        3.0991  0.0335\n",
      "      9        2.8915  0.0382\n",
      "      7        3.3621  0.0402\n",
      "      7        2.8997  0.0475\n",
      "      9        3.0701  0.0444\n",
      "      9        3.1274  0.0381\n",
      "      9        3.3675  0.0340\n",
      "      9        3.4473  0.0410\n",
      "      7        3.0688  0.0363\n",
      "      7        3.4565  0.0607\n",
      "     10        2.8915  0.0413\n",
      "Restoring best model from epoch 1.\n",
      "      7        3.0991  0.0470\n",
      "      8        2.8997  0.0389\n",
      "      8        3.3621  0.0494\n",
      "     10        3.0701  0.0378\n",
      "Restoring best model from epoch 1.\n",
      "     10        3.1274  0.0380\n",
      "Restoring best model from epoch 1.\n",
      "     10        3.3675  0.0355\n",
      "Restoring best model from epoch 1.\n",
      "     10        3.4473  0.0388\n",
      "      8        3.0688  0.0348\n",
      "Restoring best model from epoch 1.\n",
      "      8        3.4565  0.0382\n",
      "      8        3.0991  0.0452\n",
      "      9        3.3621  0.0360\n",
      "      9        2.8997  0.0449\n",
      "      9        3.0688  0.0385\n",
      "      9        3.4565  0.0420\n",
      "     10        2.8997  0.0316\n",
      "Restoring best model from epoch 1.\n",
      "     10        3.3621  0.0453\n",
      "Restoring best model from epoch 1.\n",
      "     10        3.0688  0.0390\n",
      "Restoring best model from epoch 1.\n",
      "      9        3.0991  0.0571\n",
      "     10        3.4565  0.0499\n",
      "Restoring best model from epoch 1.\n",
      "     10        3.0991  0.0470\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.2380\u001b[0m  0.0368\n",
      "      2        3.2380  0.0161\n",
      "      3        3.2380  0.0200\n",
      "      4        3.2380  0.0157\n",
      "      5        3.2380  0.0179\n",
      "      6        3.2380  0.0165\n",
      "      7        3.2380  0.0169\n",
      "      8        3.2380  0.0202\n",
      "      9        3.2380  0.0177\n",
      "     10        3.2380  0.0176\n",
      "Restoring best model from epoch 1.\n",
      "y_train breslow final [ 1.000e+00  2.000e+00  3.000e+00  6.000e+00 -1.200e+01  1.200e+01\n",
      " -1.200e+01  1.200e+01 -1.300e+01 -1.500e+01  1.700e+01  2.300e+01\n",
      "  2.400e+01 -3.000e+01 -3.000e+01 -3.100e+01 -3.100e+01  3.400e+01\n",
      " -3.700e+01 -4.100e+01  5.200e+01  5.300e+01 -5.500e+01  5.900e+01\n",
      " -6.000e+01 -6.100e+01 -6.100e+01 -6.100e+01 -6.100e+01 -6.100e+01\n",
      " -6.100e+01 -6.100e+01  6.100e+01  8.000e+01 -8.200e+01 -8.300e+01\n",
      "  8.400e+01 -8.900e+01  8.900e+01  9.200e+01  9.400e+01 -9.700e+01\n",
      " -1.030e+02 -1.050e+02 -1.060e+02  1.090e+02 -1.110e+02 -1.110e+02\n",
      "  1.130e+02  1.160e+02 -1.210e+02 -1.220e+02 -1.220e+02  1.230e+02\n",
      "  1.230e+02 -1.280e+02  1.310e+02 -1.350e+02  1.360e+02  1.380e+02\n",
      " -1.500e+02  1.510e+02  1.530e+02  1.610e+02  1.660e+02  1.660e+02\n",
      " -1.730e+02  1.800e+02  1.880e+02  1.950e+02  1.980e+02  1.980e+02\n",
      " -2.020e+02  2.150e+02  2.230e+02 -2.270e+02 -2.360e+02  2.360e+02\n",
      " -2.420e+02 -2.470e+02  2.610e+02  2.660e+02  2.740e+02  2.760e+02\n",
      "  2.840e+02 -2.890e+02  2.910e+02  2.940e+02 -2.960e+02  3.060e+02\n",
      "  3.070e+02  3.150e+02  3.220e+02 -3.240e+02  3.290e+02  3.400e+02\n",
      "  3.450e+02  3.450e+02  3.510e+02 -3.530e+02  3.570e+02  3.580e+02\n",
      " -3.580e+02 -3.580e+02  3.580e+02  3.600e+02  3.650e+02  3.660e+02\n",
      " -3.700e+02  3.720e+02 -3.750e+02 -3.770e+02 -3.780e+02  3.830e+02\n",
      "  3.870e+02 -3.920e+02 -3.960e+02  3.970e+02 -4.010e+02  4.020e+02\n",
      "  4.030e+02 -4.050e+02  4.080e+02  4.120e+02  4.180e+02  4.280e+02\n",
      " -4.280e+02 -4.280e+02 -4.300e+02 -4.320e+02 -4.400e+02  4.450e+02\n",
      " -4.480e+02  4.480e+02  4.540e+02 -4.550e+02  4.560e+02 -4.650e+02\n",
      " -4.680e+02  4.740e+02 -4.810e+02 -4.820e+02  4.900e+02 -4.910e+02\n",
      " -4.930e+02 -4.980e+02  5.060e+02 -5.100e+02 -5.100e+02 -5.110e+02\n",
      "  5.150e+02  5.160e+02  5.190e+02 -5.310e+02  5.320e+02 -5.380e+02\n",
      "  5.390e+02  5.430e+02  5.440e+02  5.500e+02  5.520e+02 -5.550e+02\n",
      " -5.570e+02  5.590e+02  5.620e+02 -5.700e+02  5.730e+02 -5.780e+02\n",
      " -5.780e+02 -5.790e+02 -5.810e+02 -5.850e+02  5.870e+02 -5.960e+02\n",
      " -6.000e+02  6.010e+02  6.040e+02 -6.080e+02 -6.160e+02  6.230e+02\n",
      " -6.300e+02  6.320e+02 -6.390e+02 -6.400e+02 -6.400e+02 -6.410e+02\n",
      "  6.450e+02 -6.490e+02 -6.520e+02 -6.530e+02 -6.660e+02  6.670e+02\n",
      " -6.710e+02  6.780e+02 -6.830e+02  6.840e+02  6.870e+02 -6.880e+02\n",
      "  6.920e+02 -6.990e+02 -6.990e+02 -6.990e+02 -7.000e+02  7.080e+02\n",
      "  7.160e+02 -7.170e+02 -7.340e+02  7.400e+02 -7.410e+02 -7.570e+02\n",
      " -7.590e+02 -7.620e+02 -7.650e+02 -7.690e+02 -7.900e+02 -8.040e+02\n",
      " -8.100e+02 -8.150e+02 -8.160e+02 -8.220e+02  8.220e+02  8.260e+02\n",
      "  8.260e+02 -8.260e+02  8.270e+02  8.350e+02  8.400e+02 -8.420e+02\n",
      " -8.490e+02 -8.620e+02 -8.670e+02  8.990e+02 -9.080e+02 -9.100e+02\n",
      " -9.100e+02 -9.110e+02 -9.110e+02 -9.110e+02 -9.130e+02  9.160e+02\n",
      " -9.230e+02 -9.250e+02  9.270e+02 -9.320e+02  9.330e+02 -9.370e+02\n",
      " -9.420e+02  9.510e+02  9.620e+02  9.650e+02  9.740e+02 -9.790e+02\n",
      " -9.830e+02 -9.900e+02 -9.950e+02  1.000e+03  1.001e+03  1.006e+03\n",
      " -1.007e+03 -1.011e+03 -1.031e+03 -1.032e+03 -1.038e+03 -1.052e+03\n",
      "  1.058e+03 -1.058e+03 -1.063e+03  1.067e+03 -1.072e+03  1.075e+03\n",
      " -1.092e+03  1.097e+03 -1.100e+03 -1.106e+03 -1.111e+03 -1.113e+03\n",
      "  1.114e+03  1.143e+03  1.150e+03  1.154e+03 -1.160e+03  1.161e+03\n",
      " -1.176e+03 -1.180e+03 -1.182e+03  1.190e+03 -1.196e+03 -1.212e+03\n",
      " -1.217e+03 -1.223e+03 -1.224e+03 -1.244e+03 -1.260e+03 -1.268e+03\n",
      " -1.280e+03 -1.297e+03 -1.311e+03  1.315e+03  1.335e+03 -1.336e+03\n",
      "  1.338e+03  1.346e+03  1.423e+03  1.426e+03  1.470e+03 -1.475e+03\n",
      " -1.492e+03 -1.505e+03 -1.519e+03 -1.531e+03 -1.541e+03 -1.645e+03\n",
      "  1.655e+03  1.679e+03 -1.682e+03 -1.690e+03  1.695e+03  1.713e+03\n",
      " -1.723e+03 -1.743e+03 -1.784e+03 -1.824e+03 -1.834e+03 -1.841e+03\n",
      " -1.845e+03  1.856e+03  1.874e+03 -1.884e+03 -1.927e+03  1.933e+03\n",
      "  1.953e+03 -1.963e+03  1.975e+03  1.984e+03 -1.992e+03 -1.997e+03\n",
      " -2.023e+03 -2.024e+03 -2.026e+03 -2.073e+03 -2.080e+03  2.086e+03\n",
      " -2.133e+03  2.133e+03 -2.134e+03 -2.142e+03 -2.148e+03  2.160e+03\n",
      " -2.165e+03 -2.167e+03  2.170e+03 -2.271e+03  2.284e+03  2.304e+03\n",
      " -2.336e+03  2.378e+03  2.409e+03 -2.447e+03 -2.471e+03 -2.510e+03\n",
      " -2.589e+03  2.625e+03  2.639e+03 -2.645e+03  2.680e+03 -2.716e+03\n",
      " -2.811e+03 -2.820e+03  2.945e+03  2.979e+03 -3.016e+03 -3.108e+03\n",
      " -3.123e+03  3.149e+03 -3.166e+03 -3.189e+03  3.253e+03 -3.387e+03\n",
      " -3.576e+03  3.600e+03 -3.636e+03 -3.724e+03 -3.747e+03  3.838e+03\n",
      " -3.850e+03  3.924e+03 -4.053e+03 -4.068e+03 -4.261e+03 -4.570e+03\n",
      "  4.601e+03 -4.694e+03 -4.765e+03  5.287e+03]\n",
      "Concordance Index 0.5010248981934461\n",
      "Integrated Brier Score: 0.19363057636204334\n",
      "y_train breslow final [ 1.000e+00  2.000e+00  3.000e+00  6.000e+00 -1.200e+01  1.200e+01\n",
      " -1.200e+01  1.200e+01 -1.300e+01 -1.500e+01  1.700e+01  2.300e+01\n",
      "  2.400e+01 -3.000e+01 -3.000e+01 -3.100e+01 -3.100e+01  3.400e+01\n",
      " -3.700e+01 -4.100e+01  5.200e+01  5.300e+01 -5.500e+01  5.900e+01\n",
      " -6.000e+01 -6.100e+01 -6.100e+01 -6.100e+01 -6.100e+01 -6.100e+01\n",
      " -6.100e+01 -6.100e+01  6.100e+01  8.000e+01 -8.200e+01 -8.300e+01\n",
      "  8.400e+01 -8.900e+01  8.900e+01  9.200e+01  9.400e+01 -9.700e+01\n",
      " -1.030e+02 -1.050e+02 -1.060e+02  1.090e+02 -1.110e+02 -1.110e+02\n",
      "  1.130e+02  1.160e+02 -1.210e+02 -1.220e+02 -1.220e+02  1.230e+02\n",
      "  1.230e+02 -1.280e+02  1.310e+02 -1.350e+02  1.360e+02  1.380e+02\n",
      " -1.500e+02  1.510e+02  1.530e+02  1.610e+02  1.660e+02  1.660e+02\n",
      " -1.730e+02  1.800e+02  1.880e+02  1.950e+02  1.980e+02  1.980e+02\n",
      " -2.020e+02  2.150e+02  2.230e+02 -2.270e+02 -2.360e+02  2.360e+02\n",
      " -2.420e+02 -2.470e+02  2.610e+02  2.660e+02  2.740e+02  2.760e+02\n",
      "  2.840e+02 -2.890e+02  2.910e+02  2.940e+02 -2.960e+02  3.060e+02\n",
      "  3.070e+02  3.150e+02  3.220e+02 -3.240e+02  3.290e+02  3.400e+02\n",
      "  3.450e+02  3.450e+02  3.510e+02 -3.530e+02  3.570e+02  3.580e+02\n",
      " -3.580e+02 -3.580e+02  3.580e+02  3.600e+02  3.650e+02  3.660e+02\n",
      " -3.700e+02  3.720e+02 -3.750e+02 -3.770e+02 -3.780e+02  3.830e+02\n",
      "  3.870e+02 -3.920e+02 -3.960e+02  3.970e+02 -4.010e+02  4.020e+02\n",
      "  4.030e+02 -4.050e+02  4.080e+02  4.120e+02  4.180e+02  4.280e+02\n",
      " -4.280e+02 -4.280e+02 -4.300e+02 -4.320e+02 -4.400e+02  4.450e+02\n",
      " -4.480e+02  4.480e+02  4.540e+02 -4.550e+02  4.560e+02 -4.650e+02\n",
      " -4.680e+02  4.740e+02 -4.810e+02 -4.820e+02  4.900e+02 -4.910e+02\n",
      " -4.930e+02 -4.980e+02  5.060e+02 -5.100e+02 -5.100e+02 -5.110e+02\n",
      "  5.150e+02  5.160e+02  5.190e+02 -5.310e+02  5.320e+02 -5.380e+02\n",
      "  5.390e+02  5.430e+02  5.440e+02  5.500e+02  5.520e+02 -5.550e+02\n",
      " -5.570e+02  5.590e+02  5.620e+02 -5.700e+02  5.730e+02 -5.780e+02\n",
      " -5.780e+02 -5.790e+02 -5.810e+02 -5.850e+02  5.870e+02 -5.960e+02\n",
      " -6.000e+02  6.010e+02  6.040e+02 -6.080e+02 -6.160e+02  6.230e+02\n",
      " -6.300e+02  6.320e+02 -6.390e+02 -6.400e+02 -6.400e+02 -6.410e+02\n",
      "  6.450e+02 -6.490e+02 -6.520e+02 -6.530e+02 -6.660e+02  6.670e+02\n",
      " -6.710e+02  6.780e+02 -6.830e+02  6.840e+02  6.870e+02 -6.880e+02\n",
      "  6.920e+02 -6.990e+02 -6.990e+02 -6.990e+02 -7.000e+02  7.080e+02\n",
      "  7.160e+02 -7.170e+02 -7.340e+02  7.400e+02 -7.410e+02 -7.570e+02\n",
      " -7.590e+02 -7.620e+02 -7.650e+02 -7.690e+02 -7.900e+02 -8.040e+02\n",
      " -8.100e+02 -8.150e+02 -8.160e+02 -8.220e+02  8.220e+02  8.260e+02\n",
      "  8.260e+02 -8.260e+02  8.270e+02  8.350e+02  8.400e+02 -8.420e+02\n",
      " -8.490e+02 -8.620e+02 -8.670e+02  8.990e+02 -9.080e+02 -9.100e+02\n",
      " -9.100e+02 -9.110e+02 -9.110e+02 -9.110e+02 -9.130e+02  9.160e+02\n",
      " -9.230e+02 -9.250e+02  9.270e+02 -9.320e+02  9.330e+02 -9.370e+02\n",
      " -9.420e+02  9.510e+02  9.620e+02  9.650e+02  9.740e+02 -9.790e+02\n",
      " -9.830e+02 -9.900e+02 -9.950e+02  1.000e+03  1.001e+03  1.006e+03\n",
      " -1.007e+03 -1.011e+03 -1.031e+03 -1.032e+03 -1.038e+03 -1.052e+03\n",
      "  1.058e+03 -1.058e+03 -1.063e+03  1.067e+03 -1.072e+03  1.075e+03\n",
      " -1.092e+03  1.097e+03 -1.100e+03 -1.106e+03 -1.111e+03 -1.113e+03\n",
      "  1.114e+03  1.143e+03  1.150e+03  1.154e+03 -1.160e+03  1.161e+03\n",
      " -1.176e+03 -1.180e+03 -1.182e+03  1.190e+03 -1.196e+03 -1.212e+03\n",
      " -1.217e+03 -1.223e+03 -1.224e+03 -1.244e+03 -1.260e+03 -1.268e+03\n",
      " -1.280e+03 -1.297e+03 -1.311e+03  1.315e+03  1.335e+03 -1.336e+03\n",
      "  1.338e+03  1.346e+03  1.423e+03  1.426e+03  1.470e+03 -1.475e+03\n",
      " -1.492e+03 -1.505e+03 -1.519e+03 -1.531e+03 -1.541e+03 -1.645e+03\n",
      "  1.655e+03  1.679e+03 -1.682e+03 -1.690e+03  1.695e+03  1.713e+03\n",
      " -1.723e+03 -1.743e+03 -1.784e+03 -1.824e+03 -1.834e+03 -1.841e+03\n",
      " -1.845e+03  1.856e+03  1.874e+03 -1.884e+03 -1.927e+03  1.933e+03\n",
      "  1.953e+03 -1.963e+03  1.975e+03  1.984e+03 -1.992e+03 -1.997e+03\n",
      " -2.023e+03 -2.024e+03 -2.026e+03 -2.073e+03 -2.080e+03  2.086e+03\n",
      " -2.133e+03  2.133e+03 -2.134e+03 -2.142e+03 -2.148e+03  2.160e+03\n",
      " -2.165e+03 -2.167e+03  2.170e+03 -2.271e+03  2.284e+03  2.304e+03\n",
      " -2.336e+03  2.378e+03  2.409e+03 -2.447e+03 -2.471e+03 -2.510e+03\n",
      " -2.589e+03  2.625e+03  2.639e+03 -2.645e+03  2.680e+03 -2.716e+03\n",
      " -2.811e+03 -2.820e+03  2.945e+03  2.979e+03 -3.016e+03 -3.108e+03\n",
      " -3.123e+03  3.149e+03 -3.166e+03 -3.189e+03  3.253e+03 -3.387e+03\n",
      " -3.576e+03  3.600e+03 -3.636e+03 -3.724e+03 -3.747e+03  3.838e+03\n",
      " -3.850e+03  3.924e+03 -4.053e+03 -4.068e+03 -4.261e+03 -4.570e+03\n",
      "  4.601e+03 -4.694e+03 -4.765e+03  5.287e+03]\n",
      "durations 4.0 4026.0\n",
      "Concordance Index 0.4928746928746929\n",
      "Integrated Brier Score: 0.1951730228338156\n",
      "(394, 20531) <class 'pandas.core.frame.DataFrame'>\n",
      "(394,) <class 'pandas.core.series.Series'>\n",
      "(99, 20531) <class 'pandas.core.frame.DataFrame'>\n",
      "(99,) <class 'pandas.core.series.Series'>\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m2.9374\u001b[0m  0.0419\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m2.8741\u001b[0m  0.0579\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m2.6835\u001b[0m  0.0425\n",
      "      2        2.9374  0.0342\n",
      "      2        2.8741  0.0323\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m2.3295\u001b[0m  0.0312\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m2.3272\u001b[0m  0.0380\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      2        2.6835  0.0357\n",
      "      3        2.8741  0.0327\n",
      "      3        2.9374  0.0353\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.4595\u001b[0m  0.0317\n",
      "      2        2.3295  0.0435\n",
      "      2        2.3272  0.0327\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur  epoch    valid_loss     dur\n",
      "\n",
      "-------  ------------  -------------  ------------  ------\n",
      "\n",
      "      1        \u001b[36m2.6535\u001b[0m  0.0537\n",
      "      1        \u001b[36m2.8612\u001b[0m  0.0298\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      3        2.6835  0.0288\n",
      "      4        2.8741  0.0325\n",
      "      2        3.4595  0.0284\n",
      "      4        2.9374  0.0409\n",
      "      3        2.3272  0.0328\n",
      "      3        2.3295  0.0336\n",
      "      2        2.8612  0.0307\n",
      "      2        2.6535  0.0366\n",
      "      5        2.8741  0.0297\n",
      "      3        3.4595  0.0323\n",
      "      4        2.3295  0.0309\n",
      "      4        2.3272  0.0310\n",
      "      4        2.6835  0.0500\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m2.9285\u001b[0m  0.0489\n",
      "      3        2.8612  0.0351\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.4164\u001b[0m  0.0557\n",
      "      3        2.6535  0.0409\n",
      "      4        3.4595  0.0304\n",
      "      5        2.9374  0.0600\n",
      "      6        2.8741  0.0410\n",
      "      5        2.3272  0.0322\n",
      "      5        2.3295  0.0319\n",
      "      5        2.6835  0.0320\n",
      "      2        2.9285  0.0310\n",
      "      4        2.8612  0.0366\n",
      "      2        3.4164  0.0300\n",
      "      5        3.4595  0.0328\n",
      "      4        2.6535  0.0355\n",
      "      7        2.8741  0.0284\n",
      "      6        2.9374  0.0373\n",
      "      6        2.3272  0.0307\n",
      "      6        2.3295  0.0334\n",
      "      3        2.9285  0.0348\n",
      "      6        2.6835  0.0389\n",
      "      3        3.4164  0.0294\n",
      "      6        3.4595  0.0296\n",
      "      5        2.6535  0.0343\n",
      "      5        2.8612  0.0456\n",
      "      8        2.8741  0.0378\n",
      "      7        2.3272  0.0325\n",
      "      7        2.3295  0.0368\n",
      "      7        2.6835  0.0336\n",
      "      4        3.4164  0.0296\n",
      "      4        2.9285  0.0410\n",
      "      7        2.9374  0.0490\n",
      "      7        3.4595  0.0361\n",
      "      6        2.6535  0.0304\n",
      "      6        2.8612  0.0330\n",
      "      8        2.3272  0.0325\n",
      "      9        2.8741  0.0424\n",
      "      5        2.9285  0.0301\n",
      "      8        2.3295  0.0401\n",
      "      8        3.4595  0.0284\n",
      "      8        2.9374  0.0364\n",
      "      7        2.6535  0.0290\n",
      "      5        3.4164  0.0417\n",
      "      7        2.8612  0.0335\n",
      "      8        2.6835  0.0595\n",
      "     10        2.8741  0.0320\n",
      "Restoring best model from epoch 1.\n",
      "      9        2.3295  0.0287\n",
      "      9        2.3272  0.0457\n",
      "      9        2.9374  0.0361\n",
      "      9        3.4595  0.0404\n",
      "      8        2.8612  0.0354\n",
      "     10        2.3295  0.0283\n",
      "Restoring best model from epoch 1.\n",
      "      8        2.6535  0.0571\n",
      "      6        2.9285  0.0655\n",
      "     10        2.9374  0.0258\n",
      "Restoring best model from epoch 1.\n",
      "      6        3.4164  0.0668\n",
      "     10        2.3272  0.0440\n",
      "      9        2.8612  0.0281\n",
      "Restoring best model from epoch 1.\n",
      "     10        3.4595  0.0349\n",
      "Restoring best model from epoch 1.\n",
      "      9        2.6835  0.0614\n",
      "     10        2.8612  0.0284\n",
      "Restoring best model from epoch 1.\n",
      "      9        2.6535  0.0436\n",
      "      7        2.9285  0.0495\n",
      "     10        2.6835  0.0513\n",
      "      7        3.4164  0.0652\n",
      "      8        2.9285  0.0276\n",
      "Restoring best model from epoch 1.\n",
      "     10        2.6535  0.0407\n",
      "Restoring best model from epoch 1.\n",
      "      8        3.4164  0.0291\n",
      "      9        2.9285  0.0324\n",
      "     10        2.9285  0.0268\n",
      "Restoring best model from epoch 1.\n",
      "      9        3.4164  0.0393\n",
      "     10        3.4164  0.0277\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m2.6870\u001b[0m  0.0483\n",
      "      2        2.6870  0.0493\n",
      "      3        2.6870  0.0214\n",
      "      4        2.6870  0.0157\n",
      "      5        2.6870  0.0178\n",
      "      6        2.6870  0.0164\n",
      "      7        2.6870  0.0159\n",
      "      8        2.6870  0.0318\n",
      "      9        2.6870  0.0191\n",
      "     10        2.6870  0.0147\n",
      "Restoring best model from epoch 1.\n",
      "y_train breslow final [ 1.000e+00  2.000e+00  3.000e+00 -4.000e+00  5.000e+00  5.000e+00\n",
      "  6.000e+00 -8.000e+00  9.000e+00 -1.200e+01  1.200e+01  1.200e+01\n",
      " -1.300e+01 -1.500e+01  2.300e+01  2.400e+01 -2.800e+01 -3.000e+01\n",
      " -3.000e+01 -3.000e+01 -3.100e+01 -3.100e+01  3.400e+01 -3.500e+01\n",
      " -3.700e+01 -4.100e+01 -4.200e+01  4.700e+01  5.200e+01  5.300e+01\n",
      " -5.500e+01  5.900e+01 -6.000e+01 -6.100e+01 -6.100e+01 -6.100e+01\n",
      " -6.100e+01 -6.100e+01 -6.100e+01  6.100e+01 -7.000e+01  8.000e+01\n",
      " -8.200e+01 -8.300e+01  8.500e+01  8.800e+01  8.900e+01  9.200e+01\n",
      "  9.400e+01 -9.700e+01 -1.030e+02 -1.060e+02 -1.060e+02  1.090e+02\n",
      " -1.110e+02  1.130e+02 -1.210e+02 -1.220e+02 -1.220e+02 -1.220e+02\n",
      " -1.220e+02  1.230e+02  1.230e+02  1.310e+02 -1.350e+02  1.430e+02\n",
      "  1.460e+02 -1.500e+02  1.510e+02 -1.530e+02  1.530e+02 -1.600e+02\n",
      "  1.660e+02 -1.730e+02  1.880e+02  1.950e+02 -2.070e+02 -2.110e+02\n",
      "  2.110e+02  2.120e+02 -2.270e+02 -2.360e+02  2.360e+02 -2.380e+02\n",
      " -2.440e+02 -2.470e+02  2.610e+02  2.660e+02  2.760e+02  2.840e+02\n",
      " -2.890e+02  2.910e+02 -2.960e+02  2.990e+02  3.020e+02  3.060e+02\n",
      "  3.070e+02  3.110e+02  3.220e+02  3.290e+02  3.400e+02  3.450e+02\n",
      "  3.450e+02  3.510e+02 -3.530e+02  3.570e+02  3.580e+02 -3.580e+02\n",
      " -3.580e+02  3.580e+02  3.600e+02  3.650e+02  3.660e+02 -3.660e+02\n",
      " -3.670e+02 -3.700e+02  3.710e+02 -3.750e+02 -3.770e+02 -3.780e+02\n",
      "  3.830e+02  3.830e+02  3.870e+02 -3.920e+02 -3.960e+02  3.970e+02\n",
      "  3.990e+02 -4.010e+02  4.030e+02 -4.050e+02 -4.050e+02 -4.060e+02\n",
      " -4.070e+02  4.080e+02  4.180e+02 -4.200e+02 -4.230e+02  4.280e+02\n",
      " -4.280e+02  4.290e+02 -4.300e+02 -4.320e+02  4.420e+02  4.450e+02\n",
      " -4.550e+02  4.550e+02  4.560e+02 -4.650e+02 -4.680e+02  4.740e+02\n",
      " -4.810e+02 -4.820e+02 -4.910e+02 -4.920e+02 -4.930e+02 -4.980e+02\n",
      "  5.010e+02  5.060e+02 -5.100e+02 -5.110e+02  5.150e+02 -5.170e+02\n",
      "  5.190e+02 -5.310e+02  5.320e+02 -5.380e+02  5.390e+02  5.430e+02\n",
      "  5.440e+02  5.500e+02  5.520e+02 -5.570e+02  5.590e+02 -5.590e+02\n",
      "  5.620e+02 -5.650e+02 -5.700e+02  5.730e+02 -5.760e+02 -5.780e+02\n",
      " -5.780e+02 -5.810e+02 -5.850e+02  5.870e+02 -5.890e+02 -6.010e+02\n",
      "  6.040e+02 -6.080e+02 -6.150e+02 -6.160e+02 -6.180e+02  6.230e+02\n",
      " -6.300e+02  6.320e+02 -6.370e+02 -6.390e+02 -6.400e+02 -6.400e+02\n",
      " -6.410e+02 -6.420e+02 -6.490e+02 -6.520e+02 -6.530e+02 -6.600e+02\n",
      " -6.660e+02  6.670e+02  6.690e+02 -6.710e+02 -6.830e+02  6.840e+02\n",
      "  6.870e+02  6.920e+02 -6.990e+02 -6.990e+02 -6.990e+02  7.080e+02\n",
      "  7.160e+02 -7.170e+02  7.340e+02  7.400e+02 -7.580e+02 -7.590e+02\n",
      " -7.690e+02  7.700e+02 -7.900e+02  8.030e+02 -8.040e+02 -8.040e+02\n",
      " -8.100e+02 -8.150e+02 -8.160e+02 -8.180e+02 -8.220e+02  8.220e+02\n",
      "  8.260e+02  8.270e+02 -8.330e+02  8.350e+02  8.400e+02 -8.490e+02\n",
      " -8.620e+02 -8.670e+02  8.810e+02  8.990e+02 -9.080e+02 -9.100e+02\n",
      " -9.100e+02 -9.110e+02 -9.110e+02 -9.130e+02  9.160e+02  9.210e+02\n",
      " -9.230e+02 -9.250e+02 -9.320e+02  9.330e+02  9.370e+02 -9.370e+02\n",
      " -9.420e+02  9.510e+02 -9.520e+02 -9.600e+02  9.620e+02  9.730e+02\n",
      "  9.740e+02 -9.790e+02 -9.830e+02 -9.900e+02 -9.950e+02  1.000e+03\n",
      "  1.001e+03  1.006e+03 -1.007e+03 -1.011e+03 -1.031e+03 -1.032e+03\n",
      " -1.038e+03  1.045e+03 -1.050e+03 -1.052e+03  1.057e+03  1.058e+03\n",
      " -1.063e+03  1.067e+03 -1.100e+03 -1.106e+03  1.107e+03 -1.111e+03\n",
      " -1.113e+03  1.114e+03  1.143e+03  1.154e+03  1.161e+03 -1.176e+03\n",
      "  1.189e+03  1.190e+03 -1.196e+03 -1.212e+03 -1.217e+03 -1.223e+03\n",
      " -1.244e+03 -1.259e+03 -1.260e+03 -1.280e+03 -1.311e+03  1.315e+03\n",
      "  1.335e+03 -1.336e+03  1.338e+03  1.344e+03  1.346e+03 -1.361e+03\n",
      " -1.386e+03  1.423e+03  1.426e+03  1.470e+03 -1.475e+03  1.485e+03\n",
      " -1.499e+03 -1.519e+03 -1.531e+03 -1.541e+03 -1.602e+03 -1.639e+03\n",
      "  1.640e+03  1.656e+03  1.679e+03 -1.690e+03  1.695e+03  1.713e+03\n",
      " -1.723e+03 -1.731e+03  1.736e+03 -1.784e+03 -1.841e+03  1.841e+03\n",
      "  1.874e+03 -1.884e+03  1.912e+03  1.933e+03  1.953e+03 -1.963e+03\n",
      "  1.975e+03  1.984e+03 -1.992e+03 -1.997e+03 -2.023e+03 -2.024e+03\n",
      " -2.026e+03 -2.080e+03  2.086e+03 -2.133e+03  2.133e+03 -2.142e+03\n",
      " -2.148e+03  2.160e+03 -2.165e+03  2.170e+03  2.224e+03 -2.227e+03\n",
      " -2.271e+03  2.284e+03  2.304e+03 -2.336e+03  2.378e+03 -2.381e+03\n",
      " -2.419e+03 -2.447e+03 -2.471e+03 -2.510e+03 -2.542e+03 -2.716e+03\n",
      "  2.803e+03 -2.820e+03  2.979e+03 -3.016e+03 -3.123e+03  3.149e+03\n",
      " -3.189e+03  3.253e+03  3.376e+03 -3.387e+03 -3.636e+03 -3.644e+03\n",
      " -3.747e+03  3.838e+03 -3.850e+03 -4.026e+03 -4.053e+03 -4.261e+03\n",
      " -4.570e+03  4.601e+03 -4.694e+03 -4.765e+03]\n",
      "Concordance Index 0.500169664065151\n",
      "Integrated Brier Score: 0.1940776286794994\n",
      "y_train breslow final [ 1.000e+00  2.000e+00  3.000e+00 -4.000e+00  5.000e+00  5.000e+00\n",
      "  6.000e+00 -8.000e+00  9.000e+00 -1.200e+01  1.200e+01  1.200e+01\n",
      " -1.300e+01 -1.500e+01  2.300e+01  2.400e+01 -2.800e+01 -3.000e+01\n",
      " -3.000e+01 -3.000e+01 -3.100e+01 -3.100e+01  3.400e+01 -3.500e+01\n",
      " -3.700e+01 -4.100e+01 -4.200e+01  4.700e+01  5.200e+01  5.300e+01\n",
      " -5.500e+01  5.900e+01 -6.000e+01 -6.100e+01 -6.100e+01 -6.100e+01\n",
      " -6.100e+01 -6.100e+01 -6.100e+01  6.100e+01 -7.000e+01  8.000e+01\n",
      " -8.200e+01 -8.300e+01  8.500e+01  8.800e+01  8.900e+01  9.200e+01\n",
      "  9.400e+01 -9.700e+01 -1.030e+02 -1.060e+02 -1.060e+02  1.090e+02\n",
      " -1.110e+02  1.130e+02 -1.210e+02 -1.220e+02 -1.220e+02 -1.220e+02\n",
      " -1.220e+02  1.230e+02  1.230e+02  1.310e+02 -1.350e+02  1.430e+02\n",
      "  1.460e+02 -1.500e+02  1.510e+02 -1.530e+02  1.530e+02 -1.600e+02\n",
      "  1.660e+02 -1.730e+02  1.880e+02  1.950e+02 -2.070e+02 -2.110e+02\n",
      "  2.110e+02  2.120e+02 -2.270e+02 -2.360e+02  2.360e+02 -2.380e+02\n",
      " -2.440e+02 -2.470e+02  2.610e+02  2.660e+02  2.760e+02  2.840e+02\n",
      " -2.890e+02  2.910e+02 -2.960e+02  2.990e+02  3.020e+02  3.060e+02\n",
      "  3.070e+02  3.110e+02  3.220e+02  3.290e+02  3.400e+02  3.450e+02\n",
      "  3.450e+02  3.510e+02 -3.530e+02  3.570e+02  3.580e+02 -3.580e+02\n",
      " -3.580e+02  3.580e+02  3.600e+02  3.650e+02  3.660e+02 -3.660e+02\n",
      " -3.670e+02 -3.700e+02  3.710e+02 -3.750e+02 -3.770e+02 -3.780e+02\n",
      "  3.830e+02  3.830e+02  3.870e+02 -3.920e+02 -3.960e+02  3.970e+02\n",
      "  3.990e+02 -4.010e+02  4.030e+02 -4.050e+02 -4.050e+02 -4.060e+02\n",
      " -4.070e+02  4.080e+02  4.180e+02 -4.200e+02 -4.230e+02  4.280e+02\n",
      " -4.280e+02  4.290e+02 -4.300e+02 -4.320e+02  4.420e+02  4.450e+02\n",
      " -4.550e+02  4.550e+02  4.560e+02 -4.650e+02 -4.680e+02  4.740e+02\n",
      " -4.810e+02 -4.820e+02 -4.910e+02 -4.920e+02 -4.930e+02 -4.980e+02\n",
      "  5.010e+02  5.060e+02 -5.100e+02 -5.110e+02  5.150e+02 -5.170e+02\n",
      "  5.190e+02 -5.310e+02  5.320e+02 -5.380e+02  5.390e+02  5.430e+02\n",
      "  5.440e+02  5.500e+02  5.520e+02 -5.570e+02  5.590e+02 -5.590e+02\n",
      "  5.620e+02 -5.650e+02 -5.700e+02  5.730e+02 -5.760e+02 -5.780e+02\n",
      " -5.780e+02 -5.810e+02 -5.850e+02  5.870e+02 -5.890e+02 -6.010e+02\n",
      "  6.040e+02 -6.080e+02 -6.150e+02 -6.160e+02 -6.180e+02  6.230e+02\n",
      " -6.300e+02  6.320e+02 -6.370e+02 -6.390e+02 -6.400e+02 -6.400e+02\n",
      " -6.410e+02 -6.420e+02 -6.490e+02 -6.520e+02 -6.530e+02 -6.600e+02\n",
      " -6.660e+02  6.670e+02  6.690e+02 -6.710e+02 -6.830e+02  6.840e+02\n",
      "  6.870e+02  6.920e+02 -6.990e+02 -6.990e+02 -6.990e+02  7.080e+02\n",
      "  7.160e+02 -7.170e+02  7.340e+02  7.400e+02 -7.580e+02 -7.590e+02\n",
      " -7.690e+02  7.700e+02 -7.900e+02  8.030e+02 -8.040e+02 -8.040e+02\n",
      " -8.100e+02 -8.150e+02 -8.160e+02 -8.180e+02 -8.220e+02  8.220e+02\n",
      "  8.260e+02  8.270e+02 -8.330e+02  8.350e+02  8.400e+02 -8.490e+02\n",
      " -8.620e+02 -8.670e+02  8.810e+02  8.990e+02 -9.080e+02 -9.100e+02\n",
      " -9.100e+02 -9.110e+02 -9.110e+02 -9.130e+02  9.160e+02  9.210e+02\n",
      " -9.230e+02 -9.250e+02 -9.320e+02  9.330e+02  9.370e+02 -9.370e+02\n",
      " -9.420e+02  9.510e+02 -9.520e+02 -9.600e+02  9.620e+02  9.730e+02\n",
      "  9.740e+02 -9.790e+02 -9.830e+02 -9.900e+02 -9.950e+02  1.000e+03\n",
      "  1.001e+03  1.006e+03 -1.007e+03 -1.011e+03 -1.031e+03 -1.032e+03\n",
      " -1.038e+03  1.045e+03 -1.050e+03 -1.052e+03  1.057e+03  1.058e+03\n",
      " -1.063e+03  1.067e+03 -1.100e+03 -1.106e+03  1.107e+03 -1.111e+03\n",
      " -1.113e+03  1.114e+03  1.143e+03  1.154e+03  1.161e+03 -1.176e+03\n",
      "  1.189e+03  1.190e+03 -1.196e+03 -1.212e+03 -1.217e+03 -1.223e+03\n",
      " -1.244e+03 -1.259e+03 -1.260e+03 -1.280e+03 -1.311e+03  1.315e+03\n",
      "  1.335e+03 -1.336e+03  1.338e+03  1.344e+03  1.346e+03 -1.361e+03\n",
      " -1.386e+03  1.423e+03  1.426e+03  1.470e+03 -1.475e+03  1.485e+03\n",
      " -1.499e+03 -1.519e+03 -1.531e+03 -1.541e+03 -1.602e+03 -1.639e+03\n",
      "  1.640e+03  1.656e+03  1.679e+03 -1.690e+03  1.695e+03  1.713e+03\n",
      " -1.723e+03 -1.731e+03  1.736e+03 -1.784e+03 -1.841e+03  1.841e+03\n",
      "  1.874e+03 -1.884e+03  1.912e+03  1.933e+03  1.953e+03 -1.963e+03\n",
      "  1.975e+03  1.984e+03 -1.992e+03 -1.997e+03 -2.023e+03 -2.024e+03\n",
      " -2.026e+03 -2.080e+03  2.086e+03 -2.133e+03  2.133e+03 -2.142e+03\n",
      " -2.148e+03  2.160e+03 -2.165e+03  2.170e+03  2.224e+03 -2.227e+03\n",
      " -2.271e+03  2.284e+03  2.304e+03 -2.336e+03  2.378e+03 -2.381e+03\n",
      " -2.419e+03 -2.447e+03 -2.471e+03 -2.510e+03 -2.542e+03 -2.716e+03\n",
      "  2.803e+03 -2.820e+03  2.979e+03 -3.016e+03 -3.123e+03  3.149e+03\n",
      " -3.189e+03  3.253e+03  3.376e+03 -3.387e+03 -3.636e+03 -3.644e+03\n",
      " -3.747e+03  3.838e+03 -3.850e+03 -4.026e+03 -4.053e+03 -4.261e+03\n",
      " -4.570e+03  4.601e+03 -4.694e+03 -4.765e+03]\n",
      "durations 12.0 5287.0\n",
      "Concordance Index 0.5212720240653201\n",
      "Integrated Brier Score: 0.20141119159809392\n",
      "(395, 20531) <class 'pandas.core.frame.DataFrame'>\n",
      "(395,) <class 'pandas.core.series.Series'>\n",
      "(98, 20531) <class 'pandas.core.frame.DataFrame'>\n",
      "(98,) <class 'pandas.core.series.Series'>\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.3302\u001b[0m  0.0470\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.9068\u001b[0m  0.0586\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      2        3.3302  0.0522\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "  epoch    valid_loss     dur      1        \u001b[36m3.9392\u001b[0m  0.0476\n",
      "\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.4198\u001b[0m  0.0353\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      2        3.9068  0.0425\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.4202\u001b[0m  0.0491\n",
      "      2        3.9392  0.0391\n",
      "      3        3.3302  0.0438\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.8166\u001b[0m  0.0372\n",
      "      2        3.4198  0.0592\n",
      "      2        3.4202  0.0358\n",
      "      3        3.9392  0.0419\n",
      "      4        3.3302  0.0407\n",
      "      2        3.8166  0.0395\n",
      "      3        3.9068  0.0881\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.8163\u001b[0m  0.0403\n",
      "      3        3.4198  0.0544\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.9246\u001b[0m  0.0580\n",
      "      4        3.9392  0.0339\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.3254\u001b[0m  0.0541\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.9020\u001b[0m  0.0519\n",
      "      5        3.3302  0.0545\n",
      "      4        3.9068  0.0369\n",
      "      3        3.4202  0.0652\n",
      "      3        3.8166  0.0443\n",
      "      5        3.9392  0.0292\n",
      "      4        3.4198  0.0374\n",
      "      2        3.9246  0.0467\n",
      "      4        3.8166  0.0289\n",
      "      2        3.3254  0.0517\n",
      "      2        3.8163  0.0590\n",
      "      5        3.9068  0.0368\n",
      "      2        3.9020  0.0477\n",
      "      5        3.4198  0.0279\n",
      "      6        3.3302  0.0450\n",
      "      4        3.4202  0.0439\n",
      "      6        3.9392  0.0475\n",
      "      5        3.8166  0.0287\n",
      "      3        3.9020  0.0284\n",
      "      6        3.4198  0.0289\n",
      "      6        3.9068  0.0348\n",
      "      3        3.3254  0.0437\n",
      "      3        3.8163  0.0419\n",
      "      3        3.9246  0.0601\n",
      "      6        3.8166  0.0293\n",
      "      7        3.3302  0.0501\n",
      "      4        3.9020  0.0278\n",
      "      5        3.4202  0.0539\n",
      "      7        3.9392  0.0448\n",
      "      7        3.4198  0.0295\n",
      "      7        3.9068  0.0289\n",
      "      4        3.8163  0.0388\n",
      "      7        3.8166  0.0346\n",
      "      5        3.9020  0.0284\n",
      "      4        3.3254  0.0492\n",
      "      4        3.9246  0.0435\n",
      "      8        3.9068  0.0313\n",
      "      8        3.4198  0.0321\n",
      "      8        3.9392  0.0456\n",
      "      8        3.3302  0.0516\n",
      "      8        3.8166  0.0357\n",
      "      5        3.8163  0.0464\n",
      "      6        3.9020  0.0400\n",
      "      9        3.4198  0.0319\n",
      "      6        3.4202  0.0666\n",
      "      5        3.9246  0.0438\n",
      "      5        3.3254  0.0530\n",
      "      9        3.9068  0.0472\n",
      "      9        3.8166  0.0280\n",
      "      9        3.3302  0.0405\n",
      "      9        3.9392  0.0453\n",
      "      7        3.9020  0.0280\n",
      "      6        3.8163  0.0336\n",
      "      7        3.4202  0.0317\n",
      "     10        3.4198  0.0390\n",
      "Restoring best model from epoch 1.\n",
      "     10        3.9068  0.0274\n",
      "Restoring best model from epoch 1.\n",
      "      6        3.9246  0.0388\n",
      "     10        3.8166  0.0299\n",
      "Restoring best model from epoch 1.\n",
      "      6        3.3254  0.0433\n",
      "      8        3.9020  0.0361\n",
      "     10        3.3302  0.0410\n",
      "      7        3.8163  0.0339\n",
      "Restoring best model from epoch 1.\n",
      "      8        3.4202  0.0367\n",
      "     10        3.9392  0.0535\n",
      "Restoring best model from epoch 1.\n",
      "      7        3.9246  0.0389\n",
      "      7        3.3254  0.0332\n",
      "      9        3.9020  0.0333\n",
      "      8        3.8163  0.0341\n",
      "      9        3.4202  0.0369\n",
      "      8        3.9246  0.0350\n",
      "     10        3.9020  0.0273\n",
      "      8        3.3254  0.0344\n",
      "Restoring best model from epoch 1.\n",
      "     10        3.4202  0.0317\n",
      "Restoring best model from epoch 1.\n",
      "      9        3.8163  0.0437\n",
      "      9        3.9246  0.0316\n",
      "      9        3.3254  0.0451\n",
      "     10        3.8163  0.0365\n",
      "Restoring best model from epoch 1.\n",
      "     10        3.9246  0.0288\n",
      "Restoring best model from epoch 1.\n",
      "     10        3.3254  0.0482\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.6009\u001b[0m  0.2848\n",
      "      2        3.6009  0.3240\n",
      "      3        3.6009  0.1273\n",
      "      4        3.6009  0.0667\n",
      "      5        3.6009  0.0230\n",
      "      6        3.6009  0.0244\n",
      "      7        3.6009  0.0188\n",
      "      8        3.6009  0.0196\n",
      "      9        3.6009  0.0386\n",
      "     10        3.6009  0.0363\n",
      "Restoring best model from epoch 1.\n",
      "y_train breslow final [ 1.000e+00  2.000e+00 -4.000e+00  5.000e+00  5.000e+00  6.000e+00\n",
      " -8.000e+00  9.000e+00 -1.200e+01  1.200e+01 -1.200e+01  1.200e+01\n",
      " -1.300e+01 -1.300e+01  1.700e+01  2.400e+01 -2.800e+01 -3.000e+01\n",
      " -3.000e+01 -3.000e+01 -3.500e+01 -3.700e+01 -4.100e+01 -4.200e+01\n",
      "  4.700e+01  5.200e+01  5.300e+01 -5.500e+01 -6.100e+01 -6.100e+01\n",
      " -6.100e+01  6.100e+01 -7.000e+01  8.000e+01 -8.200e+01  8.400e+01\n",
      "  8.500e+01  8.800e+01 -8.900e+01  9.200e+01 -9.700e+01 -1.050e+02\n",
      " -1.060e+02 -1.060e+02 -1.110e+02  1.130e+02  1.160e+02 -1.210e+02\n",
      " -1.220e+02 -1.220e+02  1.230e+02  1.230e+02 -1.280e+02 -1.350e+02\n",
      "  1.360e+02  1.380e+02  1.430e+02  1.460e+02 -1.530e+02  1.530e+02\n",
      " -1.600e+02  1.610e+02  1.660e+02  1.660e+02  1.800e+02  1.950e+02\n",
      "  1.980e+02  1.980e+02 -2.020e+02 -2.070e+02 -2.110e+02  2.110e+02\n",
      "  2.120e+02  2.150e+02  2.230e+02 -2.270e+02 -2.360e+02  2.360e+02\n",
      " -2.380e+02 -2.420e+02 -2.440e+02 -2.470e+02  2.610e+02  2.660e+02\n",
      "  2.740e+02  2.760e+02  2.840e+02 -2.890e+02  2.910e+02  2.940e+02\n",
      " -2.960e+02  2.990e+02  3.020e+02  3.060e+02  3.070e+02  3.110e+02\n",
      "  3.150e+02 -3.240e+02  3.290e+02  3.400e+02  3.400e+02  3.450e+02\n",
      "  3.450e+02  3.510e+02 -3.530e+02  3.570e+02  3.580e+02  3.580e+02\n",
      "  3.650e+02 -3.660e+02 -3.670e+02  3.710e+02  3.720e+02 -3.750e+02\n",
      " -3.780e+02  3.830e+02  3.830e+02  3.870e+02 -3.920e+02 -3.960e+02\n",
      "  3.970e+02  3.990e+02 -4.010e+02  4.020e+02  4.030e+02 -4.050e+02\n",
      " -4.050e+02 -4.060e+02 -4.070e+02  4.120e+02 -4.200e+02 -4.230e+02\n",
      "  4.280e+02 -4.280e+02 -4.280e+02  4.290e+02 -4.320e+02 -4.400e+02\n",
      "  4.420e+02 -4.480e+02  4.480e+02  4.540e+02 -4.550e+02  4.550e+02\n",
      " -4.650e+02 -4.680e+02  4.740e+02 -4.810e+02  4.900e+02 -4.910e+02\n",
      " -4.920e+02 -4.930e+02  5.010e+02  5.060e+02 -5.100e+02 -5.100e+02\n",
      "  5.160e+02 -5.170e+02  5.190e+02 -5.310e+02  5.320e+02 -5.380e+02\n",
      "  5.390e+02  5.440e+02  5.500e+02  5.520e+02 -5.550e+02 -5.570e+02\n",
      "  5.590e+02 -5.590e+02 -5.650e+02  5.730e+02 -5.760e+02 -5.780e+02\n",
      " -5.790e+02  5.870e+02 -5.890e+02 -5.960e+02 -6.000e+02 -6.010e+02\n",
      "  6.010e+02 -6.080e+02 -6.150e+02 -6.160e+02 -6.180e+02  6.230e+02\n",
      " -6.370e+02 -6.390e+02 -6.400e+02 -6.420e+02  6.450e+02 -6.490e+02\n",
      " -6.520e+02 -6.530e+02 -6.600e+02 -6.660e+02  6.670e+02  6.690e+02\n",
      " -6.710e+02  6.780e+02 -6.830e+02  6.870e+02 -6.880e+02  6.920e+02\n",
      " -6.990e+02 -6.990e+02 -6.990e+02 -7.000e+02  7.080e+02  7.340e+02\n",
      " -7.340e+02 -7.410e+02 -7.570e+02 -7.580e+02 -7.590e+02 -7.620e+02\n",
      " -7.650e+02  7.700e+02  8.030e+02 -8.040e+02 -8.100e+02 -8.150e+02\n",
      " -8.180e+02 -8.220e+02  8.260e+02  8.260e+02 -8.260e+02  8.270e+02\n",
      " -8.330e+02 -8.420e+02 -8.620e+02 -8.670e+02  8.810e+02  8.990e+02\n",
      " -9.080e+02 -9.100e+02 -9.110e+02 -9.110e+02 -9.130e+02  9.160e+02\n",
      "  9.210e+02 -9.230e+02 -9.250e+02  9.270e+02 -9.320e+02  9.370e+02\n",
      " -9.370e+02 -9.520e+02 -9.600e+02  9.620e+02  9.650e+02  9.730e+02\n",
      " -9.790e+02 -9.830e+02 -9.900e+02 -9.950e+02  1.000e+03  1.006e+03\n",
      " -1.011e+03 -1.031e+03 -1.038e+03  1.045e+03 -1.050e+03 -1.052e+03\n",
      "  1.057e+03  1.058e+03 -1.058e+03 -1.063e+03  1.067e+03 -1.072e+03\n",
      "  1.075e+03 -1.092e+03  1.097e+03 -1.100e+03  1.107e+03 -1.111e+03\n",
      " -1.113e+03  1.114e+03  1.143e+03  1.150e+03  1.154e+03 -1.160e+03\n",
      "  1.161e+03 -1.180e+03 -1.182e+03  1.189e+03  1.190e+03 -1.212e+03\n",
      " -1.217e+03 -1.223e+03 -1.224e+03 -1.244e+03 -1.259e+03 -1.260e+03\n",
      " -1.268e+03 -1.297e+03 -1.311e+03  1.335e+03  1.344e+03  1.346e+03\n",
      " -1.361e+03 -1.386e+03  1.426e+03  1.470e+03 -1.475e+03  1.485e+03\n",
      " -1.492e+03 -1.499e+03 -1.505e+03 -1.519e+03 -1.541e+03 -1.602e+03\n",
      " -1.639e+03  1.640e+03 -1.645e+03  1.655e+03  1.656e+03  1.679e+03\n",
      " -1.682e+03  1.695e+03 -1.723e+03 -1.731e+03  1.736e+03 -1.743e+03\n",
      " -1.784e+03 -1.824e+03 -1.834e+03 -1.841e+03  1.841e+03 -1.845e+03\n",
      "  1.856e+03  1.874e+03 -1.884e+03  1.912e+03 -1.927e+03  1.953e+03\n",
      "  1.975e+03 -1.992e+03 -1.997e+03 -2.023e+03 -2.024e+03 -2.026e+03\n",
      " -2.073e+03 -2.080e+03  2.086e+03 -2.134e+03 -2.142e+03 -2.148e+03\n",
      "  2.160e+03 -2.165e+03 -2.167e+03  2.170e+03  2.224e+03 -2.227e+03\n",
      " -2.271e+03 -2.336e+03 -2.381e+03  2.409e+03 -2.419e+03 -2.447e+03\n",
      " -2.471e+03 -2.510e+03 -2.542e+03 -2.589e+03  2.625e+03  2.639e+03\n",
      " -2.645e+03  2.680e+03 -2.716e+03  2.803e+03 -2.811e+03  2.945e+03\n",
      "  2.979e+03 -3.016e+03 -3.108e+03 -3.123e+03  3.149e+03 -3.166e+03\n",
      " -3.189e+03  3.253e+03  3.376e+03 -3.576e+03  3.600e+03 -3.644e+03\n",
      " -3.724e+03 -3.747e+03  3.838e+03 -3.850e+03  3.924e+03 -4.026e+03\n",
      " -4.053e+03 -4.068e+03  4.601e+03 -4.694e+03  5.287e+03]\n",
      "Concordance Index 0.5030921628721541\n",
      "Integrated Brier Score: 0.18607907264050805\n",
      "y_train breslow final [ 1.000e+00  2.000e+00 -4.000e+00  5.000e+00  5.000e+00  6.000e+00\n",
      " -8.000e+00  9.000e+00 -1.200e+01  1.200e+01 -1.200e+01  1.200e+01\n",
      " -1.300e+01 -1.300e+01  1.700e+01  2.400e+01 -2.800e+01 -3.000e+01\n",
      " -3.000e+01 -3.000e+01 -3.500e+01 -3.700e+01 -4.100e+01 -4.200e+01\n",
      "  4.700e+01  5.200e+01  5.300e+01 -5.500e+01 -6.100e+01 -6.100e+01\n",
      " -6.100e+01  6.100e+01 -7.000e+01  8.000e+01 -8.200e+01  8.400e+01\n",
      "  8.500e+01  8.800e+01 -8.900e+01  9.200e+01 -9.700e+01 -1.050e+02\n",
      " -1.060e+02 -1.060e+02 -1.110e+02  1.130e+02  1.160e+02 -1.210e+02\n",
      " -1.220e+02 -1.220e+02  1.230e+02  1.230e+02 -1.280e+02 -1.350e+02\n",
      "  1.360e+02  1.380e+02  1.430e+02  1.460e+02 -1.530e+02  1.530e+02\n",
      " -1.600e+02  1.610e+02  1.660e+02  1.660e+02  1.800e+02  1.950e+02\n",
      "  1.980e+02  1.980e+02 -2.020e+02 -2.070e+02 -2.110e+02  2.110e+02\n",
      "  2.120e+02  2.150e+02  2.230e+02 -2.270e+02 -2.360e+02  2.360e+02\n",
      " -2.380e+02 -2.420e+02 -2.440e+02 -2.470e+02  2.610e+02  2.660e+02\n",
      "  2.740e+02  2.760e+02  2.840e+02 -2.890e+02  2.910e+02  2.940e+02\n",
      " -2.960e+02  2.990e+02  3.020e+02  3.060e+02  3.070e+02  3.110e+02\n",
      "  3.150e+02 -3.240e+02  3.290e+02  3.400e+02  3.400e+02  3.450e+02\n",
      "  3.450e+02  3.510e+02 -3.530e+02  3.570e+02  3.580e+02  3.580e+02\n",
      "  3.650e+02 -3.660e+02 -3.670e+02  3.710e+02  3.720e+02 -3.750e+02\n",
      " -3.780e+02  3.830e+02  3.830e+02  3.870e+02 -3.920e+02 -3.960e+02\n",
      "  3.970e+02  3.990e+02 -4.010e+02  4.020e+02  4.030e+02 -4.050e+02\n",
      " -4.050e+02 -4.060e+02 -4.070e+02  4.120e+02 -4.200e+02 -4.230e+02\n",
      "  4.280e+02 -4.280e+02 -4.280e+02  4.290e+02 -4.320e+02 -4.400e+02\n",
      "  4.420e+02 -4.480e+02  4.480e+02  4.540e+02 -4.550e+02  4.550e+02\n",
      " -4.650e+02 -4.680e+02  4.740e+02 -4.810e+02  4.900e+02 -4.910e+02\n",
      " -4.920e+02 -4.930e+02  5.010e+02  5.060e+02 -5.100e+02 -5.100e+02\n",
      "  5.160e+02 -5.170e+02  5.190e+02 -5.310e+02  5.320e+02 -5.380e+02\n",
      "  5.390e+02  5.440e+02  5.500e+02  5.520e+02 -5.550e+02 -5.570e+02\n",
      "  5.590e+02 -5.590e+02 -5.650e+02  5.730e+02 -5.760e+02 -5.780e+02\n",
      " -5.790e+02  5.870e+02 -5.890e+02 -5.960e+02 -6.000e+02 -6.010e+02\n",
      "  6.010e+02 -6.080e+02 -6.150e+02 -6.160e+02 -6.180e+02  6.230e+02\n",
      " -6.370e+02 -6.390e+02 -6.400e+02 -6.420e+02  6.450e+02 -6.490e+02\n",
      " -6.520e+02 -6.530e+02 -6.600e+02 -6.660e+02  6.670e+02  6.690e+02\n",
      " -6.710e+02  6.780e+02 -6.830e+02  6.870e+02 -6.880e+02  6.920e+02\n",
      " -6.990e+02 -6.990e+02 -6.990e+02 -7.000e+02  7.080e+02  7.340e+02\n",
      " -7.340e+02 -7.410e+02 -7.570e+02 -7.580e+02 -7.590e+02 -7.620e+02\n",
      " -7.650e+02  7.700e+02  8.030e+02 -8.040e+02 -8.100e+02 -8.150e+02\n",
      " -8.180e+02 -8.220e+02  8.260e+02  8.260e+02 -8.260e+02  8.270e+02\n",
      " -8.330e+02 -8.420e+02 -8.620e+02 -8.670e+02  8.810e+02  8.990e+02\n",
      " -9.080e+02 -9.100e+02 -9.110e+02 -9.110e+02 -9.130e+02  9.160e+02\n",
      "  9.210e+02 -9.230e+02 -9.250e+02  9.270e+02 -9.320e+02  9.370e+02\n",
      " -9.370e+02 -9.520e+02 -9.600e+02  9.620e+02  9.650e+02  9.730e+02\n",
      " -9.790e+02 -9.830e+02 -9.900e+02 -9.950e+02  1.000e+03  1.006e+03\n",
      " -1.011e+03 -1.031e+03 -1.038e+03  1.045e+03 -1.050e+03 -1.052e+03\n",
      "  1.057e+03  1.058e+03 -1.058e+03 -1.063e+03  1.067e+03 -1.072e+03\n",
      "  1.075e+03 -1.092e+03  1.097e+03 -1.100e+03  1.107e+03 -1.111e+03\n",
      " -1.113e+03  1.114e+03  1.143e+03  1.150e+03  1.154e+03 -1.160e+03\n",
      "  1.161e+03 -1.180e+03 -1.182e+03  1.189e+03  1.190e+03 -1.212e+03\n",
      " -1.217e+03 -1.223e+03 -1.224e+03 -1.244e+03 -1.259e+03 -1.260e+03\n",
      " -1.268e+03 -1.297e+03 -1.311e+03  1.335e+03  1.344e+03  1.346e+03\n",
      " -1.361e+03 -1.386e+03  1.426e+03  1.470e+03 -1.475e+03  1.485e+03\n",
      " -1.492e+03 -1.499e+03 -1.505e+03 -1.519e+03 -1.541e+03 -1.602e+03\n",
      " -1.639e+03  1.640e+03 -1.645e+03  1.655e+03  1.656e+03  1.679e+03\n",
      " -1.682e+03  1.695e+03 -1.723e+03 -1.731e+03  1.736e+03 -1.743e+03\n",
      " -1.784e+03 -1.824e+03 -1.834e+03 -1.841e+03  1.841e+03 -1.845e+03\n",
      "  1.856e+03  1.874e+03 -1.884e+03  1.912e+03 -1.927e+03  1.953e+03\n",
      "  1.975e+03 -1.992e+03 -1.997e+03 -2.023e+03 -2.024e+03 -2.026e+03\n",
      " -2.073e+03 -2.080e+03  2.086e+03 -2.134e+03 -2.142e+03 -2.148e+03\n",
      "  2.160e+03 -2.165e+03 -2.167e+03  2.170e+03  2.224e+03 -2.227e+03\n",
      " -2.271e+03 -2.336e+03 -2.381e+03  2.409e+03 -2.419e+03 -2.447e+03\n",
      " -2.471e+03 -2.510e+03 -2.542e+03 -2.589e+03  2.625e+03  2.639e+03\n",
      " -2.645e+03  2.680e+03 -2.716e+03  2.803e+03 -2.811e+03  2.945e+03\n",
      "  2.979e+03 -3.016e+03 -3.108e+03 -3.123e+03  3.149e+03 -3.166e+03\n",
      " -3.189e+03  3.253e+03  3.376e+03 -3.576e+03  3.600e+03 -3.644e+03\n",
      " -3.724e+03 -3.747e+03  3.838e+03 -3.850e+03  3.924e+03 -4.026e+03\n",
      " -4.053e+03 -4.068e+03  4.601e+03 -4.694e+03  5.287e+03]\n",
      "durations 3.0 4765.0\n",
      "Concordance Index 0.49829517778860205\n",
      "Integrated Brier Score: 0.1958543586987051\n",
      "(395, 20531) <class 'pandas.core.frame.DataFrame'>\n",
      "(395,) <class 'pandas.core.series.Series'>\n",
      "(98, 20531) <class 'pandas.core.frame.DataFrame'>\n",
      "(98,) <class 'pandas.core.series.Series'>\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.8123\u001b[0m  0.0590\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.0305\u001b[0m  0.0341\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.5279\u001b[0m  0.0522\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.5227\u001b[0m  0.0441\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.7364\u001b[0m  0.0654\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.5380\u001b[0m  0.0480\n",
      "      2        3.8123  0.0365\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.0349\u001b[0m  0.0336\n",
      "      2        3.0305  0.0365\n",
      "      2        3.5279  0.0375\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.5304\u001b[0m  0.0658\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.8084\u001b[0m  0.0675\n",
      "      2        3.5380  0.0408\n",
      "      2        3.7364  0.0546\n",
      "      2        3.5227  0.0567\n",
      "      2        3.0349  0.0381\n",
      "      3        3.0305  0.0466\n",
      "      3        3.8123  0.0523\n",
      "      2        3.5304  0.0375\n",
      "      2        3.8084  0.0399\n",
      "      3        3.0349  0.0387\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.7384\u001b[0m  0.0828\n",
      "      3        3.5279  0.1104\n",
      "      3        3.5304  0.0740\n",
      "      3        3.7364  0.0916\n",
      "      3        3.5227  0.0909\n",
      "      3        3.5380  0.1073\n",
      "      4        3.0305  0.0786\n",
      "      4        3.8123  0.0964\n",
      "      2        3.7384  0.0478\n",
      "      4        3.0349  0.0742\n",
      "      3        3.8084  0.0973\n",
      "      4        3.5279  0.0389\n",
      "      4        3.5380  0.0338\n",
      "      4        3.7364  0.0417\n",
      "      4        3.5227  0.0457\n",
      "      5        3.0305  0.0466\n",
      "      4        3.5304  0.0529\n",
      "      5        3.8123  0.0433\n",
      "      5        3.0349  0.0399\n",
      "      4        3.8084  0.0379\n",
      "      5        3.5279  0.0386\n",
      "      5        3.7364  0.0349\n",
      "      3        3.7384  0.0546\n",
      "      5        3.5227  0.0356\n",
      "      6        3.0305  0.0352\n",
      "      5        3.5304  0.0353\n",
      "      5        3.5380  0.0585\n",
      "      6        3.8123  0.0329\n",
      "      6        3.0349  0.0395\n",
      "      5        3.8084  0.0363\n",
      "      6        3.5279  0.0503\n",
      "      6        3.5227  0.0400\n",
      "      6        3.5304  0.0362\n",
      "      7        3.0305  0.0410\n",
      "      6        3.8084  0.0308\n",
      "      4        3.7384  0.0593\n",
      "      7        3.8123  0.0431\n",
      "      7        3.0349  0.0424\n",
      "      7        3.5227  0.0382\n",
      "      7        3.5279  0.0390\n",
      "      7        3.5304  0.0367\n",
      "      6        3.7364  0.0879\n",
      "      6        3.5380  0.0699\n",
      "      8        3.0305  0.0394\n",
      "      7        3.8084  0.0402\n",
      "      5        3.7384  0.0471\n",
      "      8        3.8123  0.0464\n",
      "      8        3.5279  0.0294\n",
      "      7        3.5380  0.0295\n",
      "      8        3.5304  0.0330\n",
      "      8        3.5227  0.0368\n",
      "      9        3.0305  0.0350\n",
      "      7        3.7364  0.0416\n",
      "      8        3.0349  0.0713\n",
      "      9        3.5279  0.0287\n",
      "      9        3.5304  0.0309\n",
      "      9        3.8123  0.0405\n",
      "      8        3.5380  0.0344\n",
      "      6        3.7384  0.0443\n",
      "      8        3.8084  0.0580\n",
      "      8        3.7364  0.0320\n",
      "     10        3.0305  0.0411\n",
      "      9        3.5227  0.0452\n",
      "Restoring best model from epoch 1.\n",
      "     10        3.5279  0.0317\n",
      "Restoring best model from epoch 1.\n",
      "      9        3.0349  0.0365\n",
      "      9        3.5380  0.0282\n",
      "     10        3.8123  0.0364\n",
      "Restoring best model from epoch 1.\n",
      "     10        3.5304  0.0404\n",
      "Restoring best model from epoch 1.\n",
      "     10        3.5227  0.0269\n",
      "Restoring best model from epoch 1.\n",
      "      9        3.8084  0.0386\n",
      "      9        3.7364  0.0368\n",
      "      7        3.7384  0.0473\n",
      "     10        3.5380  0.0269\n",
      "Restoring best model from epoch 1.\n",
      "     10        3.7364  0.0335\n",
      "Restoring best model from epoch 1.\n",
      "     10        3.8084  0.0384\n",
      "Restoring best model from epoch 1.\n",
      "      8        3.7384  0.0372\n",
      "     10        3.0349  0.0591\n",
      "Restoring best model from epoch 1.\n",
      "      9        3.7384  0.0403\n",
      "     10        3.7384  0.0304\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.0999\u001b[0m  0.0431\n",
      "      2        3.0999  0.0375\n",
      "      3        3.0999  0.0451\n",
      "      4        3.0999  0.0169\n",
      "      5        3.0999  0.0226\n",
      "      6        3.0999  0.0304\n",
      "      7        3.0999  0.0177\n",
      "      8        3.0999  0.0157\n",
      "      9        3.0999  0.0161\n",
      "     10        3.0999  0.0181\n",
      "Restoring best model from epoch 1.\n",
      "y_train breslow final [ 1.000e+00  3.000e+00 -4.000e+00  5.000e+00  5.000e+00 -8.000e+00\n",
      "  9.000e+00 -1.200e+01  1.200e+01 -1.300e+01 -1.300e+01 -1.500e+01\n",
      "  1.700e+01  2.300e+01 -2.800e+01 -3.000e+01 -3.100e+01 -3.100e+01\n",
      "  3.400e+01 -3.500e+01 -3.700e+01 -4.100e+01 -4.200e+01  4.700e+01\n",
      "  5.200e+01 -5.500e+01  5.900e+01 -6.000e+01 -6.100e+01 -6.100e+01\n",
      " -6.100e+01 -6.100e+01 -6.100e+01  6.100e+01 -7.000e+01 -8.200e+01\n",
      " -8.300e+01  8.400e+01  8.500e+01  8.800e+01 -8.900e+01  8.900e+01\n",
      "  9.400e+01 -1.030e+02 -1.050e+02 -1.060e+02 -1.060e+02  1.090e+02\n",
      " -1.110e+02 -1.110e+02  1.130e+02  1.160e+02 -1.220e+02 -1.220e+02\n",
      " -1.220e+02 -1.220e+02  1.230e+02 -1.280e+02  1.310e+02 -1.350e+02\n",
      "  1.360e+02  1.380e+02  1.430e+02  1.460e+02 -1.500e+02  1.510e+02\n",
      " -1.530e+02 -1.600e+02  1.610e+02  1.660e+02 -1.730e+02  1.800e+02\n",
      "  1.880e+02  1.980e+02  1.980e+02 -2.020e+02 -2.070e+02 -2.110e+02\n",
      "  2.110e+02  2.120e+02  2.150e+02  2.230e+02  2.360e+02 -2.380e+02\n",
      " -2.420e+02 -2.440e+02 -2.470e+02  2.610e+02  2.740e+02  2.760e+02\n",
      "  2.840e+02  2.940e+02 -2.960e+02  2.990e+02  3.020e+02  3.060e+02\n",
      "  3.110e+02  3.150e+02  3.220e+02 -3.240e+02  3.400e+02  3.400e+02\n",
      "  3.450e+02  3.510e+02  3.570e+02  3.580e+02 -3.580e+02 -3.580e+02\n",
      "  3.580e+02  3.600e+02  3.660e+02 -3.660e+02 -3.670e+02 -3.700e+02\n",
      "  3.710e+02  3.720e+02 -3.750e+02 -3.770e+02  3.830e+02  3.870e+02\n",
      " -3.920e+02 -3.960e+02  3.990e+02  4.020e+02  4.030e+02 -4.050e+02\n",
      " -4.050e+02 -4.060e+02 -4.070e+02  4.080e+02  4.120e+02  4.180e+02\n",
      " -4.200e+02 -4.230e+02  4.280e+02 -4.280e+02 -4.280e+02  4.290e+02\n",
      " -4.300e+02 -4.400e+02  4.420e+02  4.450e+02 -4.480e+02  4.480e+02\n",
      "  4.540e+02 -4.550e+02  4.550e+02  4.560e+02 -4.650e+02 -4.680e+02\n",
      "  4.740e+02 -4.820e+02  4.900e+02 -4.910e+02 -4.920e+02 -4.930e+02\n",
      " -4.980e+02  5.010e+02  5.060e+02 -5.100e+02 -5.110e+02  5.150e+02\n",
      "  5.160e+02 -5.170e+02  5.190e+02 -5.310e+02  5.320e+02 -5.380e+02\n",
      "  5.430e+02  5.440e+02  5.500e+02 -5.550e+02 -5.590e+02  5.620e+02\n",
      " -5.650e+02 -5.700e+02  5.730e+02 -5.760e+02 -5.780e+02 -5.790e+02\n",
      " -5.810e+02 -5.850e+02  5.870e+02 -5.890e+02 -5.960e+02 -6.000e+02\n",
      " -6.010e+02  6.010e+02  6.040e+02 -6.150e+02 -6.160e+02 -6.180e+02\n",
      " -6.300e+02  6.320e+02 -6.370e+02 -6.390e+02 -6.400e+02 -6.400e+02\n",
      " -6.410e+02 -6.420e+02  6.450e+02 -6.490e+02 -6.520e+02 -6.530e+02\n",
      " -6.600e+02  6.670e+02  6.690e+02 -6.710e+02  6.780e+02 -6.830e+02\n",
      "  6.840e+02 -6.880e+02  6.920e+02 -6.990e+02 -6.990e+02 -7.000e+02\n",
      "  7.160e+02 -7.170e+02  7.340e+02 -7.340e+02  7.400e+02 -7.410e+02\n",
      " -7.570e+02 -7.580e+02 -7.590e+02 -7.620e+02 -7.650e+02 -7.690e+02\n",
      "  7.700e+02 -7.900e+02  8.030e+02 -8.040e+02 -8.040e+02 -8.100e+02\n",
      " -8.150e+02 -8.160e+02 -8.180e+02  8.220e+02  8.260e+02  8.260e+02\n",
      " -8.260e+02  8.270e+02 -8.330e+02  8.350e+02  8.400e+02 -8.420e+02\n",
      " -8.490e+02 -8.620e+02  8.810e+02  8.990e+02 -9.100e+02 -9.100e+02\n",
      " -9.110e+02 -9.110e+02 -9.110e+02 -9.130e+02  9.210e+02 -9.230e+02\n",
      " -9.250e+02  9.270e+02 -9.320e+02  9.330e+02  9.370e+02 -9.420e+02\n",
      "  9.510e+02 -9.520e+02 -9.600e+02  9.620e+02  9.650e+02  9.730e+02\n",
      "  9.740e+02 -9.790e+02 -9.900e+02  1.001e+03 -1.007e+03 -1.011e+03\n",
      " -1.031e+03 -1.032e+03 -1.038e+03  1.045e+03 -1.050e+03 -1.052e+03\n",
      "  1.057e+03  1.058e+03 -1.058e+03 -1.063e+03  1.067e+03 -1.072e+03\n",
      "  1.075e+03 -1.092e+03  1.097e+03 -1.106e+03  1.107e+03  1.150e+03\n",
      "  1.154e+03 -1.160e+03  1.161e+03 -1.176e+03 -1.180e+03 -1.182e+03\n",
      "  1.189e+03 -1.196e+03 -1.223e+03 -1.224e+03 -1.244e+03 -1.259e+03\n",
      " -1.268e+03 -1.280e+03 -1.297e+03  1.315e+03  1.335e+03 -1.336e+03\n",
      "  1.338e+03  1.344e+03 -1.361e+03 -1.386e+03  1.423e+03  1.485e+03\n",
      " -1.492e+03 -1.499e+03 -1.505e+03 -1.531e+03 -1.602e+03 -1.639e+03\n",
      "  1.640e+03 -1.645e+03  1.655e+03  1.656e+03 -1.682e+03 -1.690e+03\n",
      "  1.713e+03 -1.731e+03  1.736e+03 -1.743e+03 -1.824e+03 -1.834e+03\n",
      " -1.841e+03  1.841e+03 -1.845e+03  1.856e+03  1.874e+03  1.912e+03\n",
      " -1.927e+03  1.933e+03 -1.963e+03  1.984e+03 -2.026e+03 -2.073e+03\n",
      " -2.080e+03  2.086e+03 -2.133e+03  2.133e+03 -2.134e+03 -2.142e+03\n",
      " -2.165e+03 -2.167e+03  2.170e+03  2.224e+03 -2.227e+03  2.284e+03\n",
      "  2.304e+03  2.378e+03 -2.381e+03  2.409e+03 -2.419e+03 -2.510e+03\n",
      " -2.542e+03 -2.589e+03  2.625e+03  2.639e+03 -2.645e+03  2.680e+03\n",
      " -2.716e+03  2.803e+03 -2.811e+03 -2.820e+03  2.945e+03 -3.108e+03\n",
      "  3.149e+03 -3.166e+03  3.376e+03 -3.387e+03 -3.576e+03  3.600e+03\n",
      " -3.636e+03 -3.644e+03 -3.724e+03  3.838e+03  3.924e+03 -4.026e+03\n",
      " -4.068e+03 -4.261e+03 -4.570e+03 -4.765e+03  5.287e+03]\n",
      "Concordance Index 0.5043741683417797\n",
      "Integrated Brier Score: 0.1826568387657423\n",
      "y_train breslow final [ 1.000e+00  3.000e+00 -4.000e+00  5.000e+00  5.000e+00 -8.000e+00\n",
      "  9.000e+00 -1.200e+01  1.200e+01 -1.300e+01 -1.300e+01 -1.500e+01\n",
      "  1.700e+01  2.300e+01 -2.800e+01 -3.000e+01 -3.100e+01 -3.100e+01\n",
      "  3.400e+01 -3.500e+01 -3.700e+01 -4.100e+01 -4.200e+01  4.700e+01\n",
      "  5.200e+01 -5.500e+01  5.900e+01 -6.000e+01 -6.100e+01 -6.100e+01\n",
      " -6.100e+01 -6.100e+01 -6.100e+01  6.100e+01 -7.000e+01 -8.200e+01\n",
      " -8.300e+01  8.400e+01  8.500e+01  8.800e+01 -8.900e+01  8.900e+01\n",
      "  9.400e+01 -1.030e+02 -1.050e+02 -1.060e+02 -1.060e+02  1.090e+02\n",
      " -1.110e+02 -1.110e+02  1.130e+02  1.160e+02 -1.220e+02 -1.220e+02\n",
      " -1.220e+02 -1.220e+02  1.230e+02 -1.280e+02  1.310e+02 -1.350e+02\n",
      "  1.360e+02  1.380e+02  1.430e+02  1.460e+02 -1.500e+02  1.510e+02\n",
      " -1.530e+02 -1.600e+02  1.610e+02  1.660e+02 -1.730e+02  1.800e+02\n",
      "  1.880e+02  1.980e+02  1.980e+02 -2.020e+02 -2.070e+02 -2.110e+02\n",
      "  2.110e+02  2.120e+02  2.150e+02  2.230e+02  2.360e+02 -2.380e+02\n",
      " -2.420e+02 -2.440e+02 -2.470e+02  2.610e+02  2.740e+02  2.760e+02\n",
      "  2.840e+02  2.940e+02 -2.960e+02  2.990e+02  3.020e+02  3.060e+02\n",
      "  3.110e+02  3.150e+02  3.220e+02 -3.240e+02  3.400e+02  3.400e+02\n",
      "  3.450e+02  3.510e+02  3.570e+02  3.580e+02 -3.580e+02 -3.580e+02\n",
      "  3.580e+02  3.600e+02  3.660e+02 -3.660e+02 -3.670e+02 -3.700e+02\n",
      "  3.710e+02  3.720e+02 -3.750e+02 -3.770e+02  3.830e+02  3.870e+02\n",
      " -3.920e+02 -3.960e+02  3.990e+02  4.020e+02  4.030e+02 -4.050e+02\n",
      " -4.050e+02 -4.060e+02 -4.070e+02  4.080e+02  4.120e+02  4.180e+02\n",
      " -4.200e+02 -4.230e+02  4.280e+02 -4.280e+02 -4.280e+02  4.290e+02\n",
      " -4.300e+02 -4.400e+02  4.420e+02  4.450e+02 -4.480e+02  4.480e+02\n",
      "  4.540e+02 -4.550e+02  4.550e+02  4.560e+02 -4.650e+02 -4.680e+02\n",
      "  4.740e+02 -4.820e+02  4.900e+02 -4.910e+02 -4.920e+02 -4.930e+02\n",
      " -4.980e+02  5.010e+02  5.060e+02 -5.100e+02 -5.110e+02  5.150e+02\n",
      "  5.160e+02 -5.170e+02  5.190e+02 -5.310e+02  5.320e+02 -5.380e+02\n",
      "  5.430e+02  5.440e+02  5.500e+02 -5.550e+02 -5.590e+02  5.620e+02\n",
      " -5.650e+02 -5.700e+02  5.730e+02 -5.760e+02 -5.780e+02 -5.790e+02\n",
      " -5.810e+02 -5.850e+02  5.870e+02 -5.890e+02 -5.960e+02 -6.000e+02\n",
      " -6.010e+02  6.010e+02  6.040e+02 -6.150e+02 -6.160e+02 -6.180e+02\n",
      " -6.300e+02  6.320e+02 -6.370e+02 -6.390e+02 -6.400e+02 -6.400e+02\n",
      " -6.410e+02 -6.420e+02  6.450e+02 -6.490e+02 -6.520e+02 -6.530e+02\n",
      " -6.600e+02  6.670e+02  6.690e+02 -6.710e+02  6.780e+02 -6.830e+02\n",
      "  6.840e+02 -6.880e+02  6.920e+02 -6.990e+02 -6.990e+02 -7.000e+02\n",
      "  7.160e+02 -7.170e+02  7.340e+02 -7.340e+02  7.400e+02 -7.410e+02\n",
      " -7.570e+02 -7.580e+02 -7.590e+02 -7.620e+02 -7.650e+02 -7.690e+02\n",
      "  7.700e+02 -7.900e+02  8.030e+02 -8.040e+02 -8.040e+02 -8.100e+02\n",
      " -8.150e+02 -8.160e+02 -8.180e+02  8.220e+02  8.260e+02  8.260e+02\n",
      " -8.260e+02  8.270e+02 -8.330e+02  8.350e+02  8.400e+02 -8.420e+02\n",
      " -8.490e+02 -8.620e+02  8.810e+02  8.990e+02 -9.100e+02 -9.100e+02\n",
      " -9.110e+02 -9.110e+02 -9.110e+02 -9.130e+02  9.210e+02 -9.230e+02\n",
      " -9.250e+02  9.270e+02 -9.320e+02  9.330e+02  9.370e+02 -9.420e+02\n",
      "  9.510e+02 -9.520e+02 -9.600e+02  9.620e+02  9.650e+02  9.730e+02\n",
      "  9.740e+02 -9.790e+02 -9.900e+02  1.001e+03 -1.007e+03 -1.011e+03\n",
      " -1.031e+03 -1.032e+03 -1.038e+03  1.045e+03 -1.050e+03 -1.052e+03\n",
      "  1.057e+03  1.058e+03 -1.058e+03 -1.063e+03  1.067e+03 -1.072e+03\n",
      "  1.075e+03 -1.092e+03  1.097e+03 -1.106e+03  1.107e+03  1.150e+03\n",
      "  1.154e+03 -1.160e+03  1.161e+03 -1.176e+03 -1.180e+03 -1.182e+03\n",
      "  1.189e+03 -1.196e+03 -1.223e+03 -1.224e+03 -1.244e+03 -1.259e+03\n",
      " -1.268e+03 -1.280e+03 -1.297e+03  1.315e+03  1.335e+03 -1.336e+03\n",
      "  1.338e+03  1.344e+03 -1.361e+03 -1.386e+03  1.423e+03  1.485e+03\n",
      " -1.492e+03 -1.499e+03 -1.505e+03 -1.531e+03 -1.602e+03 -1.639e+03\n",
      "  1.640e+03 -1.645e+03  1.655e+03  1.656e+03 -1.682e+03 -1.690e+03\n",
      "  1.713e+03 -1.731e+03  1.736e+03 -1.743e+03 -1.824e+03 -1.834e+03\n",
      " -1.841e+03  1.841e+03 -1.845e+03  1.856e+03  1.874e+03  1.912e+03\n",
      " -1.927e+03  1.933e+03 -1.963e+03  1.984e+03 -2.026e+03 -2.073e+03\n",
      " -2.080e+03  2.086e+03 -2.133e+03  2.133e+03 -2.134e+03 -2.142e+03\n",
      " -2.165e+03 -2.167e+03  2.170e+03  2.224e+03 -2.227e+03  2.284e+03\n",
      "  2.304e+03  2.378e+03 -2.381e+03  2.409e+03 -2.419e+03 -2.510e+03\n",
      " -2.542e+03 -2.589e+03  2.625e+03  2.639e+03 -2.645e+03  2.680e+03\n",
      " -2.716e+03  2.803e+03 -2.811e+03 -2.820e+03  2.945e+03 -3.108e+03\n",
      "  3.149e+03 -3.166e+03  3.376e+03 -3.387e+03 -3.576e+03  3.600e+03\n",
      " -3.636e+03 -3.644e+03 -3.724e+03  3.838e+03  3.924e+03 -4.026e+03\n",
      " -4.068e+03 -4.261e+03 -4.570e+03 -4.765e+03  5.287e+03]\n",
      "durations 2.0 4694.0\n",
      "Concordance Index 0.504462388440289\n",
      "Integrated Brier Score: 0.22523681058979408\n",
      "split gex_?|100133144      float32\n",
      "gex_?|100134869      float32\n",
      "gex_?|10357          float32\n",
      "gex_?|10431          float32\n",
      "gex_?|155060         float32\n",
      "                      ...   \n",
      "gex_ZYG11A|440590    float32\n",
      "gex_ZYG11B|79699     float32\n",
      "gex_ZYX|7791         float32\n",
      "gex_ZZEF1|23140      float32\n",
      "gex_ZZZ3|26009       float32\n",
      "Length: 19076, dtype: object\n",
      "(241, 19076) <class 'pandas.core.frame.DataFrame'>\n",
      "(241,) <class 'pandas.core.series.Series'>\n",
      "(61, 19076) <class 'pandas.core.frame.DataFrame'>\n",
      "(61,) <class 'pandas.core.series.Series'>\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m4.5191\u001b[0m  0.0249\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m4.4978\u001b[0m  0.0248\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2        4.4978  0.0277\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m4.5519\u001b[0m  0.0558\n",
      "      2        4.5191  0.0403\n",
      "      3        4.4978  0.0197\n",
      "      4        4.4978  0.0384\n",
      "      3        4.5191  0.0723\n",
      "      5        4.4978  0.0221\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m4.4889\u001b[0m  0.0298\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.9737\u001b[0m  0.0413\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      6        4.4978  0.0238\n",
      "      4        4.5191  0.0270\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      2        3.9737  0.0237\n",
      "      2        4.4889  0.0401\n",
      "      7        4.4978  0.0269\n",
      "      3        3.9737  0.0209\n",
      "      2        4.5519  0.1355\n",
      "      5        4.5191  0.0343\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      4        3.9737  0.0237\n",
      "      8        4.4978  0.0250\n",
      "      3        4.4889  0.0391\n",
      "      6        4.5191  0.0306\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      3        4.5519  0.1248\n",
      "      5        3.9737  0.1212\n",
      "      4        4.4889  0.1199\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m4.5641\u001b[0m  0.1590\n",
      "      7        4.5191  0.2728\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      9        4.4978  0.2947\n",
      "      4        4.5519  0.1976\n",
      "      8        4.5191  0.0223\n",
      "      2        4.5641  0.0381\n",
      "      5        4.5519  0.0286\n",
      "     10        4.4978  0.0384\n",
      "      9        4.5191  0.0241\n",
      "      5        4.4889  0.2054\n",
      "      6        3.9737  0.2088\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.5824\u001b[0m  0.0729\n",
      "     10        4.5191  0.0224\n",
      "Restoring best model from epoch 1.\n",
      "      6        4.5519  0.0287\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      3        4.5641  0.0533\n",
      "      6        4.4889  0.0322\n",
      "      7        3.9737  0.0318\n",
      "      2        3.5824  0.0197\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m4.5038\u001b[0m  0.0781\n",
      "Restoring best model from epoch 1.\n",
      "      7        4.5519  0.0257\n",
      "      4        4.5641  0.0268\n",
      "      3        3.5824  0.0276\n",
      "      7        4.4889  0.0329\n",
      "      8        4.5519  0.0200\n",
      "      8        3.9737  0.0374\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.9609\u001b[0m  0.0382\n",
      "      4        3.5824  0.0198\n",
      "      9        4.5519  0.0211\n",
      "      5        4.5641  0.0346\n",
      "      8        4.4889  0.0314\n",
      "      2        4.5038  0.0579\n",
      "      2        3.9609  0.0262\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      9        3.9737  0.0355\n",
      "      5        3.5824  0.0250\n",
      "     10        4.5519  0.0172\n",
      "      6        4.5641  0.0185\n",
      "      3        4.5038  0.0178\n",
      "      3        3.9609  0.0229\n",
      "      7        4.5641  0.0171\n",
      "      9        4.4889  0.0289\n",
      "      6        3.5824  0.0211\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "     10        3.9737  0.0274\n",
      "      4        4.5038  0.0173\n",
      "      8        4.5641  0.0174\n",
      "Restoring best model from epoch 1.\n",
      "      4        3.9609  0.0240\n",
      "      7        3.5824  0.0207\n",
      "      5        4.5038  0.0173\n",
      "     10        4.4889  0.0328\n",
      "      9        4.5641  0.0170\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.5739\u001b[0m  0.0227\n",
      "      8        3.5824  0.0185\n",
      "      6        4.5038  0.0176\n",
      "      5        3.9609  0.0248\n",
      "     10        4.5641  0.0179\n",
      "Restoring best model from epoch 1.Restoring best model from epoch 1.\n",
      "\n",
      "      9        3.5824  0.0206\n",
      "      2        3.5739  0.0309\n",
      "      7        4.5038  0.0278\n",
      "      6        3.9609  0.0261\n",
      "     10        3.5824  0.0177\n",
      "Restoring best model from epoch 1.\n",
      "      8        4.5038  0.0199\n",
      "      3        3.5739  0.0255\n",
      "      7        3.9609  0.0223\n",
      "      9        4.5038  0.0184\n",
      "      4        3.5739  0.0192\n",
      "      8        3.9609  0.0276\n",
      "     10        4.5038  0.0219\n",
      "Restoring best model from epoch 1.\n",
      "      5        3.5739  0.0301\n",
      "      9        3.9609  0.0277\n",
      "      6        3.5739  0.0207\n",
      "     10        3.9609  0.0196\n",
      "Restoring best model from epoch 1.\n",
      "      7        3.5739  0.0299\n",
      "      8        3.5739  0.0179\n",
      "      9        3.5739  0.0166\n",
      "     10        3.5739  0.0167\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m4.3925\u001b[0m  0.0102\n",
      "      2        4.3925  0.0085\n",
      "      3        4.3925  0.0105\n",
      "      4        4.3925  0.0083\n",
      "      5        4.3925  0.0100\n",
      "      6        4.3925  0.0280\n",
      "      7        4.3925  0.0116\n",
      "      8        4.3925  0.0136\n",
      "      9        4.3925  0.0346\n",
      "     10        4.3925  0.0122\n",
      "Restoring best model from epoch 1.\n",
      "y_train breslow final [    9.    11.   -16.    24.    24.    31.    36.    53.    61.   -61.\n",
      "    65.    74.    75.    90.   -92.    92.   -98.  -106.  -113.  -122.\n",
      "  -130.  -133.  -147.  -148.  -165.  -168.  -168.  -183.  -186.   186.\n",
      "  -190.  -192.  -194.   197.  -204.  -238.  -239.  -243.  -253.  -255.\n",
      "   260.  -260.  -268.  -277.   286.   304.  -343.   346.  -360.  -361.\n",
      "   365.  -379.   395.   396.  -420.  -441.  -442.  -454.   455.   457.\n",
      "   479.   493.   506.  -508.   518.   524.   528.  -529.   547.  -547.\n",
      "   555.   562.   563.   565.   568.  -571.  -576.  -581.   594.   608.\n",
      "   624.   627.   629.   637.  -643.  -655.   663.   676.   676.   679.\n",
      "  -686.   695.   731.   737.  -741.  -751.   760.  -761.  -763.  -783.\n",
      "   787.  -816.   820.  -837.   840.  -847.  -848.   857.   863.  -875.\n",
      "   883.  -885.   887.  -918.  -928.  -932.  -932.   949.  -967.   976.\n",
      "  1004.  1018.  1024.  1032.  1033.  1039.  1046. -1053.  1058.  1059.\n",
      "  1064.  1069.  1088.  1089.  1091.  1106.  1123. -1127.  1155.  1157.\n",
      "  1161.  1162.  1163.  1187.  1189. -1207.  1213.  1229.  1249.  1249.\n",
      "  1259.  1278.  1279.  1321.  1324.  1329.  1341. -1342.  1348.  1354.\n",
      "  1354. -1357.  1364. -1364.  1366. -1372.  1384. -1386.  1446.  1448.\n",
      "  1451.  1484.  1484.  1492.  1516.  1562. -1573.  1579.  1583.  1620.\n",
      "  1646. -1684.  1688.  1699.  1720.  1721. -1722.  1736.  1746. -1752.\n",
      "  1757.  1767.  1769. -1785.  1799.  1815.  1875.  1891. -1900. -1919.\n",
      "  1955.  1977. -1977. -1993.  1993. -1998.  2009.  2012.  2012. -2032.\n",
      " -2078.  2089. -2143.  2148. -2182.  2182.  2218.  2342.  2345. -2369.\n",
      "  2400.  2467. -2535. -2614.  2621.  2634.  2648. -2661. -2661.  2692.\n",
      "  2717. -3096.  3224. -3325.  3337. -3525. -3532. -3825. -3871.  4624.\n",
      " -5481.]\n",
      "Concordance Index 0.49400523400888563\n",
      "Integrated Brier Score: 0.12636875036328224\n",
      "y_train breslow final [    9.    11.   -16.    24.    24.    31.    36.    53.    61.   -61.\n",
      "    65.    74.    75.    90.   -92.    92.   -98.  -106.  -113.  -122.\n",
      "  -130.  -133.  -147.  -148.  -165.  -168.  -168.  -183.  -186.   186.\n",
      "  -190.  -192.  -194.   197.  -204.  -238.  -239.  -243.  -253.  -255.\n",
      "   260.  -260.  -268.  -277.   286.   304.  -343.   346.  -360.  -361.\n",
      "   365.  -379.   395.   396.  -420.  -441.  -442.  -454.   455.   457.\n",
      "   479.   493.   506.  -508.   518.   524.   528.  -529.   547.  -547.\n",
      "   555.   562.   563.   565.   568.  -571.  -576.  -581.   594.   608.\n",
      "   624.   627.   629.   637.  -643.  -655.   663.   676.   676.   679.\n",
      "  -686.   695.   731.   737.  -741.  -751.   760.  -761.  -763.  -783.\n",
      "   787.  -816.   820.  -837.   840.  -847.  -848.   857.   863.  -875.\n",
      "   883.  -885.   887.  -918.  -928.  -932.  -932.   949.  -967.   976.\n",
      "  1004.  1018.  1024.  1032.  1033.  1039.  1046. -1053.  1058.  1059.\n",
      "  1064.  1069.  1088.  1089.  1091.  1106.  1123. -1127.  1155.  1157.\n",
      "  1161.  1162.  1163.  1187.  1189. -1207.  1213.  1229.  1249.  1249.\n",
      "  1259.  1278.  1279.  1321.  1324.  1329.  1341. -1342.  1348.  1354.\n",
      "  1354. -1357.  1364. -1364.  1366. -1372.  1384. -1386.  1446.  1448.\n",
      "  1451.  1484.  1484.  1492.  1516.  1562. -1573.  1579.  1583.  1620.\n",
      "  1646. -1684.  1688.  1699.  1720.  1721. -1722.  1736.  1746. -1752.\n",
      "  1757.  1767.  1769. -1785.  1799.  1815.  1875.  1891. -1900. -1919.\n",
      "  1955.  1977. -1977. -1993.  1993. -1998.  2009.  2012.  2012. -2032.\n",
      " -2078.  2089. -2143.  2148. -2182.  2182.  2218.  2342.  2345. -2369.\n",
      "  2400.  2467. -2535. -2614.  2621.  2634.  2648. -2661. -2661.  2692.\n",
      "  2717. -3096.  3224. -3325.  3337. -3525. -3532. -3825. -3871.  4624.\n",
      " -5481.]\n",
      "durations 8.0 4424.0\n",
      "Concordance Index 0.4898941289701636\n",
      "Integrated Brier Score: 0.13346037193196023\n",
      "(241, 19076) <class 'pandas.core.frame.DataFrame'>\n",
      "(241,) <class 'pandas.core.series.Series'>\n",
      "(61, 19076) <class 'pandas.core.frame.DataFrame'>\n",
      "(61,) <class 'pandas.core.series.Series'>\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m4.4597\u001b[0m        \u001b[32m4.7621\u001b[0m  0.1043\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      2        4.4693        4.7663  0.1257\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m4.4326\u001b[0m        \u001b[32m4.3481\u001b[0m  0.1689\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m4.3594\u001b[0m  0.0146\n",
      "      2        4.3594  0.0169\n",
      "      3        4.3594  0.0111\n",
      "      4        4.3594  0.0149\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m4.1735\u001b[0m        \u001b[32m4.4234\u001b[0m  0.2720\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m5.0203\u001b[0m        \u001b[32m4.7151\u001b[0m  0.2876\n",
      "      5        4.3594  0.0122\n",
      "      3        4.4840        4.7674  0.1646\n",
      "      6        4.3594  0.0103\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m4.5298\u001b[0m  0.0422\n",
      "      7        4.3594  0.0108\n",
      "      2        4.5298  0.0132\n",
      "      8        4.3594  0.0143\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m4.7304\u001b[0m  0.0225\n",
      "      3        4.5298  0.0191\n",
      "      9        4.3594  0.0133\n",
      "      2        \u001b[36m4.4179\u001b[0m        4.3488  0.1566\n",
      "      2        4.7304  0.0155\n",
      "      4        4.5298  0.0156\n",
      "     10        4.3594  0.0181\n",
      "Restoring best model from epoch 1.\n",
      "      3        4.7304  0.0139\n",
      "      5        4.5298  0.0199\n",
      "      2        4.1859        4.4263  0.0991\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      4        4.7304  0.0293\n",
      "      6        4.5298  0.0195\n",
      "      4        4.4702        4.7649  0.0975\n",
      "      2        5.0585        \u001b[32m4.7117\u001b[0m  0.1081\n",
      "      5        4.7304  0.0189\n",
      "      7        4.5298  0.0276\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m4.4307\u001b[0m  0.0589\n",
      "      6        4.7304  0.0152\n",
      "      8        4.5298  0.0168\n",
      "      2        4.4307  0.0193\n",
      "      7        4.7304  0.0209\n",
      "      9        4.5298  0.0116\n",
      "      3        4.4307  0.0128\n",
      "      8        4.7304  0.0130\n",
      "     10        4.5298  0.0160\n",
      "Restoring best model from epoch 1.\n",
      "      4        4.4307  0.0154\n",
      "      9        4.7304  0.0169\n",
      "      3        4.4524        4.3484  0.1335\n",
      "      5        4.4307  0.0155\n",
      "      3        4.1932        4.4289  0.1118\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m4.7666\u001b[0m  0.0406\n",
      "     10        4.7304  0.0164\n",
      "Restoring best model from epoch 1.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m4.3488\u001b[0m        \u001b[32m4.5301\u001b[0m  0.4511\n",
      "      5        4.4639        4.7631  0.1079\n",
      "      6        4.4307  0.0136\n",
      "      2        4.7666  0.0148\n",
      "      7        4.4307  0.0183\n",
      "      3        4.7666  0.0142\n",
      "      3        5.0506        4.7135  0.1312\n",
      "      8        4.4307  0.0162\n",
      "      4        4.7666  0.0164\n",
      "      9        4.4307  0.0251\n",
      "      5        4.7666  0.0242\n",
      "     10        4.4307  0.0136\n",
      "Restoring best model from epoch 1.\n",
      "      4        4.4472        4.3492  0.1050\n",
      "      6        4.7666  0.0191\n",
      "      7        4.7666  0.0130\n",
      "      6        4.4668        \u001b[32m4.7606\u001b[0m  0.1053\n",
      "      8        4.7666  0.0136\n",
      "      2        \u001b[36m4.3385\u001b[0m        4.5310  0.1231\n",
      "      9        4.7666  0.0135\n",
      "      4        4.1818        4.4300  0.1354\n",
      "     10        4.7666  0.0159\n",
      "Restoring best model from epoch 1.\n",
      "      4        5.0390        4.7124  0.1253\n",
      "      5        4.4505        \u001b[32m4.3476\u001b[0m  0.0933\n",
      "      7        4.4755        \u001b[32m4.7589\u001b[0m  0.0932\n",
      "      5        \u001b[36m4.1519\u001b[0m        4.4305  0.0881\n",
      "      3        4.3771        4.5320  0.0997\n",
      "      5        5.0252        4.7120  0.0879\n",
      "      6        4.4444        \u001b[32m4.3469\u001b[0m  0.0840\n",
      "      8        \u001b[36m4.4591\u001b[0m        \u001b[32m4.7573\u001b[0m  0.0834\n",
      "      6        4.1915        4.4294  0.0885\n",
      "      4        4.3635        4.5320  0.0854\n",
      "      6        5.0365        4.7123  0.0816\n",
      "      7        \u001b[36m4.4132\u001b[0m        4.3472  0.0767\n",
      "      9        4.4639        \u001b[32m4.7558\u001b[0m  0.0795\n",
      "      7        4.1848        4.4294  0.0847\n",
      "      5        4.3647        4.5317  0.0831\n",
      "      7        5.0626        \u001b[32m4.7113\u001b[0m  0.0795\n",
      "      8        4.4430        4.3469  0.0934\n",
      "     10        4.4849        \u001b[32m4.7512\u001b[0m  0.0776\n",
      "      8        4.1691        4.4292  0.0777\n",
      "      6        4.3398        4.5307  0.0759\n",
      "      8        5.0514        4.7115  0.0755\n",
      "      9        4.4138        4.3481  0.0769\n",
      "      7        \u001b[36m4.3282\u001b[0m        \u001b[32m4.5299\u001b[0m  0.0780\n",
      "      9        4.1621        4.4297  0.0801\n",
      "      9        5.0213        \u001b[32m4.7112\u001b[0m  0.0909\n",
      "     10        4.4517        4.3480  0.0791\n",
      "Restoring best model from epoch 6.\n",
      "     10        4.1809        4.4299  0.0770\n",
      "Restoring best model from epoch 1.\n",
      "      8        4.3460        4.5319  0.0797\n",
      "     10        5.0651        \u001b[32m4.7104\u001b[0m  0.0718\n",
      "      9        4.3503        4.5316  0.0686\n",
      "     10        4.3679        4.5318  0.0668\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m4.7042\u001b[0m        \u001b[32m4.2103\u001b[0m  0.0743\n",
      "      2        \u001b[36m4.6693\u001b[0m        \u001b[32m4.2077\u001b[0m  0.0640\n",
      "      3        4.6894        \u001b[32m4.2030\u001b[0m  0.0594\n",
      "      4        4.6935        4.2038  0.0587\n",
      "      5        \u001b[36m4.6604\u001b[0m        4.2040  0.0558\n",
      "      6        4.6900        4.2049  0.0617\n",
      "      7        \u001b[36m4.6495\u001b[0m        4.2055  0.0569\n",
      "      8        4.6865        4.2068  0.0597\n",
      "      9        4.6580        4.2091  0.0813\n",
      "     10        4.6512        4.2093  0.0549\n",
      "Restoring best model from epoch 3.\n",
      "y_train breslow final [    8.    11.    24.   -24.    24.    25.    31.    36.    53.    61.\n",
      "    65.   -67.    75.   -81.    91.   -98.  -106.  -113.  -130.   138.\n",
      "  -145.  -147.  -148.  -163.  -165.  -168.  -176.  -176.  -181.  -183.\n",
      "  -186.   186.  -190.  -192.  -194.  -220.  -238.  -239.  -243.  -253.\n",
      "  -255.   260.  -260.  -268.   286.   304.   304.   336.  -338.  -343.\n",
      "   346.  -360.  -361.   394.   395.   396.  -420.  -441.   455.   479.\n",
      "   493.   515.   518.   531.   542.  -547.  -547.   555.   562.   563.\n",
      "   565.   568.  -581.   608.   624.   627.   629.   637.   637.  -643.\n",
      "  -655.   676.   676.   679.  -684.  -686.   695.   728.   731.   737.\n",
      "   760.  -763.  -772.  -783.   787.  -816.   820.   820.   821.  -837.\n",
      "   840.  -847.  -848.   857.   863.   868.   883.  -885.   887.   914.\n",
      "  -915.  -918.   919.  -928.  -932.  -932.   949.   951.  -952.  -956.\n",
      "   962.  -967.   976.  1004.  1018.  1024.  1024.  1032.  1033.  1035.\n",
      "  1039. -1053.  1058.  1069.  1082.  1089.  1091.  1102.  1104.  1106.\n",
      "  1123. -1127.  1155.  1157.  1162.  1163.  1187.  1189. -1212.  1213.\n",
      "  1249.  1249.  1259.  1319.  1324.  1329. -1342.  1348.  1354.  1354.\n",
      " -1357.  1364. -1364.  1369. -1372.  1384. -1386.  1399.  1446.  1451.\n",
      "  1470.  1483.  1484.  1484.  1492.  1516.  1562. -1573.  1579.  1579.\n",
      "  1583.  1620.  1646.  1650. -1684.  1699.  1720.  1721. -1722.  1736.\n",
      " -1752.  1767. -1785.  1799. -1844. -1882.  1891. -1900. -1919.  1947.\n",
      "  1977. -1977. -1993.  1993. -1998.  2012.  2012. -2032.  2049. -2078.\n",
      " -2143.  2148. -2182.  2182.  2218. -2329. -2338.  2342.  2345.  2400.\n",
      " -2464.  2467. -2535. -2614.  2621.  2634.  2648. -2661. -2661.  2688.\n",
      "  2717.  2742. -3096.  3224. -3325.  3337. -3525. -3825. -3871. -4424.\n",
      "  4624.]\n",
      "Concordance Index 0.5276354973198333\n",
      "Integrated Brier Score: 0.14172934865463246\n",
      "y_train breslow final [    8.    11.    24.   -24.    24.    25.    31.    36.    53.    61.\n",
      "    65.   -67.    75.   -81.    91.   -98.  -106.  -113.  -130.   138.\n",
      "  -145.  -147.  -148.  -163.  -165.  -168.  -176.  -176.  -181.  -183.\n",
      "  -186.   186.  -190.  -192.  -194.  -220.  -238.  -239.  -243.  -253.\n",
      "  -255.   260.  -260.  -268.   286.   304.   304.   336.  -338.  -343.\n",
      "   346.  -360.  -361.   394.   395.   396.  -420.  -441.   455.   479.\n",
      "   493.   515.   518.   531.   542.  -547.  -547.   555.   562.   563.\n",
      "   565.   568.  -581.   608.   624.   627.   629.   637.   637.  -643.\n",
      "  -655.   676.   676.   679.  -684.  -686.   695.   728.   731.   737.\n",
      "   760.  -763.  -772.  -783.   787.  -816.   820.   820.   821.  -837.\n",
      "   840.  -847.  -848.   857.   863.   868.   883.  -885.   887.   914.\n",
      "  -915.  -918.   919.  -928.  -932.  -932.   949.   951.  -952.  -956.\n",
      "   962.  -967.   976.  1004.  1018.  1024.  1024.  1032.  1033.  1035.\n",
      "  1039. -1053.  1058.  1069.  1082.  1089.  1091.  1102.  1104.  1106.\n",
      "  1123. -1127.  1155.  1157.  1162.  1163.  1187.  1189. -1212.  1213.\n",
      "  1249.  1249.  1259.  1319.  1324.  1329. -1342.  1348.  1354.  1354.\n",
      " -1357.  1364. -1364.  1369. -1372.  1384. -1386.  1399.  1446.  1451.\n",
      "  1470.  1483.  1484.  1484.  1492.  1516.  1562. -1573.  1579.  1579.\n",
      "  1583.  1620.  1646.  1650. -1684.  1699.  1720.  1721. -1722.  1736.\n",
      " -1752.  1767. -1785.  1799. -1844. -1882.  1891. -1900. -1919.  1947.\n",
      "  1977. -1977. -1993.  1993. -1998.  2012.  2012. -2032.  2049. -2078.\n",
      " -2143.  2148. -2182.  2182.  2218. -2329. -2338.  2342.  2345.  2400.\n",
      " -2464.  2467. -2535. -2614.  2621.  2634.  2648. -2661. -2661.  2688.\n",
      "  2717.  2742. -3096.  3224. -3325.  3337. -3525. -3825. -3871. -4424.\n",
      "  4624.]\n",
      "durations 9.0 5481.0\n",
      "Concordance Index 0.5402298850574713\n",
      "Integrated Brier Score: 0.11865858982174901\n",
      "(242, 19076) <class 'pandas.core.frame.DataFrame'>\n",
      "(242,) <class 'pandas.core.series.Series'>\n",
      "(60, 19076) <class 'pandas.core.frame.DataFrame'>\n",
      "(60,) <class 'pandas.core.series.Series'>\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m4.2646\u001b[0m  0.0187\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.8725\u001b[0m  0.0170\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      2        4.2646  0.0163\n",
      "      2        3.8725  0.0127\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m4.6533\u001b[0m  0.0140\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      3        4.2646  0.0144\n",
      "      2        4.6533  0.0185\n",
      "      3        3.8725  0.0203\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m4.3636\u001b[0m        \u001b[32m4.6609\u001b[0m  0.1183\n",
      "      4        4.2646  0.0125\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      4        3.8725  0.0177\n",
      "      3        4.6533  0.0256\n",
      "      5        4.2646  0.0198\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m4.6194\u001b[0m        \u001b[32m4.2639\u001b[0m  0.1284\n",
      "      5        3.8725  0.0138\n",
      "      6        4.2646  0.0134\n",
      "      4        4.6533  0.0138\n",
      "      6        3.8725  0.0121\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m4.1502\u001b[0m        \u001b[32m3.8832\u001b[0m  0.1262\n",
      "      7        4.2646  0.0134\n",
      "      7        3.8725  0.0122\n",
      "      5        4.6533  0.0204\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m4.4117\u001b[0m  0.0224\n",
      "      8        4.2646  0.0108\n",
      "      8        3.8725  0.0125\n",
      "      6        4.6533  0.0110\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2        4.4117  0.0125\n",
      "      9        4.2646  0.0168\n",
      "      9        3.8725  0.0143\n",
      "      7        4.6533  0.0211\n",
      "      3        4.4117  0.0174\n",
      "     10        4.2646  0.0144\n",
      "Restoring best model from epoch 1.\n",
      "     10        3.8725  0.0160\n",
      "Restoring best model from epoch 1.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m4.3760\u001b[0m        \u001b[32m4.4256\u001b[0m  0.1636\n",
      "      4        4.4117  0.0123\n",
      "      8        4.6533  0.0137\n",
      "      9        4.6533  0.0132\n",
      "      5        4.4117  0.0242\n",
      "     10        4.6533  0.0133\n",
      "Restoring best model from epoch 1.\n",
      "      6        4.4117  0.0102\n",
      "      2        4.3897        \u001b[32m4.6589\u001b[0m  0.1456\n",
      "      7        4.4117  0.0228\n",
      "      2        \u001b[36m4.6125\u001b[0m        \u001b[32m4.2627\u001b[0m  0.1410\n",
      "      2        \u001b[36m4.1214\u001b[0m        3.8836  0.1260\n",
      "      8        4.4117  0.0122\n",
      "      9        4.4117  0.0131\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "     10        4.4117  0.0132\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m4.8516\u001b[0m  0.0176\n",
      "      2        \u001b[36m4.3673\u001b[0m        4.4264  0.1186\n",
      "      2        4.8516  0.0192\n",
      "      3        4.8516  0.0123\n",
      "      3        \u001b[36m4.3584\u001b[0m        \u001b[32m4.6582\u001b[0m  0.1057\n",
      "      4        4.8516  0.0115\n",
      "      5        4.8516  0.0125\n",
      "      3        \u001b[36m4.5874\u001b[0m        \u001b[32m4.2616\u001b[0m  0.1069\n",
      "      3        4.1293        \u001b[32m3.8816\u001b[0m  0.1074\n",
      "      6        4.8516  0.0119\n",
      "      7        4.8516  0.0123\n",
      "      3        4.3968        \u001b[32m4.4255\u001b[0m  0.0824\n",
      "      8        4.8516  0.0122\n",
      "      9        4.8516  0.0147\n",
      "     10        4.8516  0.0113\n",
      "Restoring best model from epoch 1.\n",
      "      4        \u001b[36m4.3475\u001b[0m        \u001b[32m4.6555\u001b[0m  0.0945\n",
      "      4        4.6230        \u001b[32m4.2599\u001b[0m  0.0941\n",
      "      4        \u001b[36m4.0922\u001b[0m        \u001b[32m3.8779\u001b[0m  0.1070\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m5.2079\u001b[0m        \u001b[32m4.8398\u001b[0m  0.1848\n",
      "      5        4.3841        \u001b[32m4.6529\u001b[0m  0.0797\n",
      "      4        4.3752        \u001b[32m4.4254\u001b[0m  0.1337\n",
      "      5        \u001b[36m4.5847\u001b[0m        \u001b[32m4.2586\u001b[0m  0.0839\n",
      "      5        4.1469        \u001b[32m3.8765\u001b[0m  0.0788\n",
      "      2        \u001b[36m5.1624\u001b[0m        4.8419  0.0800\n",
      "      6        4.3531        \u001b[32m4.6527\u001b[0m  0.0811\n",
      "      5        4.3927        4.4259  0.0956\n",
      "      6        4.1225        3.8777  0.0777\n",
      "      6        \u001b[36m4.5833\u001b[0m        \u001b[32m4.2576\u001b[0m  0.0913\n",
      "      3        5.1670        4.8401  0.0777\n",
      "      7        \u001b[36m4.3416\u001b[0m        \u001b[32m4.6526\u001b[0m  0.0815\n",
      "      6        \u001b[36m4.3641\u001b[0m        \u001b[32m4.4248\u001b[0m  0.0746\n",
      "      7        4.1234        3.8767  0.0808\n",
      "      7        \u001b[36m4.5772\u001b[0m        \u001b[32m4.2570\u001b[0m  0.0764\n",
      "      4        \u001b[36m5.1434\u001b[0m        \u001b[32m4.8398\u001b[0m  0.0773\n",
      "      8        \u001b[36m4.3192\u001b[0m        \u001b[32m4.6525\u001b[0m  0.0847\n",
      "      7        4.3730        \u001b[32m4.4244\u001b[0m  0.0920\n",
      "      8        4.6055        \u001b[32m4.2567\u001b[0m  0.0836\n",
      "      8        \u001b[36m4.0891\u001b[0m        \u001b[32m3.8762\u001b[0m  0.0855\n",
      "      5        5.1439        \u001b[32m4.8382\u001b[0m  0.0897\n",
      "      8        \u001b[36m4.3488\u001b[0m        \u001b[32m4.4237\u001b[0m  0.0821\n",
      "      9        \u001b[36m4.5677\u001b[0m        \u001b[32m4.2562\u001b[0m  0.0775\n",
      "      9        4.3547        \u001b[32m4.6519\u001b[0m  0.1164\n",
      "      6        5.1682        \u001b[32m4.8374\u001b[0m  0.0783\n",
      "      9        4.1151        \u001b[32m3.8759\u001b[0m  0.0946\n",
      "      9        4.3503        \u001b[32m4.4236\u001b[0m  0.0934\n",
      "     10        4.3434        4.6520  0.0796\n",
      "Restoring best model from epoch 9.\n",
      "     10        \u001b[36m4.5522\u001b[0m        \u001b[32m4.2555\u001b[0m  0.0899\n",
      "     10        4.1324        \u001b[32m3.8753\u001b[0m  0.0813\n",
      "      7        5.1599        \u001b[32m4.8364\u001b[0m  0.0955\n",
      "     10        4.3666        \u001b[32m4.4232\u001b[0m  0.0745\n",
      "      8        5.1641        \u001b[32m4.8356\u001b[0m  0.0712\n",
      "      9        5.1723        \u001b[32m4.8347\u001b[0m  0.0757\n",
      "     10        \u001b[36m5.1184\u001b[0m        \u001b[32m4.8342\u001b[0m  0.0705\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m4.4493\u001b[0m        \u001b[32m4.8501\u001b[0m  0.0879\n",
      "      2        \u001b[36m4.4454\u001b[0m        \u001b[32m4.8469\u001b[0m  0.0842\n",
      "      3        \u001b[36m4.4261\u001b[0m        4.8474  0.0645\n",
      "      4        4.4284        \u001b[32m4.8461\u001b[0m  0.0606\n",
      "      5        4.4344        \u001b[32m4.8425\u001b[0m  0.0697\n",
      "      6        \u001b[36m4.4167\u001b[0m        \u001b[32m4.8401\u001b[0m  0.0997\n",
      "      7        4.4373        \u001b[32m4.8395\u001b[0m  0.1089\n",
      "      8        4.4252        \u001b[32m4.8392\u001b[0m  0.0838\n",
      "      9        4.4411        4.8401  0.1093\n",
      "     10        4.4208        4.8403  0.0779\n",
      "Restoring best model from epoch 7.\n",
      "y_train breslow final [    8.     9.    11.   -16.    24.   -24.    24.    25.    31.    31.\n",
      "    36.   -61.    65.   -67.    74.    75.   -81.    90.    91.   -92.\n",
      "    92.   -98.  -113.  -122.  -130.  -133.   138.  -145.  -148.  -163.\n",
      "  -168.  -168.  -176.  -176.  -181.  -183.  -186.  -192.   197.  -204.\n",
      "  -220.  -238.  -243.  -253.   260.  -260.  -260.  -277.   286.   304.\n",
      "   304.   336.  -338.  -343.   346.  -360.  -361.   365.  -379.   394.\n",
      "   395.   396.  -420.  -442.  -454.   457.   479.   493.   506.  -508.\n",
      "   515.   518.   524.   528.  -529.   531.   542.   547.  -547.   555.\n",
      "   563.  -571.  -576.  -581.   594.   608.   629.   637.   637.  -643.\n",
      "  -655.   663.   676.   676.  -684.  -686.   695.   728.   731.   737.\n",
      "  -741.  -751.   760.  -761.  -763.  -772.  -783.   820.   820.   821.\n",
      "  -837.  -848.   857.   863.   868.  -875.   883.  -885.   887.   914.\n",
      "  -915.  -918.   919.  -928.  -932.  -932.   951.  -952.  -956.   962.\n",
      "  -967.  1004.  1024.  1024.  1032.  1033.  1035.  1039.  1046.  1059.\n",
      "  1064.  1069.  1082.  1088.  1089.  1091.  1102.  1104.  1106. -1127.\n",
      "  1161.  1162.  1189. -1207. -1212.  1213.  1229.  1249.  1259.  1278.\n",
      "  1279.  1319.  1321.  1324.  1329.  1341. -1342.  1348.  1354.  1354.\n",
      " -1357.  1366.  1369.  1384. -1386.  1399.  1446.  1448.  1470.  1483.\n",
      "  1516.  1562.  1579.  1583.  1583.  1620.  1646.  1650. -1684.  1688.\n",
      "  1699.  1720.  1721. -1722.  1736.  1746.  1757.  1769. -1785.  1799.\n",
      "  1815. -1844.  1875. -1882. -1900.  1947.  1955.  1977. -1977. -1993.\n",
      "  1993. -1998.  2009.  2012.  2012. -2032.  2049. -2078.  2089.  2148.\n",
      " -2182.  2182.  2218. -2329. -2338. -2369.  2400. -2464. -2535.  2634.\n",
      "  2648.  2688.  2692.  2742. -3096. -3325.  3337. -3525. -3532. -3871.\n",
      " -4424. -5481.]\n",
      "Concordance Index 0.5344492964580301\n",
      "Integrated Brier Score: 0.12595188060794044\n",
      "y_train breslow final [    8.     9.    11.   -16.    24.   -24.    24.    25.    31.    31.\n",
      "    36.   -61.    65.   -67.    74.    75.   -81.    90.    91.   -92.\n",
      "    92.   -98.  -113.  -122.  -130.  -133.   138.  -145.  -148.  -163.\n",
      "  -168.  -168.  -176.  -176.  -181.  -183.  -186.  -192.   197.  -204.\n",
      "  -220.  -238.  -243.  -253.   260.  -260.  -260.  -277.   286.   304.\n",
      "   304.   336.  -338.  -343.   346.  -360.  -361.   365.  -379.   394.\n",
      "   395.   396.  -420.  -442.  -454.   457.   479.   493.   506.  -508.\n",
      "   515.   518.   524.   528.  -529.   531.   542.   547.  -547.   555.\n",
      "   563.  -571.  -576.  -581.   594.   608.   629.   637.   637.  -643.\n",
      "  -655.   663.   676.   676.  -684.  -686.   695.   728.   731.   737.\n",
      "  -741.  -751.   760.  -761.  -763.  -772.  -783.   820.   820.   821.\n",
      "  -837.  -848.   857.   863.   868.  -875.   883.  -885.   887.   914.\n",
      "  -915.  -918.   919.  -928.  -932.  -932.   951.  -952.  -956.   962.\n",
      "  -967.  1004.  1024.  1024.  1032.  1033.  1035.  1039.  1046.  1059.\n",
      "  1064.  1069.  1082.  1088.  1089.  1091.  1102.  1104.  1106. -1127.\n",
      "  1161.  1162.  1189. -1207. -1212.  1213.  1229.  1249.  1259.  1278.\n",
      "  1279.  1319.  1321.  1324.  1329.  1341. -1342.  1348.  1354.  1354.\n",
      " -1357.  1366.  1369.  1384. -1386.  1399.  1446.  1448.  1470.  1483.\n",
      "  1516.  1562.  1579.  1583.  1583.  1620.  1646.  1650. -1684.  1688.\n",
      "  1699.  1720.  1721. -1722.  1736.  1746.  1757.  1769. -1785.  1799.\n",
      "  1815. -1844.  1875. -1882. -1900.  1947.  1955.  1977. -1977. -1993.\n",
      "  1993. -1998.  2009.  2012.  2012. -2032.  2049. -2078.  2089.  2148.\n",
      " -2182.  2182.  2218. -2329. -2338. -2369.  2400. -2464. -2535.  2634.\n",
      "  2648.  2688.  2692.  2742. -3096. -3325.  3337. -3525. -3532. -3871.\n",
      " -4424. -5481.]\n",
      "durations 53.0 4624.0\n",
      "Concordance Index 0.4334305150631681\n",
      "Integrated Brier Score: 0.1551244366295538\n",
      "(242, 19076) <class 'pandas.core.frame.DataFrame'>\n",
      "(242,) <class 'pandas.core.series.Series'>\n",
      "(60, 19076) <class 'pandas.core.frame.DataFrame'>\n",
      "(60,) <class 'pandas.core.series.Series'>\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m5.5598\u001b[0m  0.0108\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2        5.5598  0.0185\n",
      "      3        5.5598  0.0104\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      4        5.5598  0.0133\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      5        5.5598  0.0124\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m4.4511\u001b[0m  0.0164\n",
      "      6        5.5598  0.0217\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m4.7892\u001b[0m  0.0209\n",
      "      2        4.4511  0.0165\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m4.3675\u001b[0m        \u001b[32m4.5052\u001b[0m  0.1120\n",
      "      7        5.5598  0.0154\n",
      "      2        4.7892  0.0146\n",
      "      3        4.4511  0.0132\n",
      "      8        5.5598  0.0131\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      3        4.7892  0.0121\n",
      "      9        5.5598  0.0124\n",
      "      4        4.4511  0.0214\n",
      "      4        4.7892  0.0115\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m4.3385\u001b[0m        \u001b[32m4.7959\u001b[0m  0.1350\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.7225\u001b[0m  0.0178\n",
      "     10        5.5598  0.0133\n",
      "Restoring best model from epoch 1.\n",
      "      5        4.7892  0.0133\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m4.4976\u001b[0m  0.0167\n",
      "      5        4.4511  0.0231\n",
      "      2        3.7225  0.0184\n",
      "      6        4.7892  0.0197\n",
      "      6        4.4511  0.0114\n",
      "      3        3.7225  0.0110\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m4.7976\u001b[0m        \u001b[32m5.5448\u001b[0m  0.1492\n",
      "      2        4.4976  0.0231\n",
      "      7        4.7892  0.0122\n",
      "      7        4.4511  0.0126\n",
      "      4        3.7225  0.0117\n",
      "      3        4.4976  0.0116\n",
      "      8        4.7892  0.0176\n",
      "      5        3.7225  0.0148\n",
      "      8        4.4511  0.0222\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m4.4377\u001b[0m        \u001b[32m4.4472\u001b[0m  0.1473\n",
      "      4        4.4976  0.0170\n",
      "      9        4.7892  0.0135\n",
      "      6        3.7225  0.0120\n",
      "      2        \u001b[36m4.3335\u001b[0m        4.5076  0.1095\n",
      "      9        4.4511  0.0112\n",
      "      5        4.4976  0.0138\n",
      "      2        \u001b[36m4.3334\u001b[0m        4.7959  0.0914\n",
      "     10        4.4511  0.0111\n",
      "Restoring best model from epoch 1.\n",
      "      7        3.7225  0.0151\n",
      "     10        4.7892  0.0174\n",
      "Restoring best model from epoch 1.\n",
      "      6        4.4976  0.0109\n",
      "      8        3.7225  0.0139\n",
      "      7        4.4976  0.0152\n",
      "      9        3.7225  0.0129\n",
      "      8        4.4976  0.0139\n",
      "     10        3.7225  0.0189\n",
      "Restoring best model from epoch 1.\n",
      "      9        4.4976  0.0128\n",
      "     10        4.4976  0.0120\n",
      "Restoring best model from epoch 1.\n",
      "      2        4.8166        5.5452  0.1240\n",
      "      2        \u001b[36m4.4065\u001b[0m        4.4481  0.1061\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m4.2923\u001b[0m        \u001b[32m3.7379\u001b[0m  0.2535\n",
      "      3        \u001b[36m4.3114\u001b[0m        \u001b[32m4.7935\u001b[0m  0.1194\n",
      "      3        4.3558        4.5103  0.1509\n",
      "      3        \u001b[36m4.7767\u001b[0m        \u001b[32m5.5432\u001b[0m  0.0839\n",
      "      3        4.4111        4.4515  0.0903\n",
      "      2        \u001b[36m4.2730\u001b[0m        \u001b[32m3.7371\u001b[0m  0.0849\n",
      "      4        4.3672        \u001b[32m4.7914\u001b[0m  0.0782\n",
      "      4        4.3460        4.5098  0.0828\n",
      "      4        4.7808        \u001b[32m5.5429\u001b[0m  0.0796\n",
      "      3        \u001b[36m4.2435\u001b[0m        \u001b[32m3.7359\u001b[0m  0.0816\n",
      "      4        4.4237        4.4516  0.0872\n",
      "      5        4.3183        \u001b[32m4.7897\u001b[0m  0.0825\n",
      "      5        \u001b[36m4.3190\u001b[0m        4.5092  0.0938\n",
      "      5        4.7861        \u001b[32m5.5414\u001b[0m  0.0783\n",
      "      5        \u001b[36m4.3964\u001b[0m        4.4496  0.0752\n",
      "      4        4.2587        \u001b[32m3.7354\u001b[0m  0.0780\n",
      "      6        4.3187        \u001b[32m4.7889\u001b[0m  0.0803\n",
      "      6        4.3480        4.5091  0.0898\n",
      "      6        4.8141        \u001b[32m5.5389\u001b[0m  0.0949\n",
      "      6        4.4131        4.4479  0.0762\n",
      "      5        4.2636        3.7368  0.0769\n",
      "      7        \u001b[36m4.3050\u001b[0m        \u001b[32m4.7881\u001b[0m  0.0737\n",
      "      7        4.3321        4.5104  0.0910\n",
      "      7        \u001b[36m4.3681\u001b[0m        \u001b[32m4.4459\u001b[0m  0.0778\n",
      "      6        4.2468        3.7369  0.0784\n",
      "      7        4.8008        \u001b[32m5.5363\u001b[0m  0.0859\n",
      "      8        4.3432        \u001b[32m4.7873\u001b[0m  0.0715\n",
      "      7        4.2613        3.7369  0.0825\n",
      "      8        \u001b[36m4.7520\u001b[0m        5.5365  0.0822\n",
      "      8        4.3330        4.5088  0.0907\n",
      "      8        4.4192        \u001b[32m4.4435\u001b[0m  0.0891\n",
      "      9        4.3348        \u001b[32m4.7866\u001b[0m  0.0768\n",
      "      9        4.7637        5.5367  0.0831\n",
      "      8        4.2665        3.7367  0.0876\n",
      "      9        4.4088        \u001b[32m4.4429\u001b[0m  0.0820\n",
      "      9        4.3386        4.5069  0.0848\n",
      "     10        4.3471        \u001b[32m4.7854\u001b[0m  0.0811\n",
      "     10        4.7860        5.5370  0.0741\n",
      "Restoring best model from epoch 7.\n",
      "     10        4.3934        \u001b[32m4.4426\u001b[0m  0.0710\n",
      "Restoring best model from epoch 9.\n",
      "      9        4.2564        3.7383  0.0764\n",
      "     10        4.3403        \u001b[32m4.5030\u001b[0m  0.0756\n",
      "     10        4.2754        3.7417  0.0706\n",
      "Restoring best model from epoch 4.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m4.4713\u001b[0m        \u001b[32m5.0222\u001b[0m  0.0756\n",
      "      2        \u001b[36m4.4522\u001b[0m        \u001b[32m5.0197\u001b[0m  0.0691\n",
      "      3        4.4571        5.0220  0.0545\n",
      "      4        4.4691        5.0219  0.0585\n",
      "      5        4.4603        5.0208  0.0539\n",
      "      6        \u001b[36m4.4467\u001b[0m        5.0210  0.0736\n",
      "      7        4.4843        5.0203  0.0677\n",
      "      8        \u001b[36m4.4429\u001b[0m        \u001b[32m5.0190\u001b[0m  0.0607\n",
      "      9        \u001b[36m4.4426\u001b[0m        \u001b[32m5.0176\u001b[0m  0.0639\n",
      "     10        4.4559        \u001b[32m5.0160\u001b[0m  0.0672\n",
      "y_train breslow final [    8.     9.    11.   -16.   -24.    24.    25.    31.    31.    53.\n",
      "    61.   -61.    65.   -67.    74.    75.   -81.    90.    91.   -92.\n",
      "    92.   -98.  -106.  -113.  -122.  -133.   138.  -145.  -147.  -148.\n",
      "  -163.  -165.  -168.  -168.  -176.  -176.  -181.  -186.   186.  -190.\n",
      "  -194.   197.  -204.  -220.  -239.  -255.   260.  -260.  -260.  -268.\n",
      "  -277.   304.   336.  -338.  -343.  -361.   365.  -379.   394.   395.\n",
      "  -441.  -442.  -454.   455.   457.   506.  -508.   515.   518.   524.\n",
      "   528.  -529.   531.   542.   547.  -547.  -547.   562.   565.   568.\n",
      "  -571.  -576.   594.   624.   627.   629.   637.   663.   676.   676.\n",
      "   679.  -684.  -686.   695.   728.   731.   737.  -741.  -751.  -761.\n",
      "  -763.  -772.   787.  -816.   820.   821.   840.  -847.   868.  -875.\n",
      "   883.   914.  -915.  -918.   919.  -932.   949.   951.  -952.  -956.\n",
      "   962.  -967.   976.  1004.  1018.  1024.  1035.  1046. -1053.  1058.\n",
      "  1059.  1064.  1069.  1082.  1088.  1102.  1104.  1123. -1127.  1155.\n",
      "  1157.  1161.  1163.  1187.  1189. -1207. -1212.  1213.  1229.  1249.\n",
      "  1259.  1278.  1279.  1319.  1321.  1341. -1342.  1348.  1364. -1364.\n",
      "  1366.  1369. -1372.  1384.  1399.  1446.  1448.  1451.  1470.  1483.\n",
      "  1484.  1484.  1492.  1516.  1562. -1573.  1579.  1579.  1583.  1583.\n",
      "  1620.  1650. -1684.  1688.  1699.  1721. -1722.  1736.  1746. -1752.\n",
      "  1757.  1767.  1769. -1785.  1799.  1815. -1844.  1875. -1882.  1891.\n",
      " -1919.  1947.  1955.  1977. -1993. -1998.  2009.  2012.  2012. -2032.\n",
      "  2049.  2089. -2143.  2148. -2182.  2218. -2329. -2338.  2342.  2345.\n",
      " -2369. -2464.  2467. -2535. -2614.  2621.  2648. -2661. -2661.  2688.\n",
      "  2692.  2717.  2742. -3096.  3224.  3337. -3525. -3532. -3825. -4424.\n",
      "  4624. -5481.]\n",
      "Concordance Index 0.5349692511756904\n",
      "Integrated Brier Score: 0.12687368799876705\n",
      "y_train breslow final [    8.     9.    11.   -16.   -24.    24.    25.    31.    31.    53.\n",
      "    61.   -61.    65.   -67.    74.    75.   -81.    90.    91.   -92.\n",
      "    92.   -98.  -106.  -113.  -122.  -133.   138.  -145.  -147.  -148.\n",
      "  -163.  -165.  -168.  -168.  -176.  -176.  -181.  -186.   186.  -190.\n",
      "  -194.   197.  -204.  -220.  -239.  -255.   260.  -260.  -260.  -268.\n",
      "  -277.   304.   336.  -338.  -343.  -361.   365.  -379.   394.   395.\n",
      "  -441.  -442.  -454.   455.   457.   506.  -508.   515.   518.   524.\n",
      "   528.  -529.   531.   542.   547.  -547.  -547.   562.   565.   568.\n",
      "  -571.  -576.   594.   624.   627.   629.   637.   663.   676.   676.\n",
      "   679.  -684.  -686.   695.   728.   731.   737.  -741.  -751.  -761.\n",
      "  -763.  -772.   787.  -816.   820.   821.   840.  -847.   868.  -875.\n",
      "   883.   914.  -915.  -918.   919.  -932.   949.   951.  -952.  -956.\n",
      "   962.  -967.   976.  1004.  1018.  1024.  1035.  1046. -1053.  1058.\n",
      "  1059.  1064.  1069.  1082.  1088.  1102.  1104.  1123. -1127.  1155.\n",
      "  1157.  1161.  1163.  1187.  1189. -1207. -1212.  1213.  1229.  1249.\n",
      "  1259.  1278.  1279.  1319.  1321.  1341. -1342.  1348.  1364. -1364.\n",
      "  1366.  1369. -1372.  1384.  1399.  1446.  1448.  1451.  1470.  1483.\n",
      "  1484.  1484.  1492.  1516.  1562. -1573.  1579.  1579.  1583.  1583.\n",
      "  1620.  1650. -1684.  1688.  1699.  1721. -1722.  1736.  1746. -1752.\n",
      "  1757.  1767.  1769. -1785.  1799.  1815. -1844.  1875. -1882.  1891.\n",
      " -1919.  1947.  1955.  1977. -1993. -1998.  2009.  2012.  2012. -2032.\n",
      "  2049.  2089. -2143.  2148. -2182.  2218. -2329. -2338.  2342.  2345.\n",
      " -2369. -2464.  2467. -2535. -2614.  2621.  2648. -2661. -2661.  2688.\n",
      "  2692.  2717.  2742. -3096.  3224.  3337. -3525. -3532. -3825. -4424.\n",
      "  4624. -5481.]\n",
      "durations 24.0 3871.0\n",
      "Concordance Index 0.44930417495029823\n",
      "Integrated Brier Score: 0.15219347410113218\n",
      "(242, 19076) <class 'pandas.core.frame.DataFrame'>\n",
      "(242,) <class 'pandas.core.series.Series'>\n",
      "(60, 19076) <class 'pandas.core.frame.DataFrame'>\n",
      "(60,) <class 'pandas.core.series.Series'>\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m5.5244\u001b[0m  0.0102\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      2        5.5244  0.0104\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      3        5.5244  0.0104\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m5.1049\u001b[0m  0.0115\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m4.6797\u001b[0m  0.0134\n",
      "      4        5.5244  0.0128\n",
      "      2        4.6797  0.0117\n",
      "      2        5.1049  0.0162\n",
      "      5        5.5244  0.0141\n",
      "      3        4.6797  0.0113\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      3        5.1049  0.0134\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      6        5.5244  0.0129\n",
      "      4        4.6797  0.0114\n",
      "      4        5.1049  0.0112\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      5        4.6797  0.0114\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m4.3603\u001b[0m        \u001b[32m4.6596\u001b[0m  0.1131\n",
      "      7        5.5244  0.0175\n",
      "      5        5.1049  0.0130\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m4.4299\u001b[0m  0.0129\n",
      "      6        4.6797  0.0116\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m4.3834\u001b[0m        \u001b[32m4.4131\u001b[0m  0.0933\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.9159\u001b[0m  0.0172\n",
      "      6        5.1049  0.0121\n",
      "      8        5.5244  0.0142\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m4.2642\u001b[0m        \u001b[32m5.0919\u001b[0m  0.0965\n",
      "      2        4.4299  0.0115\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m4.1896\u001b[0m        \u001b[32m3.8871\u001b[0m  0.1272\n",
      "      7        4.6797  0.0139\n",
      "      2        3.9159  0.0121\n",
      "      7        5.1049  0.0113\n",
      "      3        4.4299  0.0113\n",
      "      9        5.5244  0.0171\n",
      "      3        3.9159  0.0118\n",
      "      8        4.6797  0.0131\n",
      "      8        5.1049  0.0127\n",
      "      4        4.4299  0.0111\n",
      "      4        3.9159  0.0113\n",
      "      9        5.1049  0.0109\n",
      "     10        5.5244  0.0185\n",
      "Restoring best model from epoch 1.\n",
      "      9        4.6797  0.0176\n",
      "      5        4.4299  0.0183\n",
      "      5        3.9159  0.0145\n",
      "     10        5.1049  0.0129\n",
      "Restoring best model from epoch 1.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m5.0882\u001b[0m        \u001b[32m5.5077\u001b[0m  0.1512\n",
      "     10        4.6797  0.0119\n",
      "Restoring best model from epoch 1.\n",
      "      6        4.4299  0.0125\n",
      "      6        3.9159  0.0207\n",
      "      7        4.4299  0.0162\n",
      "      7        3.9159  0.0133\n",
      "      2        \u001b[36m4.3191\u001b[0m        \u001b[32m4.6546\u001b[0m  0.0990\n",
      "      8        4.4299  0.0105\n",
      "      9        4.4299  0.0123\n",
      "      2        \u001b[36m4.2396\u001b[0m        \u001b[32m5.0915\u001b[0m  0.0896\n",
      "      8        3.9159  0.0198\n",
      "     10        4.4299  0.0115\n",
      "Restoring best model from epoch 1.\n",
      "      2        4.3870        \u001b[32m4.4120\u001b[0m  0.1147\n",
      "      2        \u001b[36m4.1648\u001b[0m        3.8890  0.1091\n",
      "      9        3.9159  0.0172\n",
      "     10        3.9159  0.0108\n",
      "Restoring best model from epoch 1.\n",
      "      2        5.1014        \u001b[32m5.5054\u001b[0m  0.0941\n",
      "      3        4.3283        \u001b[32m4.6501\u001b[0m  0.0848\n",
      "      3        4.2800        5.0916  0.0928\n",
      "      3        \u001b[36m4.3771\u001b[0m        \u001b[32m4.4082\u001b[0m  0.0817\n",
      "      3        4.1784        3.8885  0.0903\n",
      "      3        5.0886        5.5061  0.0760\n",
      "      4        \u001b[36m4.2922\u001b[0m        \u001b[32m4.6473\u001b[0m  0.0780\n",
      "      4        \u001b[36m4.2275\u001b[0m        5.0932  0.0759\n",
      "      4        \u001b[36m4.3666\u001b[0m        4.4083  0.0808\n",
      "      4        \u001b[36m4.1207\u001b[0m        3.8896  0.0795\n",
      "      4        \u001b[36m5.0880\u001b[0m        5.5061  0.0910\n",
      "      5        4.3198        \u001b[32m4.6467\u001b[0m  0.0727\n",
      "      5        4.2408        5.0949  0.0702\n",
      "      5        4.3674        \u001b[32m4.4080\u001b[0m  0.0710\n",
      "      5        4.1423        3.8899  0.0783\n",
      "      5        \u001b[36m5.0723\u001b[0m        5.5059  0.0762\n",
      "      6        4.3473        \u001b[32m4.6466\u001b[0m  0.0756\n",
      "      6        4.2675        5.0969  0.0708\n",
      "      6        4.3929        \u001b[32m4.4055\u001b[0m  0.0713\n",
      "      6        4.1874        3.8907  0.0734\n",
      "      6        5.1085        5.5065  0.0763\n",
      "      7        4.2580        5.0981  0.0727\n",
      "      7        4.3222        4.6468  0.0842\n",
      "      7        4.3762        \u001b[32m4.4043\u001b[0m  0.0707\n",
      "      7        4.1534        3.8904  0.0781\n",
      "      7        5.0820        5.5076  0.0775\n",
      "      8        4.2467        5.0986  0.0731\n",
      "      8        4.3067        \u001b[32m4.6461\u001b[0m  0.0774\n",
      "      8        \u001b[36m4.3441\u001b[0m        \u001b[32m4.4001\u001b[0m  0.0718\n",
      "      8        \u001b[36m4.0975\u001b[0m        3.8903  0.0918\n",
      "      8        \u001b[36m5.0666\u001b[0m        5.5085  0.0744\n",
      "      9        4.2448        5.1003  0.0749\n",
      "      9        4.3145        \u001b[32m4.6460\u001b[0m  0.0737\n",
      "      9        4.3779        \u001b[32m4.3964\u001b[0m  0.0734\n",
      "      9        4.1587        3.8894  0.0706\n",
      "      9        5.0722        5.5087  0.0852\n",
      "     10        4.3285        4.6469  0.0780\n",
      "Restoring best model from epoch 8.\n",
      "     10        4.3569        \u001b[32m4.3954\u001b[0m  0.0755\n",
      "     10        4.2424        5.1019  0.0862\n",
      "Restoring best model from epoch 1.\n",
      "     10        4.1618        3.8887  0.0715\n",
      "Restoring best model from epoch 1.\n",
      "     10        5.0679        5.5094  0.0726\n",
      "Restoring best model from epoch 2.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m4.4852\u001b[0m        \u001b[32m5.0029\u001b[0m  0.0872\n",
      "      2        \u001b[36m4.4802\u001b[0m        \u001b[32m5.0010\u001b[0m  0.0722\n",
      "      3        4.4823        \u001b[32m5.0007\u001b[0m  0.0586\n",
      "      4        \u001b[36m4.4698\u001b[0m        \u001b[32m4.9988\u001b[0m  0.0543\n",
      "      5        \u001b[36m4.4631\u001b[0m        4.9996  0.0694\n",
      "      6        4.4711        \u001b[32m4.9987\u001b[0m  0.0620\n",
      "      7        4.4733        4.9991  0.0657\n",
      "      8        \u001b[36m4.4597\u001b[0m        5.0014  0.0582\n",
      "      9        4.4621        5.0009  0.0573\n",
      "     10        \u001b[36m4.4422\u001b[0m        5.0009  0.0569\n",
      "Restoring best model from epoch 4.\n",
      "y_train breslow final [    8.     9.   -16.    24.   -24.    25.    31.    31.    36.    53.\n",
      "    61.   -61.   -67.    74.   -81.    90.    91.   -92.    92.  -106.\n",
      "  -122.  -130.  -133.   138.  -145.  -147.  -163.  -165.  -168.  -176.\n",
      "  -176.  -181.  -183.   186.  -190.  -192.  -194.   197.  -204.  -220.\n",
      "  -238.  -239.  -243.  -253.  -255.  -260.  -260.  -268.  -277.   286.\n",
      "   304.   304.   336.  -338.   346.  -360.   365.  -379.   394.   396.\n",
      "  -420.  -441.  -442.  -454.   455.   457.   479.   493.   506.  -508.\n",
      "   515.   524.   528.  -529.   531.   542.   547.  -547.  -547.   555.\n",
      "   562.   563.   565.   568.  -571.  -576.  -581.   594.   608.   624.\n",
      "   627.   637.   637.  -643.  -655.   663.   679.  -684.   728.  -741.\n",
      "  -751.   760.  -761.  -772.  -783.   787.  -816.   820.   820.   821.\n",
      "  -837.   840.  -847.  -848.   857.   863.   868.  -875.  -885.   887.\n",
      "   914.  -915.   919.  -928.  -932.   949.   951.  -952.  -956.   962.\n",
      "   976.  1018.  1024.  1024.  1032.  1033.  1035.  1039.  1046. -1053.\n",
      "  1058.  1059.  1064.  1082.  1088.  1089.  1091.  1102.  1104.  1106.\n",
      "  1123.  1155.  1157.  1161.  1162.  1163.  1187. -1207. -1212.  1229.\n",
      "  1249.  1249.  1278.  1279.  1319.  1321.  1324.  1329.  1341.  1354.\n",
      "  1354. -1357.  1364. -1364.  1366.  1369. -1372. -1386.  1399.  1448.\n",
      "  1451.  1470.  1483.  1484.  1484.  1492. -1573.  1579.  1579.  1583.\n",
      "  1583.  1646.  1650.  1688.  1720.  1746. -1752.  1757.  1767.  1769.\n",
      "  1815. -1844.  1875. -1882.  1891. -1900. -1919.  1947.  1955. -1977.\n",
      "  1993.  2009.  2049. -2078.  2089. -2143.  2182. -2329. -2338.  2342.\n",
      "  2345. -2369.  2400. -2464.  2467. -2614.  2621.  2634. -2661. -2661.\n",
      "  2688.  2692.  2717.  2742.  3224. -3325. -3532. -3825. -3871. -4424.\n",
      "  4624. -5481.]\n",
      "Concordance Index 0.48237465395262996\n",
      "Integrated Brier Score: 0.12403517929722271\n",
      "y_train breslow final [    8.     9.   -16.    24.   -24.    25.    31.    31.    36.    53.\n",
      "    61.   -61.   -67.    74.   -81.    90.    91.   -92.    92.  -106.\n",
      "  -122.  -130.  -133.   138.  -145.  -147.  -163.  -165.  -168.  -176.\n",
      "  -176.  -181.  -183.   186.  -190.  -192.  -194.   197.  -204.  -220.\n",
      "  -238.  -239.  -243.  -253.  -255.  -260.  -260.  -268.  -277.   286.\n",
      "   304.   304.   336.  -338.   346.  -360.   365.  -379.   394.   396.\n",
      "  -420.  -441.  -442.  -454.   455.   457.   479.   493.   506.  -508.\n",
      "   515.   524.   528.  -529.   531.   542.   547.  -547.  -547.   555.\n",
      "   562.   563.   565.   568.  -571.  -576.  -581.   594.   608.   624.\n",
      "   627.   637.   637.  -643.  -655.   663.   679.  -684.   728.  -741.\n",
      "  -751.   760.  -761.  -772.  -783.   787.  -816.   820.   820.   821.\n",
      "  -837.   840.  -847.  -848.   857.   863.   868.  -875.  -885.   887.\n",
      "   914.  -915.   919.  -928.  -932.   949.   951.  -952.  -956.   962.\n",
      "   976.  1018.  1024.  1024.  1032.  1033.  1035.  1039.  1046. -1053.\n",
      "  1058.  1059.  1064.  1082.  1088.  1089.  1091.  1102.  1104.  1106.\n",
      "  1123.  1155.  1157.  1161.  1162.  1163.  1187. -1207. -1212.  1229.\n",
      "  1249.  1249.  1278.  1279.  1319.  1321.  1324.  1329.  1341.  1354.\n",
      "  1354. -1357.  1364. -1364.  1366.  1369. -1372. -1386.  1399.  1448.\n",
      "  1451.  1470.  1483.  1484.  1484.  1492. -1573.  1579.  1579.  1583.\n",
      "  1583.  1646.  1650.  1688.  1720.  1746. -1752.  1757.  1767.  1769.\n",
      "  1815. -1844.  1875. -1882.  1891. -1900. -1919.  1947.  1955. -1977.\n",
      "  1993.  2009.  2049. -2078.  2089. -2143.  2182. -2329. -2338.  2342.\n",
      "  2345. -2369.  2400. -2464.  2467. -2614.  2621.  2634. -2661. -2661.\n",
      "  2688.  2692.  2717.  2742.  3224. -3325. -3532. -3825. -3871. -4424.\n",
      "  4624. -5481.]\n",
      "durations 11.0 3525.0\n",
      "Concordance Index 0.40823970037453183\n",
      "Integrated Brier Score: 0.17231582140741872\n",
      "split gex_?|100133144      float32\n",
      "gex_?|100134869      float32\n",
      "gex_?|10357          float32\n",
      "gex_?|10431          float32\n",
      "gex_?|155060         float32\n",
      "                      ...   \n",
      "gex_ZYG11A|440590    float32\n",
      "gex_ZYG11B|79699     float32\n",
      "gex_ZYX|7791         float32\n",
      "gex_ZZEF1|23140      float32\n",
      "gex_ZZZ3|26009       float32\n",
      "Length: 19076, dtype: object\n",
      "(311, 19076) <class 'pandas.core.frame.DataFrame'>\n",
      "(311,) <class 'pandas.core.series.Series'>\n",
      "(78, 19076) <class 'pandas.core.frame.DataFrame'>\n",
      "(78,) <class 'pandas.core.series.Series'>\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.6646\u001b[0m        \u001b[32m3.0285\u001b[0m  0.1258\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.0633\u001b[0m  0.0214\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2        3.0633  0.0271\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      3        3.0633  0.0176\n",
      "      2        2.6824        \u001b[32m3.0270\u001b[0m  0.1327\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.0277\u001b[0m  0.0358\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m2.5915\u001b[0m  0.0143\n",
      "      4        3.0633  0.0331\n",
      "      2        3.0277  0.0195\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2        2.5915  0.0165\n",
      "      5        3.0633  0.0171\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.3791\u001b[0m  0.0218\n",
      "      3        2.5915  0.0141\n",
      "      3        3.0277  0.0294\n",
      "      6        3.0633  0.0179\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.9841\u001b[0m        \u001b[32m3.0464\u001b[0m  0.1681\n",
      "      4        2.5915  0.0128\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.8662\u001b[0m        \u001b[32m2.5883\u001b[0m  0.1826\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.0265\u001b[0m  0.0322\n",
      "      5        2.5915  0.0164\n",
      "      4        3.0277  0.0241\n",
      "      7        3.0633  0.0232\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.5990\u001b[0m        \u001b[32m3.0053\u001b[0m  0.2114\n",
      "      2        3.3791  0.0430\n",
      "      6        2.5915  0.0158\n",
      "      8        3.0633  0.0149\n",
      "      5        3.0277  0.0235\n",
      "      2        3.0265  0.0302\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.1396\u001b[0m        \u001b[32m3.3898\u001b[0m  0.2159\n",
      "      7        2.5915  0.0150\n",
      "      3        3.3791  0.0232\n",
      "      9        3.0633  0.0260\n",
      "      3        3.0265  0.0239\n",
      "      4        3.3791  0.0158\n",
      "      8        2.5915  0.0183\n",
      "      6        3.0277  0.0276\n",
      "     10        3.0633  0.0153\n",
      "Restoring best model from epoch 1.\n",
      "      5        3.3791  0.0153\n",
      "      4        3.0265  0.0170\n",
      "      9        2.5915  0.0181\n",
      "      7        3.0277  0.0244\n",
      "      5        3.0265  0.0193\n",
      "      6        3.3791  0.0230\n",
      "      3        2.6758        3.0274  0.1808\n",
      "      8        3.0277  0.0137\n",
      "     10        2.5915  0.0262\n",
      "Restoring best model from epoch 1.\n",
      "      6        3.0265  0.0133\n",
      "      9        3.0277  0.0147\n",
      "      7        3.0265  0.0128\n",
      "      7        3.3791  0.0282\n",
      "      8        3.0265  0.0165\n",
      "     10        3.0277  0.0242\n",
      "      8        3.3791  0.0141\n",
      "Restoring best model from epoch 1.\n",
      "      2        \u001b[36m2.8539\u001b[0m        \u001b[32m2.5883\u001b[0m  0.1495\n",
      "      9        3.0265  0.0160\n",
      "      9        3.3791  0.0131\n",
      "     10        3.0265  0.0148\n",
      "Restoring best model from epoch 1.\n",
      "     10        3.3791  0.0155\n",
      "Restoring best model from epoch 1.\n",
      "      2        \u001b[36m2.9697\u001b[0m        3.0465  0.1842\n",
      "      2        \u001b[36m2.5809\u001b[0m        \u001b[32m3.0016\u001b[0m  0.1733\n",
      "      2        \u001b[36m3.1119\u001b[0m        \u001b[32m3.3893\u001b[0m  0.1671\n",
      "      4        2.7020        3.0273  0.1307\n",
      "      3        2.8578        \u001b[32m2.5883\u001b[0m  0.1330\n",
      "      3        2.9774        3.0509  0.1441\n",
      "      3        \u001b[36m2.5777\u001b[0m        \u001b[32m3.0009\u001b[0m  0.1447\n",
      "      3        3.1164        3.3895  0.1342\n",
      "      5        2.6649        3.0301  0.1272\n",
      "      4        2.8798        2.5892  0.1127\n",
      "      4        3.0114        3.0550  0.1283\n",
      "      4        3.1275        3.3904  0.1131\n",
      "      4        \u001b[36m2.5546\u001b[0m        \u001b[32m2.9980\u001b[0m  0.1152\n",
      "      6        2.6692        3.0314  0.1109\n",
      "      5        2.8688        2.5903  0.1194\n",
      "      5        2.5783        \u001b[32m2.9962\u001b[0m  0.1269\n",
      "      5        2.9856        3.0590  0.1520\n",
      "      5        3.1356        3.3895  0.1472\n",
      "      7        \u001b[36m2.6437\u001b[0m        3.0316  0.1758\n",
      "      6        \u001b[36m2.5280\u001b[0m        2.9977  0.1270\n",
      "      6        2.8643        2.5897  0.2073\n",
      "      8        2.6802        3.0332  0.1138\n",
      "      6        3.1372        3.3898  0.1800\n",
      "      6        2.9956        3.0650  0.1883\n",
      "      7        2.5525        2.9994  0.1227\n",
      "      7        2.8661        \u001b[32m2.5861\u001b[0m  0.1215\n",
      "      9        2.6777        3.0338  0.1157\n",
      "      7        3.1178        3.3897  0.1124\n",
      "      7        2.9768        3.0689  0.1138\n",
      "      8        2.5514        3.0021  0.1142\n",
      "      8        2.8635        \u001b[32m2.5835\u001b[0m  0.1158\n",
      "     10        2.6614        3.0355  0.1123\n",
      "Restoring best model from epoch 2.\n",
      "      8        \u001b[36m2.9514\u001b[0m        3.0743  0.1147\n",
      "      8        3.1186        3.3903  0.1244\n",
      "      9        \u001b[36m2.5166\u001b[0m        3.0026  0.1126\n",
      "      9        2.8588        2.5841  0.1050\n",
      "      9        3.1403        3.3909  0.1103\n",
      "      9        2.9620        3.0779  0.1291\n",
      "     10        2.5645        3.0037  0.1101\n",
      "Restoring best model from epoch 5.\n",
      "     10        \u001b[36m2.8478\u001b[0m        2.5848  0.1116\n",
      "Restoring best model from epoch 8.\n",
      "     10        3.1185        3.3925  0.1039\n",
      "Restoring best model from epoch 2.\n",
      "     10        2.9649        3.0810  0.1041\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.9163\u001b[0m        \u001b[32m2.7026\u001b[0m  0.1042\n",
      "      2        \u001b[36m2.9156\u001b[0m        \u001b[32m2.6995\u001b[0m  0.0652\n",
      "      3        2.9353        \u001b[32m2.6990\u001b[0m  0.0633\n",
      "      4        2.9428        \u001b[32m2.6985\u001b[0m  0.0568\n",
      "      5        \u001b[36m2.9025\u001b[0m        2.6989  0.0504\n",
      "      6        2.9363        2.7001  0.0649\n",
      "      7        2.9264        2.7007  0.0658\n",
      "      8        2.9131        2.7027  0.0630\n",
      "      9        2.9203        2.7043  0.0770\n",
      "     10        2.9079        2.7039  0.0606\n",
      "Restoring best model from epoch 4.\n",
      "y_train breslow final [ 3.000e+00  8.000e+00 -8.000e+00 -1.400e+01 -1.600e+01 -1.700e+01\n",
      " -2.100e+01 -2.100e+01  2.200e+01 -2.300e+01  2.400e+01 -2.900e+01\n",
      "  3.000e+01 -3.000e+01 -3.100e+01 -3.500e+01  4.500e+01 -4.600e+01\n",
      "  5.200e+01  5.700e+01  6.100e+01 -6.400e+01  7.600e+01 -7.700e+01\n",
      "  8.100e+01 -9.900e+01  1.050e+02  1.050e+02  1.060e+02  1.130e+02\n",
      " -1.130e+02  1.220e+02 -1.310e+02  1.410e+02  1.530e+02  1.560e+02\n",
      " -1.560e+02  1.660e+02  1.680e+02 -1.700e+02  1.740e+02 -1.760e+02\n",
      "  1.800e+02  1.800e+02 -1.830e+02  1.850e+02  1.850e+02  1.880e+02\n",
      "  1.910e+02  1.920e+02 -1.980e+02 -1.980e+02  2.000e+02  2.010e+02\n",
      "  2.050e+02  2.120e+02  2.150e+02  2.180e+02 -2.250e+02 -2.290e+02\n",
      "  2.350e+02  2.430e+02 -2.450e+02  2.450e+02  2.500e+02  2.550e+02\n",
      "  2.620e+02  2.720e+02  2.730e+02 -2.730e+02  2.750e+02  2.760e+02\n",
      "  2.790e+02 -2.800e+02  2.810e+02  2.840e+02 -2.870e+02 -2.880e+02\n",
      "  2.890e+02  2.910e+02  2.940e+02  2.960e+02  3.000e+02  3.000e+02\n",
      " -3.230e+02 -3.250e+02 -3.350e+02  3.360e+02  3.410e+02 -3.440e+02\n",
      " -3.440e+02 -3.460e+02  3.480e+02  3.480e+02  3.530e+02  3.540e+02\n",
      "  3.560e+02 -3.560e+02 -3.580e+02  3.580e+02  3.590e+02 -3.650e+02\n",
      "  3.660e+02 -3.670e+02 -3.680e+02  3.700e+02 -3.710e+02 -3.740e+02\n",
      " -3.750e+02  3.760e+02  3.770e+02  3.780e+02 -3.780e+02 -3.780e+02\n",
      " -3.790e+02 -3.810e+02 -3.830e+02 -3.830e+02 -3.830e+02 -3.880e+02\n",
      "  3.890e+02 -3.890e+02 -3.890e+02 -3.900e+02 -3.920e+02 -3.940e+02\n",
      " -3.960e+02  3.960e+02  3.980e+02 -4.000e+02  4.010e+02  4.030e+02\n",
      "  4.060e+02 -4.080e+02 -4.110e+02 -4.110e+02 -4.130e+02 -4.160e+02\n",
      " -4.190e+02  4.220e+02  4.220e+02  4.260e+02 -4.270e+02 -4.280e+02\n",
      "  4.280e+02 -4.310e+02 -4.340e+02  4.390e+02 -4.400e+02  4.460e+02\n",
      " -4.500e+02  4.570e+02 -4.630e+02  4.740e+02  4.760e+02  4.770e+02\n",
      " -4.790e+02 -4.860e+02  4.910e+02  4.910e+02 -4.910e+02 -4.940e+02\n",
      "  4.960e+02 -5.000e+02  5.070e+02  5.130e+02 -5.190e+02 -5.210e+02\n",
      " -5.230e+02 -5.250e+02  5.430e+02 -5.430e+02  5.490e+02  5.540e+02\n",
      "  5.580e+02 -5.590e+02  5.600e+02 -5.660e+02  5.700e+02 -5.730e+02\n",
      " -5.770e+02 -5.790e+02 -5.800e+02 -5.820e+02  5.880e+02 -5.930e+02\n",
      " -5.940e+02 -5.950e+02 -6.000e+02 -6.070e+02 -6.130e+02 -6.150e+02\n",
      "  6.180e+02 -6.210e+02 -6.250e+02 -6.280e+02  6.330e+02  6.350e+02\n",
      " -6.360e+02  6.400e+02 -6.440e+02 -6.470e+02 -6.500e+02  6.520e+02\n",
      "  6.610e+02 -6.640e+02 -6.660e+02  6.690e+02 -6.750e+02  6.750e+02\n",
      " -6.780e+02 -6.790e+02 -6.940e+02 -6.990e+02  7.120e+02 -7.240e+02\n",
      " -7.250e+02 -7.360e+02 -7.380e+02 -7.390e+02 -7.420e+02 -7.520e+02\n",
      " -7.540e+02  7.620e+02  7.790e+02  7.820e+02 -7.850e+02  7.920e+02\n",
      "  7.940e+02  8.010e+02 -8.120e+02 -8.130e+02 -8.190e+02 -8.250e+02\n",
      "  8.320e+02 -8.380e+02 -8.380e+02 -8.560e+02 -8.620e+02 -8.640e+02\n",
      "  8.690e+02  8.740e+02 -8.810e+02  8.810e+02 -8.820e+02 -9.000e+02\n",
      " -9.120e+02 -9.120e+02 -9.280e+02  9.400e+02 -9.400e+02 -9.420e+02\n",
      " -9.420e+02 -9.460e+02 -9.490e+02 -9.510e+02 -9.720e+02 -9.810e+02\n",
      " -9.890e+02 -9.910e+02 -1.000e+03 -1.002e+03 -1.010e+03 -1.016e+03\n",
      " -1.023e+03 -1.038e+03  1.043e+03 -1.055e+03 -1.072e+03  1.095e+03\n",
      " -1.100e+03 -1.106e+03 -1.108e+03 -1.124e+03 -1.132e+03 -1.133e+03\n",
      " -1.138e+03 -1.145e+03  1.153e+03 -1.160e+03 -1.184e+03 -1.190e+03\n",
      " -1.200e+03 -1.210e+03 -1.223e+03 -1.236e+03  1.294e+03 -1.297e+03\n",
      " -1.319e+03 -1.328e+03 -1.367e+03 -1.431e+03 -1.484e+03 -1.588e+03\n",
      " -1.645e+03  1.686e+03 -1.690e+03  1.747e+03 -1.765e+03 -1.851e+03\n",
      " -1.862e+03 -1.918e+03 -1.935e+03 -1.964e+03 -2.032e+03  2.100e+03\n",
      "  2.197e+03 -2.267e+03 -3.196e+03 -3.519e+03 -3.720e+03]\n",
      "Concordance Index 0.42583326265795945\n",
      "Integrated Brier Score: 0.22514167297270504\n",
      "y_train breslow final [ 3.000e+00  8.000e+00 -8.000e+00 -1.400e+01 -1.600e+01 -1.700e+01\n",
      " -2.100e+01 -2.100e+01  2.200e+01 -2.300e+01  2.400e+01 -2.900e+01\n",
      "  3.000e+01 -3.000e+01 -3.100e+01 -3.500e+01  4.500e+01 -4.600e+01\n",
      "  5.200e+01  5.700e+01  6.100e+01 -6.400e+01  7.600e+01 -7.700e+01\n",
      "  8.100e+01 -9.900e+01  1.050e+02  1.050e+02  1.060e+02  1.130e+02\n",
      " -1.130e+02  1.220e+02 -1.310e+02  1.410e+02  1.530e+02  1.560e+02\n",
      " -1.560e+02  1.660e+02  1.680e+02 -1.700e+02  1.740e+02 -1.760e+02\n",
      "  1.800e+02  1.800e+02 -1.830e+02  1.850e+02  1.850e+02  1.880e+02\n",
      "  1.910e+02  1.920e+02 -1.980e+02 -1.980e+02  2.000e+02  2.010e+02\n",
      "  2.050e+02  2.120e+02  2.150e+02  2.180e+02 -2.250e+02 -2.290e+02\n",
      "  2.350e+02  2.430e+02 -2.450e+02  2.450e+02  2.500e+02  2.550e+02\n",
      "  2.620e+02  2.720e+02  2.730e+02 -2.730e+02  2.750e+02  2.760e+02\n",
      "  2.790e+02 -2.800e+02  2.810e+02  2.840e+02 -2.870e+02 -2.880e+02\n",
      "  2.890e+02  2.910e+02  2.940e+02  2.960e+02  3.000e+02  3.000e+02\n",
      " -3.230e+02 -3.250e+02 -3.350e+02  3.360e+02  3.410e+02 -3.440e+02\n",
      " -3.440e+02 -3.460e+02  3.480e+02  3.480e+02  3.530e+02  3.540e+02\n",
      "  3.560e+02 -3.560e+02 -3.580e+02  3.580e+02  3.590e+02 -3.650e+02\n",
      "  3.660e+02 -3.670e+02 -3.680e+02  3.700e+02 -3.710e+02 -3.740e+02\n",
      " -3.750e+02  3.760e+02  3.770e+02  3.780e+02 -3.780e+02 -3.780e+02\n",
      " -3.790e+02 -3.810e+02 -3.830e+02 -3.830e+02 -3.830e+02 -3.880e+02\n",
      "  3.890e+02 -3.890e+02 -3.890e+02 -3.900e+02 -3.920e+02 -3.940e+02\n",
      " -3.960e+02  3.960e+02  3.980e+02 -4.000e+02  4.010e+02  4.030e+02\n",
      "  4.060e+02 -4.080e+02 -4.110e+02 -4.110e+02 -4.130e+02 -4.160e+02\n",
      " -4.190e+02  4.220e+02  4.220e+02  4.260e+02 -4.270e+02 -4.280e+02\n",
      "  4.280e+02 -4.310e+02 -4.340e+02  4.390e+02 -4.400e+02  4.460e+02\n",
      " -4.500e+02  4.570e+02 -4.630e+02  4.740e+02  4.760e+02  4.770e+02\n",
      " -4.790e+02 -4.860e+02  4.910e+02  4.910e+02 -4.910e+02 -4.940e+02\n",
      "  4.960e+02 -5.000e+02  5.070e+02  5.130e+02 -5.190e+02 -5.210e+02\n",
      " -5.230e+02 -5.250e+02  5.430e+02 -5.430e+02  5.490e+02  5.540e+02\n",
      "  5.580e+02 -5.590e+02  5.600e+02 -5.660e+02  5.700e+02 -5.730e+02\n",
      " -5.770e+02 -5.790e+02 -5.800e+02 -5.820e+02  5.880e+02 -5.930e+02\n",
      " -5.940e+02 -5.950e+02 -6.000e+02 -6.070e+02 -6.130e+02 -6.150e+02\n",
      "  6.180e+02 -6.210e+02 -6.250e+02 -6.280e+02  6.330e+02  6.350e+02\n",
      " -6.360e+02  6.400e+02 -6.440e+02 -6.470e+02 -6.500e+02  6.520e+02\n",
      "  6.610e+02 -6.640e+02 -6.660e+02  6.690e+02 -6.750e+02  6.750e+02\n",
      " -6.780e+02 -6.790e+02 -6.940e+02 -6.990e+02  7.120e+02 -7.240e+02\n",
      " -7.250e+02 -7.360e+02 -7.380e+02 -7.390e+02 -7.420e+02 -7.520e+02\n",
      " -7.540e+02  7.620e+02  7.790e+02  7.820e+02 -7.850e+02  7.920e+02\n",
      "  7.940e+02  8.010e+02 -8.120e+02 -8.130e+02 -8.190e+02 -8.250e+02\n",
      "  8.320e+02 -8.380e+02 -8.380e+02 -8.560e+02 -8.620e+02 -8.640e+02\n",
      "  8.690e+02  8.740e+02 -8.810e+02  8.810e+02 -8.820e+02 -9.000e+02\n",
      " -9.120e+02 -9.120e+02 -9.280e+02  9.400e+02 -9.400e+02 -9.420e+02\n",
      " -9.420e+02 -9.460e+02 -9.490e+02 -9.510e+02 -9.720e+02 -9.810e+02\n",
      " -9.890e+02 -9.910e+02 -1.000e+03 -1.002e+03 -1.010e+03 -1.016e+03\n",
      " -1.023e+03 -1.038e+03  1.043e+03 -1.055e+03 -1.072e+03  1.095e+03\n",
      " -1.100e+03 -1.106e+03 -1.108e+03 -1.124e+03 -1.132e+03 -1.133e+03\n",
      " -1.138e+03 -1.145e+03  1.153e+03 -1.160e+03 -1.184e+03 -1.190e+03\n",
      " -1.200e+03 -1.210e+03 -1.223e+03 -1.236e+03  1.294e+03 -1.297e+03\n",
      " -1.319e+03 -1.328e+03 -1.367e+03 -1.431e+03 -1.484e+03 -1.588e+03\n",
      " -1.645e+03  1.686e+03 -1.690e+03  1.747e+03 -1.765e+03 -1.851e+03\n",
      " -1.862e+03 -1.918e+03 -1.935e+03 -1.964e+03 -2.032e+03  2.100e+03\n",
      "  2.197e+03 -2.267e+03 -3.196e+03 -3.519e+03 -3.720e+03]\n",
      "durations 7.0 3540.0\n",
      "Concordance Index 0.5188227241615332\n",
      "Integrated Brier Score: 0.20467103481317614\n",
      "(311, 19076) <class 'pandas.core.frame.DataFrame'>\n",
      "(311,) <class 'pandas.core.series.Series'>\n",
      "(78, 19076) <class 'pandas.core.frame.DataFrame'>\n",
      "(78,) <class 'pandas.core.series.Series'>\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m2.6749\u001b[0m  0.0170\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      2        2.6749  0.0170\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      3        2.6749  0.0146\n",
      "      4        2.6749  0.0129\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m2.4327\u001b[0m  0.0177\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m2.7557\u001b[0m  0.0207\n",
      "      5        2.6749  0.0132\n",
      "      2        2.4327  0.0160\n",
      "      6        2.6749  0.0162\n",
      "      2        2.7557  0.0256\n",
      "      3        2.4327  0.0217\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m2.6984\u001b[0m  0.0564\n",
      "      4        2.4327  0.0141\n",
      "      7        2.6749  0.0294\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      3        2.7557  0.0376\n",
      "      8        2.6749  0.0144\n",
      "      5        2.4327  0.0183\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      6        2.4327  0.0137\n",
      "      4        2.7557  0.0172\n",
      "      9        2.6749  0.0173\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2        2.6984  0.0427\n",
      "      7        2.4327  0.0172\n",
      "      5        2.7557  0.0171\n",
      "     10        2.6749  0.0207\n",
      "Restoring best model from epoch 1.\n",
      "      8        2.4327  0.0142\n",
      "      6        2.7557  0.0169\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m2.3230\u001b[0m  0.0179\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      7        2.7557  0.0213\n",
      "      3        2.6984  0.0484\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      9        2.4327  0.0400\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m2.4580\u001b[0m  0.0466\n",
      "      2        2.3230  0.0334\n",
      "      8        2.7557  0.0198\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "     10        2.4327  0.0186\n",
      "Restoring best model from epoch 1.\n",
      "      3        2.3230  0.0219\n",
      "      9        2.7557  0.0202\n",
      "      4        2.6984  0.0435\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "     10        2.7557  0.0136\n",
      "Restoring best model from epoch 1.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m2.4198\u001b[0m  0.0236\n",
      "      4        2.3230  0.0221\n",
      "      2        2.4580  0.0468\n",
      "      5        2.3230  0.0149\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m2.3644\u001b[0m  0.0498\n",
      "      2        2.4198  0.0216\n",
      "      5        2.6984  0.0462\n",
      "      6        2.3230  0.0160\n",
      "      3        2.4198  0.0173\n",
      "      3        2.4580  0.0479\n",
      "      4        2.4198  0.0144\n",
      "      7        2.3230  0.0212\n",
      "      2        2.3644  0.0476\n",
      "      8        2.3230  0.0150\n",
      "      6        2.6984  0.0421\n",
      "      5        2.4198  0.0244\n",
      "      9        2.3230  0.0170\n",
      "      4        2.4580  0.0418\n",
      "      6        2.4198  0.0154\n",
      "     10        2.3230  0.0157\n",
      "Restoring best model from epoch 1.\n",
      "      7        2.4198  0.0146\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m2.7848\u001b[0m  0.0662\n",
      "      3        2.3644  0.0447\n",
      "      7        2.6984  0.0454\n",
      "      8        2.4198  0.0153\n",
      "      5        2.4580  0.0380\n",
      "      9        2.4198  0.0139\n",
      "      2        2.7848  0.0376\n",
      "     10        2.4198  0.0119\n",
      "      4        2.3644  0.0374\n",
      "Restoring best model from epoch 1.\n",
      "      8        2.6984  0.0396\n",
      "      6        2.4580  0.0381\n",
      "      3        2.7848  0.0379\n",
      "      5        2.3644  0.0377\n",
      "      9        2.6984  0.0352\n",
      "      7        2.4580  0.0370\n",
      "      4        2.7848  0.0379\n",
      "      6        2.3644  0.0376\n",
      "     10        2.6984  0.0385\n",
      "Restoring best model from epoch 1.\n",
      "      8        2.4580  0.0372\n",
      "      5        2.7848  0.0369\n",
      "      7        2.3644  0.0354\n",
      "      9        2.4580  0.0354\n",
      "      6        2.7848  0.0351\n",
      "      8        2.3644  0.0384\n",
      "     10        2.4580  0.0358\n",
      "Restoring best model from epoch 1.\n",
      "      7        2.7848  0.0357\n",
      "      9        2.3644  0.0387\n",
      "      8        2.7848  0.0358\n",
      "     10        2.3644  0.0382\n",
      "Restoring best model from epoch 1.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m2.4169\u001b[0m  0.0626\n",
      "      9        2.7848  0.0355\n",
      "      2        2.4169  0.0369\n",
      "     10        2.7848  0.0361\n",
      "Restoring best model from epoch 1.\n",
      "      3        2.4169  0.0350\n",
      "      4        2.4169  0.0355\n",
      "      5        2.4169  0.0348\n",
      "      6        2.4169  0.0450\n",
      "      7        2.4169  0.0367\n",
      "      8        2.4169  0.0342\n",
      "      9        2.4169  0.0345\n",
      "     10        2.4169  0.0337\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.2039\u001b[0m  0.0207\n",
      "      2        3.2039  0.0352\n",
      "      3        3.2039  0.0328\n",
      "      4        3.2039  0.0440\n",
      "      5        3.2039  0.0218\n",
      "      6        3.2039  0.0271\n",
      "      7        3.2039  0.0196\n",
      "      8        3.2039  0.0273\n",
      "      9        3.2039  0.0180\n",
      "     10        3.2039  0.0273\n",
      "Restoring best model from epoch 1.\n",
      "y_train breslow final [ 3.000e+00 -7.000e+00  8.000e+00 -8.000e+00 -2.000e+01 -2.100e+01\n",
      " -2.100e+01  2.200e+01 -2.300e+01  2.400e+01 -2.900e+01  3.000e+01\n",
      "  3.100e+01 -3.100e+01 -3.400e+01 -3.500e+01  4.500e+01  5.200e+01\n",
      "  5.700e+01  6.100e+01 -6.400e+01  6.700e+01  7.600e+01  8.100e+01\n",
      "  8.100e+01  8.200e+01 -9.200e+01 -9.900e+01  1.030e+02  1.050e+02\n",
      "  1.050e+02  1.060e+02  1.130e+02 -1.130e+02  1.220e+02  1.240e+02\n",
      "  1.240e+02  1.320e+02  1.380e+02  1.410e+02  1.530e+02 -1.560e+02\n",
      " -1.640e+02  1.680e+02 -1.700e+02 -1.760e+02  1.800e+02 -1.830e+02\n",
      "  1.850e+02  1.850e+02  1.880e+02 -1.890e+02  1.910e+02  1.920e+02\n",
      " -1.980e+02 -2.000e+02 -2.000e+02  2.000e+02  2.010e+02  2.050e+02\n",
      "  2.120e+02  2.180e+02 -2.240e+02 -2.290e+02  2.290e+02  2.350e+02\n",
      " -2.430e+02  2.430e+02  2.430e+02  2.500e+02  2.550e+02  2.590e+02\n",
      "  2.620e+02  2.720e+02 -2.730e+02  2.740e+02  2.760e+02  2.790e+02\n",
      " -2.800e+02 -2.800e+02  2.810e+02  2.820e+02  2.840e+02  2.840e+02\n",
      " -2.880e+02  2.890e+02  2.910e+02  2.920e+02  2.940e+02  2.960e+02\n",
      "  3.000e+02  3.120e+02 -3.230e+02 -3.250e+02 -3.370e+02  3.410e+02\n",
      "  3.420e+02 -3.420e+02 -3.440e+02  3.440e+02  3.480e+02  3.530e+02\n",
      "  3.540e+02 -3.560e+02  3.560e+02 -3.560e+02 -3.580e+02  3.580e+02\n",
      " -3.650e+02  3.660e+02 -3.670e+02 -3.680e+02 -3.710e+02 -3.740e+02\n",
      " -3.740e+02 -3.750e+02 -3.750e+02  3.760e+02 -3.770e+02  3.780e+02\n",
      " -3.780e+02 -3.790e+02 -3.830e+02 -3.840e+02 -3.850e+02 -3.880e+02\n",
      "  3.890e+02 -3.890e+02 -3.890e+02 -3.920e+02 -3.940e+02 -3.960e+02\n",
      "  3.980e+02  4.010e+02  4.060e+02  4.060e+02 -4.080e+02 -4.110e+02\n",
      " -4.110e+02 -4.130e+02 -4.160e+02 -4.190e+02  4.220e+02 -4.270e+02\n",
      " -4.280e+02  4.280e+02 -4.340e+02  4.390e+02 -4.400e+02 -4.490e+02\n",
      " -4.630e+02  4.660e+02 -4.680e+02  4.760e+02 -4.760e+02  4.770e+02\n",
      " -4.790e+02 -4.850e+02 -4.850e+02 -4.860e+02  4.910e+02  4.910e+02\n",
      " -4.910e+02 -4.940e+02  4.960e+02 -5.000e+02  5.070e+02 -5.110e+02\n",
      "  5.130e+02 -5.230e+02 -5.230e+02  5.260e+02  5.330e+02 -5.430e+02\n",
      "  5.490e+02  5.520e+02  5.540e+02  5.580e+02 -5.590e+02  5.600e+02\n",
      "  5.620e+02 -5.640e+02 -5.660e+02  5.700e+02 -5.720e+02 -5.730e+02\n",
      " -5.790e+02 -5.800e+02 -5.820e+02  5.880e+02 -5.930e+02 -5.940e+02\n",
      " -5.940e+02 -6.000e+02 -6.070e+02  6.070e+02 -6.130e+02 -6.150e+02\n",
      " -6.160e+02  6.180e+02 -6.210e+02 -6.250e+02 -6.280e+02  6.330e+02\n",
      "  6.350e+02 -6.360e+02 -6.410e+02 -6.430e+02 -6.470e+02 -6.500e+02\n",
      "  6.520e+02 -6.640e+02  6.690e+02 -6.750e+02 -6.780e+02 -6.780e+02\n",
      " -6.790e+02 -6.900e+02 -6.920e+02 -6.940e+02 -6.990e+02 -7.250e+02\n",
      " -7.360e+02 -7.380e+02 -7.390e+02 -7.420e+02 -7.520e+02  7.620e+02\n",
      "  7.660e+02  7.820e+02 -7.850e+02  7.920e+02  8.010e+02  8.050e+02\n",
      " -8.120e+02 -8.130e+02 -8.380e+02 -8.380e+02 -8.560e+02 -8.620e+02\n",
      " -8.640e+02  8.690e+02  8.740e+02 -8.810e+02  8.810e+02 -8.950e+02\n",
      " -8.990e+02 -9.000e+02 -9.120e+02 -9.280e+02  9.400e+02 -9.400e+02\n",
      " -9.420e+02 -9.460e+02 -9.490e+02 -9.510e+02 -9.720e+02 -9.910e+02\n",
      " -9.970e+02 -1.000e+03 -1.010e+03 -1.016e+03 -1.023e+03 -1.038e+03\n",
      "  1.043e+03 -1.055e+03 -1.072e+03 -1.083e+03 -1.090e+03  1.095e+03\n",
      " -1.106e+03 -1.108e+03 -1.124e+03 -1.132e+03 -1.133e+03 -1.138e+03\n",
      " -1.145e+03  1.153e+03 -1.184e+03 -1.190e+03 -1.210e+03 -1.236e+03\n",
      "  1.294e+03 -1.297e+03 -1.319e+03 -1.328e+03 -1.367e+03 -1.389e+03\n",
      "  1.407e+03 -1.431e+03 -1.484e+03 -1.588e+03 -1.645e+03 -1.646e+03\n",
      "  1.686e+03 -1.690e+03  1.747e+03 -1.765e+03  1.811e+03 -1.851e+03\n",
      " -1.862e+03 -1.935e+03 -2.032e+03  2.100e+03 -2.171e+03  2.197e+03\n",
      " -2.351e+03 -3.196e+03 -3.519e+03 -3.540e+03 -3.720e+03]\n",
      "Concordance Index 0.5045530492898914\n",
      "Integrated Brier Score: 0.22628685595886572\n",
      "y_train breslow final [ 3.000e+00 -7.000e+00  8.000e+00 -8.000e+00 -2.000e+01 -2.100e+01\n",
      " -2.100e+01  2.200e+01 -2.300e+01  2.400e+01 -2.900e+01  3.000e+01\n",
      "  3.100e+01 -3.100e+01 -3.400e+01 -3.500e+01  4.500e+01  5.200e+01\n",
      "  5.700e+01  6.100e+01 -6.400e+01  6.700e+01  7.600e+01  8.100e+01\n",
      "  8.100e+01  8.200e+01 -9.200e+01 -9.900e+01  1.030e+02  1.050e+02\n",
      "  1.050e+02  1.060e+02  1.130e+02 -1.130e+02  1.220e+02  1.240e+02\n",
      "  1.240e+02  1.320e+02  1.380e+02  1.410e+02  1.530e+02 -1.560e+02\n",
      " -1.640e+02  1.680e+02 -1.700e+02 -1.760e+02  1.800e+02 -1.830e+02\n",
      "  1.850e+02  1.850e+02  1.880e+02 -1.890e+02  1.910e+02  1.920e+02\n",
      " -1.980e+02 -2.000e+02 -2.000e+02  2.000e+02  2.010e+02  2.050e+02\n",
      "  2.120e+02  2.180e+02 -2.240e+02 -2.290e+02  2.290e+02  2.350e+02\n",
      " -2.430e+02  2.430e+02  2.430e+02  2.500e+02  2.550e+02  2.590e+02\n",
      "  2.620e+02  2.720e+02 -2.730e+02  2.740e+02  2.760e+02  2.790e+02\n",
      " -2.800e+02 -2.800e+02  2.810e+02  2.820e+02  2.840e+02  2.840e+02\n",
      " -2.880e+02  2.890e+02  2.910e+02  2.920e+02  2.940e+02  2.960e+02\n",
      "  3.000e+02  3.120e+02 -3.230e+02 -3.250e+02 -3.370e+02  3.410e+02\n",
      "  3.420e+02 -3.420e+02 -3.440e+02  3.440e+02  3.480e+02  3.530e+02\n",
      "  3.540e+02 -3.560e+02  3.560e+02 -3.560e+02 -3.580e+02  3.580e+02\n",
      " -3.650e+02  3.660e+02 -3.670e+02 -3.680e+02 -3.710e+02 -3.740e+02\n",
      " -3.740e+02 -3.750e+02 -3.750e+02  3.760e+02 -3.770e+02  3.780e+02\n",
      " -3.780e+02 -3.790e+02 -3.830e+02 -3.840e+02 -3.850e+02 -3.880e+02\n",
      "  3.890e+02 -3.890e+02 -3.890e+02 -3.920e+02 -3.940e+02 -3.960e+02\n",
      "  3.980e+02  4.010e+02  4.060e+02  4.060e+02 -4.080e+02 -4.110e+02\n",
      " -4.110e+02 -4.130e+02 -4.160e+02 -4.190e+02  4.220e+02 -4.270e+02\n",
      " -4.280e+02  4.280e+02 -4.340e+02  4.390e+02 -4.400e+02 -4.490e+02\n",
      " -4.630e+02  4.660e+02 -4.680e+02  4.760e+02 -4.760e+02  4.770e+02\n",
      " -4.790e+02 -4.850e+02 -4.850e+02 -4.860e+02  4.910e+02  4.910e+02\n",
      " -4.910e+02 -4.940e+02  4.960e+02 -5.000e+02  5.070e+02 -5.110e+02\n",
      "  5.130e+02 -5.230e+02 -5.230e+02  5.260e+02  5.330e+02 -5.430e+02\n",
      "  5.490e+02  5.520e+02  5.540e+02  5.580e+02 -5.590e+02  5.600e+02\n",
      "  5.620e+02 -5.640e+02 -5.660e+02  5.700e+02 -5.720e+02 -5.730e+02\n",
      " -5.790e+02 -5.800e+02 -5.820e+02  5.880e+02 -5.930e+02 -5.940e+02\n",
      " -5.940e+02 -6.000e+02 -6.070e+02  6.070e+02 -6.130e+02 -6.150e+02\n",
      " -6.160e+02  6.180e+02 -6.210e+02 -6.250e+02 -6.280e+02  6.330e+02\n",
      "  6.350e+02 -6.360e+02 -6.410e+02 -6.430e+02 -6.470e+02 -6.500e+02\n",
      "  6.520e+02 -6.640e+02  6.690e+02 -6.750e+02 -6.780e+02 -6.780e+02\n",
      " -6.790e+02 -6.900e+02 -6.920e+02 -6.940e+02 -6.990e+02 -7.250e+02\n",
      " -7.360e+02 -7.380e+02 -7.390e+02 -7.420e+02 -7.520e+02  7.620e+02\n",
      "  7.660e+02  7.820e+02 -7.850e+02  7.920e+02  8.010e+02  8.050e+02\n",
      " -8.120e+02 -8.130e+02 -8.380e+02 -8.380e+02 -8.560e+02 -8.620e+02\n",
      " -8.640e+02  8.690e+02  8.740e+02 -8.810e+02  8.810e+02 -8.950e+02\n",
      " -8.990e+02 -9.000e+02 -9.120e+02 -9.280e+02  9.400e+02 -9.400e+02\n",
      " -9.420e+02 -9.460e+02 -9.490e+02 -9.510e+02 -9.720e+02 -9.910e+02\n",
      " -9.970e+02 -1.000e+03 -1.010e+03 -1.016e+03 -1.023e+03 -1.038e+03\n",
      "  1.043e+03 -1.055e+03 -1.072e+03 -1.083e+03 -1.090e+03  1.095e+03\n",
      " -1.106e+03 -1.108e+03 -1.124e+03 -1.132e+03 -1.133e+03 -1.138e+03\n",
      " -1.145e+03  1.153e+03 -1.184e+03 -1.190e+03 -1.210e+03 -1.236e+03\n",
      "  1.294e+03 -1.297e+03 -1.319e+03 -1.328e+03 -1.367e+03 -1.389e+03\n",
      "  1.407e+03 -1.431e+03 -1.484e+03 -1.588e+03 -1.645e+03 -1.646e+03\n",
      "  1.686e+03 -1.690e+03  1.747e+03 -1.765e+03  1.811e+03 -1.851e+03\n",
      " -1.862e+03 -1.935e+03 -2.032e+03  2.100e+03 -2.171e+03  2.197e+03\n",
      " -2.351e+03 -3.196e+03 -3.519e+03 -3.540e+03 -3.720e+03]\n",
      "durations 14.0 2267.0\n",
      "Concordance Index 0.4557522123893805\n",
      "Integrated Brier Score: 0.22272581931978805\n",
      "(311, 19076) <class 'pandas.core.frame.DataFrame'>\n",
      "(311,) <class 'pandas.core.series.Series'>\n",
      "(78, 19076) <class 'pandas.core.frame.DataFrame'>\n",
      "(78,) <class 'pandas.core.series.Series'>\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.8747\u001b[0m  0.0201\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m2.9868\u001b[0m  0.0220\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      2        3.8747  0.0361\n",
      "      2        2.9868  0.0381\n",
      "      3        2.9868  0.0147\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      3        3.8747  0.0328\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.1444\u001b[0m  0.0697\n",
      "      4        2.9868  0.0320\n",
      "      4        3.8747  0.0226\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2        3.1444  0.0231\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      5        3.8747  0.0235\n",
      "      6        3.8747  0.0223\n",
      "      5        2.9868  0.0588\n",
      "      3        3.1444  0.0413\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m2.9918\u001b[0m  0.0484\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      4        3.1444  0.0260\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      6        2.9868  0.0334\n",
      "      5        3.1444  0.0172\n",
      "      7        3.8747  0.0555\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.0676\u001b[0m  0.0589\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      7        2.9868  0.0347\n",
      "      6        3.1444  0.0295\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.0695\u001b[0m  0.0325\n",
      "      2        2.9918  0.0792\n",
      "      7        3.1444  0.0197\n",
      "      8        3.8747  0.0463\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      8        2.9868  0.0345\n",
      "      2        3.0676  0.0463\n",
      "      9        3.8747  0.0211\n",
      "      2        3.0695  0.0408\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.1551\u001b[0m  0.1255\n",
      "      9        2.9868  0.0259\n",
      "      3        3.0695  0.0252\n",
      "      8        3.1444  0.0492\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "     10        3.8747  0.0324\n",
      "Restoring best model from epoch 1.\n",
      "     10        2.9868  0.0248\n",
      "Restoring best model from epoch 1.\n",
      "      9        3.1444  0.0231\n",
      "      3        3.0676  0.0789\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      3        2.9918  0.1019\n",
      "      4        3.0695  0.0529\n",
      "     10        3.1444  0.0307\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.4882\u001b[0m  0.0945\n",
      "Restoring best model from epoch 1.\n",
      "      2        3.1551  0.0929\n",
      "      5        3.0695  0.0340\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.4655\u001b[0m  0.0384\n",
      "      2        3.4655  0.0158\n",
      "      6        3.0695  0.0350\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.8693\u001b[0m  0.0723\n",
      "      2        3.4882  0.0696\n",
      "      3        3.4655  0.0254\n",
      "      7        3.0695  0.0145\n",
      "      4        2.9918  0.0925\n",
      "      4        3.0676  0.1005\n",
      "      3        3.1551  0.0747\n",
      "      4        3.4655  0.0211\n",
      "      2        3.8693  0.0432\n",
      "      8        3.0695  0.0360\n",
      "      5        3.4655  0.0256\n",
      "      9        3.0695  0.0202\n",
      "      3        3.4882  0.0770\n",
      "      5        2.9918  0.0644\n",
      "     10        3.0695  0.0147\n",
      "Restoring best model from epoch 1.\n",
      "      6        3.4655  0.0324\n",
      "      3        3.8693  0.0472\n",
      "      5        3.0676  0.0823\n",
      "      4        3.1551  0.0860\n",
      "      7        3.4655  0.0178\n",
      "      4        3.4882  0.0420\n",
      "      6        2.9918  0.0554\n",
      "      8        3.4655  0.0314\n",
      "      5        3.1551  0.0438\n",
      "      6        3.0676  0.0592\n",
      "      4        3.8693  0.0714\n",
      "      5        3.4882  0.0801\n",
      "      7        2.9918  0.0644\n",
      "      6        3.1551  0.0618\n",
      "      9        3.4655  0.0763\n",
      "      7        3.0676  0.0656\n",
      "      5        3.8693  0.0708\n",
      "     10        3.4655  0.0245\n",
      "Restoring best model from epoch 1.\n",
      "      6        3.4882  0.0828\n",
      "      7        3.1551  0.0787\n",
      "      8        2.9918  0.0890\n",
      "      8        3.0676  0.0815\n",
      "      6        3.8693  0.0918\n",
      "      9        2.9918  0.0490\n",
      "      9        3.0676  0.0504\n",
      "      7        3.4882  0.0713\n",
      "      8        3.1551  0.0710\n",
      "      7        3.8693  0.0506\n",
      "     10        2.9918  0.0580\n",
      "     10        3.0676  0.0500\n",
      "Restoring best model from epoch 1.\n",
      "Restoring best model from epoch 1.\n",
      "      8        3.4882  0.0568\n",
      "      9        3.1551  0.0551\n",
      "      8        3.8693  0.0701\n",
      "     10        3.1551  0.0736\n",
      "Restoring best model from epoch 1.\n",
      "      9        3.4882  0.0796\n",
      "      9        3.8693  0.0603\n",
      "     10        3.4882  0.0439\n",
      "Restoring best model from epoch 1.\n",
      "     10        3.8693  0.0517\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m2.9783\u001b[0m  0.0157\n",
      "      2        2.9783  0.0181\n",
      "      3        2.9783  0.0116\n",
      "      4        2.9783  0.0265\n",
      "      5        2.9783  0.0112\n",
      "      6        2.9783  0.0147\n",
      "      7        2.9783  0.0129\n",
      "      8        2.9783  0.0134\n",
      "      9        2.9783  0.0124\n",
      "     10        2.9783  0.0186\n",
      "Restoring best model from epoch 1.\n",
      "y_train breslow final [ 3.000e+00 -7.000e+00 -8.000e+00 -1.400e+01 -1.600e+01 -1.700e+01\n",
      " -2.000e+01 -2.100e+01 -2.100e+01 -2.300e+01  3.000e+01 -3.000e+01\n",
      "  3.100e+01 -3.100e+01 -3.400e+01 -3.500e+01  4.500e+01 -4.600e+01\n",
      "  5.200e+01  6.100e+01 -6.400e+01  6.700e+01 -7.700e+01  8.100e+01\n",
      "  8.100e+01  8.200e+01 -9.200e+01 -9.900e+01  1.030e+02  1.050e+02\n",
      "  1.050e+02  1.060e+02 -1.130e+02  1.220e+02  1.240e+02  1.240e+02\n",
      " -1.310e+02  1.320e+02  1.380e+02  1.410e+02  1.560e+02 -1.640e+02\n",
      "  1.660e+02  1.680e+02 -1.700e+02  1.740e+02 -1.760e+02  1.800e+02\n",
      "  1.800e+02  1.850e+02  1.850e+02  1.880e+02 -1.890e+02  1.910e+02\n",
      " -1.980e+02 -2.000e+02 -2.000e+02  2.000e+02  2.000e+02  2.010e+02\n",
      "  2.120e+02  2.150e+02 -2.240e+02 -2.250e+02  2.290e+02  2.350e+02\n",
      " -2.430e+02  2.430e+02 -2.450e+02  2.450e+02  2.550e+02  2.590e+02\n",
      "  2.620e+02  2.720e+02  2.730e+02  2.740e+02  2.750e+02  2.790e+02\n",
      " -2.800e+02 -2.800e+02  2.820e+02  2.840e+02  2.840e+02 -2.870e+02\n",
      " -2.880e+02  2.890e+02  2.910e+02  2.920e+02  2.940e+02  3.000e+02\n",
      "  3.120e+02 -3.250e+02 -3.350e+02  3.360e+02 -3.370e+02  3.410e+02\n",
      "  3.420e+02 -3.420e+02 -3.440e+02  3.440e+02 -3.460e+02  3.480e+02\n",
      "  3.480e+02  3.540e+02 -3.560e+02  3.560e+02 -3.580e+02  3.580e+02\n",
      "  3.590e+02  3.660e+02 -3.670e+02  3.700e+02 -3.710e+02 -3.740e+02\n",
      " -3.750e+02 -3.750e+02  3.770e+02 -3.770e+02  3.780e+02 -3.780e+02\n",
      " -3.780e+02 -3.790e+02 -3.810e+02 -3.830e+02 -3.830e+02 -3.840e+02\n",
      " -3.850e+02  3.890e+02 -3.890e+02 -3.900e+02 -3.920e+02 -3.940e+02\n",
      " -3.960e+02  3.960e+02  3.980e+02 -4.000e+02  4.010e+02  4.030e+02\n",
      "  4.060e+02  4.060e+02 -4.080e+02 -4.130e+02 -4.160e+02 -4.190e+02\n",
      "  4.220e+02  4.220e+02  4.260e+02 -4.270e+02 -4.270e+02 -4.280e+02\n",
      " -4.310e+02 -4.340e+02  4.460e+02 -4.490e+02 -4.500e+02  4.570e+02\n",
      " -4.630e+02  4.660e+02 -4.680e+02  4.740e+02 -4.760e+02  4.770e+02\n",
      " -4.790e+02 -4.850e+02 -4.850e+02 -4.860e+02 -4.910e+02  4.960e+02\n",
      "  5.070e+02 -5.110e+02 -5.190e+02 -5.210e+02 -5.230e+02 -5.250e+02\n",
      "  5.260e+02  5.330e+02  5.430e+02 -5.430e+02  5.490e+02  5.520e+02\n",
      "  5.540e+02  5.540e+02  5.600e+02  5.620e+02 -5.640e+02 -5.660e+02\n",
      "  5.700e+02 -5.720e+02 -5.770e+02 -5.790e+02 -5.800e+02 -5.820e+02\n",
      "  5.880e+02 -5.930e+02 -5.940e+02 -5.940e+02 -5.950e+02 -6.000e+02\n",
      " -6.070e+02  6.070e+02 -6.130e+02 -6.150e+02 -6.160e+02  6.180e+02\n",
      " -6.250e+02 -6.280e+02  6.350e+02 -6.360e+02  6.400e+02 -6.410e+02\n",
      " -6.430e+02 -6.440e+02 -6.470e+02 -6.500e+02  6.520e+02  6.610e+02\n",
      " -6.640e+02 -6.660e+02  6.690e+02  6.750e+02 -6.780e+02 -6.780e+02\n",
      " -6.900e+02 -6.920e+02 -6.940e+02  7.120e+02 -7.240e+02 -7.250e+02\n",
      " -7.360e+02 -7.380e+02 -7.390e+02 -7.420e+02 -7.520e+02 -7.540e+02\n",
      "  7.660e+02  7.790e+02  7.820e+02 -7.850e+02  7.920e+02  7.940e+02\n",
      "  8.010e+02  8.050e+02 -8.120e+02 -8.130e+02 -8.190e+02 -8.250e+02\n",
      "  8.320e+02 -8.560e+02  8.690e+02  8.740e+02 -8.810e+02  8.810e+02\n",
      " -8.820e+02 -8.950e+02 -8.990e+02 -9.000e+02 -9.120e+02 -9.280e+02\n",
      "  9.400e+02 -9.400e+02 -9.420e+02 -9.420e+02 -9.460e+02 -9.510e+02\n",
      " -9.720e+02 -9.810e+02 -9.890e+02 -9.910e+02 -9.970e+02 -1.000e+03\n",
      " -1.002e+03 -1.010e+03 -1.016e+03 -1.038e+03 -1.083e+03 -1.090e+03\n",
      " -1.100e+03 -1.106e+03 -1.108e+03 -1.132e+03 -1.138e+03  1.153e+03\n",
      " -1.160e+03 -1.184e+03 -1.190e+03 -1.200e+03 -1.210e+03 -1.223e+03\n",
      "  1.294e+03 -1.297e+03 -1.319e+03 -1.328e+03 -1.389e+03  1.407e+03\n",
      " -1.431e+03 -1.588e+03 -1.645e+03 -1.646e+03  1.747e+03  1.811e+03\n",
      " -1.851e+03 -1.862e+03 -1.918e+03 -1.964e+03 -2.032e+03 -2.171e+03\n",
      "  2.197e+03 -2.267e+03 -2.351e+03 -3.519e+03 -3.540e+03]\n",
      "Concordance Index 0.49337790921949337\n",
      "Integrated Brier Score: 0.20592792704236348\n",
      "y_train breslow final [ 3.000e+00 -7.000e+00 -8.000e+00 -1.400e+01 -1.600e+01 -1.700e+01\n",
      " -2.000e+01 -2.100e+01 -2.100e+01 -2.300e+01  3.000e+01 -3.000e+01\n",
      "  3.100e+01 -3.100e+01 -3.400e+01 -3.500e+01  4.500e+01 -4.600e+01\n",
      "  5.200e+01  6.100e+01 -6.400e+01  6.700e+01 -7.700e+01  8.100e+01\n",
      "  8.100e+01  8.200e+01 -9.200e+01 -9.900e+01  1.030e+02  1.050e+02\n",
      "  1.050e+02  1.060e+02 -1.130e+02  1.220e+02  1.240e+02  1.240e+02\n",
      " -1.310e+02  1.320e+02  1.380e+02  1.410e+02  1.560e+02 -1.640e+02\n",
      "  1.660e+02  1.680e+02 -1.700e+02  1.740e+02 -1.760e+02  1.800e+02\n",
      "  1.800e+02  1.850e+02  1.850e+02  1.880e+02 -1.890e+02  1.910e+02\n",
      " -1.980e+02 -2.000e+02 -2.000e+02  2.000e+02  2.000e+02  2.010e+02\n",
      "  2.120e+02  2.150e+02 -2.240e+02 -2.250e+02  2.290e+02  2.350e+02\n",
      " -2.430e+02  2.430e+02 -2.450e+02  2.450e+02  2.550e+02  2.590e+02\n",
      "  2.620e+02  2.720e+02  2.730e+02  2.740e+02  2.750e+02  2.790e+02\n",
      " -2.800e+02 -2.800e+02  2.820e+02  2.840e+02  2.840e+02 -2.870e+02\n",
      " -2.880e+02  2.890e+02  2.910e+02  2.920e+02  2.940e+02  3.000e+02\n",
      "  3.120e+02 -3.250e+02 -3.350e+02  3.360e+02 -3.370e+02  3.410e+02\n",
      "  3.420e+02 -3.420e+02 -3.440e+02  3.440e+02 -3.460e+02  3.480e+02\n",
      "  3.480e+02  3.540e+02 -3.560e+02  3.560e+02 -3.580e+02  3.580e+02\n",
      "  3.590e+02  3.660e+02 -3.670e+02  3.700e+02 -3.710e+02 -3.740e+02\n",
      " -3.750e+02 -3.750e+02  3.770e+02 -3.770e+02  3.780e+02 -3.780e+02\n",
      " -3.780e+02 -3.790e+02 -3.810e+02 -3.830e+02 -3.830e+02 -3.840e+02\n",
      " -3.850e+02  3.890e+02 -3.890e+02 -3.900e+02 -3.920e+02 -3.940e+02\n",
      " -3.960e+02  3.960e+02  3.980e+02 -4.000e+02  4.010e+02  4.030e+02\n",
      "  4.060e+02  4.060e+02 -4.080e+02 -4.130e+02 -4.160e+02 -4.190e+02\n",
      "  4.220e+02  4.220e+02  4.260e+02 -4.270e+02 -4.270e+02 -4.280e+02\n",
      " -4.310e+02 -4.340e+02  4.460e+02 -4.490e+02 -4.500e+02  4.570e+02\n",
      " -4.630e+02  4.660e+02 -4.680e+02  4.740e+02 -4.760e+02  4.770e+02\n",
      " -4.790e+02 -4.850e+02 -4.850e+02 -4.860e+02 -4.910e+02  4.960e+02\n",
      "  5.070e+02 -5.110e+02 -5.190e+02 -5.210e+02 -5.230e+02 -5.250e+02\n",
      "  5.260e+02  5.330e+02  5.430e+02 -5.430e+02  5.490e+02  5.520e+02\n",
      "  5.540e+02  5.540e+02  5.600e+02  5.620e+02 -5.640e+02 -5.660e+02\n",
      "  5.700e+02 -5.720e+02 -5.770e+02 -5.790e+02 -5.800e+02 -5.820e+02\n",
      "  5.880e+02 -5.930e+02 -5.940e+02 -5.940e+02 -5.950e+02 -6.000e+02\n",
      " -6.070e+02  6.070e+02 -6.130e+02 -6.150e+02 -6.160e+02  6.180e+02\n",
      " -6.250e+02 -6.280e+02  6.350e+02 -6.360e+02  6.400e+02 -6.410e+02\n",
      " -6.430e+02 -6.440e+02 -6.470e+02 -6.500e+02  6.520e+02  6.610e+02\n",
      " -6.640e+02 -6.660e+02  6.690e+02  6.750e+02 -6.780e+02 -6.780e+02\n",
      " -6.900e+02 -6.920e+02 -6.940e+02  7.120e+02 -7.240e+02 -7.250e+02\n",
      " -7.360e+02 -7.380e+02 -7.390e+02 -7.420e+02 -7.520e+02 -7.540e+02\n",
      "  7.660e+02  7.790e+02  7.820e+02 -7.850e+02  7.920e+02  7.940e+02\n",
      "  8.010e+02  8.050e+02 -8.120e+02 -8.130e+02 -8.190e+02 -8.250e+02\n",
      "  8.320e+02 -8.560e+02  8.690e+02  8.740e+02 -8.810e+02  8.810e+02\n",
      " -8.820e+02 -8.950e+02 -8.990e+02 -9.000e+02 -9.120e+02 -9.280e+02\n",
      "  9.400e+02 -9.400e+02 -9.420e+02 -9.420e+02 -9.460e+02 -9.510e+02\n",
      " -9.720e+02 -9.810e+02 -9.890e+02 -9.910e+02 -9.970e+02 -1.000e+03\n",
      " -1.002e+03 -1.010e+03 -1.016e+03 -1.038e+03 -1.083e+03 -1.090e+03\n",
      " -1.100e+03 -1.106e+03 -1.108e+03 -1.132e+03 -1.138e+03  1.153e+03\n",
      " -1.160e+03 -1.184e+03 -1.190e+03 -1.200e+03 -1.210e+03 -1.223e+03\n",
      "  1.294e+03 -1.297e+03 -1.319e+03 -1.328e+03 -1.389e+03  1.407e+03\n",
      " -1.431e+03 -1.588e+03 -1.645e+03 -1.646e+03  1.747e+03  1.811e+03\n",
      " -1.851e+03 -1.862e+03 -1.918e+03 -1.964e+03 -2.032e+03 -2.171e+03\n",
      "  2.197e+03 -2.267e+03 -2.351e+03 -3.519e+03 -3.540e+03]\n",
      "durations 8.0 3720.0\n",
      "Concordance Index 0.365016501650165\n",
      "Integrated Brier Score: 0.22595841668950778\n",
      "(311, 19076) <class 'pandas.core.frame.DataFrame'>\n",
      "(311,) <class 'pandas.core.series.Series'>\n",
      "(78, 19076) <class 'pandas.core.frame.DataFrame'>\n",
      "(78,) <class 'pandas.core.series.Series'>\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m2.6207\u001b[0m  0.0193\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.1845\u001b[0m  0.0210\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.1224\u001b[0m  0.0138\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2        2.6207  0.0156\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2        3.1845  0.0170\n",
      "      3        2.6207  0.0163\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.1346\u001b[0m  0.0478\n",
      "      2        3.1224  0.0183\n",
      "      3        3.1845  0.0153\n",
      "      4        2.6207  0.0154\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m2.7757\u001b[0m  0.0231\n",
      "      3        3.1224  0.0166\n",
      "      4        3.1845  0.0197\n",
      "      2        2.7757  0.0171\n",
      "      4        3.1224  0.0159\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m2.7833\u001b[0m  0.0489\n",
      "      5        2.6207  0.0243\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      5        3.1845  0.0152\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      5        3.1224  0.0145\n",
      "      2        3.1346  0.0504\n",
      "      6        2.6207  0.0165\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m2.7841\u001b[0m  0.0476\n",
      "      6        3.1224  0.0172\n",
      "      3        2.7757  0.0341\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      7        2.6207  0.0133\n",
      "      6        3.1845  0.0226\n",
      "      7        3.1224  0.0167\n",
      "      8        2.6207  0.0147\n",
      "      7        3.1845  0.0131\n",
      "      3        3.1346  0.0394\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m2.8026\u001b[0m  0.0183\n",
      "      4        2.7757  0.0235\n",
      "      2        2.7833  0.0567\n",
      "      8        3.1845  0.0152\n",
      "      9        2.6207  0.0209\n",
      "      5        2.7757  0.0162\n",
      "      2        2.8026  0.0183\n",
      "      8        3.1224  0.0249\n",
      "      2        2.7841  0.0581\n",
      "      9        3.1845  0.0203\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "     10        2.6207  0.0226\n",
      "Restoring best model from epoch 1.\n",
      "      6        2.7757  0.0221\n",
      "      9        3.1224  0.0221\n",
      "     10        3.1845  0.0175\n",
      "Restoring best model from epoch 1.\n",
      "      3        2.8026  0.0336\n",
      "      3        2.7833  0.0538\n",
      "     10        3.1224  0.0149\n",
      "Restoring best model from epoch 1.\n",
      "      4        2.8026  0.0129\n",
      "      4        3.1346  0.0669\n",
      "      7        2.7757  0.0269\n",
      "      5        2.8026  0.0127\n",
      "      8        2.7757  0.0155\n",
      "      3        2.7841  0.0620\n",
      "      6        2.8026  0.0159\n",
      "      9        2.7757  0.0121\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m2.6167\u001b[0m  0.0693\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.1207\u001b[0m  0.0421\n",
      "      4        2.7833  0.0486\n",
      "     10        2.7757  0.0175\n",
      "Restoring best model from epoch 1.\n",
      "      5        3.1346  0.0476\n",
      "      7        2.8026  0.0196\n",
      "      8        2.8026  0.0169\n",
      "      2        2.6167  0.0453\n",
      "      9        2.8026  0.0116\n",
      "      2        3.1207  0.0445\n",
      "      4        2.7841  0.0575\n",
      "     10        2.8026  0.0120\n",
      "Restoring best model from epoch 1.\n",
      "      6        3.1346  0.0468\n",
      "      5        2.7833  0.0559\n",
      "      3        2.6167  0.0416\n",
      "      5        2.7841  0.0364\n",
      "      3        3.1207  0.0411\n",
      "      6        2.7833  0.0371\n",
      "      6        2.7841  0.0400\n",
      "      4        2.6167  0.0446\n",
      "      7        3.1346  0.0714\n",
      "      7        2.7833  0.0482\n",
      "      4        3.1207  0.0629\n",
      "      7        2.7841  0.0376\n",
      "      8        3.1346  0.0375\n",
      "      5        2.6167  0.0413\n",
      "      8        2.7833  0.0396\n",
      "      5        3.1207  0.0380\n",
      "      8        2.7841  0.0423\n",
      "      9        3.1346  0.0369\n",
      "      6        2.6167  0.0404\n",
      "      9        2.7833  0.0357\n",
      "      6        3.1207  0.0388\n",
      "      9        2.7841  0.0360\n",
      "     10        3.1346  0.0366\n",
      "Restoring best model from epoch 1.\n",
      "      7        2.6167  0.0364\n",
      "     10        2.7833  0.0354\n",
      "Restoring best model from epoch 1.\n",
      "      7        3.1207  0.0367\n",
      "     10        2.7841  0.0366\n",
      "Restoring best model from epoch 1.\n",
      "      8        2.6167  0.0376\n",
      "      8        3.1207  0.0361\n",
      "      9        2.6167  0.0362\n",
      "      9        3.1207  0.0356\n",
      "     10        2.6167  0.0375\n",
      "Restoring best model from epoch 1.\n",
      "     10        3.1207  0.0401\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m2.7396\u001b[0m  0.0251\n",
      "      2        2.7396  0.0199\n",
      "      3        2.7396  0.0185\n",
      "      4        2.7396  0.0179\n",
      "      5        2.7396  0.0179\n",
      "      6        2.7396  0.0205\n",
      "      7        2.7396  0.0226\n",
      "      8        2.7396  0.0175\n",
      "      9        2.7396  0.0219\n",
      "     10        2.7396  0.0220\n",
      "Restoring best model from epoch 1.\n",
      "y_train breslow final [   -7.     8.    -8.   -14.   -16.   -17.   -20.    22.   -23.    24.\n",
      "   -29.   -30.    31.   -31.   -34.   -46.    52.    57.    61.   -64.\n",
      "    67.    76.   -77.    81.    81.    82.   -92.   103.   106.   113.\n",
      "  -113.   124.   124.  -131.   132.   138.   153.   156.  -156.  -164.\n",
      "   166.  -170.   174.   180.   180.  -183.   185.   188.  -189.   191.\n",
      "   192.  -198.  -198.  -200.  -200.   200.   200.   201.   205.   212.\n",
      "   215.   218.  -224.  -225.  -229.   229.   235.  -243.   243.   243.\n",
      "  -245.   245.   250.   259.   273.  -273.   274.   275.   276.  -280.\n",
      "   281.   282.   284.   284.  -287.   291.   292.   296.   300.   300.\n",
      "   312.  -323.  -335.   336.  -337.   342.  -342.  -344.  -344.   344.\n",
      "  -346.   348.   353.   354.  -356.   356.  -356.  -358.   358.   359.\n",
      "  -365.  -368.   370.  -371.  -374.  -374.  -375.   376.   377.  -377.\n",
      "   378.  -378.  -379.  -381.  -383.  -383.  -383.  -384.  -385.  -388.\n",
      "   389.  -389.  -390.  -392.  -394.   396.   398.  -400.   403.   406.\n",
      "  -408.  -411.  -411.  -413.  -419.   422.   422.   426.  -427.  -427.\n",
      "  -428.   428.  -431.   439.  -440.   446.  -449.  -450.   457.   466.\n",
      "  -468.   474.   476.  -476.  -485.  -485.  -486.   491.   491.  -494.\n",
      "   496.  -500.   507.  -511.   513.  -519.  -521.  -523.  -523.  -525.\n",
      "   526.   533.   543.   549.   552.   554.   554.   558.  -559.   560.\n",
      "   562.  -564.   570.  -572.  -573.  -577.  -580.  -582.  -594.  -595.\n",
      "  -600.  -607.   607.  -613.  -616.  -621.  -628.   633.   640.  -641.\n",
      "  -643.  -644.   661.  -664.  -666.   669.  -675.   675.  -678.  -679.\n",
      "  -690.  -692.  -699.   712.  -724.  -725.  -736.  -738.  -739.  -752.\n",
      "  -754.   762.   766.   779.   782.   792.   794.   801.   805.  -813.\n",
      "  -819.  -825.   832.  -838.  -838.  -862.  -864.   869.  -881.  -882.\n",
      "  -895.  -899.  -912.  -912.  -940.  -942.  -942.  -949.  -972.  -981.\n",
      "  -989.  -991.  -997. -1002. -1016. -1023. -1038.  1043. -1055. -1072.\n",
      " -1083. -1090.  1095. -1100. -1108. -1124. -1133. -1145. -1160. -1200.\n",
      " -1223. -1236.  1294. -1319. -1328. -1367. -1389.  1407. -1431. -1484.\n",
      " -1588. -1645. -1646.  1686. -1690. -1765.  1811. -1851. -1862. -1918.\n",
      " -1935. -1964. -2032.  2100. -2171. -2267. -2351. -3196. -3519. -3540.\n",
      " -3720.]\n",
      "Concordance Index 0.5172120696982575\n",
      "Integrated Brier Score: 0.23715803777998168\n",
      "y_train breslow final [   -7.     8.    -8.   -14.   -16.   -17.   -20.    22.   -23.    24.\n",
      "   -29.   -30.    31.   -31.   -34.   -46.    52.    57.    61.   -64.\n",
      "    67.    76.   -77.    81.    81.    82.   -92.   103.   106.   113.\n",
      "  -113.   124.   124.  -131.   132.   138.   153.   156.  -156.  -164.\n",
      "   166.  -170.   174.   180.   180.  -183.   185.   188.  -189.   191.\n",
      "   192.  -198.  -198.  -200.  -200.   200.   200.   201.   205.   212.\n",
      "   215.   218.  -224.  -225.  -229.   229.   235.  -243.   243.   243.\n",
      "  -245.   245.   250.   259.   273.  -273.   274.   275.   276.  -280.\n",
      "   281.   282.   284.   284.  -287.   291.   292.   296.   300.   300.\n",
      "   312.  -323.  -335.   336.  -337.   342.  -342.  -344.  -344.   344.\n",
      "  -346.   348.   353.   354.  -356.   356.  -356.  -358.   358.   359.\n",
      "  -365.  -368.   370.  -371.  -374.  -374.  -375.   376.   377.  -377.\n",
      "   378.  -378.  -379.  -381.  -383.  -383.  -383.  -384.  -385.  -388.\n",
      "   389.  -389.  -390.  -392.  -394.   396.   398.  -400.   403.   406.\n",
      "  -408.  -411.  -411.  -413.  -419.   422.   422.   426.  -427.  -427.\n",
      "  -428.   428.  -431.   439.  -440.   446.  -449.  -450.   457.   466.\n",
      "  -468.   474.   476.  -476.  -485.  -485.  -486.   491.   491.  -494.\n",
      "   496.  -500.   507.  -511.   513.  -519.  -521.  -523.  -523.  -525.\n",
      "   526.   533.   543.   549.   552.   554.   554.   558.  -559.   560.\n",
      "   562.  -564.   570.  -572.  -573.  -577.  -580.  -582.  -594.  -595.\n",
      "  -600.  -607.   607.  -613.  -616.  -621.  -628.   633.   640.  -641.\n",
      "  -643.  -644.   661.  -664.  -666.   669.  -675.   675.  -678.  -679.\n",
      "  -690.  -692.  -699.   712.  -724.  -725.  -736.  -738.  -739.  -752.\n",
      "  -754.   762.   766.   779.   782.   792.   794.   801.   805.  -813.\n",
      "  -819.  -825.   832.  -838.  -838.  -862.  -864.   869.  -881.  -882.\n",
      "  -895.  -899.  -912.  -912.  -940.  -942.  -942.  -949.  -972.  -981.\n",
      "  -989.  -991.  -997. -1002. -1016. -1023. -1038.  1043. -1055. -1072.\n",
      " -1083. -1090.  1095. -1100. -1108. -1124. -1133. -1145. -1160. -1200.\n",
      " -1223. -1236.  1294. -1319. -1328. -1367. -1389.  1407. -1431. -1484.\n",
      " -1588. -1645. -1646.  1686. -1690. -1765.  1811. -1851. -1862. -1918.\n",
      " -1935. -1964. -2032.  2100. -2171. -2267. -2351. -3196. -3519. -3540.\n",
      " -3720.]\n",
      "durations 3.0 2197.0\n",
      "Concordance Index 0.4097796143250689\n",
      "Integrated Brier Score: 0.2104516569913716\n",
      "(312, 19076) <class 'pandas.core.frame.DataFrame'>\n",
      "(312,) <class 'pandas.core.series.Series'>\n",
      "(77, 19076) <class 'pandas.core.frame.DataFrame'>\n",
      "(77,) <class 'pandas.core.series.Series'>\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m2.6581\u001b[0m  0.0194\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m2.7759\u001b[0m  0.0211\n",
      "      2        2.6581  0.0195\n",
      "      2        2.7759  0.0264\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m2.8024\u001b[0m  0.0986\n",
      "      3        2.6581  0.0160\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m2.8974\u001b[0m  0.0297\n",
      "      3        2.7759  0.0228\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m2.3628\u001b[0m  0.0265\n",
      "      4        2.6581  0.0201\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m2.8447\u001b[0m  0.0340\n",
      "      4        2.7759  0.0185\n",
      "      2        2.3628  0.0269\n",
      "      5        2.7759  0.0229\n",
      "      2        2.8024  0.0683\n",
      "      2        2.8447  0.0401\n",
      "      5        2.6581  0.0513\n",
      "      2        2.8974  0.0638\n",
      "      6        2.7759  0.0151\n",
      "      3        2.3628  0.0298\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      3        2.8447  0.0217\n",
      "      6        2.6581  0.0210\n",
      "      4        2.3628  0.0165\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m2.8942\u001b[0m  0.0965\n",
      "      3        2.8974  0.0253\n",
      "      7        2.7759  0.0232\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m2.3929\u001b[0m  0.0776\n",
      "      4        2.8447  0.0181\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      8        2.7759  0.0139\n",
      "      4        2.8974  0.0180\n",
      "      3        2.8024  0.0674\n",
      "      5        2.8447  0.0248\n",
      "      7        2.6581  0.0474\n",
      "      9        2.7759  0.0281\n",
      "      5        2.8974  0.0344\n",
      "      5        2.3628  0.0587\n",
      "      2        2.8942  0.0680\n",
      "      6        2.8447  0.0306\n",
      "      6        2.8974  0.0236\n",
      "     10        2.7759  0.0406\n",
      "Restoring best model from epoch 1.\n",
      "      6        2.3628  0.0220\n",
      "      7        2.8974  0.0177\n",
      "      7        2.8447  0.0278\n",
      "      2        2.3929  0.0946\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m2.6367\u001b[0m  0.0653\n",
      "      7        2.3628  0.0229\n",
      "      8        2.6581  0.0689\n",
      "      8        2.8447  0.0198\n",
      "      4        2.8024  0.0860\n",
      "      3        2.8942  0.0572\n",
      "      8        2.3628  0.0235\n",
      "      9        2.6581  0.0227\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m2.7431\u001b[0m  0.0937\n",
      "      9        2.8447  0.0192\n",
      "      8        2.8974  0.0426\n",
      "      3        2.3929  0.0397\n",
      "     10        2.6581  0.0165\n",
      "Restoring best model from epoch 1.\n",
      "      9        2.3628  0.0198\n",
      "     10        2.8447  0.0194\n",
      "Restoring best model from epoch 1.\n",
      "      9        2.8974  0.0260\n",
      "      2        2.6367  0.0691\n",
      "     10        2.3628  0.0208\n",
      "Restoring best model from epoch 1.\n",
      "      4        2.8942  0.0543\n",
      "      5        2.8024  0.0611\n",
      "     10        2.8974  0.0213\n",
      "      2        2.7431  0.0501\n",
      "Restoring best model from epoch 1.\n",
      "      4        2.3929  0.0567\n",
      "      3        2.6367  0.0433\n",
      "      5        2.8942  0.0592\n",
      "      6        2.8024  0.0577\n",
      "      3        2.7431  0.0572\n",
      "      5        2.3929  0.0638\n",
      "      4        2.6367  0.0513\n",
      "      6        2.8942  0.0568\n",
      "      7        2.8024  0.0548\n",
      "      4        2.7431  0.0573\n",
      "      5        2.6367  0.0422\n",
      "      6        2.3929  0.0589\n",
      "      8        2.8024  0.0426\n",
      "      7        2.8942  0.0438\n",
      "      5        2.7431  0.0423\n",
      "      6        2.6367  0.0407\n",
      "      7        2.3929  0.0452\n",
      "      9        2.8024  0.0389\n",
      "      8        2.8942  0.0383\n",
      "      6        2.7431  0.0369\n",
      "      7        2.6367  0.0368\n",
      "      8        2.3929  0.0367\n",
      "     10        2.8024  0.0357\n",
      "      9        2.8942  0.0374\n",
      "Restoring best model from epoch 1.\n",
      "      7        2.7431  0.0374\n",
      "      8        2.6367  0.0366\n",
      "      9        2.3929  0.0360\n",
      "     10        2.8942  0.0363\n",
      "Restoring best model from epoch 1.\n",
      "      8        2.7431  0.0351\n",
      "      9        2.6367  0.0358\n",
      "     10        2.3929  0.0357\n",
      "Restoring best model from epoch 1.\n",
      "      9        2.7431  0.0374\n",
      "     10        2.6367  0.0379\n",
      "Restoring best model from epoch 1.\n",
      "     10        2.7431  0.0383\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.0727\u001b[0m  0.0127\n",
      "      2        3.0727  0.0128\n",
      "      3        3.0727  0.0103\n",
      "      4        3.0727  0.0082\n",
      "      5        3.0727  0.0115\n",
      "      6        3.0727  0.0108\n",
      "      7        3.0727  0.0119\n",
      "      8        3.0727  0.0109\n",
      "      9        3.0727  0.0125\n",
      "     10        3.0727  0.0088\n",
      "Restoring best model from epoch 1.\n",
      "y_train breslow final [ 3.000e+00 -7.000e+00  8.000e+00 -1.400e+01 -1.600e+01 -1.700e+01\n",
      " -2.000e+01 -2.100e+01 -2.100e+01  2.200e+01  2.400e+01 -2.900e+01\n",
      "  3.000e+01 -3.000e+01  3.100e+01 -3.400e+01 -3.500e+01  4.500e+01\n",
      " -4.600e+01  5.700e+01  6.700e+01  7.600e+01 -7.700e+01  8.100e+01\n",
      "  8.200e+01 -9.200e+01 -9.900e+01  1.030e+02  1.050e+02  1.050e+02\n",
      "  1.130e+02  1.220e+02  1.240e+02  1.240e+02 -1.310e+02  1.320e+02\n",
      "  1.380e+02  1.410e+02  1.530e+02  1.560e+02 -1.560e+02 -1.640e+02\n",
      "  1.660e+02  1.680e+02  1.740e+02 -1.760e+02  1.800e+02 -1.830e+02\n",
      "  1.850e+02 -1.890e+02  1.920e+02 -1.980e+02 -1.980e+02 -2.000e+02\n",
      " -2.000e+02  2.000e+02  2.000e+02  2.050e+02  2.150e+02  2.180e+02\n",
      " -2.240e+02 -2.250e+02 -2.290e+02  2.290e+02 -2.430e+02  2.430e+02\n",
      "  2.430e+02 -2.450e+02  2.450e+02  2.500e+02  2.550e+02  2.590e+02\n",
      "  2.620e+02  2.720e+02  2.730e+02 -2.730e+02  2.740e+02  2.750e+02\n",
      "  2.760e+02  2.790e+02 -2.800e+02 -2.800e+02  2.810e+02  2.820e+02\n",
      "  2.840e+02 -2.870e+02 -2.880e+02  2.890e+02  2.920e+02  2.940e+02\n",
      "  2.960e+02  3.000e+02  3.000e+02  3.120e+02 -3.230e+02 -3.250e+02\n",
      " -3.350e+02  3.360e+02 -3.370e+02  3.410e+02  3.420e+02 -3.420e+02\n",
      " -3.440e+02 -3.440e+02  3.440e+02 -3.460e+02  3.480e+02  3.480e+02\n",
      "  3.530e+02 -3.560e+02 -3.560e+02  3.590e+02 -3.650e+02  3.660e+02\n",
      " -3.670e+02 -3.680e+02  3.700e+02 -3.740e+02 -3.740e+02 -3.750e+02\n",
      " -3.750e+02  3.760e+02  3.770e+02 -3.770e+02 -3.780e+02 -3.780e+02\n",
      " -3.810e+02 -3.830e+02 -3.830e+02 -3.830e+02 -3.840e+02 -3.850e+02\n",
      " -3.880e+02 -3.890e+02 -3.890e+02 -3.900e+02 -3.960e+02  3.960e+02\n",
      " -4.000e+02  4.010e+02  4.030e+02  4.060e+02  4.060e+02 -4.110e+02\n",
      " -4.110e+02 -4.160e+02  4.220e+02  4.260e+02 -4.270e+02 -4.270e+02\n",
      "  4.280e+02 -4.310e+02 -4.340e+02  4.390e+02 -4.400e+02  4.460e+02\n",
      " -4.490e+02 -4.500e+02  4.570e+02 -4.630e+02  4.660e+02 -4.680e+02\n",
      "  4.740e+02  4.760e+02 -4.760e+02  4.770e+02 -4.790e+02 -4.850e+02\n",
      " -4.850e+02  4.910e+02  4.910e+02 -4.910e+02 -4.940e+02 -5.000e+02\n",
      " -5.110e+02  5.130e+02 -5.190e+02 -5.210e+02 -5.230e+02 -5.230e+02\n",
      " -5.250e+02  5.260e+02  5.330e+02  5.430e+02 -5.430e+02  5.520e+02\n",
      "  5.540e+02  5.540e+02  5.580e+02 -5.590e+02  5.620e+02 -5.640e+02\n",
      " -5.660e+02 -5.720e+02 -5.730e+02 -5.770e+02 -5.790e+02  5.880e+02\n",
      " -5.930e+02 -5.940e+02 -5.940e+02 -5.950e+02  6.070e+02 -6.150e+02\n",
      " -6.160e+02  6.180e+02 -6.210e+02 -6.250e+02  6.330e+02  6.350e+02\n",
      " -6.360e+02  6.400e+02 -6.410e+02 -6.430e+02 -6.440e+02 -6.470e+02\n",
      " -6.500e+02  6.520e+02  6.610e+02 -6.660e+02 -6.750e+02  6.750e+02\n",
      " -6.780e+02 -6.780e+02 -6.790e+02 -6.900e+02 -6.920e+02 -6.940e+02\n",
      " -6.990e+02  7.120e+02 -7.240e+02 -7.420e+02 -7.540e+02  7.620e+02\n",
      "  7.660e+02  7.790e+02 -7.850e+02  7.940e+02  8.050e+02 -8.120e+02\n",
      " -8.190e+02 -8.250e+02  8.320e+02 -8.380e+02 -8.380e+02 -8.560e+02\n",
      " -8.620e+02 -8.640e+02  8.740e+02  8.810e+02 -8.820e+02 -8.950e+02\n",
      " -8.990e+02 -9.000e+02 -9.120e+02 -9.120e+02 -9.280e+02  9.400e+02\n",
      " -9.420e+02 -9.460e+02 -9.490e+02 -9.510e+02 -9.810e+02 -9.890e+02\n",
      " -9.970e+02 -1.000e+03 -1.002e+03 -1.010e+03 -1.023e+03  1.043e+03\n",
      " -1.055e+03 -1.072e+03 -1.083e+03 -1.090e+03  1.095e+03 -1.100e+03\n",
      " -1.106e+03 -1.124e+03 -1.132e+03 -1.133e+03 -1.138e+03 -1.145e+03\n",
      "  1.153e+03 -1.160e+03 -1.184e+03 -1.190e+03 -1.200e+03 -1.210e+03\n",
      " -1.223e+03 -1.236e+03 -1.297e+03 -1.367e+03 -1.389e+03  1.407e+03\n",
      " -1.484e+03 -1.646e+03  1.686e+03 -1.690e+03  1.747e+03 -1.765e+03\n",
      "  1.811e+03 -1.918e+03 -1.935e+03 -1.964e+03  2.100e+03 -2.171e+03\n",
      "  2.197e+03 -2.267e+03 -2.351e+03 -3.196e+03 -3.540e+03 -3.720e+03]\n",
      "Concordance Index 0.45979319774397537\n",
      "Integrated Brier Score: 0.20778749837091728\n",
      "y_train breslow final [ 3.000e+00 -7.000e+00  8.000e+00 -1.400e+01 -1.600e+01 -1.700e+01\n",
      " -2.000e+01 -2.100e+01 -2.100e+01  2.200e+01  2.400e+01 -2.900e+01\n",
      "  3.000e+01 -3.000e+01  3.100e+01 -3.400e+01 -3.500e+01  4.500e+01\n",
      " -4.600e+01  5.700e+01  6.700e+01  7.600e+01 -7.700e+01  8.100e+01\n",
      "  8.200e+01 -9.200e+01 -9.900e+01  1.030e+02  1.050e+02  1.050e+02\n",
      "  1.130e+02  1.220e+02  1.240e+02  1.240e+02 -1.310e+02  1.320e+02\n",
      "  1.380e+02  1.410e+02  1.530e+02  1.560e+02 -1.560e+02 -1.640e+02\n",
      "  1.660e+02  1.680e+02  1.740e+02 -1.760e+02  1.800e+02 -1.830e+02\n",
      "  1.850e+02 -1.890e+02  1.920e+02 -1.980e+02 -1.980e+02 -2.000e+02\n",
      " -2.000e+02  2.000e+02  2.000e+02  2.050e+02  2.150e+02  2.180e+02\n",
      " -2.240e+02 -2.250e+02 -2.290e+02  2.290e+02 -2.430e+02  2.430e+02\n",
      "  2.430e+02 -2.450e+02  2.450e+02  2.500e+02  2.550e+02  2.590e+02\n",
      "  2.620e+02  2.720e+02  2.730e+02 -2.730e+02  2.740e+02  2.750e+02\n",
      "  2.760e+02  2.790e+02 -2.800e+02 -2.800e+02  2.810e+02  2.820e+02\n",
      "  2.840e+02 -2.870e+02 -2.880e+02  2.890e+02  2.920e+02  2.940e+02\n",
      "  2.960e+02  3.000e+02  3.000e+02  3.120e+02 -3.230e+02 -3.250e+02\n",
      " -3.350e+02  3.360e+02 -3.370e+02  3.410e+02  3.420e+02 -3.420e+02\n",
      " -3.440e+02 -3.440e+02  3.440e+02 -3.460e+02  3.480e+02  3.480e+02\n",
      "  3.530e+02 -3.560e+02 -3.560e+02  3.590e+02 -3.650e+02  3.660e+02\n",
      " -3.670e+02 -3.680e+02  3.700e+02 -3.740e+02 -3.740e+02 -3.750e+02\n",
      " -3.750e+02  3.760e+02  3.770e+02 -3.770e+02 -3.780e+02 -3.780e+02\n",
      " -3.810e+02 -3.830e+02 -3.830e+02 -3.830e+02 -3.840e+02 -3.850e+02\n",
      " -3.880e+02 -3.890e+02 -3.890e+02 -3.900e+02 -3.960e+02  3.960e+02\n",
      " -4.000e+02  4.010e+02  4.030e+02  4.060e+02  4.060e+02 -4.110e+02\n",
      " -4.110e+02 -4.160e+02  4.220e+02  4.260e+02 -4.270e+02 -4.270e+02\n",
      "  4.280e+02 -4.310e+02 -4.340e+02  4.390e+02 -4.400e+02  4.460e+02\n",
      " -4.490e+02 -4.500e+02  4.570e+02 -4.630e+02  4.660e+02 -4.680e+02\n",
      "  4.740e+02  4.760e+02 -4.760e+02  4.770e+02 -4.790e+02 -4.850e+02\n",
      " -4.850e+02  4.910e+02  4.910e+02 -4.910e+02 -4.940e+02 -5.000e+02\n",
      " -5.110e+02  5.130e+02 -5.190e+02 -5.210e+02 -5.230e+02 -5.230e+02\n",
      " -5.250e+02  5.260e+02  5.330e+02  5.430e+02 -5.430e+02  5.520e+02\n",
      "  5.540e+02  5.540e+02  5.580e+02 -5.590e+02  5.620e+02 -5.640e+02\n",
      " -5.660e+02 -5.720e+02 -5.730e+02 -5.770e+02 -5.790e+02  5.880e+02\n",
      " -5.930e+02 -5.940e+02 -5.940e+02 -5.950e+02  6.070e+02 -6.150e+02\n",
      " -6.160e+02  6.180e+02 -6.210e+02 -6.250e+02  6.330e+02  6.350e+02\n",
      " -6.360e+02  6.400e+02 -6.410e+02 -6.430e+02 -6.440e+02 -6.470e+02\n",
      " -6.500e+02  6.520e+02  6.610e+02 -6.660e+02 -6.750e+02  6.750e+02\n",
      " -6.780e+02 -6.780e+02 -6.790e+02 -6.900e+02 -6.920e+02 -6.940e+02\n",
      " -6.990e+02  7.120e+02 -7.240e+02 -7.420e+02 -7.540e+02  7.620e+02\n",
      "  7.660e+02  7.790e+02 -7.850e+02  7.940e+02  8.050e+02 -8.120e+02\n",
      " -8.190e+02 -8.250e+02  8.320e+02 -8.380e+02 -8.380e+02 -8.560e+02\n",
      " -8.620e+02 -8.640e+02  8.740e+02  8.810e+02 -8.820e+02 -8.950e+02\n",
      " -8.990e+02 -9.000e+02 -9.120e+02 -9.120e+02 -9.280e+02  9.400e+02\n",
      " -9.420e+02 -9.460e+02 -9.490e+02 -9.510e+02 -9.810e+02 -9.890e+02\n",
      " -9.970e+02 -1.000e+03 -1.002e+03 -1.010e+03 -1.023e+03  1.043e+03\n",
      " -1.055e+03 -1.072e+03 -1.083e+03 -1.090e+03  1.095e+03 -1.100e+03\n",
      " -1.106e+03 -1.124e+03 -1.132e+03 -1.133e+03 -1.138e+03 -1.145e+03\n",
      "  1.153e+03 -1.160e+03 -1.184e+03 -1.190e+03 -1.200e+03 -1.210e+03\n",
      " -1.223e+03 -1.236e+03 -1.297e+03 -1.367e+03 -1.389e+03  1.407e+03\n",
      " -1.484e+03 -1.646e+03  1.686e+03 -1.690e+03  1.747e+03 -1.765e+03\n",
      "  1.811e+03 -1.918e+03 -1.935e+03 -1.964e+03  2.100e+03 -2.171e+03\n",
      "  2.197e+03 -2.267e+03 -2.351e+03 -3.196e+03 -3.540e+03 -3.720e+03]\n",
      "durations 8.0 3519.0\n",
      "Concordance Index 0.5158940397350993\n",
      "Integrated Brier Score: 0.2190485313554098\n"
     ]
    }
   ],
   "source": [
    "cancer_types = [\n",
    "    'BLCA',\n",
    "    'BRCA',\n",
    "    'HNSC',\n",
    "    'KIRC',\n",
    "    'LGG',\n",
    "    'LIHC',\n",
    "    'LUAD',\n",
    "    'LUSC',\n",
    "    'OV',\n",
    "    'STAD']\n",
    "\n",
    "agg_metrics_cindex = []\n",
    "agg_metrics_ibs = []\n",
    "\n",
    "\n",
    "class InputShapeSetter(skorch.callbacks.Callback):\n",
    "    def on_train_begin(self, net, X, y):\n",
    "        net.set_params(module__input_units=X.shape[-1])\n",
    "\n",
    "for idx, cancer_type in enumerate(cancer_types):\n",
    "    # get name of current dataset\n",
    "    data = load_tcga(path=\"/Users/JUSC/Documents/xgbsurv/xgbsurv/datasets/data/\", cancer_type=cancer_type, as_frame=True)\n",
    "    X  = data.data #.astype(np.float32)\n",
    "    y = data.target #.values #.to_numpy()\n",
    "\n",
    "    X, y = sort_X_y_pandas(X, y)\n",
    "\n",
    "    net = NeuralNet(\n",
    "        SurvivalModel, \n",
    "        module__n_layers = 1,\n",
    "        module__input_units = X.shape[1],\n",
    "        #module__num_nodes = 32,\n",
    "        #module__dropout = 0.1, # these could also be removed\n",
    "        module__out_features = 1,\n",
    "        # for split sizes when result size = 1\n",
    "        iterator_train__drop_last=True,\n",
    "        #iterator_valid__drop_last=True,\n",
    "        criterion=AHLoss,\n",
    "        optimizer=torch.optim.AdamW,\n",
    "        optimizer__weight_decay = 0.4,\n",
    "        batch_size=32, # separate train and valid->iterator_train__batch_size=128 and iterator_valid__batch_size=128 ?\n",
    "        callbacks=[\n",
    "            (\n",
    "                \"sched\",\n",
    "                LRScheduler(\n",
    "                    torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
    "                    monitor=\"valid_loss\",\n",
    "                    patience=5,\n",
    "                ),\n",
    "            ),\n",
    "            (\n",
    "                \"es\",\n",
    "                EarlyStopping(\n",
    "                    monitor=\"valid_loss\",\n",
    "                    patience=early_stopping_rounds,\n",
    "                    load_best=True,\n",
    "                ),\n",
    "            ),\n",
    "            (\"seed\", FixSeed(seed=42)),\n",
    "            (\"Input Shape Setter\",InputShapeSetter())\n",
    "        ],#[EarlyStopping(patience=10)],#,InputShapeSetter()],\n",
    "        #TODO: enable stratification, verify\n",
    "        train_split=ValidSplit(0.2), # might cause lower performance in metrics, explain in thesis\n",
    "        verbose=2\n",
    "    )\n",
    "    df_agg_metrics_cindex, df_agg_metrics_ibs, best_model,params, outer_scores, best_preds_train, best_preds_test = train_eval(X, y, net, n_iter, data.filename)\n",
    "    agg_metrics_cindex.append(df_agg_metrics_cindex)\n",
    "    agg_metrics_ibs.append(df_agg_metrics_ibs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_ah_1_cindex = pd.concat([df for df in agg_metrics_cindex]).round(4)\n",
    "df_final_ah_1_cindex.to_csv('metrics/final_deep_learning_aft_tcga_cindex.csv', index=False)\n",
    "df_final_ah_1_cindex.to_csv('/Users/JUSC/Documents/644928e0fb7e147893e8ec15/05_thesis/tables/final_deep_learning_aft_tcga_cindex.csv', index=False)  #\n",
    "df_final_ah_1_cindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_ah_1_ibs = pd.concat([df for df in agg_metrics_ibs]).round(4)\n",
    "df_final_ah_1_ibs.to_csv('metrics/final_deep_learning_ah_tcga_ibs.csv', index=False)\n",
    "df_final_ah_1_ibs.to_csv('/Users/JUSC/Documents/644928e0fb7e147893e8ec15/05_thesis/tables/final_deep_learning_ah_tcga_ibs.csv', index=False) \n",
    "df_final_ah_1_ibs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xgbsurv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
