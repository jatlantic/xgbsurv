{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import savetxt\n",
    "from xgbsurv.datasets import (load_metabric, load_flchain, load_rgbsg, load_support, load_tcga)\n",
    "from xgbsurv.models.utils import sort_X_y_pandas, transform_back, transform\n",
    "from xgbsurv.models.eh_ah_final import get_cumulative_hazard_function_ah\n",
    "import torch\n",
    "from torch import nn\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold, train_test_split\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, LabelBinarizer, OneHotEncoder\n",
    "from sklearn.compose import make_column_transformer, make_column_selector\n",
    "from sklearn.decomposition import PCA\n",
    "from loss_functions_pytorch import AHLoss, ah_likelihood_torch\n",
    "from skorch import NeuralNet\n",
    "from skorch.callbacks import EarlyStopping, Callback, LRScheduler\n",
    "import skorch.callbacks\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import check_cv\n",
    "from numbers import Number\n",
    "import torch.utils.data\n",
    "from skorch.utils import flatten\n",
    "from skorch.utils import is_pandas_ndframe\n",
    "from skorch.utils import check_indexing\n",
    "from skorch.utils import multi_indexing\n",
    "from skorch.utils import to_numpy\n",
    "from skorch.dataset import get_len\n",
    "from skorch.dataset import ValidSplit\n",
    "from pycox.evaluation import EvalSurv\n",
    "from scipy.stats import uniform as scuniform\n",
    "from scipy.stats import randint as scrandint\n",
    "from scipy.stats import loguniform as scloguniform\n",
    "import random\n",
    "import os\n",
    "#torch.set_default_dtype(torch.float64)\n",
    "#torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "# highest prediction with -loss function 8.7\n",
    "#highest prediction without -loss function 8.7"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters, put into function\n",
    "n_outer_splits = 5\n",
    "n_inner_splits = 5\n",
    "rand_state = 42\n",
    "n_iter = 1\n",
    "early_stopping_rounds=10\n",
    "base_score = 0.0\n",
    "\n",
    "# set seed for scipy/numpy\n",
    "np.random.seed(rand_state)\n",
    "\n",
    "param_grid_ah = {\n",
    "    'estimator__module__n_layers': [1, 2, 4],\n",
    "    'estimator__module__num_nodes': [64, 128, 256, 512],\n",
    "    'estimator__module__dropout': scuniform(0.0,0.7),\n",
    "    'estimator__optimizer__weight_decay': [0.4, 0.2, 0.1, 0.05, 0.02, 0.01, 0],\n",
    "    'estimator__batch_size': [64, 128, 256, 512, 1024],\n",
    "    #lr not in paper because of learning rate finder\n",
    "    # note: setting learning rate higher would make exp(partial_hazard) explode\n",
    "    #'estimator__lr': scloguniform(0.001,0.01), # scheduler unten einbauen\n",
    "    # use callback instead\n",
    "    'estimator__lr':[0.001]\n",
    "    #'max_epochs':  scrandint(10,20), # corresponds to num_rounds\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_torch(seed=42):\n",
    "    \"\"\"Sets all seeds within torch and adjacent libraries.\n",
    "\n",
    "    Args:\n",
    "        seed: Random seed to be used by the seeding functions.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    #os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    #torch.backends.cudnn.benchmark = False\n",
    "    #torch.backends.cudnn.deterministic = True\n",
    "    return None\n",
    "\n",
    "\n",
    "class FixSeed(Callback):\n",
    "    def __init__(self, seed):\n",
    "        self.seed = seed\n",
    "\n",
    "    def initialize(self):\n",
    "        seed_torch(self.seed)\n",
    "        return super().initialize()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Scorer\n",
    "def custom_scoring_function(y_true, y_pred):\n",
    "\n",
    "        #y_true = torch.from_numpy(y_true)\n",
    "        if isinstance(y_pred, np.ndarray):\n",
    "            y_pred = torch.from_numpy(y_pred)\n",
    "        if isinstance(y_true, np.ndarray):\n",
    "            y_true = torch.from_numpy(y_true)\n",
    "        if isinstance(y_pred, pd.Series):\n",
    "            y_pred = torch.tensor(y_pred.values)\n",
    "        if isinstance(y_true, pd.Series):\n",
    "            y_true = torch.tensor(y_true.values)\n",
    "        #print('loss function y_pred', y_pred)\n",
    "        #print('loss function y_true', y_true)\n",
    "        score = -ah_likelihood_torch(y_pred, y_true) #.to(torch.float32)\n",
    "        return score.numpy()\n",
    "\n",
    "scoring_function = make_scorer(custom_scoring_function, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set up Custom Splitter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Torch Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SurvivalModel(nn.Module):\n",
    "    def __init__(self, n_layers, input_units, num_nodes, dropout, out_features):\n",
    "        super(SurvivalModel, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.in_features = input_units\n",
    "        self.num_nodes = num_nodes\n",
    "        self.dropout = dropout\n",
    "        self.out_features = out_features\n",
    "        model = []\n",
    "        # first layer\n",
    "        model.append(torch.nn.Linear(input_units, num_nodes))\n",
    "        model.append(torch.nn.ReLU())\n",
    "        model.append(torch.nn.Dropout(dropout))\n",
    "        model.append(torch.nn.BatchNorm1d(num_nodes))\n",
    "\n",
    "        for i in range(n_layers-1):\n",
    "            model.append(torch.nn.Linear(num_nodes, num_nodes))\n",
    "            #init.kaiming_normal_(model[-1].weight, nonlinearity='relu')\n",
    "            model.append(torch.nn.ReLU())\n",
    "            model.append(torch.nn.Dropout(dropout))\n",
    "            model.append(torch.nn.BatchNorm1d(num_nodes))\n",
    "\n",
    "        # output layer\n",
    "        model.append(torch.nn.Linear(num_nodes, out_features))\n",
    "    \n",
    "        self.layers = nn.Sequential(*model)\n",
    "\n",
    "        # for layer in self.layers:\n",
    "        #     if isinstance(layer, nn.Linear):\n",
    "        #         #nn.init.uniform_(layer.weight, a=-0.5, b=0.5)\n",
    "        #         nn.init.kaiming_normal_(layer.weight)\n",
    "\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = X.to(torch.float32)\n",
    "        res = self.layers(X)\n",
    "        #print(res)\n",
    "        return res\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomStandardScaler(StandardScaler):\n",
    "    \n",
    "    def __init__(self, copy=True, with_mean=True, with_std=True):\n",
    "        super().__init__(copy=copy, with_mean=with_mean, with_std=with_std)\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return super().fit(X, y)\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X_transformed = super().transform(X, y)\n",
    "        return X_transformed.astype(np.float32)\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        X_transformed = super().fit_transform(X, y)\n",
    "        return X_transformed.astype(np.float32)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define stratified inner k-fold cross-validation\n",
    "class CustomSplit(StratifiedKFold):\n",
    "    def __init__(self, n_splits=5, shuffle=True, random_state=None):\n",
    "        super().__init__(n_splits=n_splits, shuffle=shuffle, random_state=random_state)\n",
    "\n",
    "    def split(self, X, y, groups=None):\n",
    "        print('split', X.dtypes)\n",
    "        try:\n",
    "            if y.shape[1]>1:\n",
    "                y = y[:,0]\n",
    "        except:\n",
    "            pass\n",
    "        bins = np.sign(y)\n",
    "        return super().split(X, bins, groups=groups)\n",
    "\n",
    "    def get_n_splits(self, X=None, y=None, groups=None):\n",
    "        return self.n_splits\n",
    "\n",
    "outer_custom_cv = CustomSplit(n_splits=n_outer_splits, shuffle=True, random_state=rand_state)\n",
    "inner_custom_cv = CustomSplit(n_splits=n_outer_splits, shuffle=True, random_state=rand_state)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Valid Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class CustomStandardScaler(StandardScaler):\n",
    "    \n",
    "    def __init__(self, copy=True, with_mean=True, with_std=True):\n",
    "        super().__init__(copy=copy, with_mean=with_mean, with_std=with_std)\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return super().fit(X, y)\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X_transformed = super().transform(X, y)\n",
    "        return X_transformed.astype(np.float32)\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        X_transformed = super().fit_transform(X, y)\n",
    "        return X_transformed.astype(np.float32)\n",
    "    \n",
    "class CustomValidSplit():\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            cv=5,\n",
    "            stratified=False,\n",
    "            random_state=None,\n",
    "    ):\n",
    "        self.stratified = stratified\n",
    "        self.random_state = random_state\n",
    "\n",
    "        if isinstance(cv, Number) and (cv <= 0):\n",
    "            raise ValueError(\"Numbers less than 0 are not allowed for cv \"\n",
    "                             \"but ValidSplit got {}\".format(cv))\n",
    "\n",
    "        if not self._is_float(cv) and random_state is not None:\n",
    "            raise ValueError(\n",
    "                \"Setting a random_state has no effect since cv is not a float. \"\n",
    "                \"You should leave random_state to its default (None), or set cv \"\n",
    "                \"to a float value.\",\n",
    "            )\n",
    "\n",
    "        self.cv = cv\n",
    "\n",
    "    def _is_stratified(self, cv):\n",
    "        return isinstance(cv, (StratifiedKFold, StratifiedShuffleSplit))\n",
    "\n",
    "    def _is_float(self, x):\n",
    "        if not isinstance(x, Number):\n",
    "            return False\n",
    "        return not float(x).is_integer()\n",
    "\n",
    "    def _check_cv_float(self):\n",
    "        cv_cls = StratifiedShuffleSplit if self.stratified else ShuffleSplit\n",
    "        return cv_cls(test_size=self.cv, random_state=self.random_state)\n",
    "\n",
    "    def _check_cv_non_float(self, y):\n",
    "        return check_cv(\n",
    "            self.cv,\n",
    "            y=y,\n",
    "            classifier=self.stratified,\n",
    "        )\n",
    "\n",
    "    def check_cv(self, y):\n",
    "        \"\"\"Resolve which cross validation strategy is used.\"\"\"\n",
    "        y_arr = None\n",
    "        if self.stratified:\n",
    "            # Try to convert y to numpy for sklearn's check_cv; if conversion\n",
    "            # doesn't work, still try.\n",
    "            try:\n",
    "                y_arr = to_numpy(y)\n",
    "            except (AttributeError, TypeError):\n",
    "                y_arr = y\n",
    "\n",
    "        if self._is_float(self.cv):\n",
    "            return self._check_cv_float()\n",
    "        return self._check_cv_non_float(y_arr)\n",
    "\n",
    "    def _is_regular(self, x):\n",
    "        return (x is None) or isinstance(x, np.ndarray) or is_pandas_ndframe(x)\n",
    "\n",
    "    def __call__(self, dataset, y=None, groups=None):\n",
    "        # key change here\n",
    "        y = np.sign(y)\n",
    "        bad_y_error = ValueError(\n",
    "            \"Stratified CV requires explicitly passing a suitable y.\")\n",
    "        if (y is None) and self.stratified:\n",
    "            raise bad_y_error\n",
    "\n",
    "        cv = self.check_cv(y)\n",
    "        if self.stratified and not self._is_stratified(cv):\n",
    "            raise bad_y_error\n",
    "\n",
    "        # pylint: disable=invalid-name\n",
    "        len_dataset = get_len(dataset)\n",
    "        if y is not None:\n",
    "            len_y = get_len(y)\n",
    "            if len_dataset != len_y:\n",
    "                raise ValueError(\"Cannot perform a CV split if dataset and y \"\n",
    "                                 \"have different lengths.\")\n",
    "\n",
    "        args = (np.arange(len_dataset),)\n",
    "        if self._is_stratified(cv):\n",
    "            args = args + (to_numpy(y),)\n",
    "\n",
    "        idx_train, idx_valid = next(iter(cv.split(*args, groups=groups)))\n",
    "        dataset_train = torch.utils.data.Subset(dataset, idx_train)\n",
    "        dataset_valid = torch.utils.data.Subset(dataset, idx_valid)\n",
    "        return dataset_train, dataset_valid\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Training Procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_eval(X, y, net, n_iter, filename):\n",
    "        model = 'ah_'\n",
    "        dataset_name = filename.split('_')[0]\n",
    "        # add IBS later\n",
    "        outer_scores = {'cindex_train_'+dataset_name:[], 'cindex_test_'+dataset_name:[],\n",
    "                        'ibs_train_'+dataset_name:[], 'ibs_test_'+dataset_name:[]}\n",
    "        best_params = {'best_params_'+dataset_name:[]}\n",
    "        best_model = {'best_model_'+dataset_name:[]}\n",
    "        ct = make_column_transformer(\n",
    "                #(OneHotEncoder(sparse_output=False), make_column_selector(dtype_include=['category', 'object'])),\n",
    "                (StandardScaler(), make_column_selector(dtype_include=['float32'],dtype_exclude=['category', 'object']))\n",
    "                ,remainder='passthrough')\n",
    "\n",
    "        pipe = Pipeline([('scaler',ct),\n",
    "                        ('estimator', net)])\n",
    "        rs = RandomizedSearchCV(pipe, param_grid_ah, scoring = scoring_function, n_jobs=-1, \n",
    "                                    n_iter=n_iter, refit=True, random_state=rand_state)\n",
    "        \n",
    "        for i, (train_index, test_index) in enumerate(outer_custom_cv.split(X, y)):\n",
    "                # Split data into training and testing sets for outer fold\n",
    "                X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "                y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "                X_train, y_train = sort_X_y_pandas(X_train, y_train)\n",
    "                X_test, y_test = sort_X_y_pandas(X_test, y_test)\n",
    "\n",
    "                # print(X_train.shape, type(X_train))\n",
    "                # print(y_train.shape, type(y_train))\n",
    "                # print(X_test.shape, type(X_test))\n",
    "                # print(y_test.shape, type(y_test))\n",
    "                # save splits and data\n",
    "                savetxt('splits/'+model+'train_index_'+str(i)+'_'+filename, train_index, delimiter=',')\n",
    "                savetxt('splits/'+model+'test_index_'+str(i)+'_'+filename, test_index, delimiter=',')\n",
    "                \n",
    "                # savetxt('splits/X_train_'+str(i)+'_'+filename, X_train, delimiter=',')\n",
    "                # savetxt('splits/X_test_'+str(i)+'_'+filename, X_test, delimiter=',')\n",
    "\n",
    "                # savetxt('splits/y_train_'+str(i)+'_'+filename, y_train, delimiter=',')\n",
    "                # savetxt('splits/y_test_'+str(i)+'_'+filename, y_test, delimiter=',')\n",
    "\n",
    "\n",
    "                rs.fit(X_train, y_train)\n",
    "                best_preds_train = rs.best_estimator_.predict(X_train)\n",
    "                best_preds_test = rs.best_estimator_.predict(X_test)\n",
    "                print('max best test predictions',np.max(best_preds_test.reshape(-1)) )\n",
    "                savetxt('predictions/'+model+'best_preds_train_'+str(i)+'_'+filename, best_preds_train, delimiter=',')\n",
    "                savetxt('predictions/'+model+'best_preds_test_'+str(i)+'_'+filename, best_preds_test, delimiter=',')\n",
    "\n",
    "                # save hyperparameter settings\n",
    "                params = rs.best_estimator_.get_params\n",
    "                best_params['best_params_'+dataset_name] += [rs.best_params_]\n",
    "                best_model['best_model_'+dataset_name] += [params]\n",
    "                # remove training part, cumhazard sollte um 10000 sein ( Integralanzahl/Intervalle)\n",
    "\n",
    "                outer_scores['cindex_train_'+dataset_name] += [np.nan]\n",
    "                outer_scores['ibs_train_'+dataset_name] += [np.nan]\n",
    "                    \n",
    "                #try:\n",
    "                cum_hazard_test = get_cumulative_hazard_function_ah(\n",
    "                        X_train.values, X_test.values, y_train.values, y_test.values,\n",
    "                        best_preds_train.reshape(-1), best_preds_test.reshape(-1)\n",
    "                        )\n",
    "                df_survival_test = np.exp(-cum_hazard_test)\n",
    "                durations_test, events_test = transform_back(y_test.values)\n",
    "                time_grid_test = np.linspace(durations_test.min(), durations_test.max(), 100)\n",
    "                ev = EvalSurv(df_survival_test, durations_test, events_test, censor_surv='km')\n",
    "                print('Concordance Index',ev.concordance_td('antolini'))\n",
    "                print('Integrated Brier Score:',ev.integrated_brier_score(time_grid_test))\n",
    "                cindex_score_test = ev.concordance_td('antolini')\n",
    "                ibs_score_test = ev.integrated_brier_score(time_grid_test)\n",
    "\n",
    "                outer_scores['cindex_test_'+dataset_name] += [cindex_score_test]\n",
    "                outer_scores['ibs_test_'+dataset_name] += [ibs_score_test]\n",
    "                #except: \n",
    "                #outer_scores['cindex_test_'+dataset_name] += [np.nan]\n",
    "                #outer_scores['ibs_test_'+dataset_name] += [np.nan]\n",
    "            \n",
    "        df_best_params = pd.DataFrame(best_params)\n",
    "        df_best_model = pd.DataFrame(best_model)\n",
    "        df_outer_scores = pd.DataFrame(outer_scores)\n",
    "        df_metrics = pd.concat([df_best_params,df_best_model,df_outer_scores], axis=1)\n",
    "        df_metrics.to_csv('metrics/'+model+'metric_summary'+str(i)+'_'+filename, index=False)\n",
    "       \n",
    "        # cindex\n",
    "        df_agg_metrics_cindex = pd.DataFrame({'dataset':[dataset_name],\n",
    "                                              'cindex_train_mean':df_outer_scores['cindex_train_'+dataset_name].mean(),\n",
    "                                              'cindex_train_std':df_outer_scores['cindex_train_'+dataset_name].std(),\n",
    "                                              'cindex_test_mean':df_outer_scores['cindex_test_'+dataset_name].mean(),\n",
    "                                              'cindex_test_std':df_outer_scores['cindex_test_'+dataset_name].std() })\n",
    "        # IBS\n",
    "        df_agg_metrics_ibs = pd.DataFrame({'dataset':[dataset_name],\n",
    "                                              'ibs_train_mean':df_outer_scores['ibs_train_'+dataset_name].mean(),\n",
    "                                              'ibs_train_std':df_outer_scores['ibs_train_'+dataset_name].std(),\n",
    "                                              'ibs_test_mean':df_outer_scores['ibs_test_'+dataset_name].mean(),\n",
    "                                              'ibs_test_std':df_outer_scores['ibs_test_'+dataset_name].std() })\n",
    "\n",
    "        return df_agg_metrics_cindex, df_agg_metrics_ibs, best_model, best_params, outer_scores, best_preds_train, best_preds_test #, X_train, X_test, y_train, y_test\n",
    "\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skorch.callbacks\n",
    "class InputShapeSetter(skorch.callbacks.Callback):\n",
    "    def on_train_begin(self, net, X, y):\n",
    "        net.set_params(module__input_units=X.shape[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_metabric\n",
      "split MKI67                float32\n",
      "EGFR                 float32\n",
      "PGR                  float32\n",
      "ERBB2                float32\n",
      "hormone_treatment      uint8\n",
      "radiotherapy           uint8\n",
      "chemotherapy           uint8\n",
      "ER_positive            uint8\n",
      "age                  float32\n",
      "dtype: object\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.7637\u001b[0m        \u001b[32m2.7443\u001b[0m  0.0536\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.1970\u001b[0m        \u001b[32m3.2081\u001b[0m  0.0540\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.0192\u001b[0m        \u001b[32m3.0146\u001b[0m  0.0541\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.1915\u001b[0m        \u001b[32m3.2133\u001b[0m  0.0541\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.1872\u001b[0m        \u001b[32m3.2058\u001b[0m  0.0541\n",
      "      2        \u001b[36m3.0180\u001b[0m        \u001b[32m3.0141\u001b[0m  0.0308\n",
      "      2        3.1936        \u001b[32m3.2054\u001b[0m  0.0307\n",
      "      2        \u001b[36m2.7499\u001b[0m        2.7445  0.0321\n",
      "      2        \u001b[36m3.1790\u001b[0m        \u001b[32m3.2127\u001b[0m  0.0318\n",
      "      2        3.2012        \u001b[32m3.2075\u001b[0m  0.0329\n",
      "      3        \u001b[36m3.0109\u001b[0m        \u001b[32m3.0135\u001b[0m  0.0243\n",
      "      3        2.7510        2.7447  0.0259\n",
      "      3        3.2018        \u001b[32m3.2064\u001b[0m  0.0260\n",
      "      3        \u001b[36m3.1771\u001b[0m        \u001b[32m3.2044\u001b[0m  0.0282\n",
      "      3        \u001b[36m3.1704\u001b[0m        \u001b[32m3.2116\u001b[0m  0.0290\n",
      "      4        3.0169        \u001b[32m3.0127\u001b[0m  0.0223\n",
      "      4        \u001b[36m3.1966\u001b[0m        \u001b[32m3.2051\u001b[0m  0.0230\n",
      "      4        2.7566        2.7450  0.0248\n",
      "      4        3.1828        \u001b[32m3.2029\u001b[0m  0.0262\n",
      "      4        3.1726        \u001b[32m3.2102\u001b[0m  0.0249\n",
      "      5        3.0161        \u001b[32m3.0120\u001b[0m  0.0248\n",
      "      5        \u001b[36m3.1886\u001b[0m        \u001b[32m3.2036\u001b[0m  0.0230\n",
      "      5        2.7516        2.7454  0.0245\n",
      "      5        3.1712        \u001b[32m3.2085\u001b[0m  0.0254\n",
      "      5        \u001b[36m3.1760\u001b[0m        \u001b[32m3.2013\u001b[0m  0.0267\n",
      "      6        \u001b[36m3.1718\u001b[0m        \u001b[32m3.2022\u001b[0m  0.0263\n",
      "      6        3.0136        \u001b[32m3.0113\u001b[0m  0.0288\n",
      "      6        \u001b[36m2.7443\u001b[0m        2.7457  0.0255\n",
      "      6        3.1756        \u001b[32m3.2066\u001b[0m  0.0245\n",
      "      6        3.1829        \u001b[32m3.1995\u001b[0m  0.0238\n",
      "      7        2.7482        2.7463  0.0216\n",
      "      7        \u001b[36m3.0081\u001b[0m        \u001b[32m3.0104\u001b[0m  0.0250\n",
      "      7        3.1721        \u001b[32m3.2011\u001b[0m  0.0254\n",
      "      7        \u001b[36m3.1648\u001b[0m        \u001b[32m3.2051\u001b[0m  0.0221\n",
      "      7        \u001b[36m3.1760\u001b[0m        \u001b[32m3.1977\u001b[0m  0.0231\n",
      "      8        2.7467        2.7464  0.0223\n",
      "      8        3.1684        \u001b[32m3.2039\u001b[0m  0.0230\n",
      "      8        3.0215        \u001b[32m3.0098\u001b[0m  0.0254\n",
      "      8        3.1799        \u001b[32m3.2001\u001b[0m  0.0265\n",
      "      8        3.1797        \u001b[32m3.1956\u001b[0m  0.0249\n",
      "      9        \u001b[36m2.7270\u001b[0m        2.7467  0.0220\n",
      "      9        3.1803        \u001b[32m3.1991\u001b[0m  0.0228\n",
      "      9        3.1705        \u001b[32m3.2031\u001b[0m  0.0249\n",
      "      9        3.1820        \u001b[32m3.1935\u001b[0m  0.0229\n",
      "      9        3.0238        \u001b[32m3.0090\u001b[0m  0.0252\n",
      "     10        2.7437        2.7469  0.0238\n",
      "Restoring best model from epoch 1.\n",
      "     10        3.1726        \u001b[32m3.1983\u001b[0m  0.0258\n",
      "Restoring best model from epoch 1.\n",
      "     10        \u001b[36m3.0068\u001b[0m        \u001b[32m3.0080\u001b[0m  0.0276\n",
      "     10        \u001b[36m3.1583\u001b[0m        \u001b[32m3.2020\u001b[0m  0.0290\n",
      "Restoring best model from epoch 1.\n",
      "     10        \u001b[36m3.1726\u001b[0m        \u001b[32m3.1912\u001b[0m  0.0283\n",
      "Restoring best model from epoch 1.\n",
      "Restoring best model from epoch 1.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.1172\u001b[0m        \u001b[32m3.1261\u001b[0m  0.0700\n",
      "      2        3.1191        \u001b[32m3.1246\u001b[0m  0.0442\n",
      "      3        \u001b[36m3.1122\u001b[0m        \u001b[32m3.1225\u001b[0m  0.0430\n",
      "      4        \u001b[36m3.1086\u001b[0m        \u001b[32m3.1193\u001b[0m  0.0439\n",
      "      5        \u001b[36m3.0992\u001b[0m        \u001b[32m3.1152\u001b[0m  0.0448\n",
      "      6        3.1022        \u001b[32m3.1111\u001b[0m  0.0431\n",
      "      7        \u001b[36m3.0932\u001b[0m        \u001b[32m3.1079\u001b[0m  0.0442\n",
      "      8        3.1012        \u001b[32m3.1050\u001b[0m  0.0433\n",
      "      9        3.0990        \u001b[32m3.1019\u001b[0m  0.0430\n",
      "     10        3.1016        \u001b[32m3.0989\u001b[0m  0.0436\n",
      "Restoring best model from epoch 1.\n",
      "max best test predictions 0.09753272\n",
      "granularity 0.0333241953125\n",
      "integration times 11165\n",
      "Concordance Index 0.5429348041744375\n",
      "Integrated Brier Score: 0.1884143262091351\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.0348\u001b[0m        \u001b[32m3.0012\u001b[0m  0.0215\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.2167\u001b[0m        \u001b[32m3.2239\u001b[0m  0.0217\n",
      "      2        3.0361        \u001b[32m3.0011\u001b[0m  0.0209\n",
      "      2        3.2195        \u001b[32m3.2235\u001b[0m  0.0214\n",
      "      3        3.0413        \u001b[32m3.0010\u001b[0m  0.0209\n",
      "      3        3.2327        \u001b[32m3.2227\u001b[0m  0.0206\n",
      "      4        3.0372        3.0012  0.0218\n",
      "      4        3.2407        \u001b[32m3.2214\u001b[0m  0.0210\n",
      "      5        3.0352        3.0013  0.0215\n",
      "      5        \u001b[36m3.2158\u001b[0m        \u001b[32m3.2196\u001b[0m  0.0198\n",
      "      6        3.0367        3.0014  0.0199\n",
      "      6        3.2186        \u001b[32m3.2170\u001b[0m  0.0209\n",
      "      7        3.0417        3.0012  0.0213\n",
      "      7        3.2186        \u001b[32m3.2140\u001b[0m  0.0207\n",
      "      8        \u001b[36m3.0234\u001b[0m        3.0013  0.0222\n",
      "      8        \u001b[36m3.2088\u001b[0m        \u001b[32m3.2107\u001b[0m  0.0201\n",
      "      9        3.0325        3.0013  0.0214\n",
      "      9        3.2140        \u001b[32m3.2069\u001b[0m  0.0207\n",
      "     10        3.0358        3.0012  0.0205\n",
      "Restoring best model from epoch 1.\n",
      "     10        \u001b[36m3.1963\u001b[0m        \u001b[32m3.2033\u001b[0m  0.0201\n",
      "Restoring best model from epoch 1.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.7772\u001b[0m        \u001b[32m2.7593\u001b[0m  0.0435\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.2195\u001b[0m        \u001b[32m3.2017\u001b[0m  0.0438\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.2181\u001b[0m        \u001b[32m3.1901\u001b[0m  0.0378\n",
      "      2        2.7910        \u001b[32m2.7592\u001b[0m  0.0289\n",
      "      2        \u001b[36m3.2181\u001b[0m        \u001b[32m3.2013\u001b[0m  0.0289\n",
      "      2        \u001b[36m3.2154\u001b[0m        \u001b[32m3.1899\u001b[0m  0.0281\n",
      "      3        2.7828        2.7593  0.0221\n",
      "      3        \u001b[36m3.2073\u001b[0m        \u001b[32m3.2006\u001b[0m  0.0230\n",
      "      3        3.2190        \u001b[32m3.1894\u001b[0m  0.0201\n",
      "      4        2.7963        2.7593  0.0233\n",
      "      4        3.2158        \u001b[32m3.1995\u001b[0m  0.0245\n",
      "      4        3.2282        \u001b[32m3.1885\u001b[0m  0.0234\n",
      "      5        2.7991        2.7593  0.0220\n",
      "      5        3.2200        \u001b[32m3.1979\u001b[0m  0.0205\n",
      "      5        \u001b[36m3.2035\u001b[0m        \u001b[32m3.1870\u001b[0m  0.0225\n",
      "      6        3.2123        \u001b[32m3.1964\u001b[0m  0.0197\n",
      "      6        2.7940        2.7594  0.0216\n",
      "      6        3.2146        \u001b[32m3.1852\u001b[0m  0.0231\n",
      "      7        2.7941        2.7595  0.0214\n",
      "      7        \u001b[36m3.2065\u001b[0m        \u001b[32m3.1948\u001b[0m  0.0270\n",
      "      7        3.2177        \u001b[32m3.1831\u001b[0m  0.0339\n",
      "      8        \u001b[36m3.1957\u001b[0m        \u001b[32m3.1930\u001b[0m  0.0241\n",
      "      8        2.7997        2.7595  0.0308\n",
      "      8        3.2082        \u001b[32m3.1809\u001b[0m  0.0203\n",
      "      9        3.1974        \u001b[32m3.1911\u001b[0m  0.0193\n",
      "      9        3.2070        \u001b[32m3.1786\u001b[0m  0.0193\n",
      "     10        \u001b[36m3.1920\u001b[0m        \u001b[32m3.1893\u001b[0m  0.0189\n",
      "Restoring best model from epoch 1.\n",
      "     10        \u001b[36m3.1924\u001b[0m        \u001b[32m3.1763\u001b[0m  0.0195\n",
      "Restoring best model from epoch 1.\n",
      "      9        2.7888        2.7595  0.0549\n",
      "     10        2.7972        2.7595  0.0222\n",
      "Restoring best model from epoch 1.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.1231\u001b[0m        \u001b[32m3.1367\u001b[0m  0.0480\n",
      "      2        3.1244        \u001b[32m3.1357\u001b[0m  0.0431\n",
      "      3        3.1250        \u001b[32m3.1339\u001b[0m  0.0432\n",
      "      4        \u001b[36m3.1196\u001b[0m        \u001b[32m3.1316\u001b[0m  0.0436\n",
      "      5        \u001b[36m3.1162\u001b[0m        \u001b[32m3.1295\u001b[0m  0.0433\n",
      "      6        3.1178        \u001b[32m3.1286\u001b[0m  0.0439\n",
      "      7        \u001b[36m3.1119\u001b[0m        \u001b[32m3.1267\u001b[0m  0.0437\n",
      "      8        \u001b[36m3.1114\u001b[0m        \u001b[32m3.1256\u001b[0m  0.0429\n",
      "      9        \u001b[36m3.0983\u001b[0m        3.1265  0.0438\n",
      "     10        3.1070        3.1262  0.0434\n",
      "Restoring best model from epoch 1.\n",
      "max best test predictions 0.013344446\n",
      "granularity 0.0333241953125\n",
      "integration times 10055\n",
      "Concordance Index 0.42396113984542466\n",
      "Integrated Brier Score: 0.18586149382104447\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.7977\u001b[0m        \u001b[32m2.7564\u001b[0m  0.0211\n",
      "      2        2.8156        2.7567  0.0214\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.2213\u001b[0m        \u001b[32m3.1873\u001b[0m  0.0247\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.2092\u001b[0m        \u001b[32m3.2205\u001b[0m  0.0226\n",
      "      3        2.8012        2.7569  0.0220\n",
      "      2        3.2225        \u001b[32m3.1866\u001b[0m  0.0227\n",
      "      2        \u001b[36m3.1986\u001b[0m        \u001b[32m3.2202\u001b[0m  0.0240\n",
      "      4        \u001b[36m2.7963\u001b[0m        2.7570  0.0222\n",
      "      3        3.2218        \u001b[32m3.1855\u001b[0m  0.0224\n",
      "      3        3.2013        \u001b[32m3.2199\u001b[0m  0.0234\n",
      "      5        2.7975        2.7570  0.0232\n",
      "      4        \u001b[36m3.2204\u001b[0m        \u001b[32m3.1842\u001b[0m  0.0238\n",
      "      4        3.2061        \u001b[32m3.2194\u001b[0m  0.0231\n",
      "      6        2.8082        2.7569  0.0211\n",
      "      5        3.2208        \u001b[32m3.1826\u001b[0m  0.0207\n",
      "      5        3.2073        \u001b[32m3.2186\u001b[0m  0.0221\n",
      "      7        2.7979        2.7568  0.0213\n",
      "      6        \u001b[36m3.2138\u001b[0m        \u001b[32m3.1805\u001b[0m  0.0196\n",
      "      6        3.1987        \u001b[32m3.2175\u001b[0m  0.0233\n",
      "      7        \u001b[36m3.2088\u001b[0m        \u001b[32m3.1780\u001b[0m  0.0199\n",
      "      8        \u001b[36m2.7886\u001b[0m        2.7567  0.0215\n",
      "      7        \u001b[36m3.1801\u001b[0m        \u001b[32m3.2160\u001b[0m  0.0243\n",
      "      8        \u001b[36m3.1864\u001b[0m        \u001b[32m3.1754\u001b[0m  0.0202\n",
      "      9        \u001b[36m2.7839\u001b[0m        2.7567  0.0221\n",
      "      8        3.1903        \u001b[32m3.2141\u001b[0m  0.0237\n",
      "      9        3.1886        \u001b[32m3.1730\u001b[0m  0.0203\n",
      "     10        2.7915        2.7567  0.0218\n",
      "Restoring best model from epoch 1.\n",
      "      9        3.1885        \u001b[32m3.2117\u001b[0m  0.0219\n",
      "     10        3.1956        \u001b[32m3.1708\u001b[0m  0.0200\n",
      "Restoring best model from epoch 1.\n",
      "     10        \u001b[36m3.1793\u001b[0m        \u001b[32m3.2091\u001b[0m  0.0216\n",
      "Restoring best model from epoch 1.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.1768\u001b[0m        \u001b[32m3.1887\u001b[0m  0.0495\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.0304\u001b[0m        \u001b[32m3.0122\u001b[0m  0.0496\n",
      "      2        \u001b[36m3.0220\u001b[0m        \u001b[32m3.0122\u001b[0m  0.0308\n",
      "      2        3.1868        \u001b[32m3.1887\u001b[0m  0.0308\n",
      "      3        \u001b[36m3.0218\u001b[0m        \u001b[32m3.0119\u001b[0m  0.0209\n",
      "      3        3.1783        \u001b[32m3.1884\u001b[0m  0.0221\n",
      "      4        3.0254        3.0122  0.0209\n",
      "      4        3.1851        \u001b[32m3.1881\u001b[0m  0.0237\n",
      "      5        3.0324        3.0135  0.0265\n",
      "      5        3.1823        \u001b[32m3.1876\u001b[0m  0.0250\n",
      "      6        \u001b[36m3.0213\u001b[0m        3.0150  0.0206\n",
      "      6        \u001b[36m3.1744\u001b[0m        \u001b[32m3.1869\u001b[0m  0.0203\n",
      "      7        3.0216        3.0169  0.0204\n",
      "      7        3.1769        \u001b[32m3.1859\u001b[0m  0.0206\n",
      "      8        \u001b[36m3.0141\u001b[0m        3.0196  0.0203\n",
      "      8        3.1776        \u001b[32m3.1848\u001b[0m  0.0206\n",
      "      9        3.0209        3.0227  0.0203\n",
      "      9        3.1771        \u001b[32m3.1836\u001b[0m  0.0206\n",
      "     10        3.0182        3.0236  0.0207\n",
      "Restoring best model from epoch 1.\n",
      "     10        \u001b[36m3.1681\u001b[0m        \u001b[32m3.1823\u001b[0m  0.0214\n",
      "Restoring best model from epoch 1.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.1169\u001b[0m        \u001b[32m3.1247\u001b[0m  0.0525\n",
      "      2        3.1258        \u001b[32m3.1235\u001b[0m  0.0426\n",
      "      3        \u001b[36m3.1138\u001b[0m        \u001b[32m3.1222\u001b[0m  0.0426\n",
      "      4        3.1197        \u001b[32m3.1206\u001b[0m  0.0414\n",
      "      5        \u001b[36m3.1096\u001b[0m        \u001b[32m3.1183\u001b[0m  0.0430\n",
      "      6        \u001b[36m3.1055\u001b[0m        \u001b[32m3.1152\u001b[0m  0.0417\n",
      "      7        \u001b[36m3.0978\u001b[0m        \u001b[32m3.1109\u001b[0m  0.0427\n",
      "      8        3.1029        \u001b[32m3.1064\u001b[0m  0.0435\n",
      "      9        3.0986        \u001b[32m3.1027\u001b[0m  0.0543\n",
      "     10        3.1071        \u001b[32m3.1004\u001b[0m  0.0442\n",
      "Restoring best model from epoch 1.\n",
      "max best test predictions 0.008007843\n",
      "granularity 0.0333241953125\n",
      "integration times 9124\n",
      "Concordance Index 0.4175207660131237\n",
      "Integrated Brier Score: 0.1937393239229615\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.7728\u001b[0m        \u001b[32m2.7553\u001b[0m  0.0220\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.0288\u001b[0m        \u001b[32m3.0126\u001b[0m  0.0224\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.1938\u001b[0m        \u001b[32m3.1754\u001b[0m  0.0289\n",
      "      2        \u001b[36m3.0177\u001b[0m        \u001b[32m3.0123\u001b[0m  0.0206\n",
      "      2        2.7809        \u001b[32m2.7552\u001b[0m  0.0276\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.2142\u001b[0m        \u001b[32m3.2416\u001b[0m  0.0303\n",
      "      2        \u001b[36m3.1866\u001b[0m        \u001b[32m3.1748\u001b[0m  0.0270\n",
      "      3        3.0195        3.0127  0.0200\n",
      "      3        \u001b[36m2.7701\u001b[0m        2.7553  0.0240\n",
      "      2        \u001b[36m3.1978\u001b[0m        \u001b[32m3.2414\u001b[0m  0.0240\n",
      "      4        3.0224        3.0130  0.0208\n",
      "      3        3.1901        \u001b[32m3.1740\u001b[0m  0.0251\n",
      "      4        2.7855        \u001b[32m2.7551\u001b[0m  0.0238\n",
      "      5        3.0313        3.0133  0.0192\n",
      "      3        3.2052        \u001b[32m3.2411\u001b[0m  0.0249\n",
      "      4        \u001b[36m3.1825\u001b[0m        \u001b[32m3.1733\u001b[0m  0.0245\n",
      "      5        \u001b[36m2.7594\u001b[0m        \u001b[32m2.7547\u001b[0m  0.0230\n",
      "      6        3.0215        3.0137  0.0260\n",
      "      4        \u001b[36m3.1909\u001b[0m        \u001b[32m3.2403\u001b[0m  0.0289\n",
      "      5        3.1886        \u001b[32m3.1725\u001b[0m  0.0257\n",
      "      6        2.7692        \u001b[32m2.7542\u001b[0m  0.0263\n",
      "      7        3.0193        3.0146  0.0228\n",
      "      5        3.2051        \u001b[32m3.2394\u001b[0m  0.0245\n",
      "      6        \u001b[36m3.1768\u001b[0m        \u001b[32m3.1712\u001b[0m  0.0269\n",
      "      7        \u001b[36m2.7531\u001b[0m        \u001b[32m2.7537\u001b[0m  0.0270\n",
      "      8        3.0250        3.0150  0.0281\n",
      "      6        3.1926        \u001b[32m3.2380\u001b[0m  0.0235\n",
      "      7        3.1810        \u001b[32m3.1697\u001b[0m  0.0265\n",
      "      8        2.7757        \u001b[32m2.7532\u001b[0m  0.0231\n",
      "      9        3.0228        3.0154  0.0247\n",
      "      7        3.1924        \u001b[32m3.2364\u001b[0m  0.0249\n",
      "      8        3.1788        \u001b[32m3.1678\u001b[0m  0.0229\n",
      "      9        \u001b[36m2.7522\u001b[0m        \u001b[32m2.7526\u001b[0m  0.0213\n",
      "     10        3.0178        3.0159  0.0254\n",
      "Restoring best model from epoch 1.\n",
      "      8        3.1950        \u001b[32m3.2343\u001b[0m  0.0278\n",
      "     10        2.7655        \u001b[32m2.7522\u001b[0m  0.0231\n",
      "Restoring best model from epoch 1.\n",
      "      9        3.1872        \u001b[32m3.1656\u001b[0m  0.0244\n",
      "      9        \u001b[36m3.1749\u001b[0m        \u001b[32m3.2318\u001b[0m  0.0199\n",
      "     10        3.1812        \u001b[32m3.1627\u001b[0m  0.0208\n",
      "Restoring best model from epoch 1.\n",
      "     10        3.1861        \u001b[32m3.2293\u001b[0m  0.0189\n",
      "Restoring best model from epoch 1.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.2118\u001b[0m        \u001b[32m3.1747\u001b[0m  0.0411\n",
      "      2        \u001b[36m3.2086\u001b[0m        \u001b[32m3.1731\u001b[0m  0.0193\n",
      "      3        3.2103        \u001b[32m3.1713\u001b[0m  0.0181\n",
      "      4        \u001b[36m3.1987\u001b[0m        \u001b[32m3.1690\u001b[0m  0.0184\n",
      "      5        3.2046        \u001b[32m3.1659\u001b[0m  0.0177\n",
      "      6        3.2022        \u001b[32m3.1625\u001b[0m  0.0192\n",
      "      7        3.2000        \u001b[32m3.1585\u001b[0m  0.0184\n",
      "      8        \u001b[36m3.1893\u001b[0m        \u001b[32m3.1543\u001b[0m  0.0185\n",
      "      9        3.1953        \u001b[32m3.1506\u001b[0m  0.0181\n",
      "     10        \u001b[36m3.1762\u001b[0m        \u001b[32m3.1469\u001b[0m  0.0183\n",
      "Restoring best model from epoch 1.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.1214\u001b[0m        \u001b[32m3.1196\u001b[0m  0.0467\n",
      "      2        3.1216        \u001b[32m3.1187\u001b[0m  0.0424\n",
      "      3        \u001b[36m3.1158\u001b[0m        \u001b[32m3.1170\u001b[0m  0.0428\n",
      "      4        \u001b[36m3.1141\u001b[0m        \u001b[32m3.1148\u001b[0m  0.0415\n",
      "      5        3.1195        \u001b[32m3.1119\u001b[0m  0.0407\n",
      "      6        \u001b[36m3.1136\u001b[0m        \u001b[32m3.1089\u001b[0m  0.0412\n",
      "      7        \u001b[36m3.1114\u001b[0m        \u001b[32m3.1059\u001b[0m  0.0421\n",
      "      8        \u001b[36m3.1012\u001b[0m        \u001b[32m3.1022\u001b[0m  0.0410\n",
      "      9        \u001b[36m3.0993\u001b[0m        \u001b[32m3.0985\u001b[0m  0.0416\n",
      "     10        3.1132        \u001b[32m3.0953\u001b[0m  0.0415\n",
      "Restoring best model from epoch 1.\n",
      "max best test predictions -0.005752515\n",
      "granularity 0.0333241953125\n",
      "integration times 10595\n",
      "Concordance Index 0.4277467373152176\n",
      "Integrated Brier Score: 0.17340833437001005\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.7843\u001b[0m        \u001b[32m2.7602\u001b[0m  0.0225\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.0120\u001b[0m        \u001b[32m2.9833\u001b[0m  0.0324\n",
      "      2        \u001b[36m2.7651\u001b[0m        2.7605  0.0359\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.2174\u001b[0m        \u001b[32m3.1851\u001b[0m  0.0374\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.2385\u001b[0m        \u001b[32m3.2066\u001b[0m  0.0360\n",
      "      2        \u001b[36m3.0074\u001b[0m        \u001b[32m2.9829\u001b[0m  0.0243\n",
      "      3        2.7826        2.7607  0.0319\n",
      "      2        \u001b[36m3.2046\u001b[0m        \u001b[32m3.1842\u001b[0m  0.0315\n",
      "      2        \u001b[36m3.2352\u001b[0m        \u001b[32m3.2052\u001b[0m  0.0312\n",
      "      3        \u001b[36m2.9987\u001b[0m        \u001b[32m2.9827\u001b[0m  0.0274\n",
      "      4        2.7917        2.7608  0.0231\n",
      "      3        \u001b[36m3.2306\u001b[0m        \u001b[32m3.2036\u001b[0m  0.0213\n",
      "      3        \u001b[36m3.1928\u001b[0m        \u001b[32m3.1832\u001b[0m  0.0248\n",
      "      4        3.0057        \u001b[32m2.9825\u001b[0m  0.0256\n",
      "      5        2.7909        2.7606  0.0239\n",
      "      4        \u001b[36m3.2274\u001b[0m        \u001b[32m3.2017\u001b[0m  0.0225\n",
      "      4        3.2140        \u001b[32m3.1822\u001b[0m  0.0253\n",
      "      5        \u001b[36m2.9952\u001b[0m        \u001b[32m2.9823\u001b[0m  0.0242\n",
      "      6        2.7928        \u001b[32m2.7600\u001b[0m  0.0209\n",
      "      5        \u001b[36m3.2243\u001b[0m        \u001b[32m3.1996\u001b[0m  0.0232\n",
      "      5        3.2096        \u001b[32m3.1811\u001b[0m  0.0220\n",
      "      6        3.0113        \u001b[32m2.9822\u001b[0m  0.0221\n",
      "      7        2.7829        \u001b[32m2.7594\u001b[0m  0.0206\n",
      "      6        3.2040        \u001b[32m3.1798\u001b[0m  0.0216\n",
      "      6        \u001b[36m3.2198\u001b[0m        \u001b[32m3.1971\u001b[0m  0.0239\n",
      "      7        3.0137        \u001b[32m2.9820\u001b[0m  0.0257\n",
      "      8        2.7865        \u001b[32m2.7586\u001b[0m  0.0250\n",
      "      7        3.2339        \u001b[32m3.1942\u001b[0m  0.0251\n",
      "      7        3.2147        \u001b[32m3.1788\u001b[0m  0.0263\n",
      "      8        \u001b[36m2.9904\u001b[0m        \u001b[32m2.9817\u001b[0m  0.0218\n",
      "      9        2.7902        \u001b[32m2.7579\u001b[0m  0.0217\n",
      "      8        3.2383        \u001b[32m3.1914\u001b[0m  0.0223\n",
      "      8        3.2072        \u001b[32m3.1780\u001b[0m  0.0224\n",
      "      9        3.0036        \u001b[32m2.9813\u001b[0m  0.0213\n",
      "     10        2.7726        \u001b[32m2.7570\u001b[0m  0.0214\n",
      "Restoring best model from epoch 1.\n",
      "      9        \u001b[36m3.2152\u001b[0m        \u001b[32m3.1883\u001b[0m  0.0237\n",
      "      9        \u001b[36m3.1899\u001b[0m        \u001b[32m3.1778\u001b[0m  0.0228\n",
      "     10        3.0118        \u001b[32m2.9808\u001b[0m  0.0214\n",
      "Restoring best model from epoch 1.\n",
      "     10        3.1944        3.1786  0.0222\n",
      "Restoring best model from epoch 1.\n",
      "     10        3.2272        \u001b[32m3.1854\u001b[0m  0.0231\n",
      "Restoring best model from epoch 1.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.1830\u001b[0m        \u001b[32m3.2126\u001b[0m  0.0375\n",
      "      2        3.1882        \u001b[32m3.2122\u001b[0m  0.0201\n",
      "      3        3.2003        \u001b[32m3.2114\u001b[0m  0.0195\n",
      "      4        3.1956        \u001b[32m3.2104\u001b[0m  0.0267\n",
      "      5        3.1835        \u001b[32m3.2092\u001b[0m  0.0190\n",
      "      6        \u001b[36m3.1803\u001b[0m        \u001b[32m3.2075\u001b[0m  0.0188\n",
      "      7        3.1839        \u001b[32m3.2055\u001b[0m  0.0186\n",
      "      8        \u001b[36m3.1796\u001b[0m        \u001b[32m3.2032\u001b[0m  0.0187\n",
      "      9        \u001b[36m3.1758\u001b[0m        \u001b[32m3.2005\u001b[0m  0.0184\n",
      "     10        3.1859        \u001b[32m3.1978\u001b[0m  0.0188\n",
      "Restoring best model from epoch 1.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.1281\u001b[0m        \u001b[32m3.1232\u001b[0m  0.0461\n",
      "      2        \u001b[36m3.1166\u001b[0m        \u001b[32m3.1214\u001b[0m  0.0420\n",
      "      3        3.1172        \u001b[32m3.1181\u001b[0m  0.0422\n",
      "      4        \u001b[36m3.1126\u001b[0m        \u001b[32m3.1128\u001b[0m  0.0422\n",
      "      5        \u001b[36m3.1096\u001b[0m        \u001b[32m3.1075\u001b[0m  0.0425\n",
      "      6        \u001b[36m3.1093\u001b[0m        \u001b[32m3.1047\u001b[0m  0.0425\n",
      "      7        \u001b[36m3.1052\u001b[0m        \u001b[32m3.1030\u001b[0m  0.0422\n",
      "      8        \u001b[36m3.0996\u001b[0m        3.1036  0.0418\n",
      "      9        3.1078        3.1072  0.0429\n",
      "     10        3.1033        3.1088  0.0426\n",
      "Restoring best model from epoch 1.\n",
      "max best test predictions -0.0040259846\n",
      "granularity 0.0333241953125\n",
      "integration times 10505\n",
      "Concordance Index 0.40716729695660003\n",
      "Integrated Brier Score: 0.1825965617841864\n",
      "load_flchain\n",
      "split age           float32\n",
      "sex             uint8\n",
      "sample_yr     float32\n",
      "kappa         float32\n",
      "lambda        float32\n",
      "flc_grp       float32\n",
      "creatinine    float32\n",
      "mgus            uint8\n",
      "dtype: object\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m1.6374\u001b[0m        \u001b[32m1.5658\u001b[0m  0.1584\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.1854\u001b[0m        \u001b[32m2.1650\u001b[0m  0.1593\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.9057\u001b[0m        \u001b[32m2.8673\u001b[0m  0.1743\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.1802\u001b[0m        \u001b[32m3.1493\u001b[0m  0.1716\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.1318\u001b[0m        \u001b[32m3.0862\u001b[0m  0.1957\n",
      "      2        \u001b[36m1.6129\u001b[0m        \u001b[32m1.5402\u001b[0m  0.2058\n",
      "      2        \u001b[36m2.1031\u001b[0m        \u001b[32m2.0886\u001b[0m  0.2030\n",
      "      2        \u001b[36m2.8648\u001b[0m        \u001b[32m2.8221\u001b[0m  0.2022\n",
      "      2        \u001b[36m3.1697\u001b[0m        \u001b[32m3.1164\u001b[0m  0.1924\n",
      "      2        \u001b[36m3.1045\u001b[0m        \u001b[32m3.0495\u001b[0m  0.2244\n",
      "      3        \u001b[36m1.5762\u001b[0m        \u001b[32m1.5316\u001b[0m  0.1346\n",
      "      3        \u001b[36m2.0621\u001b[0m        \u001b[32m2.0459\u001b[0m  0.1418\n",
      "      3        \u001b[36m2.8075\u001b[0m        \u001b[32m2.7354\u001b[0m  0.1576\n",
      "      3        \u001b[36m3.0490\u001b[0m        \u001b[32m2.9694\u001b[0m  0.1579\n",
      "      3        \u001b[36m3.1191\u001b[0m        \u001b[32m3.0414\u001b[0m  0.2151\n",
      "      4        \u001b[36m1.5611\u001b[0m        1.5325  0.1432\n",
      "      4        \u001b[36m2.7744\u001b[0m        \u001b[32m2.6927\u001b[0m  0.1507\n",
      "      4        \u001b[36m2.0435\u001b[0m        \u001b[32m2.0289\u001b[0m  0.2065\n",
      "      4        \u001b[36m2.9970\u001b[0m        \u001b[32m2.9300\u001b[0m  0.1372\n",
      "      4        \u001b[36m3.0621\u001b[0m        \u001b[32m2.9926\u001b[0m  0.1360\n",
      "      5        1.5636        1.5317  0.1387\n",
      "      5        \u001b[36m2.7590\u001b[0m        \u001b[32m2.6772\u001b[0m  0.1510\n",
      "      5        \u001b[36m2.0365\u001b[0m        \u001b[32m2.0162\u001b[0m  0.1413\n",
      "      5        \u001b[36m2.9795\u001b[0m        \u001b[32m2.9099\u001b[0m  0.1527\n",
      "      5        \u001b[36m3.0324\u001b[0m        \u001b[32m2.9699\u001b[0m  0.1478\n",
      "      6        1.5704        1.5322  0.1467\n",
      "      6        \u001b[36m2.0318\u001b[0m        \u001b[32m2.0132\u001b[0m  0.1385\n",
      "      6        \u001b[36m2.7462\u001b[0m        \u001b[32m2.6670\u001b[0m  0.1564\n",
      "      6        \u001b[36m2.9713\u001b[0m        \u001b[32m2.9035\u001b[0m  0.1448\n",
      "      6        \u001b[36m3.0135\u001b[0m        \u001b[32m2.9623\u001b[0m  0.1569\n",
      "      7        1.5674        1.5322  0.1373\n",
      "      7        \u001b[36m2.0286\u001b[0m        \u001b[32m2.0110\u001b[0m  0.1342\n",
      "      7        \u001b[36m2.7393\u001b[0m        \u001b[32m2.6644\u001b[0m  0.1579\n",
      "      7        \u001b[36m2.9640\u001b[0m        \u001b[32m2.9008\u001b[0m  0.1505\n",
      "      8        1.5650        1.5338  0.1302\n",
      "      7        \u001b[36m2.9993\u001b[0m        \u001b[32m2.9454\u001b[0m  0.1568\n",
      "      8        \u001b[36m2.0184\u001b[0m        \u001b[32m2.0077\u001b[0m  0.1474\n",
      "      8        \u001b[36m2.7324\u001b[0m        \u001b[32m2.6583\u001b[0m  0.1705\n",
      "      9        1.5617        1.5342  0.1466\n",
      "      8        \u001b[36m2.9986\u001b[0m        \u001b[32m2.9409\u001b[0m  0.1608\n",
      "      8        \u001b[36m2.9545\u001b[0m        \u001b[32m2.8960\u001b[0m  0.1839\n",
      "      9        2.0238        \u001b[32m2.0076\u001b[0m  0.1964\n",
      "     10        1.5642        1.5337  0.1384\n",
      "Restoring best model from epoch 2.\n",
      "      9        2.7327           nan  0.1413\n",
      "      9        \u001b[36m2.9876\u001b[0m        \u001b[32m2.9319\u001b[0m  0.1527\n",
      "      9        2.9551        \u001b[32m2.8924\u001b[0m  0.1833\n",
      "     10        2.0195        \u001b[32m2.0048\u001b[0m  0.1340\n",
      "Restoring best model from epoch 5.\n",
      "     10        \u001b[36m2.7316\u001b[0m           nan  0.1346\n",
      "Restoring best model from epoch 7.\n",
      "     10        2.9925        \u001b[32m2.9315\u001b[0m  0.1220\n",
      "Restoring best model from epoch 9.\n",
      "     10        \u001b[36m2.9541\u001b[0m        2.8980  0.1739\n",
      "Restoring best model from epoch 8.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.6241\u001b[0m        \u001b[32m2.6001\u001b[0m  0.1760\n",
      "      2        \u001b[36m2.5519\u001b[0m        \u001b[32m2.5341\u001b[0m  0.1693\n",
      "      3        \u001b[36m2.5018\u001b[0m        \u001b[32m2.4857\u001b[0m  0.1724\n",
      "      4        \u001b[36m2.4749\u001b[0m        \u001b[32m2.4692\u001b[0m  0.1783\n",
      "      5        \u001b[36m2.4649\u001b[0m        \u001b[32m2.4643\u001b[0m  0.1705\n",
      "      6        \u001b[36m2.4642\u001b[0m        2.4709  0.1714\n",
      "      7        \u001b[36m2.4586\u001b[0m        \u001b[32m2.4630\u001b[0m  0.1714\n",
      "      8        \u001b[36m2.4565\u001b[0m        \u001b[32m2.4581\u001b[0m  0.1708\n",
      "      9        2.4620        \u001b[32m2.4571\u001b[0m  0.1710\n",
      "     10        \u001b[36m2.4470\u001b[0m        \u001b[32m2.4412\u001b[0m  0.1698\n",
      "Restoring best model from epoch 8.\n",
      "max best test predictions 9.97147\n",
      "granularity 0.999999\n",
      "integration times 111637241\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 61\u001b[0m\n\u001b[1;32m     16\u001b[0m X, y \u001b[39m=\u001b[39m sort_X_y_pandas(X, y)\n\u001b[1;32m     18\u001b[0m net \u001b[39m=\u001b[39m NeuralNet(\n\u001b[1;32m     19\u001b[0m     SurvivalModel, \n\u001b[1;32m     20\u001b[0m     module__n_layers \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     59\u001b[0m     verbose\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m\n\u001b[1;32m     60\u001b[0m )\n\u001b[0;32m---> 61\u001b[0m df_agg_metrics_cindex, df_agg_metrics_ibs, best_model,params, outer_scores, best_preds_train, best_preds_test \u001b[39m=\u001b[39m train_eval(X, y, net, n_iter, data\u001b[39m.\u001b[39;49mfilename)\n\u001b[1;32m     62\u001b[0m agg_metrics_cindex\u001b[39m.\u001b[39mappend(df_agg_metrics_cindex)\n\u001b[1;32m     63\u001b[0m agg_metrics_ibs\u001b[39m.\u001b[39mappend(df_agg_metrics_ibs)\n",
      "Cell \u001b[0;32mIn[10], line 58\u001b[0m, in \u001b[0;36mtrain_eval\u001b[0;34m(X, y, net, n_iter, filename)\u001b[0m\n\u001b[1;32m     55\u001b[0m outer_scores[\u001b[39m'\u001b[39m\u001b[39mibs_train_\u001b[39m\u001b[39m'\u001b[39m\u001b[39m+\u001b[39mdataset_name] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m [np\u001b[39m.\u001b[39mnan]\n\u001b[1;32m     57\u001b[0m \u001b[39m#try:\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m cum_hazard_test \u001b[39m=\u001b[39m get_cumulative_hazard_function_ah(\n\u001b[1;32m     59\u001b[0m         X_train\u001b[39m.\u001b[39;49mvalues, X_test\u001b[39m.\u001b[39;49mvalues, y_train\u001b[39m.\u001b[39;49mvalues, y_test\u001b[39m.\u001b[39;49mvalues,\n\u001b[1;32m     60\u001b[0m         best_preds_train\u001b[39m.\u001b[39;49mreshape(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m), best_preds_test\u001b[39m.\u001b[39;49mreshape(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m     61\u001b[0m         )\n\u001b[1;32m     62\u001b[0m df_survival_test \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mexp(\u001b[39m-\u001b[39mcum_hazard_test)\n\u001b[1;32m     63\u001b[0m durations_test, events_test \u001b[39m=\u001b[39m transform_back(y_test\u001b[39m.\u001b[39mvalues)\n",
      "File \u001b[0;32m~/miniconda3/envs/xgbsurv/lib/python3.11/site-packages/xgbsurv/models/eh_ah_final.py:355\u001b[0m, in \u001b[0;36mget_cumulative_hazard_function_ah\u001b[0;34m(X_train, X_test, y_train, y_test, predictor_train, predictor_test)\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mintegration times\u001b[39m\u001b[39m'\u001b[39m,integration_values\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m])\n\u001b[1;32m    352\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, integration_values\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]):\n\u001b[1;32m    353\u001b[0m     integration_values[_] \u001b[39m=\u001b[39m (\n\u001b[1;32m    354\u001b[0m         integration_values[_ \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m]\n\u001b[0;32m--> 355\u001b[0m         \u001b[39m+\u001b[39m quadrature(\n\u001b[1;32m    356\u001b[0m             func\u001b[39m=\u001b[39;49mhazard_function_integrate,\n\u001b[1;32m    357\u001b[0m             a\u001b[39m=\u001b[39;49mintegration_times[_ \u001b[39m-\u001b[39;49m \u001b[39m1\u001b[39;49m],\n\u001b[1;32m    358\u001b[0m             b\u001b[39m=\u001b[39;49mintegration_times[_],\n\u001b[1;32m    359\u001b[0m             vec_func\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    360\u001b[0m         )[\u001b[39m0\u001b[39m]\n\u001b[1;32m    361\u001b[0m     )\n\u001b[1;32m    363\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_samples):\n\u001b[1;32m    364\u001b[0m     cumulative_hazard[_] \u001b[39m=\u001b[39m (\n\u001b[1;32m    365\u001b[0m         integration_values[\n\u001b[1;32m    366\u001b[0m             np\u001b[39m.\u001b[39mdigitize(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    371\u001b[0m         \u001b[39m*\u001b[39m theta_min[_]\n\u001b[1;32m    372\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/xgbsurv/lib/python3.11/site-packages/scipy/integrate/_quadrature.py:365\u001b[0m, in \u001b[0;36mquadrature\u001b[0;34m(func, a, b, args, tol, rtol, maxiter, vec_func, miniter)\u001b[0m\n\u001b[1;32m    363\u001b[0m maxiter \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(miniter\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m, maxiter)\n\u001b[1;32m    364\u001b[0m \u001b[39mfor\u001b[39;00m n \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(miniter, maxiter\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m--> 365\u001b[0m     newval \u001b[39m=\u001b[39m fixed_quad(vfunc, a, b, (), n)[\u001b[39m0\u001b[39m]\n\u001b[1;32m    366\u001b[0m     err \u001b[39m=\u001b[39m \u001b[39mabs\u001b[39m(newval\u001b[39m-\u001b[39mval)\n\u001b[1;32m    367\u001b[0m     val \u001b[39m=\u001b[39m newval\n",
      "File \u001b[0;32m~/miniconda3/envs/xgbsurv/lib/python3.11/site-packages/scipy/integrate/_quadrature.py:245\u001b[0m, in \u001b[0;36mfixed_quad\u001b[0;34m(func, a, b, args, n)\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mGaussian quadrature is only available for \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    243\u001b[0m                      \u001b[39m\"\u001b[39m\u001b[39mfinite limits.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    244\u001b[0m y \u001b[39m=\u001b[39m (b\u001b[39m-\u001b[39ma)\u001b[39m*\u001b[39m(x\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m/\u001b[39m\u001b[39m2.0\u001b[39m \u001b[39m+\u001b[39m a\n\u001b[0;32m--> 245\u001b[0m \u001b[39mreturn\u001b[39;00m (b\u001b[39m-\u001b[39ma)\u001b[39m/\u001b[39m\u001b[39m2.0\u001b[39m \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39msum(w\u001b[39m*\u001b[39mfunc(y, \u001b[39m*\u001b[39;49margs), axis\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m), \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/xgbsurv/lib/python3.11/site-packages/scipy/integrate/_quadrature.py:282\u001b[0m, in \u001b[0;36mvectorize1.<locals>.vfunc\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    280\u001b[0m x \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(x)\n\u001b[1;32m    281\u001b[0m \u001b[39m# call with first point to get output type\u001b[39;00m\n\u001b[0;32m--> 282\u001b[0m y0 \u001b[39m=\u001b[39m func(x[\u001b[39m0\u001b[39;49m], \u001b[39m*\u001b[39;49margs)\n\u001b[1;32m    283\u001b[0m n \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(x)\n\u001b[1;32m    284\u001b[0m dtype \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(y0, \u001b[39m'\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mtype\u001b[39m(y0))\n",
      "File \u001b[0;32m~/miniconda3/envs/xgbsurv/lib/python3.11/site-packages/xgbsurv/models/eh_ah_final.py:333\u001b[0m, in \u001b[0;36mget_cumulative_hazard_function_ah.<locals>.hazard_function_integrate\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mhazard_function_integrate\u001b[39m(s):\n\u001b[0;32m--> 333\u001b[0m     \u001b[39mreturn\u001b[39;00m baseline_hazard_estimator_ah(\n\u001b[1;32m    334\u001b[0m         time\u001b[39m=\u001b[39;49ms,\n\u001b[1;32m    335\u001b[0m         time_train\u001b[39m=\u001b[39;49mtime_train,\n\u001b[1;32m    336\u001b[0m         event_train\u001b[39m=\u001b[39;49mevent_train,\n\u001b[1;32m    337\u001b[0m         predictor_train\u001b[39m=\u001b[39;49mpredictor_train,\n\u001b[1;32m    338\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data_set_fns = [ load_metabric, load_flchain, load_rgbsg, load_support] #load_metabric, , load_flchain, load_rgbsg, load_support, load_tcga]\n",
    "data_set_fns_str = ['load_metabric', 'load_flchain', 'load_rgbsg', 'load_support'] \n",
    "one_hot_dict = {'load_flchain': ['mgus'], 'load_support':['cancer'], 'load_rgbsg':['grade']}\n",
    "agg_metrics_cindex = []\n",
    "agg_metrics_ibs = []\n",
    "\n",
    "for idx, dataset in enumerate(data_set_fns):\n",
    "    # get name of current dataset\n",
    "    data = dataset(path=\"/Users/JUSC/Documents/xgbsurv/xgbsurv/datasets/data/\", as_frame=True)\n",
    "    X  = data.data #.astype(np.float32)\n",
    "    y = data.target #.values #.to_numpy()\n",
    "\n",
    "    print(data_set_fns_str[idx])\n",
    "    #if data_set_fns_str[idx] in one_hot_dict.keys():\n",
    "    #    X = pd.get_dummies(X, columns=one_hot_dict[data_set_fns_str[idx]])\n",
    "    X, y = sort_X_y_pandas(X, y)\n",
    "    \n",
    "    net = NeuralNet(\n",
    "        SurvivalModel, \n",
    "        module__n_layers = 1,\n",
    "        module__input_units = X.shape[1],\n",
    "        #module__num_nodes = 32,\n",
    "        #module__dropout = 0.1, # these could also be removed\n",
    "        module__out_features = 1,\n",
    "        # for split sizes when result size = 1\n",
    "        iterator_train__drop_last=True,\n",
    "        #iterator_valid__drop_last=True,\n",
    "        criterion=AHLoss,\n",
    "        optimizer=torch.optim.AdamW,\n",
    "        optimizer__weight_decay = 0.4,\n",
    "        batch_size=32, # separate train and valid->iterator_train__batch_size=128 and iterator_valid__batch_size=128 ?\n",
    "        callbacks=[\n",
    "            (\n",
    "                \"sched\",\n",
    "                LRScheduler(\n",
    "                    torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
    "                    monitor=\"valid_loss\",\n",
    "                    patience=5,\n",
    "                ),\n",
    "            ),\n",
    "            (\n",
    "                \"es\",\n",
    "                EarlyStopping(\n",
    "                    monitor=\"valid_loss\",\n",
    "                    patience=early_stopping_rounds,\n",
    "                    load_best=True,\n",
    "                    threshold = 0.01,\n",
    "                    #, threshold_mode='rel', \n",
    "                    # lower_is_better=True\n",
    "                ),\n",
    "            ),\n",
    "            (\"seed\", FixSeed(seed=42)),\n",
    "            #(\"Input Shape Setter\",InputShapeSetter())\n",
    "        ],\n",
    "        \n",
    "        train_split = CustomValidSplit(0.2, stratified=True, random_state=rand_state), \n",
    "        #max_epochs=1, #0,#100\n",
    "        #train_split=None,\n",
    "        verbose=2\n",
    "    )\n",
    "    df_agg_metrics_cindex, df_agg_metrics_ibs, best_model,params, outer_scores, best_preds_train, best_preds_test = train_eval(X, y, net, n_iter, data.filename)\n",
    "    agg_metrics_cindex.append(df_agg_metrics_cindex)\n",
    "    agg_metrics_ibs.append(df_agg_metrics_ibs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>cindex_train_mean</th>\n",
       "      <th>cindex_train_std</th>\n",
       "      <th>cindex_test_mean</th>\n",
       "      <th>cindex_test_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>METABRIC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FLCHAIN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RGBSG</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SUPPORT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    dataset  cindex_train_mean  cindex_train_std  cindex_test_mean   \n",
       "0  METABRIC                NaN               NaN               NaN  \\\n",
       "0   FLCHAIN                NaN               NaN               NaN   \n",
       "0     RGBSG                NaN               NaN               NaN   \n",
       "0   SUPPORT                NaN               NaN               NaN   \n",
       "\n",
       "   cindex_test_std  \n",
       "0              NaN  \n",
       "0              NaN  \n",
       "0              NaN  \n",
       "0              NaN  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final_ah_1_cindex = pd.concat([df for df in agg_metrics_cindex]).round(4)\n",
    "df_final_ah_1_cindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>cindex_train_mean</th>\n",
       "      <th>cindex_train_std</th>\n",
       "      <th>cindex_test_mean</th>\n",
       "      <th>cindex_test_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>METABRIC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FLCHAIN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RGBSG</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SUPPORT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    dataset  cindex_train_mean  cindex_train_std  cindex_test_mean   \n",
       "0  METABRIC                NaN               NaN               NaN  \\\n",
       "0   FLCHAIN                NaN               NaN               NaN   \n",
       "0     RGBSG                NaN               NaN               NaN   \n",
       "0   SUPPORT                NaN               NaN               NaN   \n",
       "\n",
       "   cindex_test_std  \n",
       "0              NaN  \n",
       "0              NaN  \n",
       "0              NaN  \n",
       "0              NaN  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final_ah_1_cindex = pd.concat([df for df in agg_metrics_cindex]).round(4)\n",
    "df_final_ah_1_cindex.to_csv('metrics/final_deep_learning_ah_1_cindex.csv', index=False)\n",
    "df_final_ah_1_cindex.to_csv('/Users/JUSC/Documents/644928e0fb7e147893e8ec15/05_thesis/tables/final_deep_learning_ah_1_cindex.csv', index=False)  #\n",
    "df_final_ah_1_cindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>ibs_train_mean</th>\n",
       "      <th>ibs_train_std</th>\n",
       "      <th>ibs_test_mean</th>\n",
       "      <th>ibs_test_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>METABRIC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FLCHAIN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RGBSG</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SUPPORT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    dataset  ibs_train_mean  ibs_train_std  ibs_test_mean  ibs_test_std\n",
       "0  METABRIC             NaN            NaN            NaN           NaN\n",
       "0   FLCHAIN             NaN            NaN            NaN           NaN\n",
       "0     RGBSG             NaN            NaN            NaN           NaN\n",
       "0   SUPPORT             NaN            NaN            NaN           NaN"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final_ah_1_ibs = pd.concat([df for df in agg_metrics_ibs]).round(4)\n",
    "df_final_ah_1_ibs.to_csv('metrics/final_deep_learning_ah_1_ibs.csv', index=False)\n",
    "df_final_ah_1_ibs.to_csv('/Users/JUSC/Documents/644928e0fb7e147893e8ec15/05_thesis/tables/final_deep_learning_ah_1_ibs.csv', index=False) \n",
    "df_final_ah_1_ibs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TCGA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_ah_tcga = {\n",
    "    'estimator__module__n_layers': [1, 2, 4],\n",
    "    'estimator__module__num_nodes': [64, 128, 256, 512],\n",
    "    'estimator__module__dropout': scuniform(0.0,0.7),\n",
    "    'estimator__optimizer__weight_decay': [0.4, 0.2, 0.1, 0.05, 0.02, 0.01, 0],\n",
    "    'estimator__batch_size': [64, 128, 256, 512, 1024],\n",
    "    #lr not in paper because of learning rate finder\n",
    "    # note: setting learning rate higher would make exp(partial_hazard) explode\n",
    "    #'estimator__lr': scloguniform(0.001,0.01), # scheduler unten einbauen\n",
    "    # use callback instead\n",
    "    'estimator__lr':[0.001],\n",
    "    #'estimator__max_epochs':  scrandint(10,20), # corresponds to num_rounds\n",
    "    'pca__n_components':[8, 16, 32, 64]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_eval(X, y, net, n_iter, filename):\n",
    "        model = 'ah_'\n",
    "        dataset_name = filename.split('_')[0]\n",
    "        # add IBS later\n",
    "        outer_scores = { 'cindex_test_'+dataset_name:[],\n",
    "                        'ibs_test_'+dataset_name:[]}\n",
    "        best_params = {'best_params_'+dataset_name:[]}\n",
    "        best_model = {'best_model_'+dataset_name:[]}\n",
    "        ct = make_column_transformer(\n",
    "                #(OneHotEncoder(sparse_output=False), make_column_selector(dtype_include=['category', 'object']))\n",
    "                (StandardScaler(), make_column_selector(dtype_include=['float32']))\n",
    "                ,remainder='drop')\n",
    "        pipe = Pipeline([('scaler',ct),\n",
    "                         ('pca', PCA()),#n_components=10\n",
    "                        ('estimator', net)])\n",
    "        rs = RandomizedSearchCV(pipe, param_grid_ah_tcga, scoring = scoring_function, n_jobs=-1, \n",
    "                                    n_iter=2, refit=True, random_state=rand_state)\n",
    "        for i, (train_index, test_index) in enumerate(outer_custom_cv.split(X, y)):\n",
    "                # Split data into training and testing sets for outer fold\n",
    "                X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "                y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "                X_train, y_train = sort_X_y_pandas(X_train, y_train)\n",
    "                X_test, y_test = sort_X_y_pandas(X_test, y_test)\n",
    "\n",
    "                #print(X_train.shape, type(X_train))\n",
    "                #print(y_train.shape, type(y_train))\n",
    "                #print(X_test.shape, type(X_test))\n",
    "                #print(y_test.shape, type(y_test))\n",
    "                # save splits and data\n",
    "                savetxt('splits/train_index_'+str(i)+'_'+filename, train_index, delimiter=',')\n",
    "                savetxt('splits/test_index_'+str(i)+'_'+filename, test_index, delimiter=',')\n",
    "                \n",
    "                # savetxt('splits/X_train_'+str(i)+'_'+filename, X_train, delimiter=',')\n",
    "                # savetxt('splits/X_test_'+str(i)+'_'+filename, X_test, delimiter=',')\n",
    "\n",
    "                # savetxt('splits/y_train_'+str(i)+'_'+filename, y_train, delimiter=',')\n",
    "                # savetxt('splits/y_test_'+str(i)+'_'+filename, y_test, delimiter=',')\n",
    "\n",
    "                rs.fit(X_train, y_train)\n",
    "                best_preds_train = rs.best_estimator_.predict(X_train)\n",
    "                best_preds_test = rs.best_estimator_.predict(X_test)\n",
    "\n",
    "                savetxt('predictions/'+model+'best_preds_train_'+str(i)+'_'+filename, best_preds_train, delimiter=',')\n",
    "                savetxt('predictions/'+model+'best_preds_test_'+str(i)+'_'+filename, best_preds_test, delimiter=',')\n",
    "\n",
    "                \n",
    "                # save hyperparameter settings\n",
    "                params = rs.best_estimator_.get_params\n",
    "                best_params['best_params_'+dataset_name] += [rs.best_params_]\n",
    "                best_model['best_model_'+dataset_name] += [params]\n",
    "\n",
    "                    \n",
    "                try:\n",
    "                    cum_hazard_test = get_cumulative_hazard_function_ah(\n",
    "                            X_train.values, X_test.values, y_train.values, y_test.values,\n",
    "                            best_preds_train.reshape(-1), best_preds_test.reshape(-1)\n",
    "                            )\n",
    "                    df_survival_test = np.exp(-cum_hazard_test)\n",
    "                    durations_test, events_test = transform_back(y_test.values)\n",
    "                    time_grid_test = np.linspace(durations_test.min(), durations_test.max(), 100)\n",
    "                    ev = EvalSurv(df_survival_test, durations_test, events_test, censor_surv='km')\n",
    "                    print('Concordance Index',ev.concordance_td('antolini'))\n",
    "                    print('Integrated Brier Score:',ev.integrated_brier_score(time_grid_test))\n",
    "                    cindex_score_test = ev.concordance_td('antolini')\n",
    "                    ibs_score_test = ev.integrated_brier_score(time_grid_test)\n",
    "\n",
    "                    outer_scores['cindex_test_'+dataset_name] += [cindex_score_test]\n",
    "                    outer_scores['ibs_test_'+dataset_name] += [ibs_score_test]\n",
    "                except: \n",
    "                    outer_scores['cindex_test_'+dataset_name] += [np.nan]\n",
    "                    outer_scores['ibs_test_'+dataset_name] += [np.nan]\n",
    "            \n",
    "        df_best_params = pd.DataFrame(best_params)\n",
    "        df_best_model = pd.DataFrame(best_model)\n",
    "        df_outer_scores = pd.DataFrame(outer_scores)\n",
    "        df_metrics = pd.concat([df_best_params,df_best_model,df_outer_scores], axis=1)\n",
    "        df_metrics.to_csv('metrics/'+model+'metric_summary'+str(i)+'_'+filename, index=False)\n",
    "        # cindex\n",
    "        df_agg_metrics_cindex = pd.DataFrame({'dataset':[dataset_name],\n",
    "                                              'cindex_test_mean':df_outer_scores['cindex_test_'+dataset_name].mean(),\n",
    "                                              'cindex_test_std':df_outer_scores['cindex_test_'+dataset_name].std() })\n",
    "        # IBS\n",
    "        df_agg_metrics_ibs = pd.DataFrame({'dataset':[dataset_name],\n",
    "                                              'ibs_test_mean':df_outer_scores['ibs_test_'+dataset_name].mean(),\n",
    "                                              'ibs_test_std':df_outer_scores['ibs_test_'+dataset_name].std() })\n",
    "        \n",
    "        return df_agg_metrics_cindex, df_agg_metrics_ibs, best_model, best_params, outer_scores, best_preds_train, best_preds_test \n",
    "\n",
    "                \n",
    "#cv=inner_custom_cv,pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split gex_?|100130426      float32\n",
      "gex_?|100133144      float32\n",
      "gex_?|100134869      float32\n",
      "gex_?|10357          float32\n",
      "gex_?|10431          float32\n",
      "                      ...   \n",
      "gex_ZYG11A|440590    float32\n",
      "gex_ZYG11B|79699     float32\n",
      "gex_ZYX|7791         float32\n",
      "gex_ZZEF1|23140      float32\n",
      "gex_ZZZ3|26009       float32\n",
      "Length: 20531, dtype: object\n",
      "0.999999\n",
      "integration times 5006\n",
      "Concordance Index 0.4966850828729282\n",
      "Integrated Brier Score: 0.21002021217610758\n",
      "0.999999\n",
      "integration times 3605\n",
      "Concordance Index 0.4636810486073184\n",
      "Integrated Brier Score: 0.23022000907134266\n",
      "0.999999\n",
      "integration times 5521\n",
      "Concordance Index 0.5095579450418161\n",
      "Integrated Brier Score: 0.23069390264334527\n",
      "0.999999\n",
      "integration times 6102\n",
      "Concordance Index 0.5189003436426117\n",
      "Integrated Brier Score: 0.2403212408080216\n",
      "0.999999\n",
      "integration times 3308\n",
      "Concordance Index 0.5100193923723335\n",
      "Integrated Brier Score: 0.2108856228207395\n",
      "split gex_?|100130426      float32\n",
      "gex_?|100133144      float32\n",
      "gex_?|100134869      float32\n",
      "gex_?|10357          float32\n",
      "gex_?|10431          float32\n",
      "                      ...   \n",
      "gex_ZYG11A|440590    float32\n",
      "gex_ZYG11B|79699     float32\n",
      "gex_ZYX|7791         float32\n",
      "gex_ZZEF1|23140      float32\n",
      "gex_ZZZ3|26009       float32\n",
      "Length: 20531, dtype: object\n",
      "0.999999\n",
      "integration times 128458\n",
      "Concordance Index 0.6141493055555556\n",
      "Integrated Brier Score: 0.21212404035081694\n",
      "0.999999\n",
      "integration times 132192\n",
      "Concordance Index 0.5287403179779862\n",
      "Integrated Brier Score: 0.21191492339823836\n",
      "0.999999\n",
      "integration times 11664\n",
      "Concordance Index 0.5746268656716418\n",
      "Integrated Brier Score: 0.2023067661190666\n",
      "0.999999\n",
      "integration times 8160\n",
      "Concordance Index 0.3586615545486232\n",
      "Integrated Brier Score: 0.1836787448669659\n",
      "0.999999\n",
      "integration times 12076\n",
      "Concordance Index 0.45642829230168036\n",
      "Integrated Brier Score: 0.21901561692446334\n",
      "split gex_?|100130426      float32\n",
      "gex_?|100133144      float32\n",
      "gex_?|100134869      float32\n",
      "gex_?|10357          float32\n",
      "gex_?|10431          float32\n",
      "                      ...   \n",
      "gex_ZYG11A|440590    float32\n",
      "gex_ZYG11B|79699     float32\n",
      "gex_ZYX|7791         float32\n",
      "gex_ZZEF1|23140      float32\n",
      "gex_ZZZ3|26009       float32\n",
      "Length: 20531, dtype: object\n",
      "0.999999\n",
      "integration times 5617\n",
      "Concordance Index 0.497037295224817\n",
      "Integrated Brier Score: 0.21062071890563414\n",
      "0.999999\n",
      "integration times 4366\n",
      "Concordance Index 0.5278189910979229\n",
      "Integrated Brier Score: 0.1948161512930267\n",
      "0.999999\n",
      "integration times 7070\n",
      "Concordance Index 0.412781954887218\n",
      "Integrated Brier Score: 0.2055447599802149\n",
      "0.999999\n",
      "integration times 6113\n",
      "Concordance Index 0.4378265412748171\n",
      "Integrated Brier Score: 0.2090581821948773\n",
      "0.999999\n",
      "integration times 5163\n",
      "Concordance Index 0.42846768336964414\n",
      "Integrated Brier Score: 0.23708465254328337\n",
      "split gex_?|100130426      float32\n",
      "gex_?|100133144      float32\n",
      "gex_?|100134869      float32\n",
      "gex_?|10357          float32\n",
      "gex_?|10431          float32\n",
      "                      ...   \n",
      "gex_ZYG11A|440590    float32\n",
      "gex_ZYG11B|79699     float32\n",
      "gex_ZYX|7791         float32\n",
      "gex_ZZEF1|23140      float32\n",
      "gex_ZZZ3|26009       float32\n",
      "Length: 20531, dtype: object\n",
      "0.999999\n",
      "integration times 3669\n",
      "Concordance Index 0.5374732334047109\n",
      "Integrated Brier Score: 0.2061974347237981\n",
      "0.999999\n",
      "integration times 7135\n",
      "Concordance Index 0.5284738041002278\n",
      "Integrated Brier Score: 0.20980267250451234\n",
      "0.999999\n",
      "integration times 4360\n",
      "Concordance Index 0.5525200188412623\n",
      "Integrated Brier Score: 0.1912961570334101\n",
      "0.999999\n",
      "integration times 4656\n",
      "Concordance Index 0.5254087494476359\n",
      "Integrated Brier Score: 0.21180030701339878\n",
      "0.999999\n",
      "integration times 4939\n",
      "Concordance Index 0.5240793201133145\n",
      "Integrated Brier Score: 0.1918912039128326\n",
      "split gex_?|100130426      float32\n",
      "gex_?|100133144      float32\n",
      "gex_?|100134869      float32\n",
      "gex_?|10357          float32\n",
      "gex_?|10431          float32\n",
      "                      ...   \n",
      "gex_ZYG11A|440590    float32\n",
      "gex_ZYG11B|79699     float32\n",
      "gex_ZYX|7791         float32\n",
      "gex_ZZEF1|23140      float32\n",
      "gex_ZZZ3|26009       float32\n",
      "Length: 20531, dtype: object\n",
      "0.999999\n",
      "integration times 5673\n",
      "Concordance Index 0.4698914116485686\n",
      "Integrated Brier Score: 0.1986350623477569\n",
      "0.999999\n",
      "integration times 7088\n",
      "Concordance Index 0.37095363079615046\n",
      "Integrated Brier Score: 0.2083238065573121\n",
      "0.999999\n",
      "integration times 5362\n",
      "Concordance Index 0.3525252525252525\n",
      "Integrated Brier Score: 0.1807101441555301\n",
      "0.999999\n",
      "integration times 6363\n",
      "Concordance Index 0.635893011216566\n",
      "Integrated Brier Score: 0.2009838223175985\n",
      "0.999999\n",
      "integration times 8846\n",
      "Concordance Index 0.6025862068965517\n",
      "Integrated Brier Score: 0.19441006419166457\n",
      "split gex_?|100130426      float32\n",
      "gex_?|100133144      float32\n",
      "gex_?|100134869      float32\n",
      "gex_?|10357          float32\n",
      "gex_?|10431          float32\n",
      "                      ...   \n",
      "gex_ZYG11A|440590    float32\n",
      "gex_ZYG11B|79699     float32\n",
      "gex_ZYX|7791         float32\n",
      "gex_ZZEF1|23140      float32\n",
      "gex_ZZZ3|26009       float32\n",
      "Length: 20531, dtype: object\n",
      "0.999999\n",
      "integration times 3584\n",
      "Concordance Index 0.3861490031479538\n",
      "Integrated Brier Score: 0.22101745202279857\n",
      "0.999999\n",
      "integration times 3896\n",
      "Concordance Index 0.464354527938343\n",
      "Integrated Brier Score: 0.20169560175260567\n",
      "0.999999\n",
      "integration times 3915\n",
      "Concordance Index 0.41141396933560476\n",
      "Integrated Brier Score: 0.22582499207095152\n",
      "0.999999\n",
      "integration times 4660\n",
      "Concordance Index 0.5219465648854962\n",
      "Integrated Brier Score: 0.20831208441074772\n",
      "0.999999\n",
      "integration times 3304\n",
      "Concordance Index 0.4366312346688471\n",
      "Integrated Brier Score: 0.2273087833261004\n",
      "split gex_?|100130426      float32\n",
      "gex_?|100133144      float32\n",
      "gex_?|100134869      float32\n",
      "gex_?|10357          float32\n",
      "gex_?|10431          float32\n",
      "                      ...   \n",
      "gex_ZYG11A|440590    float32\n",
      "gex_ZYG11B|79699     float32\n",
      "gex_ZYX|7791         float32\n",
      "gex_ZZEF1|23140      float32\n",
      "gex_ZZZ3|26009       float32\n",
      "Length: 20531, dtype: object\n",
      "0.999999\n",
      "integration times 3868\n",
      "Concordance Index 0.5030549898167006\n",
      "Integrated Brier Score: 0.20431054318288777\n",
      "0.999999\n",
      "integration times 5093\n",
      "Concordance Index 0.4307856761090326\n",
      "Integrated Brier Score: 0.21029456247549924\n",
      "0.999999\n",
      "integration times 8232\n",
      "Concordance Index 0.5807692307692308\n",
      "Integrated Brier Score: 0.2033767174461617\n",
      "0.999999\n",
      "integration times 5072\n",
      "Concordance Index 0.4648148148148148\n",
      "Integrated Brier Score: 0.21979541333196742\n",
      "0.999999\n",
      "integration times 7904\n",
      "Concordance Index 0.6135437881873728\n",
      "Integrated Brier Score: 0.18670648948559604\n",
      "split gex_?|100130426      float32\n",
      "gex_?|100133144      float32\n",
      "gex_?|100134869      float32\n",
      "gex_?|10357          float32\n",
      "gex_?|10431          float32\n",
      "                      ...   \n",
      "gex_ZYG11A|440590    float32\n",
      "gex_ZYG11B|79699     float32\n",
      "gex_ZYX|7791         float32\n",
      "gex_ZZEF1|23140      float32\n",
      "gex_ZZZ3|26009       float32\n",
      "Length: 20531, dtype: object\n",
      "0.999999\n",
      "integration times 3586\n",
      "Concordance Index 0.4920906370243694\n",
      "Integrated Brier Score: 0.20677604713087522\n",
      "0.999999\n",
      "integration times 4858\n",
      "Concordance Index 0.3891891891891892\n",
      "Integrated Brier Score: 0.20310349485931592\n",
      "0.999999\n",
      "integration times 5397\n",
      "Concordance Index 0.6020627417275461\n",
      "Integrated Brier Score: 0.20713700361380818\n",
      "0.999999\n",
      "integration times 5303\n",
      "Concordance Index 0.47978567949342427\n",
      "Integrated Brier Score: 0.1953430695446712\n",
      "0.999999\n",
      "integration times 4867\n",
      "Concordance Index 0.506587335316617\n",
      "Integrated Brier Score: 0.21921722329759638\n",
      "split gex_?|100133144      float32\n",
      "gex_?|100134869      float32\n",
      "gex_?|10357          float32\n",
      "gex_?|10431          float32\n",
      "gex_?|155060         float32\n",
      "                      ...   \n",
      "gex_ZYG11A|440590    float32\n",
      "gex_ZYG11B|79699     float32\n",
      "gex_ZYX|7791         float32\n",
      "gex_ZZEF1|23140      float32\n",
      "gex_ZZZ3|26009       float32\n",
      "Length: 19076, dtype: object\n",
      "0.999999\n",
      "integration times 5867\n",
      "Concordance Index 0.5081809432146295\n",
      "Integrated Brier Score: 0.14551688582780914\n",
      "0.999999\n",
      "integration times 7856\n",
      "Concordance Index 0.47648902821316613\n",
      "Integrated Brier Score: 0.1260097303584315\n",
      "1.999999\n",
      "integration times 3547\n",
      "Concordance Index 0.5063168124392614\n",
      "Integrated Brier Score: 0.15193093396653234\n",
      "0.999999\n",
      "integration times 11342\n",
      "Concordance Index 0.4433399602385686\n",
      "Integrated Brier Score: 0.15318337038354088\n",
      "0.999999\n",
      "integration times 11635\n",
      "Concordance Index 0.39419475655430714\n",
      "Integrated Brier Score: 0.16708084607292673\n",
      "split gex_?|100133144      float32\n",
      "gex_?|100134869      float32\n",
      "gex_?|10357          float32\n",
      "gex_?|10431          float32\n",
      "gex_?|155060         float32\n",
      "                      ...   \n",
      "gex_ZYG11A|440590    float32\n",
      "gex_ZYG11B|79699     float32\n",
      "gex_ZYX|7791         float32\n",
      "gex_ZZEF1|23140      float32\n",
      "gex_ZZZ3|26009       float32\n",
      "Length: 19076, dtype: object\n",
      "0.999999\n",
      "integration times 5581\n",
      "Concordance Index 0.5715263518138262\n",
      "Integrated Brier Score: 0.2195586810787003\n",
      "0.999999\n",
      "integration times 4245\n",
      "Concordance Index 0.42920353982300885\n",
      "Integrated Brier Score: 0.21613505774020086\n",
      "0.999999\n",
      "integration times 5180\n",
      "Concordance Index 0.570957095709571\n",
      "Integrated Brier Score: 0.21398359986277163\n",
      "0.999999\n",
      "integration times 3460\n",
      "Concordance Index 0.43870523415977963\n",
      "Integrated Brier Score: 0.2083721066126565\n",
      "0.999999\n",
      "integration times 5001\n",
      "Concordance Index 0.47019867549668876\n",
      "Integrated Brier Score: 0.22696259490702492\n"
     ]
    }
   ],
   "source": [
    "cancer_types = [\n",
    "    'BLCA',\n",
    "    'BRCA',\n",
    "    'HNSC',\n",
    "    'KIRC',\n",
    "    'LGG',\n",
    "    'LIHC',\n",
    "    'LUAD',\n",
    "    'LUSC',\n",
    "    'OV',\n",
    "    'STAD']\n",
    "\n",
    "agg_metrics_cindex = []\n",
    "agg_metrics_ibs = []\n",
    "\n",
    "\n",
    "class InputShapeSetter(skorch.callbacks.Callback):\n",
    "    def on_train_begin(self, net, X, y):\n",
    "        net.set_params(module__input_units=X.shape[-1])\n",
    "\n",
    "for idx, cancer_type in enumerate(cancer_types):\n",
    "    # get name of current dataset\n",
    "    data = load_tcga(path=\"/Users/JUSC/Documents/xgbsurv/xgbsurv/datasets/data/\", cancer_type=cancer_type, as_frame=True)\n",
    "    X  = data.data #.astype(np.float32)\n",
    "    y = data.target #.values #.to_numpy()\n",
    "\n",
    "    X, y = sort_X_y_pandas(X, y)\n",
    "\n",
    "    net = NeuralNet(\n",
    "        SurvivalModel, \n",
    "        module__n_layers = 1,\n",
    "        module__input_units = X.shape[1],\n",
    "        #module__num_nodes = 32,\n",
    "        #module__dropout = 0.1, # these could also be removed\n",
    "        module__out_features = 1,\n",
    "        # for split sizes when result size = 1\n",
    "        iterator_train__drop_last=True,\n",
    "        #iterator_valid__drop_last=True,\n",
    "        criterion=AHLoss,\n",
    "        optimizer=torch.optim.AdamW,\n",
    "        optimizer__weight_decay = 0.4,\n",
    "        batch_size=32, # separate train and valid->iterator_train__batch_size=128 and iterator_valid__batch_size=128 ?\n",
    "        callbacks=[\n",
    "            (\n",
    "                \"sched\",\n",
    "                LRScheduler(\n",
    "                    torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
    "                    monitor=\"valid_loss\",\n",
    "                    patience=5,\n",
    "                ),\n",
    "            ),\n",
    "            (\n",
    "                \"es\",\n",
    "                EarlyStopping(\n",
    "                    monitor=\"valid_loss\",\n",
    "                    patience=early_stopping_rounds,\n",
    "                    load_best=True,\n",
    "                ),\n",
    "            ),\n",
    "            (\"seed\", FixSeed(seed=42)),\n",
    "            (\"Input Shape Setter\",InputShapeSetter())\n",
    "        ],#[EarlyStopping(patience=10)],#,InputShapeSetter()],\n",
    "        #TODO: enable stratification, verify\n",
    "        train_split=ValidSplit(0.2), # might cause lower performance in metrics, explain in thesis\n",
    "        verbose=0\n",
    "    )\n",
    "    df_agg_metrics_cindex, df_agg_metrics_ibs, best_model,params, outer_scores, best_preds_train, best_preds_test = train_eval(X, y, net, n_iter, data.filename)\n",
    "    agg_metrics_cindex.append(df_agg_metrics_cindex)\n",
    "    agg_metrics_ibs.append(df_agg_metrics_ibs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_ah_1_cindex = pd.concat([df for df in agg_metrics_cindex]).round(4)\n",
    "df_final_ah_1_cindex.to_csv('metrics/final_deep_learning_ah_tcga_cindex.csv', index=False)\n",
    "df_final_ah_1_cindex.to_csv('/Users/JUSC/Documents/644928e0fb7e147893e8ec15/05_thesis/tables/final_deep_learning_ah_tcga_cindex.csv', index=False)  #\n",
    "df_final_ah_1_cindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_ah_1_ibs = pd.concat([df for df in agg_metrics_ibs]).round(4)\n",
    "df_final_ah_1_ibs.to_csv('metrics/final_deep_learning_ah_tcga_ibs.csv', index=False)\n",
    "df_final_ah_1_ibs.to_csv('/Users/JUSC/Documents/644928e0fb7e147893e8ec15/05_thesis/tables/final_deep_learning_ah_tcga_ibs.csv', index=False) \n",
    "df_final_ah_1_ibs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xgbsurv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
