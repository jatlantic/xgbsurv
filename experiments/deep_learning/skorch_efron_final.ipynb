{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import savetxt\n",
    "from xgbsurv.datasets import (load_metabric, load_flchain, load_rgbsg, load_support, load_tcga)\n",
    "from xgbsurv.models.utils import sort_X_y_pandas, transform_back, transform\n",
    "from xgbsurv.models.efron_final import get_cumulative_hazard_function_efron, efron_estimator\n",
    "import torch\n",
    "from torch import nn\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold, train_test_split\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, LabelBinarizer, OneHotEncoder\n",
    "from sklearn.compose import make_column_transformer, make_column_selector\n",
    "from sklearn.decomposition import PCA\n",
    "from loss_functions_pytorch import EfronLoss, efron_likelihood_torch\n",
    "from skorch import NeuralNet\n",
    "from skorch.callbacks import EarlyStopping, Callback, LRScheduler\n",
    "from skorch.dataset import ValidSplit\n",
    "from pycox.evaluation import EvalSurv\n",
    "from scipy.stats import uniform as scuniform\n",
    "from scipy.stats import randint as scrandint\n",
    "from scipy.stats import loguniform as scloguniform\n",
    "import random\n",
    "import os\n",
    "#torch.set_default_dtype(torch.float64)\n",
    "#torch.set_default_tensor_type(torch.DoubleTensor)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters, put into function\n",
    "n_outer_splits = 5\n",
    "n_inner_splits = 5\n",
    "rand_state = 42\n",
    "n_iter = 10 # set to 50\n",
    "#n_iter_cind = 200\n",
    "early_stopping_rounds=15\n",
    "base_score = 0.0\n",
    "\n",
    "param_grid_breslow = {\n",
    "    'estimator__module__n_layers': [1, 2, 4],\n",
    "    'estimator__module__n_layers': [1, 2, 4],\n",
    "    'estimator__module__num_nodes': [64, 128, 256, 512],\n",
    "    'estimator__module__dropout': scuniform(0.0,0.7),\n",
    "    'estimator__optimizer__weight_decay': [0.4, 0.2, 0.1, 0.05, 0.02, 0.01, 0],\n",
    "    'estimator__batch_size': [64, 128, 256, 512, 1024],\n",
    "    #lr not in paper because of learning rate finder\n",
    "    # note: setting learning rate higher would make exp(partial_hazard) explode\n",
    "    #'estimator__lr': scloguniform(0.001,0.01), # scheduler unten einbauen\n",
    "    # use callback instead\n",
    "    'estimator__lr':[0.01]\n",
    "    #'max_epochs':  scrandint(10,20), # corresponds to num_rounds\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_torch(seed=42):\n",
    "    \"\"\"Sets all seeds within torch and adjacent libraries.\n",
    "\n",
    "    Args:\n",
    "        seed: Random seed to be used by the seeding functions.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    #os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    #torch.backends.cudnn.benchmark = False\n",
    "    #torch.backends.cudnn.deterministic = True\n",
    "    return None\n",
    "\n",
    "\n",
    "class FixSeed(Callback):\n",
    "    def __init__(self, seed):\n",
    "        self.seed = seed\n",
    "\n",
    "    def initialize(self):\n",
    "        seed_torch(self.seed)\n",
    "        return super().initialize()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Scorer\n",
    "def custom_scoring_function(y_true, y_pred):\n",
    "\n",
    "        #y_true = torch.from_numpy(y_true)\n",
    "        if isinstance(y_pred, np.ndarray):\n",
    "            y_pred = torch.from_numpy(y_pred)\n",
    "        if isinstance(y_true, np.ndarray):\n",
    "            y_true = torch.from_numpy(y_true)\n",
    "        if isinstance(y_pred, pd.Series):\n",
    "            y_pred = torch.tensor(y_pred.values)\n",
    "        if isinstance(y_true, pd.Series):\n",
    "            y_true = torch.tensor(y_true.values)\n",
    "        score = efron_likelihood_torch(y_true, y_pred) #.to(torch.float32)\n",
    "        return score.numpy()\n",
    "\n",
    "scoring_function = make_scorer(custom_scoring_function, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set up Custom Splitter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Torch Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SurvivalModel(nn.Module):\n",
    "    def __init__(self, n_layers, input_units, num_nodes, dropout, out_features):\n",
    "        super(SurvivalModel, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.in_features = input_units\n",
    "        self.num_nodes = num_nodes\n",
    "        self.dropout = dropout\n",
    "        self.out_features = out_features\n",
    "        model = []\n",
    "        # first layer\n",
    "        model.append(torch.nn.Linear(input_units, num_nodes))\n",
    "        model.append(torch.nn.ReLU())\n",
    "        model.append(torch.nn.Dropout(dropout))\n",
    "        model.append(torch.nn.BatchNorm1d(num_nodes))\n",
    "\n",
    "        for i in range(n_layers-1):\n",
    "            model.append(torch.nn.Linear(num_nodes, num_nodes))\n",
    "            #init.kaiming_normal_(model[-1].weight, nonlinearity='relu')\n",
    "            model.append(torch.nn.ReLU())\n",
    "            model.append(torch.nn.Dropout(dropout))\n",
    "            model.append(torch.nn.BatchNorm1d(num_nodes))\n",
    "\n",
    "        # output layer\n",
    "        model.append(torch.nn.Linear(num_nodes, out_features))\n",
    "    \n",
    "        self.layers = nn.Sequential(*model)\n",
    "\n",
    "        # for layer in self.layers:\n",
    "        #     if isinstance(layer, nn.Linear):\n",
    "        #         #nn.init.uniform_(layer.weight, a=-0.5, b=0.5)\n",
    "        #         nn.init.kaiming_normal_(layer.weight)\n",
    "\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = X.to(torch.float32)\n",
    "        res = self.layers(X)\n",
    "        #print(res)\n",
    "        return res\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomStandardScaler(StandardScaler):\n",
    "    \n",
    "    def __init__(self, copy=True, with_mean=True, with_std=True):\n",
    "        super().__init__(copy=copy, with_mean=with_mean, with_std=with_std)\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return super().fit(X, y)\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X_transformed = super().transform(X, y)\n",
    "        return X_transformed.astype(np.float32)\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        X_transformed = super().fit_transform(X, y)\n",
    "        return X_transformed.astype(np.float32)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define stratified inner k-fold cross-validation\n",
    "class CustomSplit(StratifiedKFold):\n",
    "    def __init__(self, n_splits=5, shuffle=True, random_state=None):\n",
    "        super().__init__(n_splits=n_splits, shuffle=shuffle, random_state=random_state)\n",
    "\n",
    "    def split(self, X, y, groups=None):\n",
    "        print('split', X.dtypes)\n",
    "        try:\n",
    "            if y.shape[1]>1:\n",
    "                y = y[:,0]\n",
    "        except:\n",
    "            pass\n",
    "        bins = np.sign(y)\n",
    "        return super().split(X, bins, groups=groups)\n",
    "\n",
    "    def get_n_splits(self, X=None, y=None, groups=None):\n",
    "        return self.n_splits\n",
    "\n",
    "outer_custom_cv = CustomSplit(n_splits=n_outer_splits, shuffle=True, random_state=rand_state)\n",
    "inner_custom_cv = CustomSplit(n_splits=n_outer_splits, shuffle=True, random_state=rand_state)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Training Procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_eval(X, y, net, n_iter, filename):\n",
    "        model = '_efron_'\n",
    "        dataset_name = filename.split('_')[0]\n",
    "        # add IBS later\n",
    "        outer_scores = {'cindex_train_'+dataset_name:[], 'cindex_test_'+dataset_name:[],\n",
    "                        'ibs_train_'+dataset_name:[], 'ibs_test_'+dataset_name:[]}\n",
    "        best_params = {'best_params_'+dataset_name:[]}\n",
    "        best_model = {'best_model_'+dataset_name:[]}\n",
    "        ct = make_column_transformer(\n",
    "                #(OneHotEncoder(sparse_output=False), make_column_selector(dtype_include=['category', 'object']))\n",
    "                (StandardScaler(), make_column_selector(dtype_include=['float32']))\n",
    "                ,remainder='passthrough')\n",
    "\n",
    "        pipe = Pipeline([('scaler',ct),\n",
    "                        ('estimator', net)])\n",
    "        rs = RandomizedSearchCV(pipe, param_grid_breslow, scoring = scoring_function, n_jobs=-1, \n",
    "                                    n_iter=2, refit=True)\n",
    "        for i, (train_index, test_index) in enumerate(outer_custom_cv.split(X, y)):\n",
    "                # Split data into training and testing sets for outer fold\n",
    "                X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "                y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "                X_train, y_train = sort_X_y_pandas(X_train, y_train)\n",
    "                X_test, y_test = sort_X_y_pandas(X_test, y_test)\n",
    "\n",
    "                print(X_train.shape, type(X_train))\n",
    "                print(y_train.shape, type(y_train))\n",
    "                print(X_test.shape, type(X_test))\n",
    "                print(y_test.shape, type(y_test))\n",
    "                # save splits and data\n",
    "                savetxt('splits/train_index_'+str(i)+'_'+filename, train_index, delimiter=',')\n",
    "                savetxt('splits/test_index_'+str(i)+'_'+filename, test_index, delimiter=',')\n",
    "                \n",
    "                savetxt('splits/X_train_'+str(i)+'_'+filename, X_train, delimiter=',')\n",
    "                savetxt('splits/X_test_'+str(i)+'_'+filename, X_test, delimiter=',')\n",
    "\n",
    "                savetxt('splits/y_train_'+str(i)+'_'+filename, y_train, delimiter=',')\n",
    "                savetxt('splits/y_test_'+str(i)+'_'+filename, y_test, delimiter=',')\n",
    "\n",
    "                strat = np.sign(y_train)\n",
    "                valid_split = ValidSplit(cv=0.1, stratified=strat, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                rs.fit(X_train, y_train)\n",
    "                best_preds_train = rs.best_estimator_.predict(X_train)\n",
    "                best_preds_test = rs.best_estimator_.predict(X_test)\n",
    "                # save hyperparameter settings\n",
    "                params = rs.best_estimator_.get_params\n",
    "                best_params['best_params_'+dataset_name] += [rs.best_params_]\n",
    "                best_model['best_model_'+dataset_name] += [params]\n",
    "                try:\n",
    "                    cum_hazard_train = get_cumulative_hazard_function_efron(\n",
    "                            X_train.values, X_train.values, y_train.values, y_train.values,\n",
    "                            best_preds_train.reshape(-1), best_preds_train.reshape(-1)\n",
    "                            )\n",
    "\n",
    "                    df_survival_train = np.exp(-cum_hazard_train)\n",
    "                    durations_train, events_train = transform_back(y_train.values)\n",
    "                    time_grid_train = np.linspace(durations_train.min(), durations_train.max(), 100)\n",
    "                    ev = EvalSurv(df_survival_train, durations_train, events_train, censor_surv='km')\n",
    "                    print('Concordance Index',ev.concordance_td('antolini'))\n",
    "                    print('Integrated Brier Score:',ev.integrated_brier_score(time_grid_train))\n",
    "                    cindex_score_train = ev.concordance_td('antolini')\n",
    "                    ibs_score_train = ev.integrated_brier_score(time_grid_train)\n",
    "\n",
    "                    outer_scores['cindex_train_'+dataset_name] += [cindex_score_train]\n",
    "                    outer_scores['ibs_train_'+dataset_name] += [ibs_score_train]\n",
    "\n",
    "                except:\n",
    "                    outer_scores['cindex_train_'+dataset_name] += [np.nan]\n",
    "                    outer_scores['ibs_train_'+dataset_name] += [np.nan]\n",
    "                    \n",
    "                try:\n",
    "                    cum_hazard_test = get_cumulative_hazard_function_efron(\n",
    "                            X_train.values, X_test.values, y_train.values, y_test.values,\n",
    "                            best_preds_train.reshape(-1), best_preds_test.reshape(-1)\n",
    "                            )\n",
    "                    df_survival_test = np.exp(-cum_hazard_test)\n",
    "                    durations_test, events_test = transform_back(y_test.values)\n",
    "                    print('durations',durations_test.min(), durations_test.max())\n",
    "                    time_grid_test = np.linspace(durations_test.min(), durations_test.max(), 100)\n",
    "                    ev = EvalSurv(df_survival_test, durations_test, events_test, censor_surv='km')\n",
    "                    print('Concordance Index',ev.concordance_td('antolini'))\n",
    "                    print('Integrated Brier Score:',ev.integrated_brier_score(time_grid_test))\n",
    "                    cindex_score_test = ev.concordance_td('antolini')\n",
    "                    ibs_score_test = ev.integrated_brier_score(time_grid_test)\n",
    "\n",
    "                    outer_scores['cindex_test_'+dataset_name] += [cindex_score_test]\n",
    "                    outer_scores['ibs_test_'+dataset_name] += [ibs_score_test]\n",
    "                except: \n",
    "                    outer_scores['cindex_test_'+dataset_name] += [np.nan]\n",
    "                    outer_scores['ibs_test_'+dataset_name] += [np.nan]\n",
    "            \n",
    "        df_best_params = pd.DataFrame(best_params)\n",
    "        df_best_model = pd.DataFrame(best_model)\n",
    "        df_outer_scores = pd.DataFrame(outer_scores)\n",
    "        df_metrics = pd.concat([df_best_params,df_best_model,df_outer_scores], axis=1)\n",
    "        df_metrics.to_csv('metrics/metric_summary'+model+str(i)+'_'+filename, index=False)\n",
    "        return best_model, best_params, outer_scores, best_preds_train, best_preds_test, X_train, X_test, y_train, y_test\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_metabric\n",
      "split MKI67                float32\n",
      "EGFR                 float32\n",
      "PGR                  float32\n",
      "ERBB2                float32\n",
      "hormone_treatment    float32\n",
      "radiotherapy         float32\n",
      "chemotherapy         float32\n",
      "ER_positive          float32\n",
      "age                  float32\n",
      "dtype: object\n",
      "(1522, 9) <class 'pandas.core.frame.DataFrame'>\n",
      "(1522,) <class 'pandas.core.series.Series'>\n",
      "(381, 9) <class 'pandas.core.frame.DataFrame'>\n",
      "(381,) <class 'pandas.core.series.Series'>\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1     \u001b[36m1442.8818\u001b[0m      \u001b[32m559.0554\u001b[0m  0.0404\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1     \u001b[36m1553.1927\u001b[0m      \u001b[32m616.2332\u001b[0m  0.0416\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1     \u001b[36m1716.5940\u001b[0m      \u001b[32m617.3030\u001b[0m  0.0468\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1     \u001b[36m1753.2616\u001b[0m      \u001b[32m628.2709\u001b[0m  0.0523\n",
      "      2     \u001b[36m1406.7965\u001b[0m      \u001b[32m556.3452\u001b[0m  0.0458\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1     \u001b[36m1819.9631\u001b[0m      \u001b[32m658.3480\u001b[0m  0.0590\n",
      "      2     \u001b[36m1512.8889\u001b[0m      \u001b[32m614.3843\u001b[0m  0.0427\n",
      "      2     \u001b[36m1694.4189\u001b[0m      \u001b[32m614.0413\u001b[0m  0.0536\n",
      "      2     \u001b[36m1719.0514\u001b[0m      \u001b[32m625.5859\u001b[0m  0.0397\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m316.6585\u001b[0m      \u001b[32m274.8339\u001b[0m  0.1468\n",
      "      3     \u001b[36m1390.5923\u001b[0m      \u001b[32m555.1536\u001b[0m  0.0499\n",
      "      2     \u001b[36m1794.9298\u001b[0m      \u001b[32m655.8419\u001b[0m  0.0489\n",
      "      3     \u001b[36m1495.9976\u001b[0m      \u001b[32m613.7124\u001b[0m  0.0420\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m339.2095\u001b[0m      \u001b[32m274.2747\u001b[0m  0.1496\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m274.8444\u001b[0m      \u001b[32m251.7478\u001b[0m  0.1635\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m335.0533\u001b[0m      \u001b[32m276.7986\u001b[0m  0.1676\n",
      "      3     \u001b[36m1672.6887\u001b[0m      \u001b[32m612.7909\u001b[0m  0.0454\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m346.8381\u001b[0m      \u001b[32m291.6557\u001b[0m  0.1763\n",
      "      3     \u001b[36m1706.0361\u001b[0m      \u001b[32m624.8619\u001b[0m  0.0573\n",
      "      4     \u001b[36m1476.2400\u001b[0m      \u001b[32m613.5198\u001b[0m  0.0535\n",
      "      4     \u001b[36m1372.2579\u001b[0m      \u001b[32m554.8777\u001b[0m  0.0563\n",
      "      3     \u001b[36m1784.9075\u001b[0m      \u001b[32m655.4622\u001b[0m  0.0595\n",
      "      4     \u001b[36m1670.5601\u001b[0m      \u001b[32m612.7604\u001b[0m  0.0507\n",
      "      4     1709.3409      625.0342  0.0546\n",
      "      5     \u001b[36m1347.6501\u001b[0m      554.9109  0.0421\n",
      "      5     \u001b[36m1468.9384\u001b[0m      \u001b[32m613.3782\u001b[0m  0.0479\n",
      "      4     1791.0281      655.9068  0.0509\n",
      "      5     \u001b[36m1662.7932\u001b[0m      613.0209  0.0430\n",
      "      5     \u001b[36m1696.5591\u001b[0m      625.4380  0.0444\n",
      "      6     1352.1639      555.0190  0.0449\n",
      "      6     1469.5990      \u001b[32m613.1584\u001b[0m  0.0477\n",
      "      5     \u001b[36m1772.3165\u001b[0m      656.5490  0.0484\n",
      "      2      \u001b[36m264.8384\u001b[0m      \u001b[32m244.1173\u001b[0m  0.1538\n",
      "      2      \u001b[36m289.7040\u001b[0m      \u001b[32m260.2180\u001b[0m  0.1789\n",
      "      2      \u001b[36m310.6415\u001b[0m      \u001b[32m265.0955\u001b[0m  0.1544\n",
      "      6     \u001b[36m1660.2269\u001b[0m      613.3097  0.0512\n",
      "      6     1697.1177      625.8748  0.0402\n",
      "      7     1359.3311      555.2384  0.0387\n",
      "      2      \u001b[36m316.2638\u001b[0m      \u001b[32m271.4639\u001b[0m  0.1812\n",
      "      7     \u001b[36m1467.6771\u001b[0m      \u001b[32m612.9557\u001b[0m  0.0410\n",
      "      2      \u001b[36m332.8896\u001b[0m      \u001b[32m287.8110\u001b[0m  0.1664\n",
      "      6     \u001b[36m1769.9009\u001b[0m      657.0701  0.0531\n",
      "      7     \u001b[36m1658.8184\u001b[0m      613.5212  0.0455\n",
      "      7     \u001b[36m1687.7222\u001b[0m      626.2140  0.0423\n",
      "      8     1472.5121      \u001b[32m612.7275\u001b[0m  0.0381\n",
      "      8     1355.3104      555.3663  0.0584\n",
      "      8     \u001b[36m1657.0684\u001b[0m      613.6191  0.0390\n",
      "      7     \u001b[36m1765.8547\u001b[0m      657.4371  0.0451\n",
      "      9     \u001b[36m1465.8119\u001b[0m      \u001b[32m612.4425\u001b[0m  0.0471\n",
      "      9     1360.9066      555.4983  0.0439\n",
      "      8     \u001b[36m1686.9133\u001b[0m      626.4407  0.0616\n",
      "      9     1660.9485      613.6454  0.0398\n",
      "      8     \u001b[36m1765.7812\u001b[0m      657.6341  0.0452\n",
      "      3      \u001b[36m305.4612\u001b[0m      271.1721  0.1501\n",
      "      3      \u001b[36m252.8833\u001b[0m      245.3332  0.1612\n",
      "     10     \u001b[36m1457.5819\u001b[0m      \u001b[32m612.1580\u001b[0m  0.0446\n",
      "     10     1349.8762      555.5671  0.0415\n",
      "Restoring best model from epoch 4.\n",
      "      9     1689.3364      626.5205  0.0437\n",
      "      3      \u001b[36m281.2178\u001b[0m      262.1387  0.1695\n",
      "     10     \u001b[36m1644.0854\u001b[0m      613.6326  0.0463\n",
      "      3      \u001b[36m310.8573\u001b[0m      276.2855  0.1588\n",
      "Restoring best model from epoch 3.\n",
      "      9     \u001b[36m1761.9341\u001b[0m      657.7291  0.0438\n",
      "     10     \u001b[36m1678.4659\u001b[0m      626.5093  0.0410\n",
      "Restoring best model from epoch 3.\n",
      "      3      \u001b[36m325.1698\u001b[0m      \u001b[32m287.3103\u001b[0m  0.1755\n",
      "     10     \u001b[36m1746.5676\u001b[0m      657.7633  0.0402\n",
      "Restoring best model from epoch 3.\n",
      "      4      \u001b[36m247.9657\u001b[0m      \u001b[32m237.5616\u001b[0m  0.1399\n",
      "      4      307.2386      \u001b[32m263.6777\u001b[0m  0.1466\n",
      "      4      \u001b[36m275.2858\u001b[0m      \u001b[32m258.4916\u001b[0m  0.1391\n",
      "      4      311.7911      \u001b[32m270.2500\u001b[0m  0.1403\n",
      "      4      \u001b[36m321.1779\u001b[0m      \u001b[32m284.1105\u001b[0m  0.1429\n",
      "      5      \u001b[36m245.3705\u001b[0m      238.2374  0.1389\n",
      "      5      \u001b[36m296.3748\u001b[0m      \u001b[32m262.0409\u001b[0m  0.1399\n",
      "      5      \u001b[36m271.4774\u001b[0m      259.8250  0.1366\n",
      "      5      \u001b[36m305.4678\u001b[0m      \u001b[32m268.2953\u001b[0m  0.1405\n",
      "      5      \u001b[36m313.8417\u001b[0m      284.8610  0.1425\n",
      "      6      \u001b[36m244.6296\u001b[0m      239.8196  0.1356\n",
      "      6      300.3561      262.1005  0.1389\n",
      "      6      \u001b[36m270.4896\u001b[0m      \u001b[32m256.0859\u001b[0m  0.1364\n",
      "      6      \u001b[36m305.2021\u001b[0m      270.2409  0.1388\n",
      "      6      317.8436      \u001b[32m282.5524\u001b[0m  0.1399\n",
      "      7      245.0984      \u001b[32m236.9390\u001b[0m  0.1348\n",
      "      7      \u001b[36m296.3259\u001b[0m      264.8166  0.1382\n",
      "      7      \u001b[36m269.4987\u001b[0m      257.9689  0.1363\n",
      "      7      \u001b[36m301.3824\u001b[0m      268.6389  0.1370\n",
      "      7      315.3001      \u001b[32m281.7139\u001b[0m  0.1410\n",
      "      8      244.9426      241.0534  0.1368\n",
      "      8      \u001b[36m296.0734\u001b[0m      263.8441  0.1387\n",
      "      8      \u001b[36m269.1566\u001b[0m      257.9019  0.1371\n",
      "      8      \u001b[36m300.7090\u001b[0m      270.2801  0.1394\n",
      "      8      \u001b[36m312.1979\u001b[0m      284.4695  0.1412\n",
      "      9      \u001b[36m243.6476\u001b[0m      238.0230  0.1359\n",
      "      9      269.4519      258.8777  0.1364\n",
      "      9      \u001b[36m293.9577\u001b[0m      264.1044  0.1391\n",
      "      9      301.3605      \u001b[32m267.7381\u001b[0m  0.1398\n",
      "      9      313.0861      284.3635  0.1429\n",
      "     10      \u001b[36m243.4098\u001b[0m      239.3381  0.1367\n",
      "Restoring best model from epoch 7.\n",
      "     10      \u001b[36m267.3536\u001b[0m      257.7688  0.1379\n",
      "Restoring best model from epoch 6.\n",
      "     10      294.1292      263.6686  0.1414\n",
      "Restoring best model from epoch 5.\n",
      "     10      300.7660      268.6033  0.1394\n",
      "Restoring best model from epoch 9.\n",
      "     10      313.6018      284.0943  0.1396\n",
      "Restoring best model from epoch 7.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1     \u001b[36m1600.1346\u001b[0m      \u001b[32m890.8427\u001b[0m  0.0949\n",
      "      2     \u001b[36m1562.2360\u001b[0m      \u001b[32m887.7031\u001b[0m  0.0747\n",
      "      3     \u001b[36m1561.1127\u001b[0m      \u001b[32m886.7946\u001b[0m  0.0743\n",
      "      4     \u001b[36m1541.7158\u001b[0m      887.3998  0.0743\n",
      "      5     1549.1658      888.2322  0.0721\n",
      "      6     \u001b[36m1535.6586\u001b[0m      889.0702  0.0736\n",
      "      7     \u001b[36m1535.3071\u001b[0m      889.2795  0.0745\n",
      "      8     \u001b[36m1531.2006\u001b[0m      889.2634  0.0727\n",
      "      9     1531.5829      888.6517  0.0748\n",
      "     10     \u001b[36m1530.0847\u001b[0m      888.3943  0.0762\n",
      "Restoring best model from epoch 3.\n",
      "Concordance Index 0.6021569719940935\n",
      "Integrated Brier Score: 0.16748840589238245\n",
      "durations 0.76666665 337.03333\n",
      "Concordance Index 0.5634300226181501\n",
      "Integrated Brier Score: 0.1809534534347\n",
      "(1522, 9) <class 'pandas.core.frame.DataFrame'>\n",
      "(1522,) <class 'pandas.core.series.Series'>\n",
      "(381, 9) <class 'pandas.core.frame.DataFrame'>\n",
      "(381,) <class 'pandas.core.series.Series'>\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m516.4738\u001b[0m  0.0066\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m606.7001\u001b[0m  0.0069\n",
      "      2      516.4738  0.0089\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m712.1039\u001b[0m  0.0072\n",
      "      2      606.7001  0.0069\n",
      "      2      712.1039  0.0071\n",
      "      3      606.7001  0.0077\n",
      "      3      516.4738  0.0128\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m743.8466\u001b[0m  0.0116\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m776.0490\u001b[0m  0.0082\n",
      "      3      712.1039  0.0083\n",
      "      4      606.7001  0.0077\n",
      "      4      516.4738  0.0065\n",
      "      4      712.1039  0.0071\n",
      "      2      776.0490  0.0082\n",
      "      5      606.7001  0.0068\n",
      "      5      712.1039  0.0069\n",
      "      3      776.0490  0.0074\n",
      "      5      516.4738  0.0126\n",
      "      6      606.7001  0.0108\n",
      "      2      743.8466  0.0210\n",
      "      6      712.1039  0.0074\n",
      "      4      776.0490  0.0073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/pycox/evaluation/eval_surv.py:36: FutureWarning: is_monotonic is deprecated and will be removed in a future version. Use is_monotonic_increasing instead.\n",
      "  assert pd.Series(self.index_surv).is_monotonic\n",
      "/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/pycox/evaluation/eval_surv.py:36: FutureWarning: is_monotonic is deprecated and will be removed in a future version. Use is_monotonic_increasing instead.\n",
      "  assert pd.Series(self.index_surv).is_monotonic\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      6      516.4738  0.0116\n",
      "      3      743.8466  0.0075\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m271.3890\u001b[0m      \u001b[32m222.2282\u001b[0m  0.0933\n",
      "      7      606.7001  0.0103\n",
      "      7      712.1039  0.0081\n",
      "      5      776.0490  0.0103\n",
      "      4      743.8466  0.0073\n",
      "      8      712.1039  0.0071\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m287.2089\u001b[0m      \u001b[32m263.8742\u001b[0m  0.0989\n",
      "      7      516.4738  0.0127\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m304.2133\u001b[0m      \u001b[32m322.8533\u001b[0m  0.0931\n",
      "      8      606.7001  0.0123\n",
      "      6      776.0490  0.0105\n",
      "      5      743.8466  0.0093\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m299.3781\u001b[0m      \u001b[32m306.7532\u001b[0m  0.1011\n",
      "      9      712.1039  0.0093\n",
      "      8      516.4738  0.0079\n",
      "      9      606.7001  0.0080\n",
      "      6      743.8466  0.0075\n",
      "      7      776.0490  0.0086\n",
      "      9      516.4738  0.0064\n",
      "     10      712.1039  0.0085\n",
      "Restoring best model from epoch 1.\n",
      "      7      743.8466  0.0069\n",
      "      8      776.0490  0.0071\n",
      "     10      516.4738  0.0063\n",
      "Restoring best model from epoch 1.\n",
      "     10      606.7001  0.0129\n",
      "Restoring best model from epoch 1.\n",
      "      8      743.8466  0.0068\n",
      "      9      776.0490  0.0070\n",
      "      9      743.8466  0.0068\n",
      "     10      776.0490  0.0070\n",
      "Restoring best model from epoch 1.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m315.0867\u001b[0m      \u001b[32m335.0247\u001b[0m  0.1109\n",
      "     10      743.8466  0.0070\n",
      "Restoring best model from epoch 1.\n",
      "      2      \u001b[36m263.0862\u001b[0m      \u001b[32m219.3842\u001b[0m  0.0891\n",
      "      2      \u001b[36m275.4454\u001b[0m      \u001b[32m262.1144\u001b[0m  0.0961\n",
      "      2      \u001b[36m296.8143\u001b[0m      324.5205  0.0950\n",
      "      2      \u001b[36m291.3771\u001b[0m      308.6572  0.0918\n",
      "      2      \u001b[36m310.9161\u001b[0m      335.5975  0.0919\n",
      "      3      \u001b[36m259.2103\u001b[0m      \u001b[32m219.3132\u001b[0m  0.0849\n",
      "      3      \u001b[36m272.3544\u001b[0m      \u001b[32m258.3741\u001b[0m  0.0881      3      \u001b[36m290.9961\u001b[0m      324.9675  0.0874\n",
      "\n",
      "      3      \u001b[36m283.6545\u001b[0m      \u001b[32m303.8018\u001b[0m  0.0875\n",
      "      3      \u001b[36m303.8452\u001b[0m      341.1749  0.0895\n",
      "      4      \u001b[36m258.6379\u001b[0m      219.4674  0.0844\n",
      "      4      \u001b[36m271.7882\u001b[0m      \u001b[32m258.1568\u001b[0m  0.0864\n",
      "      4      \u001b[36m289.2137\u001b[0m      325.9565  0.0875\n",
      "      4      \u001b[36m282.8243\u001b[0m      305.6209  0.0863\n",
      "      4      \u001b[36m303.8408\u001b[0m      338.6898  0.0874\n",
      "      5      \u001b[36m257.3287\u001b[0m      \u001b[32m218.8339\u001b[0m  0.0835\n",
      "      5      283.4377      307.0168  0.0861\n",
      "      5      \u001b[36m270.8068\u001b[0m      258.2273  0.0870\n",
      "      5      \u001b[36m288.9239\u001b[0m      326.6053  0.0872\n",
      "      5      \u001b[36m301.7740\u001b[0m      342.7824  0.0876\n",
      "      6      \u001b[36m256.6042\u001b[0m      220.1196  0.0844\n",
      "      6      \u001b[36m280.7677\u001b[0m      305.6596  0.0870\n",
      "      6      \u001b[36m269.4617\u001b[0m      258.7180  0.0875\n",
      "      6      \u001b[36m287.0684\u001b[0m      327.6826  0.0879\n",
      "      6      \u001b[36m300.8597\u001b[0m      339.5957  0.0890\n",
      "      7      \u001b[36m255.2858\u001b[0m      221.1308  0.0846\n",
      "      7      282.5794      \u001b[32m303.3840\u001b[0m  0.0900      7      \u001b[36m269.3130\u001b[0m      \u001b[32m257.6867\u001b[0m  0.0893\n",
      "\n",
      "      7      289.1187      327.4900  0.0893\n",
      "      7      \u001b[36m300.6488\u001b[0m      340.6865  0.0898\n",
      "      8      \u001b[36m254.7192\u001b[0m      222.6955  0.0854\n",
      "      8      \u001b[36m280.6087\u001b[0m      303.9591  0.0866\n",
      "      8      \u001b[36m269.1316\u001b[0m      \u001b[32m256.4945\u001b[0m  0.0864\n",
      "      8      \u001b[36m286.0616\u001b[0m      326.7058  0.0871\n",
      "      8      \u001b[36m299.2577\u001b[0m      340.1741  0.0883\n",
      "      9      255.1408      220.0209  0.0841\n",
      "      9      \u001b[36m279.1663\u001b[0m      304.1072  0.0871\n",
      "      9      269.6415      256.7565  0.0868\n",
      "      9      287.5096      325.9587  0.0880\n",
      "      9      299.9278      339.6192  0.0882\n",
      "     10      254.7564      220.1694  0.0833\n",
      "Restoring best model from epoch 5.\n",
      "     10      \u001b[36m278.8986\u001b[0m      305.3215  0.0861     10      \u001b[36m268.3677\u001b[0m      257.9509  0.0859\n",
      "\n",
      "Restoring best model from epoch 7.\n",
      "Restoring best model from epoch 8.\n",
      "     10      287.0168      325.9521  0.0865\n",
      "Restoring best model from epoch 1.\n",
      "     10      300.5492      339.3667  0.0864\n",
      "Restoring best model from epoch 1.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1     \u001b[36m3618.6743\u001b[0m      \u001b[32m877.0316\u001b[0m  0.0755\n",
      "      2     4781.0483      \u001b[32m856.3856\u001b[0m  0.0796\n",
      "      3     3686.4932      \u001b[32m845.1796\u001b[0m  0.0925\n",
      "      4     \u001b[36m3536.5364\u001b[0m      \u001b[32m840.9476\u001b[0m  0.0766\n",
      "      5     \u001b[36m3517.2695\u001b[0m      \u001b[32m840.3555\u001b[0m  0.0777\n",
      "      6     \u001b[36m3499.0391\u001b[0m      841.8451  0.0766\n",
      "      7     \u001b[36m3495.6306\u001b[0m      844.3489  0.0753\n",
      "      8     \u001b[36m3486.2712\u001b[0m      846.3069  0.0764\n",
      "      9     \u001b[36m3485.6289\u001b[0m      846.8459  0.0744\n",
      "     10     \u001b[36m3480.5459\u001b[0m      846.1390  0.0754\n",
      "Restoring best model from epoch 5.\n",
      "Concordance Index 0.5911105437132104\n",
      "Integrated Brier Score: 0.1698625954672464\n",
      "durations 0.1 330.36667\n",
      "Concordance Index 0.5563924560954985\n",
      "Integrated Brier Score: 0.17991014044878306\n",
      "(1522, 9) <class 'pandas.core.frame.DataFrame'>\n",
      "(1522,) <class 'pandas.core.series.Series'>\n",
      "(381, 9) <class 'pandas.core.frame.DataFrame'>\n",
      "(381,) <class 'pandas.core.series.Series'>\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m515.5874\u001b[0m  0.0067\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m593.6058\u001b[0m  0.0068\n",
      "      2      515.5874  0.0065\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m703.7762\u001b[0m  0.0094\n",
      "      3      515.5874  0.0067\n",
      "      2      593.6058  0.0087\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m713.9573\u001b[0m  0.0094\n",
      "      4      515.5874  0.0065\n",
      "      3      593.6058  0.0075\n",
      "      2      703.7762  0.0100\n",
      "      5      515.5874  0.0068\n",
      "      2      713.9573  0.0076\n",
      "      4      593.6058  0.0085\n",
      "      3      703.7762  0.0081\n",
      "      6      515.5874  0.0079\n",
      "      3      713.9573  0.0094\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m807.8309\u001b[0m  0.0142\n",
      "      4      703.7762  0.0069\n",
      "      5      593.6058  0.0095\n",
      "      7      515.5874  0.0070\n",
      "      5      703.7762  0.0079\n",
      "      2      807.8309  0.0091\n",
      "      6      593.6058  0.0069\n",
      "      4      713.9573  0.0111\n",
      "      8      515.5874  0.0066\n",
      "      7      593.6058  0.0065\n",
      "      6      703.7762  0.0069\n",
      "      3      807.8309  0.0077\n",
      "      5      713.9573  0.0074\n",
      "      9      515.5874  0.0064\n",
      "      8      593.6058  0.0064\n",
      "      4      807.8309  0.0074\n",
      "     10      515.5874  0.0064\n",
      "      7      703.7762  0.0087\n",
      "Restoring best model from epoch 1.\n",
      "      6      713.9573  0.0075\n",
      "      9      593.6058  0.0093\n",
      "      5      807.8309  0.0084\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m280.4401\u001b[0m      \u001b[32m256.7744\u001b[0m  0.0995\n",
      "      7      713.9573  0.0093\n",
      "      8      703.7762  0.0123\n",
      "     10      593.6058  0.0073\n",
      "Restoring best model from epoch 1.\n",
      "      6      807.8309  0.0078\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m261.2100\u001b[0m      \u001b[32m223.2269\u001b[0m  0.1101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/pycox/evaluation/eval_surv.py:36: FutureWarning: is_monotonic is deprecated and will be removed in a future version. Use is_monotonic_increasing instead.\n",
      "  assert pd.Series(self.index_surv).is_monotonic\n",
      "/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/pycox/evaluation/eval_surv.py:36: FutureWarning: is_monotonic is deprecated and will be removed in a future version. Use is_monotonic_increasing instead.\n",
      "  assert pd.Series(self.index_surv).is_monotonic\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      8      713.9573  0.0077\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m305.2678\u001b[0m      \u001b[32m303.0040\u001b[0m  0.1053\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m311.5889\u001b[0m      \u001b[32m303.7360\u001b[0m  0.0990\n",
      "      9      703.7762  0.0104\n",
      "      7      807.8309  0.0093\n",
      "      9      713.9573  0.0088\n",
      "     10      703.7762  0.0084\n",
      "Restoring best model from epoch 1.\n",
      "      8      807.8309  0.0087\n",
      "     10      713.9573  0.0073\n",
      "Restoring best model from epoch 1.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m317.1685\u001b[0m      \u001b[32m343.5718\u001b[0m  0.1124\n",
      "      9      807.8309  0.0090\n",
      "     10      807.8309  0.0088\n",
      "Restoring best model from epoch 1.\n",
      "      2      \u001b[36m272.9078\u001b[0m      \u001b[32m256.4216\u001b[0m  0.0984\n",
      "      2      \u001b[36m254.6189\u001b[0m      \u001b[32m220.8305\u001b[0m  0.0943\n",
      "      2      \u001b[36m300.8359\u001b[0m      304.2607  0.0925\n",
      "      2      \u001b[36m292.4870\u001b[0m      \u001b[32m302.1260\u001b[0m  0.0984\n",
      "      2      \u001b[36m312.4268\u001b[0m      345.7338  0.0963\n",
      "      3      \u001b[36m267.8986\u001b[0m      \u001b[32m254.4744\u001b[0m  0.0845\n",
      "      3      \u001b[36m251.1430\u001b[0m      223.1912  0.0858\n",
      "      3      \u001b[36m296.2721\u001b[0m      \u001b[32m303.0405\u001b[0m  0.0880\n",
      "      3      \u001b[36m289.8317\u001b[0m      \u001b[32m300.5448\u001b[0m  0.0882\n",
      "      3      \u001b[36m305.2391\u001b[0m      344.2910  0.0903\n",
      "      4      \u001b[36m265.6913\u001b[0m      255.9532  0.0835\n",
      "      4      \u001b[36m249.2068\u001b[0m      222.8372  0.0831\n",
      "      4      \u001b[36m294.0397\u001b[0m      303.3909  0.0866\n",
      "      4      \u001b[36m287.5797\u001b[0m      \u001b[32m299.7319\u001b[0m  0.0883\n",
      "      4      \u001b[36m304.5826\u001b[0m      345.5440  0.0883\n",
      "      5      \u001b[36m263.6173\u001b[0m      256.7632  0.0840\n",
      "      5      \u001b[36m245.8985\u001b[0m      223.7335  0.0829\n",
      "      5      294.3531      \u001b[32m302.4054\u001b[0m  0.0871\n",
      "      5      \u001b[36m286.0199\u001b[0m      300.9975  0.0870\n",
      "      5      \u001b[36m303.3663\u001b[0m      349.7095  0.0871\n",
      "      6      246.1602      223.6915  0.0826\n",
      "      6      \u001b[36m263.4334\u001b[0m      255.6377  0.0845\n",
      "      6      \u001b[36m293.1296\u001b[0m      302.6516  0.0871\n",
      "      6      287.4552      300.7901  0.0886\n",
      "      6      \u001b[36m302.8761\u001b[0m      348.1544  0.0883\n",
      "      7      \u001b[36m245.6194\u001b[0m      223.5614  0.0838\n",
      "      7      264.7504      257.0091  0.0857\n",
      "      7      \u001b[36m292.3081\u001b[0m      303.5854  0.0877\n",
      "      7      \u001b[36m284.3324\u001b[0m      302.6937  0.0900\n",
      "      7      \u001b[36m300.2537\u001b[0m      348.4329  0.0899\n",
      "      8      245.9470      223.9140  0.0878\n",
      "      8      264.4937      255.5603  0.0887\n",
      "      8      \u001b[36m291.3385\u001b[0m      302.8268  0.0907\n",
      "      8      284.8445      302.2053  0.0922\n",
      "      8      301.6508      347.7419  0.0971\n",
      "      9      \u001b[36m245.3788\u001b[0m      223.8455  0.0882\n",
      "      9      263.4341      255.8058  0.0866\n",
      "      9      293.2409      302.5487  0.0883\n",
      "      9      285.3002      301.6133  0.0897\n",
      "      9      300.5877      347.2569  0.0895\n",
      "     10      \u001b[36m245.1514\u001b[0m      223.7030  0.0822\n",
      "Restoring best model from epoch 2.\n",
      "     10      \u001b[36m262.3365\u001b[0m      255.6365  0.0855\n",
      "Restoring best model from epoch 3.\n",
      "     10      \u001b[36m290.6876\u001b[0m      303.4097  0.0873\n",
      "Restoring best model from epoch 5.\n",
      "     10      285.2599      303.2760  0.0871\n",
      "Restoring best model from epoch 4.\n",
      "     10      300.5642      347.1027  0.0886\n",
      "Restoring best model from epoch 1.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m300.1740\u001b[0m      \u001b[32m252.9757\u001b[0m  0.1140\n",
      "      2      \u001b[36m289.0806\u001b[0m      \u001b[32m252.9022\u001b[0m  0.1232\n",
      "      3      \u001b[36m288.2983\u001b[0m      253.1125  0.1154\n",
      "      4      \u001b[36m286.1555\u001b[0m      \u001b[32m248.5172\u001b[0m  0.1259\n",
      "      5      \u001b[36m284.9051\u001b[0m      250.2358  0.1243\n",
      "      6      285.1802      249.7111  0.1233\n",
      "      7      \u001b[36m283.1701\u001b[0m      250.4539  0.1270\n",
      "      8      \u001b[36m282.9256\u001b[0m      250.7716  0.1112\n",
      "      9      \u001b[36m282.1566\u001b[0m      249.5947  0.1206\n",
      "     10      \u001b[36m281.3166\u001b[0m      250.9525  0.1225\n",
      "Restoring best model from epoch 4.\n",
      "Concordance Index 0.6259016450805045\n",
      "Integrated Brier Score: 0.1640111183693069\n",
      "durations 1.4333333 301.23334\n",
      "Concordance Index 0.6115020914695796\n",
      "Integrated Brier Score: 0.17926259795252258\n",
      "(1523, 9) <class 'pandas.core.frame.DataFrame'>\n",
      "(1523,) <class 'pandas.core.series.Series'>\n",
      "(380, 9) <class 'pandas.core.frame.DataFrame'>\n",
      "(380,) <class 'pandas.core.series.Series'>\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m558.2759\u001b[0m  0.0083\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m606.8842\u001b[0m  0.0084\n",
      "      2      558.2759  0.0105\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m632.6849\u001b[0m  0.0086\n",
      "      2      606.8842  0.0087\n",
      "      3      558.2759  0.0076\n",
      "      2      632.6849  0.0082\n",
      "      3      606.8842  0.0077\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m674.6183\u001b[0m  0.0077\n",
      "      4      558.2759  0.0068\n",
      "      3      632.6849  0.0099\n",
      "      5      558.2759  0.0067\n",
      "      4      606.8842  0.0094\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m718.4949\u001b[0m  0.0095\n",
      "      6      558.2759  0.0075\n",
      "      5      606.8842  0.0074\n",
      "      2      674.6183  0.0152\n",
      "      4      632.6849  0.0107\n",
      "      2      718.4949  0.0077\n",
      "      3      674.6183  0.0074\n",
      "      7      558.2759  0.0101\n",
      "      6      606.8842  0.0108\n",
      "      5      632.6849  0.0105\n",
      "      4      674.6183  0.0076\n",
      "      3      718.4949  0.0092\n",
      "      7      606.8842  0.0081\n",
      "      8      558.2759  0.0085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/pycox/evaluation/eval_surv.py:36: FutureWarning: is_monotonic is deprecated and will be removed in a future version. Use is_monotonic_increasing instead.\n",
      "  assert pd.Series(self.index_surv).is_monotonic\n",
      "/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/pycox/evaluation/eval_surv.py:36: FutureWarning: is_monotonic is deprecated and will be removed in a future version. Use is_monotonic_increasing instead.\n",
      "  assert pd.Series(self.index_surv).is_monotonic\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      6      632.6849  0.0110\n",
      "      5      674.6183  0.0095\n",
      "      4      718.4949  0.0099\n",
      "      8      606.8842  0.0079\n",
      "      5      718.4949  0.0072\n",
      "      7      632.6849  0.0083\n",
      "      9      558.2759  0.0170\n",
      "      6      674.6183  0.0134\n",
      "      6      718.4949  0.0069\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m252.1219\u001b[0m      \u001b[32m233.0888\u001b[0m  0.1120\n",
      "      8      632.6849  0.0073\n",
      "      9      606.8842  0.0181\n",
      "      7      674.6183  0.0077\n",
      "     10      558.2759  0.0167\n",
      "Restoring best model from epoch 1.\n",
      "      7      718.4949  0.0167\n",
      "     10      606.8842  0.0134\n",
      "Restoring best model from epoch 1.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m277.5827\u001b[0m      \u001b[32m255.4323\u001b[0m  0.1331\n",
      "      8      674.6183  0.0196\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m311.5094\u001b[0m      \u001b[32m289.0778\u001b[0m  0.1335\n",
      "      9      632.6849  0.0337\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m306.2003\u001b[0m      \u001b[32m266.1632\u001b[0m  0.1428\n",
      "      8      718.4949  0.0213\n",
      "      9      674.6183  0.0143\n",
      "     10      632.6849  0.0113\n",
      "Restoring best model from epoch 1.\n",
      "      9      718.4949  0.0090\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m334.1359\u001b[0m      \u001b[32m309.9458\u001b[0m  0.1428\n",
      "     10      674.6183  0.0104\n",
      "Restoring best model from epoch 1.\n",
      "     10      718.4949  0.0081\n",
      "Restoring best model from epoch 1.\n",
      "      2      \u001b[36m243.7153\u001b[0m      234.3644  0.1143\n",
      "      2      \u001b[36m271.3310\u001b[0m      257.3586  0.1017\n",
      "      2      \u001b[36m307.8989\u001b[0m      289.2458  0.1019\n",
      "      2      \u001b[36m291.3367\u001b[0m      268.0016  0.0988\n",
      "      2      \u001b[36m329.1324\u001b[0m      311.0765  0.0977\n",
      "      3      \u001b[36m241.2567\u001b[0m      235.0952  0.0900\n",
      "      3      \u001b[36m267.3850\u001b[0m      256.1509  0.0922\n",
      "      3      \u001b[36m288.0674\u001b[0m      270.0806  0.0918\n",
      "      3      \u001b[36m302.4639\u001b[0m      291.3194  0.0948\n",
      "      3      \u001b[36m324.2659\u001b[0m      313.8811  0.0966\n",
      "      4      \u001b[36m239.0263\u001b[0m      233.1301  0.0906\n",
      "      4      \u001b[36m263.0033\u001b[0m      256.5204  0.0930\n",
      "      4      \u001b[36m284.7730\u001b[0m      269.0508  0.0952\n",
      "      4      \u001b[36m298.2719\u001b[0m      292.4145  0.0964\n",
      "      4      \u001b[36m319.0952\u001b[0m      314.1244  0.0940\n",
      "      5      \u001b[36m238.0419\u001b[0m      234.1524  0.0900\n",
      "      5      263.1954      255.8220  0.0925\n",
      "      5      285.0965      271.9631  0.0925\n",
      "      5      298.9083      290.0422  0.0941\n",
      "      5      321.3697      312.4031  0.0945\n",
      "      6      \u001b[36m237.2951\u001b[0m      233.6343  0.0897\n",
      "      6      \u001b[36m262.9939\u001b[0m      256.7891  0.0906\n",
      "      6      \u001b[36m283.9747\u001b[0m      267.9853  0.0930\n",
      "      6      \u001b[36m296.6493\u001b[0m      290.1497  0.0949\n",
      "      6      319.9791      310.0401  0.0957\n",
      "      7      \u001b[36m236.7981\u001b[0m      233.7366  0.0924\n",
      "      7      \u001b[36m262.4384\u001b[0m      256.9657  0.0947\n",
      "      7      \u001b[36m283.9055\u001b[0m      268.3173  0.0959\n",
      "      7      \u001b[36m295.4941\u001b[0m      294.9704  0.1020\n",
      "      7      \u001b[36m317.8120\u001b[0m      315.2071  0.1032\n",
      "      8      237.6775      233.7273  0.0972\n",
      "      8      \u001b[36m260.4021\u001b[0m      256.7794  0.1026\n",
      "      8      \u001b[36m281.1920\u001b[0m      267.8349  0.1007\n",
      "      8      \u001b[36m295.0525\u001b[0m      292.8822  0.0991\n",
      "      8      \u001b[36m317.6101\u001b[0m      313.7002  0.0987\n",
      "      9      \u001b[36m235.3884\u001b[0m      233.5403  0.0923\n",
      "      9      260.4855      256.6715  0.0902\n",
      "      9      \u001b[36m279.3325\u001b[0m      267.7807  0.0928\n",
      "      9      295.1848      290.8726  0.0948\n",
      "      9      \u001b[36m317.1297\u001b[0m      312.4713  0.0955\n",
      "     10      236.1197      233.3717  0.0924\n",
      "Restoring best model from epoch 1.\n",
      "     10      260.4662      256.8053  0.0951\n",
      "Restoring best model from epoch 1.\n",
      "     10      281.5508      267.7296  0.0943\n",
      "Restoring best model from epoch 1.\n",
      "     10      296.2084      290.1966  0.0938\n",
      "Restoring best model from epoch 1.\n",
      "     10      319.0716      311.8202  0.0948\n",
      "Restoring best model from epoch 1.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m293.0289\u001b[0m      \u001b[32m266.6117\u001b[0m  0.1235\n",
      "      2      \u001b[36m286.6802\u001b[0m      269.2489  0.1293\n",
      "      3      \u001b[36m282.3272\u001b[0m      268.9695  0.1137\n",
      "      4      \u001b[36m281.3950\u001b[0m      271.6729  0.1137\n",
      "      5      \u001b[36m279.4082\u001b[0m      271.9592  0.1098\n",
      "      6      279.6674      270.0027  0.1100\n",
      "      7      \u001b[36m278.8928\u001b[0m      269.8448  0.1106\n",
      "      8      \u001b[36m277.4293\u001b[0m      270.0946  0.1138\n",
      "      9      278.2804      270.2849  0.1106\n",
      "     10      \u001b[36m276.1311\u001b[0m      270.4547  0.1104\n",
      "Restoring best model from epoch 1.\n",
      "Concordance Index 0.5217959949317971\n",
      "Integrated Brier Score: 0.18317809942033889\n",
      "durations 1.2333333 355.2\n",
      "Concordance Index 0.5096591963383709\n",
      "Integrated Brier Score: 0.1740620574254905\n",
      "(1523, 9) <class 'pandas.core.frame.DataFrame'>\n",
      "(1523,) <class 'pandas.core.series.Series'>\n",
      "(380, 9) <class 'pandas.core.frame.DataFrame'>\n",
      "(380,) <class 'pandas.core.series.Series'>\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m568.2552\u001b[0m  0.0080\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m629.9693\u001b[0m  0.0070\n",
      "      2      568.2552  0.0069\n",
      "      2      629.9693  0.0069\n",
      "      3      568.2552  0.0066\n",
      "      3      629.9693  0.0067\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m706.8821\u001b[0m  0.0070\n",
      "      4      568.2552  0.0071\n",
      "      4      629.9693  0.0067\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m676.3395\u001b[0m  0.0097\n",
      "      5      568.2552  0.0067\n",
      "      2      706.8821  0.0101\n",
      "      5      629.9693  0.0068\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m712.3888\u001b[0m  0.0117\n",
      "      6      568.2552  0.0065\n",
      "      3      706.8821  0.0069\n",
      "      2      676.3395  0.0094\n",
      "      6      629.9693  0.0120\n",
      "      3      676.3395  0.0073\n",
      "      2      712.3888  0.0091\n",
      "      4      706.8821  0.0101\n",
      "      7      568.2552  0.0139\n",
      "      3      712.3888  0.0074\n",
      "      4      676.3395  0.0092\n",
      "      7      629.9693  0.0104\n",
      "      5      706.8821  0.0100\n",
      "      8      568.2552  0.0090\n",
      "      8      629.9693  0.0069\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m255.4019\u001b[0m      \u001b[32m246.9402\u001b[0m  0.0951\n",
      "      4      712.3888  0.0105\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m274.1907\u001b[0m      \u001b[32m276.4004\u001b[0m  0.0923\n",
      "      6      706.8821  0.0071\n",
      "      5      676.3395  0.0102\n",
      "      9      568.2552  0.0080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/pycox/evaluation/eval_surv.py:36: FutureWarning: is_monotonic is deprecated and will be removed in a future version. Use is_monotonic_increasing instead.\n",
      "  assert pd.Series(self.index_surv).is_monotonic\n",
      "/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/pycox/evaluation/eval_surv.py:36: FutureWarning: is_monotonic is deprecated and will be removed in a future version. Use is_monotonic_increasing instead.\n",
      "  assert pd.Series(self.index_surv).is_monotonic\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      7      706.8821  0.0071\n",
      "      5      712.3888  0.0084\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m296.8741\u001b[0m      \u001b[32m306.2928\u001b[0m  0.0948\n",
      "      9      629.9693  0.0109\n",
      "      6      676.3395  0.0088\n",
      "     10      568.2552  0.0085\n",
      "Restoring best model from epoch 1.\n",
      "      6      712.3888  0.0073\n",
      "     10      629.9693  0.0069\n",
      "Restoring best model from epoch 1.\n",
      "      7      676.3395  0.0071\n",
      "      8      706.8821  0.0084\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m324.7582\u001b[0m      \u001b[32m305.5092\u001b[0m  0.0942\n",
      "      9      706.8821  0.0071\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m319.4371\u001b[0m      \u001b[32m288.9980\u001b[0m  0.1064\n",
      "      7      712.3888  0.0108\n",
      "      8      676.3395  0.0121\n",
      "      8      712.3888  0.0072\n",
      "     10      706.8821  0.0095\n",
      "Restoring best model from epoch 1.\n",
      "      9      676.3395  0.0072\n",
      "      9      712.3888  0.0070\n",
      "     10      676.3395  0.0074\n",
      "Restoring best model from epoch 1.\n",
      "     10      712.3888  0.0073\n",
      "Restoring best model from epoch 1.\n",
      "      2      \u001b[36m249.8796\u001b[0m      \u001b[32m243.0415\u001b[0m  0.0894\n",
      "      2      \u001b[36m268.7220\u001b[0m      \u001b[32m272.2913\u001b[0m  0.0923\n",
      "      2      \u001b[36m296.2435\u001b[0m      \u001b[32m306.2186\u001b[0m  0.0958\n",
      "      2      \u001b[36m314.9468\u001b[0m      \u001b[32m305.2465\u001b[0m  0.0929\n",
      "      2      \u001b[36m307.8162\u001b[0m      \u001b[32m286.8502\u001b[0m  0.0888\n",
      "      3      \u001b[36m244.6545\u001b[0m      \u001b[32m241.9221\u001b[0m  0.0846\n",
      "      3      \u001b[36m265.1726\u001b[0m      273.3434  0.0865\n",
      "      3      \u001b[36m290.4847\u001b[0m      307.0909  0.0883\n",
      "      3      \u001b[36m312.7317\u001b[0m      \u001b[32m305.0731\u001b[0m  0.0910\n",
      "      3      \u001b[36m307.3728\u001b[0m      \u001b[32m286.5246\u001b[0m  0.0891\n",
      "      4      \u001b[36m243.8360\u001b[0m      242.0371  0.0836\n",
      "      4      \u001b[36m265.0401\u001b[0m      \u001b[32m271.9641\u001b[0m  0.0862\n",
      "      4      \u001b[36m287.7528\u001b[0m      \u001b[32m305.6791\u001b[0m  0.0875\n",
      "      4      \u001b[36m305.5276\u001b[0m      287.2723  0.0886\n",
      "      4      \u001b[36m309.8304\u001b[0m      \u001b[32m304.1982\u001b[0m  0.0896\n",
      "      5      \u001b[36m242.6613\u001b[0m      \u001b[32m240.4878\u001b[0m  0.0829\n",
      "      5      \u001b[36m263.2168\u001b[0m      272.4999  0.0850\n",
      "      5      288.6960      306.1034  0.0860\n",
      "      5      \u001b[36m303.7727\u001b[0m      \u001b[32m286.1845\u001b[0m  0.0885\n",
      "      5      \u001b[36m309.1277\u001b[0m      \u001b[32m304.0527\u001b[0m  0.0892\n",
      "      6      \u001b[36m240.1599\u001b[0m      241.8662  0.0832\n",
      "      6      \u001b[36m261.7090\u001b[0m      \u001b[32m271.9474\u001b[0m  0.0843\n",
      "      6      \u001b[36m285.3837\u001b[0m      306.8820  0.0869\n",
      "      6      \u001b[36m303.2995\u001b[0m      287.4445  0.0874\n",
      "      6      309.4410      305.4662  0.0888\n",
      "      7      \u001b[36m239.4426\u001b[0m      241.4225  0.0829\n",
      "      7      \u001b[36m260.3781\u001b[0m      \u001b[32m271.4698\u001b[0m  0.0847\n",
      "      7      285.4896      307.5972  0.0866\n",
      "      7      \u001b[36m302.9260\u001b[0m      288.4277  0.0885\n",
      "      7      \u001b[36m309.0934\u001b[0m      304.2478  0.0900\n",
      "      8      \u001b[36m237.7938\u001b[0m      241.3997  0.0840\n",
      "      8      260.5337      271.8194  0.0860\n",
      "      8      286.1207      306.4683  0.0874\n",
      "      8      \u001b[36m302.3003\u001b[0m      287.5737  0.0887\n",
      "      8      \u001b[36m308.1934\u001b[0m      \u001b[32m303.7191\u001b[0m  0.0906\n",
      "      9      238.4627      242.7904  0.0845\n",
      "      9      \u001b[36m260.2300\u001b[0m      272.4270  0.0861\n",
      "      9      \u001b[36m285.1171\u001b[0m      307.2156  0.0874\n",
      "      9      \u001b[36m302.0046\u001b[0m      288.1628  0.0897\n",
      "      9      308.2082      304.8486  0.0906\n",
      "     10      238.1947      242.2422  0.0831\n",
      "Restoring best model from epoch 5.\n",
      "     10      260.2913      272.7824  0.0837\n",
      "Restoring best model from epoch 7.\n",
      "     10      \u001b[36m283.6142\u001b[0m      306.9920  0.0864\n",
      "Restoring best model from epoch 4.\n",
      "     10      \u001b[36m301.4206\u001b[0m      288.3135  0.0876\n",
      "Restoring best model from epoch 5.\n",
      "     10      \u001b[36m307.3180\u001b[0m      304.6466  0.0883\n",
      "Restoring best model from epoch 8.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1     \u001b[36m3561.9026\u001b[0m      \u001b[32m880.9750\u001b[0m  0.0735\n",
      "      2     4574.0146      \u001b[32m867.6483\u001b[0m  0.0757\n",
      "      3     3608.9636      \u001b[32m863.2870\u001b[0m  0.0740\n",
      "      4     \u001b[36m3458.6824\u001b[0m      \u001b[32m862.6125\u001b[0m  0.0786\n",
      "      5     \u001b[36m3438.9099\u001b[0m      863.0607  0.0775\n",
      "      6     \u001b[36m3435.2627\u001b[0m      864.2438  0.0752\n",
      "      7     3443.2488      865.5958  0.0775\n",
      "      8     3440.3953      866.0100  0.0761\n",
      "      9     \u001b[36m3425.6934\u001b[0m      865.8800  0.0764\n",
      "     10     \u001b[36m3405.5762\u001b[0m      866.2565  0.1920\n",
      "Restoring best model from epoch 4.\n",
      "Concordance Index 0.6039610745614035\n",
      "Integrated Brier Score: 0.1671943612190772\n",
      "durations 1.2666667 351.0\n",
      "Concordance Index 0.587110070632491\n",
      "Integrated Brier Score: 0.17693777025097396\n",
      "load_flchain\n",
      "split age            float32\n",
      "sex           category\n",
      "sample_yr      float32\n",
      "kappa          float32\n",
      "lambda         float32\n",
      "flc_grp        float32\n",
      "creatinine     float32\n",
      "mgus_0.0         uint8\n",
      "mgus_1.0         uint8\n",
      "dtype: object\n",
      "(6296, 9) <class 'pandas.core.frame.DataFrame'>\n",
      "(6296,) <class 'pandas.core.series.Series'>\n",
      "(1575, 9) <class 'pandas.core.frame.DataFrame'>\n",
      "(1575,) <class 'pandas.core.series.Series'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/pycox/evaluation/eval_surv.py:36: FutureWarning: is_monotonic is deprecated and will be removed in a future version. Use is_monotonic_increasing instead.\n",
      "  assert pd.Series(self.index_surv).is_monotonic\n",
      "/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/pycox/evaluation/eval_surv.py:36: FutureWarning: is_monotonic is deprecated and will be removed in a future version. Use is_monotonic_increasing instead.\n",
      "  assert pd.Series(self.index_surv).is_monotonic\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1     \u001b[36m1132.7626\u001b[0m      \u001b[32m934.3302\u001b[0m  0.2093\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1     \u001b[36m2247.2872\u001b[0m     \u001b[32m1354.1697\u001b[0m  0.2184\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1     \u001b[36m2553.7782\u001b[0m     \u001b[32m1860.8048\u001b[0m  0.2272\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1     \u001b[36m2715.9485\u001b[0m     \u001b[32m1997.5682\u001b[0m  0.2459\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1     \u001b[36m2676.3245\u001b[0m     \u001b[32m2063.7544\u001b[0m  0.2349\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m76.5684\u001b[0m       \u001b[32m79.8806\u001b[0m  0.3255\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m104.8489\u001b[0m      \u001b[32m105.6273\u001b[0m  0.3973\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m161.1451\u001b[0m      \u001b[32m163.6947\u001b[0m  0.3856\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m158.7029\u001b[0m      \u001b[32m158.0960\u001b[0m  0.3968\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m144.7044\u001b[0m      \u001b[32m147.6939\u001b[0m  0.4060\n",
      "      2      \u001b[36m970.7433\u001b[0m      944.4321  0.2058\n",
      "      2     \u001b[36m1443.4609\u001b[0m     1378.1716  0.2184\n",
      "      2     \u001b[36m2007.8513\u001b[0m     1892.1407  0.2273\n",
      "      2     \u001b[36m2176.3799\u001b[0m     2095.3953  0.2311\n",
      "      2     \u001b[36m2167.6593\u001b[0m     2029.5057  0.2575\n",
      "      3      \u001b[36m947.4218\u001b[0m      947.2871  0.2098\n",
      "      2       \u001b[36m73.0540\u001b[0m       \u001b[32m79.3382\u001b[0m  0.3287\n",
      "      3     \u001b[36m1366.7436\u001b[0m     1402.0312  0.2287\n",
      "      3     \u001b[36m1890.1225\u001b[0m     1924.7736  0.2137\n",
      "      3     \u001b[36m2068.8619\u001b[0m     2116.2520  0.2234\n",
      "      2      \u001b[36m102.9202\u001b[0m      107.4544  0.3782\n",
      "      2      \u001b[36m140.9234\u001b[0m      153.6002  0.3735\n",
      "      2      \u001b[36m156.2934\u001b[0m      \u001b[32m161.8290\u001b[0m  0.3848\n",
      "      3     \u001b[36m2040.8254\u001b[0m     2053.3848  0.2406\n",
      "      2      \u001b[36m154.9727\u001b[0m      \u001b[32m156.8318\u001b[0m  0.4125\n",
      "      4      \u001b[36m939.1485\u001b[0m      935.9500  0.2200\n",
      "      4     1375.2571     1373.8030  0.2211\n",
      "      4     1900.8755     1909.6471  0.2210\n",
      "      3       \u001b[36m72.9008\u001b[0m       \u001b[32m73.0371\u001b[0m  0.3323\n",
      "      4     2074.9471     2111.3577  0.2472\n",
      "      4     2046.0778     2046.7321  0.2238\n",
      "      5      \u001b[36m926.9997\u001b[0m      939.0156  0.2313\n",
      "      5     \u001b[36m1343.3799\u001b[0m     1359.6257  0.2116\n",
      "      3      \u001b[36m140.0667\u001b[0m      \u001b[32m146.8380\u001b[0m  0.3477\n",
      "      5     \u001b[36m1875.2317\u001b[0m     1877.2120  0.2296\n",
      "      3      \u001b[36m101.6320\u001b[0m      \u001b[32m104.9263\u001b[0m  0.3886\n",
      "      3      \u001b[36m154.4628\u001b[0m      161.8342  0.3957\n",
      "      3      \u001b[36m153.5548\u001b[0m      157.5172  0.4127\n",
      "      5     \u001b[36m2032.1330\u001b[0m     2020.1329  0.2234\n",
      "      5     \u001b[36m2066.6859\u001b[0m     2072.5117  0.2514\n",
      "      6      \u001b[36m918.8342\u001b[0m      \u001b[32m928.5141\u001b[0m  0.2153\n",
      "      6     \u001b[36m1331.8210\u001b[0m     1369.1687  0.1996\n",
      "      4       \u001b[36m72.2232\u001b[0m       74.7770  0.3447\n",
      "      6     \u001b[36m1851.1441\u001b[0m     1861.8430  0.2287\n",
      "      6     \u001b[36m1999.6438\u001b[0m     \u001b[32m1988.8940\u001b[0m  0.2485\n",
      "      6     \u001b[36m2029.1604\u001b[0m     \u001b[32m2039.1941\u001b[0m  0.2432\n",
      "      7      \u001b[36m916.6577\u001b[0m      \u001b[32m917.3007\u001b[0m  0.1934\n",
      "      4      \u001b[36m101.1048\u001b[0m      \u001b[32m104.0227\u001b[0m  0.3476\n",
      "      7     1333.9886     \u001b[32m1326.7207\u001b[0m  0.2018\n",
      "      4      \u001b[36m139.6097\u001b[0m      \u001b[32m146.4993\u001b[0m  0.3978\n",
      "      4      \u001b[36m153.9501\u001b[0m      162.0176  0.4066\n",
      "      7     \u001b[36m1848.6296\u001b[0m     1865.5393  0.2103\n",
      "      4      \u001b[36m152.6159\u001b[0m      161.2884  0.4109\n",
      "      7     \u001b[36m1996.0624\u001b[0m     \u001b[32m1976.9003\u001b[0m  0.2173\n",
      "      5       \u001b[36m71.9533\u001b[0m       \u001b[32m72.4812\u001b[0m  0.3802\n",
      "      8      \u001b[36m915.2489\u001b[0m      957.9200  0.2023\n",
      "      8     \u001b[36m1325.7898\u001b[0m     \u001b[32m1324.2710\u001b[0m  0.2083\n",
      "      7     2029.7957     \u001b[32m2028.2311\u001b[0m  0.2547\n",
      "      8     \u001b[36m1845.3988\u001b[0m     \u001b[32m1853.9071\u001b[0m  0.2447\n",
      "      5      \u001b[36m100.7574\u001b[0m      104.5294  0.3500\n",
      "      5      140.1747      149.5208  0.3686\n",
      "      9      \u001b[36m911.5806\u001b[0m      968.4083  0.2199\n",
      "      8     \u001b[36m1992.8025\u001b[0m     1979.1804  0.2313\n",
      "      9     \u001b[36m1321.8570\u001b[0m     1327.4709  0.2167\n",
      "      8     \u001b[36m2023.4656\u001b[0m     \u001b[32m2023.2133\u001b[0m  0.2348\n",
      "      5      \u001b[36m153.8525\u001b[0m      162.0476  0.4119\n",
      "      5      153.0144      159.5505  0.3773\n",
      "      9     \u001b[36m1844.1680\u001b[0m     \u001b[32m1845.4690\u001b[0m  0.2094\n",
      "      6       72.0511       74.3989  0.3795\n",
      "     10      912.1623      \u001b[32m907.4827\u001b[0m  0.1954\n",
      "     10     \u001b[36m1316.6946\u001b[0m     1335.9692  0.2048\n",
      "Restoring best model from epoch 8.\n",
      "      9     \u001b[36m1989.7838\u001b[0m     \u001b[32m1962.6027\u001b[0m  0.2527\n",
      "      9     \u001b[36m2013.9586\u001b[0m     2028.1453  0.2453\n",
      "      6      \u001b[36m100.3929\u001b[0m      107.5155  0.3714\n",
      "      6      140.0497      150.4436  0.3474\n",
      "     10     1844.7757     \u001b[32m1840.1779\u001b[0m  0.2146\n",
      "      6      154.1862      162.6525  0.3602\n",
      "      6      \u001b[36m152.5852\u001b[0m           nan  0.3537\n",
      "     10     \u001b[36m1984.9733\u001b[0m     \u001b[32m1960.0962\u001b[0m  0.2057\n",
      "      7       \u001b[36m71.7738\u001b[0m       75.3276  0.3185\n",
      "     10     \u001b[36m2012.7548\u001b[0m     \u001b[32m2020.1243\u001b[0m  0.2063\n",
      "      7      100.8422      104.5901  0.3178\n",
      "      7      \u001b[36m138.6238\u001b[0m      147.4853  0.3243\n",
      "      7      \u001b[36m153.1616\u001b[0m      \u001b[32m161.2078\u001b[0m  0.3323\n",
      "      8       \u001b[36m71.5706\u001b[0m       \u001b[32m71.9195\u001b[0m  0.2995\n",
      "      7      \u001b[36m151.8697\u001b[0m      158.3253  0.3317\n",
      "      8      \u001b[36m100.2717\u001b[0m      106.0617  0.3146\n",
      "      8      139.6390      150.1822  0.3247\n",
      "      9       71.6309       74.4524  0.2990\n",
      "      8      153.5827      161.9493  0.3321\n",
      "      8      152.1088      164.1357  0.3292\n",
      "      9      \u001b[36m100.1050\u001b[0m      104.9436  0.3154\n",
      "      9      139.1313      147.5566  0.3256\n",
      "     10       \u001b[36m71.3768\u001b[0m       72.7510  0.2994\n",
      "Restoring best model from epoch 8.\n",
      "      9      153.2698      \u001b[32m160.8769\u001b[0m  0.3303\n",
      "      9      \u001b[36m151.5589\u001b[0m      158.5135  0.3293\n",
      "     10      100.3893      104.6270  0.3156\n",
      "Restoring best model from epoch 4.\n",
      "     10      138.8370      149.5850  0.3236\n",
      "Restoring best model from epoch 4.\n",
      "     10      \u001b[36m153.0756\u001b[0m      161.3745  0.3267\n",
      "Restoring best model from epoch 9.\n",
      "     10      \u001b[36m150.7072\u001b[0m      158.9253  0.3249\n",
      "Restoring best model from epoch 2.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1     \u001b[36m1968.2003\u001b[0m     \u001b[32m1468.8823\u001b[0m  0.2439\n",
      "      2     \u001b[36m1713.6366\u001b[0m     1491.0912  0.2450\n",
      "      3     \u001b[36m1690.1015\u001b[0m     \u001b[32m1464.5986\u001b[0m  0.2414\n",
      "      4     \u001b[36m1632.4429\u001b[0m     \u001b[32m1426.9321\u001b[0m  0.2452\n",
      "      5     \u001b[36m1606.0727\u001b[0m     \u001b[32m1416.5475\u001b[0m  0.2412\n",
      "      6     \u001b[36m1599.7456\u001b[0m     \u001b[32m1416.3914\u001b[0m  0.2421\n",
      "      7     \u001b[36m1593.8111\u001b[0m     \u001b[32m1413.4926\u001b[0m  0.2472\n",
      "      8     \u001b[36m1589.3294\u001b[0m     \u001b[32m1412.1802\u001b[0m  0.2405\n",
      "      9     \u001b[36m1587.5952\u001b[0m     1413.0817  0.2415\n",
      "     10     \u001b[36m1585.3681\u001b[0m     1414.2500  0.2400\n",
      "Restoring best model from epoch 8.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/pycox/evaluation/eval_surv.py:36: FutureWarning: is_monotonic is deprecated and will be removed in a future version. Use is_monotonic_increasing instead.\n",
      "  assert pd.Series(self.index_surv).is_monotonic\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concordance Index 0.7892602373896233\n",
      "Integrated Brier Score: 0.10264072727155817\n",
      "durations 1.0 5215.0\n",
      "Concordance Index 0.7999081261484231\n",
      "Integrated Brier Score: 0.10374214047695596\n",
      "(6297, 9) <class 'pandas.core.frame.DataFrame'>\n",
      "(6297,) <class 'pandas.core.series.Series'>\n",
      "(1574, 9) <class 'pandas.core.frame.DataFrame'>\n",
      "(1574,) <class 'pandas.core.series.Series'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/pycox/evaluation/eval_surv.py:36: FutureWarning: is_monotonic is deprecated and will be removed in a future version. Use is_monotonic_increasing instead.\n",
      "  assert pd.Series(self.index_surv).is_monotonic\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m32.6089\u001b[0m       \u001b[32m31.6413\u001b[0m  0.4244\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m44.8537\u001b[0m       \u001b[32m45.2798\u001b[0m  0.4336\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m60.2694\u001b[0m       \u001b[32m58.6035\u001b[0m  0.4341\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m66.1613\u001b[0m       \u001b[32m63.6303\u001b[0m  0.4606\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m67.7624\u001b[0m       \u001b[32m63.8072\u001b[0m  0.4610\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m188.8795\u001b[0m      \u001b[32m181.1588\u001b[0m  0.4677\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m398.1186\u001b[0m      \u001b[32m512.7636\u001b[0m  0.4964\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m372.6089\u001b[0m      \u001b[32m346.3249\u001b[0m  0.5167\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m278.4946\u001b[0m      \u001b[32m273.6314\u001b[0m  0.5292\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m407.4364\u001b[0m      \u001b[32m401.7021\u001b[0m  0.5423\n",
      "      2       \u001b[36m31.4430\u001b[0m       \u001b[32m31.4155\u001b[0m  0.3919\n",
      "      2       \u001b[36m43.3683\u001b[0m       45.7055  0.4393\n",
      "      2       \u001b[36m58.4934\u001b[0m       \u001b[32m58.3852\u001b[0m  0.4354\n",
      "      2       \u001b[36m65.7792\u001b[0m       \u001b[32m63.5518\u001b[0m  0.4359\n",
      "      2       \u001b[36m64.7350\u001b[0m       \u001b[32m62.7678\u001b[0m  0.4620\n",
      "      2      \u001b[36m174.8688\u001b[0m      186.2067  0.4553\n",
      "      2      \u001b[36m369.0430\u001b[0m      \u001b[32m373.8590\u001b[0m  0.5034\n",
      "      2      \u001b[36m333.7814\u001b[0m      \u001b[32m343.3261\u001b[0m  0.5303\n",
      "      2      \u001b[36m241.0425\u001b[0m      \u001b[32m258.6909\u001b[0m  0.5353\n",
      "      2      \u001b[36m376.0790\u001b[0m      \u001b[32m385.7150\u001b[0m  0.5467\n",
      "      3       \u001b[36m31.1233\u001b[0m       31.6092  0.3840\n",
      "      3       58.5353       59.0506  0.4292\n",
      "      3       \u001b[36m42.9581\u001b[0m       45.8215  0.4424\n",
      "      3       65.8508       \u001b[32m63.1713\u001b[0m  0.4545\n",
      "      3       \u001b[36m64.5879\u001b[0m       63.0216  0.4399\n",
      "      3      \u001b[36m172.3565\u001b[0m      \u001b[32m178.8097\u001b[0m  0.4570\n",
      "      3      \u001b[36m365.9430\u001b[0m      \u001b[32m367.6152\u001b[0m  0.5976\n",
      "      4       \u001b[36m30.8760\u001b[0m       31.5246  0.5041\n",
      "      3      \u001b[36m329.6750\u001b[0m      \u001b[32m339.3020\u001b[0m  0.6186\n",
      "      3      \u001b[36m240.4490\u001b[0m      \u001b[32m257.2431\u001b[0m  0.6475\n",
      "      4       \u001b[36m58.0563\u001b[0m       58.6861  0.4743\n",
      "      3      \u001b[36m374.1527\u001b[0m      387.7250  0.6506\n",
      "      4       \u001b[36m42.8358\u001b[0m       46.3544  0.5864\n",
      "      4      \u001b[36m171.2467\u001b[0m      181.5184  0.6355\n",
      "      4       \u001b[36m65.5730\u001b[0m       \u001b[32m63.0262\u001b[0m  0.6890\n",
      "      4       \u001b[36m64.3165\u001b[0m       63.0309  0.7106\n",
      "      5       \u001b[36m57.8051\u001b[0m       58.5531  0.4281\n",
      "      5       30.8943       31.8961  0.5185\n",
      "      4      366.0132      \u001b[32m367.0775\u001b[0m  0.5653\n",
      "      4      \u001b[36m328.1217\u001b[0m      \u001b[32m336.8836\u001b[0m  0.5583\n",
      "      4      \u001b[36m238.8724\u001b[0m      \u001b[32m254.4541\u001b[0m  0.5290\n",
      "      5       \u001b[36m42.3436\u001b[0m       45.9208  0.3932\n",
      "      4      \u001b[36m371.7410\u001b[0m      \u001b[32m385.6962\u001b[0m  0.5233\n",
      "      5       64.3254       64.1446  0.4015\n",
      "      5       \u001b[36m65.1932\u001b[0m       63.5840  0.4455\n",
      "      5      \u001b[36m170.2177\u001b[0m      180.2478  0.4772\n",
      "      6       30.9277       31.8225  0.3754\n",
      "      6       \u001b[36m57.6730\u001b[0m       59.0114  0.4229\n",
      "      6       42.4071       45.6245  0.4221\n",
      "      5      \u001b[36m363.5366\u001b[0m      \u001b[32m365.9548\u001b[0m  0.5393\n",
      "      5      \u001b[36m238.0197\u001b[0m      254.5599  0.5029\n",
      "      5      \u001b[36m326.5753\u001b[0m      338.2472  0.5335\n",
      "      5      \u001b[36m370.9487\u001b[0m      \u001b[32m374.7938\u001b[0m  0.5329\n",
      "      6       \u001b[36m63.9450\u001b[0m       62.8485  0.4596\n",
      "      6       \u001b[36m64.9872\u001b[0m       63.1116  0.4533\n",
      "      7       \u001b[36m30.5649\u001b[0m       31.7219  0.4171\n",
      "      6      \u001b[36m169.4324\u001b[0m      \u001b[32m178.6374\u001b[0m  0.4795\n",
      "      7       \u001b[36m57.4349\u001b[0m       58.7460  0.4672\n",
      "      7       42.4291       \u001b[32m44.9202\u001b[0m  0.4134\n",
      "      6      \u001b[36m237.9964\u001b[0m      \u001b[32m254.2701\u001b[0m  0.4881\n",
      "      6      \u001b[36m325.4891\u001b[0m      340.1743  0.5048\n",
      "      6      \u001b[36m362.4882\u001b[0m      368.0343  0.5433\n",
      "      6      371.0256      \u001b[32m374.0906\u001b[0m  0.4845\n",
      "      7       \u001b[36m63.8557\u001b[0m       63.0096  0.4143\n",
      "      7       \u001b[36m64.9567\u001b[0m       63.1954  0.4068\n",
      "      8       30.5919       31.5652  0.3742\n",
      "      7      \u001b[36m169.2832\u001b[0m      179.3128  0.4450\n",
      "      8       \u001b[36m42.1876\u001b[0m       \u001b[32m44.5621\u001b[0m  0.4206\n",
      "      8       \u001b[36m57.2449\u001b[0m       58.9168  0.5457\n",
      "      7      \u001b[36m325.0720\u001b[0m      339.1995  0.5422\n",
      "      7      \u001b[36m237.2037\u001b[0m      \u001b[32m253.8805\u001b[0m  0.5791\n",
      "      7      \u001b[36m361.9592\u001b[0m      369.2252  0.5763\n",
      "      9       \u001b[36m30.1289\u001b[0m       31.4867  0.5072\n",
      "      8       \u001b[36m64.9172\u001b[0m       63.9089  0.5380\n",
      "      8       63.9258       \u001b[32m62.7212\u001b[0m  0.5479\n",
      "      7      373.0635      376.0831  0.5913\n",
      "      8      \u001b[36m168.3797\u001b[0m      179.9017  0.5294\n",
      "      9       \u001b[36m42.0242\u001b[0m       46.7704  0.5278\n",
      "      9       \u001b[36m57.0698\u001b[0m       58.6280  0.5437\n",
      "     10       \u001b[36m29.7874\u001b[0m       31.5037  0.4363\n",
      "Restoring best model from epoch 2.\n",
      "      9       63.8977       62.7724  0.4324\n",
      "      9       64.9660       63.3247  0.4522\n",
      "      8      \u001b[36m236.0548\u001b[0m      254.7894  0.5355\n",
      "      8      325.4965      341.2455  0.5494\n",
      "      8      \u001b[36m361.2572\u001b[0m      367.7344  0.5573\n",
      "      9      \u001b[36m167.4493\u001b[0m      \u001b[32m178.2852\u001b[0m  0.4879\n",
      "      8      371.3721      \u001b[32m371.2669\u001b[0m  0.5550\n",
      "     10       \u001b[36m41.9391\u001b[0m       45.2238  0.4315\n",
      "Restoring best model from epoch 8.\n",
      "     10       \u001b[36m56.8972\u001b[0m       58.6201  0.4132\n",
      "Restoring best model from epoch 2.\n",
      "     10       \u001b[36m63.7255\u001b[0m       62.7341  0.4028\n",
      "Restoring best model from epoch 8.\n",
      "     10       \u001b[36m64.6886\u001b[0m       63.3369  0.4164\n",
      "Restoring best model from epoch 4.\n",
      "      9      \u001b[36m324.2136\u001b[0m      337.1808  0.4556\n",
      "      9      236.5975      \u001b[32m253.6414\u001b[0m  0.4633\n",
      "      9      \u001b[36m361.1360\u001b[0m      367.5787  0.4508\n",
      "     10      \u001b[36m167.3863\u001b[0m      181.3007  0.4262\n",
      "Restoring best model from epoch 9.\n",
      "      9      \u001b[36m368.8646\u001b[0m      \u001b[32m369.5969\u001b[0m  0.4548\n",
      "     10      \u001b[36m234.6464\u001b[0m      255.0334  0.4283\n",
      "Restoring best model from epoch 9.\n",
      "     10      \u001b[36m323.4661\u001b[0m      338.5320  0.4367\n",
      "Restoring best model from epoch 4.\n",
      "     10      \u001b[36m359.8422\u001b[0m      367.0549  0.4420\n",
      "Restoring best model from epoch 5.\n",
      "     10      \u001b[36m368.7160\u001b[0m      \u001b[32m368.7723\u001b[0m  0.4409\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m54.8324\u001b[0m       \u001b[32m48.0084\u001b[0m  0.4940\n",
      "      2       \u001b[36m53.3771\u001b[0m       \u001b[32m47.7006\u001b[0m  0.4837\n",
      "      3       \u001b[36m52.9977\u001b[0m       47.7174  0.4756\n",
      "      4       53.2209       \u001b[32m47.5479\u001b[0m  0.4825\n",
      "      5       \u001b[36m52.7392\u001b[0m       48.0008  0.4763\n",
      "      6       \u001b[36m52.5602\u001b[0m       47.7743  0.4745\n",
      "      7       \u001b[36m52.3360\u001b[0m       47.9265  0.4699\n",
      "      8       52.3815       47.6628  0.4774\n",
      "      9       \u001b[36m52.2210\u001b[0m       47.7813  0.4738\n",
      "     10       \u001b[36m51.9700\u001b[0m       47.6488  0.4783\n",
      "Restoring best model from epoch 4.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/pycox/evaluation/eval_surv.py:36: FutureWarning: is_monotonic is deprecated and will be removed in a future version. Use is_monotonic_increasing instead.\n",
      "  assert pd.Series(self.index_surv).is_monotonic\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concordance Index 0.7988506378373427\n",
      "Integrated Brier Score: 0.09912649479415031\n",
      "durations 1.0 5123.0\n",
      "Concordance Index 0.8048386048627422\n",
      "Integrated Brier Score: 0.09776734913552253\n",
      "(6297, 9) <class 'pandas.core.frame.DataFrame'>\n",
      "(6297,) <class 'pandas.core.series.Series'>\n",
      "(1574, 9) <class 'pandas.core.frame.DataFrame'>\n",
      "(1574,) <class 'pandas.core.series.Series'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/pycox/evaluation/eval_surv.py:36: FutureWarning: is_monotonic is deprecated and will be removed in a future version. Use is_monotonic_increasing instead.\n",
      "  assert pd.Series(self.index_surv).is_monotonic\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m31.9348\u001b[0m       \u001b[32m30.8790\u001b[0m  0.3957\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m61.6850\u001b[0m       \u001b[32m55.2658\u001b[0m  0.4109\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m46.9085\u001b[0m       \u001b[32m40.2280\u001b[0m  0.4324\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m66.7236\u001b[0m       \u001b[32m62.2651\u001b[0m  0.4378\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m68.6133\u001b[0m       \u001b[32m63.8959\u001b[0m  0.4331\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1           nan      \u001b[32m185.4028\u001b[0m  0.4611\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m301.7450\u001b[0m      \u001b[32m251.4940\u001b[0m  0.5029\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m434.6162\u001b[0m      \u001b[32m372.1170\u001b[0m  0.5366\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m433.7356\u001b[0m      \u001b[32m384.7438\u001b[0m  0.5455\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1           nan      \u001b[32m335.2468\u001b[0m  0.5805\n",
      "      2       \u001b[36m30.9117\u001b[0m       \u001b[32m30.6044\u001b[0m  0.4130\n",
      "      2       \u001b[36m44.7230\u001b[0m       \u001b[32m39.7034\u001b[0m  0.3969\n",
      "      2       \u001b[36m60.2135\u001b[0m       55.5565  0.4461\n",
      "      2       \u001b[36m66.7198\u001b[0m       63.9228  0.4248\n",
      "      2       \u001b[36m65.9403\u001b[0m       \u001b[32m62.0828\u001b[0m  0.4619\n",
      "      2      \u001b[36m172.7607\u001b[0m      \u001b[32m174.6564\u001b[0m  0.4880\n",
      "      2      \u001b[36m374.2264\u001b[0m      \u001b[32m365.0549\u001b[0m  0.5037\n",
      "      2      \u001b[36m380.7548\u001b[0m      \u001b[32m379.8470\u001b[0m  0.4891\n",
      "      2      \u001b[36m345.7430\u001b[0m      337.2962  0.4820\n",
      "      2      \u001b[36m261.0317\u001b[0m      \u001b[32m233.1461\u001b[0m  0.5845\n",
      "      3       \u001b[36m44.0439\u001b[0m       39.7101  0.4162\n",
      "      3       \u001b[36m59.9179\u001b[0m       55.3243  0.4077\n",
      "      3       \u001b[36m66.2468\u001b[0m       \u001b[32m63.3647\u001b[0m  0.3969\n",
      "      3       \u001b[36m30.6941\u001b[0m       30.8745  0.4799\n",
      "      3       \u001b[36m64.8411\u001b[0m       \u001b[32m61.9454\u001b[0m  0.4384\n",
      "      3      \u001b[36m168.7873\u001b[0m      174.7636  0.4750\n",
      "      3      \u001b[36m248.7787\u001b[0m      \u001b[32m231.9102\u001b[0m  0.4662\n",
      "      3      \u001b[36m373.2432\u001b[0m      \u001b[32m374.9450\u001b[0m  0.5013\n",
      "      3      \u001b[36m341.1072\u001b[0m      \u001b[32m330.2099\u001b[0m  0.5144\n",
      "      3      \u001b[36m366.2273\u001b[0m      \u001b[32m362.7687\u001b[0m  0.5369\n",
      "      4       44.4866       \u001b[32m39.5562\u001b[0m  0.4065\n",
      "      4       \u001b[36m59.1815\u001b[0m       55.2935  0.4369\n",
      "      4       \u001b[36m65.6948\u001b[0m       63.6532  0.4357\n",
      "      4       \u001b[36m30.4397\u001b[0m       \u001b[32m30.3236\u001b[0m  0.4539\n",
      "      4       \u001b[36m64.7826\u001b[0m       62.1392  0.4380\n",
      "      4      \u001b[36m168.0900\u001b[0m      \u001b[32m174.1246\u001b[0m  0.4554\n",
      "      4      \u001b[36m247.9328\u001b[0m      232.9601  0.4629\n",
      "      5       \u001b[36m43.7786\u001b[0m       \u001b[32m39.3750\u001b[0m  0.4062\n",
      "      4      367.0025      365.2207  0.4947\n",
      "      4      \u001b[36m335.9528\u001b[0m      \u001b[32m325.8412\u001b[0m  0.5057\n",
      "      4      \u001b[36m370.3039\u001b[0m      \u001b[32m372.5546\u001b[0m  0.5398\n",
      "      5       \u001b[36m58.9252\u001b[0m       55.3027  0.4445\n",
      "      5       \u001b[36m30.1774\u001b[0m       \u001b[32m30.2334\u001b[0m  0.4336\n",
      "      5       \u001b[36m64.2696\u001b[0m       62.0356  0.4254\n",
      "      5       \u001b[36m65.6357\u001b[0m       63.5593  0.5122\n",
      "      5      168.5508      \u001b[32m173.6111\u001b[0m  0.4648\n",
      "      6       43.8813       \u001b[32m39.0694\u001b[0m  0.4063\n",
      "      5      \u001b[36m246.7740\u001b[0m      232.8712  0.4634\n",
      "      6       59.4928       \u001b[32m55.0615\u001b[0m  0.4218\n",
      "      5      \u001b[36m365.8592\u001b[0m      363.9934  0.4959\n",
      "      6       \u001b[36m29.8299\u001b[0m       \u001b[32m29.6827\u001b[0m  0.4468\n",
      "      5      \u001b[36m368.4097\u001b[0m      372.9232  0.4969\n",
      "      5      \u001b[36m332.2280\u001b[0m      326.0683  0.5077\n",
      "      6       \u001b[36m65.5039\u001b[0m       63.5991  0.4684\n",
      "      6       64.2986       \u001b[32m61.7359\u001b[0m  0.5003\n",
      "      6      168.2558      \u001b[32m172.4895\u001b[0m  0.4794\n",
      "      7       \u001b[36m43.6461\u001b[0m       39.6290  0.3863\n",
      "      6      \u001b[36m246.2850\u001b[0m      233.0218  0.4767\n",
      "      7       \u001b[36m58.4752\u001b[0m       \u001b[32m54.9770\u001b[0m  0.4511\n",
      "      7       30.1663       30.3090  0.4008\n",
      "      7       \u001b[36m65.2894\u001b[0m       63.7387  0.4196\n",
      "      6      \u001b[36m361.9161\u001b[0m      363.8930  0.5075\n",
      "      7       64.3619       61.8644  0.4461\n",
      "      6      \u001b[36m367.2032\u001b[0m      372.9603  0.5431\n",
      "      6      332.5304      326.1936  0.5451\n",
      "      8       \u001b[36m43.5155\u001b[0m       39.1987  0.4226\n",
      "      7      169.1082      173.7891  0.4712\n",
      "      8       58.5074       \u001b[32m54.8291\u001b[0m  0.4276\n",
      "      8       29.8968       29.9159  0.4251\n",
      "      7      \u001b[36m245.1864\u001b[0m      232.2153  0.4837\n",
      "      8       \u001b[36m65.2382\u001b[0m       63.5568  0.4411\n",
      "      8       \u001b[36m63.9407\u001b[0m       61.9011  0.4360\n",
      "      7      362.8668      364.0584  0.5374\n",
      "      7      \u001b[36m367.1900\u001b[0m      374.0475  0.4968\n",
      "      9       43.6912       39.3233  0.4030\n",
      "      7      \u001b[36m332.0014\u001b[0m      326.4664  0.5328\n",
      "      8      \u001b[36m166.9289\u001b[0m      \u001b[32m172.3389\u001b[0m  0.4554\n",
      "      9       58.5949       \u001b[32m54.8148\u001b[0m  0.4229\n",
      "      9       30.0515       \u001b[32m29.5003\u001b[0m  0.4285\n",
      "      8      246.0950      235.1319  0.4832\n",
      "      9       \u001b[36m64.8414\u001b[0m       63.6067  0.4156\n",
      "      9       \u001b[36m63.8386\u001b[0m       61.8473  0.4203\n",
      "     10       43.6637       39.4420  0.4104\n",
      "Restoring best model from epoch 6.\n",
      "      8      \u001b[36m366.3244\u001b[0m      373.7120  0.5250\n",
      "      8      363.3364      366.7709  0.5386\n",
      "      8      \u001b[36m330.8470\u001b[0m      327.2024  0.4995\n",
      "      9      \u001b[36m165.3598\u001b[0m      172.6260  0.4636\n",
      "     10       \u001b[36m29.7169\u001b[0m       29.6309  0.4044\n",
      "Restoring best model from epoch 9.\n",
      "     10       \u001b[36m58.3855\u001b[0m       55.1783  0.4271\n",
      "Restoring best model from epoch 9.\n",
      "     10       \u001b[36m64.5038\u001b[0m       63.5307  0.4210\n",
      "Restoring best model from epoch 3.\n",
      "     10       \u001b[36m63.8292\u001b[0m       \u001b[32m61.7120\u001b[0m  0.3965\n",
      "      9      \u001b[36m245.1805\u001b[0m      235.0089  0.4754\n",
      "     10      166.6244      172.8576  0.4072\n",
      "Restoring best model from epoch 8.\n",
      "      9      363.9478      367.3259  0.4523\n",
      "      9      \u001b[36m365.5130\u001b[0m      \u001b[32m372.2626\u001b[0m  0.4582\n",
      "      9      \u001b[36m330.6019\u001b[0m      328.2480  0.4388\n",
      "     10      \u001b[36m243.1835\u001b[0m      \u001b[32m231.4286\u001b[0m  0.4184\n",
      "     10      \u001b[36m330.1399\u001b[0m      328.0154  0.4319\n",
      "Restoring best model from epoch 4.\n",
      "     10      \u001b[36m363.6971\u001b[0m      373.6252  0.4371\n",
      "Restoring best model from epoch 9.\n",
      "     10      362.1663      363.7537  0.4390\n",
      "Restoring best model from epoch 3.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m330.1861\u001b[0m      \u001b[32m294.9610\u001b[0m  0.4453\n",
      "      2      \u001b[36m301.3899\u001b[0m      \u001b[32m290.7466\u001b[0m  0.4412\n",
      "      3      \u001b[36m299.3770\u001b[0m      291.0948  0.4359\n",
      "      4      \u001b[36m298.0077\u001b[0m      \u001b[32m288.2023\u001b[0m  0.4381\n",
      "      5      \u001b[36m295.2840\u001b[0m      \u001b[32m287.2347\u001b[0m  0.4422\n",
      "      6      296.0568      287.5021  0.4433\n",
      "      7      295.6870      287.3532  0.4352\n",
      "      8      \u001b[36m295.0988\u001b[0m      287.8507  0.4395\n",
      "      9      \u001b[36m294.3400\u001b[0m      287.6333  0.4348\n",
      "     10      294.3834      287.5928  0.4425\n",
      "Restoring best model from epoch 5.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/pycox/evaluation/eval_surv.py:36: FutureWarning: is_monotonic is deprecated and will be removed in a future version. Use is_monotonic_increasing instead.\n",
      "  assert pd.Series(self.index_surv).is_monotonic\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concordance Index 0.8015267130949442\n",
      "Integrated Brier Score: 0.09801181463394257\n",
      "durations 1.0 5166.0\n",
      "Concordance Index 0.7869990755441599\n",
      "Integrated Brier Score: 0.10219977438413154\n",
      "(6297, 9) <class 'pandas.core.frame.DataFrame'>\n",
      "(6297,) <class 'pandas.core.series.Series'>\n",
      "(1574, 9) <class 'pandas.core.frame.DataFrame'>\n",
      "(1574,) <class 'pandas.core.series.Series'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/pycox/evaluation/eval_surv.py:36: FutureWarning: is_monotonic is deprecated and will be removed in a future version. Use is_monotonic_increasing instead.\n",
      "  assert pd.Series(self.index_surv).is_monotonic\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m44.2740\u001b[0m       \u001b[32m46.1580\u001b[0m  0.3926\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m30.4116\u001b[0m       \u001b[32m31.8677\u001b[0m  0.3959\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m66.2189\u001b[0m       \u001b[32m62.3383\u001b[0m  0.4357\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m67.6241\u001b[0m       \u001b[32m65.2236\u001b[0m  0.4337\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m60.9366\u001b[0m       \u001b[32m57.2915\u001b[0m  0.4740\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m359.4213\u001b[0m      \u001b[32m337.0289\u001b[0m  0.4958\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1           nan      \u001b[32m198.1433\u001b[0m  0.5070\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m275.1262\u001b[0m      \u001b[32m271.2950\u001b[0m  0.5116\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m400.8744\u001b[0m      \u001b[32m371.8101\u001b[0m  0.5023\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m398.5365\u001b[0m      \u001b[32m374.2150\u001b[0m  0.5257\n",
      "      2       \u001b[36m29.2602\u001b[0m       32.0763  0.4137\n",
      "      2       \u001b[36m42.2757\u001b[0m       \u001b[32m44.2681\u001b[0m  0.4236\n",
      "      2       \u001b[36m65.9995\u001b[0m       \u001b[32m63.1519\u001b[0m  0.4320\n",
      "      2       \u001b[36m64.5687\u001b[0m       \u001b[32m61.4731\u001b[0m  0.4543\n",
      "      2       \u001b[36m59.3437\u001b[0m       \u001b[32m57.2830\u001b[0m  0.4400\n",
      "      2      \u001b[36m336.3579\u001b[0m      \u001b[32m333.6039\u001b[0m  0.4765\n",
      "      2      \u001b[36m245.8500\u001b[0m      \u001b[32m259.5846\u001b[0m  0.4855\n",
      "      2      \u001b[36m167.2304\u001b[0m      \u001b[32m188.5154\u001b[0m  0.5130\n",
      "      2      \u001b[36m371.9124\u001b[0m      \u001b[32m371.3428\u001b[0m  0.5189\n",
      "      2      \u001b[36m363.4417\u001b[0m      \u001b[32m362.5540\u001b[0m  0.5168\n",
      "      3       \u001b[36m28.9136\u001b[0m       \u001b[32m31.4451\u001b[0m  0.4053\n",
      "      3       \u001b[36m41.9866\u001b[0m       44.9357  0.4361\n",
      "      3       \u001b[36m65.7005\u001b[0m       \u001b[32m62.8402\u001b[0m  0.4178\n",
      "      3       \u001b[36m58.5513\u001b[0m       57.6033  0.3923\n",
      "      3       \u001b[36m64.3381\u001b[0m       61.7724  0.5058\n",
      "      3      \u001b[36m164.1279\u001b[0m      189.5301  0.4865\n",
      "      3      \u001b[36m239.1922\u001b[0m      \u001b[32m257.6641\u001b[0m  0.5122\n",
      "      3      \u001b[36m370.9837\u001b[0m      \u001b[32m371.1150\u001b[0m  0.4714\n",
      "      3      \u001b[36m333.7115\u001b[0m      \u001b[32m330.7136\u001b[0m  0.5680\n",
      "      4       \u001b[36m28.8606\u001b[0m       31.8284  0.4078\n",
      "      3      \u001b[36m363.2074\u001b[0m      \u001b[32m361.7117\u001b[0m  0.5396\n",
      "      4       42.1761       45.4019  0.4508\n",
      "      4       \u001b[36m65.2538\u001b[0m       63.1442  0.4105\n",
      "      4       58.6852       58.3746  0.4062\n",
      "      4       \u001b[36m63.8800\u001b[0m       62.5170  0.4615\n",
      "      4      \u001b[36m163.3366\u001b[0m      190.2247  0.4732\n",
      "      4      \u001b[36m238.3231\u001b[0m      \u001b[32m257.4466\u001b[0m  0.4676\n",
      "      4      \u001b[36m370.3437\u001b[0m      \u001b[32m370.1230\u001b[0m  0.4922\n",
      "      5       \u001b[36m28.4359\u001b[0m       31.9560  0.4476\n",
      "      4      \u001b[36m331.7445\u001b[0m      \u001b[32m329.2778\u001b[0m  0.5137\n",
      "      5       \u001b[36m41.7146\u001b[0m       44.9958  0.4181\n",
      "      5       \u001b[36m58.1679\u001b[0m       57.9027  0.4076\n",
      "      5       \u001b[36m64.5272\u001b[0m       63.0670  0.4231\n",
      "      4      \u001b[36m362.8034\u001b[0m      \u001b[32m359.5863\u001b[0m  0.5386\n",
      "      5       \u001b[36m63.8555\u001b[0m       62.3019  0.4488\n",
      "      6       28.5739       31.7136  0.3899\n",
      "      5      \u001b[36m162.8406\u001b[0m      190.9465  0.4600\n",
      "      6       41.8181       \u001b[32m43.9311\u001b[0m  0.3713\n",
      "      5      239.2288      257.7628  0.4891\n",
      "      6       \u001b[36m58.1005\u001b[0m       57.8759  0.4375\n",
      "      5      \u001b[36m369.1351\u001b[0m      \u001b[32m368.3363\u001b[0m  0.5256\n",
      "      6       65.0546       \u001b[32m62.4996\u001b[0m  0.4613\n",
      "      5      \u001b[36m330.2244\u001b[0m      330.5440  0.5174\n",
      "      5      \u001b[36m362.0363\u001b[0m      \u001b[32m359.5056\u001b[0m  0.5318\n",
      "      6       \u001b[36m63.8248\u001b[0m       61.7121  0.4278\n",
      "      7       28.4697       31.6927  0.4030\n",
      "      7       \u001b[36m41.3831\u001b[0m       \u001b[32m43.8287\u001b[0m  0.4027\n",
      "      6      163.0256      193.3833  0.4864\n",
      "      6      \u001b[36m236.0352\u001b[0m      257.4482  0.4739\n",
      "      7       \u001b[36m64.4099\u001b[0m       \u001b[32m62.4317\u001b[0m  0.4244\n",
      "      7       \u001b[36m57.8452\u001b[0m       60.8804  0.4934\n",
      "      6      \u001b[36m369.1263\u001b[0m      \u001b[32m366.3280\u001b[0m  0.5025\n",
      "      6      \u001b[36m328.7079\u001b[0m      330.0775  0.5047\n",
      "      7       \u001b[36m63.4247\u001b[0m       63.1039  0.4259\n",
      "      6      \u001b[36m360.2612\u001b[0m      \u001b[32m359.3245\u001b[0m  0.5208\n",
      "      8       \u001b[36m28.2164\u001b[0m       31.8099  0.4297\n",
      "      8       41.5190       44.0992  0.4173\n",
      "      7      162.9553      192.2681  0.4766\n",
      "      8       \u001b[36m64.3457\u001b[0m       62.6575  0.4285\n",
      "      7      238.0582      \u001b[32m257.3406\u001b[0m  0.4879\n",
      "      8       58.1225       \u001b[32m56.8814\u001b[0m  0.4296\n",
      "      7      \u001b[36m367.0659\u001b[0m      366.4618  0.5173\n",
      "      8       \u001b[36m63.2517\u001b[0m       62.2953  0.4320\n",
      "      7      328.9912      \u001b[32m329.2418\u001b[0m  0.5178\n",
      "      9       28.2856       31.9040  0.4165\n",
      "      9       41.4557       44.2502  0.3948\n",
      "      7      361.1514      \u001b[32m358.9469\u001b[0m  0.5229\n",
      "      9       64.6143       62.5563  0.4528\n",
      "      8      \u001b[36m161.3495\u001b[0m      191.5831  0.4774\n",
      "      9       \u001b[36m57.8398\u001b[0m       57.4909  0.4338\n",
      "      8      236.5143      \u001b[32m257.1311\u001b[0m  0.4852\n",
      "      9       63.3187       61.8800  0.4313\n",
      "     10       \u001b[36m28.0065\u001b[0m       31.8413  0.3832\n",
      "Restoring best model from epoch 3.\n",
      "     10       41.4070       \u001b[32m43.7842\u001b[0m  0.3992\n",
      "      8      \u001b[36m328.2928\u001b[0m      \u001b[32m328.9828\u001b[0m  0.4919\n",
      "      8      367.5322      368.0592  0.5531\n",
      "      8      \u001b[36m359.8839\u001b[0m      359.1594  0.5195\n",
      "     10       \u001b[36m64.0057\u001b[0m       62.6399  0.4210\n",
      "Restoring best model from epoch 7.\n",
      "     10       \u001b[36m57.7661\u001b[0m       57.5329  0.4143\n",
      "Restoring best model from epoch 8.\n",
      "      9      \u001b[36m159.4100\u001b[0m      191.2471  0.4405\n",
      "      9      236.0653      259.3084  0.4543\n",
      "     10       \u001b[36m63.0163\u001b[0m       61.9235  0.3822\n",
      "Restoring best model from epoch 2.\n",
      "      9      \u001b[36m327.6241\u001b[0m      329.4564  0.4313\n",
      "      9      \u001b[36m365.8389\u001b[0m      367.1312  0.4385\n",
      "      9      \u001b[36m358.4868\u001b[0m      360.2402  0.4418\n",
      "     10      159.5934      191.3320  0.4076\n",
      "Restoring best model from epoch 2.\n",
      "     10      \u001b[36m235.9941\u001b[0m      261.4053  0.4223\n",
      "Restoring best model from epoch 8.\n",
      "     10      \u001b[36m327.0887\u001b[0m      \u001b[32m328.1744\u001b[0m  0.4296\n",
      "     10      \u001b[36m365.3584\u001b[0m      367.3293  0.4340\n",
      "Restoring best model from epoch 6.\n",
      "     10      358.9248      359.8883  0.4322\n",
      "Restoring best model from epoch 7.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m309.9135\u001b[0m      \u001b[32m300.4202\u001b[0m  0.4337\n",
      "      2      \u001b[36m293.8736\u001b[0m      \u001b[32m295.1738\u001b[0m  0.4327\n",
      "      3      \u001b[36m290.6635\u001b[0m      \u001b[32m294.2631\u001b[0m  0.4305\n",
      "      4      290.7152      \u001b[32m293.9146\u001b[0m  0.4387\n",
      "      5      \u001b[36m289.9373\u001b[0m      294.4200  0.4271\n",
      "      6      \u001b[36m289.8165\u001b[0m      295.0582  0.4377\n",
      "      7      \u001b[36m289.0642\u001b[0m      295.5177  0.4374\n",
      "      8      \u001b[36m288.7175\u001b[0m      295.9323  0.4307\n",
      "      9      288.7273      295.7652  0.4371\n",
      "     10      \u001b[36m288.3531\u001b[0m      297.3809  0.4305\n",
      "Restoring best model from epoch 4.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/pycox/evaluation/eval_surv.py:36: FutureWarning: is_monotonic is deprecated and will be removed in a future version. Use is_monotonic_increasing instead.\n",
      "  assert pd.Series(self.index_surv).is_monotonic\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concordance Index 0.8068622647283072\n",
      "Integrated Brier Score: 0.0975460191416738\n",
      "durations 1.0 5171.0\n",
      "Concordance Index 0.778009361243848\n",
      "Integrated Brier Score: 0.10135364249261401\n",
      "(6297, 9) <class 'pandas.core.frame.DataFrame'>\n",
      "(6297,) <class 'pandas.core.series.Series'>\n",
      "(1574, 9) <class 'pandas.core.frame.DataFrame'>\n",
      "(1574,) <class 'pandas.core.series.Series'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/pycox/evaluation/eval_surv.py:36: FutureWarning: is_monotonic is deprecated and will be removed in a future version. Use is_monotonic_increasing instead.\n",
      "  assert pd.Series(self.index_surv).is_monotonic\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m31.2599\u001b[0m       \u001b[32m30.1748\u001b[0m  0.3788\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m45.8949\u001b[0m       \u001b[32m43.6916\u001b[0m  0.3961\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m61.8435\u001b[0m       \u001b[32m56.2228\u001b[0m  0.4015\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m66.8608\u001b[0m       \u001b[32m63.2777\u001b[0m  0.4277\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m67.9818\u001b[0m       \u001b[32m63.2852\u001b[0m  0.4353\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m279.1763\u001b[0m      \u001b[32m275.9826\u001b[0m  0.4683\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m191.4240\u001b[0m      \u001b[32m192.0654\u001b[0m  0.4870\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m396.9717\u001b[0m      \u001b[32m377.2777\u001b[0m  0.5452\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m373.1565\u001b[0m      \u001b[32m339.4610\u001b[0m  0.5623\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m416.8964\u001b[0m      \u001b[32m406.4926\u001b[0m  0.5729\n",
      "      2       \u001b[36m43.8200\u001b[0m       \u001b[32m43.5467\u001b[0m  0.4073\n",
      "      2       \u001b[36m30.3955\u001b[0m       \u001b[32m30.1186\u001b[0m  0.4323\n",
      "      2       \u001b[36m59.5746\u001b[0m       56.5434  0.4328\n",
      "      2       \u001b[36m66.0989\u001b[0m       \u001b[32m62.6083\u001b[0m  0.4367\n",
      "      2       \u001b[36m65.0331\u001b[0m       \u001b[32m62.8718\u001b[0m  0.4515\n",
      "      2      \u001b[36m255.6605\u001b[0m      \u001b[32m267.0960\u001b[0m  0.4834\n",
      "      2      \u001b[36m170.6897\u001b[0m      \u001b[32m182.9610\u001b[0m  0.5258\n",
      "      2      \u001b[36m371.5102\u001b[0m      \u001b[32m368.9429\u001b[0m  0.4904\n",
      "      2      \u001b[36m342.1036\u001b[0m      \u001b[32m336.9537\u001b[0m  0.5304\n",
      "      3       \u001b[36m29.7245\u001b[0m       30.2413  0.3595\n",
      "      2      \u001b[36m379.7927\u001b[0m      \u001b[32m380.2768\u001b[0m  0.5602\n",
      "      3       \u001b[36m59.1504\u001b[0m       \u001b[32m56.2219\u001b[0m  0.4115\n",
      "      3       \u001b[36m43.4416\u001b[0m       44.1828  0.4559\n",
      "      3       \u001b[36m64.4183\u001b[0m       62.8914  0.4005\n",
      "      3       \u001b[36m65.4587\u001b[0m       62.8780  0.4069\n",
      "      3      \u001b[36m250.2584\u001b[0m      275.4698  0.4954\n",
      "      3      \u001b[36m169.3479\u001b[0m      \u001b[32m174.0077\u001b[0m  0.4816\n",
      "      4       \u001b[36m29.1391\u001b[0m       30.6756  0.4121\n",
      "      3      \u001b[36m369.2589\u001b[0m      \u001b[32m368.8871\u001b[0m  0.4972\n",
      "      3      \u001b[36m336.9807\u001b[0m      \u001b[32m330.9753\u001b[0m  0.4894\n",
      "      4       \u001b[36m58.7695\u001b[0m       56.6841  0.4566\n",
      "      4       \u001b[36m42.7426\u001b[0m       43.8466  0.4545\n",
      "      4       \u001b[36m65.0606\u001b[0m       62.8994  0.4248\n",
      "      4       \u001b[36m64.1519\u001b[0m       63.0047  0.4395\n",
      "      3      \u001b[36m379.5740\u001b[0m      \u001b[32m371.9207\u001b[0m  0.5405\n",
      "      4      \u001b[36m249.1947\u001b[0m      \u001b[32m254.3699\u001b[0m  0.4634\n",
      "      4      \u001b[36m168.0318\u001b[0m      \u001b[32m171.4071\u001b[0m  0.4679\n",
      "      5       \u001b[36m29.1348\u001b[0m       30.2902  0.4209\n",
      "      4      \u001b[36m365.9594\u001b[0m      \u001b[32m365.1511\u001b[0m  0.5160\n",
      "      5       58.7825       56.4433  0.4265\n",
      "      5       42.9829       43.9148  0.4224\n",
      "      5       \u001b[36m63.9761\u001b[0m       \u001b[32m62.8504\u001b[0m  0.4069\n",
      "      4      \u001b[36m333.6577\u001b[0m      \u001b[32m329.9373\u001b[0m  0.5373\n",
      "      5       65.0861       62.7015  0.4534\n",
      "      4      \u001b[36m374.2340\u001b[0m      \u001b[32m362.6467\u001b[0m  0.5226\n",
      "      6       29.1946       30.9720  0.3975\n",
      "      5      \u001b[36m244.9854\u001b[0m      \u001b[32m254.2410\u001b[0m  0.4996\n",
      "      5      \u001b[36m166.9928\u001b[0m      172.6612  0.4620\n",
      "      6       42.8948       \u001b[32m43.3881\u001b[0m  0.4278\n",
      "      6       \u001b[36m63.6801\u001b[0m       63.1491  0.4430\n",
      "      6       \u001b[36m58.6036\u001b[0m       56.3340  0.4571\n",
      "      5      366.0531      365.2537  0.4892\n",
      "      5      334.2685      \u001b[32m325.8066\u001b[0m  0.4632\n",
      "      6       \u001b[36m64.3073\u001b[0m       63.0061  0.4615\n",
      "      7       \u001b[36m28.9276\u001b[0m       33.3112  0.3911\n",
      "      5      \u001b[36m371.2482\u001b[0m      \u001b[32m360.2185\u001b[0m  0.5192\n",
      "      6      \u001b[36m166.7840\u001b[0m      173.5476  0.4693\n",
      "      6      245.1183      255.7417  0.4887\n",
      "      7       \u001b[36m42.5198\u001b[0m       \u001b[32m43.3093\u001b[0m  0.4359\n",
      "      7       \u001b[36m58.1004\u001b[0m       56.5207  0.4255\n",
      "      7       \u001b[36m63.4862\u001b[0m       62.9708  0.4338\n",
      "      7       64.4757       \u001b[32m62.4674\u001b[0m  0.4613\n",
      "      6      \u001b[36m365.1582\u001b[0m      \u001b[32m363.6276\u001b[0m  0.5120\n",
      "      6      \u001b[36m333.3540\u001b[0m      \u001b[32m323.7472\u001b[0m  0.5252\n",
      "      8       29.1619       34.1371  0.3827\n",
      "      6      \u001b[36m370.4463\u001b[0m      \u001b[32m358.9300\u001b[0m  0.4958\n",
      "      7      245.4873      \u001b[32m253.5507\u001b[0m  0.4769\n",
      "      8       \u001b[36m57.8458\u001b[0m       57.1407  0.4156\n",
      "      7      \u001b[36m166.7668\u001b[0m      172.8868  0.5187\n",
      "      8       63.8057       63.0356  0.4239\n",
      "      8       42.5481       43.6882  0.4483\n",
      "      8       64.3512       63.2109  0.4402\n",
      "      9       \u001b[36m28.4835\u001b[0m       31.4211  0.3800\n",
      "      7      \u001b[36m364.9873\u001b[0m      364.9471  0.5302\n",
      "      7      \u001b[36m332.0662\u001b[0m      \u001b[32m322.9694\u001b[0m  0.4983\n",
      "      7      370.7301      \u001b[32m358.7796\u001b[0m  0.5041\n",
      "      9       57.9481       56.7337  0.4350\n",
      "      9       42.8405       43.5922  0.4306\n",
      "      9       63.5858       62.8564  0.4485\n",
      "      8      245.9464      \u001b[32m251.0310\u001b[0m  0.4847\n",
      "      8      \u001b[36m165.3913\u001b[0m      172.7175  0.4763\n",
      "      9       64.4114       62.8630  0.4270\n",
      "     10       28.7413       30.6311  0.4207\n",
      "Restoring best model from epoch 2.\n",
      "      8      365.6258      364.2330  0.5204\n",
      "      8      \u001b[36m330.8155\u001b[0m      \u001b[32m322.7108\u001b[0m  0.5161\n",
      "     10       42.6777       45.9817  0.3841\n",
      "Restoring best model from epoch 7.\n",
      "      8      \u001b[36m368.3235\u001b[0m      \u001b[32m358.5848\u001b[0m  0.4830\n",
      "     10       58.1601       56.7221  0.4076\n",
      "Restoring best model from epoch 1.\n",
      "     10       63.7592       \u001b[32m62.7464\u001b[0m  0.4197\n",
      "      9      \u001b[36m164.9684\u001b[0m      173.9522  0.4498\n",
      "      9      \u001b[36m243.4840\u001b[0m      253.9266  0.4689\n",
      "     10       64.7376       \u001b[32m62.4336\u001b[0m  0.4155\n",
      "      9      \u001b[36m364.5313\u001b[0m      365.0214  0.4416\n",
      "      9      \u001b[36m329.5152\u001b[0m      322.7502  0.4446\n",
      "      9      \u001b[36m366.2846\u001b[0m      358.9438  0.4500\n",
      "     10      \u001b[36m163.9426\u001b[0m      175.4090  0.4061\n",
      "Restoring best model from epoch 4.\n",
      "     10      \u001b[36m242.2311\u001b[0m      \u001b[32m250.5702\u001b[0m  0.4193\n",
      "     10      \u001b[36m363.6095\u001b[0m      364.1526  0.4367\n",
      "Restoring best model from epoch 6.\n",
      "     10      \u001b[36m329.0088\u001b[0m      323.3116  0.4354\n",
      "Restoring best model from epoch 8.\n",
      "     10      366.2979      359.3749  0.4437\n",
      "Restoring best model from epoch 8.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m313.7142\u001b[0m      \u001b[32m307.1033\u001b[0m  0.4408\n",
      "      2      \u001b[36m294.0246\u001b[0m      308.4372  0.4367\n",
      "      3      \u001b[36m292.0125\u001b[0m      \u001b[32m303.8539\u001b[0m  0.4533\n",
      "      4      \u001b[36m291.2738\u001b[0m      \u001b[32m302.9989\u001b[0m  0.4381\n",
      "      5      \u001b[36m290.9008\u001b[0m      303.5154  0.4437\n",
      "      6      \u001b[36m290.7545\u001b[0m      303.2533  0.4466\n",
      "      7      \u001b[36m290.4052\u001b[0m      303.6803  0.4405\n",
      "      8      \u001b[36m289.8712\u001b[0m      304.5871  0.4385\n",
      "      9      290.3248      303.7450  0.4361\n",
      "     10      290.2684      307.1030  0.4265\n",
      "Restoring best model from epoch 4.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/pycox/evaluation/eval_surv.py:36: FutureWarning: is_monotonic is deprecated and will be removed in a future version. Use is_monotonic_increasing instead.\n",
      "  assert pd.Series(self.index_surv).is_monotonic\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concordance Index 0.7963675977542859\n",
      "Integrated Brier Score: 0.10133199504691294\n",
      "durations 1.0 5187.0\n",
      "Concordance Index 0.7944194432910247\n",
      "Integrated Brier Score: 0.09900474177011949\n",
      "load_rgbsg\n",
      "split horm_treatment      category\n",
      "menopause           category\n",
      "age                  float32\n",
      "n_positive_nodes     float32\n",
      "progesterone         float32\n",
      "estrogene            float32\n",
      "grade_0.0              uint8\n",
      "grade_1.0              uint8\n",
      "grade_2.0              uint8\n",
      "dtype: object\n",
      "(1785, 9) <class 'pandas.core.frame.DataFrame'>\n",
      "(1785,) <class 'pandas.core.series.Series'>\n",
      "(447, 9) <class 'pandas.core.frame.DataFrame'>\n",
      "(447,) <class 'pandas.core.series.Series'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/pycox/evaluation/eval_surv.py:36: FutureWarning: is_monotonic is deprecated and will be removed in a future version. Use is_monotonic_increasing instead.\n",
      "  assert pd.Series(self.index_surv).is_monotonic\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m106.4342\u001b[0m       \u001b[32m90.5647\u001b[0m  0.1168\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m112.1225\u001b[0m       \u001b[32m92.7967\u001b[0m  0.1223\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m118.9879\u001b[0m       \u001b[32m99.9257\u001b[0m  0.1300\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m134.5665\u001b[0m      \u001b[32m115.6042\u001b[0m  0.1326\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m153.2099\u001b[0m      \u001b[32m134.9823\u001b[0m  0.1351\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m715.1565\u001b[0m      \u001b[32m518.3607\u001b[0m  0.1405\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m704.9236\u001b[0m      \u001b[32m519.3843\u001b[0m  0.1460\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m750.1959\u001b[0m      \u001b[32m562.6589\u001b[0m  0.1472\n",
      "      2      \u001b[36m104.2098\u001b[0m       \u001b[32m90.3765\u001b[0m  0.1404\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m860.5891\u001b[0m      \u001b[32m641.0411\u001b[0m  0.1724\n",
      "      2      \u001b[36m108.3191\u001b[0m       \u001b[32m92.4982\u001b[0m  0.1442\n",
      "      2      \u001b[36m116.2320\u001b[0m       99.9505  0.1410\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m998.8260\u001b[0m      \u001b[32m741.4636\u001b[0m  0.1780\n",
      "      2      \u001b[36m131.9075\u001b[0m      \u001b[32m114.6385\u001b[0m  0.1348\n",
      "      2      \u001b[36m151.2569\u001b[0m      \u001b[32m133.5807\u001b[0m  0.1362\n",
      "      2      \u001b[36m588.0531\u001b[0m      527.9629  0.1443\n",
      "      2      \u001b[36m607.2696\u001b[0m      \u001b[32m513.9004\u001b[0m  0.1448\n",
      "      3      \u001b[36m102.3102\u001b[0m       \u001b[32m90.1782\u001b[0m  0.1289\n",
      "      2      \u001b[36m656.8117\u001b[0m      \u001b[32m548.7989\u001b[0m  0.1538\n",
      "      3      \u001b[36m107.4392\u001b[0m       \u001b[32m91.7108\u001b[0m  0.1358\n",
      "      2      \u001b[36m742.0646\u001b[0m      643.0014  0.1674\n",
      "      3      \u001b[36m129.9701\u001b[0m      \u001b[32m114.0084\u001b[0m  0.1377\n",
      "      3      \u001b[36m115.6188\u001b[0m       \u001b[32m98.6874\u001b[0m  0.1574\n",
      "      2      \u001b[36m865.8393\u001b[0m      754.7288  0.1625\n",
      "      3      \u001b[36m148.9870\u001b[0m      134.0353  0.1549\n",
      "      3      \u001b[36m577.0630\u001b[0m      519.0603  0.1408\n",
      "      3      \u001b[36m593.9269\u001b[0m      522.3608  0.1553\n",
      "      4      \u001b[36m102.2456\u001b[0m       \u001b[32m89.7384\u001b[0m  0.1287\n",
      "      3      \u001b[36m641.2261\u001b[0m      \u001b[32m546.7748\u001b[0m  0.1556\n",
      "      4      \u001b[36m106.3630\u001b[0m       \u001b[32m91.1781\u001b[0m  0.1298\n",
      "      4      \u001b[36m129.8911\u001b[0m      \u001b[32m113.5682\u001b[0m  0.1262\n",
      "      4      \u001b[36m115.3585\u001b[0m       \u001b[32m98.4086\u001b[0m  0.1399\n",
      "      3      \u001b[36m735.5970\u001b[0m      \u001b[32m634.7952\u001b[0m  0.1581\n",
      "      3      \u001b[36m845.5338\u001b[0m      746.0611  0.1498\n",
      "      4      \u001b[36m148.3705\u001b[0m      134.3258  0.1527\n",
      "      4      \u001b[36m572.3153\u001b[0m      \u001b[32m517.2429\u001b[0m  0.1504\n",
      "      5      \u001b[36m101.7106\u001b[0m       89.9458  0.1363\n",
      "      4      596.0481      515.1619  0.1495\n",
      "      5      106.3857       91.3461  0.1431\n",
      "      4      642.4518      \u001b[32m545.4523\u001b[0m  0.1506\n",
      "      5      \u001b[36m128.9549\u001b[0m      \u001b[32m113.4939\u001b[0m  0.1367\n",
      "      5      \u001b[36m114.6052\u001b[0m       \u001b[32m97.8204\u001b[0m  0.1390\n",
      "      5      \u001b[36m147.6997\u001b[0m      133.6983  0.1226\n",
      "      4      \u001b[36m728.4980\u001b[0m      637.9055  0.1555\n",
      "      6      102.2137       90.3776  0.1227\n",
      "      4      \u001b[36m834.7366\u001b[0m      \u001b[32m738.5124\u001b[0m  0.1732\n",
      "      5      \u001b[36m561.8658\u001b[0m      \u001b[32m513.6707\u001b[0m  0.1527\n",
      "      6      \u001b[36m106.1856\u001b[0m       \u001b[32m90.9187\u001b[0m  0.1191\n",
      "      5      \u001b[36m589.4102\u001b[0m      \u001b[32m510.8791\u001b[0m  0.1594\n",
      "      6      129.0866      113.9114  0.1399\n",
      "      6      \u001b[36m114.5198\u001b[0m       \u001b[32m97.7130\u001b[0m  0.1223\n",
      "      5      \u001b[36m638.7860\u001b[0m      549.5570  0.1566\n",
      "      6      \u001b[36m146.9033\u001b[0m      \u001b[32m133.3715\u001b[0m  0.1412\n",
      "      7      101.8523       90.0857  0.1191\n",
      "      5      \u001b[36m725.6603\u001b[0m      \u001b[32m631.7893\u001b[0m  0.1664\n",
      "      6      \u001b[36m560.4116\u001b[0m      \u001b[32m505.4116\u001b[0m  0.1374\n",
      "      5      \u001b[36m828.1634\u001b[0m      744.1397  0.1535\n",
      "      7      106.2639       \u001b[32m90.4669\u001b[0m  0.1452\n",
      "      6      \u001b[36m589.3601\u001b[0m      \u001b[32m508.3434\u001b[0m  0.1584\n",
      "      7      \u001b[36m112.9273\u001b[0m       \u001b[32m97.3612\u001b[0m  0.1316\n",
      "      7      129.2628      113.6900  0.1465\n",
      "      6      639.1096      \u001b[32m544.4517\u001b[0m  0.1505\n",
      "      8      102.2676       90.3612  0.1434\n",
      "      7      147.1135      \u001b[32m133.1682\u001b[0m  0.1504\n",
      "      7      560.4853      \u001b[32m502.4209\u001b[0m  0.1411\n",
      "      8      \u001b[36m106.0451\u001b[0m       \u001b[32m90.4040\u001b[0m  0.1261\n",
      "      6      \u001b[36m724.6991\u001b[0m      632.3344  0.1705\n",
      "      8      113.6184       97.8109  0.1268\n",
      "      8      129.2462      113.7247  0.1262\n",
      "      6      \u001b[36m824.3513\u001b[0m      \u001b[32m734.6999\u001b[0m  0.1812\n",
      "      7      \u001b[36m587.5401\u001b[0m      \u001b[32m508.1228\u001b[0m  0.1470\n",
      "      7      \u001b[36m632.8711\u001b[0m      546.0827  0.1484\n",
      "      8      147.7027      \u001b[32m133.0610\u001b[0m  0.1226\n",
      "      9      \u001b[36m101.1220\u001b[0m       \u001b[32m89.6714\u001b[0m  0.1258\n",
      "      8      561.0607      505.9759  0.1591\n",
      "      9      \u001b[36m106.0024\u001b[0m       90.6978  0.1504\n",
      "      9      113.7432       \u001b[32m97.2061\u001b[0m  0.1301\n",
      "      7      726.5342      635.1590  0.1589\n",
      "      9      129.4778      \u001b[32m113.3022\u001b[0m  0.1362\n",
      "      8      \u001b[36m585.6198\u001b[0m      \u001b[32m507.7682\u001b[0m  0.1319\n",
      "      7      \u001b[36m821.7398\u001b[0m      \u001b[32m731.0863\u001b[0m  0.1599\n",
      "     10      102.7627       90.3225  0.1342\n",
      "Restoring best model from epoch 9.\n",
      "      9      147.0924      \u001b[32m132.9809\u001b[0m  0.1384\n",
      "      8      633.4152      545.0027  0.1515\n",
      "     10      113.6901       97.7761  0.1297\n",
      "     10      \u001b[36m105.3716\u001b[0m       90.5207  0.1358\n",
      "Restoring best model from epoch 8.\n",
      "Restoring best model from epoch 9.\n",
      "      9      \u001b[36m558.2128\u001b[0m      503.3467  0.1383\n",
      "      9      \u001b[36m582.4708\u001b[0m      508.4813  0.1354\n",
      "     10      \u001b[36m128.7216\u001b[0m      \u001b[32m113.2626\u001b[0m  0.1478\n",
      "      8      \u001b[36m721.8391\u001b[0m      632.6035  0.1630\n",
      "      8      \u001b[36m821.1470\u001b[0m      735.2893  0.1416\n",
      "     10      147.0742      \u001b[32m132.8815\u001b[0m  0.1313\n",
      "      9      \u001b[36m632.0892\u001b[0m      546.6730  0.1347\n",
      "     10      \u001b[36m556.9808\u001b[0m      506.2683  0.1252\n",
      "Restoring best model from epoch 7.\n",
      "     10      \u001b[36m580.9139\u001b[0m      509.7592  0.1256\n",
      "Restoring best model from epoch 8.\n",
      "      9      \u001b[36m719.0514\u001b[0m      \u001b[32m631.6771\u001b[0m  0.1326\n",
      "      9      \u001b[36m818.4225\u001b[0m      735.8756  0.1357\n",
      "     10      634.6941      546.4874  0.1271\n",
      "Restoring best model from epoch 6.\n",
      "     10      \u001b[36m718.2398\u001b[0m      \u001b[32m631.0065\u001b[0m  0.1323\n",
      "     10      819.9530      734.4933  0.1351\n",
      "Restoring best model from epoch 7.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m797.7921\u001b[0m      \u001b[32m518.9055\u001b[0m  0.1326\n",
      "      2      \u001b[36m696.3282\u001b[0m      \u001b[32m511.9804\u001b[0m  0.1355\n",
      "      3      \u001b[36m665.4601\u001b[0m      \u001b[32m510.3550\u001b[0m  0.1346\n",
      "      4      669.3626      520.4833  0.1330\n",
      "      5      668.1676      511.4496  0.1308\n",
      "      6      665.8647      513.4866  0.1325\n",
      "      7      \u001b[36m660.5080\u001b[0m      511.8023  0.1339\n",
      "      8      662.9616      \u001b[32m508.9971\u001b[0m  0.1352\n",
      "      9      663.6325      \u001b[32m508.2854\u001b[0m  0.1339\n",
      "     10      663.6637      509.0119  0.1309\n",
      "Restoring best model from epoch 9.\n",
      "Concordance Index 0.6598732874026587\n",
      "Integrated Brier Score: 0.18093886434741302\n",
      "durations 0.52566737 87.359344\n",
      "Concordance Index 0.6558073654390935\n",
      "Integrated Brier Score: 0.17870653920567464\n",
      "(1785, 9) <class 'pandas.core.frame.DataFrame'>\n",
      "(1785,) <class 'pandas.core.series.Series'>\n",
      "(447, 9) <class 'pandas.core.frame.DataFrame'>\n",
      "(447,) <class 'pandas.core.series.Series'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/pycox/evaluation/eval_surv.py:36: FutureWarning: is_monotonic is deprecated and will be removed in a future version. Use is_monotonic_increasing instead.\n",
      "  assert pd.Series(self.index_surv).is_monotonic\n",
      "/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/pycox/evaluation/eval_surv.py:36: FutureWarning: is_monotonic is deprecated and will be removed in a future version. Use is_monotonic_increasing instead.\n",
      "  assert pd.Series(self.index_surv).is_monotonic\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m103.7590\u001b[0m       \u001b[32m92.6279\u001b[0m  0.0787\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m107.6924\u001b[0m      \u001b[32m101.0999\u001b[0m  0.0808\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m115.9881\u001b[0m      \u001b[32m104.0756\u001b[0m  0.0922\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m131.2614\u001b[0m      \u001b[32m118.7590\u001b[0m  0.0918\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m149.4000\u001b[0m      \u001b[32m141.0423\u001b[0m  0.0957\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m257.8528\u001b[0m      \u001b[32m217.2874\u001b[0m  0.0972\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m264.3420\u001b[0m      \u001b[32m239.0679\u001b[0m  0.0933\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m287.2556\u001b[0m      \u001b[32m246.3005\u001b[0m  0.0887\n",
      "      2       \u001b[36m99.7351\u001b[0m       93.0237  0.0922\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m325.1513\u001b[0m      \u001b[32m286.3325\u001b[0m  0.1037\n",
      "      2      \u001b[36m111.6437\u001b[0m      \u001b[32m102.8038\u001b[0m  0.0878\n",
      "      2      \u001b[36m102.9188\u001b[0m      \u001b[32m100.0655\u001b[0m  0.1106\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m374.7183\u001b[0m      \u001b[32m340.2484\u001b[0m  0.1230\n",
      "      2      \u001b[36m126.2144\u001b[0m      \u001b[32m118.0454\u001b[0m  0.1012\n",
      "      2      \u001b[36m144.3543\u001b[0m      \u001b[32m140.5011\u001b[0m  0.0942\n",
      "      2      \u001b[36m251.0406\u001b[0m      \u001b[32m234.3392\u001b[0m  0.0897\n",
      "      2      \u001b[36m248.8355\u001b[0m      \u001b[32m216.9350\u001b[0m  0.0994\n",
      "      2      \u001b[36m273.8278\u001b[0m      \u001b[32m239.8557\u001b[0m  0.0966\n",
      "      3       \u001b[36m99.1013\u001b[0m       \u001b[32m91.6906\u001b[0m  0.0914\n",
      "      3      \u001b[36m101.8063\u001b[0m       \u001b[32m99.2486\u001b[0m  0.0843\n",
      "      2      \u001b[36m311.8859\u001b[0m      \u001b[32m276.6615\u001b[0m  0.1097\n",
      "      3      \u001b[36m110.4116\u001b[0m      \u001b[32m101.7553\u001b[0m  0.0980\n",
      "      3      \u001b[36m248.8182\u001b[0m      236.6460  0.0821\n",
      "      2      \u001b[36m357.3509\u001b[0m      \u001b[32m338.2407\u001b[0m  0.1070      3      \u001b[36m143.5608\u001b[0m      \u001b[32m140.3337\u001b[0m  0.0965\n",
      "\n",
      "      3      274.0782      242.5135  0.0866\n",
      "      3      \u001b[36m125.2552\u001b[0m      \u001b[32m117.1558\u001b[0m  0.1086\n",
      "      3      \u001b[36m241.8035\u001b[0m      \u001b[32m216.5996\u001b[0m  0.0971\n",
      "      4      \u001b[36m101.0175\u001b[0m       \u001b[32m99.2380\u001b[0m  0.0809\n",
      "      4       \u001b[36m98.3790\u001b[0m       91.7624  0.1030\n",
      "      4      \u001b[36m109.1071\u001b[0m      \u001b[32m101.6863\u001b[0m  0.0828\n",
      "      4      \u001b[36m246.1447\u001b[0m      234.6573  0.0919\n",
      "      3      \u001b[36m307.1298\u001b[0m      283.2387  0.1216\n",
      "      4      \u001b[36m142.8459\u001b[0m      \u001b[32m140.2654\u001b[0m  0.1067\n",
      "      4      \u001b[36m267.2407\u001b[0m      240.5886  0.1030\n",
      "      4      \u001b[36m241.7762\u001b[0m      \u001b[32m215.7309\u001b[0m  0.1018\n",
      "      3      \u001b[36m350.9711\u001b[0m      \u001b[32m335.6297\u001b[0m  0.1187\n",
      "      4      \u001b[36m124.4782\u001b[0m      117.5635  0.1247\n",
      "      5       \u001b[36m98.3377\u001b[0m       91.7885  0.0929\n",
      "      5      \u001b[36m100.9601\u001b[0m       99.2987  0.1041\n",
      "      5      109.1347      \u001b[32m101.4227\u001b[0m  0.1159\n",
      "      5      246.2774      \u001b[32m234.2617\u001b[0m  0.0997\n",
      "      5      \u001b[36m240.6021\u001b[0m      216.8762  0.0855\n",
      "      5      268.4277      \u001b[32m238.5457\u001b[0m  0.1041\n",
      "      4      \u001b[36m303.4582\u001b[0m      277.5466  0.1163\n",
      "      4      \u001b[36m349.4457\u001b[0m      335.6485  0.1073\n",
      "      6       \u001b[36m97.8963\u001b[0m       \u001b[32m91.5774\u001b[0m  0.0848\n",
      "      5      \u001b[36m124.2105\u001b[0m      \u001b[32m117.1264\u001b[0m  0.1055\n",
      "      5      \u001b[36m142.3827\u001b[0m      \u001b[32m139.8210\u001b[0m  0.1351\n",
      "      6      101.0232       99.2496  0.1061\n",
      "      6      \u001b[36m244.3246\u001b[0m      235.8372  0.1041\n",
      "      6      109.3716      101.8182  0.1163\n",
      "      6      \u001b[36m240.5753\u001b[0m      \u001b[32m215.6208\u001b[0m  0.0954\n",
      "      6      267.5762      239.5242  0.0832\n",
      "      7       \u001b[36m97.8478\u001b[0m       92.4729  0.0840\n",
      "      5      \u001b[36m348.0111\u001b[0m      \u001b[32m335.0862\u001b[0m  0.0970\n",
      "      5      303.8854      278.4716  0.1133\n",
      "      6      \u001b[36m142.3229\u001b[0m      140.1351  0.1073\n",
      "      7      \u001b[36m243.8765\u001b[0m      235.9082  0.0904\n",
      "      7      \u001b[36m265.4234\u001b[0m      240.7581  0.0788\n",
      "      6      124.4980      117.4559  0.1363\n",
      "      7      \u001b[36m240.1341\u001b[0m      216.1601  0.0868\n",
      "      7      109.7676      101.9875  0.0911\n",
      "      7      101.2412       99.2941  0.1297\n",
      "      8       98.5338       91.8338  0.0762\n",
      "      6      349.6448      \u001b[32m334.0329\u001b[0m  0.0949\n",
      "      7      \u001b[36m142.0824\u001b[0m      140.1091  0.0970\n",
      "      8      244.5913      236.5147  0.0912\n",
      "      8      \u001b[36m238.6890\u001b[0m      216.6985  0.0852\n",
      "      8      109.8244      101.4282  0.0971\n",
      "      8      266.1900      240.2750  0.1156\n",
      "      9       98.0470       \u001b[32m91.3960\u001b[0m  0.0938\n",
      "      7      348.2960      335.3578  0.0999\n",
      "      6      \u001b[36m302.1228\u001b[0m      277.2026  0.2018\n",
      "      8      \u001b[36m100.8943\u001b[0m       99.4375  0.1438\n",
      "      8      143.2942      140.1783  0.1127\n",
      "      9      239.3987      220.1779  0.1006\n",
      "      9      245.8407      235.3035  0.1132\n",
      "      9      265.4633      239.5042  0.1014\n",
      "      7      124.3351      117.5883  0.2157\n",
      "      9      109.3019      101.9172  0.1163\n",
      "     10       97.8816       91.4910  0.1152\n",
      "Restoring best model from epoch 9.\n",
      "      8      \u001b[36m347.7168\u001b[0m      334.0737  0.1256\n",
      "      9      \u001b[36m100.2961\u001b[0m       99.3201  0.1184\n",
      "      7      303.2464      278.0146  0.1339\n",
      "     10      239.7217      216.0391  0.0961\n",
      "Restoring best model from epoch 6.\n",
      "      9      142.8472      \u001b[32m139.6047\u001b[0m  0.1073\n",
      "     10      \u001b[36m243.2841\u001b[0m      251.4763  0.0998\n",
      "Restoring best model from epoch 5.\n",
      "     10      266.5905      242.8836  0.0991\n",
      "Restoring best model from epoch 5.\n",
      "      8      124.5653      \u001b[32m116.9795\u001b[0m  0.1074\n",
      "     10      \u001b[36m109.1003\u001b[0m      \u001b[32m101.3461\u001b[0m  0.1023\n",
      "      9      \u001b[36m347.3043\u001b[0m      334.2330  0.0940\n",
      "     10      100.8963       \u001b[32m99.2033\u001b[0m  0.0827\n",
      "      8      \u001b[36m301.9903\u001b[0m      277.8863  0.0901\n",
      "     10      142.9337      140.1810  0.0972\n",
      "Restoring best model from epoch 9.\n",
      "      9      124.6395      117.2581  0.0868\n",
      "     10      347.8266      334.0553  0.0913\n",
      "Restoring best model from epoch 6.\n",
      "      9      \u001b[36m300.3028\u001b[0m      278.1660  0.0876\n",
      "     10      124.6345      117.4519  0.0853\n",
      "Restoring best model from epoch 8.\n",
      "     10      301.2171      278.0634  0.0894\n",
      "Restoring best model from epoch 2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:778: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 767, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/sklearn/metrics/_scorer.py\", line 234, in __call__\n",
      "    return self._score(\n",
      "  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/sklearn/metrics/_scorer.py\", line 282, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)\n",
      "  File \"/var/folders/jr/dh6mkdzs31lc5pkqymtdbh180000gp/T/ipykernel_83010/1822012356.py\", line 14, in custom_scoring_function\n",
      "AttributeError: 'int' object has no attribute 'numpy'\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:778: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 767, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/sklearn/metrics/_scorer.py\", line 234, in __call__\n",
      "    return self._score(\n",
      "  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/sklearn/metrics/_scorer.py\", line 282, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)\n",
      "  File \"/var/folders/jr/dh6mkdzs31lc5pkqymtdbh180000gp/T/ipykernel_83010/1822012356.py\", line 14, in custom_scoring_function\n",
      "AttributeError: 'int' object has no attribute 'numpy'\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [nan nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m119.0548\u001b[0m      \u001b[32m109.9539\u001b[0m  0.1150\n",
      "      2      \u001b[36m116.2713\u001b[0m      \u001b[32m109.0147\u001b[0m  0.1072\n",
      "      3      \u001b[36m115.5427\u001b[0m      \u001b[32m108.7427\u001b[0m  0.1109\n",
      "      4      \u001b[36m115.1563\u001b[0m      108.7938  0.1062\n",
      "      5      \u001b[36m114.9808\u001b[0m      \u001b[32m108.6775\u001b[0m  0.1068\n",
      "      6      115.3082      108.7091  0.1057\n",
      "      7      \u001b[36m114.6407\u001b[0m      108.6842  0.1058\n",
      "      8      114.7449      108.6929  0.1063\n",
      "      9      115.1023      108.8166  0.1059\n",
      "     10      114.8224      \u001b[32m108.4576\u001b[0m  0.1037\n",
      "Concordance Index 0.6724687505494662\n",
      "Integrated Brier Score: 0.17672688701514408\n",
      "durations 1.87269 84.0\n",
      "Concordance Index 0.6529077184775675\n",
      "Integrated Brier Score: 0.18988832939225028\n",
      "(1786, 9) <class 'pandas.core.frame.DataFrame'>\n",
      "(1786,) <class 'pandas.core.series.Series'>\n",
      "(446, 9) <class 'pandas.core.frame.DataFrame'>\n",
      "(446,) <class 'pandas.core.series.Series'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/pycox/evaluation/eval_surv.py:36: FutureWarning: is_monotonic is deprecated and will be removed in a future version. Use is_monotonic_increasing instead.\n",
      "  assert pd.Series(self.index_surv).is_monotonic\n",
      "/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/pycox/evaluation/eval_surv.py:36: FutureWarning: is_monotonic is deprecated and will be removed in a future version. Use is_monotonic_increasing instead.\n",
      "  assert pd.Series(self.index_surv).is_monotonic\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m104.8007\u001b[0m       \u001b[32m93.2880\u001b[0m  0.0881\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m106.6483\u001b[0m       \u001b[32m95.9274\u001b[0m  0.0881\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m116.8657\u001b[0m      \u001b[32m104.2338\u001b[0m  0.0884\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m258.2902\u001b[0m      \u001b[32m220.0420\u001b[0m  0.0809\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m130.6302\u001b[0m      \u001b[32m120.1868\u001b[0m  0.0959\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m258.5566\u001b[0m      \u001b[32m223.8082\u001b[0m  0.1022\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m278.9034\u001b[0m      \u001b[32m238.8795\u001b[0m  0.0979\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m151.2348\u001b[0m      \u001b[32m136.7139\u001b[0m  0.1168\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m317.1722\u001b[0m      \u001b[32m279.9393\u001b[0m  0.1060\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m370.3075\u001b[0m      \u001b[32m319.6974\u001b[0m  0.1070\n",
      "      2      \u001b[36m101.0448\u001b[0m       \u001b[32m92.3343\u001b[0m  0.0871\n",
      "      2      \u001b[36m111.9119\u001b[0m      \u001b[32m103.4954\u001b[0m  0.0930\n",
      "      2      \u001b[36m102.1470\u001b[0m       \u001b[32m95.6545\u001b[0m  0.0988\n",
      "      2      \u001b[36m244.3882\u001b[0m      \u001b[32m216.7233\u001b[0m  0.0890\n",
      "      2      \u001b[36m126.5912\u001b[0m      120.4395  0.1009\n",
      "      2      \u001b[36m271.9922\u001b[0m      239.8751  0.0970\n",
      "      2      \u001b[36m248.0997\u001b[0m      \u001b[32m219.9457\u001b[0m  0.0995\n",
      "      2      \u001b[36m310.5337\u001b[0m      \u001b[32m279.1580\u001b[0m  0.1002\n",
      "      2      \u001b[36m147.6949\u001b[0m      \u001b[32m136.3953\u001b[0m  0.1177\n",
      "      3       \u001b[36m99.5019\u001b[0m       93.1814  0.0870\n",
      "      2      \u001b[36m364.7565\u001b[0m      319.7798  0.1026\n",
      "      3      \u001b[36m242.4863\u001b[0m      \u001b[32m216.1011\u001b[0m  0.0827\n",
      "      3      102.7002       \u001b[32m95.2094\u001b[0m  0.0860\n",
      "      3      \u001b[36m111.8656\u001b[0m      \u001b[32m103.3624\u001b[0m  0.1005\n",
      "      3      \u001b[36m267.9510\u001b[0m      \u001b[32m237.2956\u001b[0m  0.0884\n",
      "      3      \u001b[36m126.0168\u001b[0m      \u001b[32m119.8837\u001b[0m  0.1068\n",
      "      3      248.3800      220.8211  0.1151\n",
      "      3      \u001b[36m304.2912\u001b[0m      \u001b[32m278.6592\u001b[0m  0.0982\n",
      "      3      \u001b[36m146.5221\u001b[0m      \u001b[32m136.3002\u001b[0m  0.1014\n",
      "      4       \u001b[36m99.2174\u001b[0m       93.4198  0.0925\n",
      "      4      \u001b[36m101.7064\u001b[0m       95.2332  0.0858\n",
      "      3      \u001b[36m354.7000\u001b[0m      320.3893  0.1059\n",
      "      4      243.2270      217.8583  0.1009\n",
      "      4      \u001b[36m111.3242\u001b[0m      \u001b[32m102.3337\u001b[0m  0.0851\n",
      "      4      \u001b[36m263.7787\u001b[0m      237.6034  0.0845\n",
      "      4      \u001b[36m125.5668\u001b[0m      \u001b[32m119.6276\u001b[0m  0.1129\n",
      "      4      \u001b[36m242.3911\u001b[0m      \u001b[32m218.8697\u001b[0m  0.0872\n",
      "      4      \u001b[36m301.5399\u001b[0m      280.4831  0.0914\n",
      "      5       99.2297       94.0657  0.0975\n",
      "      5      \u001b[36m100.7457\u001b[0m       \u001b[32m94.7896\u001b[0m  0.0905\n",
      "      5      \u001b[36m241.8683\u001b[0m      217.4035  0.0811\n",
      "      4      \u001b[36m146.2642\u001b[0m      \u001b[32m135.8771\u001b[0m  0.1076\n",
      "      4      357.7806      320.2660  0.1043\n",
      "      5      \u001b[36m110.4374\u001b[0m      102.7636  0.1032\n",
      "      5      265.5008      237.9569  0.0863\n",
      "      5      242.7940      219.0783  0.1000\n",
      "      6      \u001b[36m239.2274\u001b[0m      216.2547  0.0842\n",
      "      5      \u001b[36m301.4889\u001b[0m      \u001b[32m278.0990\u001b[0m  0.1059\n",
      "      6       \u001b[36m98.3404\u001b[0m       94.2713  0.0929\n",
      "      6      \u001b[36m100.4400\u001b[0m       95.0073  0.0909\n",
      "      5      \u001b[36m125.3195\u001b[0m      \u001b[32m119.5688\u001b[0m  0.1163\n",
      "      5      \u001b[36m145.8822\u001b[0m      136.0450  0.1036\n",
      "      6      \u001b[36m110.1024\u001b[0m      102.6918  0.0914\n",
      "      5      \u001b[36m353.1703\u001b[0m      \u001b[32m318.4344\u001b[0m  0.1022\n",
      "      6      264.5370      237.4237  0.0967\n",
      "      6      243.5994      219.1717  0.0863\n",
      "      7       98.8605       94.4044  0.0833\n",
      "      7      239.9181      216.7283  0.0952\n",
      "      7      \u001b[36m100.4400\u001b[0m       95.3462  0.0919\n",
      "      6      301.4963      \u001b[32m277.8027\u001b[0m  0.0985\n",
      "      6      \u001b[36m124.6031\u001b[0m      119.6360  0.1114\n",
      "      7      110.2082      102.6665  0.0958\n",
      "      7      \u001b[36m262.8249\u001b[0m      238.0005  0.0856\n",
      "      6      \u001b[36m352.5995\u001b[0m      319.9934  0.0971\n",
      "      6      \u001b[36m145.3840\u001b[0m      135.9738  0.1242\n",
      "      7      242.6497      219.1537  0.0856\n",
      "      8      101.1579       95.6219  0.0843\n",
      "      8      \u001b[36m238.6928\u001b[0m      217.0616  0.0853\n",
      "      8       \u001b[36m98.3253\u001b[0m       94.8984  0.1026\n",
      "      7      \u001b[36m301.1605\u001b[0m      279.4453  0.0991\n",
      "      8      263.7543      238.0410  0.0861\n",
      "      8      110.2722      102.5232  0.0995\n",
      "      7      \u001b[36m124.5372\u001b[0m      \u001b[32m119.5578\u001b[0m  0.1093\n",
      "      7      \u001b[36m352.2877\u001b[0m      318.4390  0.1050\n",
      "      8      \u001b[36m242.2216\u001b[0m      219.5577  0.0879\n",
      "      7      145.7361      136.6334  0.1200\n",
      "      9      100.8660       95.3494  0.0885\n",
      "      9      \u001b[36m238.2738\u001b[0m      217.0257  0.0953\n",
      "      9       \u001b[36m97.8544\u001b[0m       94.5429  0.0931\n",
      "      9      263.5015      \u001b[32m237.2830\u001b[0m  0.0936\n",
      "      8      \u001b[36m300.5601\u001b[0m      279.1861  0.1135\n",
      "      8      124.5519      119.6907  0.0926\n",
      "      9      110.2698      102.4410  0.1059\n",
      "      9      \u001b[36m241.6072\u001b[0m      220.7804  0.0892\n",
      "      8      \u001b[36m351.1986\u001b[0m      321.1546  0.1076\n",
      "      8      \u001b[36m145.2752\u001b[0m      136.0658  0.0912\n",
      "     10       98.1560       94.3683  0.0770\n",
      "Restoring best model from epoch 2.\n",
      "     10      \u001b[36m100.2631\u001b[0m       95.1601  0.1080\n",
      "Restoring best model from epoch 5.\n",
      "     10      239.2021      216.9561  0.1029\n",
      "Restoring best model from epoch 3.\n",
      "      9      300.7440      279.4752  0.0876\n",
      "      9      124.8541      119.7002  0.0844\n",
      "     10      \u001b[36m261.8461\u001b[0m      237.4801  0.0977\n",
      "Restoring best model from epoch 3.\n",
      "     10      \u001b[36m109.9572\u001b[0m      102.4250  0.0884\n",
      "Restoring best model from epoch 4.\n",
      "     10      \u001b[36m240.3131\u001b[0m      221.2411  0.0862\n",
      "Restoring best model from epoch 4.\n",
      "      9      354.1756      319.4985  0.1025\n",
      "      9      145.5891      136.2698  0.0920\n",
      "     10      \u001b[36m299.5660\u001b[0m      277.8116  0.0844\n",
      "Restoring best model from epoch 6.\n",
      "     10      124.9433      119.9534  0.0838\n",
      "Restoring best model from epoch 5.\n",
      "     10      351.4922      321.6197  0.0871\n",
      "Restoring best model from epoch 5.\n",
      "     10      145.6078      137.2641  0.0886\n",
      "Restoring best model from epoch 4.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m121.7411\u001b[0m      \u001b[32m111.5970\u001b[0m  0.1140\n",
      "      2      \u001b[36m118.4969\u001b[0m      112.5719  0.1085\n",
      "      3      \u001b[36m116.7101\u001b[0m      114.0658  0.1106\n",
      "      4      \u001b[36m116.6198\u001b[0m      114.4377  0.1113\n",
      "      5      \u001b[36m116.2664\u001b[0m      124.7972  0.1249\n",
      "      6      116.6558      123.1212  0.1068\n",
      "      7      116.4531      120.7778  0.1065\n",
      "      8      \u001b[36m115.6155\u001b[0m      120.1211  0.1081\n",
      "      9      \u001b[36m115.2082\u001b[0m      119.3218  0.1073\n",
      "     10      115.4200      117.7822  0.1069\n",
      "Restoring best model from epoch 1.\n",
      "Concordance Index 0.6493656961448212\n",
      "Integrated Brier Score: 0.18417004585196722\n",
      "durations 0.49281314 85.81519\n",
      "Concordance Index 0.6512851325188598\n",
      "Integrated Brier Score: 0.18672272479409505\n",
      "(1786, 9) <class 'pandas.core.frame.DataFrame'>\n",
      "(1786,) <class 'pandas.core.series.Series'>\n",
      "(446, 9) <class 'pandas.core.frame.DataFrame'>\n",
      "(446,) <class 'pandas.core.series.Series'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/pycox/evaluation/eval_surv.py:36: FutureWarning: is_monotonic is deprecated and will be removed in a future version. Use is_monotonic_increasing instead.\n",
      "  assert pd.Series(self.index_surv).is_monotonic\n",
      "/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/pycox/evaluation/eval_surv.py:36: FutureWarning: is_monotonic is deprecated and will be removed in a future version. Use is_monotonic_increasing instead.\n",
      "  assert pd.Series(self.index_surv).is_monotonic\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m108.6369\u001b[0m       \u001b[32m95.2676\u001b[0m  0.0881\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m105.0174\u001b[0m       \u001b[32m89.4833\u001b[0m  0.0955\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m115.6564\u001b[0m      \u001b[32m100.0909\u001b[0m  0.0919\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m132.6349\u001b[0m      \u001b[32m118.2267\u001b[0m  0.0945\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m151.0288\u001b[0m      \u001b[32m136.1281\u001b[0m  0.0944\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m250.8967\u001b[0m      \u001b[32m212.7052\u001b[0m  0.0977\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m268.2217\u001b[0m      \u001b[32m229.3197\u001b[0m  0.1012\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m288.8365\u001b[0m      \u001b[32m236.4115\u001b[0m  0.0963\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m339.0514\u001b[0m      \u001b[32m285.4069\u001b[0m  0.0976\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m380.5226\u001b[0m      \u001b[32m324.8355\u001b[0m  0.1058\n",
      "      2      \u001b[36m102.9556\u001b[0m       \u001b[32m93.9936\u001b[0m  0.0911\n",
      "      2      \u001b[36m111.8930\u001b[0m       \u001b[32m99.5891\u001b[0m  0.0964\n",
      "      2      \u001b[36m128.6319\u001b[0m      \u001b[32m117.4747\u001b[0m  0.0994\n",
      "      2      \u001b[36m250.8457\u001b[0m      \u001b[32m225.0352\u001b[0m  0.0854\n",
      "      2      \u001b[36m247.9907\u001b[0m      \u001b[32m211.1646\u001b[0m  0.0940\n",
      "      2      \u001b[36m101.7500\u001b[0m       \u001b[32m89.3683\u001b[0m  0.1238\n",
      "      2      \u001b[36m145.3444\u001b[0m      \u001b[32m135.6650\u001b[0m  0.1051\n",
      "      2      \u001b[36m271.3469\u001b[0m      236.4458  0.1031\n",
      "      2      \u001b[36m354.9632\u001b[0m      \u001b[32m321.3477\u001b[0m  0.0954\n",
      "      2      \u001b[36m311.5009\u001b[0m      \u001b[32m282.0902\u001b[0m  0.1186\n",
      "      3      103.1009       \u001b[32m93.3666\u001b[0m  0.0920\n",
      "      3      \u001b[36m110.6534\u001b[0m       \u001b[32m99.1644\u001b[0m  0.0854\n",
      "      3      \u001b[36m249.4093\u001b[0m      \u001b[32m224.5823\u001b[0m  0.0882\n",
      "      3      \u001b[36m239.3007\u001b[0m      213.9065  0.0888\n",
      "      3      \u001b[36m128.0611\u001b[0m      117.8066  0.1030\n",
      "      3      \u001b[36m100.7954\u001b[0m       90.6320  0.0931\n",
      "      3      \u001b[36m144.4087\u001b[0m      135.8753  0.1086\n",
      "      3      \u001b[36m269.3686\u001b[0m      \u001b[32m236.2429\u001b[0m  0.0970\n",
      "      3      \u001b[36m309.7468\u001b[0m      282.4124  0.1000\n",
      "      4      \u001b[36m101.9024\u001b[0m       93.7151  0.0904\n",
      "      4      \u001b[36m110.3380\u001b[0m       99.4305  0.0894\n",
      "      3      \u001b[36m352.5525\u001b[0m      324.0679  0.1252\n",
      "      4       \u001b[36m99.7007\u001b[0m       90.0240  0.0898\n",
      "      4      \u001b[36m235.9775\u001b[0m      211.9741  0.0984\n",
      "      4      \u001b[36m246.8757\u001b[0m      \u001b[32m222.3833\u001b[0m  0.0995\n",
      "      4      \u001b[36m127.6860\u001b[0m      118.1706  0.1068\n",
      "      4      \u001b[36m266.7785\u001b[0m      \u001b[32m235.7863\u001b[0m  0.0880\n",
      "      4      \u001b[36m144.2343\u001b[0m      135.8338  0.1077\n",
      "      5      \u001b[36m101.1207\u001b[0m       \u001b[32m93.3131\u001b[0m  0.0847\n",
      "      5      \u001b[36m110.0488\u001b[0m       99.5522  0.0853\n",
      "      4      \u001b[36m307.6785\u001b[0m      \u001b[32m280.8953\u001b[0m  0.1094\n",
      "      4      \u001b[36m350.8186\u001b[0m      \u001b[32m320.3151\u001b[0m  0.1039\n",
      "      5       99.8346       90.8875  0.0907\n",
      "      5      \u001b[36m246.8435\u001b[0m      223.3725  0.1026\n",
      "      5      \u001b[36m127.3929\u001b[0m      118.1044  0.0937\n",
      "      5      268.2079      236.6167  0.1000\n",
      "      5      \u001b[36m235.9072\u001b[0m      211.4019  0.1185\n",
      "      5      \u001b[36m143.8628\u001b[0m      135.9494  0.0964\n",
      "      6      \u001b[36m109.8651\u001b[0m       99.6058  0.0819\n",
      "      6      101.3501       93.7877  0.0910\n",
      "      5      \u001b[36m307.2428\u001b[0m      \u001b[32m280.6461\u001b[0m  0.1117\n",
      "      5      \u001b[36m347.6711\u001b[0m      322.2801  0.0965\n",
      "      6       \u001b[36m99.6140\u001b[0m       91.0629  0.1041\n",
      "      6      \u001b[36m245.2954\u001b[0m      223.6842  0.0896\n",
      "      6      \u001b[36m127.0553\u001b[0m      118.3738  0.0973\n",
      "      6      \u001b[36m233.8593\u001b[0m      212.2613  0.0910\n",
      "      7      \u001b[36m109.8061\u001b[0m       99.6533  0.0810\n",
      "      6      \u001b[36m265.2062\u001b[0m      236.8445  0.1040\n",
      "      7      101.2979       93.7969  0.0905\n",
      "      6      143.9641      136.0102  0.1100\n",
      "      6      \u001b[36m305.1310\u001b[0m      281.1436  0.1012\n",
      "      6      347.8571      321.1448  0.1078\n",
      "      7       \u001b[36m99.3391\u001b[0m       91.3559  0.0998\n",
      "      7      \u001b[36m245.0452\u001b[0m      222.7128  0.0993\n",
      "      7      234.0241      217.2852  0.0924\n",
      "      8      101.4753       94.5061  0.0863\n",
      "      7      \u001b[36m126.8569\u001b[0m      118.4589  0.1120\n",
      "      7      266.2076      237.4315  0.1071\n",
      "      7      \u001b[36m143.2857\u001b[0m      136.3315  0.1111\n",
      "      8      \u001b[36m109.6705\u001b[0m       99.9852  0.1448\n",
      "      7      \u001b[36m304.2455\u001b[0m      281.4199  0.0946\n",
      "      8      234.5368      212.9260  0.0863\n",
      "      7      \u001b[36m346.3284\u001b[0m      321.7991  0.1038\n",
      "      8      100.1109       90.8088  0.1002\n",
      "      8      \u001b[36m243.8606\u001b[0m      223.4483  0.0992\n",
      "      9      101.2485       94.0504  0.1028\n",
      "      8      \u001b[36m264.0856\u001b[0m      236.1459  0.1011\n",
      "      8      127.0593      118.2638  0.1159\n",
      "      9      109.6790      100.1815  0.1042\n",
      "      8      304.8750      281.2249  0.0974\n",
      "      8      143.5504      136.0984  0.1172\n",
      "      9       \u001b[36m98.7786\u001b[0m       90.7395  0.0924\n",
      "      9      246.0039      222.9837  0.0925\n",
      "      9      \u001b[36m232.5547\u001b[0m      213.3125  0.1027\n",
      "      8      \u001b[36m345.6658\u001b[0m      322.2509  0.1198\n",
      "      9      266.0070      238.9904  0.0911\n",
      "     10      \u001b[36m100.6815\u001b[0m       95.1337  0.1059\n",
      "Restoring best model from epoch 5.\n",
      "      9      \u001b[36m126.4328\u001b[0m      118.2496  0.0920\n",
      "     10      \u001b[36m108.6712\u001b[0m      100.0798  0.0965\n",
      "Restoring best model from epoch 3.\n",
      "     10      232.7953      213.7472  0.0807\n",
      "Restoring best model from epoch 2.\n",
      "      9      \u001b[36m303.7704\u001b[0m      281.6432  0.1077\n",
      "      9      \u001b[36m143.0773\u001b[0m      135.9485  0.1073\n",
      "     10       98.9378       90.9043  0.0899\n",
      "Restoring best model from epoch 2.\n",
      "     10      244.7914      224.7844  0.0940\n",
      "Restoring best model from epoch 4.\n",
      "      9      \u001b[36m344.9417\u001b[0m      322.1886  0.0945\n",
      "     10      \u001b[36m126.1572\u001b[0m      118.3042  0.0865\n",
      "Restoring best model from epoch 2.\n",
      "     10      264.1856      238.4412  0.0945\n",
      "Restoring best model from epoch 4.\n",
      "     10      \u001b[36m303.7105\u001b[0m      283.0803  0.0883\n",
      "Restoring best model from epoch 5.\n",
      "     10      \u001b[36m142.6544\u001b[0m      135.9636  0.0896\n",
      "Restoring best model from epoch 2.\n",
      "     10      345.3854      322.0364  0.0934\n",
      "Restoring best model from epoch 4.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m299.5861\u001b[0m      \u001b[32m262.1009\u001b[0m  0.1259\n",
      "      2      \u001b[36m289.0683\u001b[0m      \u001b[32m259.5562\u001b[0m  0.1144\n",
      "      3      \u001b[36m284.3998\u001b[0m      \u001b[32m258.1722\u001b[0m  0.1174\n",
      "      4      \u001b[36m282.5916\u001b[0m      258.2436  0.1149\n",
      "      5      282.7926      \u001b[32m257.7671\u001b[0m  0.1155\n",
      "      6      \u001b[36m281.8535\u001b[0m      258.5275  0.1156\n",
      "      7      283.8696      257.8036  0.1151\n",
      "      8      \u001b[36m281.6640\u001b[0m      258.4558  0.1166\n",
      "      9      282.1027      258.2741  0.1123\n",
      "     10      \u001b[36m281.1838\u001b[0m      258.0912  0.1131\n",
      "Restoring best model from epoch 5.\n",
      "Concordance Index 0.6702753826799996\n",
      "Integrated Brier Score: 0.18067131340580958\n",
      "durations 1.4784395 84.0\n",
      "Concordance Index 0.6340278338582466\n",
      "Integrated Brier Score: 0.1850666713756179\n",
      "(1786, 9) <class 'pandas.core.frame.DataFrame'>\n",
      "(1786,) <class 'pandas.core.series.Series'>\n",
      "(446, 9) <class 'pandas.core.frame.DataFrame'>\n",
      "(446,) <class 'pandas.core.series.Series'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/pycox/evaluation/eval_surv.py:36: FutureWarning: is_monotonic is deprecated and will be removed in a future version. Use is_monotonic_increasing instead.\n",
      "  assert pd.Series(self.index_surv).is_monotonic\n",
      "/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/pycox/evaluation/eval_surv.py:36: FutureWarning: is_monotonic is deprecated and will be removed in a future version. Use is_monotonic_increasing instead.\n",
      "  assert pd.Series(self.index_surv).is_monotonic\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m101.7282\u001b[0m       \u001b[32m87.4560\u001b[0m  0.0829\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m116.8231\u001b[0m       \u001b[32m99.8416\u001b[0m  0.0833\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m110.2602\u001b[0m       \u001b[32m93.0723\u001b[0m  0.0901\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m261.6484\u001b[0m      \u001b[32m208.3373\u001b[0m  0.0919\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m132.5224\u001b[0m      \u001b[32m120.3552\u001b[0m  0.1036\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m285.9399\u001b[0m      \u001b[32m238.1077\u001b[0m  0.0878\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m262.4409\u001b[0m      \u001b[32m218.9673\u001b[0m  0.0928\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m153.9261\u001b[0m      \u001b[32m138.0943\u001b[0m  0.1125\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m320.6074\u001b[0m      \u001b[32m284.7851\u001b[0m  0.0957\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m372.7395\u001b[0m      \u001b[32m327.2125\u001b[0m  0.1087\n",
      "      2      \u001b[36m112.7194\u001b[0m       \u001b[32m98.5643\u001b[0m  0.0866\n",
      "      2       \u001b[36m99.0836\u001b[0m       90.2498  0.1093\n",
      "      2      \u001b[36m105.8217\u001b[0m       \u001b[32m91.9573\u001b[0m  0.1084\n",
      "      2      \u001b[36m274.3006\u001b[0m      \u001b[32m235.2442\u001b[0m  0.0870\n",
      "      2      \u001b[36m244.4971\u001b[0m      \u001b[32m203.8742\u001b[0m  0.0987\n",
      "      2      \u001b[36m127.3618\u001b[0m      \u001b[32m119.9085\u001b[0m  0.1029\n",
      "      2      \u001b[36m253.5017\u001b[0m      \u001b[32m218.9240\u001b[0m  0.1009\n",
      "      2      \u001b[36m147.4893\u001b[0m      \u001b[32m137.9987\u001b[0m  0.1009\n",
      "      2      \u001b[36m305.2023\u001b[0m      \u001b[32m283.0988\u001b[0m  0.1089\n",
      "      2      \u001b[36m358.3776\u001b[0m      \u001b[32m323.9662\u001b[0m  0.1152\n",
      "      3       \u001b[36m98.2953\u001b[0m       \u001b[32m87.2786\u001b[0m  0.0948\n",
      "      3      \u001b[36m105.3161\u001b[0m       92.1467  0.0858\n",
      "      3      \u001b[36m112.0578\u001b[0m       99.8312  0.1194\n",
      "      3      \u001b[36m241.0751\u001b[0m      206.1479  0.0884\n",
      "      3      \u001b[36m269.9904\u001b[0m      236.1561  0.0983\n",
      "      3      \u001b[36m126.8877\u001b[0m      \u001b[32m119.4772\u001b[0m  0.0904\n",
      "      3      \u001b[36m251.8572\u001b[0m      219.1915  0.0950\n",
      "      3      \u001b[36m146.6056\u001b[0m      \u001b[32m137.4093\u001b[0m  0.1086\n",
      "      4      \u001b[36m104.3467\u001b[0m       92.7164  0.0773\n",
      "      3      \u001b[36m302.8881\u001b[0m      283.1233  0.1086\n",
      "      4       \u001b[36m97.5797\u001b[0m       88.3599  0.0817\n",
      "      3      \u001b[36m354.4162\u001b[0m      324.6132  0.0959\n",
      "      4      \u001b[36m239.2114\u001b[0m      206.2186  0.0889\n",
      "      4      \u001b[36m269.8147\u001b[0m      \u001b[32m234.5321\u001b[0m  0.0918\n",
      "      4      \u001b[36m126.4148\u001b[0m      119.5431  0.0923\n",
      "      4      \u001b[36m111.5270\u001b[0m       \u001b[32m97.4290\u001b[0m  0.1145\n",
      "      4      \u001b[36m249.4387\u001b[0m      \u001b[32m216.4363\u001b[0m  0.0984\n",
      "      5      104.7284       92.0361  0.0909\n",
      "      4      \u001b[36m146.3787\u001b[0m      137.5923  0.1213\n",
      "      5       97.7496       87.6498  0.0969\n",
      "      4      \u001b[36m302.5931\u001b[0m      \u001b[32m282.5690\u001b[0m  0.1036\n",
      "      5      \u001b[36m237.6940\u001b[0m      207.2202  0.0895\n",
      "      5      126.6463      \u001b[32m119.2608\u001b[0m  0.0809\n",
      "      4      \u001b[36m351.1073\u001b[0m      \u001b[32m322.6010\u001b[0m  0.1152\n",
      "      5      \u001b[36m268.9327\u001b[0m      \u001b[32m234.2060\u001b[0m  0.1002\n",
      "      5      250.0101      217.2546  0.0889\n",
      "      5      111.6265       98.1101  0.1074\n",
      "      6       \u001b[36m97.0526\u001b[0m       89.5333  0.0789\n",
      "      6      104.7709       \u001b[32m91.1346\u001b[0m  0.0959\n",
      "      5      \u001b[36m146.0280\u001b[0m      \u001b[32m137.3749\u001b[0m  0.1037\n",
      "      6      239.7101      208.0879  0.0902\n",
      "      5      \u001b[36m302.3089\u001b[0m      282.8041  0.1069\n",
      "      6      250.1634      216.8571  0.0795\n",
      "      6      268.9889      234.5324  0.0897\n",
      "      6      \u001b[36m126.3180\u001b[0m      \u001b[32m119.1150\u001b[0m  0.1110\n",
      "      5      352.6293      323.5567  0.1058\n",
      "      6      \u001b[36m111.1729\u001b[0m       97.7280  0.0851\n",
      "      7       97.2259       88.7764  0.0916\n",
      "      7      104.4932       92.6496  0.1073\n",
      "      6      \u001b[36m145.8574\u001b[0m      \u001b[32m137.3711\u001b[0m  0.0975\n",
      "      7      239.3074      205.0506  0.0930\n",
      "      7      \u001b[36m266.7914\u001b[0m      \u001b[32m234.0383\u001b[0m  0.0828\n",
      "      7      249.5702      217.6460  0.0893\n",
      "      6      \u001b[36m301.4708\u001b[0m      283.9785  0.1054\n",
      "      7      \u001b[36m126.2578\u001b[0m      119.1167  0.0959\n",
      "      6      351.5789      324.3292  0.0902\n",
      "      8       97.1546       87.9201  0.0872\n",
      "      8      104.5552       \u001b[32m90.7395\u001b[0m  0.0856\n",
      "      7      \u001b[36m145.6764\u001b[0m      \u001b[32m136.9862\u001b[0m  0.0864\n",
      "      7      \u001b[36m111.1179\u001b[0m       98.4129  0.1515\n",
      "      8      238.5387      206.5645  0.0964\n",
      "      8      250.1725      217.5997  0.0925\n",
      "      7      301.9862      \u001b[32m282.3810\u001b[0m  0.0909\n",
      "      8      267.0296      234.8258  0.1057\n",
      "      8      \u001b[36m125.6654\u001b[0m      \u001b[32m118.7770\u001b[0m  0.0999\n",
      "      7      \u001b[36m351.0302\u001b[0m      323.7455  0.1084\n",
      "      9       \u001b[36m96.6709\u001b[0m       89.6091  0.1024\n",
      "      9      104.7826       92.2426  0.0943\n",
      "      8      146.1555      136.9983  0.0926\n",
      "      8      111.2553       \u001b[32m97.3662\u001b[0m  0.0902\n",
      "      9      \u001b[36m249.2104\u001b[0m      218.9359  0.0862\n",
      "      9      \u001b[36m237.1082\u001b[0m      206.3241  0.1035\n",
      "      8      \u001b[36m300.9411\u001b[0m      283.0889  0.0916\n",
      "      9      \u001b[36m266.6364\u001b[0m      234.4615  0.1066\n",
      "      9      126.0677      119.2330  0.0951\n",
      "      8      \u001b[36m349.9302\u001b[0m      324.4646  0.1065\n",
      "     10       \u001b[36m96.2834\u001b[0m       88.5116  0.0949\n",
      "Restoring best model from epoch 3.\n",
      "     10      \u001b[36m103.7906\u001b[0m       91.7549  0.0940\n",
      "Restoring best model from epoch 8.\n",
      "      9      111.5351       98.1694  0.0957\n",
      "     10      \u001b[36m247.5265\u001b[0m      218.0568  0.0890\n",
      "Restoring best model from epoch 4.\n",
      "      9      145.8139      137.8754  0.1057\n",
      "      9      \u001b[36m300.4336\u001b[0m      284.0048  0.0845\n",
      "     10      237.6343      206.3766  0.0944\n",
      "Restoring best model from epoch 2.\n",
      "     10      \u001b[36m265.8695\u001b[0m      235.1611  0.0918\n",
      "Restoring best model from epoch 7.\n",
      "     10      125.9302      118.8618  0.0958\n",
      "Restoring best model from epoch 8.\n",
      "      9      \u001b[36m349.9203\u001b[0m      324.5403  0.0981\n",
      "     10      \u001b[36m110.7465\u001b[0m       \u001b[32m97.2775\u001b[0m  0.0848\n",
      "     10      145.8575      137.3722  0.0900\n",
      "Restoring best model from epoch 7.\n",
      "     10      \u001b[36m299.4354\u001b[0m      283.6013  0.0871\n",
      "Restoring best model from epoch 7.\n",
      "     10      350.3107      324.5856  0.0891\n",
      "Restoring best model from epoch 4.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m120.5109\u001b[0m      \u001b[32m114.3699\u001b[0m  0.1127\n",
      "      2      \u001b[36m117.0970\u001b[0m      \u001b[32m113.1822\u001b[0m  0.1076\n",
      "      3      \u001b[36m115.9411\u001b[0m      113.2396  0.1056\n",
      "      4      116.3877      113.7743  0.1045\n",
      "      5      \u001b[36m115.8903\u001b[0m      113.7241  0.1053\n",
      "      6      \u001b[36m115.5439\u001b[0m      113.8922  0.1051\n",
      "      7      115.5784      113.4313  0.1059\n",
      "      8      \u001b[36m115.2178\u001b[0m      113.8060  0.1065\n",
      "      9      \u001b[36m115.0701\u001b[0m      113.4762  0.1070\n",
      "     10      \u001b[36m114.9202\u001b[0m      113.5877  0.1060\n",
      "Restoring best model from epoch 2.\n",
      "Concordance Index 0.6688399577722344\n",
      "Integrated Brier Score: 0.18416773281210178\n",
      "durations 0.26283368 84.20534\n",
      "Concordance Index 0.6683712981013166\n",
      "Integrated Brier Score: 0.18693997570739734\n",
      "load_support\n",
      "split age                  float32\n",
      "sex                 category\n",
      "race                 float32\n",
      "n_comorbidities      float32\n",
      "diabetes            category\n",
      "dementia            category\n",
      "blood_pressure       float32\n",
      "heart_rate           float32\n",
      "respiration_rate     float32\n",
      "temperature          float32\n",
      "white_blood_cell     float32\n",
      "serum_sodium         float32\n",
      "serum_creatinine     float32\n",
      "cancer_0.0             uint8\n",
      "cancer_1.0             uint8\n",
      "cancer_2.0             uint8\n",
      "dtype: object\n",
      "(7098, 16) <class 'pandas.core.frame.DataFrame'>\n",
      "(7098,) <class 'pandas.core.series.Series'>\n",
      "(1775, 16) <class 'pandas.core.frame.DataFrame'>\n",
      "(1775,) <class 'pandas.core.series.Series'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/pycox/evaluation/eval_surv.py:36: FutureWarning: is_monotonic is deprecated and will be removed in a future version. Use is_monotonic_increasing instead.\n",
      "  assert pd.Series(self.index_surv).is_monotonic\n",
      "/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/pycox/evaluation/eval_surv.py:36: FutureWarning: is_monotonic is deprecated and will be removed in a future version. Use is_monotonic_increasing instead.\n",
      "  assert pd.Series(self.index_surv).is_monotonic\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m124.0263\u001b[0m      \u001b[32m124.0201\u001b[0m  0.4027\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m131.3084\u001b[0m      \u001b[32m130.6045\u001b[0m  0.3984\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m123.6923\u001b[0m      \u001b[32m124.2135\u001b[0m  0.4292\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m166.0955\u001b[0m      \u001b[32m165.3362\u001b[0m  0.4335\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m302.9884\u001b[0m      \u001b[32m298.4634\u001b[0m  0.4317\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m301.5585\u001b[0m      \u001b[32m298.2152\u001b[0m  0.4425\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m318.9834\u001b[0m      \u001b[32m314.6294\u001b[0m  0.4402\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m159.1990\u001b[0m      \u001b[32m154.5824\u001b[0m  0.4874\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m388.6021\u001b[0m      \u001b[32m375.0564\u001b[0m  0.4856\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m405.3323\u001b[0m      \u001b[32m398.9168\u001b[0m  0.4752\n",
      "      2      \u001b[36m122.9795\u001b[0m      \u001b[32m123.9253\u001b[0m  0.3749\n",
      "      2      \u001b[36m129.7127\u001b[0m      \u001b[32m130.4583\u001b[0m  0.4036\n",
      "      2      \u001b[36m122.5019\u001b[0m      \u001b[32m124.1584\u001b[0m  0.4399\n",
      "      2      \u001b[36m295.6660\u001b[0m      299.4402  0.4173\n",
      "      2      \u001b[36m296.8653\u001b[0m      299.0013  0.4272\n",
      "      2      \u001b[36m164.8817\u001b[0m      \u001b[32m165.0539\u001b[0m  0.4576\n",
      "      2      \u001b[36m313.4984\u001b[0m      315.4156  0.4367\n",
      "      2      \u001b[36m157.5988\u001b[0m      154.6516  0.4722\n",
      "      2      \u001b[36m380.1105\u001b[0m      \u001b[32m374.7528\u001b[0m  0.4538\n",
      "      2      \u001b[36m398.7806\u001b[0m      400.6817  0.4787\n",
      "      3      \u001b[36m122.4891\u001b[0m      123.9338  0.3734\n",
      "      3      \u001b[36m129.5110\u001b[0m      \u001b[32m130.3463\u001b[0m  0.4228\n",
      "      3      \u001b[36m122.0283\u001b[0m      \u001b[32m124.1521\u001b[0m  0.4197\n",
      "      3      \u001b[36m295.9577\u001b[0m      300.0542  0.4239\n",
      "      3      \u001b[36m295.5438\u001b[0m      299.5546  0.4490\n",
      "      3      \u001b[36m312.5667\u001b[0m      315.0241  0.4369\n",
      "      3      \u001b[36m164.7635\u001b[0m      165.1810  0.4551\n",
      "      3      \u001b[36m157.2697\u001b[0m      154.7659  0.4853\n",
      "      3      \u001b[36m379.2097\u001b[0m      \u001b[32m374.6362\u001b[0m  0.5136\n",
      "      3      \u001b[36m397.8367\u001b[0m      \u001b[32m398.9131\u001b[0m  0.5389\n",
      "      4      \u001b[36m122.2857\u001b[0m      \u001b[32m123.8149\u001b[0m  0.4214\n",
      "      4      \u001b[36m129.1972\u001b[0m      \u001b[32m130.2294\u001b[0m  0.4323\n",
      "      4      \u001b[36m121.8617\u001b[0m      \u001b[32m124.1411\u001b[0m  0.3992\n",
      "      4      \u001b[36m295.7543\u001b[0m      299.7198  0.4143\n",
      "      4      \u001b[36m164.5891\u001b[0m      165.0785  0.3858\n",
      "      4      \u001b[36m294.7929\u001b[0m      298.7317  0.4371\n",
      "      4      \u001b[36m312.0701\u001b[0m      314.8946  0.4662\n",
      "      4      \u001b[36m157.2295\u001b[0m      \u001b[32m154.3166\u001b[0m  0.4353\n",
      "      5      \u001b[36m122.1312\u001b[0m      124.1410  0.3819\n",
      "      4      379.4645      \u001b[32m374.1046\u001b[0m  0.4707\n",
      "      4      \u001b[36m397.5679\u001b[0m      399.8831  0.4732\n",
      "      5      \u001b[36m121.6493\u001b[0m      124.1682  0.3854\n",
      "      5      \u001b[36m128.9500\u001b[0m      130.4180  0.4088\n",
      "      5      \u001b[36m294.8727\u001b[0m      299.1628  0.4605\n",
      "      5      \u001b[36m164.3548\u001b[0m      165.2888  0.4692\n",
      "      5      \u001b[36m311.9400\u001b[0m      314.9180  0.4093\n",
      "      5      \u001b[36m294.4643\u001b[0m      298.7206  0.4581\n",
      "      5      \u001b[36m157.1234\u001b[0m      154.5774  0.4529\n",
      "      6      \u001b[36m121.9266\u001b[0m      124.0743  0.3914\n",
      "      5      \u001b[36m379.1664\u001b[0m      \u001b[32m373.5023\u001b[0m  0.4737\n",
      "      6      \u001b[36m121.5022\u001b[0m      \u001b[32m124.1325\u001b[0m  0.4014\n",
      "      6      \u001b[36m128.8367\u001b[0m      130.3943  0.4133\n",
      "      5      \u001b[36m397.3788\u001b[0m      399.0403  0.4921\n",
      "      6      \u001b[36m294.4299\u001b[0m      298.7338  0.3928\n",
      "      6      \u001b[36m294.1886\u001b[0m      \u001b[32m298.1877\u001b[0m  0.4141\n",
      "      6      \u001b[36m311.5593\u001b[0m      \u001b[32m314.5196\u001b[0m  0.4388\n",
      "      6      \u001b[36m164.1947\u001b[0m      165.1370  0.4712\n",
      "      7      \u001b[36m121.8372\u001b[0m      124.0915  0.4093\n",
      "      6      \u001b[36m156.9292\u001b[0m      154.5019  0.4839\n",
      "      7      \u001b[36m121.3591\u001b[0m      124.2227  0.3746\n",
      "      7      128.8596      130.4855  0.4159\n",
      "      6      \u001b[36m378.3820\u001b[0m      374.7644  0.5300\n",
      "      7      294.4403      \u001b[32m298.0776\u001b[0m  0.4214\n",
      "      6      \u001b[36m397.0143\u001b[0m      \u001b[32m398.8181\u001b[0m  0.5070\n",
      "      7      \u001b[36m293.6035\u001b[0m      \u001b[32m297.9163\u001b[0m  0.4240\n",
      "      7      311.7128      315.0771  0.4326\n",
      "      8      121.8622      124.1627  0.4033\n",
      "      7      \u001b[36m163.9347\u001b[0m      \u001b[32m164.8803\u001b[0m  0.4800\n",
      "      8      \u001b[36m121.3159\u001b[0m      \u001b[32m124.1277\u001b[0m  0.4152\n",
      "      7      \u001b[36m156.7247\u001b[0m      154.3327  0.4942\n",
      "      8      \u001b[36m128.7570\u001b[0m      130.4668  0.4317\n",
      "      8      294.5326      298.3042  0.4276\n",
      "      8      \u001b[36m293.2588\u001b[0m      298.1477  0.4360\n",
      "      7      \u001b[36m378.3371\u001b[0m      374.0600  0.5383\n",
      "      7      \u001b[36m396.6237\u001b[0m      \u001b[32m398.4302\u001b[0m  0.4975\n",
      "      8      \u001b[36m311.1897\u001b[0m      314.7965  0.4381\n",
      "      9      \u001b[36m121.8166\u001b[0m      123.9779  0.3917\n",
      "      8      163.9900      165.0506  0.4338\n",
      "      9      121.4926      \u001b[32m123.9895\u001b[0m  0.3874\n",
      "      9      \u001b[36m128.5872\u001b[0m      130.4405  0.4057\n",
      "      8      \u001b[36m156.5627\u001b[0m      154.5120  0.4971\n",
      "      9      \u001b[36m294.0964\u001b[0m      \u001b[32m298.0230\u001b[0m  0.4116\n",
      "      9      \u001b[36m310.8093\u001b[0m      \u001b[32m314.2602\u001b[0m  0.4282\n",
      "      8      \u001b[36m377.9508\u001b[0m      374.6005  0.4671\n",
      "      9      \u001b[36m292.9975\u001b[0m      298.2422  0.4905\n",
      "     10      \u001b[36m121.6278\u001b[0m      124.1567  0.4475\n",
      "Restoring best model from epoch 4.\n",
      "      8      \u001b[36m396.1815\u001b[0m      398.6565  0.5036\n",
      "      9      163.9453      165.0039  0.4782\n",
      "     10      \u001b[36m121.2232\u001b[0m      124.1401  0.4530\n",
      "Restoring best model from epoch 9.\n",
      "     10      128.6999      130.4195  0.4347\n",
      "Restoring best model from epoch 4.\n",
      "     10      \u001b[36m294.0307\u001b[0m      \u001b[32m297.9639\u001b[0m  0.3703\n",
      "      9      156.6234      154.3790  0.4776\n",
      "     10      310.9660      314.4110  0.4344\n",
      "Restoring best model from epoch 9.\n",
      "     10      \u001b[36m292.8262\u001b[0m      298.2374  0.4206\n",
      "Restoring best model from epoch 7.\n",
      "      9      \u001b[36m377.8086\u001b[0m      374.5703  0.4690\n",
      "      9      \u001b[36m395.2806\u001b[0m      400.4872  0.4614\n",
      "     10      \u001b[36m163.8229\u001b[0m      \u001b[32m164.8326\u001b[0m  0.4115\n",
      "     10      \u001b[36m156.5556\u001b[0m      154.6168  0.3937\n",
      "Restoring best model from epoch 4.\n",
      "     10      378.4038      374.6844  0.4074\n",
      "Restoring best model from epoch 5.\n",
      "     10      \u001b[36m395.0269\u001b[0m      400.2434  0.3974\n",
      "Restoring best model from epoch 7.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m141.3825\u001b[0m      \u001b[32m142.0685\u001b[0m  0.4536\n",
      "      2      \u001b[36m139.9550\u001b[0m      \u001b[32m142.0621\u001b[0m  0.4692\n",
      "      3      \u001b[36m139.7294\u001b[0m      \u001b[32m141.7838\u001b[0m  0.4732\n",
      "      4      \u001b[36m139.2878\u001b[0m      142.0785  0.4671\n",
      "      5      \u001b[36m139.2470\u001b[0m      141.8499  0.4606\n",
      "      6      \u001b[36m139.2207\u001b[0m      141.9056  0.4525\n",
      "      7      \u001b[36m139.0831\u001b[0m      141.8022  0.4619\n",
      "      8      139.0933      141.9029  0.4469\n",
      "      9      \u001b[36m138.9494\u001b[0m      141.8917  0.4518\n",
      "     10      \u001b[36m138.6677\u001b[0m      141.8147  0.4457\n",
      "Restoring best model from epoch 3.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/pycox/evaluation/eval_surv.py:36: FutureWarning: is_monotonic is deprecated and will be removed in a future version. Use is_monotonic_increasing instead.\n",
      "  assert pd.Series(self.index_surv).is_monotonic\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concordance Index 0.5975996427595404\n",
      "Integrated Brier Score: 0.19579720445357937\n",
      "durations 3.0 2024.0\n",
      "Concordance Index 0.5871250741306627\n",
      "Integrated Brier Score: 0.20000452814229397\n",
      "(7098, 16) <class 'pandas.core.frame.DataFrame'>\n",
      "(7098,) <class 'pandas.core.series.Series'>\n",
      "(1775, 16) <class 'pandas.core.frame.DataFrame'>\n",
      "(1775,) <class 'pandas.core.series.Series'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/pycox/evaluation/eval_surv.py:36: FutureWarning: is_monotonic is deprecated and will be removed in a future version. Use is_monotonic_increasing instead.\n",
      "  assert pd.Series(self.index_surv).is_monotonic\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1     \u001b[36m1631.6097\u001b[0m     \u001b[32m1522.2317\u001b[0m  0.3463\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1     \u001b[36m1631.7936\u001b[0m     \u001b[32m1519.3457\u001b[0m  0.3516\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1     \u001b[36m1720.1249\u001b[0m     \u001b[32m1605.3218\u001b[0m  0.3682\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1     \u001b[36m2098.0450\u001b[0m     \u001b[32m1914.6310\u001b[0m  0.4138\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1     \u001b[36m2197.3352\u001b[0m     \u001b[32m1998.2247\u001b[0m  0.4333\n",
      "      2     \u001b[36m1607.6857\u001b[0m     1522.6580  0.3414\n",
      "      2     \u001b[36m1605.9679\u001b[0m     \u001b[32m1518.1426\u001b[0m  0.3684\n",
      "      2     \u001b[36m1692.4287\u001b[0m     1606.2024  0.3564\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m310.1298\u001b[0m      \u001b[32m305.7002\u001b[0m  0.8080\n",
      "      2     \u001b[36m2058.7286\u001b[0m     1914.6460  0.3815\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m314.3764\u001b[0m      \u001b[32m303.6213\u001b[0m  0.8496\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m331.9328\u001b[0m      \u001b[32m323.4735\u001b[0m  0.8540\n",
      "      2     \u001b[36m2160.4332\u001b[0m     1998.5943  0.3864\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m407.4829\u001b[0m      \u001b[32m382.9250\u001b[0m  0.8871\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m425.3167\u001b[0m      \u001b[32m403.1511\u001b[0m  0.9301\n",
      "      3     \u001b[36m1601.5860\u001b[0m     \u001b[32m1520.1392\u001b[0m  0.3530\n",
      "      3     \u001b[36m1598.7398\u001b[0m     \u001b[32m1515.9787\u001b[0m  0.3363\n",
      "      3     \u001b[36m1688.6791\u001b[0m     \u001b[32m1604.6161\u001b[0m  0.3469\n",
      "      3     \u001b[36m2055.0766\u001b[0m     \u001b[32m1913.6667\u001b[0m  0.4216\n",
      "      3     \u001b[36m2154.8965\u001b[0m     1999.0122  0.3960\n",
      "      4     \u001b[36m1600.9196\u001b[0m     \u001b[32m1518.0783\u001b[0m  0.3393\n",
      "      4     \u001b[36m1597.0887\u001b[0m     \u001b[32m1512.9873\u001b[0m  0.3541\n",
      "      4     \u001b[36m1686.5669\u001b[0m     \u001b[32m1603.1198\u001b[0m  0.3429\n",
      "      2      \u001b[36m299.7913\u001b[0m      \u001b[32m301.3736\u001b[0m  0.8270\n",
      "      2      \u001b[36m300.7529\u001b[0m      303.9220  0.7938\n",
      "      4     \u001b[36m2053.6005\u001b[0m     \u001b[32m1912.7296\u001b[0m  0.3844\n",
      "      4     2155.3783     1998.4299  0.4025\n",
      "      2      \u001b[36m318.3866\u001b[0m      \u001b[32m323.0829\u001b[0m  0.8259\n",
      "      5     \u001b[36m1598.0843\u001b[0m     1518.1409  0.3631\n",
      "      5     1597.1048     \u001b[32m1512.4680\u001b[0m  0.3528\n",
      "      2      \u001b[36m383.6572\u001b[0m      \u001b[32m380.8261\u001b[0m  0.8988\n",
      "      2      \u001b[36m404.6849\u001b[0m      \u001b[32m398.5465\u001b[0m  0.8599\n",
      "      5     \u001b[36m1684.0639\u001b[0m     \u001b[32m1602.9993\u001b[0m  0.3765\n",
      "      5     \u001b[36m2051.5287\u001b[0m     1912.7942  0.4007\n",
      "      5     \u001b[36m2153.8897\u001b[0m     \u001b[32m1997.6187\u001b[0m  0.3920\n",
      "      6     \u001b[36m1594.7881\u001b[0m     1512.6699  0.3619\n",
      "      6     \u001b[36m1597.8721\u001b[0m     1518.1533  0.3846\n",
      "      6     \u001b[36m1683.9411\u001b[0m     \u001b[32m1602.8318\u001b[0m  0.4046\n",
      "      6     2051.6277     \u001b[32m1912.6205\u001b[0m  0.3755\n",
      "      3      \u001b[36m298.0277\u001b[0m      \u001b[32m301.5889\u001b[0m  0.8088\n",
      "      7     \u001b[36m1594.5191\u001b[0m     1513.1706  0.3093\n",
      "      6     \u001b[36m2153.7365\u001b[0m     \u001b[32m1997.1875\u001b[0m  0.4241\n",
      "      3      \u001b[36m296.7699\u001b[0m      \u001b[32m300.6978\u001b[0m  0.8539\n",
      "      7     \u001b[36m1596.8917\u001b[0m     1518.3292  0.3318\n",
      "      3      \u001b[36m314.3203\u001b[0m      \u001b[32m319.6729\u001b[0m  0.8630\n",
      "      7     \u001b[36m1683.0190\u001b[0m     \u001b[32m1602.5290\u001b[0m  0.3469\n",
      "      3      \u001b[36m400.4301\u001b[0m      \u001b[32m397.7686\u001b[0m  0.8238\n",
      "      3      \u001b[36m379.7298\u001b[0m      \u001b[32m380.7722\u001b[0m  0.8397\n",
      "      7     \u001b[36m2048.5140\u001b[0m     \u001b[32m1912.3158\u001b[0m  0.3607\n",
      "      8     1594.9798     1514.3040  0.3296\n",
      "      8     1597.0715     1518.6689  0.3487\n",
      "      7     \u001b[36m2152.5449\u001b[0m     1997.5194  0.4068\n",
      "      8     \u001b[36m1681.7184\u001b[0m     1603.2988  0.3554\n",
      "      9     \u001b[36m1592.2203\u001b[0m     1514.0847  0.3346\n",
      "      8     2049.3098     \u001b[32m1911.6270\u001b[0m  0.3818\n",
      "      9     1597.1403     1518.5407  0.3467\n",
      "      4      \u001b[36m296.0749\u001b[0m      \u001b[32m301.4731\u001b[0m  0.8213\n",
      "      8     2152.9928     1997.5394  0.4134\n",
      "      4      \u001b[36m295.0317\u001b[0m      300.8866  0.8325\n",
      "      9     1682.2906     1603.4547  0.4097\n",
      "      4      \u001b[36m312.1941\u001b[0m      \u001b[32m318.7355\u001b[0m  0.8858\n",
      "     10     \u001b[36m1591.3360\u001b[0m     1513.5872  0.3555\n",
      "Restoring best model from epoch 5.\n",
      "      4      \u001b[36m378.7313\u001b[0m      \u001b[32m378.8496\u001b[0m  0.8494\n",
      "      4      \u001b[36m398.9126\u001b[0m      \u001b[32m397.3565\u001b[0m  0.8923\n",
      "     10     \u001b[36m1594.8338\u001b[0m     \u001b[32m1518.0456\u001b[0m  0.3649\n",
      "Restoring best model from epoch 4.\n",
      "      9     \u001b[36m2047.1402\u001b[0m     \u001b[32m1911.5572\u001b[0m  0.4095\n",
      "     10     \u001b[36m1681.1702\u001b[0m     1603.3598  0.3415\n",
      "Restoring best model from epoch 7.\n",
      "      9     \u001b[36m2151.8829\u001b[0m     1997.5103  0.4456\n",
      "     10     2048.5310     1912.0894  0.3406\n",
      "Restoring best model from epoch 8.\n",
      "      5      \u001b[36m295.1780\u001b[0m      \u001b[32m300.8478\u001b[0m  0.7401\n",
      "      5      \u001b[36m294.5286\u001b[0m      \u001b[32m300.3312\u001b[0m  0.7280\n",
      "     10     \u001b[36m2151.4691\u001b[0m     1997.4750  0.3488\n",
      "Restoring best model from epoch 6.\n",
      "      5      \u001b[36m310.9665\u001b[0m      318.7563  0.7298\n",
      "      5      \u001b[36m377.6014\u001b[0m      \u001b[32m378.8255\u001b[0m  0.7359\n",
      "      5      \u001b[36m398.0592\u001b[0m      \u001b[32m397.3543\u001b[0m  0.7394\n",
      "      6      \u001b[36m294.3901\u001b[0m      300.9947  0.6853\n",
      "      6      \u001b[36m293.6268\u001b[0m      \u001b[32m299.9078\u001b[0m  0.6872\n",
      "      6      \u001b[36m310.2345\u001b[0m      319.4783  0.7039\n",
      "      6      \u001b[36m376.8644\u001b[0m      379.2114  0.7207\n",
      "      6      \u001b[36m397.3803\u001b[0m      397.8500  0.7259\n",
      "      7      \u001b[36m294.0021\u001b[0m      \u001b[32m300.7798\u001b[0m  0.6844\n",
      "      7      \u001b[36m293.2315\u001b[0m      300.0170  0.6847\n",
      "      7      \u001b[36m309.8209\u001b[0m      318.9167  0.6988\n",
      "      7      \u001b[36m376.8597\u001b[0m      \u001b[32m378.7753\u001b[0m  0.7213\n",
      "      7      \u001b[36m396.8141\u001b[0m      398.3014  0.7261\n",
      "      8      \u001b[36m293.5661\u001b[0m      \u001b[32m300.7509\u001b[0m  0.6936\n",
      "      8      \u001b[36m293.1786\u001b[0m      \u001b[32m299.8348\u001b[0m  0.6880\n",
      "      8      309.8376      318.8509  0.7043\n",
      "      8      \u001b[36m376.4519\u001b[0m      379.2571  0.7206\n",
      "      8      \u001b[36m396.5036\u001b[0m      397.7103  0.7239\n",
      "      9      \u001b[36m293.5651\u001b[0m      301.0564  0.6836\n",
      "      9      \u001b[36m292.6682\u001b[0m      299.8486  0.6837\n",
      "      9      \u001b[36m309.4329\u001b[0m      318.8720  0.6983\n",
      "      9      \u001b[36m376.3067\u001b[0m      378.8861  0.7154\n",
      "      9      \u001b[36m396.3948\u001b[0m      397.9385  0.7220\n",
      "     10      \u001b[36m293.4380\u001b[0m      \u001b[32m300.6052\u001b[0m  0.6796\n",
      "     10      \u001b[36m292.6074\u001b[0m      300.2551  0.6845\n",
      "Restoring best model from epoch 8.\n",
      "     10      \u001b[36m309.0560\u001b[0m      318.7827  0.6974\n",
      "Restoring best model from epoch 4.\n",
      "     10      \u001b[36m375.9561\u001b[0m      379.1157  0.7147\n",
      "Restoring best model from epoch 7.\n",
      "     10      \u001b[36m396.0598\u001b[0m      \u001b[32m396.8721\u001b[0m  0.7226\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1     \u001b[36m1851.2468\u001b[0m     \u001b[32m1710.2778\u001b[0m  0.4219\n",
      "      2     \u001b[36m1821.9871\u001b[0m     \u001b[32m1709.1981\u001b[0m  0.4200\n",
      "      3     \u001b[36m1820.0816\u001b[0m     \u001b[32m1708.0224\u001b[0m  0.4128\n",
      "      4     \u001b[36m1817.4043\u001b[0m     \u001b[32m1707.8491\u001b[0m  0.4099\n",
      "      5     1817.4726     1708.2227  0.4101\n",
      "      6     \u001b[36m1815.8749\u001b[0m     \u001b[32m1707.6561\u001b[0m  0.4199\n",
      "      7     \u001b[36m1814.6460\u001b[0m     \u001b[32m1707.4298\u001b[0m  0.4169\n",
      "      8     \u001b[36m1813.2742\u001b[0m     1707.7530  0.4164\n",
      "      9     1815.7978     \u001b[32m1707.3191\u001b[0m  0.4233\n",
      "     10     1813.7967     1707.4065  0.4154\n",
      "Restoring best model from epoch 7.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/pycox/evaluation/eval_surv.py:36: FutureWarning: is_monotonic is deprecated and will be removed in a future version. Use is_monotonic_increasing instead.\n",
      "  assert pd.Series(self.index_surv).is_monotonic\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concordance Index 0.6022286148330224\n",
      "Integrated Brier Score: 0.1982350939466629\n",
      "durations 3.0 2026.0\n",
      "Concordance Index 0.5964557206194817\n",
      "Integrated Brier Score: 0.19771296061445162\n",
      "(7098, 16) <class 'pandas.core.frame.DataFrame'>\n",
      "(7098,) <class 'pandas.core.series.Series'>\n",
      "(1775, 16) <class 'pandas.core.frame.DataFrame'>\n",
      "(1775,) <class 'pandas.core.series.Series'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/pycox/evaluation/eval_surv.py:36: FutureWarning: is_monotonic is deprecated and will be removed in a future version. Use is_monotonic_increasing instead.\n",
      "  assert pd.Series(self.index_surv).is_monotonic\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1     \u001b[36m1654.3765\u001b[0m     \u001b[32m1519.6995\u001b[0m  0.3524\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1     \u001b[36m1654.6725\u001b[0m     \u001b[32m1523.7416\u001b[0m  0.3603\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1     \u001b[36m1730.8246\u001b[0m     \u001b[32m1593.3090\u001b[0m  0.3562\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1     \u001b[36m2087.5662\u001b[0m     \u001b[32m1924.8121\u001b[0m  0.3583\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1     \u001b[36m2200.7100\u001b[0m     \u001b[32m2009.9210\u001b[0m  0.4210\n",
      "      2     \u001b[36m1620.5692\u001b[0m     1519.8488  0.3299\n",
      "      2     \u001b[36m1622.0004\u001b[0m     1524.3564  0.3623\n",
      "      2     \u001b[36m1700.0514\u001b[0m     \u001b[32m1592.9277\u001b[0m  0.3755\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m313.7460\u001b[0m      \u001b[32m304.6587\u001b[0m  0.8128\n",
      "      2     \u001b[36m2058.6622\u001b[0m     \u001b[32m1923.7812\u001b[0m  0.4052\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m317.6024\u001b[0m      \u001b[32m310.3114\u001b[0m  0.8511\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m412.1896\u001b[0m      \u001b[32m383.1714\u001b[0m  0.8632\n",
      "      2     \u001b[36m2170.4337\u001b[0m     \u001b[32m2007.0099\u001b[0m  0.4253\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m436.5515\u001b[0m      \u001b[32m400.4892\u001b[0m  0.8891\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m340.9923\u001b[0m      \u001b[32m324.1998\u001b[0m  0.9416\n",
      "      3     \u001b[36m1615.2461\u001b[0m     \u001b[32m1518.6050\u001b[0m  0.3491\n",
      "      3     \u001b[36m1618.0019\u001b[0m     \u001b[32m1522.6026\u001b[0m  0.3556\n",
      "      3     \u001b[36m1696.5878\u001b[0m     \u001b[32m1591.8063\u001b[0m  0.3574\n",
      "      3     \u001b[36m2053.7453\u001b[0m     1924.4564  0.3767\n",
      "      3     \u001b[36m2164.3233\u001b[0m     2007.1508  0.4211\n",
      "      4     \u001b[36m1614.8801\u001b[0m     \u001b[32m1515.7620\u001b[0m  0.3810\n",
      "      4     \u001b[36m1616.3738\u001b[0m     \u001b[32m1519.7375\u001b[0m  0.3701\n",
      "      4     \u001b[36m1694.7716\u001b[0m     \u001b[32m1589.7607\u001b[0m  0.3589\n",
      "      4     \u001b[36m2051.1134\u001b[0m     \u001b[32m1923.6058\u001b[0m  0.3713\n",
      "      2      \u001b[36m300.0100\u001b[0m      \u001b[32m303.5090\u001b[0m  0.7798\n",
      "      2      \u001b[36m301.7654\u001b[0m      \u001b[32m303.1209\u001b[0m  0.8491\n",
      "      4     \u001b[36m2164.0679\u001b[0m     \u001b[32m2006.7852\u001b[0m  0.4106\n",
      "      2      \u001b[36m315.5859\u001b[0m      \u001b[32m318.9824\u001b[0m  0.7938\n",
      "      2      \u001b[36m385.2487\u001b[0m      \u001b[32m379.8976\u001b[0m  0.8836\n",
      "      2      \u001b[36m405.8482\u001b[0m      \u001b[32m397.5848\u001b[0m  0.8528\n",
      "      5     \u001b[36m1612.8072\u001b[0m     \u001b[32m1514.7864\u001b[0m  0.3326\n",
      "      5     \u001b[36m1613.9037\u001b[0m     \u001b[32m1519.1324\u001b[0m  0.3618\n",
      "      5     \u001b[36m1692.3203\u001b[0m     \u001b[32m1589.0592\u001b[0m  0.3795\n",
      "      5     2051.2018     \u001b[32m1923.0659\u001b[0m  0.3820\n",
      "      6     1612.8305     1515.4126  0.3616\n",
      "      5     \u001b[36m2162.5926\u001b[0m     \u001b[32m2006.6154\u001b[0m  0.4260\n",
      "      6     1614.9776     1519.3442  0.3530\n",
      "      6     1692.6976     1589.1806  0.3981\n",
      "      6     \u001b[36m2050.1355\u001b[0m     \u001b[32m1923.0383\u001b[0m  0.4342\n",
      "      3      \u001b[36m296.3559\u001b[0m      \u001b[32m303.4378\u001b[0m  0.8160\n",
      "      7     \u001b[36m1609.8360\u001b[0m     \u001b[32m1514.4907\u001b[0m  0.3657\n",
      "      7     \u001b[36m1612.2217\u001b[0m     1519.6723  0.3167\n",
      "      6     \u001b[36m2159.5429\u001b[0m     2006.9712  0.3989\n",
      "      3      \u001b[36m311.4321\u001b[0m      \u001b[32m318.7412\u001b[0m  0.8196\n",
      "      3      \u001b[36m402.2874\u001b[0m      397.7302  0.8877\n",
      "      7     \u001b[36m1690.9923\u001b[0m     1589.2237  0.3880\n",
      "      3      \u001b[36m296.9061\u001b[0m      303.4872  0.9740\n",
      "      3      \u001b[36m380.1801\u001b[0m      \u001b[32m378.7083\u001b[0m  0.9899\n",
      "      7     \u001b[36m2047.8972\u001b[0m     \u001b[32m1922.4300\u001b[0m  0.3774\n",
      "      8     1610.7021     \u001b[32m1514.0282\u001b[0m  0.3625\n",
      "      8     \u001b[36m1611.9988\u001b[0m     1519.6031  0.3520\n",
      "      7     2160.8979     2006.8642  0.4335\n",
      "      8     \u001b[36m1690.9821\u001b[0m     \u001b[32m1588.3283\u001b[0m  0.4094\n",
      "      8     2048.5084     1922.6952  0.3682\n",
      "      9     1612.7292     1520.2484  0.3447\n",
      "      9     1612.5871     1514.5399  0.3576\n",
      "      4      \u001b[36m295.6014\u001b[0m      \u001b[32m302.3218\u001b[0m  0.8251\n",
      "      4      \u001b[36m310.5316\u001b[0m      \u001b[32m318.2235\u001b[0m  0.8035\n",
      "      8     \u001b[36m2159.0034\u001b[0m     2006.6355  0.4059\n",
      "      9     \u001b[36m1690.7116\u001b[0m     1588.3389  0.4045\n",
      "      4      \u001b[36m296.3109\u001b[0m      \u001b[32m303.0566\u001b[0m  0.8159\n",
      "      4      \u001b[36m401.0994\u001b[0m      \u001b[32m396.4805\u001b[0m  0.8627\n",
      "      9     2048.3864     1922.7374  0.4152\n",
      "     10     \u001b[36m1611.9387\u001b[0m     1519.7686  0.3401\n",
      "Restoring best model from epoch 5.\n",
      "     10     \u001b[36m1609.1150\u001b[0m     \u001b[32m1513.9720\u001b[0m  0.3641\n",
      "Restoring best model from epoch 8.\n",
      "      4      \u001b[36m379.0862\u001b[0m      379.4302  0.8555\n",
      "      9     2160.2063     2006.6235  0.3719\n",
      "     10     \u001b[36m1690.0858\u001b[0m     1589.1735  0.3204\n",
      "Restoring best model from epoch 8.\n",
      "     10     2048.1390     1922.9009  0.3324\n",
      "Restoring best model from epoch 7.\n",
      "      5      \u001b[36m294.2492\u001b[0m      \u001b[32m301.6800\u001b[0m  0.7374\n",
      "      5      \u001b[36m309.9019\u001b[0m      \u001b[32m318.1482\u001b[0m  0.7388\n",
      "     10     2160.6931     2006.7394  0.3407\n",
      "Restoring best model from epoch 4.\n",
      "      5      \u001b[36m295.5717\u001b[0m      \u001b[32m302.3675\u001b[0m  0.7121\n",
      "      5      \u001b[36m399.4593\u001b[0m      396.6823  0.7469\n",
      "      5      \u001b[36m378.7211\u001b[0m      378.9199  0.7268\n",
      "      6      \u001b[36m293.8768\u001b[0m      301.9660  0.6974\n",
      "      6      \u001b[36m309.0398\u001b[0m      318.3695  0.7004\n",
      "      6      \u001b[36m295.3289\u001b[0m      302.5279  0.7037\n",
      "      6      \u001b[36m398.5565\u001b[0m      396.9346  0.7416\n",
      "      6      \u001b[36m377.9237\u001b[0m      378.9084  0.7363\n",
      "      7      \u001b[36m293.7045\u001b[0m      \u001b[32m301.4676\u001b[0m  0.6910\n",
      "      7      \u001b[36m308.9383\u001b[0m      318.7115  0.6959\n",
      "      7      \u001b[36m294.4361\u001b[0m      \u001b[32m302.0373\u001b[0m  0.6972\n",
      "      7      \u001b[36m398.3606\u001b[0m      396.6947  0.7259\n",
      "      7      \u001b[36m377.8591\u001b[0m      379.1603  0.7263\n",
      "      8      \u001b[36m293.4699\u001b[0m      301.5277  0.6885\n",
      "      8      \u001b[36m308.1566\u001b[0m      318.5810  0.6946\n",
      "      8      \u001b[36m294.1101\u001b[0m      302.3595  0.6991\n",
      "      8      398.3627      396.5887  0.7351\n",
      "      8      \u001b[36m377.1779\u001b[0m      380.1473  0.7364\n",
      "      9      \u001b[36m293.0098\u001b[0m      301.6653  0.6885\n",
      "      9      308.7089      319.5050  0.6957\n",
      "      9      294.2820      302.5120  0.6958\n",
      "      9      \u001b[36m398.2086\u001b[0m      396.7007  0.7313\n",
      "      9      377.2892      379.9961  0.7267\n",
      "     10      293.0348      301.5878  0.6914\n",
      "Restoring best model from epoch 7.\n",
      "     10      308.6461      318.5664  0.6926\n",
      "Restoring best model from epoch 5.\n",
      "     10      \u001b[36m293.9567\u001b[0m      302.3452  0.6945\n",
      "Restoring best model from epoch 7.\n",
      "     10      \u001b[36m397.8348\u001b[0m      396.7827  0.7205\n",
      "Restoring best model from epoch 4.\n",
      "     10      \u001b[36m376.3503\u001b[0m      \u001b[32m378.5343\u001b[0m  0.7151\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m358.7938\u001b[0m      \u001b[32m339.2536\u001b[0m  0.7074\n",
      "      2      \u001b[36m342.7337\u001b[0m      \u001b[32m338.6944\u001b[0m  0.6985\n",
      "      3      \u001b[36m339.4845\u001b[0m      339.0059  0.7032\n",
      "      4      \u001b[36m338.1056\u001b[0m      \u001b[32m338.4397\u001b[0m  0.6988\n",
      "      5      \u001b[36m337.3077\u001b[0m      \u001b[32m338.2355\u001b[0m  0.6812\n",
      "      6      \u001b[36m337.1750\u001b[0m      338.3674  0.6877\n",
      "      7      \u001b[36m336.8068\u001b[0m      \u001b[32m338.0254\u001b[0m  0.6914\n",
      "      8      \u001b[36m336.7040\u001b[0m      \u001b[32m337.9568\u001b[0m  0.6892\n",
      "      9      336.7223      338.0201  0.6762\n",
      "     10      \u001b[36m336.3641\u001b[0m      338.2286  0.6790\n",
      "Restoring best model from epoch 8.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/pycox/evaluation/eval_surv.py:36: FutureWarning: is_monotonic is deprecated and will be removed in a future version. Use is_monotonic_increasing instead.\n",
      "  assert pd.Series(self.index_surv).is_monotonic\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concordance Index 0.5994081344585557\n",
      "Integrated Brier Score: 0.1962765548198142\n",
      "durations 3.0 2029.0\n",
      "Concordance Index 0.5858071847691031\n",
      "Integrated Brier Score: 0.19820621552043144\n",
      "(7099, 16) <class 'pandas.core.frame.DataFrame'>\n",
      "(7099,) <class 'pandas.core.series.Series'>\n",
      "(1774, 16) <class 'pandas.core.frame.DataFrame'>\n",
      "(1774,) <class 'pandas.core.series.Series'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/pycox/evaluation/eval_surv.py:36: FutureWarning: is_monotonic is deprecated and will be removed in a future version. Use is_monotonic_increasing instead.\n",
      "  assert pd.Series(self.index_surv).is_monotonic\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1     \u001b[36m1654.3526\u001b[0m     \u001b[32m1499.3692\u001b[0m  0.3417\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1     \u001b[36m1658.1699\u001b[0m     \u001b[32m1503.6561\u001b[0m  0.3483\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1     \u001b[36m1744.4500\u001b[0m     \u001b[32m1583.7843\u001b[0m  0.3373\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1     \u001b[36m2102.7757\u001b[0m     \u001b[32m1890.9436\u001b[0m  0.3745\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1     \u001b[36m2194.1981\u001b[0m     \u001b[32m2024.7054\u001b[0m  0.3940\n",
      "      2     \u001b[36m1626.7612\u001b[0m     \u001b[32m1496.3441\u001b[0m  0.3344\n",
      "      2     \u001b[36m1630.1081\u001b[0m     \u001b[32m1500.7872\u001b[0m  0.3542\n",
      "      2     \u001b[36m1714.7822\u001b[0m     \u001b[32m1582.0932\u001b[0m  0.3789\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m320.3470\u001b[0m      \u001b[32m305.2311\u001b[0m  0.7777\n",
      "      2     \u001b[36m2066.5329\u001b[0m     \u001b[32m1890.1211\u001b[0m  0.3440\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m314.7878\u001b[0m      \u001b[32m302.8643\u001b[0m  0.8172\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m334.7030\u001b[0m      \u001b[32m320.3330\u001b[0m  0.8261\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m422.1964\u001b[0m      \u001b[32m405.1536\u001b[0m  0.8266\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m404.5972\u001b[0m      \u001b[32m380.5924\u001b[0m  0.8507\n",
      "      2     \u001b[36m2158.0031\u001b[0m     \u001b[32m2022.8027\u001b[0m  0.3982\n",
      "      3     \u001b[36m1618.2617\u001b[0m     \u001b[32m1494.7269\u001b[0m  0.3577\n",
      "      3     \u001b[36m1623.1968\u001b[0m     \u001b[32m1499.7408\u001b[0m  0.3369\n",
      "      3     \u001b[36m1710.6814\u001b[0m     1582.2848  0.3299\n",
      "      3     \u001b[36m2063.7822\u001b[0m     1890.8449  0.3769\n",
      "      3     \u001b[36m2155.7403\u001b[0m     2023.1256  0.4232\n",
      "      4     \u001b[36m1622.4719\u001b[0m     1499.8543  0.3272\n",
      "      4     \u001b[36m1616.5658\u001b[0m     \u001b[32m1494.1321\u001b[0m  0.3724\n",
      "      4     \u001b[36m1709.9709\u001b[0m     \u001b[32m1581.9189\u001b[0m  0.3635\n",
      "      4     \u001b[36m2062.7706\u001b[0m     \u001b[32m1890.1143\u001b[0m  0.3964\n",
      "      2      \u001b[36m300.1807\u001b[0m      \u001b[32m299.8166\u001b[0m  0.7803\n",
      "      2      \u001b[36m300.5356\u001b[0m      \u001b[32m300.0349\u001b[0m  0.7923\n",
      "      2      \u001b[36m318.7464\u001b[0m      \u001b[32m316.7509\u001b[0m  0.7979\n",
      "      2      \u001b[36m385.8487\u001b[0m      \u001b[32m375.6561\u001b[0m  0.7952\n",
      "      2      \u001b[36m403.2265\u001b[0m      \u001b[32m403.0338\u001b[0m  0.8566\n",
      "      4     \u001b[36m2154.5870\u001b[0m     2023.6341  0.4144\n",
      "      5     \u001b[36m1621.3894\u001b[0m     \u001b[32m1498.7681\u001b[0m  0.3383\n",
      "      5     \u001b[36m1616.3485\u001b[0m     \u001b[32m1493.9561\u001b[0m  0.3487\n",
      "      5     \u001b[36m1709.0036\u001b[0m     \u001b[32m1580.8322\u001b[0m  0.3569\n",
      "      5     2063.7097     \u001b[32m1888.4167\u001b[0m  0.3660\n",
      "      6     \u001b[36m1619.4681\u001b[0m     \u001b[32m1498.1334\u001b[0m  0.3027\n",
      "      5     \u001b[36m2154.3457\u001b[0m     2023.7008  0.3643\n",
      "      6     \u001b[36m1613.9150\u001b[0m     \u001b[32m1493.8471\u001b[0m  0.3021\n",
      "      6     \u001b[36m1706.8992\u001b[0m     \u001b[32m1579.9803\u001b[0m  0.3488\n",
      "      6     \u001b[36m2060.5101\u001b[0m     \u001b[32m1887.8575\u001b[0m  0.3488\n",
      "      3      \u001b[36m296.5023\u001b[0m      \u001b[32m298.1560\u001b[0m  0.7945\n",
      "      7     \u001b[36m1617.9970\u001b[0m     1498.5874  0.3609\n",
      "      7     \u001b[36m1612.4713\u001b[0m     \u001b[32m1493.1116\u001b[0m  0.3428\n",
      "      3      \u001b[36m297.2518\u001b[0m      \u001b[32m299.0664\u001b[0m  0.8865\n",
      "      6     \u001b[36m2152.9701\u001b[0m     2023.0997  0.4023\n",
      "      7     \u001b[36m1705.3434\u001b[0m     1580.1523  0.3244\n",
      "      3      \u001b[36m400.2081\u001b[0m      403.7391  0.8316\n",
      "      3      \u001b[36m313.6331\u001b[0m      \u001b[32m316.0390\u001b[0m  0.9066\n",
      "      3      \u001b[36m381.8884\u001b[0m      \u001b[32m375.3786\u001b[0m  0.8926\n",
      "      7     \u001b[36m2059.2309\u001b[0m     1887.9934  0.3514\n",
      "      8     1613.5268     1493.1784  0.3180\n",
      "      8     \u001b[36m1617.5495\u001b[0m     \u001b[32m1498.0707\u001b[0m  0.3540\n",
      "      8     \u001b[36m1705.2621\u001b[0m     1580.2466  0.3330\n",
      "      7     \u001b[36m2152.3335\u001b[0m     \u001b[32m2021.7067\u001b[0m  0.3804\n",
      "      8     \u001b[36m2058.2581\u001b[0m     \u001b[32m1887.6997\u001b[0m  0.3751\n",
      "      9     1613.4184     \u001b[32m1493.0456\u001b[0m  0.3377\n",
      "      9     1618.6616     \u001b[32m1497.3738\u001b[0m  0.3498\n",
      "      9     1706.3628     \u001b[32m1579.6741\u001b[0m  0.3733\n",
      "      4      \u001b[36m294.9650\u001b[0m      298.1998  0.8007\n",
      "      4      \u001b[36m296.6786\u001b[0m      299.2040  0.7753\n",
      "      8     \u001b[36m2151.3851\u001b[0m     \u001b[32m2021.2438\u001b[0m  0.3838\n",
      "      9     2060.7174     \u001b[32m1887.5769\u001b[0m  0.3450\n",
      "      4      \u001b[36m313.0431\u001b[0m      \u001b[32m315.4974\u001b[0m  0.8204\n",
      "      4      \u001b[36m380.4999\u001b[0m      \u001b[32m374.9455\u001b[0m  0.8099\n",
      "      4      \u001b[36m399.3717\u001b[0m      \u001b[32m401.7634\u001b[0m  0.8452\n",
      "     10     \u001b[36m1610.8334\u001b[0m     1493.2399  0.3558\n",
      "Restoring best model from epoch 7.\n",
      "     10     \u001b[36m1617.3859\u001b[0m     1497.5829  0.3501\n",
      "Restoring best model from epoch 9.\n",
      "     10     \u001b[36m1704.6923\u001b[0m     \u001b[32m1579.6685\u001b[0m  0.3666\n",
      "Restoring best model from epoch 9.\n",
      "      9     2152.5459     2022.1764  0.3772\n",
      "     10     2059.2913     1887.8863  0.3789\n",
      "Restoring best model from epoch 9.\n",
      "     10     2152.5374     2022.3401  0.3333\n",
      "Restoring best model from epoch 8.\n",
      "      5      \u001b[36m295.6465\u001b[0m      \u001b[32m299.0104\u001b[0m  0.7553\n",
      "      5      \u001b[36m294.2061\u001b[0m      \u001b[32m297.2212\u001b[0m  0.7778\n",
      "      5      \u001b[36m311.6785\u001b[0m      \u001b[32m315.2601\u001b[0m  0.7420\n",
      "      5      \u001b[36m397.7103\u001b[0m      \u001b[32m401.1497\u001b[0m  0.7740\n",
      "      5      \u001b[36m379.4038\u001b[0m      \u001b[32m374.6324\u001b[0m  0.7852\n",
      "      6      \u001b[36m295.2732\u001b[0m      \u001b[32m298.6433\u001b[0m  0.6835\n",
      "      6      \u001b[36m294.1134\u001b[0m      297.3198  0.6844\n",
      "      6      \u001b[36m311.4609\u001b[0m      \u001b[32m315.2203\u001b[0m  0.6893\n",
      "      6      \u001b[36m378.3816\u001b[0m      374.8478  0.7161\n",
      "      6      \u001b[36m396.7494\u001b[0m      \u001b[32m400.7082\u001b[0m  0.7238\n",
      "      7      \u001b[36m294.6032\u001b[0m      \u001b[32m298.6035\u001b[0m  0.6776\n",
      "      7      \u001b[36m293.4794\u001b[0m      \u001b[32m296.5360\u001b[0m  0.6827\n",
      "      7      \u001b[36m310.6443\u001b[0m      315.5939  0.6877\n",
      "      7      \u001b[36m378.2077\u001b[0m      374.6955  0.7105\n",
      "      7      \u001b[36m396.3627\u001b[0m      \u001b[32m400.4652\u001b[0m  0.7186\n",
      "      8      \u001b[36m294.2071\u001b[0m      \u001b[32m298.3132\u001b[0m  0.6797\n",
      "      8      \u001b[36m293.3084\u001b[0m      296.7986  0.6798\n",
      "      8      \u001b[36m310.6318\u001b[0m      \u001b[32m315.1201\u001b[0m  0.6865\n",
      "      8      \u001b[36m377.8258\u001b[0m      \u001b[32m374.5206\u001b[0m  0.7116\n",
      "      8      \u001b[36m396.1538\u001b[0m      \u001b[32m400.0643\u001b[0m  0.7196\n",
      "      9      \u001b[36m293.9302\u001b[0m      \u001b[32m298.0699\u001b[0m  0.6802\n",
      "      9      \u001b[36m293.1571\u001b[0m      296.7144  0.6804\n",
      "      9      \u001b[36m310.3453\u001b[0m      315.3664  0.6869\n",
      "      9      \u001b[36m377.8187\u001b[0m      \u001b[32m374.2425\u001b[0m  0.7113\n",
      "      9      \u001b[36m395.7841\u001b[0m      400.2856  0.7191\n",
      "     10      294.0593      \u001b[32m298.0554\u001b[0m  0.6789\n",
      "Restoring best model from epoch 9.\n",
      "     10      \u001b[36m292.7148\u001b[0m      296.8348  0.6798\n",
      "Restoring best model from epoch 7.\n",
      "     10      \u001b[36m310.2280\u001b[0m      \u001b[32m315.0816\u001b[0m  0.6838\n",
      "     10      \u001b[36m377.5781\u001b[0m      \u001b[32m374.1501\u001b[0m  0.7075\n",
      "     10      395.9295      \u001b[32m399.9945\u001b[0m  0.7083\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1     \u001b[36m1850.1476\u001b[0m     \u001b[32m1692.9842\u001b[0m  0.4160\n",
      "      2     \u001b[36m1825.4328\u001b[0m     1693.5149  0.4108\n",
      "      3     \u001b[36m1824.0137\u001b[0m     \u001b[32m1692.5859\u001b[0m  0.4240\n",
      "      4     \u001b[36m1821.7366\u001b[0m     \u001b[32m1691.8892\u001b[0m  0.4207\n",
      "      5     \u001b[36m1818.3037\u001b[0m     1692.4601  0.4236\n",
      "      6     1819.8569     \u001b[32m1691.6816\u001b[0m  0.4220\n",
      "      7     1819.4503     \u001b[32m1691.1917\u001b[0m  0.4259\n",
      "      8     \u001b[36m1816.9033\u001b[0m     1691.7972  0.4173\n",
      "      9     1818.8465     \u001b[32m1690.8034\u001b[0m  0.4205\n",
      "     10     1818.3803     1690.8511  0.4174\n",
      "Restoring best model from epoch 9.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/pycox/evaluation/eval_surv.py:36: FutureWarning: is_monotonic is deprecated and will be removed in a future version. Use is_monotonic_increasing instead.\n",
      "  assert pd.Series(self.index_surv).is_monotonic\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concordance Index 0.5868860448721398\n",
      "Integrated Brier Score: 0.19866395689762062\n",
      "durations 3.0 2029.0\n",
      "Concordance Index 0.5967965232950506\n",
      "Integrated Brier Score: 0.1995458058993539\n",
      "(7099, 16) <class 'pandas.core.frame.DataFrame'>\n",
      "(7099,) <class 'pandas.core.series.Series'>\n",
      "(1774, 16) <class 'pandas.core.frame.DataFrame'>\n",
      "(1774,) <class 'pandas.core.series.Series'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/pycox/evaluation/eval_surv.py:36: FutureWarning: is_monotonic is deprecated and will be removed in a future version. Use is_monotonic_increasing instead.\n",
      "  assert pd.Series(self.index_surv).is_monotonic\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1     \u001b[36m1639.3902\u001b[0m     \u001b[32m1508.9588\u001b[0m  0.3510\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1     \u001b[36m1638.4174\u001b[0m     \u001b[32m1508.9292\u001b[0m  0.3437\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1     \u001b[36m2095.6031\u001b[0m     \u001b[32m1929.1237\u001b[0m  0.3689\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1     \u001b[36m2211.3889\u001b[0m     \u001b[32m2045.1043\u001b[0m  0.3799\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1     \u001b[36m1719.3617\u001b[0m     \u001b[32m1585.9002\u001b[0m  0.4193\n",
      "      2     \u001b[36m1607.8524\u001b[0m     1509.5791  0.3552\n",
      "      2     \u001b[36m1608.0039\u001b[0m     1509.3027  0.4345\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m316.3619\u001b[0m      \u001b[32m296.5996\u001b[0m  0.8431\n",
      "      2     \u001b[36m2058.9111\u001b[0m     1930.4034  0.3987\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m329.5438\u001b[0m      \u001b[32m299.4080\u001b[0m  0.8447\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m423.8699\u001b[0m      \u001b[32m407.2086\u001b[0m  0.8449\n",
      "      2     \u001b[36m2169.3463\u001b[0m     \u001b[32m2044.3562\u001b[0m  0.4076\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m409.1458\u001b[0m      \u001b[32m380.9628\u001b[0m  0.8830\n",
      "      2     \u001b[36m1689.3459\u001b[0m     \u001b[32m1585.7500\u001b[0m  0.4321\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m341.9801\u001b[0m      \u001b[32m314.6404\u001b[0m  0.9013\n",
      "      3     \u001b[36m1603.2969\u001b[0m     \u001b[32m1508.3711\u001b[0m  0.3432\n",
      "      3     \u001b[36m1603.9123\u001b[0m     \u001b[32m1508.8335\u001b[0m  0.3186\n",
      "      3     \u001b[36m2056.2724\u001b[0m     1930.3869  0.3772\n",
      "      3     \u001b[36m1686.7796\u001b[0m     \u001b[32m1584.6051\u001b[0m  0.3429\n",
      "      3     \u001b[36m2167.5361\u001b[0m     \u001b[32m2043.5060\u001b[0m  0.4039\n",
      "      4     \u001b[36m1602.1931\u001b[0m     \u001b[32m1506.2205\u001b[0m  0.3427\n",
      "      4     1604.0096     \u001b[32m1507.7546\u001b[0m  0.3737\n",
      "      4     \u001b[36m1685.0877\u001b[0m     \u001b[32m1583.2773\u001b[0m  0.3504\n",
      "      4     \u001b[36m2055.4132\u001b[0m     \u001b[32m1928.2835\u001b[0m  0.3875\n",
      "      2      \u001b[36m300.3169\u001b[0m      \u001b[32m295.6422\u001b[0m  0.8164\n",
      "      4     \u001b[36m2166.3712\u001b[0m     \u001b[32m2042.7421\u001b[0m  0.3852\n",
      "      2      \u001b[36m301.1499\u001b[0m      \u001b[32m297.0598\u001b[0m  0.8728\n",
      "      2      \u001b[36m316.9830\u001b[0m      \u001b[32m311.6481\u001b[0m  0.8120\n",
      "      5     \u001b[36m1600.5327\u001b[0m     \u001b[32m1504.6086\u001b[0m  0.3414\n",
      "      2      \u001b[36m403.3967\u001b[0m      \u001b[32m404.1819\u001b[0m  0.9014\n",
      "      2      \u001b[36m384.5206\u001b[0m      \u001b[32m378.9912\u001b[0m  0.8797\n",
      "      5     \u001b[36m1600.5122\u001b[0m     \u001b[32m1507.3254\u001b[0m  0.3541\n",
      "      5     \u001b[36m1681.7115\u001b[0m     \u001b[32m1582.9146\u001b[0m  0.3287\n",
      "      5     \u001b[36m2051.9551\u001b[0m     \u001b[32m1927.5412\u001b[0m  0.3876\n",
      "      5     \u001b[36m2164.9946\u001b[0m     \u001b[32m2042.6424\u001b[0m  0.3783\n",
      "      6     \u001b[36m1594.6749\u001b[0m     \u001b[32m1503.5065\u001b[0m  0.3577\n",
      "      6     \u001b[36m1599.2466\u001b[0m     \u001b[32m1506.8255\u001b[0m  0.3320\n",
      "      6     1681.9297     \u001b[32m1582.6371\u001b[0m  0.3523\n",
      "      6     2052.8886     \u001b[32m1927.1573\u001b[0m  0.3909\n",
      "      7     \u001b[36m1594.5288\u001b[0m     \u001b[32m1503.2229\u001b[0m  0.3358\n",
      "      6     \u001b[36m2164.5182\u001b[0m     2043.4810  0.4173\n",
      "      3      \u001b[36m296.0944\u001b[0m      296.2223  0.8611\n",
      "      7     \u001b[36m1596.6290\u001b[0m     \u001b[32m1506.4314\u001b[0m  0.3612\n",
      "      3      \u001b[36m297.3542\u001b[0m      \u001b[32m296.2964\u001b[0m  0.8741\n",
      "      3      \u001b[36m312.6123\u001b[0m      \u001b[32m311.3188\u001b[0m  0.8763\n",
      "      7     \u001b[36m1679.1176\u001b[0m     \u001b[32m1581.9518\u001b[0m  0.3823\n",
      "      3      \u001b[36m380.9366\u001b[0m      379.1007  0.8983\n",
      "      3      \u001b[36m401.4484\u001b[0m      \u001b[32m402.4489\u001b[0m  0.9045\n",
      "      7     \u001b[36m2049.4815\u001b[0m     1927.5481  0.3864\n",
      "      8     1595.4307     1503.3985  0.3552\n",
      "      7     \u001b[36m2163.9848\u001b[0m     2042.7872  0.4275\n",
      "      8     1599.2376     \u001b[32m1506.2480\u001b[0m  0.3902\n",
      "      8     1681.3223     1581.9866  0.3671\n",
      "      9     1595.8992     1503.5439  0.3566\n",
      "      8     2051.1233     1927.6024  0.4086\n",
      "      9     1598.4353     \u001b[32m1506.1518\u001b[0m  0.3147\n",
      "      8     \u001b[36m2163.9330\u001b[0m     \u001b[32m2042.2159\u001b[0m  0.4375\n",
      "      9     1680.6889     1582.0997  0.3388\n",
      "      4      \u001b[36m296.1502\u001b[0m      \u001b[32m296.2100\u001b[0m  0.8051\n",
      "      4      \u001b[36m311.8646\u001b[0m      \u001b[32m311.0283\u001b[0m  0.8377\n",
      "      4      \u001b[36m295.1624\u001b[0m      \u001b[32m295.3709\u001b[0m  0.9835\n",
      "     10     \u001b[36m1594.0406\u001b[0m     1503.4515  0.3729\n",
      "Restoring best model from epoch 7.\n",
      "      9     2051.2168     1927.2980  0.3694\n",
      "      4      \u001b[36m399.5004\u001b[0m      402.6452  0.9000\n",
      "     10     \u001b[36m1596.6208\u001b[0m     \u001b[32m1505.2123\u001b[0m  0.4524\n",
      "      4      \u001b[36m380.0682\u001b[0m      \u001b[32m378.1796\u001b[0m  1.0454\n",
      "     10     1679.6418     \u001b[32m1581.0164\u001b[0m  0.3993\n",
      "      9     2164.6421     \u001b[32m2042.0457\u001b[0m  0.4688\n",
      "     10     2049.8704     \u001b[32m1926.3239\u001b[0m  0.3897\n",
      "     10     \u001b[36m2162.7945\u001b[0m     \u001b[32m2041.4558\u001b[0m  0.3702\n",
      "      5      \u001b[36m295.4098\u001b[0m      296.2758  0.7709\n",
      "      5      \u001b[36m310.8409\u001b[0m      \u001b[32m310.4975\u001b[0m  0.7333\n",
      "      5      \u001b[36m294.7627\u001b[0m      295.4585  0.7731\n",
      "      5      \u001b[36m399.0570\u001b[0m      402.4757  0.8021\n",
      "      5      \u001b[36m378.7507\u001b[0m      \u001b[32m377.7339\u001b[0m  0.7732\n",
      "      6      \u001b[36m295.1465\u001b[0m      \u001b[32m295.6858\u001b[0m  0.7173\n",
      "      6      311.0784      \u001b[32m310.3755\u001b[0m  0.7227\n",
      "      6      \u001b[36m294.0015\u001b[0m      295.4196  0.7099\n",
      "      6      \u001b[36m398.5011\u001b[0m      402.8712  0.7494\n",
      "      6      378.8772      \u001b[32m377.3919\u001b[0m  0.7418\n",
      "      7      \u001b[36m294.6319\u001b[0m      \u001b[32m295.6349\u001b[0m  0.7002\n",
      "      7      \u001b[36m310.1017\u001b[0m      310.4930  0.6980\n",
      "      7      \u001b[36m293.6625\u001b[0m      295.6914  0.6935\n",
      "      7      \u001b[36m398.1146\u001b[0m      \u001b[32m402.3929\u001b[0m  0.7358\n",
      "      7      \u001b[36m378.0321\u001b[0m      \u001b[32m377.0438\u001b[0m  0.7332\n",
      "      8      \u001b[36m294.3174\u001b[0m      \u001b[32m295.3907\u001b[0m  0.6979\n",
      "      8      \u001b[36m309.8076\u001b[0m      310.5788  0.6999\n",
      "      8      293.7989      \u001b[32m295.2745\u001b[0m  0.7032\n",
      "      8      \u001b[36m397.4458\u001b[0m      \u001b[32m402.2672\u001b[0m  0.7526\n",
      "      8      \u001b[36m377.5738\u001b[0m      377.0787  0.7529\n",
      "      9      \u001b[36m294.1295\u001b[0m      295.6833  0.7053\n",
      "      9      310.0015      310.9369  0.7155\n",
      "      9      293.6839      \u001b[32m295.1142\u001b[0m  0.7074\n",
      "      9      \u001b[36m397.1660\u001b[0m      402.3549  0.7332\n",
      "      9      \u001b[36m377.4481\u001b[0m      \u001b[32m376.9044\u001b[0m  0.7345\n",
      "     10      294.1958      295.5760  0.6934\n",
      "Restoring best model from epoch 8.\n",
      "     10      \u001b[36m309.4190\u001b[0m      310.6672  0.6932\n",
      "Restoring best model from epoch 6.\n",
      "     10      \u001b[36m293.5376\u001b[0m      295.3099  0.6896\n",
      "Restoring best model from epoch 9.\n",
      "     10      397.2502      \u001b[32m402.1301\u001b[0m  0.7202\n",
      "     10      \u001b[36m377.2079\u001b[0m      377.0497  0.7173\n",
      "Restoring best model from epoch 9.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1     \u001b[36m1852.9570\u001b[0m     \u001b[32m1670.7243\u001b[0m  0.4297\n",
      "      2     \u001b[36m1828.2874\u001b[0m     \u001b[32m1668.8789\u001b[0m  0.4217\n",
      "      3     \u001b[36m1825.9210\u001b[0m     \u001b[32m1666.4176\u001b[0m  0.4101\n",
      "      4     \u001b[36m1823.8060\u001b[0m     \u001b[32m1665.5009\u001b[0m  0.4189\n",
      "      5     \u001b[36m1823.0747\u001b[0m     \u001b[32m1665.3805\u001b[0m  0.4189\n",
      "      6     1823.1536     1665.4230  0.4205\n",
      "      7     \u001b[36m1820.8734\u001b[0m     \u001b[32m1664.8933\u001b[0m  0.4233\n",
      "      8     \u001b[36m1819.9689\u001b[0m     1665.2026  0.4276\n",
      "      9     1820.3080     1665.3456  0.4153\n",
      "     10     \u001b[36m1819.0709\u001b[0m     1665.3031  0.4177\n",
      "Restoring best model from epoch 7.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/pycox/evaluation/eval_surv.py:36: FutureWarning: is_monotonic is deprecated and will be removed in a future version. Use is_monotonic_increasing instead.\n",
      "  assert pd.Series(self.index_surv).is_monotonic\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concordance Index 0.5918104177904105\n",
      "Integrated Brier Score: 0.19864858066171123\n",
      "durations 3.0 2029.0\n",
      "Concordance Index 0.5972852111690634\n",
      "Integrated Brier Score: 0.19577887728238652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/pycox/evaluation/eval_surv.py:36: FutureWarning: is_monotonic is deprecated and will be removed in a future version. Use is_monotonic_increasing instead.\n",
      "  assert pd.Series(self.index_surv).is_monotonic\n"
     ]
    }
   ],
   "source": [
    "data_set_fns = [load_metabric,  load_flchain, load_rgbsg, load_support] #, load_flchain, load_rgbsg, load_support, load_tcga]\n",
    "data_set_fns_str = ['load_metabric', 'load_flchain', 'load_rgbsg', 'load_support'] \n",
    "one_hot_dict = {'load_flchain': ['mgus'], 'load_support':['cancer'], 'load_rgbsg':['grade']}\n",
    "\n",
    "for idx, dataset in enumerate(data_set_fns):\n",
    "    # get name of current dataset\n",
    "    data = dataset(path=\"/Users/JUSC/Documents/xgbsurv/xgbsurv/datasets/data/\", as_frame=True)\n",
    "    X  = data.data #.astype(np.float32)\n",
    "    y = data.target #.values #.to_numpy()\n",
    "\n",
    "    print(data_set_fns_str[idx])\n",
    "    if data_set_fns_str[idx] in one_hot_dict.keys():\n",
    "        X = pd.get_dummies(X, columns=one_hot_dict[data_set_fns_str[idx]])\n",
    "    X, y = sort_X_y_pandas(X, y)\n",
    "    \n",
    "    net = NeuralNet(\n",
    "        SurvivalModel, \n",
    "        module__n_layers = 1,\n",
    "        module__input_units = X.shape[1],\n",
    "        #module__num_nodes = 32,\n",
    "        #module__dropout = 0.1, # these could also be removed\n",
    "        module__out_features = 1,\n",
    "        # for split sizes when result size = 1\n",
    "        iterator_train__drop_last=True,\n",
    "        #iterator_valid__drop_last=True,\n",
    "        criterion=EfronLoss,\n",
    "        optimizer=torch.optim.AdamW,\n",
    "        optimizer__weight_decay = 0.4,\n",
    "        batch_size=32, # separate train and valid->iterator_train__batch_size=128 and iterator_valid__batch_size=128 ?\n",
    "        callbacks=[\n",
    "            (\n",
    "                \"sched\",\n",
    "                LRScheduler(\n",
    "                    torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
    "                    monitor=\"valid_loss\",\n",
    "                    patience=5,\n",
    "                ),\n",
    "            ),\n",
    "            (\n",
    "                \"es\",\n",
    "                EarlyStopping(\n",
    "                    monitor=\"valid_loss\",\n",
    "                    patience=10,\n",
    "                    load_best=True,\n",
    "                ),\n",
    "            ),\n",
    "            (\"seed\", FixSeed(seed=42)),\n",
    "        ],\n",
    "        \n",
    "        #[EarlyStopping(patience=10)],\n",
    "        # add extensive callback, and random number seed\n",
    "        #TODO: enable stratification, verify\n",
    "        train_split=ValidSplit(0.2), # might cause lower performance in metrics, explain in thesis\n",
    "        #lr=0.001,\n",
    "        #max_epochs=1, #0,#100\n",
    "        #train_split=None,\n",
    "        verbose=1\n",
    "    )\n",
    "    best_model,params, outer_scores, best_preds_train, best_preds_test, X_train, X_test, y_train, y_test = train_eval(X, y, net, n_iter, data.filename)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TCGA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_breslow_tcga = {\n",
    "    'estimator__module__n_layers': [1, 2, 4],\n",
    "    'estimator__module__n_layers': [1, 2, 4],\n",
    "    'estimator__module__num_nodes': [64, 128, 256, 512],\n",
    "    'estimator__module__dropout': scuniform(0.0,0.7),\n",
    "    'estimator__optimizer__weight_decay': [0.4, 0.2, 0.1, 0.05, 0.02, 0.01, 0],\n",
    "    'estimator__batch_size': [64, 128, 256, 512, 1024],\n",
    "    #lr not in paper because of learning rate finder\n",
    "    # note: setting learning rate higher would make exp(partial_hazard) explode\n",
    "    #'estimator__lr': scloguniform(0.001,0.01), # scheduler unten einbauen\n",
    "    # use callback instead\n",
    "    'estimator__lr':[0.01],\n",
    "    #'estimator__max_epochs':  scrandint(10,20), # corresponds to num_rounds\n",
    "    'pca__n_components': [8, 10, 12, 14, 16]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_eval(X, y, net, n_iter, filename):\n",
    "        model = '_efron_'\n",
    "        dataset_name = filename.split('_')[0]\n",
    "        # add IBS later\n",
    "        outer_scores = {'cindex_train_'+dataset_name:[], 'cindex_test_'+dataset_name:[],\n",
    "                        'ibs_train_'+dataset_name:[], 'ibs_test_'+dataset_name:[]}\n",
    "        best_params = {'best_params_'+dataset_name:[]}\n",
    "        best_model = {'best_model_'+dataset_name:[]}\n",
    "        ct = make_column_transformer(\n",
    "                #(OneHotEncoder(sparse_output=False), make_column_selector(dtype_include=['category', 'object']))\n",
    "                (StandardScaler(), make_column_selector(dtype_include=['float32']))\n",
    "                ,remainder='drop')\n",
    "        pipe = Pipeline([('scaler',ct),\n",
    "                         ('pca', PCA()),#n_components=10\n",
    "                        ('estimator', net)])\n",
    "        rs = RandomizedSearchCV(pipe, param_grid_breslow_tcga, scoring = scoring_function, n_jobs=-1, \n",
    "                                    n_iter=2, refit=True)\n",
    "        for i, (train_index, test_index) in enumerate(outer_custom_cv.split(X, y)):\n",
    "                # Split data into training and testing sets for outer fold\n",
    "                X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "                y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "                X_train, y_train = sort_X_y_pandas(X_train, y_train)\n",
    "                X_test, y_test = sort_X_y_pandas(X_test, y_test)\n",
    "\n",
    "                #print(X_train.shape, type(X_train))\n",
    "                #print(y_train.shape, type(y_train))\n",
    "                #print(X_test.shape, type(X_test))\n",
    "                #print(y_test.shape, type(y_test))\n",
    "                # save splits and data\n",
    "                savetxt('splits/train_index_'+str(i)+'_'+filename, train_index, delimiter=',')\n",
    "                savetxt('splits/test_index_'+str(i)+'_'+filename, test_index, delimiter=',')\n",
    "                \n",
    "                savetxt('splits/X_train_'+str(i)+'_'+filename, X_train, delimiter=',')\n",
    "                savetxt('splits/X_test_'+str(i)+'_'+filename, X_test, delimiter=',')\n",
    "\n",
    "                savetxt('splits/y_train_'+str(i)+'_'+filename, y_train, delimiter=',')\n",
    "                savetxt('splits/y_test_'+str(i)+'_'+filename, y_test, delimiter=',')\n",
    "\n",
    "                strat = np.sign(y_train)\n",
    "                valid_split = ValidSplit(cv=0.1, stratified=strat, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                rs.fit(X_train, y_train)\n",
    "                best_preds_train = rs.best_estimator_.predict(X_train)\n",
    "                best_preds_test = rs.best_estimator_.predict(X_test)\n",
    "                # save hyperparameter settings\n",
    "                params = rs.best_estimator_.get_params\n",
    "                best_params['best_params_'+dataset_name] += [rs.best_params_]\n",
    "                best_model['best_model_'+dataset_name] += [params]\n",
    "                try:\n",
    "                    cum_hazard_train = get_cumulative_hazard_function_efron(\n",
    "                            X_train.values, X_train.values, y_train.values, y_train.values,\n",
    "                            best_preds_train.reshape(-1), best_preds_train.reshape(-1)\n",
    "                            )\n",
    "\n",
    "                    df_survival_train = np.exp(-cum_hazard_train)\n",
    "                    durations_train, events_train = transform_back(y_train.values)\n",
    "                    time_grid_train = np.linspace(durations_train.min(), durations_train.max(), 100)\n",
    "                    ev = EvalSurv(df_survival_train, durations_train, events_train, censor_surv='km')\n",
    "                    print('Concordance Index',ev.concordance_td('antolini'))\n",
    "                    print('Integrated Brier Score:',ev.integrated_brier_score(time_grid_train))\n",
    "                    cindex_score_train = ev.concordance_td('antolini')\n",
    "                    ibs_score_train = ev.integrated_brier_score(time_grid_train)\n",
    "\n",
    "                    outer_scores['cindex_train_'+dataset_name] += [cindex_score_train]\n",
    "                    outer_scores['ibs_train_'+dataset_name] += [ibs_score_train]\n",
    "\n",
    "                except:\n",
    "                    outer_scores['cindex_train_'+dataset_name] += [np.nan]\n",
    "                    outer_scores['ibs_train_'+dataset_name] += [np.nan]\n",
    "                    \n",
    "                try:\n",
    "                    cum_hazard_test = get_cumulative_hazard_function_efron(\n",
    "                            X_train.values, X_test.values, y_train.values, y_test.values,\n",
    "                            best_preds_train.reshape(-1), best_preds_test.reshape(-1)\n",
    "                            )\n",
    "                    df_survival_test = np.exp(-cum_hazard_test)\n",
    "                    durations_test, events_test = transform_back(y_test.values)\n",
    "                    print('durations',durations_test.min(), durations_test.max())\n",
    "                    time_grid_test = np.linspace(durations_test.min(), durations_test.max(), 100)\n",
    "                    ev = EvalSurv(df_survival_test, durations_test, events_test, censor_surv='km')\n",
    "                    print('Concordance Index',ev.concordance_td('antolini'))\n",
    "                    print('Integrated Brier Score:',ev.integrated_brier_score(time_grid_test))\n",
    "                    cindex_score_test = ev.concordance_td('antolini')\n",
    "                    ibs_score_test = ev.integrated_brier_score(time_grid_test)\n",
    "\n",
    "                    outer_scores['cindex_test_'+dataset_name] += [cindex_score_test]\n",
    "                    outer_scores['ibs_test_'+dataset_name] += [ibs_score_test]\n",
    "                except: \n",
    "                    outer_scores['cindex_test_'+dataset_name] += [np.nan]\n",
    "                    outer_scores['ibs_test_'+dataset_name] += [np.nan]\n",
    "            \n",
    "                df_best_params = pd.DataFrame(best_params)\n",
    "                df_best_model = pd.DataFrame(best_model)\n",
    "                df_outer_scores = pd.DataFrame(outer_scores)\n",
    "                df_metrics = pd.concat([df_best_params,df_best_model,df_outer_scores], axis=1)\n",
    "                df_metrics.to_csv('metrics/metric_summary'+model+str(i)+'_'+filename, index=False)\n",
    "        return best_model, best_params, outer_scores, best_preds_train, best_preds_test, X_train, X_test, y_train, y_test\n",
    "\n",
    "                \n",
    "#cv=inner_custom_cv,pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split gex_?|100130426      float32\n",
      "gex_?|100133144      float32\n",
      "gex_?|100134869      float32\n",
      "gex_?|10357          float32\n",
      "gex_?|10431          float32\n",
      "                      ...   \n",
      "gex_ZYG11A|440590    float32\n",
      "gex_ZYG11B|79699     float32\n",
      "gex_ZYX|7791         float32\n",
      "gex_ZZEF1|23140      float32\n",
      "gex_ZZZ3|26009       float32\n",
      "Length: 20531, dtype: object\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m111.4011\u001b[0m  0.0027\n",
      "      2      111.4011  0.0016\n",
      "      3      111.4011  0.0015\n",
      "      4      111.4011  0.0015\n",
      "      5      111.4011  0.0015\n",
      "      6      111.4011  0.0015\n",
      "      7      111.4011  0.0015\n",
      "      8      111.4011  0.0015\n",
      "      9      111.4011  0.0015\n",
      "     10      111.4011  0.0022\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m195.5269\u001b[0m  0.0041\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2      195.5269  0.0035\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m176.3496\u001b[0m  0.0039\n",
      "      3      195.5269  0.0019\n",
      "      2      176.3496  0.0017\n",
      "      4      195.5269  0.0016\n",
      "      3      176.3496  0.0016\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "      4      176.3496  0.0016\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m162.8707\u001b[0m  0.0067\n",
      "      5      195.5269  0.0032\n",
      "      5      176.3496  0.0015\n",
      "      2      162.8707  0.0020\n",
      "      6      176.3496  0.0015\n",
      "      3      162.8707  0.0016\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m205.7822\u001b[0m  0.0029\n",
      "      7      176.3496  0.0018\n",
      "      4      162.8707  0.0015\n",
      "      2      205.7822  0.0017\n",
      "      8      176.3496  0.0016\n",
      "      5      162.8707  0.0015\n",
      "      3      205.7822  0.0016\n",
      "      9      176.3496  0.0015\n",
      "      6      162.8707  0.0015\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      6      195.5269  0.0083\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      4      205.7822  0.0015\n",
      "     10      176.3496  0.0016\n",
      "Restoring best model from epoch 1.\n",
      "      7      195.5269  0.0018\n",
      "      5      205.7822  0.0015\n",
      "      7      162.8707  0.0029\n",
      "      8      195.5269  0.0016\n",
      "      8      162.8707  0.0016\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m110.1934\u001b[0m  0.0025\n",
      "      9      195.5269  0.0015\n",
      "      9      162.8707  0.0015\n",
      "      2      110.1934  0.0016\n",
      "     10      195.5269  0.0015\n",
      "Restoring best model from epoch 1.\n",
      "     10      162.8707  0.0015\n",
      "Restoring best model from epoch 1.\n",
      "      3      110.1934  0.0016\n",
      "      4      110.1934  0.0015\n",
      "      6      205.7822  0.0076\n",
      "      7      205.7822  0.0018\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      5      110.1934  0.0036\n",
      "      8      205.7822  0.0019\n",
      "      6      110.1934  0.0015\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      7      110.1934  0.0020\n",
      "      9      205.7822  0.0032\n",
      "     10      205.7822  0.0026\n",
      "Restoring best model from epoch 1.\n",
      "      8      110.1934  0.0035\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m194.8201\u001b[0m  0.0037\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      9      110.1934  0.0032\n",
      "      2      194.8201  0.0025\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      3      194.8201  0.0019\n",
      "     10      110.1934  0.0028\n",
      "Restoring best model from epoch 1.\n",
      "      4      194.8201  0.0022\n",
      "      5      194.8201  0.0015\n",
      "      6      194.8201  0.0015\n",
      "      7      194.8201  0.0025\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m100.1409\u001b[0m  0.0039\n",
      "      8      194.8201  0.0015\n",
      "      2      100.1409  0.0018\n",
      "      9      194.8201  0.0015\n",
      "      3      100.1409  0.0016\n",
      "     10      194.8201  0.0015\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      4      100.1409  0.0015\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      5      100.1409  0.0015\n",
      "      6      100.1409  0.0015\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m106.8826\u001b[0m  0.0024\n",
      "      7      100.1409  0.0015\n",
      "      2      106.8826  0.0016\n",
      "      8      100.1409  0.0015\n",
      "      3      106.8826  0.0015\n",
      "      9      100.1409  0.0015\n",
      "      4      106.8826  0.0015\n",
      "     10      100.1409  0.0015\n",
      "Restoring best model from epoch 1.\n",
      "      5      106.8826  0.0015\n",
      "      6      106.8826  0.0015\n",
      "      7      106.8826  0.0015\n",
      "      8      106.8826  0.0015\n",
      "      9      106.8826  0.0022\n",
      "     10      106.8826  0.0038\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m98.9209\u001b[0m  0.0026\n",
      "      2       98.9209  0.0018\n",
      "      3       98.9209  0.0016\n",
      "      4       98.9209  0.0015\n",
      "      5       98.9209  0.0015\n",
      "      6       98.9209  0.0015\n",
      "      7       98.9209  0.0018\n",
      "      8       98.9209  0.0017\n",
      "      9       98.9209  0.0016\n",
      "     10       98.9209  0.0049\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m102.7934\u001b[0m  0.0052\n",
      "      2      102.7934  0.0028\n",
      "      3      102.7934  0.0052\n",
      "      4      102.7934  0.0045\n",
      "      5      102.7934  0.0031\n",
      "      6      102.7934  0.0035\n",
      "      7      102.7934  0.0033\n",
      "      8      102.7934  0.0032\n",
      "      9      102.7934  0.0029\n",
      "     10      102.7934  0.0032\n",
      "Restoring best model from epoch 1.\n",
      "y_train breslow final [  -13.   -15.    19.   -20.    20.   -35.   -46.   -55.    56.   -59.\n",
      "    62.   -64.    65.   -67.   -68.    68.    69.    76.   -82.   -84.\n",
      "    88.   -89.    90.    92.    93.    98.    99.  -105.  -110.  -117.\n",
      "   118.   122.   128.  -129.   142.   144.   146.   149.   154.   154.\n",
      "  -158.   163.  -163.   168.   168.   173.   182.  -187.   191.   200.\n",
      "   206.   213.   220.  -224.   232.   237.   246.   248.   250.   251.\n",
      "  -251.   254.   254.   258.   259.   261.   262.   272.   272.   273.\n",
      "   274.  -276.   278.  -293.   294.   303.   311.   321.   324.   324.\n",
      "   328.  -330.   332.  -333.  -337.   344.   356.  -359.  -361.   364.\n",
      "  -364.  -365.  -366.  -368.  -368.  -369.   370.  -370.   370.  -372.\n",
      "  -372.  -373.  -376.  -377.  -379.  -382.  -383.  -384.  -384.   385.\n",
      "   386.  -389.   391.   393.  -394.  -398.  -398.   400.   406.   408.\n",
      "   413.  -415.  -415.  -416.   418.  -425.  -428.  -433.   434.   437.\n",
      "   453.   454.  -455.  -460.   460.  -466.  -466.   467.  -467.   467.\n",
      "  -469.   474.  -474.  -475.  -477.   486.   495.  -495.  -495.  -503.\n",
      "  -507.   508.  -508.   510.   510.  -512.   522.  -522.  -536.   536.\n",
      "  -536.  -539.  -540.   544.   544.  -546.   547.   547.   550.  -560.\n",
      "  -562.   565.  -573.   575.   577.  -578.  -578.  -580.  -581.  -582.\n",
      "  -588.   590.  -590.   599.  -603.  -610.   617.  -618.   623.   630.\n",
      "  -636.  -638.  -641.  -642.  -649.   651.  -665.   665.   674.   680.\n",
      "   685.   690.   696.  -700.   706.   712.   719.  -731.   734.   739.\n",
      "  -750.  -758.  -773.   778.  -789.   795.  -798.  -799.  -812.  -813.\n",
      "   819.  -820.   823.  -832.  -832.  -842.   859.   864.  -864.  -887.\n",
      "  -893.  -897.   904.  -906.  -921.   941.  -945.   974.  -997.  1004.\n",
      "  1005.  1008. -1029.  1036. -1048.  1077. -1090. -1094. -1108. -1110.\n",
      " -1115. -1127.  1163. -1174. -1181.  1270. -1326.  1348. -1350. -1370.\n",
      "  1420.  1423. -1429. -1454. -1455. -1529. -1538. -1542.  1556. -1582.\n",
      " -1604. -1621. -1639. -1649.  1670. -1708. -1714.  1718. -1761. -1792.\n",
      "  1804. -1806. -1830. -1845.  1869. -1884. -1912. -1947. -1949. -1952.\n",
      "  1971. -2009.  2020. -2020. -2024. -2027. -2044. -2049. -2139. -2177.\n",
      " -2312. -2330. -2380. -2423. -2625.  2641. -2656. -2703. -2790.  2828.\n",
      " -2868. -2886.  2954. -2964. -3011.  3183. -3314. -3364. -3432. -3817.\n",
      " -3981. -4967. -5041. -5050.]\n",
      "Concordance Index 0.5380360632766695\n",
      "Integrated Brier Score: 0.370768670807669\n",
      "y_train breslow final [  -13.   -15.    19.   -20.    20.   -35.   -46.   -55.    56.   -59.\n",
      "    62.   -64.    65.   -67.   -68.    68.    69.    76.   -82.   -84.\n",
      "    88.   -89.    90.    92.    93.    98.    99.  -105.  -110.  -117.\n",
      "   118.   122.   128.  -129.   142.   144.   146.   149.   154.   154.\n",
      "  -158.   163.  -163.   168.   168.   173.   182.  -187.   191.   200.\n",
      "   206.   213.   220.  -224.   232.   237.   246.   248.   250.   251.\n",
      "  -251.   254.   254.   258.   259.   261.   262.   272.   272.   273.\n",
      "   274.  -276.   278.  -293.   294.   303.   311.   321.   324.   324.\n",
      "   328.  -330.   332.  -333.  -337.   344.   356.  -359.  -361.   364.\n",
      "  -364.  -365.  -366.  -368.  -368.  -369.   370.  -370.   370.  -372.\n",
      "  -372.  -373.  -376.  -377.  -379.  -382.  -383.  -384.  -384.   385.\n",
      "   386.  -389.   391.   393.  -394.  -398.  -398.   400.   406.   408.\n",
      "   413.  -415.  -415.  -416.   418.  -425.  -428.  -433.   434.   437.\n",
      "   453.   454.  -455.  -460.   460.  -466.  -466.   467.  -467.   467.\n",
      "  -469.   474.  -474.  -475.  -477.   486.   495.  -495.  -495.  -503.\n",
      "  -507.   508.  -508.   510.   510.  -512.   522.  -522.  -536.   536.\n",
      "  -536.  -539.  -540.   544.   544.  -546.   547.   547.   550.  -560.\n",
      "  -562.   565.  -573.   575.   577.  -578.  -578.  -580.  -581.  -582.\n",
      "  -588.   590.  -590.   599.  -603.  -610.   617.  -618.   623.   630.\n",
      "  -636.  -638.  -641.  -642.  -649.   651.  -665.   665.   674.   680.\n",
      "   685.   690.   696.  -700.   706.   712.   719.  -731.   734.   739.\n",
      "  -750.  -758.  -773.   778.  -789.   795.  -798.  -799.  -812.  -813.\n",
      "   819.  -820.   823.  -832.  -832.  -842.   859.   864.  -864.  -887.\n",
      "  -893.  -897.   904.  -906.  -921.   941.  -945.   974.  -997.  1004.\n",
      "  1005.  1008. -1029.  1036. -1048.  1077. -1090. -1094. -1108. -1110.\n",
      " -1115. -1127.  1163. -1174. -1181.  1270. -1326.  1348. -1350. -1370.\n",
      "  1420.  1423. -1429. -1454. -1455. -1529. -1538. -1542.  1556. -1582.\n",
      " -1604. -1621. -1639. -1649.  1670. -1708. -1714.  1718. -1761. -1792.\n",
      "  1804. -1806. -1830. -1845.  1869. -1884. -1912. -1947. -1949. -1952.\n",
      "  1971. -2009.  2020. -2020. -2024. -2027. -2044. -2049. -2139. -2177.\n",
      " -2312. -2330. -2380. -2423. -2625.  2641. -2656. -2703. -2790.  2828.\n",
      " -2868. -2886.  2954. -2964. -3011.  3183. -3314. -3364. -3432. -3817.\n",
      " -3981. -4967. -5041. -5050.]\n",
      "durations 17.0 4343.0\n",
      "Concordance Index 0.5524861878453039\n",
      "Integrated Brier Score: 0.33220508277938676\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m259.9547\u001b[0m       \u001b[32m69.7718\u001b[0m  0.0175\n",
      "      2      \u001b[36m258.1624\u001b[0m       70.5339  0.0156\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      3      266.6395       71.9736  0.0149\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m198.0335\u001b[0m      \u001b[32m127.5266\u001b[0m  0.0127\n",
      "      2      \u001b[36m192.7317\u001b[0m       \u001b[32m93.3313\u001b[0m  0.0120\n",
      "      4      \u001b[36m253.7364\u001b[0m       73.5777  0.0229\n",
      "      3      \u001b[36m179.4439\u001b[0m       \u001b[32m86.5549\u001b[0m  0.0110\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      4      \u001b[36m175.4792\u001b[0m       \u001b[32m83.7945\u001b[0m  0.0107\n",
      "      5      259.0532       74.5869  0.0157\n",
      "      5      \u001b[36m171.7268\u001b[0m       \u001b[32m83.7625\u001b[0m  0.0104\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      6      262.0086       74.7023  0.0137\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      6      \u001b[36m167.7424\u001b[0m       87.7789  0.0104\n",
      "      7      257.7091       73.6052  0.0164\n",
      "      7      \u001b[36m164.9112\u001b[0m       89.4994  0.0103\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m205.9205\u001b[0m       \u001b[32m96.6450\u001b[0m  0.0281\n",
      "      8      166.6582       88.8593  0.0155\n",
      "      8      258.2928       72.8502  0.0171\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m235.9228\u001b[0m       \u001b[32m74.5057\u001b[0m  0.0412\n",
      "      9      \u001b[36m161.0076\u001b[0m       87.9405  0.0150\n",
      "      2      \u001b[36m191.7894\u001b[0m      100.1932  0.0274\n",
      "      9      261.2443       72.2289  0.0177\n",
      "     10      \u001b[36m159.6946\u001b[0m       89.0397  0.0149\n",
      "Restoring best model from epoch 5.\n",
      "      2      \u001b[36m227.3885\u001b[0m       75.0613  0.0214\n",
      "     10      \u001b[36m244.6528\u001b[0m       71.9091  0.0158\n",
      "Restoring best model from epoch 1.\n",
      "      3      193.1011      101.0114  0.0206\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      3      230.8223       75.2560  0.0142\n",
      "      4      \u001b[36m188.4686\u001b[0m       99.5902  0.0170\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m223.1638\u001b[0m       \u001b[32m95.9267\u001b[0m  0.0151\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m267.7611\u001b[0m       \u001b[32m93.5881\u001b[0m  0.0117\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.      4      \u001b[36m225.2548\u001b[0m       \u001b[32m74.1215\u001b[0m  0.0128\n",
      "\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2      226.4313       \u001b[32m73.1407\u001b[0m  0.0112\n",
      "      2      \u001b[36m258.0912\u001b[0m       \u001b[32m81.4993\u001b[0m  0.0110\n",
      "      5      234.0999       \u001b[32m73.1724\u001b[0m  0.0154\n",
      "      5      \u001b[36m185.4634\u001b[0m       98.3804  0.0215\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m218.4302\u001b[0m       \u001b[32m92.9375\u001b[0m  0.0155\n",
      "      3      \u001b[36m205.9950\u001b[0m       \u001b[32m72.4172\u001b[0m  0.0099\n",
      "      3      \u001b[36m243.3802\u001b[0m       \u001b[32m77.3408\u001b[0m  0.0142\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m193.2448\u001b[0m       \u001b[32m88.4039\u001b[0m  0.0222\n",
      "      6      \u001b[36m225.2074\u001b[0m       \u001b[32m72.9158\u001b[0m  0.0130\n",
      "      2      \u001b[36m212.6038\u001b[0m       \u001b[32m92.5580\u001b[0m  0.0133\n",
      "      4      217.5901       \u001b[32m70.9599\u001b[0m  0.0108\n",
      "      6      192.6546       \u001b[32m94.7046\u001b[0m  0.0174\n",
      "      4      243.8724       \u001b[32m74.0861\u001b[0m  0.0108\n",
      "      2      \u001b[36m183.8930\u001b[0m       94.1482  0.0136\n",
      "      7      \u001b[36m220.5805\u001b[0m       \u001b[32m72.6709\u001b[0m  0.0134\n",
      "      3      220.7151       94.2132  0.0132\n",
      "      5      \u001b[36m204.6544\u001b[0m       \u001b[32m70.5159\u001b[0m  0.0104\n",
      "      5      \u001b[36m234.2200\u001b[0m       \u001b[32m72.1133\u001b[0m  0.0126\n",
      "      3      189.4635       92.3883  0.0119\n",
      "      8      \u001b[36m217.2740\u001b[0m       \u001b[32m72.6135\u001b[0m  0.0123\n",
      "      6      \u001b[36m204.4673\u001b[0m       72.4652  0.0098\n",
      "      7      \u001b[36m185.4215\u001b[0m       \u001b[32m91.5873\u001b[0m  0.0218\n",
      "      4      224.3950       \u001b[32m91.3287\u001b[0m  0.0120\n",
      "      6      \u001b[36m232.2081\u001b[0m       72.8804  0.0103\n",
      "      7      \u001b[36m199.6557\u001b[0m       74.7855  0.0100\n",
      "      4      \u001b[36m183.4728\u001b[0m       \u001b[32m88.3638\u001b[0m  0.0133\n",
      "      9      219.4993       \u001b[32m72.5729\u001b[0m  0.0130\n",
      "      7      234.2918       74.2472  0.0107\n",
      "      5      \u001b[36m211.1778\u001b[0m       \u001b[32m88.8435\u001b[0m  0.0121\n",
      "      8      188.0296       \u001b[32m89.1955\u001b[0m  0.0134\n",
      "      8      \u001b[36m196.8039\u001b[0m       76.2519  0.0114\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      5      \u001b[36m173.6392\u001b[0m       \u001b[32m84.4091\u001b[0m  0.0124\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      8      \u001b[36m229.4073\u001b[0m       76.0200  0.0114\n",
      "     10      218.0745       72.6551  0.0129\n",
      "Restoring best model from epoch 9.\n",
      "      6      226.6103       \u001b[32m87.0037\u001b[0m  0.0125\n",
      "      9      189.0055       \u001b[32m86.0651\u001b[0m  0.0139\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      9      \u001b[36m193.1768\u001b[0m       76.9571  0.0101\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      9      \u001b[36m226.0729\u001b[0m       77.3399  0.0109\n",
      "      6      181.9592       \u001b[32m81.6362\u001b[0m  0.0134\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m223.1333\u001b[0m      \u001b[32m106.1569\u001b[0m  0.0133\n",
      "     10      \u001b[36m192.0096\u001b[0m       77.5873  0.0101\n",
      "Restoring best model from epoch 5.\n",
      "     10      \u001b[36m184.7851\u001b[0m       \u001b[32m84.6746\u001b[0m  0.0129\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m190.2452\u001b[0m       \u001b[32m88.9124\u001b[0m  0.0114\n",
      "      7      \u001b[36m210.1791\u001b[0m       \u001b[32m84.9000\u001b[0m  0.0176\n",
      "     10      228.2504       77.9806  0.0107\n",
      "Restoring best model from epoch 5.\n",
      "      7      176.5968       \u001b[32m81.2679\u001b[0m  0.0122\n",
      "      2      \u001b[36m207.2733\u001b[0m       \u001b[32m99.0798\u001b[0m  0.0157\n",
      "      8      215.0400       \u001b[32m83.5560\u001b[0m  0.0125\n",
      "      2      \u001b[36m173.6115\u001b[0m       \u001b[32m88.2950\u001b[0m  0.0151\n",
      "      8      \u001b[36m173.5422\u001b[0m       81.4406  0.0124\n",
      "      3      \u001b[36m203.0688\u001b[0m       \u001b[32m96.0242\u001b[0m  0.0103\n",
      "      3      \u001b[36m171.1027\u001b[0m       88.7177  0.0102\n",
      "      9      220.4776       \u001b[32m82.8561\u001b[0m  0.0118\n",
      "      4      \u001b[36m191.7651\u001b[0m       \u001b[32m92.0531\u001b[0m  0.0105\n",
      "      4      \u001b[36m162.5848\u001b[0m       89.1275  0.0101\n",
      "     10      \u001b[36m205.3933\u001b[0m       83.1246  0.0117\n",
      "Restoring best model from epoch 9.\n",
      "      9      \u001b[36m164.4710\u001b[0m       82.5870  0.0215\n",
      "      5      \u001b[36m189.0367\u001b[0m       \u001b[32m90.4123\u001b[0m  0.0099\n",
      "      5      \u001b[36m160.0115\u001b[0m       88.8989  0.0103\n",
      "      6      196.8790       \u001b[32m89.7938\u001b[0m  0.0115\n",
      "     10      178.7919       83.6869  0.0150\n",
      "Restoring best model from epoch 7.\n",
      "      6      \u001b[36m154.5978\u001b[0m       90.5874  0.0094\n",
      "      7      \u001b[36m154.5058\u001b[0m       93.1033  0.0093\n",
      "      7      191.1439       90.9111  0.0113\n",
      "      8      \u001b[36m148.1745\u001b[0m       98.0303  0.0106\n",
      "      8      \u001b[36m187.8833\u001b[0m       90.0651  0.0110\n",
      "      9      150.5821      103.9411  0.0111\n",
      "      9      \u001b[36m184.5344\u001b[0m       \u001b[32m89.6975\u001b[0m  0.0106\n",
      "     10      \u001b[36m144.7250\u001b[0m      106.9980  0.0100\n",
      "Restoring best model from epoch 2.\n",
      "     10      \u001b[36m181.9090\u001b[0m       89.8615  0.0114\n",
      "Restoring best model from epoch 9.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m248.6980\u001b[0m       \u001b[32m74.6826\u001b[0m  0.0253\n",
      "      2      \u001b[36m238.2985\u001b[0m       \u001b[32m69.9292\u001b[0m  0.0244\n",
      "      3      \u001b[36m235.6570\u001b[0m       70.4634  0.0250\n",
      "      4      \u001b[36m229.6257\u001b[0m       70.4203  0.0248\n",
      "      5      234.1897       70.9055  0.0232\n",
      "      6      234.9981       71.7709  0.0235\n",
      "      7      \u001b[36m227.6334\u001b[0m       72.1672  0.0237\n",
      "      8      229.7929       72.9959  0.0225\n",
      "      9      233.5055       73.4986  0.0253\n",
      "     10      228.3308       73.5001  0.0232\n",
      "Restoring best model from epoch 2.\n",
      "y_train breslow final [  -13.   -17.    19.   -20.   -28.   -35.   -37.   -46.   -55.    56.\n",
      "    56.    57.   -59.   -64.   -64.    65.   -67.   -68.    68.    69.\n",
      "    76.    81.   -82.   -84.   -89.    90.    92.    93.   -95.  -105.\n",
      "   106.  -110.  -117.   118.   122.   128.  -129.   131.   149.   154.\n",
      "  -158.   163.  -163.   168.   168.   182.  -187.  -189.   191.   200.\n",
      "   205.   206.   211.   213.   216.   220.   223.  -224.   232.   232.\n",
      "   237.   246.   248.   250.   251.  -251.   253.   254.   254.   259.\n",
      "   261.   272.   272.   272.   273.   274.  -276.   278.   294.   294.\n",
      "   311.   321.   324.   324.  -330.   332.  -337.   340.   344.   344.\n",
      "  -345.  -345.   356.  -359.  -361.   362.  -364.  -366.  -368.  -368.\n",
      "  -369.   370.  -372.  -372.  -373.  -377.  -382.  -383.  -384.   385.\n",
      "   385.   386.   388.   393.  -394.  -398.  -398.  -399.   406.  -410.\n",
      "   413.   413.   413.   415.  -415.  -416.   418.  -423.  -425.  -428.\n",
      "  -428.   453.   455.  -457.   460.  -466.  -466.   467.  -467.   467.\n",
      "   474.  -474.  -475.  -477.  -480.  -481.  -484.  -485.   486.  -491.\n",
      "   492.   495.  -495.  -495.  -503.   508.  -508.   510.  -512.  -524.\n",
      "   530.  -536.   536.  -536.   539.  -540.  -542.   544.   544.  -546.\n",
      "   547.   550.  -560.  -562.   565.   565.  -572.  -572.   577.  -578.\n",
      "   579.  -580.  -581.  -582.   590.   593.   599.   602.  -603.  -610.\n",
      "   612.   615.  -618.   623.   630.  -633.  -636.  -640.  -641.  -642.\n",
      "  -646.  -648.  -649.   651.  -665.   665.   674.   680.   685.   690.\n",
      "  -691.   696.  -699.  -700.   706.   712.   719.  -731.   734.  -750.\n",
      "  -758.  -761.  -773.   778.  -783.  -789.   795.  -799.  -812.  -813.\n",
      "   819.   823.  -832.   835.  -840.  -842.  -851.   864.  -864.  -873.\n",
      "  -887.  -893.  -899.  -906.   941.  -945.   949.   974.  -997. -1003.\n",
      "  1005.  1008. -1029.  1036. -1048.  1064. -1072.  1077. -1090. -1108.\n",
      " -1110. -1115. -1127. -1186. -1219.  1270. -1326.  1348.  1367.  1420.\n",
      "  1423. -1429. -1454. -1460. -1522. -1538. -1542.  1556. -1561. -1582.\n",
      " -1621. -1639. -1649. -1708.  1718. -1792.  1804. -1830. -1845.  1869.\n",
      " -1884. -1912. -2008.  2020. -2020. -2024. -2027. -2044. -2049. -2109.\n",
      " -2139. -2177. -2293. -2330. -2380. -2423. -2625.  2641. -2656. -2703.\n",
      "  2828. -2868.  2954. -2964. -3011.  3183. -3314. -3364. -3420. -3432.\n",
      " -3981. -4343. -4967. -5041. -5050.]\n",
      "Concordance Index 0.63643108885179\n",
      "Integrated Brier Score: 0.19269004901035783\n",
      "y_train breslow final [  -13.   -17.    19.   -20.   -28.   -35.   -37.   -46.   -55.    56.\n",
      "    56.    57.   -59.   -64.   -64.    65.   -67.   -68.    68.    69.\n",
      "    76.    81.   -82.   -84.   -89.    90.    92.    93.   -95.  -105.\n",
      "   106.  -110.  -117.   118.   122.   128.  -129.   131.   149.   154.\n",
      "  -158.   163.  -163.   168.   168.   182.  -187.  -189.   191.   200.\n",
      "   205.   206.   211.   213.   216.   220.   223.  -224.   232.   232.\n",
      "   237.   246.   248.   250.   251.  -251.   253.   254.   254.   259.\n",
      "   261.   272.   272.   272.   273.   274.  -276.   278.   294.   294.\n",
      "   311.   321.   324.   324.  -330.   332.  -337.   340.   344.   344.\n",
      "  -345.  -345.   356.  -359.  -361.   362.  -364.  -366.  -368.  -368.\n",
      "  -369.   370.  -372.  -372.  -373.  -377.  -382.  -383.  -384.   385.\n",
      "   385.   386.   388.   393.  -394.  -398.  -398.  -399.   406.  -410.\n",
      "   413.   413.   413.   415.  -415.  -416.   418.  -423.  -425.  -428.\n",
      "  -428.   453.   455.  -457.   460.  -466.  -466.   467.  -467.   467.\n",
      "   474.  -474.  -475.  -477.  -480.  -481.  -484.  -485.   486.  -491.\n",
      "   492.   495.  -495.  -495.  -503.   508.  -508.   510.  -512.  -524.\n",
      "   530.  -536.   536.  -536.   539.  -540.  -542.   544.   544.  -546.\n",
      "   547.   550.  -560.  -562.   565.   565.  -572.  -572.   577.  -578.\n",
      "   579.  -580.  -581.  -582.   590.   593.   599.   602.  -603.  -610.\n",
      "   612.   615.  -618.   623.   630.  -633.  -636.  -640.  -641.  -642.\n",
      "  -646.  -648.  -649.   651.  -665.   665.   674.   680.   685.   690.\n",
      "  -691.   696.  -699.  -700.   706.   712.   719.  -731.   734.  -750.\n",
      "  -758.  -761.  -773.   778.  -783.  -789.   795.  -799.  -812.  -813.\n",
      "   819.   823.  -832.   835.  -840.  -842.  -851.   864.  -864.  -873.\n",
      "  -887.  -893.  -899.  -906.   941.  -945.   949.   974.  -997. -1003.\n",
      "  1005.  1008. -1029.  1036. -1048.  1064. -1072.  1077. -1090. -1108.\n",
      " -1110. -1115. -1127. -1186. -1219.  1270. -1326.  1348.  1367.  1420.\n",
      "  1423. -1429. -1454. -1460. -1522. -1538. -1542.  1556. -1561. -1582.\n",
      " -1621. -1639. -1649. -1708.  1718. -1792.  1804. -1830. -1845.  1869.\n",
      " -1884. -1912. -2008.  2020. -2020. -2024. -2027. -2044. -2049. -2109.\n",
      " -2139. -2177. -2293. -2330. -2380. -2423. -2625.  2641. -2656. -2703.\n",
      "  2828. -2868.  2954. -2964. -3011.  3183. -3314. -3364. -3420. -3432.\n",
      " -3981. -4343. -4967. -5041. -5050.]\n",
      "durations 15.0 3817.0\n",
      "Concordance Index 0.5745494265428728\n",
      "Integrated Brier Score: 0.28091360229487494\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m242.4409\u001b[0m       \u001b[32m47.3536\u001b[0m  0.0151\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m214.0191\u001b[0m       \u001b[32m64.3211\u001b[0m  0.0115\n",
      "      2      244.6859       \u001b[32m45.8469\u001b[0m  0.0147\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2      \u001b[36m205.6922\u001b[0m       65.8552  0.0114\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m249.3285\u001b[0m       \u001b[32m79.6212\u001b[0m  0.0145\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m251.4267\u001b[0m       \u001b[32m52.4156\u001b[0m  0.0169\n",
      "      3      245.9580       46.6068  0.0142\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m233.4898\u001b[0m       \u001b[32m66.2672\u001b[0m  0.0126\n",
      "      3      \u001b[36m198.7411\u001b[0m       \u001b[32m62.4336\u001b[0m  0.0122\n",
      "      2      \u001b[36m246.7041\u001b[0m       \u001b[32m78.1288\u001b[0m  0.0137\n",
      "      2      256.4211       \u001b[32m50.5383\u001b[0m  0.0135\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2      243.2268       \u001b[32m57.4711\u001b[0m  0.0108\n",
      "      4      \u001b[36m193.8358\u001b[0m       \u001b[32m59.7473\u001b[0m  0.0102\n",
      "      4      \u001b[36m231.0373\u001b[0m       48.0133  0.0197\n",
      "      3      \u001b[36m239.5230\u001b[0m       \u001b[32m76.1259\u001b[0m  0.0137\n",
      "      3      260.6987       50.9903  0.0130\n",
      "      3      \u001b[36m215.8020\u001b[0m       \u001b[32m55.6735\u001b[0m  0.0101\n",
      "      5      200.1050       \u001b[32m59.0422\u001b[0m  0.0097\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m206.3226\u001b[0m       \u001b[32m61.8852\u001b[0m  0.0153\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      5      237.0893       48.8019  0.0162\n",
      "      4      \u001b[36m234.1285\u001b[0m       \u001b[32m73.6566\u001b[0m  0.0128\n",
      "      4      254.5029       50.5867  0.0125\n",
      "      4      \u001b[36m210.8071\u001b[0m       \u001b[32m55.3182\u001b[0m  0.0109\n",
      "      6      \u001b[36m187.0031\u001b[0m       60.1929  0.0152\n",
      "      2      210.8488       63.2138  0.0145\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m250.5922\u001b[0m       \u001b[32m53.3373\u001b[0m  0.0107\n",
      "      5      \u001b[36m209.1614\u001b[0m       55.9705  0.0102\n",
      "      6      234.9049       48.9490  0.0129\n",
      "      5      \u001b[36m223.7644\u001b[0m       \u001b[32m72.3131\u001b[0m  0.0141\n",
      "      5      \u001b[36m241.2218\u001b[0m       50.7047  0.0149\n",
      "      7      187.7895       61.5110  0.0108\n",
      "      3      218.4715       65.2367  0.0126\n",
      "      2      \u001b[36m244.8129\u001b[0m       \u001b[32m50.7309\u001b[0m  0.0128\n",
      "      6      \u001b[36m206.3689\u001b[0m       58.0616  0.0125\n",
      "      6      241.2868       \u001b[32m70.8867\u001b[0m  0.0126\n",
      "      6      \u001b[36m240.2393\u001b[0m       50.9208  0.0128\n",
      "      3      \u001b[36m229.8050\u001b[0m       51.8622  0.0104\n",
      "      4      \u001b[36m202.2908\u001b[0m       65.5333  0.0141\n",
      "      7      244.3304       48.7493  0.0219\n",
      "      7      209.0611       60.3871  0.0105\n",
      "      8      188.3643       62.8266  0.0178\n",
      "      7      243.7243       51.1176  0.0132\n",
      "      7      234.7066       \u001b[32m69.8603\u001b[0m  0.0168\n",
      "      9      \u001b[36m183.3329\u001b[0m       63.9153  0.0100\n",
      "      8      \u001b[36m204.0813\u001b[0m       62.3214  0.0107\n",
      "      5      208.4082       65.3007  0.0143\n",
      "      8      \u001b[36m229.5649\u001b[0m       48.4548  0.0145\n",
      "      4      \u001b[36m224.3015\u001b[0m       51.3759  0.0163\n",
      "      8      \u001b[36m237.0577\u001b[0m       51.5456  0.0131\n",
      "     10      \u001b[36m179.8838\u001b[0m       64.5531  0.0113\n",
      "Restoring best model from epoch 5.\n",
      "      8      228.8939       \u001b[32m69.6134\u001b[0m  0.0125\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      6      \u001b[36m200.6512\u001b[0m       64.9283  0.0119\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      9      232.4364       48.0062  0.0130\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      5      \u001b[36m217.8003\u001b[0m       \u001b[32m50.6111\u001b[0m  0.0133\n",
      "      9      \u001b[36m202.4061\u001b[0m       64.2865  0.0192\n",
      "      9      252.6441       52.0730  0.0128\n",
      "      9      237.9369       \u001b[32m69.1059\u001b[0m  0.0130\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m256.6803\u001b[0m       \u001b[32m55.4000\u001b[0m  0.0114\n",
      "      7      217.2172       65.2517  0.0136\n",
      "     10      239.2448       47.7442  0.0125\n",
      "Restoring best model from epoch 2.\n",
      "     10      \u001b[36m197.6303\u001b[0m       66.1587  0.0102\n",
      "Restoring best model from epoch 4.\n",
      "      6      218.2368       \u001b[32m50.1769\u001b[0m  0.0104\n",
      "     10      239.5706       52.4972  0.0124\n",
      "Restoring best model from epoch 2.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m237.1391\u001b[0m       \u001b[32m56.3169\u001b[0m  0.0183\n",
      "     10      \u001b[36m221.0733\u001b[0m       \u001b[32m69.0373\u001b[0m  0.0128\n",
      "      7      \u001b[36m213.0741\u001b[0m       50.2802  0.0102\n",
      "      8      204.2315       64.5296  0.0129\n",
      "      2      272.6374       55.8426  0.0181\n",
      "      9      209.8720       63.9799  0.0119\n",
      "      2      \u001b[36m229.7429\u001b[0m       56.8242  0.0189\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      3      \u001b[36m244.6725\u001b[0m       57.6137  0.0123\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      8      \u001b[36m208.0324\u001b[0m       51.1347  0.0189\n",
      "     10      \u001b[36m198.3971\u001b[0m       63.1544  0.0116\n",
      "Restoring best model from epoch 1.\n",
      "      4      \u001b[36m237.9191\u001b[0m       57.6550  0.0119\n",
      "      3      235.7282       56.5808  0.0183\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m246.1065\u001b[0m       \u001b[32m91.5305\u001b[0m  0.0126\n",
      "      9      209.8669       52.4031  0.0160\n",
      "      5      \u001b[36m233.9204\u001b[0m       56.8645  0.0130\n",
      "      4      231.3426       56.4027  0.0123\n",
      "     10      \u001b[36m206.6129\u001b[0m       54.3784  0.0101\n",
      "Restoring best model from epoch 6.\n",
      "      2      277.1840       \u001b[32m78.4776\u001b[0m  0.0162\n",
      "      6      \u001b[36m232.6588\u001b[0m       55.8245  0.0116\n",
      "      5      \u001b[36m227.3461\u001b[0m       \u001b[32m56.0622\u001b[0m  0.0133\n",
      "      3      \u001b[36m233.7119\u001b[0m       \u001b[32m73.9592\u001b[0m  0.0118\n",
      "      6      231.3734       56.2864  0.0119\n",
      "      7      233.1293       \u001b[32m54.9464\u001b[0m  0.0212\n",
      "      4      \u001b[36m227.6064\u001b[0m       \u001b[32m72.1618\u001b[0m  0.0114\n",
      "      7      \u001b[36m220.6255\u001b[0m       56.7521  0.0130\n",
      "      8      \u001b[36m231.0734\u001b[0m       \u001b[32m54.4557\u001b[0m  0.0133\n",
      "      5      \u001b[36m226.3271\u001b[0m       \u001b[32m71.1189\u001b[0m  0.0141\n",
      "      8      231.6538       56.7993  0.0133\n",
      "      9      \u001b[36m230.4614\u001b[0m       \u001b[32m54.1693\u001b[0m  0.0105\n",
      "      6      \u001b[36m224.7471\u001b[0m       \u001b[32m70.3827\u001b[0m  0.0102\n",
      "      9      230.7390       56.7720  0.0126\n",
      "     10      \u001b[36m229.4075\u001b[0m       \u001b[32m53.8745\u001b[0m  0.0101\n",
      "      7      \u001b[36m219.3994\u001b[0m       70.6281  0.0099\n",
      "     10      221.6588       56.3824  0.0125\n",
      "Restoring best model from epoch 5.\n",
      "      8      \u001b[36m218.5097\u001b[0m       70.9663  0.0104\n",
      "      9      \u001b[36m215.9244\u001b[0m       71.2890  0.0110\n",
      "     10      \u001b[36m215.4440\u001b[0m       71.5266  0.0102\n",
      "Restoring best model from epoch 6.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m231.3957\u001b[0m       \u001b[32m88.4208\u001b[0m  0.0238\n",
      "      2      \u001b[36m230.7109\u001b[0m       \u001b[32m83.4228\u001b[0m  0.0236\n",
      "      3      \u001b[36m227.5532\u001b[0m       \u001b[32m83.0819\u001b[0m  0.0227\n",
      "      4      \u001b[36m226.5198\u001b[0m       \u001b[32m83.0516\u001b[0m  0.0233\n",
      "      5      \u001b[36m222.8818\u001b[0m       83.5509  0.0231\n",
      "      6      \u001b[36m217.4512\u001b[0m       84.8720  0.0229\n",
      "      7      220.2997       85.7952  0.0235\n",
      "      8      220.9660       85.6060  0.0285\n",
      "      9      227.5964       85.2880  0.0232\n",
      "     10      \u001b[36m215.0699\u001b[0m       84.8577  0.0240\n",
      "Restoring best model from epoch 4.\n",
      "y_train breslow final [  -13.   -15.   -17.    19.   -20.    20.   -28.   -35.   -37.   -46.\n",
      "   -55.    56.    56.    57.    62.   -64.   -64.    65.   -68.    68.\n",
      "    81.   -82.    88.   -89.    90.   -95.    98.    99.  -105.   106.\n",
      "  -110.   118.   128.  -129.   131.   142.   144.   146.   149.   154.\n",
      "  -158.   163.  -163.   168.   173.  -189.   191.   200.   200.   205.\n",
      "   206.   211.   216.   220.   223.  -224.   232.   232.   237.   250.\n",
      "   253.   254.   258.   259.   261.   262.   272.   272.   273.   274.\n",
      "  -293.   294.   294.   303.   321.   324.   328.  -330.   332.  -333.\n",
      "   340.   344.   344.  -345.  -345.   356.  -361.   362.   364.  -365.\n",
      "  -366.  -368.  -369.   370.  -370.   370.  -373.  -376.  -377.  -379.\n",
      "  -382.  -384.   385.   386.   388.  -389.   391.  -398.  -399.   400.\n",
      "   406.   408.  -410.   413.   413.  -415.   415.  -415.  -416.   418.\n",
      "  -423.  -428.  -428.  -433.   434.   437.   454.  -455.   455.  -457.\n",
      "  -460.  -466.   467.  -467.   467.  -469.  -474.  -475.  -480.  -481.\n",
      "  -484.  -485.  -491.   492.   495.  -495.  -503.  -507.   508.   510.\n",
      "  -512.   522.  -522.  -524.   530.   536.  -536.   539.  -539.  -542.\n",
      "   544.  -546.   547.   550.  -560.  -562.   565.   565.  -572.  -572.\n",
      "  -573.   575.   577.  -578.   579.  -581.  -582.  -588.   590.  -590.\n",
      "   593.   599.   602.  -603.  -610.   612.   615.   617.   623.   630.\n",
      "  -633.  -636.  -638.  -640.  -641.  -642.  -646.  -648.  -649.   651.\n",
      "  -665.   665.   680.   685.  -691.   696.  -699.  -700.   706.   719.\n",
      "  -731.   734.   739.  -750.  -758.  -761.   778.  -783.  -789.   795.\n",
      "  -798.  -812.  -813.   819.  -820.   823.  -832.  -832.   835.  -840.\n",
      "  -842.  -851.   859.   864.  -873.  -887.  -893.  -897.  -899.   904.\n",
      "  -906.  -921.   941.   949.   974.  -997. -1003.  1004.  1005.  1008.\n",
      " -1029.  1036. -1048.  1064. -1072. -1094. -1110. -1115. -1127.  1163.\n",
      " -1174. -1181. -1186. -1219.  1270.  1348. -1350.  1367. -1370.  1420.\n",
      " -1429. -1454. -1455. -1460. -1522. -1529. -1538. -1561. -1582. -1604.\n",
      " -1621. -1649.  1670. -1708. -1714.  1718. -1761.  1804. -1806. -1830.\n",
      " -1845.  1869. -1884. -1912. -1947. -1949. -1952.  1971. -2008. -2009.\n",
      " -2020. -2024. -2027. -2044. -2109. -2139. -2177. -2293. -2312. -2330.\n",
      " -2423.  2641. -2790.  2828. -2868. -2886. -2964. -3011.  3183. -3364.\n",
      " -3420. -3432. -3817. -4343. -5041.]\n",
      "Concordance Index 0.6728931364031278\n",
      "Integrated Brier Score: 0.1967562848176492\n",
      "y_train breslow final [  -13.   -15.   -17.    19.   -20.    20.   -28.   -35.   -37.   -46.\n",
      "   -55.    56.    56.    57.    62.   -64.   -64.    65.   -68.    68.\n",
      "    81.   -82.    88.   -89.    90.   -95.    98.    99.  -105.   106.\n",
      "  -110.   118.   128.  -129.   131.   142.   144.   146.   149.   154.\n",
      "  -158.   163.  -163.   168.   173.  -189.   191.   200.   200.   205.\n",
      "   206.   211.   216.   220.   223.  -224.   232.   232.   237.   250.\n",
      "   253.   254.   258.   259.   261.   262.   272.   272.   273.   274.\n",
      "  -293.   294.   294.   303.   321.   324.   328.  -330.   332.  -333.\n",
      "   340.   344.   344.  -345.  -345.   356.  -361.   362.   364.  -365.\n",
      "  -366.  -368.  -369.   370.  -370.   370.  -373.  -376.  -377.  -379.\n",
      "  -382.  -384.   385.   386.   388.  -389.   391.  -398.  -399.   400.\n",
      "   406.   408.  -410.   413.   413.  -415.   415.  -415.  -416.   418.\n",
      "  -423.  -428.  -428.  -433.   434.   437.   454.  -455.   455.  -457.\n",
      "  -460.  -466.   467.  -467.   467.  -469.  -474.  -475.  -480.  -481.\n",
      "  -484.  -485.  -491.   492.   495.  -495.  -503.  -507.   508.   510.\n",
      "  -512.   522.  -522.  -524.   530.   536.  -536.   539.  -539.  -542.\n",
      "   544.  -546.   547.   550.  -560.  -562.   565.   565.  -572.  -572.\n",
      "  -573.   575.   577.  -578.   579.  -581.  -582.  -588.   590.  -590.\n",
      "   593.   599.   602.  -603.  -610.   612.   615.   617.   623.   630.\n",
      "  -633.  -636.  -638.  -640.  -641.  -642.  -646.  -648.  -649.   651.\n",
      "  -665.   665.   680.   685.  -691.   696.  -699.  -700.   706.   719.\n",
      "  -731.   734.   739.  -750.  -758.  -761.   778.  -783.  -789.   795.\n",
      "  -798.  -812.  -813.   819.  -820.   823.  -832.  -832.   835.  -840.\n",
      "  -842.  -851.   859.   864.  -873.  -887.  -893.  -897.  -899.   904.\n",
      "  -906.  -921.   941.   949.   974.  -997. -1003.  1004.  1005.  1008.\n",
      " -1029.  1036. -1048.  1064. -1072. -1094. -1110. -1115. -1127.  1163.\n",
      " -1174. -1181. -1186. -1219.  1270.  1348. -1350.  1367. -1370.  1420.\n",
      " -1429. -1454. -1455. -1460. -1522. -1529. -1538. -1561. -1582. -1604.\n",
      " -1621. -1649.  1670. -1708. -1714.  1718. -1761.  1804. -1806. -1830.\n",
      " -1845.  1869. -1884. -1912. -1947. -1949. -1952.  1971. -2008. -2009.\n",
      " -2020. -2024. -2027. -2044. -2109. -2139. -2177. -2293. -2312. -2330.\n",
      " -2423.  2641. -2790.  2828. -2868. -2886. -2964. -3011.  3183. -3364.\n",
      " -3420. -3432. -3817. -4343. -5041.]\n",
      "durations 59.0 5050.0\n",
      "Concordance Index 0.6063321385902031\n",
      "Integrated Brier Score: 0.210025935913591\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m227.9112\u001b[0m       \u001b[32m84.1196\u001b[0m  0.0200\n",
      "      2      \u001b[36m206.5973\u001b[0m       \u001b[32m81.8561\u001b[0m  0.0151\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      3      211.7348       \u001b[32m75.5329\u001b[0m  0.0155\n",
      "      4      209.7269       \u001b[32m71.3692\u001b[0m  0.0133\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m216.3599\u001b[0m       \u001b[32m57.5747\u001b[0m  0.0211\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      5      206.6799       \u001b[32m69.3727\u001b[0m  0.0153\n",
      "      2      \u001b[36m202.4060\u001b[0m       \u001b[32m57.1640\u001b[0m  0.0151\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m250.4218\u001b[0m       \u001b[32m67.3414\u001b[0m  0.0156\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      6      \u001b[36m205.4895\u001b[0m       69.4976  0.0173\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m197.9052\u001b[0m       \u001b[32m58.9142\u001b[0m  0.0119\n",
      "      3      \u001b[36m197.3689\u001b[0m       58.1364  0.0164\n",
      "      2      \u001b[36m245.6107\u001b[0m       \u001b[32m65.8832\u001b[0m  0.0161\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      7      213.5042       70.0556  0.0141\n",
      "      2      203.4284       \u001b[32m55.3602\u001b[0m  0.0111\n",
      "      4      \u001b[36m196.5423\u001b[0m       59.2021  0.0126\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m213.7289\u001b[0m       \u001b[32m95.8256\u001b[0m  0.0142\n",
      "      3      249.5343       66.3288  0.0162\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      3      \u001b[36m187.0331\u001b[0m       60.0857  0.0107\n",
      "      5      199.8125       62.3716  0.0134\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m224.4179\u001b[0m       \u001b[32m87.2645\u001b[0m  0.0130\n",
      "      2      224.0809       \u001b[32m69.0172\u001b[0m  0.0122\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m248.9391\u001b[0m       \u001b[32m99.3746\u001b[0m  0.0225\n",
      "      4      \u001b[36m241.9364\u001b[0m       66.2237  0.0139\n",
      "      8      \u001b[36m202.3495\u001b[0m       70.7831  0.0218\n",
      "      4      \u001b[36m177.6867\u001b[0m       65.7271  0.0105\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m247.0682\u001b[0m       \u001b[32m68.4330\u001b[0m  0.0121\n",
      "      6      197.7521       68.2592  0.0150\n",
      "      2      252.7611       \u001b[32m85.5461\u001b[0m  0.0131\n",
      "      3      \u001b[36m193.2431\u001b[0m       71.0438  0.0115\n",
      "      5      179.3515       68.3597  0.0151\n",
      "      9      202.7358       70.9417  0.0152\n",
      "      2      \u001b[36m229.8320\u001b[0m       \u001b[32m98.3091\u001b[0m  0.0168\n",
      "      2      \u001b[36m242.7215\u001b[0m       \u001b[32m68.3797\u001b[0m  0.0145\n",
      "      5      \u001b[36m229.9370\u001b[0m       67.4254  0.0174\n",
      "      7      \u001b[36m194.4658\u001b[0m       73.5175  0.0137\n",
      "      3      \u001b[36m219.2460\u001b[0m       \u001b[32m78.4432\u001b[0m  0.0158\n",
      "      4      193.7763       70.9333  0.0124\n",
      "      3      \u001b[36m235.7831\u001b[0m       71.5755  0.0112\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "     10      205.5730       71.5191  0.0165\n",
      "Restoring best model from epoch 5.\n",
      "      3      234.9272       \u001b[32m96.0928\u001b[0m  0.0167\n",
      "      6      243.4158       68.1423  0.0176\n",
      "      5      \u001b[36m193.0391\u001b[0m       \u001b[32m68.8027\u001b[0m  0.0108\n",
      "      4      \u001b[36m211.0249\u001b[0m       \u001b[32m77.2068\u001b[0m  0.0128\n",
      "      6      \u001b[36m171.4828\u001b[0m       73.0021  0.0205\n",
      "      8      198.1216       73.5002  0.0183\n",
      "      4      \u001b[36m229.0329\u001b[0m       69.7479  0.0181\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m236.7507\u001b[0m       \u001b[32m77.8926\u001b[0m  0.0150\n",
      "      7      244.3611       67.6997  0.0147\n",
      "      7      174.6618       70.2935  0.0106\n",
      "      5      212.3822       77.5405  0.0159\n",
      "      4      \u001b[36m226.9162\u001b[0m       \u001b[32m92.8703\u001b[0m  0.0225\n",
      "      5      \u001b[36m223.8360\u001b[0m       \u001b[32m67.0283\u001b[0m  0.0103\n",
      "      6      \u001b[36m187.8913\u001b[0m       \u001b[32m67.7149\u001b[0m  0.0239\n",
      "      8      233.9975       67.2933  0.0130\n",
      "      8      172.8209       68.0056  0.0129\n",
      "      2      \u001b[36m227.3809\u001b[0m       \u001b[32m76.9487\u001b[0m  0.0201\n",
      "      6      224.5491       \u001b[32m65.3097\u001b[0m  0.0101\n",
      "      9      \u001b[36m192.9922\u001b[0m       72.4187  0.0283\n",
      "      6      212.8299       \u001b[32m76.9104\u001b[0m  0.0163\n",
      "      9      242.1611       66.6872  0.0123\n",
      "      9      \u001b[36m170.6002\u001b[0m       66.9965  0.0114\n",
      "      7      \u001b[36m184.8322\u001b[0m       68.8035  0.0178\n",
      "      7      \u001b[36m217.5621\u001b[0m       \u001b[32m64.7292\u001b[0m  0.0110\n",
      "      5      \u001b[36m226.2614\u001b[0m       \u001b[32m90.3246\u001b[0m  0.0227\n",
      "     10      198.7172       68.4868  0.0176\n",
      "Restoring best model from epoch 2.\n",
      "     10      234.9226       \u001b[32m65.8679\u001b[0m  0.0124\n",
      "      7      \u001b[36m204.9035\u001b[0m       \u001b[32m76.1003\u001b[0m  0.0161\n",
      "      3      227.5768       \u001b[32m76.9455\u001b[0m  0.0211\n",
      "      8      187.1283       70.2242  0.0103\n",
      "      8      223.1623       \u001b[32m64.6835\u001b[0m  0.0106\n",
      "     10      \u001b[36m168.7148\u001b[0m       68.6932  0.0146\n",
      "Restoring best model from epoch 2.\n",
      "      6      250.5844       \u001b[32m89.4402\u001b[0m  0.0173\n",
      "      9      \u001b[36m182.5682\u001b[0m       70.6296  0.0103\n",
      "      9      225.4435       64.9583  0.0107\n",
      "      4      \u001b[36m218.3194\u001b[0m       78.6284  0.0169\n",
      "      8      \u001b[36m200.1750\u001b[0m       \u001b[32m75.1420\u001b[0m  0.0160\n",
      "     10      \u001b[36m211.2401\u001b[0m       65.5741  0.0109\n",
      "Restoring best model from epoch 8.\n",
      "     10      184.2409       70.9467  0.0137\n",
      "Restoring best model from epoch 6.\n",
      "      9      \u001b[36m198.4683\u001b[0m       \u001b[32m74.7076\u001b[0m  0.0143\n",
      "      7      233.7008       \u001b[32m89.0855\u001b[0m  0.0237\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      5      \u001b[36m214.4402\u001b[0m       79.2490  0.0171\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "     10      201.7158       74.8735  0.0105\n",
      "Restoring best model from epoch 9.\n",
      "      6      228.5632       78.6957  0.0133\n",
      "      8      227.5576       \u001b[32m88.2561\u001b[0m  0.0214\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m238.3533\u001b[0m       \u001b[32m97.3584\u001b[0m  0.0228\n",
      "      7      \u001b[36m213.9608\u001b[0m       77.9058  0.0135\n",
      "      9      231.4581       \u001b[32m87.5633\u001b[0m  0.0139\n",
      "      2      244.2713       \u001b[32m87.1931\u001b[0m  0.0106\n",
      "      8      214.1538       77.4144  0.0123\n",
      "     10      226.8880       \u001b[32m86.5425\u001b[0m  0.0123\n",
      "      3      \u001b[36m218.9645\u001b[0m       \u001b[32m86.8222\u001b[0m  0.0102\n",
      "      9      \u001b[36m210.0713\u001b[0m       \u001b[32m76.6450\u001b[0m  0.0165\n",
      "      4      \u001b[36m215.9592\u001b[0m       87.3633  0.0121\n",
      "     10      217.4133       \u001b[32m75.9298\u001b[0m  0.0128\n",
      "      5      216.3653       87.6683  0.0106\n",
      "      6      \u001b[36m215.1937\u001b[0m       87.7268  0.0102\n",
      "      7      \u001b[36m213.9291\u001b[0m       88.6938  0.0104\n",
      "      8      \u001b[36m211.8776\u001b[0m       89.4676  0.0097\n",
      "      9      \u001b[36m210.2926\u001b[0m       89.9704  0.0096\n",
      "     10      \u001b[36m205.2562\u001b[0m       90.5413  0.0096\n",
      "Restoring best model from epoch 3.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m223.0069\u001b[0m       \u001b[32m99.0710\u001b[0m  0.0236\n",
      "      2      \u001b[36m221.3530\u001b[0m       \u001b[32m92.9089\u001b[0m  0.0237\n",
      "      3      \u001b[36m217.9957\u001b[0m       95.1760  0.0221\n",
      "      4      \u001b[36m217.4899\u001b[0m       93.2876  0.0218\n",
      "      5      \u001b[36m217.3242\u001b[0m       93.3903  0.0222\n",
      "      6      \u001b[36m209.8369\u001b[0m       93.9458  0.0224\n",
      "      7      216.4594       93.7781  0.0219\n",
      "      8      212.7904       \u001b[32m92.6093\u001b[0m  0.0221\n",
      "      9      213.2373       \u001b[32m90.7343\u001b[0m  0.0227\n",
      "     10      \u001b[36m209.5357\u001b[0m       \u001b[32m90.1884\u001b[0m  0.0224\n",
      "y_train breslow final [  -13.   -15.   -17.    19.   -20.    20.   -28.   -35.   -37.   -46.\n",
      "    56.    56.    57.   -59.    62.   -64.   -64.   -67.    69.    76.\n",
      "    81.   -82.   -84.    88.    92.    93.   -95.    98.    99.  -105.\n",
      "   106.  -110.  -117.   118.   122.   131.   142.   144.   146.   154.\n",
      "   154.  -158.   168.   168.   173.   182.  -187.  -189.   200.   200.\n",
      "   205.   211.   213.   216.   223.   232.   246.   248.   251.  -251.\n",
      "   253.   254.   254.   258.   259.   262.   272.   272.   272.   273.\n",
      "  -276.   278.  -293.   294.   294.   303.   311.   324.   328.  -330.\n",
      "  -333.  -337.   340.   344.  -345.  -345.  -359.   362.   364.  -364.\n",
      "  -365.  -368.   370.  -370.   370.  -372.  -372.  -376.  -379.  -383.\n",
      "  -384.  -384.   385.   385.   386.   388.  -389.   391.   393.  -394.\n",
      "  -398.  -398.  -399.   400.   408.  -410.   413.   413.   413.  -415.\n",
      "   415.  -415.  -416.  -423.  -425.  -428.  -433.   434.   437.   453.\n",
      "   454.  -455.   455.  -457.  -460.   460.  -466.  -467.  -469.   474.\n",
      "  -475.  -477.  -480.  -481.  -484.  -485.   486.  -491.   492.  -495.\n",
      "  -495.  -507.   508.  -508.   510.   510.   522.  -522.  -524.   530.\n",
      "  -536.   539.  -539.  -540.  -542.   544.  -546.   547.   547.   565.\n",
      "   565.  -572.  -572.  -573.   575.   577.  -578.  -578.   579.  -580.\n",
      "  -588.   590.  -590.   593.   599.   602.   612.   615.   617.  -618.\n",
      "   623.   630.  -633.  -638.  -640.  -641.  -642.  -646.  -648.  -649.\n",
      "  -665.   665.   674.   680.   685.   690.  -691.   696.  -699.   706.\n",
      "   712.  -731.   739.  -750.  -761.  -773.  -783.   795.  -798.  -799.\n",
      "  -812.   819.  -820.   823.  -832.   835.  -840.  -851.   859.  -864.\n",
      "  -873.  -897.  -899.   904.  -906.  -921.   941.  -945.   949. -1003.\n",
      "  1004.  1008.  1036. -1048.  1064. -1072.  1077. -1090. -1094. -1108.\n",
      " -1110. -1127.  1163. -1174. -1181. -1186. -1219.  1270. -1326.  1348.\n",
      " -1350.  1367. -1370.  1423. -1429. -1454. -1455. -1460. -1522. -1529.\n",
      " -1538. -1542.  1556. -1561. -1604. -1621. -1639. -1649.  1670. -1708.\n",
      " -1714. -1761. -1792.  1804. -1806. -1845.  1869. -1884. -1947. -1949.\n",
      " -1952.  1971. -2008. -2009.  2020. -2024. -2027. -2044. -2049. -2109.\n",
      " -2177. -2293. -2312. -2380. -2625.  2641. -2656. -2703. -2790.  2828.\n",
      " -2868. -2886.  2954. -2964. -3011.  3183. -3314. -3364. -3420. -3432.\n",
      " -3817. -3981. -4343. -4967. -5050.]\n",
      "Concordance Index 0.6399180717603599\n",
      "Integrated Brier Score: 0.2193498108738935\n",
      "y_train breslow final [  -13.   -15.   -17.    19.   -20.    20.   -28.   -35.   -37.   -46.\n",
      "    56.    56.    57.   -59.    62.   -64.   -64.   -67.    69.    76.\n",
      "    81.   -82.   -84.    88.    92.    93.   -95.    98.    99.  -105.\n",
      "   106.  -110.  -117.   118.   122.   131.   142.   144.   146.   154.\n",
      "   154.  -158.   168.   168.   173.   182.  -187.  -189.   200.   200.\n",
      "   205.   211.   213.   216.   223.   232.   246.   248.   251.  -251.\n",
      "   253.   254.   254.   258.   259.   262.   272.   272.   272.   273.\n",
      "  -276.   278.  -293.   294.   294.   303.   311.   324.   328.  -330.\n",
      "  -333.  -337.   340.   344.  -345.  -345.  -359.   362.   364.  -364.\n",
      "  -365.  -368.   370.  -370.   370.  -372.  -372.  -376.  -379.  -383.\n",
      "  -384.  -384.   385.   385.   386.   388.  -389.   391.   393.  -394.\n",
      "  -398.  -398.  -399.   400.   408.  -410.   413.   413.   413.  -415.\n",
      "   415.  -415.  -416.  -423.  -425.  -428.  -433.   434.   437.   453.\n",
      "   454.  -455.   455.  -457.  -460.   460.  -466.  -467.  -469.   474.\n",
      "  -475.  -477.  -480.  -481.  -484.  -485.   486.  -491.   492.  -495.\n",
      "  -495.  -507.   508.  -508.   510.   510.   522.  -522.  -524.   530.\n",
      "  -536.   539.  -539.  -540.  -542.   544.  -546.   547.   547.   565.\n",
      "   565.  -572.  -572.  -573.   575.   577.  -578.  -578.   579.  -580.\n",
      "  -588.   590.  -590.   593.   599.   602.   612.   615.   617.  -618.\n",
      "   623.   630.  -633.  -638.  -640.  -641.  -642.  -646.  -648.  -649.\n",
      "  -665.   665.   674.   680.   685.   690.  -691.   696.  -699.   706.\n",
      "   712.  -731.   739.  -750.  -761.  -773.  -783.   795.  -798.  -799.\n",
      "  -812.   819.  -820.   823.  -832.   835.  -840.  -851.   859.  -864.\n",
      "  -873.  -897.  -899.   904.  -906.  -921.   941.  -945.   949. -1003.\n",
      "  1004.  1008.  1036. -1048.  1064. -1072.  1077. -1090. -1094. -1108.\n",
      " -1110. -1127.  1163. -1174. -1181. -1186. -1219.  1270. -1326.  1348.\n",
      " -1350.  1367. -1370.  1423. -1429. -1454. -1455. -1460. -1522. -1529.\n",
      " -1538. -1542.  1556. -1561. -1604. -1621. -1639. -1649.  1670. -1708.\n",
      " -1714. -1761. -1792.  1804. -1806. -1845.  1869. -1884. -1947. -1949.\n",
      " -1952.  1971. -2008. -2009.  2020. -2024. -2027. -2044. -2049. -2109.\n",
      " -2177. -2293. -2312. -2380. -2625.  2641. -2656. -2703. -2790.  2828.\n",
      " -2868. -2886.  2954. -2964. -3011.  3183. -3314. -3364. -3420. -3432.\n",
      " -3817. -3981. -4343. -4967. -5050.]\n",
      "durations 55.0 5041.0\n",
      "Concordance Index 0.5933562428407789\n",
      "Integrated Brier Score: 0.23337334933817805\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m227.2924\u001b[0m       \u001b[32m64.0785\u001b[0m  0.0145\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m234.2950\u001b[0m       \u001b[32m65.2424\u001b[0m  0.0144\n",
      "      2      \u001b[36m209.5124\u001b[0m       67.9114  0.0230\n",
      "      2      235.6110       \u001b[32m64.0626\u001b[0m  0.0202\n",
      "      3      215.6873       65.9818  0.0158\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      3      \u001b[36m227.3258\u001b[0m       \u001b[32m63.5238\u001b[0m  0.0136\n",
      "      4      \u001b[36m206.0458\u001b[0m       65.9197  0.0136\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      4      \u001b[36m227.0305\u001b[0m       63.6958  0.0127\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m254.1511\u001b[0m       \u001b[32m69.5032\u001b[0m  0.0136\n",
      "      5      \u001b[36m196.9558\u001b[0m       65.5453  0.0127\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m283.7505\u001b[0m       \u001b[32m78.8192\u001b[0m  0.0136\n",
      "      5      \u001b[36m226.0998\u001b[0m       64.8230  0.0123\n",
      "      2      255.5649       \u001b[32m69.2695\u001b[0m  0.0140\n",
      "      6      207.7786       \u001b[32m63.0717\u001b[0m  0.0124\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m198.8544\u001b[0m       \u001b[32m52.5589\u001b[0m  0.0118\n",
      "      6      \u001b[36m213.2150\u001b[0m       66.0391  0.0121\n",
      "      2      \u001b[36m272.0072\u001b[0m       \u001b[32m78.1999\u001b[0m  0.0135\n",
      "      3      \u001b[36m240.8121\u001b[0m       69.8153  0.0140\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2      \u001b[36m187.7224\u001b[0m       \u001b[32m50.3872\u001b[0m  0.0110\n",
      "      7      204.2811       \u001b[32m62.1763\u001b[0m  0.0180\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      3      279.3638       78.9254  0.0146\n",
      "      7      220.0541       67.3247  0.0169\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m252.2069\u001b[0m       \u001b[32m75.5371\u001b[0m  0.0105\n",
      "      3      \u001b[36m177.7287\u001b[0m       50.5232  0.0103\n",
      "      4      243.3461       71.7139  0.0175\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      8      201.5279       \u001b[32m61.9080\u001b[0m  0.0121\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m218.4444\u001b[0m       \u001b[32m94.5992\u001b[0m  0.0103\n",
      "      8      225.0174       67.9591  0.0122\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      4      \u001b[36m167.0780\u001b[0m       50.5127  0.0095\n",
      "      2      \u001b[36m249.8436\u001b[0m       \u001b[32m73.9458\u001b[0m  0.0106\n",
      "      4      273.9712       78.7188  0.0193\n",
      "      5      244.7980       73.1343  0.0124\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m234.3487\u001b[0m       \u001b[32m72.1660\u001b[0m  0.0109\n",
      "      2      224.0846       \u001b[32m78.7541\u001b[0m  0.0103\n",
      "      9      198.6216       62.3501  0.0119\n",
      "      5      173.3800       52.0483  0.0095\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m275.9956\u001b[0m       \u001b[32m96.5897\u001b[0m  0.0109\n",
      "      9      225.0343       68.7742  0.0155\n",
      "      3      \u001b[36m230.9093\u001b[0m       75.7828  0.0160\n",
      "      5      273.0980       78.7486  0.0136\n",
      "      6      167.7352       53.6352  0.0095\n",
      "     10      \u001b[36m196.6019\u001b[0m       62.9455  0.0142\n",
      "      2      239.0318       \u001b[32m66.5373\u001b[0m  0.0148\n",
      "Restoring best model from epoch 8.\n",
      "      3      \u001b[36m197.7453\u001b[0m       \u001b[32m68.5206\u001b[0m  0.0143\n",
      "      2      296.2927       \u001b[32m84.1430\u001b[0m  0.0110\n",
      "      6      \u001b[36m233.1224\u001b[0m       73.8994  0.0190\n",
      "      4      231.6712       76.8363  0.0108\n",
      "     10      223.7858       69.3683  0.0162\n",
      "      3      \u001b[36m214.6740\u001b[0m       69.9754  0.0101\n",
      "Restoring best model from epoch 3.\n",
      "      7      \u001b[36m162.9219\u001b[0m       53.6596  0.0142\n",
      "      3      \u001b[36m264.8821\u001b[0m       \u001b[32m81.2812\u001b[0m  0.0104\n",
      "      4      \u001b[36m189.6774\u001b[0m       \u001b[32m66.0612\u001b[0m  0.0147\n",
      "      6      273.1656       78.8959  0.0197\n",
      "      7      246.9430       74.1446  0.0151\n",
      "      5      \u001b[36m225.0080\u001b[0m       76.2593  0.0120\n",
      "      8      168.4986       52.9175  0.0107\n",
      "      4      219.2338       71.8435  0.0161\n",
      "      4      \u001b[36m259.3618\u001b[0m       81.6696  0.0133\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      5      197.1380       \u001b[32m65.7276\u001b[0m  0.0120\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      7      272.0175       78.9044  0.0132\n",
      "      6      \u001b[36m217.9722\u001b[0m       76.2873  0.0115\n",
      "      8      237.3436       74.1451  0.0139\n",
      "      5      \u001b[36m209.4981\u001b[0m       68.9196  0.0123\n",
      "      9      167.3729       53.3139  0.0147\n",
      "      7      \u001b[36m217.4576\u001b[0m       76.8267  0.0109\n",
      "      8      \u001b[36m271.1576\u001b[0m       79.2142  0.0134\n",
      "      6      190.0882       67.7410  0.0186\n",
      "      5      259.6994       \u001b[32m80.3824\u001b[0m  0.0202\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m202.7811\u001b[0m       \u001b[32m51.4564\u001b[0m  0.0148\n",
      "      9      245.6639       73.8156  0.0194\n",
      "     10      \u001b[36m161.1616\u001b[0m       54.2839  0.0136\n",
      "Restoring best model from epoch 2.\n",
      "      8      \u001b[36m213.9455\u001b[0m       78.2302  0.0122\n",
      "      6      \u001b[36m202.9979\u001b[0m       \u001b[32m65.9161\u001b[0m  0.0171\n",
      "      7      \u001b[36m184.5701\u001b[0m       70.4247  0.0128\n",
      "      9      278.3213       78.7935  0.0181\n",
      "      9      215.7189       81.9425  0.0102\n",
      "      6      \u001b[36m256.2349\u001b[0m       \u001b[32m79.1988\u001b[0m  0.0186\n",
      "      2      \u001b[36m193.9277\u001b[0m       \u001b[32m50.0637\u001b[0m  0.0189\n",
      "      7      \u001b[36m200.7440\u001b[0m       \u001b[32m64.6477\u001b[0m  0.0119\n",
      "     10      245.9681       73.7836  0.0173\n",
      "Restoring best model from epoch 2.\n",
      "      8      \u001b[36m180.2247\u001b[0m       72.4341  0.0121\n",
      "     10      \u001b[36m209.9799\u001b[0m       84.4194  0.0102\n",
      "Restoring best model from epoch 2.\n",
      "     10      273.2505       78.4552  0.0161\n",
      "Restoring best model from epoch 2.\n",
      "      7      258.9499       79.4064  0.0116\n",
      "      3      195.1466       \u001b[32m49.5401\u001b[0m  0.0122\n",
      "      8      202.1563       65.4366  0.0186\n",
      "      9      182.4120       73.1615  0.0152\n",
      "      8      \u001b[36m254.7922\u001b[0m       79.4530  0.0146\n",
      "      4      \u001b[36m187.6027\u001b[0m       50.1132  0.0130\n",
      "      9      \u001b[36m197.4368\u001b[0m       67.5465  0.0101\n",
      "     10      \u001b[36m176.2288\u001b[0m       72.8067  0.0102\n",
      "Restoring best model from epoch 5.\n",
      "      9      256.0214       79.5696  0.0113\n",
      "      5      198.7408       50.9699  0.0133\n",
      "     10      \u001b[36m195.9086\u001b[0m       70.5135  0.0099\n",
      "Restoring best model from epoch 7.\n",
      "     10      \u001b[36m246.9175\u001b[0m       79.7904  0.0115\n",
      "Restoring best model from epoch 6.\n",
      "      6      \u001b[36m185.7065\u001b[0m       52.0578  0.0128\n",
      "      7      193.7479       52.8628  0.0140\n",
      "      8      187.2467       53.4657  0.0146\n",
      "      9      191.1895       53.7663  0.0193\n",
      "     10      186.1516       53.3571  0.0165\n",
      "Restoring best model from epoch 3.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m222.3093\u001b[0m      \u001b[32m104.1144\u001b[0m  0.0230\n",
      "      2      \u001b[36m214.3600\u001b[0m      \u001b[32m103.4435\u001b[0m  0.0222\n",
      "      3      220.0378      \u001b[32m102.5309\u001b[0m  0.0224\n",
      "      4      \u001b[36m214.2429\u001b[0m      \u001b[32m101.2903\u001b[0m  0.0222\n",
      "      5      223.0340      \u001b[32m100.3796\u001b[0m  0.0246\n",
      "      6      214.4726       \u001b[32m99.5709\u001b[0m  0.0260\n",
      "      7      214.5019       \u001b[32m98.5907\u001b[0m  0.0230\n",
      "      8      \u001b[36m211.3374\u001b[0m       \u001b[32m98.2990\u001b[0m  0.0224\n",
      "      9      214.5025       \u001b[32m98.2366\u001b[0m  0.0219\n",
      "     10      \u001b[36m208.5550\u001b[0m       \u001b[32m97.9202\u001b[0m  0.0227\n",
      "y_train breslow final [  -15.   -17.    20.   -28.   -37.   -55.    56.    57.   -59.    62.\n",
      "   -64.    65.   -67.   -68.    68.    69.    76.    81.   -84.    88.\n",
      "   -89.    90.    92.    93.   -95.    98.    99.   106.  -117.   122.\n",
      "   128.  -129.   131.   142.   144.   146.   149.   154.   154.   163.\n",
      "  -163.   168.   173.   182.  -187.  -189.   191.   200.   200.   205.\n",
      "   206.   211.   213.   216.   220.   223.  -224.   232.   232.   237.\n",
      "   246.   248.   250.   251.  -251.   253.   254.   258.   261.   262.\n",
      "   272.   272.   274.  -276.   278.  -293.   294.   303.   311.   321.\n",
      "   324.   324.   328.   332.  -333.  -337.   340.   344.   344.  -345.\n",
      "  -345.   356.  -359.  -361.   362.   364.  -364.  -365.  -366.  -368.\n",
      "  -368.  -369.  -370.   370.  -372.  -372.  -373.  -376.  -377.  -379.\n",
      "  -382.  -383.  -384.  -384.   385.   385.   388.  -389.   391.   393.\n",
      "  -394.  -398.  -399.   400.   406.   408.  -410.   413.   413.   413.\n",
      "  -415.   415.   418.  -423.  -425.  -428.  -428.  -433.   434.   437.\n",
      "   453.   454.  -455.   455.  -457.  -460.   460.  -466.  -466.   467.\n",
      "   467.  -469.   474.  -474.  -477.  -480.  -481.  -484.  -485.   486.\n",
      "  -491.   492.   495.  -495.  -503.  -507.  -508.   510.   510.  -512.\n",
      "   522.  -522.  -524.   530.  -536.   536.  -536.   539.  -539.  -540.\n",
      "  -542.   544.   544.   547.   547.   550.  -560.  -562.   565.  -572.\n",
      "  -572.  -573.   575.  -578.  -578.   579.  -580.  -581.  -582.  -588.\n",
      "  -590.   593.   602.  -603.  -610.   612.   615.   617.  -618.  -633.\n",
      "  -636.  -638.  -640.  -646.  -648.   651.   674.   690.  -691.  -699.\n",
      "  -700.   712.   719.   734.   739.  -758.  -761.  -773.   778.  -783.\n",
      "  -789.  -798.  -799.  -813.  -820.  -832.  -832.   835.  -840.  -842.\n",
      "  -851.   859.   864.  -864.  -873.  -887.  -893.  -897.  -899.   904.\n",
      "  -921.  -945.   949.   974.  -997. -1003.  1004.  1005. -1029.  1064.\n",
      " -1072.  1077. -1090. -1094. -1108. -1115.  1163. -1174. -1181. -1186.\n",
      " -1219. -1326. -1350.  1367. -1370.  1420.  1423. -1455. -1460. -1522.\n",
      " -1529. -1542.  1556. -1561. -1582. -1604. -1639.  1670. -1714.  1718.\n",
      " -1761. -1792. -1806. -1830. -1912. -1947. -1949. -1952.  1971. -2008.\n",
      " -2009.  2020. -2020. -2049. -2109. -2139. -2293. -2312. -2330. -2380.\n",
      " -2423. -2625. -2656. -2703. -2790. -2886.  2954. -3314. -3420. -3817.\n",
      " -3981. -4343. -4967. -5041. -5050.]\n",
      "Concordance Index 0.6532093285422391\n",
      "Integrated Brier Score: 0.19490068600296948\n",
      "y_train breslow final [  -15.   -17.    20.   -28.   -37.   -55.    56.    57.   -59.    62.\n",
      "   -64.    65.   -67.   -68.    68.    69.    76.    81.   -84.    88.\n",
      "   -89.    90.    92.    93.   -95.    98.    99.   106.  -117.   122.\n",
      "   128.  -129.   131.   142.   144.   146.   149.   154.   154.   163.\n",
      "  -163.   168.   173.   182.  -187.  -189.   191.   200.   200.   205.\n",
      "   206.   211.   213.   216.   220.   223.  -224.   232.   232.   237.\n",
      "   246.   248.   250.   251.  -251.   253.   254.   258.   261.   262.\n",
      "   272.   272.   274.  -276.   278.  -293.   294.   303.   311.   321.\n",
      "   324.   324.   328.   332.  -333.  -337.   340.   344.   344.  -345.\n",
      "  -345.   356.  -359.  -361.   362.   364.  -364.  -365.  -366.  -368.\n",
      "  -368.  -369.  -370.   370.  -372.  -372.  -373.  -376.  -377.  -379.\n",
      "  -382.  -383.  -384.  -384.   385.   385.   388.  -389.   391.   393.\n",
      "  -394.  -398.  -399.   400.   406.   408.  -410.   413.   413.   413.\n",
      "  -415.   415.   418.  -423.  -425.  -428.  -428.  -433.   434.   437.\n",
      "   453.   454.  -455.   455.  -457.  -460.   460.  -466.  -466.   467.\n",
      "   467.  -469.   474.  -474.  -477.  -480.  -481.  -484.  -485.   486.\n",
      "  -491.   492.   495.  -495.  -503.  -507.  -508.   510.   510.  -512.\n",
      "   522.  -522.  -524.   530.  -536.   536.  -536.   539.  -539.  -540.\n",
      "  -542.   544.   544.   547.   547.   550.  -560.  -562.   565.  -572.\n",
      "  -572.  -573.   575.  -578.  -578.   579.  -580.  -581.  -582.  -588.\n",
      "  -590.   593.   602.  -603.  -610.   612.   615.   617.  -618.  -633.\n",
      "  -636.  -638.  -640.  -646.  -648.   651.   674.   690.  -691.  -699.\n",
      "  -700.   712.   719.   734.   739.  -758.  -761.  -773.   778.  -783.\n",
      "  -789.  -798.  -799.  -813.  -820.  -832.  -832.   835.  -840.  -842.\n",
      "  -851.   859.   864.  -864.  -873.  -887.  -893.  -897.  -899.   904.\n",
      "  -921.  -945.   949.   974.  -997. -1003.  1004.  1005. -1029.  1064.\n",
      " -1072.  1077. -1090. -1094. -1108. -1115.  1163. -1174. -1181. -1186.\n",
      " -1219. -1326. -1350.  1367. -1370.  1420.  1423. -1455. -1460. -1522.\n",
      " -1529. -1542.  1556. -1561. -1582. -1604. -1639.  1670. -1714.  1718.\n",
      " -1761. -1792. -1806. -1830. -1912. -1947. -1949. -1952.  1971. -2008.\n",
      " -2009.  2020. -2020. -2049. -2109. -2139. -2293. -2312. -2330. -2380.\n",
      " -2423. -2625. -2656. -2703. -2790. -2886.  2954. -3314. -3420. -3817.\n",
      " -3981. -4343. -4967. -5041. -5050.]\n",
      "durations 13.0 3432.0\n",
      "Concordance Index 0.6095669036845508\n",
      "Integrated Brier Score: 0.22184698686879128\n",
      "split gex_?|100130426      float32\n",
      "gex_?|100133144      float32\n",
      "gex_?|100134869      float32\n",
      "gex_?|10357          float32\n",
      "gex_?|10431          float32\n",
      "                      ...   \n",
      "gex_ZYG11A|440590    float32\n",
      "gex_ZYG11B|79699     float32\n",
      "gex_ZYX|7791         float32\n",
      "gex_ZZEF1|23140      float32\n",
      "gex_ZZZ3|26009       float32\n",
      "Length: 20531, dtype: object\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m63.0155\u001b[0m       \u001b[32m53.6488\u001b[0m  0.0450\n",
      "      2       \u001b[36m61.9621\u001b[0m       54.9567  0.0464\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      3       \u001b[36m59.8262\u001b[0m       60.1849  0.0402\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m67.6275\u001b[0m       \u001b[32m70.0543\u001b[0m  0.0329\n",
      "      4       61.3063       62.9753  0.0396\n",
      "      2       \u001b[36m56.7903\u001b[0m       \u001b[32m58.2581\u001b[0m  0.0299\n",
      "      3       57.1453       58.3088  0.0318\n",
      "      5       61.2701       62.1428  0.0397\n",
      "      4       \u001b[36m55.2877\u001b[0m       63.0588  0.0283\n",
      "      6       60.2064       58.0671  0.0438\n",
      "      5       55.8821       65.0081  0.0299\n",
      "      6       \u001b[36m52.9933\u001b[0m       63.7804  0.0295\n",
      "      7       \u001b[36m59.3529\u001b[0m       56.2883  0.0427\n",
      "      7       \u001b[36m51.4908\u001b[0m       64.2878  0.0290\n",
      "      8       61.8828       56.7359  0.0403\n",
      "      8       \u001b[36m50.6016\u001b[0m       65.7077  0.0286\n",
      "      9       \u001b[36m58.3665\u001b[0m       57.5371  0.0391\n",
      "      9       \u001b[36m49.2440\u001b[0m       70.1172  0.0297\n",
      "     10       49.4555       71.3078  0.0284\n",
      "Restoring best model from epoch 2.\n",
      "     10       60.3766       57.8328  0.0394\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m68.4527\u001b[0m       \u001b[32m69.3845\u001b[0m  0.0519\n",
      "      2       \u001b[36m67.8798\u001b[0m       \u001b[32m65.1105\u001b[0m  0.0484\n",
      "      3       68.5889       \u001b[32m63.5930\u001b[0m  0.0503\n",
      "      4       \u001b[36m67.1854\u001b[0m       \u001b[32m62.8195\u001b[0m  0.0482\n",
      "      5       \u001b[36m66.3549\u001b[0m       63.0109  0.0480\n",
      "      6       \u001b[36m66.2851\u001b[0m       64.9717  0.0484\n",
      "      7       \u001b[36m65.1370\u001b[0m       65.0848  0.0468\n",
      "      8       65.1989       65.4174  0.0463\n",
      "      9       \u001b[36m64.3750\u001b[0m       64.9655  0.0446\n",
      "     10       64.4051       64.9642  0.0491\n",
      "Restoring best model from epoch 4.\n",
      "y_train breslow final [-1.000e+00 -1.000e+00 -5.000e+00 -5.000e+00 -7.000e+00 -9.000e+00\n",
      " -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01\n",
      " -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01\n",
      " -1.100e+01 -1.600e+01 -1.900e+01 -2.100e+01 -2.400e+01 -2.600e+01\n",
      " -3.000e+01 -3.000e+01 -3.000e+01 -3.000e+01 -3.100e+01 -3.100e+01\n",
      " -3.100e+01 -3.100e+01 -3.400e+01 -3.400e+01 -4.900e+01 -5.100e+01\n",
      " -5.200e+01 -5.900e+01 -6.400e+01 -7.000e+01 -7.200e+01 -7.600e+01\n",
      " -7.800e+01 -7.800e+01 -7.900e+01 -8.000e+01 -9.000e+01 -9.200e+01\n",
      " -9.800e+01 -1.180e+02 -1.340e+02 -1.490e+02  1.580e+02 -1.600e+02\n",
      "  1.600e+02 -1.620e+02 -1.630e+02  1.720e+02 -1.720e+02  1.740e+02\n",
      " -1.780e+02 -1.870e+02 -1.960e+02  1.970e+02 -2.000e+02 -2.100e+02\n",
      " -2.130e+02 -2.150e+02 -2.170e+02 -2.180e+02 -2.220e+02 -2.240e+02\n",
      "  2.240e+02 -2.250e+02  2.270e+02 -2.270e+02 -2.310e+02 -2.420e+02\n",
      " -2.430e+02 -2.500e+02  2.550e+02 -2.580e+02 -2.590e+02 -2.660e+02\n",
      "  2.660e+02 -2.710e+02 -2.730e+02 -2.750e+02 -2.850e+02 -2.870e+02\n",
      " -2.880e+02 -2.930e+02  2.950e+02 -2.970e+02 -3.000e+02 -3.020e+02\n",
      "  3.020e+02 -3.030e+02 -3.030e+02 -3.040e+02 -3.040e+02 -3.040e+02\n",
      " -3.040e+02 -3.070e+02 -3.130e+02 -3.170e+02 -3.170e+02  3.200e+02\n",
      " -3.200e+02  3.220e+02 -3.220e+02 -3.230e+02 -3.260e+02 -3.280e+02\n",
      " -3.320e+02 -3.340e+02  3.360e+02 -3.360e+02 -3.370e+02 -3.390e+02\n",
      " -3.430e+02 -3.430e+02 -3.450e+02 -3.450e+02  3.480e+02 -3.480e+02\n",
      " -3.500e+02 -3.580e+02 -3.580e+02 -3.600e+02  3.620e+02 -3.630e+02\n",
      " -3.640e+02 -3.650e+02 -3.650e+02 -3.650e+02  3.650e+02 -3.650e+02\n",
      " -3.660e+02 -3.680e+02 -3.700e+02 -3.710e+02 -3.710e+02 -3.730e+02\n",
      " -3.730e+02 -3.750e+02 -3.750e+02 -3.750e+02 -3.750e+02 -3.760e+02\n",
      " -3.760e+02 -3.790e+02 -3.800e+02 -3.800e+02 -3.820e+02 -3.830e+02\n",
      " -3.830e+02  3.850e+02 -3.850e+02  3.850e+02 -3.850e+02 -3.850e+02\n",
      " -3.930e+02 -3.930e+02 -3.940e+02 -3.940e+02 -3.950e+02 -3.960e+02\n",
      " -3.960e+02 -3.960e+02 -3.970e+02 -3.980e+02 -4.030e+02 -4.030e+02\n",
      " -4.040e+02 -4.050e+02 -4.080e+02 -4.080e+02 -4.110e+02 -4.120e+02\n",
      " -4.130e+02 -4.140e+02 -4.160e+02 -4.180e+02 -4.210e+02 -4.240e+02\n",
      " -4.250e+02 -4.260e+02  4.260e+02 -4.260e+02 -4.280e+02 -4.280e+02\n",
      " -4.310e+02 -4.310e+02 -4.330e+02 -4.340e+02 -4.350e+02 -4.370e+02\n",
      " -4.390e+02 -4.390e+02 -4.390e+02 -4.400e+02 -4.410e+02 -4.410e+02\n",
      " -4.410e+02 -4.460e+02  4.460e+02 -4.470e+02 -4.470e+02 -4.470e+02\n",
      " -4.480e+02 -4.480e+02 -4.500e+02 -4.510e+02 -4.540e+02 -4.550e+02\n",
      " -4.560e+02 -4.570e+02 -4.580e+02 -4.610e+02 -4.630e+02 -4.660e+02\n",
      " -4.670e+02 -4.700e+02 -4.710e+02 -4.720e+02 -4.750e+02 -4.760e+02\n",
      " -4.770e+02 -4.770e+02 -4.770e+02 -4.800e+02 -4.820e+02 -4.880e+02\n",
      " -4.880e+02 -4.890e+02 -4.920e+02 -4.950e+02 -4.960e+02 -4.960e+02\n",
      " -4.990e+02 -4.990e+02 -5.010e+02 -5.010e+02 -5.020e+02 -5.040e+02\n",
      " -5.060e+02 -5.080e+02 -5.090e+02 -5.100e+02 -5.130e+02 -5.160e+02\n",
      " -5.180e+02 -5.180e+02 -5.180e+02 -5.190e+02 -5.190e+02 -5.210e+02\n",
      " -5.220e+02 -5.230e+02  5.240e+02 -5.260e+02 -5.280e+02 -5.280e+02\n",
      " -5.290e+02 -5.300e+02 -5.320e+02 -5.320e+02 -5.330e+02 -5.380e+02\n",
      " -5.380e+02  5.380e+02 -5.470e+02  5.480e+02 -5.490e+02 -5.510e+02\n",
      " -5.520e+02 -5.520e+02 -5.540e+02 -5.540e+02  5.580e+02 -5.620e+02\n",
      " -5.630e+02  5.630e+02 -5.650e+02 -5.670e+02 -5.680e+02  5.710e+02\n",
      " -5.720e+02  5.730e+02 -5.740e+02 -5.750e+02 -5.750e+02 -5.760e+02\n",
      " -5.770e+02 -5.790e+02 -5.790e+02 -5.840e+02  5.840e+02 -5.840e+02\n",
      " -5.850e+02 -5.860e+02 -5.880e+02 -5.880e+02 -5.900e+02 -5.910e+02\n",
      " -5.940e+02 -5.950e+02 -5.950e+02 -5.960e+02 -5.980e+02 -6.020e+02\n",
      " -6.060e+02 -6.070e+02 -6.080e+02 -6.110e+02 -6.110e+02 -6.120e+02\n",
      "  6.120e+02 -6.120e+02 -6.120e+02  6.140e+02  6.160e+02 -6.160e+02\n",
      " -6.180e+02 -6.200e+02 -6.200e+02 -6.200e+02 -6.220e+02 -6.220e+02\n",
      " -6.240e+02 -6.260e+02 -6.260e+02 -6.290e+02 -6.300e+02 -6.310e+02\n",
      " -6.350e+02 -6.350e+02 -6.390e+02  6.390e+02 -6.400e+02 -6.430e+02\n",
      " -6.430e+02 -6.440e+02 -6.480e+02 -6.520e+02 -6.580e+02 -6.590e+02\n",
      " -6.590e+02 -6.600e+02 -6.610e+02 -6.620e+02 -6.640e+02 -6.720e+02\n",
      " -6.750e+02 -6.770e+02  6.780e+02 -6.790e+02 -6.800e+02 -6.810e+02\n",
      " -6.940e+02 -6.980e+02 -7.010e+02 -7.020e+02 -7.060e+02 -7.070e+02\n",
      " -7.070e+02 -7.090e+02 -7.100e+02 -7.140e+02 -7.140e+02 -7.150e+02\n",
      " -7.150e+02  7.230e+02 -7.240e+02 -7.250e+02 -7.260e+02 -7.270e+02\n",
      " -7.270e+02 -7.270e+02 -7.280e+02 -7.380e+02 -7.420e+02 -7.450e+02\n",
      " -7.470e+02 -7.470e+02 -7.520e+02 -7.520e+02  7.540e+02 -7.540e+02\n",
      " -7.540e+02 -7.550e+02 -7.590e+02 -7.590e+02 -7.600e+02 -7.600e+02\n",
      " -7.610e+02 -7.620e+02 -7.620e+02 -7.620e+02 -7.650e+02 -7.670e+02\n",
      " -7.690e+02 -7.770e+02 -7.830e+02 -7.850e+02  7.850e+02  7.860e+02\n",
      " -7.890e+02 -7.910e+02  7.920e+02  7.920e+02 -7.920e+02 -8.030e+02\n",
      " -8.060e+02  8.110e+02 -8.120e+02 -8.190e+02  8.210e+02 -8.220e+02\n",
      "  8.250e+02 -8.290e+02 -8.370e+02 -8.390e+02 -8.470e+02 -8.490e+02\n",
      " -8.520e+02 -8.580e+02  8.600e+02 -8.650e+02 -8.670e+02 -8.750e+02\n",
      " -8.780e+02 -8.810e+02  8.830e+02 -8.830e+02 -8.890e+02 -8.900e+02\n",
      "  9.040e+02 -9.060e+02 -9.070e+02 -9.110e+02  9.120e+02 -9.120e+02\n",
      " -9.120e+02 -9.150e+02  9.210e+02 -9.230e+02 -9.260e+02 -9.310e+02\n",
      " -9.310e+02 -9.420e+02 -9.430e+02  9.430e+02 -9.430e+02 -9.520e+02\n",
      " -9.540e+02 -9.650e+02 -9.650e+02 -9.680e+02 -9.720e+02 -9.730e+02\n",
      " -9.740e+02 -9.740e+02 -9.750e+02 -9.750e+02  9.760e+02 -9.890e+02\n",
      "  9.910e+02  9.910e+02 -9.960e+02 -9.960e+02 -1.000e+03 -1.001e+03\n",
      " -1.001e+03 -1.001e+03  1.004e+03 -1.004e+03 -1.004e+03 -1.005e+03\n",
      " -1.006e+03 -1.007e+03  1.009e+03 -1.009e+03 -1.015e+03 -1.025e+03\n",
      " -1.026e+03 -1.026e+03 -1.027e+03  1.032e+03  1.034e+03  1.034e+03\n",
      " -1.035e+03 -1.039e+03 -1.043e+03 -1.043e+03 -1.047e+03  1.048e+03\n",
      " -1.051e+03 -1.051e+03 -1.059e+03 -1.062e+03 -1.063e+03 -1.066e+03\n",
      "  1.072e+03 -1.074e+03 -1.079e+03 -1.085e+03 -1.088e+03 -1.093e+03\n",
      "  1.093e+03 -1.101e+03  1.104e+03 -1.106e+03 -1.120e+03 -1.121e+03\n",
      " -1.124e+03  1.127e+03 -1.132e+03 -1.133e+03 -1.138e+03 -1.139e+03\n",
      " -1.140e+03 -1.141e+03  1.142e+03 -1.148e+03 -1.150e+03 -1.156e+03\n",
      " -1.156e+03 -1.157e+03 -1.161e+03 -1.162e+03 -1.163e+03 -1.165e+03\n",
      " -1.167e+03 -1.174e+03 -1.179e+03 -1.185e+03 -1.186e+03 -1.189e+03\n",
      " -1.191e+03 -1.196e+03 -1.198e+03 -1.203e+03 -1.206e+03 -1.208e+03\n",
      " -1.210e+03 -1.217e+03 -1.220e+03 -1.220e+03 -1.229e+03 -1.229e+03\n",
      " -1.232e+03 -1.239e+03 -1.246e+03 -1.247e+03 -1.248e+03 -1.251e+03\n",
      " -1.269e+03  1.275e+03 -1.277e+03 -1.285e+03  1.286e+03 -1.288e+03\n",
      " -1.291e+03 -1.301e+03 -1.308e+03 -1.308e+03 -1.308e+03 -1.309e+03\n",
      " -1.309e+03 -1.318e+03 -1.321e+03  1.324e+03 -1.326e+03 -1.330e+03\n",
      " -1.347e+03 -1.351e+03 -1.359e+03 -1.363e+03  1.365e+03 -1.371e+03\n",
      " -1.371e+03  1.388e+03 -1.417e+03 -1.417e+03 -1.419e+03 -1.434e+03\n",
      " -1.437e+03  1.439e+03 -1.448e+03 -1.461e+03 -1.463e+03 -1.471e+03\n",
      " -1.474e+03 -1.474e+03 -1.476e+03 -1.477e+03 -1.492e+03  1.508e+03\n",
      " -1.519e+03 -1.522e+03 -1.523e+03 -1.528e+03 -1.532e+03 -1.534e+03\n",
      " -1.542e+03 -1.545e+03 -1.545e+03 -1.546e+03 -1.547e+03 -1.548e+03\n",
      " -1.550e+03  1.556e+03 -1.561e+03 -1.563e+03 -1.569e+03 -1.572e+03\n",
      " -1.587e+03 -1.596e+03 -1.604e+03 -1.604e+03 -1.611e+03 -1.611e+03\n",
      " -1.611e+03 -1.612e+03 -1.613e+03 -1.616e+03 -1.620e+03 -1.631e+03\n",
      " -1.639e+03 -1.640e+03  1.642e+03 -1.642e+03 -1.644e+03 -1.648e+03\n",
      "  1.649e+03 -1.662e+03 -1.673e+03 -1.673e+03  1.673e+03 -1.682e+03\n",
      " -1.683e+03 -1.686e+03 -1.688e+03  1.692e+03 -1.692e+03  1.699e+03\n",
      " -1.712e+03 -1.728e+03 -1.728e+03 -1.732e+03 -1.742e+03  1.759e+03\n",
      " -1.780e+03  1.781e+03 -1.783e+03 -1.800e+03  1.812e+03 -1.820e+03\n",
      " -1.836e+03 -1.842e+03 -1.847e+03 -1.853e+03 -1.855e+03 -1.864e+03\n",
      " -1.866e+03 -1.871e+03 -1.873e+03 -1.876e+03 -1.882e+03 -1.887e+03\n",
      "  1.900e+03 -1.914e+03 -1.919e+03 -1.925e+03 -1.928e+03 -1.935e+03\n",
      " -1.953e+03 -1.980e+03  1.993e+03 -1.996e+03  2.009e+03 -2.012e+03\n",
      " -2.019e+03 -2.031e+03 -2.033e+03 -2.041e+03 -2.064e+03 -2.072e+03\n",
      " -2.082e+03 -2.091e+03 -2.108e+03 -2.109e+03 -2.124e+03  2.127e+03\n",
      " -2.128e+03 -2.136e+03 -2.140e+03 -2.142e+03 -2.155e+03 -2.156e+03\n",
      " -2.161e+03 -2.184e+03 -2.190e+03 -2.190e+03 -2.191e+03  2.192e+03\n",
      " -2.193e+03 -2.197e+03  2.207e+03 -2.222e+03 -2.231e+03 -2.240e+03\n",
      " -2.246e+03 -2.248e+03 -2.255e+03 -2.263e+03 -2.278e+03 -2.288e+03\n",
      " -2.289e+03  2.296e+03 -2.306e+03 -2.311e+03 -2.329e+03 -2.330e+03\n",
      " -2.335e+03  2.348e+03  2.361e+03 -2.362e+03 -2.365e+03 -2.371e+03\n",
      " -2.372e+03 -2.385e+03 -2.403e+03 -2.426e+03 -2.442e+03  2.469e+03\n",
      " -2.477e+03  2.483e+03 -2.513e+03 -2.515e+03  2.520e+03  2.534e+03\n",
      " -2.535e+03 -2.541e+03  2.551e+03 -2.558e+03 -2.559e+03  2.573e+03\n",
      " -2.596e+03 -2.596e+03 -2.605e+03 -2.612e+03 -2.618e+03 -2.629e+03\n",
      " -2.632e+03 -2.632e+03  2.636e+03 -2.645e+03 -2.645e+03 -2.650e+03\n",
      " -2.653e+03 -2.654e+03 -2.695e+03 -2.709e+03  2.712e+03 -2.721e+03\n",
      " -2.759e+03  2.763e+03 -2.767e+03 -2.770e+03  2.798e+03 -2.813e+03\n",
      " -2.838e+03  2.854e+03  2.866e+03 -2.868e+03  2.911e+03 -2.920e+03\n",
      "  2.965e+03  2.965e+03 -2.971e+03 -2.976e+03 -2.991e+03 -3.001e+03\n",
      " -3.004e+03 -3.009e+03 -3.011e+03 -3.015e+03 -3.021e+03 -3.022e+03\n",
      " -3.030e+03 -3.035e+03  3.063e+03 -3.072e+03 -3.091e+03 -3.094e+03\n",
      " -3.102e+03 -3.112e+03 -3.128e+03 -3.152e+03 -3.159e+03 -3.172e+03\n",
      " -3.202e+03 -3.203e+03 -3.204e+03 -3.226e+03 -3.247e+03 -3.248e+03\n",
      " -3.256e+03 -3.261e+03  3.262e+03 -3.276e+03 -3.283e+03 -3.283e+03\n",
      " -3.286e+03 -3.287e+03 -3.296e+03 -3.307e+03 -3.342e+03 -3.361e+03\n",
      " -3.364e+03  3.409e+03 -3.430e+03 -3.456e+03  3.461e+03  3.462e+03\n",
      "  3.472e+03  3.492e+03 -3.519e+03 -3.603e+03 -3.660e+03  3.669e+03\n",
      " -3.709e+03  3.736e+03 -3.807e+03  3.873e+03  3.926e+03  3.945e+03\n",
      " -3.957e+03  3.959e+03 -4.005e+03 -4.047e+03 -4.052e+03 -4.080e+03\n",
      " -4.088e+03 -4.159e+03 -4.275e+03 -4.285e+03 -4.361e+03  4.456e+03\n",
      " -4.929e+03 -5.042e+03 -5.062e+03 -5.156e+03 -5.739e+03  6.456e+03\n",
      "  6.593e+03 -7.106e+03  7.455e+03 -8.008e+03 -8.391e+03 -8.556e+03]\n",
      "Concordance Index 0.6414717291642177\n",
      "Integrated Brier Score: 0.19099640177029975\n",
      "y_train breslow final [-1.000e+00 -1.000e+00 -5.000e+00 -5.000e+00 -7.000e+00 -9.000e+00\n",
      " -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01\n",
      " -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01\n",
      " -1.100e+01 -1.600e+01 -1.900e+01 -2.100e+01 -2.400e+01 -2.600e+01\n",
      " -3.000e+01 -3.000e+01 -3.000e+01 -3.000e+01 -3.100e+01 -3.100e+01\n",
      " -3.100e+01 -3.100e+01 -3.400e+01 -3.400e+01 -4.900e+01 -5.100e+01\n",
      " -5.200e+01 -5.900e+01 -6.400e+01 -7.000e+01 -7.200e+01 -7.600e+01\n",
      " -7.800e+01 -7.800e+01 -7.900e+01 -8.000e+01 -9.000e+01 -9.200e+01\n",
      " -9.800e+01 -1.180e+02 -1.340e+02 -1.490e+02  1.580e+02 -1.600e+02\n",
      "  1.600e+02 -1.620e+02 -1.630e+02  1.720e+02 -1.720e+02  1.740e+02\n",
      " -1.780e+02 -1.870e+02 -1.960e+02  1.970e+02 -2.000e+02 -2.100e+02\n",
      " -2.130e+02 -2.150e+02 -2.170e+02 -2.180e+02 -2.220e+02 -2.240e+02\n",
      "  2.240e+02 -2.250e+02  2.270e+02 -2.270e+02 -2.310e+02 -2.420e+02\n",
      " -2.430e+02 -2.500e+02  2.550e+02 -2.580e+02 -2.590e+02 -2.660e+02\n",
      "  2.660e+02 -2.710e+02 -2.730e+02 -2.750e+02 -2.850e+02 -2.870e+02\n",
      " -2.880e+02 -2.930e+02  2.950e+02 -2.970e+02 -3.000e+02 -3.020e+02\n",
      "  3.020e+02 -3.030e+02 -3.030e+02 -3.040e+02 -3.040e+02 -3.040e+02\n",
      " -3.040e+02 -3.070e+02 -3.130e+02 -3.170e+02 -3.170e+02  3.200e+02\n",
      " -3.200e+02  3.220e+02 -3.220e+02 -3.230e+02 -3.260e+02 -3.280e+02\n",
      " -3.320e+02 -3.340e+02  3.360e+02 -3.360e+02 -3.370e+02 -3.390e+02\n",
      " -3.430e+02 -3.430e+02 -3.450e+02 -3.450e+02  3.480e+02 -3.480e+02\n",
      " -3.500e+02 -3.580e+02 -3.580e+02 -3.600e+02  3.620e+02 -3.630e+02\n",
      " -3.640e+02 -3.650e+02 -3.650e+02 -3.650e+02  3.650e+02 -3.650e+02\n",
      " -3.660e+02 -3.680e+02 -3.700e+02 -3.710e+02 -3.710e+02 -3.730e+02\n",
      " -3.730e+02 -3.750e+02 -3.750e+02 -3.750e+02 -3.750e+02 -3.760e+02\n",
      " -3.760e+02 -3.790e+02 -3.800e+02 -3.800e+02 -3.820e+02 -3.830e+02\n",
      " -3.830e+02  3.850e+02 -3.850e+02  3.850e+02 -3.850e+02 -3.850e+02\n",
      " -3.930e+02 -3.930e+02 -3.940e+02 -3.940e+02 -3.950e+02 -3.960e+02\n",
      " -3.960e+02 -3.960e+02 -3.970e+02 -3.980e+02 -4.030e+02 -4.030e+02\n",
      " -4.040e+02 -4.050e+02 -4.080e+02 -4.080e+02 -4.110e+02 -4.120e+02\n",
      " -4.130e+02 -4.140e+02 -4.160e+02 -4.180e+02 -4.210e+02 -4.240e+02\n",
      " -4.250e+02 -4.260e+02  4.260e+02 -4.260e+02 -4.280e+02 -4.280e+02\n",
      " -4.310e+02 -4.310e+02 -4.330e+02 -4.340e+02 -4.350e+02 -4.370e+02\n",
      " -4.390e+02 -4.390e+02 -4.390e+02 -4.400e+02 -4.410e+02 -4.410e+02\n",
      " -4.410e+02 -4.460e+02  4.460e+02 -4.470e+02 -4.470e+02 -4.470e+02\n",
      " -4.480e+02 -4.480e+02 -4.500e+02 -4.510e+02 -4.540e+02 -4.550e+02\n",
      " -4.560e+02 -4.570e+02 -4.580e+02 -4.610e+02 -4.630e+02 -4.660e+02\n",
      " -4.670e+02 -4.700e+02 -4.710e+02 -4.720e+02 -4.750e+02 -4.760e+02\n",
      " -4.770e+02 -4.770e+02 -4.770e+02 -4.800e+02 -4.820e+02 -4.880e+02\n",
      " -4.880e+02 -4.890e+02 -4.920e+02 -4.950e+02 -4.960e+02 -4.960e+02\n",
      " -4.990e+02 -4.990e+02 -5.010e+02 -5.010e+02 -5.020e+02 -5.040e+02\n",
      " -5.060e+02 -5.080e+02 -5.090e+02 -5.100e+02 -5.130e+02 -5.160e+02\n",
      " -5.180e+02 -5.180e+02 -5.180e+02 -5.190e+02 -5.190e+02 -5.210e+02\n",
      " -5.220e+02 -5.230e+02  5.240e+02 -5.260e+02 -5.280e+02 -5.280e+02\n",
      " -5.290e+02 -5.300e+02 -5.320e+02 -5.320e+02 -5.330e+02 -5.380e+02\n",
      " -5.380e+02  5.380e+02 -5.470e+02  5.480e+02 -5.490e+02 -5.510e+02\n",
      " -5.520e+02 -5.520e+02 -5.540e+02 -5.540e+02  5.580e+02 -5.620e+02\n",
      " -5.630e+02  5.630e+02 -5.650e+02 -5.670e+02 -5.680e+02  5.710e+02\n",
      " -5.720e+02  5.730e+02 -5.740e+02 -5.750e+02 -5.750e+02 -5.760e+02\n",
      " -5.770e+02 -5.790e+02 -5.790e+02 -5.840e+02  5.840e+02 -5.840e+02\n",
      " -5.850e+02 -5.860e+02 -5.880e+02 -5.880e+02 -5.900e+02 -5.910e+02\n",
      " -5.940e+02 -5.950e+02 -5.950e+02 -5.960e+02 -5.980e+02 -6.020e+02\n",
      " -6.060e+02 -6.070e+02 -6.080e+02 -6.110e+02 -6.110e+02 -6.120e+02\n",
      "  6.120e+02 -6.120e+02 -6.120e+02  6.140e+02  6.160e+02 -6.160e+02\n",
      " -6.180e+02 -6.200e+02 -6.200e+02 -6.200e+02 -6.220e+02 -6.220e+02\n",
      " -6.240e+02 -6.260e+02 -6.260e+02 -6.290e+02 -6.300e+02 -6.310e+02\n",
      " -6.350e+02 -6.350e+02 -6.390e+02  6.390e+02 -6.400e+02 -6.430e+02\n",
      " -6.430e+02 -6.440e+02 -6.480e+02 -6.520e+02 -6.580e+02 -6.590e+02\n",
      " -6.590e+02 -6.600e+02 -6.610e+02 -6.620e+02 -6.640e+02 -6.720e+02\n",
      " -6.750e+02 -6.770e+02  6.780e+02 -6.790e+02 -6.800e+02 -6.810e+02\n",
      " -6.940e+02 -6.980e+02 -7.010e+02 -7.020e+02 -7.060e+02 -7.070e+02\n",
      " -7.070e+02 -7.090e+02 -7.100e+02 -7.140e+02 -7.140e+02 -7.150e+02\n",
      " -7.150e+02  7.230e+02 -7.240e+02 -7.250e+02 -7.260e+02 -7.270e+02\n",
      " -7.270e+02 -7.270e+02 -7.280e+02 -7.380e+02 -7.420e+02 -7.450e+02\n",
      " -7.470e+02 -7.470e+02 -7.520e+02 -7.520e+02  7.540e+02 -7.540e+02\n",
      " -7.540e+02 -7.550e+02 -7.590e+02 -7.590e+02 -7.600e+02 -7.600e+02\n",
      " -7.610e+02 -7.620e+02 -7.620e+02 -7.620e+02 -7.650e+02 -7.670e+02\n",
      " -7.690e+02 -7.770e+02 -7.830e+02 -7.850e+02  7.850e+02  7.860e+02\n",
      " -7.890e+02 -7.910e+02  7.920e+02  7.920e+02 -7.920e+02 -8.030e+02\n",
      " -8.060e+02  8.110e+02 -8.120e+02 -8.190e+02  8.210e+02 -8.220e+02\n",
      "  8.250e+02 -8.290e+02 -8.370e+02 -8.390e+02 -8.470e+02 -8.490e+02\n",
      " -8.520e+02 -8.580e+02  8.600e+02 -8.650e+02 -8.670e+02 -8.750e+02\n",
      " -8.780e+02 -8.810e+02  8.830e+02 -8.830e+02 -8.890e+02 -8.900e+02\n",
      "  9.040e+02 -9.060e+02 -9.070e+02 -9.110e+02  9.120e+02 -9.120e+02\n",
      " -9.120e+02 -9.150e+02  9.210e+02 -9.230e+02 -9.260e+02 -9.310e+02\n",
      " -9.310e+02 -9.420e+02 -9.430e+02  9.430e+02 -9.430e+02 -9.520e+02\n",
      " -9.540e+02 -9.650e+02 -9.650e+02 -9.680e+02 -9.720e+02 -9.730e+02\n",
      " -9.740e+02 -9.740e+02 -9.750e+02 -9.750e+02  9.760e+02 -9.890e+02\n",
      "  9.910e+02  9.910e+02 -9.960e+02 -9.960e+02 -1.000e+03 -1.001e+03\n",
      " -1.001e+03 -1.001e+03  1.004e+03 -1.004e+03 -1.004e+03 -1.005e+03\n",
      " -1.006e+03 -1.007e+03  1.009e+03 -1.009e+03 -1.015e+03 -1.025e+03\n",
      " -1.026e+03 -1.026e+03 -1.027e+03  1.032e+03  1.034e+03  1.034e+03\n",
      " -1.035e+03 -1.039e+03 -1.043e+03 -1.043e+03 -1.047e+03  1.048e+03\n",
      " -1.051e+03 -1.051e+03 -1.059e+03 -1.062e+03 -1.063e+03 -1.066e+03\n",
      "  1.072e+03 -1.074e+03 -1.079e+03 -1.085e+03 -1.088e+03 -1.093e+03\n",
      "  1.093e+03 -1.101e+03  1.104e+03 -1.106e+03 -1.120e+03 -1.121e+03\n",
      " -1.124e+03  1.127e+03 -1.132e+03 -1.133e+03 -1.138e+03 -1.139e+03\n",
      " -1.140e+03 -1.141e+03  1.142e+03 -1.148e+03 -1.150e+03 -1.156e+03\n",
      " -1.156e+03 -1.157e+03 -1.161e+03 -1.162e+03 -1.163e+03 -1.165e+03\n",
      " -1.167e+03 -1.174e+03 -1.179e+03 -1.185e+03 -1.186e+03 -1.189e+03\n",
      " -1.191e+03 -1.196e+03 -1.198e+03 -1.203e+03 -1.206e+03 -1.208e+03\n",
      " -1.210e+03 -1.217e+03 -1.220e+03 -1.220e+03 -1.229e+03 -1.229e+03\n",
      " -1.232e+03 -1.239e+03 -1.246e+03 -1.247e+03 -1.248e+03 -1.251e+03\n",
      " -1.269e+03  1.275e+03 -1.277e+03 -1.285e+03  1.286e+03 -1.288e+03\n",
      " -1.291e+03 -1.301e+03 -1.308e+03 -1.308e+03 -1.308e+03 -1.309e+03\n",
      " -1.309e+03 -1.318e+03 -1.321e+03  1.324e+03 -1.326e+03 -1.330e+03\n",
      " -1.347e+03 -1.351e+03 -1.359e+03 -1.363e+03  1.365e+03 -1.371e+03\n",
      " -1.371e+03  1.388e+03 -1.417e+03 -1.417e+03 -1.419e+03 -1.434e+03\n",
      " -1.437e+03  1.439e+03 -1.448e+03 -1.461e+03 -1.463e+03 -1.471e+03\n",
      " -1.474e+03 -1.474e+03 -1.476e+03 -1.477e+03 -1.492e+03  1.508e+03\n",
      " -1.519e+03 -1.522e+03 -1.523e+03 -1.528e+03 -1.532e+03 -1.534e+03\n",
      " -1.542e+03 -1.545e+03 -1.545e+03 -1.546e+03 -1.547e+03 -1.548e+03\n",
      " -1.550e+03  1.556e+03 -1.561e+03 -1.563e+03 -1.569e+03 -1.572e+03\n",
      " -1.587e+03 -1.596e+03 -1.604e+03 -1.604e+03 -1.611e+03 -1.611e+03\n",
      " -1.611e+03 -1.612e+03 -1.613e+03 -1.616e+03 -1.620e+03 -1.631e+03\n",
      " -1.639e+03 -1.640e+03  1.642e+03 -1.642e+03 -1.644e+03 -1.648e+03\n",
      "  1.649e+03 -1.662e+03 -1.673e+03 -1.673e+03  1.673e+03 -1.682e+03\n",
      " -1.683e+03 -1.686e+03 -1.688e+03  1.692e+03 -1.692e+03  1.699e+03\n",
      " -1.712e+03 -1.728e+03 -1.728e+03 -1.732e+03 -1.742e+03  1.759e+03\n",
      " -1.780e+03  1.781e+03 -1.783e+03 -1.800e+03  1.812e+03 -1.820e+03\n",
      " -1.836e+03 -1.842e+03 -1.847e+03 -1.853e+03 -1.855e+03 -1.864e+03\n",
      " -1.866e+03 -1.871e+03 -1.873e+03 -1.876e+03 -1.882e+03 -1.887e+03\n",
      "  1.900e+03 -1.914e+03 -1.919e+03 -1.925e+03 -1.928e+03 -1.935e+03\n",
      " -1.953e+03 -1.980e+03  1.993e+03 -1.996e+03  2.009e+03 -2.012e+03\n",
      " -2.019e+03 -2.031e+03 -2.033e+03 -2.041e+03 -2.064e+03 -2.072e+03\n",
      " -2.082e+03 -2.091e+03 -2.108e+03 -2.109e+03 -2.124e+03  2.127e+03\n",
      " -2.128e+03 -2.136e+03 -2.140e+03 -2.142e+03 -2.155e+03 -2.156e+03\n",
      " -2.161e+03 -2.184e+03 -2.190e+03 -2.190e+03 -2.191e+03  2.192e+03\n",
      " -2.193e+03 -2.197e+03  2.207e+03 -2.222e+03 -2.231e+03 -2.240e+03\n",
      " -2.246e+03 -2.248e+03 -2.255e+03 -2.263e+03 -2.278e+03 -2.288e+03\n",
      " -2.289e+03  2.296e+03 -2.306e+03 -2.311e+03 -2.329e+03 -2.330e+03\n",
      " -2.335e+03  2.348e+03  2.361e+03 -2.362e+03 -2.365e+03 -2.371e+03\n",
      " -2.372e+03 -2.385e+03 -2.403e+03 -2.426e+03 -2.442e+03  2.469e+03\n",
      " -2.477e+03  2.483e+03 -2.513e+03 -2.515e+03  2.520e+03  2.534e+03\n",
      " -2.535e+03 -2.541e+03  2.551e+03 -2.558e+03 -2.559e+03  2.573e+03\n",
      " -2.596e+03 -2.596e+03 -2.605e+03 -2.612e+03 -2.618e+03 -2.629e+03\n",
      " -2.632e+03 -2.632e+03  2.636e+03 -2.645e+03 -2.645e+03 -2.650e+03\n",
      " -2.653e+03 -2.654e+03 -2.695e+03 -2.709e+03  2.712e+03 -2.721e+03\n",
      " -2.759e+03  2.763e+03 -2.767e+03 -2.770e+03  2.798e+03 -2.813e+03\n",
      " -2.838e+03  2.854e+03  2.866e+03 -2.868e+03  2.911e+03 -2.920e+03\n",
      "  2.965e+03  2.965e+03 -2.971e+03 -2.976e+03 -2.991e+03 -3.001e+03\n",
      " -3.004e+03 -3.009e+03 -3.011e+03 -3.015e+03 -3.021e+03 -3.022e+03\n",
      " -3.030e+03 -3.035e+03  3.063e+03 -3.072e+03 -3.091e+03 -3.094e+03\n",
      " -3.102e+03 -3.112e+03 -3.128e+03 -3.152e+03 -3.159e+03 -3.172e+03\n",
      " -3.202e+03 -3.203e+03 -3.204e+03 -3.226e+03 -3.247e+03 -3.248e+03\n",
      " -3.256e+03 -3.261e+03  3.262e+03 -3.276e+03 -3.283e+03 -3.283e+03\n",
      " -3.286e+03 -3.287e+03 -3.296e+03 -3.307e+03 -3.342e+03 -3.361e+03\n",
      " -3.364e+03  3.409e+03 -3.430e+03 -3.456e+03  3.461e+03  3.462e+03\n",
      "  3.472e+03  3.492e+03 -3.519e+03 -3.603e+03 -3.660e+03  3.669e+03\n",
      " -3.709e+03  3.736e+03 -3.807e+03  3.873e+03  3.926e+03  3.945e+03\n",
      " -3.957e+03  3.959e+03 -4.005e+03 -4.047e+03 -4.052e+03 -4.080e+03\n",
      " -4.088e+03 -4.159e+03 -4.275e+03 -4.285e+03 -4.361e+03  4.456e+03\n",
      " -4.929e+03 -5.042e+03 -5.062e+03 -5.156e+03 -5.739e+03  6.456e+03\n",
      "  6.593e+03 -7.106e+03  7.455e+03 -8.008e+03 -8.391e+03 -8.556e+03]\n",
      "durations 5.0 8605.0\n",
      "Concordance Index 0.5381944444444444\n",
      "Integrated Brier Score: 0.24450130850101098\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m166.2278\u001b[0m      \u001b[32m112.1142\u001b[0m  0.0773\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m79.1207\u001b[0m       \u001b[32m81.3508\u001b[0m  0.0872\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2      \u001b[36m159.9155\u001b[0m      113.2174  0.0372\n",
      "      2       \u001b[36m71.9729\u001b[0m       \u001b[32m75.9502\u001b[0m  0.0584\n",
      "      3      162.0939      \u001b[32m107.7626\u001b[0m  0.0395\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m179.1424\u001b[0m      \u001b[32m129.0722\u001b[0m  0.0504\n",
      "      4      160.3162      \u001b[32m103.1379\u001b[0m  0.0356\n",
      "      3       \u001b[36m67.8452\u001b[0m       \u001b[32m73.6712\u001b[0m  0.0527\n",
      "      2      \u001b[36m171.6364\u001b[0m      \u001b[32m126.9213\u001b[0m  0.0489\n",
      "      5      \u001b[36m154.9371\u001b[0m       \u001b[32m99.9599\u001b[0m  0.0438\n",
      "      4       68.4423       \u001b[32m72.3132\u001b[0m  0.0492\n",
      "      3      \u001b[36m169.4402\u001b[0m      \u001b[32m122.1958\u001b[0m  0.0408\n",
      "      6      155.7627       \u001b[32m98.2962\u001b[0m  0.0323\n",
      "      4      171.0787      \u001b[32m117.8654\u001b[0m  0.0373\n",
      "      7      \u001b[36m150.7393\u001b[0m       \u001b[32m97.6670\u001b[0m  0.0364\n",
      "      5       69.0046       \u001b[32m72.1036\u001b[0m  0.0539\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      5      \u001b[36m167.1888\u001b[0m      \u001b[32m114.6766\u001b[0m  0.0396\n",
      "      8      154.1427       97.8575  0.0399\n",
      "      6       \u001b[36m66.2828\u001b[0m       72.9151  0.0502\n",
      "      6      \u001b[36m165.8787\u001b[0m      \u001b[32m111.9367\u001b[0m  0.0363\n",
      "      9      153.2242       98.9654  0.0305\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m67.7950\u001b[0m       \u001b[32m57.2882\u001b[0m  0.0709\n",
      "      7      \u001b[36m162.3228\u001b[0m      \u001b[32m110.6165\u001b[0m  0.0322\n",
      "      7       \u001b[36m64.7694\u001b[0m       73.0621  0.0536\n",
      "     10      \u001b[36m149.1265\u001b[0m      100.1800  0.0395\n",
      "Restoring best model from epoch 7.\n",
      "      8      166.0399      \u001b[32m110.3480\u001b[0m  0.0394\n",
      "      2       \u001b[36m63.8204\u001b[0m       \u001b[32m45.9575\u001b[0m  0.0658\n",
      "      8       65.7937       72.7463  0.0544\n",
      "      9      164.7577      111.1332  0.0404\n",
      "      9       67.2876       72.1344  0.0544\n",
      "     10      162.8028      112.1985  0.0409\n",
      "      3       \u001b[36m62.7674\u001b[0m       46.0854  0.0604\n",
      "Restoring best model from epoch 8.\n",
      "      4       \u001b[36m58.0639\u001b[0m       49.8529  0.0486\n",
      "     10       66.0369       \u001b[32m70.8439\u001b[0m  0.0540\n",
      "      5       58.3918       50.7743  0.0529\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      6       \u001b[36m57.8872\u001b[0m       48.8366  0.0591\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m165.8280\u001b[0m       \u001b[32m61.0325\u001b[0m  0.0595\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m163.2511\u001b[0m       \u001b[32m91.3505\u001b[0m  0.0621\n",
      "      7       \u001b[36m57.5548\u001b[0m       48.9237  0.0470\n",
      "      2      \u001b[36m158.5236\u001b[0m       65.1023  0.0346\n",
      "      2      \u001b[36m157.2028\u001b[0m       \u001b[32m89.0668\u001b[0m  0.0416\n",
      "      8       \u001b[36m56.0194\u001b[0m       48.2207  0.0455\n",
      "      3      \u001b[36m153.1652\u001b[0m       65.1427  0.0362\n",
      "      3      \u001b[36m155.2906\u001b[0m       \u001b[32m86.7015\u001b[0m  0.0326\n",
      "      9       58.2344       48.5485  0.0439\n",
      "      4      154.7488       64.1349  0.0355\n",
      "      4      158.5843       \u001b[32m84.6722\u001b[0m  0.0315\n",
      "      5      \u001b[36m153.1617\u001b[0m       62.7253  0.0299\n",
      "      5      156.2647       \u001b[32m83.0854\u001b[0m  0.0284\n",
      "     10       58.2537       48.9127  0.0466\n",
      "Restoring best model from epoch 2.\n",
      "      6      154.0168       62.1217  0.0295\n",
      "      6      \u001b[36m153.0561\u001b[0m       \u001b[32m81.8096\u001b[0m  0.0308\n",
      "      7      \u001b[36m150.7494\u001b[0m       61.7021  0.0287\n",
      "      7      \u001b[36m148.8259\u001b[0m       \u001b[32m81.1296\u001b[0m  0.0278\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      8      \u001b[36m149.5242\u001b[0m       61.5807  0.0282\n",
      "      8      153.0715       \u001b[32m80.9987\u001b[0m  0.0288\n",
      "      9      149.7054       61.8222  0.0294\n",
      "      9      149.5078       81.2409  0.0346\n",
      "     10      \u001b[36m148.3274\u001b[0m       62.0584  0.0308\n",
      "Restoring best model from epoch 1.\n",
      "     10      148.9745       81.5762  0.0348\n",
      "Restoring best model from epoch 8.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m77.5540\u001b[0m       \u001b[32m62.0259\u001b[0m  0.0848\n",
      "      2       \u001b[36m70.5218\u001b[0m       \u001b[32m54.8487\u001b[0m  0.0469\n",
      "      3       \u001b[36m67.0456\u001b[0m       \u001b[32m53.8735\u001b[0m  0.0512\n",
      "      4       \u001b[36m65.3507\u001b[0m       55.3074  0.0430\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      5       \u001b[36m65.0836\u001b[0m       \u001b[32m53.5821\u001b[0m  0.0395\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m145.7547\u001b[0m       \u001b[32m52.1231\u001b[0m  0.0364\n",
      "      6       \u001b[36m65.0011\u001b[0m       \u001b[32m51.0404\u001b[0m  0.0392\n",
      "      2      \u001b[36m141.9648\u001b[0m       54.3190  0.0406\n",
      "      7       66.0519       52.1204  0.0424\n",
      "      3      \u001b[36m136.1848\u001b[0m       54.5368  0.0323\n",
      "      4      137.8549       54.2425  0.0264\n",
      "      8       \u001b[36m63.2854\u001b[0m       54.8222  0.0434\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      5      137.8234       53.9181  0.0258\n",
      "      9       \u001b[36m62.1354\u001b[0m       56.1404  0.0402\n",
      "      6      137.0543       53.5773  0.0252\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m86.2324\u001b[0m      \u001b[32m131.1211\u001b[0m  0.0479\n",
      "      7      \u001b[36m133.4981\u001b[0m       53.3419  0.0262\n",
      "     10       64.0321       55.7969  0.0392\n",
      "Restoring best model from epoch 6.\n",
      "      8      135.2643       53.5097  0.0276\n",
      "      2       \u001b[36m75.6874\u001b[0m       \u001b[32m96.3149\u001b[0m  0.0463\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m78.4035\u001b[0m       \u001b[32m88.3904\u001b[0m  0.0585\n",
      "      9      \u001b[36m131.4898\u001b[0m       53.9821  0.0279\n",
      "     10      \u001b[36m130.8632\u001b[0m       54.3758  0.0262\n",
      "      3       \u001b[36m75.5386\u001b[0m       96.3603  0.0482\n",
      "Restoring best model from epoch 1.\n",
      "      2       \u001b[36m72.2828\u001b[0m       \u001b[32m78.2153\u001b[0m  0.0450\n",
      "      4       \u001b[36m75.0631\u001b[0m       97.9147  0.0418\n",
      "      3       \u001b[36m69.0418\u001b[0m       78.2791  0.0406\n",
      "      5       \u001b[36m74.8078\u001b[0m       \u001b[32m94.4157\u001b[0m  0.0400\n",
      "      4       \u001b[36m68.2712\u001b[0m       \u001b[32m77.4933\u001b[0m  0.0382\n",
      "      6       \u001b[36m73.8284\u001b[0m       \u001b[32m93.3686\u001b[0m  0.0397\n",
      "      5       \u001b[36m68.0719\u001b[0m       78.2555  0.0428\n",
      "      6       \u001b[36m67.6305\u001b[0m       \u001b[32m77.4463\u001b[0m  0.0431\n",
      "      7       \u001b[36m71.9784\u001b[0m       94.3950  0.0523\n",
      "      7       \u001b[36m65.9477\u001b[0m       \u001b[32m76.8039\u001b[0m  0.0411\n",
      "      8       \u001b[36m71.1791\u001b[0m       96.4524  0.0402\n",
      "      8       \u001b[36m65.1268\u001b[0m       \u001b[32m76.2632\u001b[0m  0.0397\n",
      "      9       73.4206       96.6516  0.0384\n",
      "      9       \u001b[36m64.0251\u001b[0m       77.3130  0.0383\n",
      "     10       72.5630       94.2817  0.0382\n",
      "Restoring best model from epoch 6.\n",
      "     10       64.6992       77.2986  0.0390\n",
      "Restoring best model from epoch 8.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m159.7136\u001b[0m      \u001b[32m120.0238\u001b[0m  0.0327\n",
      "      2      \u001b[36m152.4079\u001b[0m      \u001b[32m119.9326\u001b[0m  0.0288\n",
      "      3      \u001b[36m149.8489\u001b[0m      \u001b[32m114.8568\u001b[0m  0.0286\n",
      "      4      \u001b[36m148.4414\u001b[0m      \u001b[32m111.1282\u001b[0m  0.0282\n",
      "      5      \u001b[36m146.3156\u001b[0m      \u001b[32m109.3119\u001b[0m  0.0292\n",
      "      6      \u001b[36m145.0510\u001b[0m      \u001b[32m108.8285\u001b[0m  0.0295\n",
      "      7      145.1893      \u001b[32m108.7564\u001b[0m  0.0286\n",
      "      8      146.3532      109.4021  0.0281\n",
      "      9      147.7318      110.2765  0.0287\n",
      "     10      \u001b[36m143.2900\u001b[0m      111.1389  0.0321\n",
      "Restoring best model from epoch 7.\n",
      "y_train breslow final [-1.000e+00 -1.000e+00 -5.000e+00 -5.000e+00 -8.000e+00 -9.000e+00\n",
      " -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01\n",
      " -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01 -1.100e+01\n",
      " -1.600e+01 -1.900e+01 -2.100e+01 -2.400e+01 -2.600e+01 -3.000e+01\n",
      " -3.000e+01 -3.100e+01 -3.100e+01 -3.400e+01 -4.000e+01 -5.100e+01\n",
      " -5.200e+01 -5.400e+01 -5.900e+01 -7.200e+01 -7.600e+01 -7.800e+01\n",
      " -7.800e+01 -7.800e+01 -7.900e+01 -8.000e+01 -8.400e+01 -9.800e+01\n",
      "  1.160e+02 -1.180e+02 -1.340e+02 -1.490e+02  1.580e+02 -1.600e+02\n",
      "  1.600e+02 -1.620e+02 -1.630e+02 -1.700e+02 -1.700e+02  1.720e+02\n",
      " -1.720e+02  1.740e+02 -1.780e+02 -1.860e+02 -1.870e+02 -1.960e+02\n",
      "  1.970e+02 -2.000e+02 -2.130e+02 -2.140e+02 -2.160e+02 -2.170e+02\n",
      " -2.180e+02 -2.220e+02 -2.240e+02  2.240e+02 -2.250e+02  2.270e+02\n",
      "  2.390e+02 -2.420e+02 -2.420e+02 -2.430e+02 -2.520e+02  2.550e+02\n",
      " -2.580e+02 -2.590e+02  2.660e+02 -2.710e+02 -2.730e+02 -2.730e+02\n",
      " -2.740e+02 -2.750e+02 -2.850e+02 -2.880e+02 -2.930e+02 -2.930e+02\n",
      " -2.970e+02 -3.020e+02  3.020e+02 -3.030e+02 -3.030e+02 -3.040e+02\n",
      " -3.040e+02  3.040e+02 -3.070e+02 -3.130e+02 -3.130e+02 -3.170e+02\n",
      "  3.200e+02 -3.200e+02 -3.220e+02 -3.220e+02 -3.230e+02 -3.320e+02\n",
      "  3.360e+02 -3.360e+02 -3.370e+02 -3.390e+02 -3.430e+02 -3.430e+02\n",
      " -3.450e+02 -3.470e+02  3.480e+02 -3.480e+02 -3.500e+02 -3.520e+02\n",
      " -3.580e+02 -3.580e+02 -3.600e+02  3.620e+02 -3.630e+02 -3.640e+02\n",
      " -3.650e+02 -3.650e+02 -3.650e+02 -3.650e+02 -3.660e+02 -3.660e+02\n",
      " -3.680e+02 -3.700e+02 -3.710e+02 -3.710e+02 -3.730e+02 -3.750e+02\n",
      " -3.750e+02 -3.750e+02 -3.750e+02 -3.750e+02 -3.760e+02  3.770e+02\n",
      " -3.790e+02 -3.800e+02 -3.810e+02 -3.810e+02 -3.820e+02 -3.830e+02\n",
      " -3.830e+02 -3.850e+02 -3.850e+02 -3.850e+02  3.850e+02 -3.850e+02\n",
      " -3.850e+02 -3.920e+02 -3.930e+02 -3.940e+02 -3.940e+02 -3.960e+02\n",
      " -3.960e+02 -3.960e+02 -3.960e+02 -3.970e+02 -3.980e+02 -4.020e+02\n",
      " -4.040e+02 -4.050e+02 -4.080e+02 -4.080e+02 -4.090e+02 -4.100e+02\n",
      " -4.100e+02 -4.110e+02 -4.130e+02 -4.140e+02 -4.160e+02 -4.170e+02\n",
      " -4.180e+02 -4.230e+02 -4.240e+02 -4.240e+02 -4.260e+02 -4.260e+02\n",
      " -4.280e+02 -4.300e+02 -4.310e+02 -4.330e+02 -4.340e+02 -4.350e+02\n",
      " -4.390e+02 -4.400e+02 -4.410e+02 -4.410e+02 -4.460e+02  4.460e+02\n",
      " -4.470e+02 -4.470e+02 -4.480e+02 -4.500e+02 -4.500e+02 -4.550e+02\n",
      " -4.570e+02 -4.610e+02 -4.610e+02 -4.630e+02 -4.630e+02 -4.660e+02\n",
      " -4.660e+02 -4.700e+02 -4.750e+02 -4.760e+02 -4.770e+02 -4.770e+02\n",
      " -4.800e+02 -4.880e+02 -4.880e+02 -4.920e+02 -4.950e+02 -4.960e+02\n",
      " -4.960e+02 -4.990e+02 -4.990e+02 -5.010e+02 -5.030e+02 -5.040e+02\n",
      " -5.060e+02 -5.080e+02 -5.090e+02 -5.100e+02 -5.110e+02 -5.130e+02\n",
      " -5.160e+02 -5.180e+02 -5.180e+02 -5.180e+02 -5.190e+02 -5.210e+02\n",
      " -5.220e+02  5.240e+02 -5.250e+02 -5.260e+02 -5.280e+02 -5.290e+02\n",
      " -5.300e+02 -5.300e+02 -5.310e+02 -5.320e+02 -5.320e+02 -5.320e+02\n",
      " -5.330e+02  5.380e+02 -5.410e+02 -5.410e+02 -5.450e+02 -5.460e+02\n",
      " -5.470e+02  5.480e+02 -5.510e+02 -5.520e+02 -5.520e+02 -5.520e+02\n",
      " -5.540e+02 -5.540e+02 -5.540e+02 -5.540e+02 -5.540e+02  5.580e+02\n",
      " -5.620e+02 -5.620e+02  5.630e+02 -5.650e+02 -5.660e+02  5.710e+02\n",
      " -5.720e+02 -5.740e+02 -5.740e+02 -5.750e+02 -5.750e+02 -5.750e+02\n",
      " -5.760e+02 -5.770e+02 -5.770e+02 -5.770e+02 -5.790e+02 -5.790e+02\n",
      " -5.840e+02 -5.840e+02  5.840e+02 -5.840e+02 -5.850e+02 -5.860e+02\n",
      " -5.880e+02 -5.880e+02 -5.900e+02 -5.910e+02 -5.950e+02 -5.950e+02\n",
      " -5.960e+02 -5.980e+02 -6.000e+02 -6.020e+02 -6.060e+02 -6.070e+02\n",
      " -6.070e+02 -6.080e+02  6.120e+02 -6.120e+02 -6.120e+02 -6.140e+02\n",
      "  6.140e+02  6.160e+02 -6.160e+02 -6.180e+02 -6.200e+02 -6.200e+02\n",
      " -6.220e+02 -6.220e+02 -6.240e+02 -6.260e+02 -6.260e+02 -6.270e+02\n",
      " -6.290e+02 -6.300e+02 -6.310e+02 -6.350e+02 -6.350e+02 -6.390e+02\n",
      " -6.400e+02 -6.400e+02 -6.410e+02 -6.430e+02 -6.430e+02 -6.440e+02\n",
      " -6.460e+02 -6.470e+02 -6.480e+02 -6.510e+02 -6.520e+02 -6.550e+02\n",
      " -6.590e+02 -6.590e+02 -6.600e+02 -6.610e+02 -6.620e+02 -6.640e+02\n",
      " -6.660e+02 -6.660e+02 -6.720e+02 -6.750e+02 -6.750e+02 -6.770e+02\n",
      "  6.780e+02 -6.790e+02 -6.790e+02 -6.800e+02 -6.810e+02 -6.830e+02\n",
      " -6.940e+02 -6.940e+02 -6.980e+02 -7.010e+02 -7.020e+02 -7.030e+02\n",
      " -7.060e+02 -7.070e+02 -7.070e+02 -7.090e+02 -7.100e+02 -7.140e+02\n",
      " -7.150e+02 -7.180e+02 -7.220e+02 -7.240e+02 -7.260e+02 -7.270e+02\n",
      " -7.270e+02 -7.270e+02 -7.340e+02 -7.380e+02 -7.420e+02 -7.470e+02\n",
      " -7.470e+02 -7.480e+02  7.490e+02 -7.540e+02 -7.540e+02 -7.550e+02\n",
      " -7.590e+02 -7.600e+02 -7.600e+02 -7.610e+02 -7.620e+02 -7.620e+02\n",
      "  7.630e+02 -7.670e+02 -7.670e+02 -7.690e+02 -7.770e+02 -7.830e+02\n",
      "  7.850e+02  7.860e+02 -7.880e+02 -7.890e+02 -7.910e+02  7.920e+02\n",
      " -7.920e+02 -7.950e+02 -7.980e+02 -8.030e+02 -8.060e+02  8.110e+02\n",
      " -8.120e+02 -8.200e+02 -8.220e+02  8.250e+02 -8.290e+02 -8.470e+02\n",
      " -8.490e+02 -8.560e+02 -8.580e+02 -8.600e+02 -8.650e+02 -8.650e+02\n",
      " -8.780e+02 -8.780e+02  8.790e+02 -8.810e+02  8.830e+02 -8.830e+02\n",
      " -8.890e+02 -8.900e+02 -8.990e+02  9.040e+02 -9.070e+02 -9.080e+02\n",
      " -9.110e+02 -9.120e+02 -9.150e+02 -9.180e+02  9.210e+02 -9.230e+02\n",
      " -9.310e+02 -9.420e+02  9.430e+02 -9.430e+02  9.590e+02 -9.650e+02\n",
      " -9.650e+02  9.670e+02 -9.680e+02 -9.730e+02 -9.740e+02 -9.750e+02\n",
      "  9.760e+02 -9.840e+02 -9.870e+02 -9.890e+02 -9.900e+02  9.910e+02\n",
      "  9.910e+02 -9.960e+02 -9.970e+02 -1.000e+03 -1.001e+03 -1.001e+03\n",
      "  1.004e+03 -1.004e+03 -1.004e+03 -1.004e+03 -1.005e+03 -1.006e+03\n",
      " -1.007e+03  1.009e+03 -1.009e+03 -1.010e+03 -1.013e+03 -1.025e+03\n",
      " -1.026e+03 -1.026e+03 -1.027e+03  1.034e+03  1.034e+03 -1.034e+03\n",
      " -1.039e+03 -1.042e+03 -1.043e+03 -1.047e+03 -1.051e+03 -1.051e+03\n",
      " -1.059e+03 -1.062e+03 -1.063e+03 -1.066e+03  1.072e+03 -1.074e+03\n",
      " -1.079e+03 -1.085e+03 -1.088e+03 -1.093e+03  1.093e+03 -1.099e+03\n",
      " -1.101e+03 -1.102e+03 -1.106e+03 -1.112e+03 -1.119e+03 -1.120e+03\n",
      " -1.120e+03 -1.121e+03 -1.124e+03 -1.132e+03 -1.133e+03 -1.138e+03\n",
      " -1.140e+03  1.142e+03 -1.148e+03  1.148e+03 -1.148e+03 -1.150e+03\n",
      "  1.152e+03 -1.156e+03 -1.158e+03 -1.161e+03 -1.162e+03 -1.163e+03\n",
      " -1.165e+03 -1.167e+03 -1.174e+03  1.174e+03 -1.185e+03 -1.186e+03\n",
      " -1.189e+03 -1.191e+03 -1.196e+03 -1.198e+03 -1.203e+03 -1.203e+03\n",
      " -1.206e+03 -1.208e+03 -1.208e+03 -1.210e+03 -1.217e+03 -1.219e+03\n",
      " -1.220e+03 -1.224e+03 -1.229e+03 -1.229e+03 -1.232e+03 -1.233e+03\n",
      " -1.234e+03 -1.234e+03 -1.239e+03 -1.246e+03 -1.248e+03 -1.251e+03\n",
      " -1.266e+03 -1.269e+03 -1.270e+03  1.272e+03  1.275e+03 -1.277e+03\n",
      "  1.286e+03 -1.291e+03 -1.301e+03 -1.308e+03 -1.308e+03 -1.308e+03\n",
      " -1.309e+03 -1.321e+03  1.324e+03 -1.325e+03 -1.326e+03 -1.330e+03\n",
      " -1.347e+03 -1.351e+03 -1.359e+03 -1.363e+03 -1.363e+03  1.365e+03\n",
      " -1.369e+03 -1.371e+03 -1.371e+03  1.388e+03  1.411e+03 -1.417e+03\n",
      " -1.417e+03  1.430e+03 -1.434e+03  1.439e+03 -1.449e+03 -1.461e+03\n",
      " -1.463e+03 -1.467e+03 -1.471e+03 -1.474e+03 -1.492e+03 -1.505e+03\n",
      "  1.508e+03 -1.516e+03 -1.522e+03 -1.528e+03 -1.532e+03 -1.534e+03\n",
      " -1.535e+03  1.542e+03 -1.542e+03 -1.545e+03 -1.545e+03 -1.546e+03\n",
      " -1.547e+03 -1.550e+03  1.556e+03 -1.561e+03  1.563e+03 -1.563e+03\n",
      " -1.569e+03 -1.572e+03 -1.587e+03 -1.596e+03 -1.604e+03 -1.604e+03\n",
      " -1.611e+03 -1.611e+03 -1.612e+03 -1.613e+03 -1.614e+03 -1.616e+03\n",
      " -1.631e+03 -1.639e+03 -1.640e+03 -1.642e+03 -1.644e+03 -1.648e+03\n",
      "  1.649e+03 -1.673e+03 -1.673e+03  1.673e+03 -1.683e+03 -1.686e+03\n",
      "  1.688e+03 -1.688e+03  1.692e+03 -1.692e+03  1.694e+03 -1.728e+03\n",
      " -1.732e+03 -1.780e+03  1.781e+03 -1.783e+03  1.793e+03  1.812e+03\n",
      " -1.820e+03 -1.836e+03 -1.842e+03 -1.853e+03 -1.855e+03 -1.864e+03\n",
      " -1.866e+03 -1.871e+03 -1.873e+03 -1.876e+03 -1.882e+03  1.884e+03\n",
      " -1.887e+03 -1.914e+03 -1.919e+03  1.920e+03 -1.925e+03 -1.926e+03\n",
      "  1.927e+03 -1.935e+03 -1.935e+03 -1.953e+03 -1.972e+03 -1.980e+03\n",
      " -1.988e+03  1.993e+03 -1.996e+03  2.009e+03 -2.012e+03 -2.019e+03\n",
      " -2.031e+03 -2.033e+03 -2.041e+03 -2.048e+03 -2.064e+03 -2.082e+03\n",
      " -2.091e+03  2.097e+03 -2.108e+03 -2.109e+03 -2.124e+03  2.127e+03\n",
      " -2.128e+03 -2.140e+03 -2.142e+03 -2.155e+03 -2.156e+03 -2.164e+03\n",
      " -2.184e+03 -2.190e+03 -2.190e+03 -2.191e+03  2.192e+03 -2.193e+03\n",
      " -2.197e+03  2.207e+03 -2.222e+03 -2.231e+03 -2.236e+03 -2.240e+03\n",
      " -2.246e+03 -2.248e+03 -2.255e+03 -2.255e+03 -2.255e+03 -2.263e+03\n",
      "  2.273e+03 -2.281e+03 -2.282e+03 -2.288e+03 -2.306e+03 -2.311e+03\n",
      " -2.329e+03 -2.335e+03  2.361e+03 -2.362e+03 -2.365e+03 -2.371e+03\n",
      " -2.372e+03 -2.372e+03  2.373e+03 -2.381e+03 -2.385e+03 -2.403e+03\n",
      " -2.406e+03  2.417e+03 -2.426e+03 -2.442e+03 -2.442e+03  2.469e+03\n",
      " -2.477e+03  2.483e+03 -2.483e+03 -2.486e+03 -2.489e+03 -2.513e+03\n",
      " -2.515e+03  2.520e+03 -2.541e+03  2.551e+03 -2.559e+03 -2.590e+03\n",
      " -2.596e+03 -2.605e+03 -2.612e+03 -2.629e+03 -2.632e+03  2.636e+03\n",
      " -2.645e+03 -2.645e+03 -2.650e+03 -2.653e+03 -2.707e+03 -2.709e+03\n",
      "  2.712e+03 -2.721e+03 -2.759e+03  2.763e+03 -2.767e+03 -2.770e+03\n",
      "  2.798e+03 -2.813e+03 -2.838e+03  2.854e+03 -2.856e+03  2.866e+03\n",
      " -2.868e+03  2.911e+03 -2.920e+03 -2.953e+03  2.965e+03  2.965e+03\n",
      " -2.971e+03 -2.976e+03 -2.989e+03 -2.991e+03 -3.001e+03 -3.004e+03\n",
      " -3.009e+03 -3.011e+03 -3.017e+03 -3.022e+03 -3.035e+03  3.063e+03\n",
      " -3.088e+03 -3.094e+03 -3.102e+03 -3.121e+03  3.126e+03 -3.128e+03\n",
      " -3.152e+03 -3.159e+03 -3.172e+03 -3.202e+03 -3.203e+03 -3.204e+03\n",
      " -3.226e+03 -3.248e+03 -3.256e+03 -3.261e+03 -3.276e+03 -3.283e+03\n",
      " -3.286e+03 -3.287e+03 -3.296e+03 -3.307e+03 -3.316e+03 -3.342e+03\n",
      " -3.361e+03 -3.364e+03  3.418e+03 -3.430e+03  3.461e+03  3.462e+03\n",
      "  3.492e+03 -3.506e+03 -3.603e+03 -3.607e+03  3.669e+03  3.736e+03\n",
      " -3.807e+03  3.873e+03  3.926e+03  3.941e+03  3.945e+03 -3.957e+03\n",
      "  3.959e+03 -4.005e+03 -4.047e+03 -4.080e+03 -4.159e+03 -4.233e+03\n",
      "  4.267e+03 -4.275e+03 -4.285e+03 -4.354e+03 -4.361e+03 -4.894e+03\n",
      " -4.929e+03 -5.156e+03 -5.176e+03 -5.739e+03 -6.292e+03  6.456e+03\n",
      "  6.593e+03 -7.106e+03 -7.126e+03 -7.777e+03 -8.391e+03 -8.556e+03\n",
      " -8.605e+03]\n",
      "Concordance Index 0.5658072626360855\n",
      "Integrated Brier Score: 0.19798898517850336\n",
      "y_train breslow final [-1.000e+00 -1.000e+00 -5.000e+00 -5.000e+00 -8.000e+00 -9.000e+00\n",
      " -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01\n",
      " -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01 -1.100e+01\n",
      " -1.600e+01 -1.900e+01 -2.100e+01 -2.400e+01 -2.600e+01 -3.000e+01\n",
      " -3.000e+01 -3.100e+01 -3.100e+01 -3.400e+01 -4.000e+01 -5.100e+01\n",
      " -5.200e+01 -5.400e+01 -5.900e+01 -7.200e+01 -7.600e+01 -7.800e+01\n",
      " -7.800e+01 -7.800e+01 -7.900e+01 -8.000e+01 -8.400e+01 -9.800e+01\n",
      "  1.160e+02 -1.180e+02 -1.340e+02 -1.490e+02  1.580e+02 -1.600e+02\n",
      "  1.600e+02 -1.620e+02 -1.630e+02 -1.700e+02 -1.700e+02  1.720e+02\n",
      " -1.720e+02  1.740e+02 -1.780e+02 -1.860e+02 -1.870e+02 -1.960e+02\n",
      "  1.970e+02 -2.000e+02 -2.130e+02 -2.140e+02 -2.160e+02 -2.170e+02\n",
      " -2.180e+02 -2.220e+02 -2.240e+02  2.240e+02 -2.250e+02  2.270e+02\n",
      "  2.390e+02 -2.420e+02 -2.420e+02 -2.430e+02 -2.520e+02  2.550e+02\n",
      " -2.580e+02 -2.590e+02  2.660e+02 -2.710e+02 -2.730e+02 -2.730e+02\n",
      " -2.740e+02 -2.750e+02 -2.850e+02 -2.880e+02 -2.930e+02 -2.930e+02\n",
      " -2.970e+02 -3.020e+02  3.020e+02 -3.030e+02 -3.030e+02 -3.040e+02\n",
      " -3.040e+02  3.040e+02 -3.070e+02 -3.130e+02 -3.130e+02 -3.170e+02\n",
      "  3.200e+02 -3.200e+02 -3.220e+02 -3.220e+02 -3.230e+02 -3.320e+02\n",
      "  3.360e+02 -3.360e+02 -3.370e+02 -3.390e+02 -3.430e+02 -3.430e+02\n",
      " -3.450e+02 -3.470e+02  3.480e+02 -3.480e+02 -3.500e+02 -3.520e+02\n",
      " -3.580e+02 -3.580e+02 -3.600e+02  3.620e+02 -3.630e+02 -3.640e+02\n",
      " -3.650e+02 -3.650e+02 -3.650e+02 -3.650e+02 -3.660e+02 -3.660e+02\n",
      " -3.680e+02 -3.700e+02 -3.710e+02 -3.710e+02 -3.730e+02 -3.750e+02\n",
      " -3.750e+02 -3.750e+02 -3.750e+02 -3.750e+02 -3.760e+02  3.770e+02\n",
      " -3.790e+02 -3.800e+02 -3.810e+02 -3.810e+02 -3.820e+02 -3.830e+02\n",
      " -3.830e+02 -3.850e+02 -3.850e+02 -3.850e+02  3.850e+02 -3.850e+02\n",
      " -3.850e+02 -3.920e+02 -3.930e+02 -3.940e+02 -3.940e+02 -3.960e+02\n",
      " -3.960e+02 -3.960e+02 -3.960e+02 -3.970e+02 -3.980e+02 -4.020e+02\n",
      " -4.040e+02 -4.050e+02 -4.080e+02 -4.080e+02 -4.090e+02 -4.100e+02\n",
      " -4.100e+02 -4.110e+02 -4.130e+02 -4.140e+02 -4.160e+02 -4.170e+02\n",
      " -4.180e+02 -4.230e+02 -4.240e+02 -4.240e+02 -4.260e+02 -4.260e+02\n",
      " -4.280e+02 -4.300e+02 -4.310e+02 -4.330e+02 -4.340e+02 -4.350e+02\n",
      " -4.390e+02 -4.400e+02 -4.410e+02 -4.410e+02 -4.460e+02  4.460e+02\n",
      " -4.470e+02 -4.470e+02 -4.480e+02 -4.500e+02 -4.500e+02 -4.550e+02\n",
      " -4.570e+02 -4.610e+02 -4.610e+02 -4.630e+02 -4.630e+02 -4.660e+02\n",
      " -4.660e+02 -4.700e+02 -4.750e+02 -4.760e+02 -4.770e+02 -4.770e+02\n",
      " -4.800e+02 -4.880e+02 -4.880e+02 -4.920e+02 -4.950e+02 -4.960e+02\n",
      " -4.960e+02 -4.990e+02 -4.990e+02 -5.010e+02 -5.030e+02 -5.040e+02\n",
      " -5.060e+02 -5.080e+02 -5.090e+02 -5.100e+02 -5.110e+02 -5.130e+02\n",
      " -5.160e+02 -5.180e+02 -5.180e+02 -5.180e+02 -5.190e+02 -5.210e+02\n",
      " -5.220e+02  5.240e+02 -5.250e+02 -5.260e+02 -5.280e+02 -5.290e+02\n",
      " -5.300e+02 -5.300e+02 -5.310e+02 -5.320e+02 -5.320e+02 -5.320e+02\n",
      " -5.330e+02  5.380e+02 -5.410e+02 -5.410e+02 -5.450e+02 -5.460e+02\n",
      " -5.470e+02  5.480e+02 -5.510e+02 -5.520e+02 -5.520e+02 -5.520e+02\n",
      " -5.540e+02 -5.540e+02 -5.540e+02 -5.540e+02 -5.540e+02  5.580e+02\n",
      " -5.620e+02 -5.620e+02  5.630e+02 -5.650e+02 -5.660e+02  5.710e+02\n",
      " -5.720e+02 -5.740e+02 -5.740e+02 -5.750e+02 -5.750e+02 -5.750e+02\n",
      " -5.760e+02 -5.770e+02 -5.770e+02 -5.770e+02 -5.790e+02 -5.790e+02\n",
      " -5.840e+02 -5.840e+02  5.840e+02 -5.840e+02 -5.850e+02 -5.860e+02\n",
      " -5.880e+02 -5.880e+02 -5.900e+02 -5.910e+02 -5.950e+02 -5.950e+02\n",
      " -5.960e+02 -5.980e+02 -6.000e+02 -6.020e+02 -6.060e+02 -6.070e+02\n",
      " -6.070e+02 -6.080e+02  6.120e+02 -6.120e+02 -6.120e+02 -6.140e+02\n",
      "  6.140e+02  6.160e+02 -6.160e+02 -6.180e+02 -6.200e+02 -6.200e+02\n",
      " -6.220e+02 -6.220e+02 -6.240e+02 -6.260e+02 -6.260e+02 -6.270e+02\n",
      " -6.290e+02 -6.300e+02 -6.310e+02 -6.350e+02 -6.350e+02 -6.390e+02\n",
      " -6.400e+02 -6.400e+02 -6.410e+02 -6.430e+02 -6.430e+02 -6.440e+02\n",
      " -6.460e+02 -6.470e+02 -6.480e+02 -6.510e+02 -6.520e+02 -6.550e+02\n",
      " -6.590e+02 -6.590e+02 -6.600e+02 -6.610e+02 -6.620e+02 -6.640e+02\n",
      " -6.660e+02 -6.660e+02 -6.720e+02 -6.750e+02 -6.750e+02 -6.770e+02\n",
      "  6.780e+02 -6.790e+02 -6.790e+02 -6.800e+02 -6.810e+02 -6.830e+02\n",
      " -6.940e+02 -6.940e+02 -6.980e+02 -7.010e+02 -7.020e+02 -7.030e+02\n",
      " -7.060e+02 -7.070e+02 -7.070e+02 -7.090e+02 -7.100e+02 -7.140e+02\n",
      " -7.150e+02 -7.180e+02 -7.220e+02 -7.240e+02 -7.260e+02 -7.270e+02\n",
      " -7.270e+02 -7.270e+02 -7.340e+02 -7.380e+02 -7.420e+02 -7.470e+02\n",
      " -7.470e+02 -7.480e+02  7.490e+02 -7.540e+02 -7.540e+02 -7.550e+02\n",
      " -7.590e+02 -7.600e+02 -7.600e+02 -7.610e+02 -7.620e+02 -7.620e+02\n",
      "  7.630e+02 -7.670e+02 -7.670e+02 -7.690e+02 -7.770e+02 -7.830e+02\n",
      "  7.850e+02  7.860e+02 -7.880e+02 -7.890e+02 -7.910e+02  7.920e+02\n",
      " -7.920e+02 -7.950e+02 -7.980e+02 -8.030e+02 -8.060e+02  8.110e+02\n",
      " -8.120e+02 -8.200e+02 -8.220e+02  8.250e+02 -8.290e+02 -8.470e+02\n",
      " -8.490e+02 -8.560e+02 -8.580e+02 -8.600e+02 -8.650e+02 -8.650e+02\n",
      " -8.780e+02 -8.780e+02  8.790e+02 -8.810e+02  8.830e+02 -8.830e+02\n",
      " -8.890e+02 -8.900e+02 -8.990e+02  9.040e+02 -9.070e+02 -9.080e+02\n",
      " -9.110e+02 -9.120e+02 -9.150e+02 -9.180e+02  9.210e+02 -9.230e+02\n",
      " -9.310e+02 -9.420e+02  9.430e+02 -9.430e+02  9.590e+02 -9.650e+02\n",
      " -9.650e+02  9.670e+02 -9.680e+02 -9.730e+02 -9.740e+02 -9.750e+02\n",
      "  9.760e+02 -9.840e+02 -9.870e+02 -9.890e+02 -9.900e+02  9.910e+02\n",
      "  9.910e+02 -9.960e+02 -9.970e+02 -1.000e+03 -1.001e+03 -1.001e+03\n",
      "  1.004e+03 -1.004e+03 -1.004e+03 -1.004e+03 -1.005e+03 -1.006e+03\n",
      " -1.007e+03  1.009e+03 -1.009e+03 -1.010e+03 -1.013e+03 -1.025e+03\n",
      " -1.026e+03 -1.026e+03 -1.027e+03  1.034e+03  1.034e+03 -1.034e+03\n",
      " -1.039e+03 -1.042e+03 -1.043e+03 -1.047e+03 -1.051e+03 -1.051e+03\n",
      " -1.059e+03 -1.062e+03 -1.063e+03 -1.066e+03  1.072e+03 -1.074e+03\n",
      " -1.079e+03 -1.085e+03 -1.088e+03 -1.093e+03  1.093e+03 -1.099e+03\n",
      " -1.101e+03 -1.102e+03 -1.106e+03 -1.112e+03 -1.119e+03 -1.120e+03\n",
      " -1.120e+03 -1.121e+03 -1.124e+03 -1.132e+03 -1.133e+03 -1.138e+03\n",
      " -1.140e+03  1.142e+03 -1.148e+03  1.148e+03 -1.148e+03 -1.150e+03\n",
      "  1.152e+03 -1.156e+03 -1.158e+03 -1.161e+03 -1.162e+03 -1.163e+03\n",
      " -1.165e+03 -1.167e+03 -1.174e+03  1.174e+03 -1.185e+03 -1.186e+03\n",
      " -1.189e+03 -1.191e+03 -1.196e+03 -1.198e+03 -1.203e+03 -1.203e+03\n",
      " -1.206e+03 -1.208e+03 -1.208e+03 -1.210e+03 -1.217e+03 -1.219e+03\n",
      " -1.220e+03 -1.224e+03 -1.229e+03 -1.229e+03 -1.232e+03 -1.233e+03\n",
      " -1.234e+03 -1.234e+03 -1.239e+03 -1.246e+03 -1.248e+03 -1.251e+03\n",
      " -1.266e+03 -1.269e+03 -1.270e+03  1.272e+03  1.275e+03 -1.277e+03\n",
      "  1.286e+03 -1.291e+03 -1.301e+03 -1.308e+03 -1.308e+03 -1.308e+03\n",
      " -1.309e+03 -1.321e+03  1.324e+03 -1.325e+03 -1.326e+03 -1.330e+03\n",
      " -1.347e+03 -1.351e+03 -1.359e+03 -1.363e+03 -1.363e+03  1.365e+03\n",
      " -1.369e+03 -1.371e+03 -1.371e+03  1.388e+03  1.411e+03 -1.417e+03\n",
      " -1.417e+03  1.430e+03 -1.434e+03  1.439e+03 -1.449e+03 -1.461e+03\n",
      " -1.463e+03 -1.467e+03 -1.471e+03 -1.474e+03 -1.492e+03 -1.505e+03\n",
      "  1.508e+03 -1.516e+03 -1.522e+03 -1.528e+03 -1.532e+03 -1.534e+03\n",
      " -1.535e+03  1.542e+03 -1.542e+03 -1.545e+03 -1.545e+03 -1.546e+03\n",
      " -1.547e+03 -1.550e+03  1.556e+03 -1.561e+03  1.563e+03 -1.563e+03\n",
      " -1.569e+03 -1.572e+03 -1.587e+03 -1.596e+03 -1.604e+03 -1.604e+03\n",
      " -1.611e+03 -1.611e+03 -1.612e+03 -1.613e+03 -1.614e+03 -1.616e+03\n",
      " -1.631e+03 -1.639e+03 -1.640e+03 -1.642e+03 -1.644e+03 -1.648e+03\n",
      "  1.649e+03 -1.673e+03 -1.673e+03  1.673e+03 -1.683e+03 -1.686e+03\n",
      "  1.688e+03 -1.688e+03  1.692e+03 -1.692e+03  1.694e+03 -1.728e+03\n",
      " -1.732e+03 -1.780e+03  1.781e+03 -1.783e+03  1.793e+03  1.812e+03\n",
      " -1.820e+03 -1.836e+03 -1.842e+03 -1.853e+03 -1.855e+03 -1.864e+03\n",
      " -1.866e+03 -1.871e+03 -1.873e+03 -1.876e+03 -1.882e+03  1.884e+03\n",
      " -1.887e+03 -1.914e+03 -1.919e+03  1.920e+03 -1.925e+03 -1.926e+03\n",
      "  1.927e+03 -1.935e+03 -1.935e+03 -1.953e+03 -1.972e+03 -1.980e+03\n",
      " -1.988e+03  1.993e+03 -1.996e+03  2.009e+03 -2.012e+03 -2.019e+03\n",
      " -2.031e+03 -2.033e+03 -2.041e+03 -2.048e+03 -2.064e+03 -2.082e+03\n",
      " -2.091e+03  2.097e+03 -2.108e+03 -2.109e+03 -2.124e+03  2.127e+03\n",
      " -2.128e+03 -2.140e+03 -2.142e+03 -2.155e+03 -2.156e+03 -2.164e+03\n",
      " -2.184e+03 -2.190e+03 -2.190e+03 -2.191e+03  2.192e+03 -2.193e+03\n",
      " -2.197e+03  2.207e+03 -2.222e+03 -2.231e+03 -2.236e+03 -2.240e+03\n",
      " -2.246e+03 -2.248e+03 -2.255e+03 -2.255e+03 -2.255e+03 -2.263e+03\n",
      "  2.273e+03 -2.281e+03 -2.282e+03 -2.288e+03 -2.306e+03 -2.311e+03\n",
      " -2.329e+03 -2.335e+03  2.361e+03 -2.362e+03 -2.365e+03 -2.371e+03\n",
      " -2.372e+03 -2.372e+03  2.373e+03 -2.381e+03 -2.385e+03 -2.403e+03\n",
      " -2.406e+03  2.417e+03 -2.426e+03 -2.442e+03 -2.442e+03  2.469e+03\n",
      " -2.477e+03  2.483e+03 -2.483e+03 -2.486e+03 -2.489e+03 -2.513e+03\n",
      " -2.515e+03  2.520e+03 -2.541e+03  2.551e+03 -2.559e+03 -2.590e+03\n",
      " -2.596e+03 -2.605e+03 -2.612e+03 -2.629e+03 -2.632e+03  2.636e+03\n",
      " -2.645e+03 -2.645e+03 -2.650e+03 -2.653e+03 -2.707e+03 -2.709e+03\n",
      "  2.712e+03 -2.721e+03 -2.759e+03  2.763e+03 -2.767e+03 -2.770e+03\n",
      "  2.798e+03 -2.813e+03 -2.838e+03  2.854e+03 -2.856e+03  2.866e+03\n",
      " -2.868e+03  2.911e+03 -2.920e+03 -2.953e+03  2.965e+03  2.965e+03\n",
      " -2.971e+03 -2.976e+03 -2.989e+03 -2.991e+03 -3.001e+03 -3.004e+03\n",
      " -3.009e+03 -3.011e+03 -3.017e+03 -3.022e+03 -3.035e+03  3.063e+03\n",
      " -3.088e+03 -3.094e+03 -3.102e+03 -3.121e+03  3.126e+03 -3.128e+03\n",
      " -3.152e+03 -3.159e+03 -3.172e+03 -3.202e+03 -3.203e+03 -3.204e+03\n",
      " -3.226e+03 -3.248e+03 -3.256e+03 -3.261e+03 -3.276e+03 -3.283e+03\n",
      " -3.286e+03 -3.287e+03 -3.296e+03 -3.307e+03 -3.316e+03 -3.342e+03\n",
      " -3.361e+03 -3.364e+03  3.418e+03 -3.430e+03  3.461e+03  3.462e+03\n",
      "  3.492e+03 -3.506e+03 -3.603e+03 -3.607e+03  3.669e+03  3.736e+03\n",
      " -3.807e+03  3.873e+03  3.926e+03  3.941e+03  3.945e+03 -3.957e+03\n",
      "  3.959e+03 -4.005e+03 -4.047e+03 -4.080e+03 -4.159e+03 -4.233e+03\n",
      "  4.267e+03 -4.275e+03 -4.285e+03 -4.354e+03 -4.361e+03 -4.894e+03\n",
      " -4.929e+03 -5.156e+03 -5.176e+03 -5.739e+03 -6.292e+03  6.456e+03\n",
      "  6.593e+03 -7.106e+03 -7.126e+03 -7.777e+03 -8.391e+03 -8.556e+03\n",
      " -8.605e+03]\n",
      "durations 5.0 8008.0\n",
      "Concordance Index 0.5140644109253975\n",
      "Integrated Brier Score: 0.20394792409957263\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m77.7088\u001b[0m       \u001b[32m80.9302\u001b[0m  0.0798\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m178.6639\u001b[0m       \u001b[32m99.7606\u001b[0m  0.0388\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m165.8774\u001b[0m       \u001b[32m82.5237\u001b[0m  0.0385\n",
      "      2       \u001b[36m72.6548\u001b[0m       \u001b[32m59.4217\u001b[0m  0.0676\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m197.0850\u001b[0m      \u001b[32m112.0678\u001b[0m  0.0370\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m88.4737\u001b[0m      \u001b[32m121.1075\u001b[0m  0.0604\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m73.0249\u001b[0m       \u001b[32m81.4008\u001b[0m  0.0568\n",
      "      2      \u001b[36m158.1637\u001b[0m       \u001b[32m76.1021\u001b[0m  0.0340\n",
      "      2      \u001b[36m170.7580\u001b[0m      100.5347  0.0354\n",
      "      2      \u001b[36m186.8407\u001b[0m      \u001b[32m107.6522\u001b[0m  0.0368\n",
      "      3       \u001b[36m69.9213\u001b[0m       \u001b[32m58.3148\u001b[0m  0.0515\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      3      \u001b[36m152.9066\u001b[0m       \u001b[32m73.5108\u001b[0m  0.0335\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m59.6790\u001b[0m       \u001b[32m67.8409\u001b[0m  0.0745\n",
      "      3      \u001b[36m167.4543\u001b[0m       \u001b[32m99.4253\u001b[0m  0.0390\n",
      "      2       \u001b[36m83.3473\u001b[0m       \u001b[32m89.1032\u001b[0m  0.0623\n",
      "      3      \u001b[36m180.7175\u001b[0m      \u001b[32m106.6761\u001b[0m  0.0303\n",
      "      2       \u001b[36m70.7274\u001b[0m       \u001b[32m57.5298\u001b[0m  0.0603\n",
      "      4      \u001b[36m152.6905\u001b[0m       \u001b[32m73.0355\u001b[0m  0.0282\n",
      "      4      \u001b[36m164.0857\u001b[0m       99.9955  0.0380\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m143.8033\u001b[0m       \u001b[32m73.0523\u001b[0m  0.0390\n",
      "      4      \u001b[36m179.9778\u001b[0m      107.1365  0.0372\n",
      "      4       \u001b[36m69.7085\u001b[0m       62.2940  0.0644\n",
      "      3       \u001b[36m80.0715\u001b[0m       \u001b[32m89.0246\u001b[0m  0.0447\n",
      "      2       \u001b[36m59.1579\u001b[0m       \u001b[32m62.9863\u001b[0m  0.0609\n",
      "      5      \u001b[36m151.3020\u001b[0m       73.5573  0.0393\n",
      "      2      \u001b[36m136.8071\u001b[0m       73.6668  0.0316\n",
      "      3       71.6966       60.4516  0.0540\n",
      "      5      \u001b[36m162.5877\u001b[0m      101.6007  0.0366\n",
      "      5      \u001b[36m178.0132\u001b[0m      108.4147  0.0419\n",
      "      6      \u001b[36m150.8553\u001b[0m       73.8828  0.0320\n",
      "      4       \u001b[36m79.2475\u001b[0m       92.8446  0.0524\n",
      "      5       \u001b[36m68.4058\u001b[0m       61.3987  0.0593\n",
      "      3      \u001b[36m133.0676\u001b[0m       73.6084  0.0421\n",
      "      6      \u001b[36m162.2136\u001b[0m      102.0918  0.0388\n",
      "      3       \u001b[36m55.0774\u001b[0m       \u001b[32m62.7100\u001b[0m  0.0550\n",
      "      6      179.3282      109.4895  0.0340\n",
      "      4       \u001b[36m68.9672\u001b[0m       62.9172  0.0562\n",
      "      7      \u001b[36m149.0763\u001b[0m       74.0017  0.0327\n",
      "      4      \u001b[36m129.2568\u001b[0m       73.8201  0.0322\n",
      "      7      \u001b[36m161.8524\u001b[0m      101.8535  0.0351\n",
      "      6       \u001b[36m66.1708\u001b[0m       61.8600  0.0465\n",
      "      5       \u001b[36m77.8690\u001b[0m       93.3541  0.0529\n",
      "      7      \u001b[36m176.7136\u001b[0m      109.2828  0.0366\n",
      "      8      151.8931       74.6189  0.0321\n",
      "      4       55.9341       \u001b[32m62.2486\u001b[0m  0.0494\n",
      "      5      \u001b[36m128.0438\u001b[0m       74.4055  0.0332\n",
      "      8      \u001b[36m160.7626\u001b[0m      101.9052  0.0357\n",
      "      8      \u001b[36m176.2027\u001b[0m      108.9843  0.0340\n",
      "      5       \u001b[36m67.1991\u001b[0m       59.7207  0.0670\n",
      "      9      \u001b[36m147.0641\u001b[0m       75.2065  0.0350\n",
      "      7       67.0232       63.2560  0.0563\n",
      "      6      \u001b[36m126.5785\u001b[0m       74.9845  0.0323\n",
      "      6       \u001b[36m76.3320\u001b[0m       92.4138  0.0611\n",
      "      9      \u001b[36m156.1440\u001b[0m      102.4687  0.0357\n",
      "      5       56.5040       \u001b[32m59.5229\u001b[0m  0.0626\n",
      "     10      148.7509       76.0068  0.0383\n",
      "Restoring best model from epoch 4.\n",
      "      7      \u001b[36m126.0912\u001b[0m       74.5788  0.0308\n",
      "      9      \u001b[36m173.7271\u001b[0m      109.2214  0.0475\n",
      "      6       \u001b[36m64.3300\u001b[0m       61.0551  0.0562\n",
      "     10      159.2342      103.3649  0.0349\n",
      "Restoring best model from epoch 3.\n",
      "      8       \u001b[36m65.2536\u001b[0m       59.7354  0.0560\n",
      "     10      176.3874      109.6166  0.0293\n",
      "Restoring best model from epoch 3.\n",
      "      7       77.0768       92.0671  0.0538\n",
      "      8      \u001b[36m125.3433\u001b[0m       74.4884  0.0317\n",
      "      6       \u001b[36m52.9590\u001b[0m       \u001b[32m57.7950\u001b[0m  0.0547\n",
      "      9      126.6147       74.5485  0.0276\n",
      "      7       64.7315       61.3313  0.0542\n",
      "      9       67.0520       59.1986  0.0471\n",
      "     10      \u001b[36m124.9203\u001b[0m       75.1318  0.0264\n",
      "Restoring best model from epoch 1.\n",
      "      7       53.7003       58.3382  0.0471\n",
      "      8       \u001b[36m75.5344\u001b[0m       90.7016  0.0637\n",
      "     10       65.3160       59.7630  0.0440\n",
      "      8       \u001b[36m63.3600\u001b[0m       60.8428  0.0522\n",
      "Restoring best model from epoch 3.\n",
      "      8       \u001b[36m51.8673\u001b[0m       59.9283  0.0482\n",
      "      9       \u001b[36m74.7935\u001b[0m       90.9975  0.0531\n",
      "      9       \u001b[36m62.8220\u001b[0m       59.2391  0.0554\n",
      "      9       \u001b[36m50.8012\u001b[0m       62.2529  0.0413\n",
      "     10       \u001b[36m72.6406\u001b[0m       91.7895  0.0453\n",
      "Restoring best model from epoch 3.\n",
      "     10       \u001b[36m61.7438\u001b[0m       59.4608  0.0431\n",
      "Restoring best model from epoch 2.\n",
      "     10       53.3359       60.5556  0.0445\n",
      "Restoring best model from epoch 6.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m173.4735\u001b[0m       \u001b[32m72.1027\u001b[0m  0.0317\n",
      "      2      \u001b[36m164.6952\u001b[0m       \u001b[32m70.9500\u001b[0m  0.0282\n",
      "      3      \u001b[36m159.5668\u001b[0m       73.4220  0.0280\n",
      "      4      \u001b[36m158.6242\u001b[0m       74.8179  0.0271\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      5      \u001b[36m154.5895\u001b[0m       75.1273  0.0259\n",
      "      6      157.6579       74.6397  0.0268\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m81.1869\u001b[0m       \u001b[32m99.0482\u001b[0m  0.0454\n",
      "      7      154.7432       73.3819  0.0271\n",
      "      2       \u001b[36m71.9950\u001b[0m       \u001b[32m90.8804\u001b[0m  0.0377\n",
      "      8      158.0337       73.5164  0.0265\n",
      "      9      \u001b[36m152.5069\u001b[0m       73.7067  0.0263\n",
      "      3       \u001b[36m69.0084\u001b[0m       91.7381  0.0386\n",
      "     10      \u001b[36m152.4570\u001b[0m       74.3969  0.0253\n",
      "Restoring best model from epoch 2.\n",
      "      4       69.9140       94.3057  0.0377\n",
      "      5       71.2868       \u001b[32m89.4275\u001b[0m  0.0376\n",
      "      6       \u001b[36m67.1792\u001b[0m       \u001b[32m86.2747\u001b[0m  0.0382\n",
      "      7       69.4328       86.9136  0.0379\n",
      "      8       \u001b[36m66.8696\u001b[0m       \u001b[32m86.1730\u001b[0m  0.0370\n",
      "      9       68.3048       \u001b[32m85.0417\u001b[0m  0.0369\n",
      "     10       66.9958       86.3872  0.0368\n",
      "Restoring best model from epoch 9.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m161.7629\u001b[0m      \u001b[32m113.3477\u001b[0m  0.0297\n",
      "      2      \u001b[36m152.3196\u001b[0m      \u001b[32m112.6906\u001b[0m  0.0292\n",
      "      3      153.1300      113.6892  0.0293\n",
      "      4      \u001b[36m147.1254\u001b[0m      115.0590  0.0303\n",
      "      5      \u001b[36m144.8454\u001b[0m      116.0886  0.0280\n",
      "      6      146.1888      116.0292  0.0283\n",
      "      7      145.1855      115.4895  0.0280\n",
      "      8      146.4310      114.9683  0.0305\n",
      "      9      \u001b[36m142.9887\u001b[0m      114.7336  0.0312\n",
      "     10      143.2015      114.8688  0.0314\n",
      "Restoring best model from epoch 2.\n",
      "y_train breslow final [-1.000e+00 -1.000e+00 -5.000e+00 -5.000e+00 -7.000e+00 -8.000e+00\n",
      " -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01\n",
      " -1.000e+01 -1.000e+01 -1.000e+01 -1.100e+01 -1.600e+01 -2.100e+01\n",
      " -2.400e+01 -2.600e+01 -3.000e+01 -3.000e+01 -3.000e+01 -3.100e+01\n",
      " -3.100e+01 -3.100e+01 -3.400e+01 -3.400e+01 -4.000e+01 -4.900e+01\n",
      " -5.100e+01 -5.200e+01 -5.400e+01 -5.900e+01 -6.400e+01 -7.000e+01\n",
      " -7.200e+01 -7.800e+01 -7.800e+01 -7.900e+01 -8.000e+01 -8.400e+01\n",
      " -9.000e+01 -9.200e+01 -9.800e+01  1.160e+02 -1.180e+02 -1.340e+02\n",
      "  1.580e+02  1.600e+02 -1.620e+02 -1.630e+02 -1.700e+02 -1.700e+02\n",
      "  1.720e+02 -1.720e+02  1.740e+02 -1.860e+02 -1.870e+02 -1.960e+02\n",
      "  1.970e+02 -2.000e+02 -2.100e+02 -2.130e+02 -2.140e+02 -2.150e+02\n",
      " -2.160e+02 -2.170e+02 -2.180e+02 -2.240e+02 -2.250e+02  2.270e+02\n",
      " -2.270e+02 -2.310e+02  2.390e+02 -2.420e+02 -2.420e+02 -2.430e+02\n",
      " -2.500e+02 -2.520e+02 -2.580e+02 -2.590e+02 -2.660e+02  2.660e+02\n",
      " -2.730e+02 -2.730e+02 -2.740e+02 -2.870e+02 -2.880e+02 -2.930e+02\n",
      " -2.930e+02  2.950e+02 -2.970e+02 -3.000e+02 -3.020e+02 -3.030e+02\n",
      " -3.030e+02 -3.040e+02 -3.040e+02  3.040e+02 -3.070e+02 -3.130e+02\n",
      " -3.130e+02 -3.170e+02  3.200e+02 -3.200e+02  3.220e+02 -3.220e+02\n",
      " -3.220e+02 -3.230e+02 -3.260e+02 -3.280e+02 -3.340e+02  3.360e+02\n",
      " -3.360e+02 -3.370e+02 -3.390e+02 -3.430e+02 -3.430e+02 -3.450e+02\n",
      " -3.450e+02 -3.470e+02  3.480e+02 -3.500e+02 -3.520e+02 -3.580e+02\n",
      " -3.580e+02 -3.600e+02  3.620e+02 -3.630e+02 -3.650e+02 -3.650e+02\n",
      "  3.650e+02 -3.650e+02 -3.650e+02 -3.660e+02 -3.660e+02 -3.680e+02\n",
      " -3.700e+02 -3.710e+02 -3.730e+02 -3.750e+02 -3.750e+02 -3.750e+02\n",
      " -3.760e+02 -3.760e+02  3.770e+02 -3.790e+02 -3.800e+02 -3.800e+02\n",
      " -3.810e+02 -3.810e+02 -3.820e+02  3.850e+02 -3.850e+02 -3.850e+02\n",
      " -3.850e+02  3.850e+02 -3.850e+02 -3.850e+02 -3.920e+02 -3.930e+02\n",
      " -3.940e+02 -3.940e+02 -3.950e+02 -3.960e+02 -3.960e+02 -3.960e+02\n",
      " -4.020e+02 -4.030e+02 -4.030e+02 -4.040e+02 -4.050e+02 -4.080e+02\n",
      " -4.090e+02 -4.100e+02 -4.100e+02 -4.120e+02 -4.130e+02 -4.160e+02\n",
      " -4.170e+02 -4.180e+02 -4.210e+02 -4.230e+02 -4.240e+02 -4.240e+02\n",
      " -4.250e+02 -4.260e+02  4.260e+02 -4.260e+02 -4.280e+02 -4.300e+02\n",
      " -4.310e+02 -4.310e+02 -4.330e+02 -4.340e+02 -4.350e+02 -4.370e+02\n",
      " -4.390e+02 -4.390e+02 -4.390e+02 -4.400e+02 -4.410e+02 -4.410e+02\n",
      "  4.460e+02 -4.470e+02 -4.470e+02 -4.470e+02 -4.480e+02 -4.480e+02\n",
      " -4.500e+02 -4.510e+02 -4.540e+02 -4.560e+02 -4.570e+02 -4.580e+02\n",
      " -4.610e+02 -4.630e+02 -4.630e+02 -4.660e+02 -4.660e+02 -4.670e+02\n",
      " -4.710e+02 -4.720e+02 -4.750e+02 -4.760e+02 -4.770e+02 -4.770e+02\n",
      " -4.770e+02 -4.820e+02 -4.880e+02 -4.890e+02 -4.960e+02 -5.010e+02\n",
      " -5.010e+02 -5.020e+02 -5.030e+02 -5.040e+02 -5.040e+02 -5.110e+02\n",
      " -5.130e+02 -5.160e+02 -5.180e+02 -5.190e+02 -5.190e+02 -5.210e+02\n",
      " -5.220e+02 -5.230e+02 -5.250e+02 -5.280e+02 -5.290e+02 -5.300e+02\n",
      " -5.300e+02 -5.310e+02 -5.320e+02 -5.320e+02 -5.320e+02 -5.330e+02\n",
      " -5.330e+02 -5.380e+02 -5.380e+02 -5.410e+02 -5.410e+02 -5.450e+02\n",
      " -5.460e+02 -5.470e+02  5.480e+02 -5.490e+02 -5.520e+02 -5.520e+02\n",
      " -5.540e+02 -5.540e+02 -5.540e+02 -5.540e+02 -5.540e+02  5.580e+02\n",
      " -5.620e+02 -5.620e+02 -5.630e+02 -5.660e+02 -5.670e+02 -5.680e+02\n",
      " -5.720e+02  5.730e+02 -5.740e+02 -5.740e+02 -5.750e+02 -5.750e+02\n",
      " -5.750e+02 -5.760e+02 -5.770e+02 -5.770e+02 -5.770e+02 -5.790e+02\n",
      " -5.790e+02 -5.840e+02  5.840e+02 -5.860e+02 -5.880e+02 -5.880e+02\n",
      " -5.900e+02 -5.910e+02 -5.940e+02 -5.950e+02 -5.950e+02 -5.980e+02\n",
      " -6.000e+02 -6.020e+02 -6.070e+02 -6.070e+02 -6.080e+02 -6.080e+02\n",
      " -6.110e+02 -6.110e+02 -6.120e+02 -6.120e+02 -6.140e+02 -6.160e+02\n",
      " -6.160e+02 -6.180e+02 -6.200e+02 -6.200e+02 -6.220e+02 -6.260e+02\n",
      " -6.260e+02 -6.270e+02 -6.290e+02 -6.350e+02 -6.350e+02 -6.350e+02\n",
      " -6.390e+02  6.390e+02 -6.400e+02 -6.410e+02 -6.460e+02 -6.470e+02\n",
      " -6.480e+02 -6.510e+02 -6.550e+02 -6.580e+02 -6.590e+02 -6.590e+02\n",
      " -6.600e+02 -6.610e+02 -6.640e+02 -6.660e+02 -6.660e+02 -6.750e+02\n",
      " -6.770e+02 -6.770e+02  6.780e+02 -6.790e+02 -6.790e+02 -6.800e+02\n",
      " -6.810e+02 -6.830e+02 -6.940e+02 -6.980e+02 -7.010e+02 -7.020e+02\n",
      " -7.030e+02 -7.060e+02 -7.070e+02 -7.070e+02 -7.090e+02 -7.140e+02\n",
      " -7.150e+02 -7.150e+02 -7.180e+02 -7.220e+02  7.230e+02 -7.250e+02\n",
      " -7.260e+02 -7.270e+02 -7.280e+02 -7.340e+02 -7.380e+02 -7.450e+02\n",
      " -7.470e+02 -7.470e+02 -7.480e+02  7.490e+02 -7.520e+02 -7.520e+02\n",
      "  7.540e+02 -7.540e+02 -7.540e+02 -7.550e+02 -7.590e+02 -7.590e+02\n",
      " -7.600e+02 -7.620e+02 -7.620e+02  7.630e+02 -7.650e+02 -7.670e+02\n",
      " -7.770e+02 -7.830e+02 -7.850e+02  7.850e+02  7.860e+02 -7.880e+02\n",
      " -7.890e+02  7.920e+02  7.920e+02 -7.920e+02 -7.950e+02 -7.980e+02\n",
      " -8.060e+02 -8.120e+02 -8.190e+02 -8.200e+02  8.210e+02  8.250e+02\n",
      " -8.290e+02 -8.370e+02 -8.390e+02 -8.470e+02 -8.490e+02 -8.520e+02\n",
      " -8.560e+02 -8.580e+02 -8.600e+02  8.600e+02 -8.650e+02 -8.650e+02\n",
      " -8.670e+02 -8.750e+02 -8.780e+02 -8.780e+02  8.790e+02 -8.810e+02\n",
      "  8.830e+02 -8.830e+02 -8.990e+02  9.040e+02 -9.060e+02 -9.070e+02\n",
      " -9.080e+02  9.120e+02 -9.120e+02 -9.120e+02 -9.150e+02 -9.180e+02\n",
      "  9.210e+02 -9.260e+02 -9.310e+02 -9.310e+02 -9.420e+02 -9.430e+02\n",
      "  9.430e+02 -9.430e+02 -9.430e+02 -9.520e+02 -9.540e+02  9.590e+02\n",
      " -9.650e+02 -9.650e+02  9.670e+02 -9.680e+02 -9.720e+02 -9.740e+02\n",
      " -9.740e+02 -9.750e+02 -9.750e+02 -9.840e+02 -9.870e+02 -9.890e+02\n",
      " -9.900e+02  9.910e+02  9.910e+02 -9.960e+02 -9.960e+02 -9.970e+02\n",
      " -1.001e+03 -1.001e+03 -1.004e+03 -1.004e+03 -1.004e+03 -1.005e+03\n",
      "  1.009e+03 -1.009e+03 -1.010e+03 -1.013e+03 -1.015e+03 -1.025e+03\n",
      " -1.026e+03 -1.026e+03 -1.027e+03  1.032e+03  1.034e+03 -1.034e+03\n",
      " -1.035e+03 -1.039e+03 -1.042e+03 -1.043e+03 -1.043e+03 -1.043e+03\n",
      " -1.047e+03  1.048e+03 -1.051e+03 -1.051e+03 -1.051e+03 -1.059e+03\n",
      " -1.062e+03 -1.062e+03 -1.063e+03 -1.066e+03  1.072e+03 -1.074e+03\n",
      " -1.079e+03 -1.085e+03 -1.093e+03 -1.099e+03 -1.101e+03 -1.102e+03\n",
      "  1.104e+03 -1.106e+03 -1.112e+03 -1.119e+03 -1.120e+03 -1.121e+03\n",
      " -1.124e+03  1.127e+03 -1.133e+03 -1.139e+03 -1.140e+03 -1.141e+03\n",
      "  1.142e+03 -1.148e+03  1.148e+03 -1.148e+03  1.152e+03 -1.156e+03\n",
      " -1.156e+03 -1.157e+03 -1.158e+03 -1.161e+03 -1.165e+03 -1.167e+03\n",
      " -1.174e+03  1.174e+03 -1.179e+03 -1.185e+03 -1.186e+03 -1.191e+03\n",
      " -1.196e+03 -1.198e+03 -1.203e+03 -1.203e+03 -1.206e+03 -1.208e+03\n",
      " -1.208e+03 -1.210e+03 -1.217e+03 -1.219e+03 -1.220e+03 -1.220e+03\n",
      " -1.224e+03 -1.232e+03 -1.233e+03 -1.234e+03 -1.234e+03 -1.246e+03\n",
      " -1.247e+03 -1.248e+03 -1.251e+03 -1.266e+03 -1.269e+03 -1.270e+03\n",
      "  1.272e+03 -1.285e+03 -1.288e+03 -1.291e+03 -1.301e+03 -1.308e+03\n",
      " -1.308e+03 -1.308e+03 -1.309e+03 -1.309e+03 -1.318e+03 -1.325e+03\n",
      " -1.326e+03 -1.330e+03 -1.347e+03 -1.359e+03 -1.363e+03 -1.369e+03\n",
      " -1.371e+03 -1.371e+03  1.388e+03  1.411e+03 -1.417e+03 -1.417e+03\n",
      " -1.419e+03  1.430e+03 -1.437e+03  1.439e+03 -1.448e+03 -1.449e+03\n",
      " -1.461e+03 -1.463e+03 -1.467e+03 -1.471e+03 -1.474e+03 -1.474e+03\n",
      " -1.476e+03 -1.477e+03 -1.505e+03 -1.516e+03 -1.519e+03 -1.523e+03\n",
      " -1.532e+03 -1.534e+03 -1.535e+03  1.542e+03 -1.542e+03 -1.545e+03\n",
      " -1.546e+03 -1.547e+03 -1.548e+03 -1.550e+03  1.556e+03 -1.561e+03\n",
      "  1.563e+03 -1.563e+03 -1.569e+03 -1.572e+03 -1.587e+03 -1.596e+03\n",
      " -1.604e+03 -1.611e+03 -1.611e+03 -1.614e+03 -1.616e+03 -1.620e+03\n",
      " -1.631e+03 -1.639e+03 -1.640e+03  1.642e+03 -1.644e+03 -1.648e+03\n",
      "  1.649e+03 -1.662e+03 -1.673e+03 -1.673e+03  1.673e+03 -1.682e+03\n",
      " -1.683e+03  1.688e+03 -1.688e+03 -1.692e+03  1.694e+03  1.699e+03\n",
      " -1.712e+03 -1.728e+03 -1.728e+03 -1.732e+03 -1.742e+03  1.759e+03\n",
      "  1.793e+03 -1.800e+03 -1.820e+03 -1.836e+03 -1.847e+03 -1.855e+03\n",
      " -1.864e+03 -1.866e+03 -1.871e+03 -1.873e+03 -1.876e+03 -1.882e+03\n",
      "  1.884e+03 -1.887e+03  1.900e+03 -1.919e+03  1.920e+03 -1.925e+03\n",
      " -1.926e+03  1.927e+03 -1.928e+03 -1.935e+03 -1.935e+03 -1.972e+03\n",
      " -1.988e+03  1.993e+03 -1.996e+03 -2.012e+03 -2.019e+03 -2.031e+03\n",
      " -2.033e+03 -2.041e+03 -2.048e+03 -2.064e+03 -2.072e+03 -2.082e+03\n",
      " -2.091e+03  2.097e+03 -2.108e+03 -2.128e+03 -2.136e+03 -2.155e+03\n",
      " -2.156e+03 -2.161e+03 -2.164e+03 -2.184e+03 -2.190e+03 -2.190e+03\n",
      " -2.191e+03  2.192e+03 -2.193e+03  2.207e+03 -2.222e+03 -2.231e+03\n",
      " -2.236e+03 -2.240e+03 -2.246e+03 -2.255e+03 -2.255e+03 -2.263e+03\n",
      "  2.273e+03 -2.278e+03 -2.281e+03 -2.282e+03 -2.288e+03 -2.289e+03\n",
      "  2.296e+03 -2.311e+03 -2.330e+03  2.348e+03  2.361e+03 -2.362e+03\n",
      " -2.371e+03 -2.372e+03  2.373e+03 -2.381e+03 -2.403e+03 -2.406e+03\n",
      "  2.417e+03 -2.426e+03 -2.442e+03  2.469e+03 -2.477e+03  2.483e+03\n",
      " -2.483e+03 -2.486e+03 -2.489e+03  2.520e+03  2.534e+03 -2.535e+03\n",
      " -2.541e+03  2.551e+03 -2.558e+03 -2.559e+03  2.573e+03 -2.590e+03\n",
      " -2.596e+03 -2.596e+03 -2.605e+03 -2.612e+03 -2.618e+03 -2.629e+03\n",
      " -2.632e+03 -2.632e+03  2.636e+03 -2.650e+03 -2.653e+03 -2.654e+03\n",
      " -2.695e+03 -2.707e+03  2.712e+03 -2.721e+03  2.763e+03 -2.767e+03\n",
      "  2.798e+03 -2.813e+03 -2.838e+03  2.854e+03 -2.856e+03  2.866e+03\n",
      " -2.868e+03 -2.953e+03  2.965e+03  2.965e+03 -2.971e+03 -2.976e+03\n",
      " -2.989e+03 -2.991e+03 -2.991e+03 -3.001e+03 -3.015e+03 -3.017e+03\n",
      " -3.021e+03 -3.022e+03 -3.030e+03 -3.035e+03 -3.072e+03 -3.088e+03\n",
      " -3.091e+03 -3.094e+03 -3.112e+03 -3.121e+03  3.126e+03 -3.128e+03\n",
      " -3.152e+03 -3.159e+03 -3.172e+03 -3.203e+03 -3.226e+03 -3.247e+03\n",
      " -3.248e+03 -3.256e+03 -3.261e+03  3.262e+03 -3.276e+03 -3.283e+03\n",
      " -3.283e+03 -3.286e+03 -3.296e+03 -3.316e+03 -3.361e+03 -3.364e+03\n",
      "  3.409e+03  3.418e+03 -3.430e+03 -3.456e+03  3.461e+03  3.462e+03\n",
      "  3.472e+03  3.492e+03 -3.506e+03 -3.519e+03 -3.607e+03 -3.660e+03\n",
      "  3.669e+03 -3.709e+03  3.926e+03  3.941e+03  3.945e+03 -3.957e+03\n",
      " -4.052e+03 -4.080e+03 -4.088e+03 -4.159e+03 -4.233e+03  4.267e+03\n",
      " -4.275e+03 -4.285e+03 -4.354e+03  4.456e+03 -4.894e+03 -5.042e+03\n",
      " -5.062e+03 -5.176e+03 -5.739e+03 -6.292e+03  6.456e+03  6.593e+03\n",
      " -7.106e+03 -7.126e+03  7.455e+03 -7.777e+03 -8.008e+03 -8.391e+03\n",
      " -8.605e+03]\n",
      "Concordance Index 0.651966135409844\n",
      "Integrated Brier Score: 0.21040946171849834\n",
      "y_train breslow final [-1.000e+00 -1.000e+00 -5.000e+00 -5.000e+00 -7.000e+00 -8.000e+00\n",
      " -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01\n",
      " -1.000e+01 -1.000e+01 -1.000e+01 -1.100e+01 -1.600e+01 -2.100e+01\n",
      " -2.400e+01 -2.600e+01 -3.000e+01 -3.000e+01 -3.000e+01 -3.100e+01\n",
      " -3.100e+01 -3.100e+01 -3.400e+01 -3.400e+01 -4.000e+01 -4.900e+01\n",
      " -5.100e+01 -5.200e+01 -5.400e+01 -5.900e+01 -6.400e+01 -7.000e+01\n",
      " -7.200e+01 -7.800e+01 -7.800e+01 -7.900e+01 -8.000e+01 -8.400e+01\n",
      " -9.000e+01 -9.200e+01 -9.800e+01  1.160e+02 -1.180e+02 -1.340e+02\n",
      "  1.580e+02  1.600e+02 -1.620e+02 -1.630e+02 -1.700e+02 -1.700e+02\n",
      "  1.720e+02 -1.720e+02  1.740e+02 -1.860e+02 -1.870e+02 -1.960e+02\n",
      "  1.970e+02 -2.000e+02 -2.100e+02 -2.130e+02 -2.140e+02 -2.150e+02\n",
      " -2.160e+02 -2.170e+02 -2.180e+02 -2.240e+02 -2.250e+02  2.270e+02\n",
      " -2.270e+02 -2.310e+02  2.390e+02 -2.420e+02 -2.420e+02 -2.430e+02\n",
      " -2.500e+02 -2.520e+02 -2.580e+02 -2.590e+02 -2.660e+02  2.660e+02\n",
      " -2.730e+02 -2.730e+02 -2.740e+02 -2.870e+02 -2.880e+02 -2.930e+02\n",
      " -2.930e+02  2.950e+02 -2.970e+02 -3.000e+02 -3.020e+02 -3.030e+02\n",
      " -3.030e+02 -3.040e+02 -3.040e+02  3.040e+02 -3.070e+02 -3.130e+02\n",
      " -3.130e+02 -3.170e+02  3.200e+02 -3.200e+02  3.220e+02 -3.220e+02\n",
      " -3.220e+02 -3.230e+02 -3.260e+02 -3.280e+02 -3.340e+02  3.360e+02\n",
      " -3.360e+02 -3.370e+02 -3.390e+02 -3.430e+02 -3.430e+02 -3.450e+02\n",
      " -3.450e+02 -3.470e+02  3.480e+02 -3.500e+02 -3.520e+02 -3.580e+02\n",
      " -3.580e+02 -3.600e+02  3.620e+02 -3.630e+02 -3.650e+02 -3.650e+02\n",
      "  3.650e+02 -3.650e+02 -3.650e+02 -3.660e+02 -3.660e+02 -3.680e+02\n",
      " -3.700e+02 -3.710e+02 -3.730e+02 -3.750e+02 -3.750e+02 -3.750e+02\n",
      " -3.760e+02 -3.760e+02  3.770e+02 -3.790e+02 -3.800e+02 -3.800e+02\n",
      " -3.810e+02 -3.810e+02 -3.820e+02  3.850e+02 -3.850e+02 -3.850e+02\n",
      " -3.850e+02  3.850e+02 -3.850e+02 -3.850e+02 -3.920e+02 -3.930e+02\n",
      " -3.940e+02 -3.940e+02 -3.950e+02 -3.960e+02 -3.960e+02 -3.960e+02\n",
      " -4.020e+02 -4.030e+02 -4.030e+02 -4.040e+02 -4.050e+02 -4.080e+02\n",
      " -4.090e+02 -4.100e+02 -4.100e+02 -4.120e+02 -4.130e+02 -4.160e+02\n",
      " -4.170e+02 -4.180e+02 -4.210e+02 -4.230e+02 -4.240e+02 -4.240e+02\n",
      " -4.250e+02 -4.260e+02  4.260e+02 -4.260e+02 -4.280e+02 -4.300e+02\n",
      " -4.310e+02 -4.310e+02 -4.330e+02 -4.340e+02 -4.350e+02 -4.370e+02\n",
      " -4.390e+02 -4.390e+02 -4.390e+02 -4.400e+02 -4.410e+02 -4.410e+02\n",
      "  4.460e+02 -4.470e+02 -4.470e+02 -4.470e+02 -4.480e+02 -4.480e+02\n",
      " -4.500e+02 -4.510e+02 -4.540e+02 -4.560e+02 -4.570e+02 -4.580e+02\n",
      " -4.610e+02 -4.630e+02 -4.630e+02 -4.660e+02 -4.660e+02 -4.670e+02\n",
      " -4.710e+02 -4.720e+02 -4.750e+02 -4.760e+02 -4.770e+02 -4.770e+02\n",
      " -4.770e+02 -4.820e+02 -4.880e+02 -4.890e+02 -4.960e+02 -5.010e+02\n",
      " -5.010e+02 -5.020e+02 -5.030e+02 -5.040e+02 -5.040e+02 -5.110e+02\n",
      " -5.130e+02 -5.160e+02 -5.180e+02 -5.190e+02 -5.190e+02 -5.210e+02\n",
      " -5.220e+02 -5.230e+02 -5.250e+02 -5.280e+02 -5.290e+02 -5.300e+02\n",
      " -5.300e+02 -5.310e+02 -5.320e+02 -5.320e+02 -5.320e+02 -5.330e+02\n",
      " -5.330e+02 -5.380e+02 -5.380e+02 -5.410e+02 -5.410e+02 -5.450e+02\n",
      " -5.460e+02 -5.470e+02  5.480e+02 -5.490e+02 -5.520e+02 -5.520e+02\n",
      " -5.540e+02 -5.540e+02 -5.540e+02 -5.540e+02 -5.540e+02  5.580e+02\n",
      " -5.620e+02 -5.620e+02 -5.630e+02 -5.660e+02 -5.670e+02 -5.680e+02\n",
      " -5.720e+02  5.730e+02 -5.740e+02 -5.740e+02 -5.750e+02 -5.750e+02\n",
      " -5.750e+02 -5.760e+02 -5.770e+02 -5.770e+02 -5.770e+02 -5.790e+02\n",
      " -5.790e+02 -5.840e+02  5.840e+02 -5.860e+02 -5.880e+02 -5.880e+02\n",
      " -5.900e+02 -5.910e+02 -5.940e+02 -5.950e+02 -5.950e+02 -5.980e+02\n",
      " -6.000e+02 -6.020e+02 -6.070e+02 -6.070e+02 -6.080e+02 -6.080e+02\n",
      " -6.110e+02 -6.110e+02 -6.120e+02 -6.120e+02 -6.140e+02 -6.160e+02\n",
      " -6.160e+02 -6.180e+02 -6.200e+02 -6.200e+02 -6.220e+02 -6.260e+02\n",
      " -6.260e+02 -6.270e+02 -6.290e+02 -6.350e+02 -6.350e+02 -6.350e+02\n",
      " -6.390e+02  6.390e+02 -6.400e+02 -6.410e+02 -6.460e+02 -6.470e+02\n",
      " -6.480e+02 -6.510e+02 -6.550e+02 -6.580e+02 -6.590e+02 -6.590e+02\n",
      " -6.600e+02 -6.610e+02 -6.640e+02 -6.660e+02 -6.660e+02 -6.750e+02\n",
      " -6.770e+02 -6.770e+02  6.780e+02 -6.790e+02 -6.790e+02 -6.800e+02\n",
      " -6.810e+02 -6.830e+02 -6.940e+02 -6.980e+02 -7.010e+02 -7.020e+02\n",
      " -7.030e+02 -7.060e+02 -7.070e+02 -7.070e+02 -7.090e+02 -7.140e+02\n",
      " -7.150e+02 -7.150e+02 -7.180e+02 -7.220e+02  7.230e+02 -7.250e+02\n",
      " -7.260e+02 -7.270e+02 -7.280e+02 -7.340e+02 -7.380e+02 -7.450e+02\n",
      " -7.470e+02 -7.470e+02 -7.480e+02  7.490e+02 -7.520e+02 -7.520e+02\n",
      "  7.540e+02 -7.540e+02 -7.540e+02 -7.550e+02 -7.590e+02 -7.590e+02\n",
      " -7.600e+02 -7.620e+02 -7.620e+02  7.630e+02 -7.650e+02 -7.670e+02\n",
      " -7.770e+02 -7.830e+02 -7.850e+02  7.850e+02  7.860e+02 -7.880e+02\n",
      " -7.890e+02  7.920e+02  7.920e+02 -7.920e+02 -7.950e+02 -7.980e+02\n",
      " -8.060e+02 -8.120e+02 -8.190e+02 -8.200e+02  8.210e+02  8.250e+02\n",
      " -8.290e+02 -8.370e+02 -8.390e+02 -8.470e+02 -8.490e+02 -8.520e+02\n",
      " -8.560e+02 -8.580e+02 -8.600e+02  8.600e+02 -8.650e+02 -8.650e+02\n",
      " -8.670e+02 -8.750e+02 -8.780e+02 -8.780e+02  8.790e+02 -8.810e+02\n",
      "  8.830e+02 -8.830e+02 -8.990e+02  9.040e+02 -9.060e+02 -9.070e+02\n",
      " -9.080e+02  9.120e+02 -9.120e+02 -9.120e+02 -9.150e+02 -9.180e+02\n",
      "  9.210e+02 -9.260e+02 -9.310e+02 -9.310e+02 -9.420e+02 -9.430e+02\n",
      "  9.430e+02 -9.430e+02 -9.430e+02 -9.520e+02 -9.540e+02  9.590e+02\n",
      " -9.650e+02 -9.650e+02  9.670e+02 -9.680e+02 -9.720e+02 -9.740e+02\n",
      " -9.740e+02 -9.750e+02 -9.750e+02 -9.840e+02 -9.870e+02 -9.890e+02\n",
      " -9.900e+02  9.910e+02  9.910e+02 -9.960e+02 -9.960e+02 -9.970e+02\n",
      " -1.001e+03 -1.001e+03 -1.004e+03 -1.004e+03 -1.004e+03 -1.005e+03\n",
      "  1.009e+03 -1.009e+03 -1.010e+03 -1.013e+03 -1.015e+03 -1.025e+03\n",
      " -1.026e+03 -1.026e+03 -1.027e+03  1.032e+03  1.034e+03 -1.034e+03\n",
      " -1.035e+03 -1.039e+03 -1.042e+03 -1.043e+03 -1.043e+03 -1.043e+03\n",
      " -1.047e+03  1.048e+03 -1.051e+03 -1.051e+03 -1.051e+03 -1.059e+03\n",
      " -1.062e+03 -1.062e+03 -1.063e+03 -1.066e+03  1.072e+03 -1.074e+03\n",
      " -1.079e+03 -1.085e+03 -1.093e+03 -1.099e+03 -1.101e+03 -1.102e+03\n",
      "  1.104e+03 -1.106e+03 -1.112e+03 -1.119e+03 -1.120e+03 -1.121e+03\n",
      " -1.124e+03  1.127e+03 -1.133e+03 -1.139e+03 -1.140e+03 -1.141e+03\n",
      "  1.142e+03 -1.148e+03  1.148e+03 -1.148e+03  1.152e+03 -1.156e+03\n",
      " -1.156e+03 -1.157e+03 -1.158e+03 -1.161e+03 -1.165e+03 -1.167e+03\n",
      " -1.174e+03  1.174e+03 -1.179e+03 -1.185e+03 -1.186e+03 -1.191e+03\n",
      " -1.196e+03 -1.198e+03 -1.203e+03 -1.203e+03 -1.206e+03 -1.208e+03\n",
      " -1.208e+03 -1.210e+03 -1.217e+03 -1.219e+03 -1.220e+03 -1.220e+03\n",
      " -1.224e+03 -1.232e+03 -1.233e+03 -1.234e+03 -1.234e+03 -1.246e+03\n",
      " -1.247e+03 -1.248e+03 -1.251e+03 -1.266e+03 -1.269e+03 -1.270e+03\n",
      "  1.272e+03 -1.285e+03 -1.288e+03 -1.291e+03 -1.301e+03 -1.308e+03\n",
      " -1.308e+03 -1.308e+03 -1.309e+03 -1.309e+03 -1.318e+03 -1.325e+03\n",
      " -1.326e+03 -1.330e+03 -1.347e+03 -1.359e+03 -1.363e+03 -1.369e+03\n",
      " -1.371e+03 -1.371e+03  1.388e+03  1.411e+03 -1.417e+03 -1.417e+03\n",
      " -1.419e+03  1.430e+03 -1.437e+03  1.439e+03 -1.448e+03 -1.449e+03\n",
      " -1.461e+03 -1.463e+03 -1.467e+03 -1.471e+03 -1.474e+03 -1.474e+03\n",
      " -1.476e+03 -1.477e+03 -1.505e+03 -1.516e+03 -1.519e+03 -1.523e+03\n",
      " -1.532e+03 -1.534e+03 -1.535e+03  1.542e+03 -1.542e+03 -1.545e+03\n",
      " -1.546e+03 -1.547e+03 -1.548e+03 -1.550e+03  1.556e+03 -1.561e+03\n",
      "  1.563e+03 -1.563e+03 -1.569e+03 -1.572e+03 -1.587e+03 -1.596e+03\n",
      " -1.604e+03 -1.611e+03 -1.611e+03 -1.614e+03 -1.616e+03 -1.620e+03\n",
      " -1.631e+03 -1.639e+03 -1.640e+03  1.642e+03 -1.644e+03 -1.648e+03\n",
      "  1.649e+03 -1.662e+03 -1.673e+03 -1.673e+03  1.673e+03 -1.682e+03\n",
      " -1.683e+03  1.688e+03 -1.688e+03 -1.692e+03  1.694e+03  1.699e+03\n",
      " -1.712e+03 -1.728e+03 -1.728e+03 -1.732e+03 -1.742e+03  1.759e+03\n",
      "  1.793e+03 -1.800e+03 -1.820e+03 -1.836e+03 -1.847e+03 -1.855e+03\n",
      " -1.864e+03 -1.866e+03 -1.871e+03 -1.873e+03 -1.876e+03 -1.882e+03\n",
      "  1.884e+03 -1.887e+03  1.900e+03 -1.919e+03  1.920e+03 -1.925e+03\n",
      " -1.926e+03  1.927e+03 -1.928e+03 -1.935e+03 -1.935e+03 -1.972e+03\n",
      " -1.988e+03  1.993e+03 -1.996e+03 -2.012e+03 -2.019e+03 -2.031e+03\n",
      " -2.033e+03 -2.041e+03 -2.048e+03 -2.064e+03 -2.072e+03 -2.082e+03\n",
      " -2.091e+03  2.097e+03 -2.108e+03 -2.128e+03 -2.136e+03 -2.155e+03\n",
      " -2.156e+03 -2.161e+03 -2.164e+03 -2.184e+03 -2.190e+03 -2.190e+03\n",
      " -2.191e+03  2.192e+03 -2.193e+03  2.207e+03 -2.222e+03 -2.231e+03\n",
      " -2.236e+03 -2.240e+03 -2.246e+03 -2.255e+03 -2.255e+03 -2.263e+03\n",
      "  2.273e+03 -2.278e+03 -2.281e+03 -2.282e+03 -2.288e+03 -2.289e+03\n",
      "  2.296e+03 -2.311e+03 -2.330e+03  2.348e+03  2.361e+03 -2.362e+03\n",
      " -2.371e+03 -2.372e+03  2.373e+03 -2.381e+03 -2.403e+03 -2.406e+03\n",
      "  2.417e+03 -2.426e+03 -2.442e+03  2.469e+03 -2.477e+03  2.483e+03\n",
      " -2.483e+03 -2.486e+03 -2.489e+03  2.520e+03  2.534e+03 -2.535e+03\n",
      " -2.541e+03  2.551e+03 -2.558e+03 -2.559e+03  2.573e+03 -2.590e+03\n",
      " -2.596e+03 -2.596e+03 -2.605e+03 -2.612e+03 -2.618e+03 -2.629e+03\n",
      " -2.632e+03 -2.632e+03  2.636e+03 -2.650e+03 -2.653e+03 -2.654e+03\n",
      " -2.695e+03 -2.707e+03  2.712e+03 -2.721e+03  2.763e+03 -2.767e+03\n",
      "  2.798e+03 -2.813e+03 -2.838e+03  2.854e+03 -2.856e+03  2.866e+03\n",
      " -2.868e+03 -2.953e+03  2.965e+03  2.965e+03 -2.971e+03 -2.976e+03\n",
      " -2.989e+03 -2.991e+03 -2.991e+03 -3.001e+03 -3.015e+03 -3.017e+03\n",
      " -3.021e+03 -3.022e+03 -3.030e+03 -3.035e+03 -3.072e+03 -3.088e+03\n",
      " -3.091e+03 -3.094e+03 -3.112e+03 -3.121e+03  3.126e+03 -3.128e+03\n",
      " -3.152e+03 -3.159e+03 -3.172e+03 -3.203e+03 -3.226e+03 -3.247e+03\n",
      " -3.248e+03 -3.256e+03 -3.261e+03  3.262e+03 -3.276e+03 -3.283e+03\n",
      " -3.283e+03 -3.286e+03 -3.296e+03 -3.316e+03 -3.361e+03 -3.364e+03\n",
      "  3.409e+03  3.418e+03 -3.430e+03 -3.456e+03  3.461e+03  3.462e+03\n",
      "  3.472e+03  3.492e+03 -3.506e+03 -3.519e+03 -3.607e+03 -3.660e+03\n",
      "  3.669e+03 -3.709e+03  3.926e+03  3.941e+03  3.945e+03 -3.957e+03\n",
      " -4.052e+03 -4.080e+03 -4.088e+03 -4.159e+03 -4.233e+03  4.267e+03\n",
      " -4.275e+03 -4.285e+03 -4.354e+03  4.456e+03 -4.894e+03 -5.042e+03\n",
      " -5.062e+03 -5.176e+03 -5.739e+03 -6.292e+03  6.456e+03  6.593e+03\n",
      " -7.106e+03 -7.126e+03  7.455e+03 -7.777e+03 -8.008e+03 -8.391e+03\n",
      " -8.605e+03]\n",
      "durations 5.0 8556.0\n",
      "Concordance Index 0.5100746268656716\n",
      "Integrated Brier Score: 0.17943674452267058\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m177.6334\u001b[0m      \u001b[32m100.0803\u001b[0m  0.0721\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m170.6819\u001b[0m      \u001b[32m127.1943\u001b[0m  0.0741\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m160.5002\u001b[0m      \u001b[32m112.0579\u001b[0m  0.0575\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m77.2032\u001b[0m      \u001b[32m108.3421\u001b[0m  0.0794\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m145.1245\u001b[0m       \u001b[32m97.5344\u001b[0m  0.0448\n",
      "      2      \u001b[36m172.0447\u001b[0m       \u001b[32m95.7562\u001b[0m  0.0468\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m147.8532\u001b[0m       \u001b[32m71.9767\u001b[0m  0.0594\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m80.8127\u001b[0m       \u001b[32m83.9614\u001b[0m  0.0964\n",
      "      2      \u001b[36m163.5615\u001b[0m      \u001b[32m119.3377\u001b[0m  0.0584\n",
      "      2      \u001b[36m152.0753\u001b[0m      \u001b[32m110.1395\u001b[0m  0.0477\n",
      "      3      \u001b[36m167.7578\u001b[0m       \u001b[32m93.6896\u001b[0m  0.0339\n",
      "      2      \u001b[36m139.1876\u001b[0m       \u001b[32m92.2853\u001b[0m  0.0364\n",
      "      2      \u001b[36m140.3666\u001b[0m       73.9295  0.0327\n",
      "      2       \u001b[36m73.1166\u001b[0m       \u001b[32m97.3599\u001b[0m  0.0621\n",
      "      3      \u001b[36m160.1768\u001b[0m      \u001b[32m114.5004\u001b[0m  0.0320\n",
      "      3      \u001b[36m151.6088\u001b[0m      \u001b[32m104.0990\u001b[0m  0.0313\n",
      "      4      \u001b[36m166.8022\u001b[0m       \u001b[32m93.3777\u001b[0m  0.0307\n",
      "      3      \u001b[36m137.9567\u001b[0m       \u001b[32m86.6261\u001b[0m  0.0334\n",
      "      2       \u001b[36m74.9979\u001b[0m       \u001b[32m80.8440\u001b[0m  0.0513\n",
      "      3      \u001b[36m139.5246\u001b[0m       73.7055  0.0301\n",
      "      4      \u001b[36m146.7247\u001b[0m       \u001b[32m99.7490\u001b[0m  0.0297\n",
      "      4      \u001b[36m159.4638\u001b[0m      \u001b[32m111.4997\u001b[0m  0.0332\n",
      "      5      168.1124       \u001b[32m93.3626\u001b[0m  0.0299\n",
      "      4      \u001b[36m136.8622\u001b[0m       \u001b[32m83.7113\u001b[0m  0.0288\n",
      "      3       \u001b[36m70.0216\u001b[0m      102.0259  0.0476\n",
      "      4      \u001b[36m137.6699\u001b[0m       73.1612  0.0278\n",
      "      3       \u001b[36m72.9784\u001b[0m       83.5712  0.0456\n",
      "      5      160.2184      \u001b[32m109.9261\u001b[0m  0.0316\n",
      "      5      \u001b[36m145.5180\u001b[0m       \u001b[32m97.5599\u001b[0m  0.0432\n",
      "      5      \u001b[36m133.5685\u001b[0m       \u001b[32m82.5826\u001b[0m  0.0298\n",
      "      6      167.1067       \u001b[32m93.0147\u001b[0m  0.0327\n",
      "      5      \u001b[36m137.0364\u001b[0m       72.6755  0.0262\n",
      "      6      \u001b[36m155.9023\u001b[0m      \u001b[32m108.9962\u001b[0m  0.0304\n",
      "      6      \u001b[36m143.6198\u001b[0m       \u001b[32m96.8029\u001b[0m  0.0295\n",
      "      4       \u001b[36m68.9233\u001b[0m      102.0036  0.0597\n",
      "      6      \u001b[36m133.2539\u001b[0m       \u001b[32m82.4457\u001b[0m  0.0296\n",
      "      7      \u001b[36m166.1594\u001b[0m       \u001b[32m92.5966\u001b[0m  0.0295\n",
      "      4       \u001b[36m72.2706\u001b[0m       84.0396  0.0438\n",
      "      6      \u001b[36m133.4967\u001b[0m       72.1659  0.0334\n",
      "      7      159.1674      \u001b[32m108.4152\u001b[0m  0.0353\n",
      "      7      144.5852       96.9660  0.0309\n",
      "      8      \u001b[36m163.2414\u001b[0m       \u001b[32m92.2438\u001b[0m  0.0323\n",
      "      7      134.1576       83.0207  0.0369\n",
      "      7      \u001b[36m132.4746\u001b[0m       \u001b[32m71.9148\u001b[0m  0.0308\n",
      "      5       69.0729       98.0907  0.0468\n",
      "      5       \u001b[36m70.9100\u001b[0m       \u001b[32m80.7950\u001b[0m  0.0523\n",
      "      8      \u001b[36m143.1176\u001b[0m       97.7962  0.0308\n",
      "      8      156.4367      \u001b[32m107.9395\u001b[0m  0.0349\n",
      "      9      164.3094       \u001b[32m92.1413\u001b[0m  0.0325\n",
      "      8      \u001b[36m131.7903\u001b[0m       71.9842  0.0301\n",
      "      8      133.5473       83.8743  0.0328\n",
      "      9      \u001b[36m143.0631\u001b[0m       98.7794  0.0285\n",
      "      6       \u001b[36m68.5764\u001b[0m       \u001b[32m95.2677\u001b[0m  0.0447\n",
      "      9      \u001b[36m155.7375\u001b[0m      \u001b[32m107.4323\u001b[0m  0.0315\n",
      "     10      165.1654       92.1866  0.0298\n",
      "Restoring best model from epoch 9.\n",
      "      9      \u001b[36m130.6237\u001b[0m       84.4414  0.0272\n",
      "      9      133.8515       72.0444  0.0311\n",
      "      6       72.7297       \u001b[32m78.5706\u001b[0m  0.0482\n",
      "     10      144.8245       99.6155  0.0284\n",
      "Restoring best model from epoch 6.\n",
      "     10      157.8517      \u001b[32m107.1697\u001b[0m  0.0302\n",
      "     10      \u001b[36m129.8558\u001b[0m       72.0233  0.0273\n",
      "Restoring best model from epoch 7.\n",
      "     10      \u001b[36m129.4280\u001b[0m       84.8674  0.0326\n",
      "Restoring best model from epoch 6.\n",
      "      7       \u001b[36m66.9448\u001b[0m       \u001b[32m94.1650\u001b[0m  0.0453\n",
      "      7       \u001b[36m69.7295\u001b[0m       78.7459  0.0450\n",
      "      8       68.6551       94.4410  0.0451\n",
      "      8       71.3702       78.8229  0.0491\n",
      "      9       67.7226       96.4416  0.0437\n",
      "      9       \u001b[36m68.7645\u001b[0m       80.8343  0.0443\n",
      "     10       67.5651       95.6400  0.0433\n",
      "Restoring best model from epoch 7.\n",
      "     10       69.9983       80.5529  0.0441\n",
      "Restoring best model from epoch 6.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m159.7007\u001b[0m      \u001b[32m101.3594\u001b[0m  0.0308\n",
      "      2      \u001b[36m154.7812\u001b[0m       \u001b[32m98.0485\u001b[0m  0.0279\n",
      "      3      \u001b[36m154.2551\u001b[0m       \u001b[32m96.0766\u001b[0m  0.0286\n",
      "      4      \u001b[36m151.6728\u001b[0m       \u001b[32m94.8343\u001b[0m  0.0286\n",
      "      5      \u001b[36m147.2723\u001b[0m       94.8527  0.0310\n",
      "      6      148.2233       95.2988  0.0301\n",
      "      7      \u001b[36m144.0907\u001b[0m       95.5178  0.0286\n",
      "      8      148.4729       95.3650  0.0286\n",
      "      9      145.1087       95.2388  0.0319\n",
      "     10      147.8977       94.9675  0.0395\n",
      "Restoring best model from epoch 4.\n",
      "y_train breslow final [-1.000e+00 -5.000e+00 -5.000e+00 -5.000e+00 -7.000e+00 -8.000e+00\n",
      " -9.000e+00 -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01\n",
      " -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01\n",
      " -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01 -1.100e+01 -1.900e+01\n",
      " -2.100e+01 -2.600e+01 -3.000e+01 -3.000e+01 -3.000e+01 -3.100e+01\n",
      " -3.100e+01 -3.100e+01 -3.400e+01 -3.400e+01 -4.000e+01 -4.900e+01\n",
      " -5.200e+01 -5.400e+01 -6.400e+01 -7.000e+01 -7.200e+01 -7.600e+01\n",
      " -7.800e+01 -7.800e+01 -7.900e+01 -8.400e+01 -9.000e+01 -9.200e+01\n",
      " -9.800e+01  1.160e+02 -1.340e+02 -1.490e+02 -1.600e+02 -1.700e+02\n",
      " -1.700e+02  1.720e+02 -1.720e+02  1.740e+02 -1.780e+02 -1.860e+02\n",
      " -2.000e+02 -2.100e+02 -2.140e+02 -2.150e+02 -2.160e+02 -2.220e+02\n",
      " -2.240e+02  2.240e+02 -2.250e+02 -2.270e+02 -2.310e+02  2.390e+02\n",
      " -2.420e+02 -2.430e+02 -2.500e+02 -2.520e+02  2.550e+02 -2.580e+02\n",
      " -2.590e+02 -2.660e+02  2.660e+02 -2.710e+02 -2.730e+02 -2.740e+02\n",
      " -2.750e+02 -2.850e+02 -2.870e+02 -2.930e+02  2.950e+02 -3.000e+02\n",
      "  3.020e+02 -3.030e+02 -3.030e+02 -3.040e+02 -3.040e+02 -3.040e+02\n",
      " -3.040e+02  3.040e+02 -3.130e+02 -3.170e+02 -3.170e+02  3.200e+02\n",
      "  3.220e+02 -3.220e+02 -3.220e+02 -3.230e+02 -3.260e+02 -3.280e+02\n",
      " -3.320e+02 -3.340e+02 -3.360e+02 -3.450e+02 -3.470e+02 -3.480e+02\n",
      " -3.500e+02 -3.520e+02 -3.580e+02 -3.580e+02 -3.580e+02 -3.600e+02\n",
      " -3.640e+02 -3.650e+02 -3.650e+02  3.650e+02 -3.650e+02 -3.650e+02\n",
      " -3.660e+02 -3.660e+02 -3.710e+02 -3.710e+02 -3.730e+02 -3.730e+02\n",
      " -3.750e+02 -3.750e+02 -3.750e+02 -3.750e+02 -3.760e+02 -3.760e+02\n",
      "  3.770e+02 -3.790e+02 -3.800e+02 -3.800e+02 -3.810e+02 -3.810e+02\n",
      " -3.830e+02 -3.830e+02  3.850e+02 -3.850e+02 -3.850e+02 -3.850e+02\n",
      " -3.920e+02 -3.930e+02 -3.930e+02 -3.940e+02 -3.940e+02 -3.940e+02\n",
      " -3.950e+02 -3.960e+02 -3.960e+02 -3.960e+02 -3.960e+02 -3.960e+02\n",
      " -3.970e+02 -3.980e+02 -4.020e+02 -4.030e+02 -4.030e+02 -4.050e+02\n",
      " -4.080e+02 -4.080e+02 -4.090e+02 -4.100e+02 -4.100e+02 -4.110e+02\n",
      " -4.120e+02 -4.130e+02 -4.140e+02 -4.170e+02 -4.180e+02 -4.210e+02\n",
      " -4.230e+02 -4.240e+02 -4.250e+02  4.260e+02 -4.280e+02 -4.280e+02\n",
      " -4.300e+02 -4.310e+02 -4.330e+02 -4.340e+02 -4.350e+02 -4.370e+02\n",
      " -4.390e+02 -4.390e+02 -4.390e+02 -4.410e+02 -4.410e+02 -4.410e+02\n",
      " -4.460e+02  4.460e+02 -4.470e+02 -4.470e+02 -4.470e+02 -4.480e+02\n",
      " -4.480e+02 -4.500e+02 -4.500e+02 -4.510e+02 -4.540e+02 -4.550e+02\n",
      " -4.560e+02 -4.580e+02 -4.610e+02 -4.610e+02 -4.630e+02 -4.660e+02\n",
      " -4.660e+02 -4.670e+02 -4.700e+02 -4.710e+02 -4.720e+02 -4.750e+02\n",
      " -4.760e+02 -4.770e+02 -4.800e+02 -4.820e+02 -4.880e+02 -4.880e+02\n",
      " -4.890e+02 -4.920e+02 -4.950e+02 -4.960e+02 -4.960e+02 -4.990e+02\n",
      " -4.990e+02 -5.010e+02 -5.020e+02 -5.030e+02 -5.040e+02 -5.040e+02\n",
      " -5.060e+02 -5.080e+02 -5.090e+02 -5.100e+02 -5.110e+02 -5.180e+02\n",
      " -5.180e+02 -5.180e+02 -5.190e+02 -5.210e+02 -5.220e+02 -5.230e+02\n",
      "  5.240e+02 -5.250e+02 -5.260e+02 -5.280e+02 -5.280e+02 -5.290e+02\n",
      " -5.300e+02 -5.300e+02 -5.310e+02 -5.320e+02 -5.320e+02 -5.330e+02\n",
      " -5.330e+02 -5.380e+02 -5.380e+02  5.380e+02 -5.410e+02 -5.410e+02\n",
      " -5.450e+02 -5.460e+02 -5.470e+02  5.480e+02 -5.490e+02 -5.510e+02\n",
      " -5.520e+02 -5.520e+02 -5.540e+02 -5.540e+02 -5.540e+02 -5.620e+02\n",
      " -5.630e+02  5.630e+02 -5.650e+02 -5.660e+02 -5.670e+02 -5.680e+02\n",
      "  5.710e+02 -5.720e+02  5.730e+02 -5.740e+02 -5.750e+02 -5.750e+02\n",
      " -5.750e+02 -5.750e+02 -5.770e+02 -5.770e+02 -5.770e+02 -5.790e+02\n",
      " -5.790e+02 -5.840e+02 -5.840e+02  5.840e+02 -5.840e+02 -5.850e+02\n",
      " -5.880e+02 -5.880e+02 -5.880e+02 -5.900e+02 -5.940e+02 -5.950e+02\n",
      " -5.960e+02 -5.980e+02 -6.000e+02 -6.060e+02 -6.070e+02 -6.080e+02\n",
      " -6.080e+02 -6.110e+02 -6.110e+02 -6.120e+02  6.120e+02 -6.120e+02\n",
      " -6.140e+02  6.140e+02  6.160e+02 -6.160e+02 -6.160e+02 -6.200e+02\n",
      " -6.200e+02 -6.220e+02 -6.220e+02 -6.240e+02 -6.260e+02 -6.260e+02\n",
      " -6.270e+02 -6.300e+02 -6.310e+02 -6.350e+02 -6.350e+02 -6.390e+02\n",
      "  6.390e+02 -6.400e+02 -6.400e+02 -6.410e+02 -6.430e+02 -6.430e+02\n",
      " -6.440e+02 -6.460e+02 -6.470e+02 -6.510e+02 -6.520e+02 -6.550e+02\n",
      " -6.580e+02 -6.590e+02 -6.590e+02 -6.600e+02 -6.610e+02 -6.620e+02\n",
      " -6.640e+02 -6.660e+02 -6.660e+02 -6.720e+02 -6.750e+02 -6.750e+02\n",
      " -6.770e+02 -6.770e+02  6.780e+02 -6.790e+02 -6.790e+02 -6.810e+02\n",
      " -6.830e+02 -6.940e+02 -6.940e+02 -7.010e+02 -7.030e+02 -7.060e+02\n",
      " -7.100e+02 -7.140e+02 -7.140e+02 -7.150e+02 -7.180e+02 -7.220e+02\n",
      "  7.230e+02 -7.240e+02 -7.250e+02 -7.260e+02 -7.270e+02 -7.270e+02\n",
      " -7.280e+02 -7.340e+02 -7.380e+02 -7.420e+02 -7.450e+02 -7.470e+02\n",
      " -7.470e+02 -7.470e+02 -7.480e+02  7.490e+02 -7.520e+02 -7.520e+02\n",
      "  7.540e+02 -7.540e+02 -7.550e+02 -7.590e+02 -7.600e+02 -7.600e+02\n",
      " -7.610e+02 -7.620e+02 -7.620e+02  7.630e+02 -7.650e+02 -7.670e+02\n",
      " -7.670e+02 -7.690e+02 -7.830e+02 -7.850e+02  7.850e+02 -7.880e+02\n",
      " -7.890e+02 -7.910e+02  7.920e+02 -7.920e+02 -7.950e+02 -7.980e+02\n",
      " -8.030e+02  8.110e+02 -8.190e+02 -8.200e+02  8.210e+02 -8.220e+02\n",
      "  8.250e+02 -8.370e+02 -8.390e+02 -8.520e+02 -8.560e+02 -8.580e+02\n",
      " -8.600e+02  8.600e+02 -8.650e+02 -8.650e+02 -8.670e+02 -8.750e+02\n",
      " -8.780e+02 -8.780e+02  8.790e+02 -8.810e+02 -8.830e+02 -8.890e+02\n",
      " -8.900e+02 -8.990e+02  9.040e+02 -9.060e+02 -9.080e+02 -9.110e+02\n",
      "  9.120e+02 -9.120e+02 -9.180e+02  9.210e+02 -9.230e+02 -9.260e+02\n",
      " -9.310e+02 -9.420e+02 -9.430e+02 -9.430e+02 -9.430e+02 -9.520e+02\n",
      " -9.540e+02  9.590e+02 -9.650e+02 -9.650e+02  9.670e+02 -9.680e+02\n",
      " -9.720e+02 -9.730e+02 -9.740e+02 -9.750e+02 -9.750e+02  9.760e+02\n",
      " -9.840e+02 -9.870e+02 -9.900e+02 -9.960e+02 -9.960e+02 -9.970e+02\n",
      " -1.000e+03 -1.001e+03 -1.001e+03  1.004e+03 -1.004e+03 -1.004e+03\n",
      " -1.005e+03 -1.006e+03 -1.007e+03  1.009e+03 -1.010e+03 -1.013e+03\n",
      " -1.015e+03 -1.025e+03 -1.027e+03  1.032e+03  1.034e+03 -1.034e+03\n",
      " -1.035e+03 -1.042e+03 -1.043e+03 -1.043e+03 -1.043e+03  1.048e+03\n",
      " -1.051e+03 -1.051e+03 -1.051e+03 -1.062e+03 -1.062e+03 -1.063e+03\n",
      "  1.072e+03 -1.085e+03 -1.088e+03 -1.093e+03  1.093e+03 -1.099e+03\n",
      " -1.101e+03 -1.102e+03  1.104e+03 -1.112e+03 -1.119e+03 -1.120e+03\n",
      " -1.120e+03 -1.121e+03 -1.124e+03  1.127e+03 -1.132e+03 -1.138e+03\n",
      " -1.139e+03 -1.141e+03 -1.148e+03  1.148e+03 -1.148e+03 -1.150e+03\n",
      "  1.152e+03 -1.156e+03 -1.156e+03 -1.157e+03 -1.158e+03 -1.161e+03\n",
      " -1.162e+03 -1.163e+03 -1.165e+03 -1.167e+03  1.174e+03 -1.179e+03\n",
      " -1.185e+03 -1.186e+03 -1.189e+03 -1.191e+03 -1.196e+03 -1.203e+03\n",
      " -1.203e+03 -1.208e+03 -1.217e+03 -1.219e+03 -1.220e+03 -1.224e+03\n",
      " -1.229e+03 -1.229e+03 -1.233e+03 -1.234e+03 -1.234e+03 -1.239e+03\n",
      " -1.246e+03 -1.247e+03 -1.248e+03 -1.266e+03 -1.269e+03 -1.270e+03\n",
      "  1.272e+03  1.275e+03 -1.277e+03 -1.285e+03  1.286e+03 -1.288e+03\n",
      " -1.291e+03 -1.309e+03 -1.318e+03 -1.321e+03  1.324e+03 -1.325e+03\n",
      " -1.347e+03 -1.351e+03 -1.359e+03 -1.363e+03 -1.363e+03  1.365e+03\n",
      " -1.369e+03 -1.371e+03  1.388e+03  1.411e+03 -1.419e+03  1.430e+03\n",
      " -1.434e+03 -1.437e+03 -1.448e+03 -1.449e+03 -1.461e+03 -1.467e+03\n",
      " -1.471e+03 -1.474e+03 -1.476e+03 -1.477e+03 -1.492e+03 -1.505e+03\n",
      "  1.508e+03 -1.516e+03 -1.519e+03 -1.522e+03 -1.523e+03 -1.528e+03\n",
      " -1.534e+03 -1.535e+03  1.542e+03 -1.545e+03 -1.545e+03 -1.547e+03\n",
      " -1.548e+03 -1.561e+03  1.563e+03 -1.569e+03 -1.587e+03 -1.604e+03\n",
      " -1.604e+03 -1.611e+03 -1.611e+03 -1.611e+03 -1.612e+03 -1.613e+03\n",
      " -1.614e+03 -1.620e+03 -1.631e+03  1.642e+03 -1.642e+03 -1.644e+03\n",
      " -1.648e+03  1.649e+03 -1.662e+03 -1.673e+03  1.673e+03 -1.682e+03\n",
      " -1.686e+03  1.688e+03 -1.688e+03  1.692e+03  1.694e+03  1.699e+03\n",
      " -1.712e+03 -1.728e+03 -1.742e+03  1.759e+03 -1.780e+03  1.781e+03\n",
      " -1.783e+03  1.793e+03 -1.800e+03  1.812e+03 -1.820e+03 -1.836e+03\n",
      " -1.842e+03 -1.847e+03 -1.853e+03 -1.864e+03 -1.866e+03 -1.871e+03\n",
      " -1.876e+03  1.884e+03 -1.887e+03  1.900e+03 -1.914e+03 -1.919e+03\n",
      "  1.920e+03 -1.926e+03  1.927e+03 -1.928e+03 -1.935e+03 -1.953e+03\n",
      " -1.972e+03 -1.980e+03 -1.988e+03  2.009e+03 -2.012e+03 -2.019e+03\n",
      " -2.031e+03 -2.033e+03 -2.048e+03 -2.072e+03  2.097e+03 -2.109e+03\n",
      " -2.124e+03  2.127e+03 -2.128e+03 -2.136e+03 -2.140e+03 -2.142e+03\n",
      " -2.155e+03 -2.156e+03 -2.161e+03 -2.164e+03 -2.184e+03 -2.190e+03\n",
      " -2.197e+03  2.207e+03 -2.236e+03 -2.248e+03 -2.255e+03 -2.255e+03\n",
      " -2.255e+03 -2.263e+03  2.273e+03 -2.278e+03 -2.281e+03 -2.282e+03\n",
      " -2.288e+03 -2.289e+03  2.296e+03 -2.306e+03 -2.311e+03 -2.329e+03\n",
      " -2.330e+03 -2.335e+03  2.348e+03  2.361e+03 -2.365e+03 -2.372e+03\n",
      " -2.372e+03  2.373e+03 -2.381e+03 -2.385e+03 -2.406e+03  2.417e+03\n",
      " -2.426e+03 -2.442e+03 -2.442e+03  2.469e+03 -2.477e+03  2.483e+03\n",
      " -2.483e+03 -2.486e+03 -2.489e+03 -2.513e+03 -2.515e+03  2.520e+03\n",
      "  2.534e+03 -2.535e+03 -2.541e+03  2.551e+03 -2.558e+03 -2.559e+03\n",
      "  2.573e+03 -2.590e+03 -2.596e+03 -2.596e+03 -2.618e+03 -2.632e+03\n",
      " -2.645e+03 -2.645e+03 -2.650e+03 -2.653e+03 -2.654e+03 -2.695e+03\n",
      " -2.707e+03 -2.709e+03 -2.759e+03  2.763e+03 -2.770e+03 -2.813e+03\n",
      " -2.838e+03 -2.856e+03  2.866e+03  2.911e+03 -2.920e+03 -2.953e+03\n",
      "  2.965e+03  2.965e+03 -2.971e+03 -2.989e+03 -2.991e+03 -2.991e+03\n",
      " -3.001e+03 -3.004e+03 -3.009e+03 -3.011e+03 -3.015e+03 -3.017e+03\n",
      " -3.021e+03 -3.030e+03 -3.035e+03  3.063e+03 -3.072e+03 -3.088e+03\n",
      " -3.091e+03 -3.102e+03 -3.112e+03 -3.121e+03  3.126e+03 -3.128e+03\n",
      " -3.152e+03 -3.172e+03 -3.202e+03 -3.203e+03 -3.204e+03 -3.226e+03\n",
      " -3.247e+03 -3.256e+03  3.262e+03 -3.283e+03 -3.283e+03 -3.287e+03\n",
      " -3.307e+03 -3.316e+03 -3.342e+03 -3.364e+03  3.409e+03  3.418e+03\n",
      " -3.430e+03 -3.456e+03  3.461e+03  3.462e+03  3.472e+03 -3.506e+03\n",
      " -3.519e+03 -3.603e+03 -3.607e+03 -3.660e+03  3.669e+03 -3.709e+03\n",
      "  3.736e+03 -3.807e+03  3.873e+03  3.941e+03 -3.957e+03  3.959e+03\n",
      " -4.005e+03 -4.047e+03 -4.052e+03 -4.080e+03 -4.088e+03 -4.233e+03\n",
      "  4.267e+03 -4.285e+03 -4.354e+03 -4.361e+03  4.456e+03 -4.894e+03\n",
      " -4.929e+03 -5.042e+03 -5.062e+03 -5.156e+03 -5.176e+03 -6.292e+03\n",
      " -7.126e+03  7.455e+03 -7.777e+03 -8.008e+03 -8.391e+03 -8.556e+03\n",
      " -8.605e+03]\n",
      "Concordance Index 0.5850117676204633\n",
      "Integrated Brier Score: 0.20752416166511614\n",
      "y_train breslow final [-1.000e+00 -5.000e+00 -5.000e+00 -5.000e+00 -7.000e+00 -8.000e+00\n",
      " -9.000e+00 -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01\n",
      " -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01\n",
      " -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01 -1.100e+01 -1.900e+01\n",
      " -2.100e+01 -2.600e+01 -3.000e+01 -3.000e+01 -3.000e+01 -3.100e+01\n",
      " -3.100e+01 -3.100e+01 -3.400e+01 -3.400e+01 -4.000e+01 -4.900e+01\n",
      " -5.200e+01 -5.400e+01 -6.400e+01 -7.000e+01 -7.200e+01 -7.600e+01\n",
      " -7.800e+01 -7.800e+01 -7.900e+01 -8.400e+01 -9.000e+01 -9.200e+01\n",
      " -9.800e+01  1.160e+02 -1.340e+02 -1.490e+02 -1.600e+02 -1.700e+02\n",
      " -1.700e+02  1.720e+02 -1.720e+02  1.740e+02 -1.780e+02 -1.860e+02\n",
      " -2.000e+02 -2.100e+02 -2.140e+02 -2.150e+02 -2.160e+02 -2.220e+02\n",
      " -2.240e+02  2.240e+02 -2.250e+02 -2.270e+02 -2.310e+02  2.390e+02\n",
      " -2.420e+02 -2.430e+02 -2.500e+02 -2.520e+02  2.550e+02 -2.580e+02\n",
      " -2.590e+02 -2.660e+02  2.660e+02 -2.710e+02 -2.730e+02 -2.740e+02\n",
      " -2.750e+02 -2.850e+02 -2.870e+02 -2.930e+02  2.950e+02 -3.000e+02\n",
      "  3.020e+02 -3.030e+02 -3.030e+02 -3.040e+02 -3.040e+02 -3.040e+02\n",
      " -3.040e+02  3.040e+02 -3.130e+02 -3.170e+02 -3.170e+02  3.200e+02\n",
      "  3.220e+02 -3.220e+02 -3.220e+02 -3.230e+02 -3.260e+02 -3.280e+02\n",
      " -3.320e+02 -3.340e+02 -3.360e+02 -3.450e+02 -3.470e+02 -3.480e+02\n",
      " -3.500e+02 -3.520e+02 -3.580e+02 -3.580e+02 -3.580e+02 -3.600e+02\n",
      " -3.640e+02 -3.650e+02 -3.650e+02  3.650e+02 -3.650e+02 -3.650e+02\n",
      " -3.660e+02 -3.660e+02 -3.710e+02 -3.710e+02 -3.730e+02 -3.730e+02\n",
      " -3.750e+02 -3.750e+02 -3.750e+02 -3.750e+02 -3.760e+02 -3.760e+02\n",
      "  3.770e+02 -3.790e+02 -3.800e+02 -3.800e+02 -3.810e+02 -3.810e+02\n",
      " -3.830e+02 -3.830e+02  3.850e+02 -3.850e+02 -3.850e+02 -3.850e+02\n",
      " -3.920e+02 -3.930e+02 -3.930e+02 -3.940e+02 -3.940e+02 -3.940e+02\n",
      " -3.950e+02 -3.960e+02 -3.960e+02 -3.960e+02 -3.960e+02 -3.960e+02\n",
      " -3.970e+02 -3.980e+02 -4.020e+02 -4.030e+02 -4.030e+02 -4.050e+02\n",
      " -4.080e+02 -4.080e+02 -4.090e+02 -4.100e+02 -4.100e+02 -4.110e+02\n",
      " -4.120e+02 -4.130e+02 -4.140e+02 -4.170e+02 -4.180e+02 -4.210e+02\n",
      " -4.230e+02 -4.240e+02 -4.250e+02  4.260e+02 -4.280e+02 -4.280e+02\n",
      " -4.300e+02 -4.310e+02 -4.330e+02 -4.340e+02 -4.350e+02 -4.370e+02\n",
      " -4.390e+02 -4.390e+02 -4.390e+02 -4.410e+02 -4.410e+02 -4.410e+02\n",
      " -4.460e+02  4.460e+02 -4.470e+02 -4.470e+02 -4.470e+02 -4.480e+02\n",
      " -4.480e+02 -4.500e+02 -4.500e+02 -4.510e+02 -4.540e+02 -4.550e+02\n",
      " -4.560e+02 -4.580e+02 -4.610e+02 -4.610e+02 -4.630e+02 -4.660e+02\n",
      " -4.660e+02 -4.670e+02 -4.700e+02 -4.710e+02 -4.720e+02 -4.750e+02\n",
      " -4.760e+02 -4.770e+02 -4.800e+02 -4.820e+02 -4.880e+02 -4.880e+02\n",
      " -4.890e+02 -4.920e+02 -4.950e+02 -4.960e+02 -4.960e+02 -4.990e+02\n",
      " -4.990e+02 -5.010e+02 -5.020e+02 -5.030e+02 -5.040e+02 -5.040e+02\n",
      " -5.060e+02 -5.080e+02 -5.090e+02 -5.100e+02 -5.110e+02 -5.180e+02\n",
      " -5.180e+02 -5.180e+02 -5.190e+02 -5.210e+02 -5.220e+02 -5.230e+02\n",
      "  5.240e+02 -5.250e+02 -5.260e+02 -5.280e+02 -5.280e+02 -5.290e+02\n",
      " -5.300e+02 -5.300e+02 -5.310e+02 -5.320e+02 -5.320e+02 -5.330e+02\n",
      " -5.330e+02 -5.380e+02 -5.380e+02  5.380e+02 -5.410e+02 -5.410e+02\n",
      " -5.450e+02 -5.460e+02 -5.470e+02  5.480e+02 -5.490e+02 -5.510e+02\n",
      " -5.520e+02 -5.520e+02 -5.540e+02 -5.540e+02 -5.540e+02 -5.620e+02\n",
      " -5.630e+02  5.630e+02 -5.650e+02 -5.660e+02 -5.670e+02 -5.680e+02\n",
      "  5.710e+02 -5.720e+02  5.730e+02 -5.740e+02 -5.750e+02 -5.750e+02\n",
      " -5.750e+02 -5.750e+02 -5.770e+02 -5.770e+02 -5.770e+02 -5.790e+02\n",
      " -5.790e+02 -5.840e+02 -5.840e+02  5.840e+02 -5.840e+02 -5.850e+02\n",
      " -5.880e+02 -5.880e+02 -5.880e+02 -5.900e+02 -5.940e+02 -5.950e+02\n",
      " -5.960e+02 -5.980e+02 -6.000e+02 -6.060e+02 -6.070e+02 -6.080e+02\n",
      " -6.080e+02 -6.110e+02 -6.110e+02 -6.120e+02  6.120e+02 -6.120e+02\n",
      " -6.140e+02  6.140e+02  6.160e+02 -6.160e+02 -6.160e+02 -6.200e+02\n",
      " -6.200e+02 -6.220e+02 -6.220e+02 -6.240e+02 -6.260e+02 -6.260e+02\n",
      " -6.270e+02 -6.300e+02 -6.310e+02 -6.350e+02 -6.350e+02 -6.390e+02\n",
      "  6.390e+02 -6.400e+02 -6.400e+02 -6.410e+02 -6.430e+02 -6.430e+02\n",
      " -6.440e+02 -6.460e+02 -6.470e+02 -6.510e+02 -6.520e+02 -6.550e+02\n",
      " -6.580e+02 -6.590e+02 -6.590e+02 -6.600e+02 -6.610e+02 -6.620e+02\n",
      " -6.640e+02 -6.660e+02 -6.660e+02 -6.720e+02 -6.750e+02 -6.750e+02\n",
      " -6.770e+02 -6.770e+02  6.780e+02 -6.790e+02 -6.790e+02 -6.810e+02\n",
      " -6.830e+02 -6.940e+02 -6.940e+02 -7.010e+02 -7.030e+02 -7.060e+02\n",
      " -7.100e+02 -7.140e+02 -7.140e+02 -7.150e+02 -7.180e+02 -7.220e+02\n",
      "  7.230e+02 -7.240e+02 -7.250e+02 -7.260e+02 -7.270e+02 -7.270e+02\n",
      " -7.280e+02 -7.340e+02 -7.380e+02 -7.420e+02 -7.450e+02 -7.470e+02\n",
      " -7.470e+02 -7.470e+02 -7.480e+02  7.490e+02 -7.520e+02 -7.520e+02\n",
      "  7.540e+02 -7.540e+02 -7.550e+02 -7.590e+02 -7.600e+02 -7.600e+02\n",
      " -7.610e+02 -7.620e+02 -7.620e+02  7.630e+02 -7.650e+02 -7.670e+02\n",
      " -7.670e+02 -7.690e+02 -7.830e+02 -7.850e+02  7.850e+02 -7.880e+02\n",
      " -7.890e+02 -7.910e+02  7.920e+02 -7.920e+02 -7.950e+02 -7.980e+02\n",
      " -8.030e+02  8.110e+02 -8.190e+02 -8.200e+02  8.210e+02 -8.220e+02\n",
      "  8.250e+02 -8.370e+02 -8.390e+02 -8.520e+02 -8.560e+02 -8.580e+02\n",
      " -8.600e+02  8.600e+02 -8.650e+02 -8.650e+02 -8.670e+02 -8.750e+02\n",
      " -8.780e+02 -8.780e+02  8.790e+02 -8.810e+02 -8.830e+02 -8.890e+02\n",
      " -8.900e+02 -8.990e+02  9.040e+02 -9.060e+02 -9.080e+02 -9.110e+02\n",
      "  9.120e+02 -9.120e+02 -9.180e+02  9.210e+02 -9.230e+02 -9.260e+02\n",
      " -9.310e+02 -9.420e+02 -9.430e+02 -9.430e+02 -9.430e+02 -9.520e+02\n",
      " -9.540e+02  9.590e+02 -9.650e+02 -9.650e+02  9.670e+02 -9.680e+02\n",
      " -9.720e+02 -9.730e+02 -9.740e+02 -9.750e+02 -9.750e+02  9.760e+02\n",
      " -9.840e+02 -9.870e+02 -9.900e+02 -9.960e+02 -9.960e+02 -9.970e+02\n",
      " -1.000e+03 -1.001e+03 -1.001e+03  1.004e+03 -1.004e+03 -1.004e+03\n",
      " -1.005e+03 -1.006e+03 -1.007e+03  1.009e+03 -1.010e+03 -1.013e+03\n",
      " -1.015e+03 -1.025e+03 -1.027e+03  1.032e+03  1.034e+03 -1.034e+03\n",
      " -1.035e+03 -1.042e+03 -1.043e+03 -1.043e+03 -1.043e+03  1.048e+03\n",
      " -1.051e+03 -1.051e+03 -1.051e+03 -1.062e+03 -1.062e+03 -1.063e+03\n",
      "  1.072e+03 -1.085e+03 -1.088e+03 -1.093e+03  1.093e+03 -1.099e+03\n",
      " -1.101e+03 -1.102e+03  1.104e+03 -1.112e+03 -1.119e+03 -1.120e+03\n",
      " -1.120e+03 -1.121e+03 -1.124e+03  1.127e+03 -1.132e+03 -1.138e+03\n",
      " -1.139e+03 -1.141e+03 -1.148e+03  1.148e+03 -1.148e+03 -1.150e+03\n",
      "  1.152e+03 -1.156e+03 -1.156e+03 -1.157e+03 -1.158e+03 -1.161e+03\n",
      " -1.162e+03 -1.163e+03 -1.165e+03 -1.167e+03  1.174e+03 -1.179e+03\n",
      " -1.185e+03 -1.186e+03 -1.189e+03 -1.191e+03 -1.196e+03 -1.203e+03\n",
      " -1.203e+03 -1.208e+03 -1.217e+03 -1.219e+03 -1.220e+03 -1.224e+03\n",
      " -1.229e+03 -1.229e+03 -1.233e+03 -1.234e+03 -1.234e+03 -1.239e+03\n",
      " -1.246e+03 -1.247e+03 -1.248e+03 -1.266e+03 -1.269e+03 -1.270e+03\n",
      "  1.272e+03  1.275e+03 -1.277e+03 -1.285e+03  1.286e+03 -1.288e+03\n",
      " -1.291e+03 -1.309e+03 -1.318e+03 -1.321e+03  1.324e+03 -1.325e+03\n",
      " -1.347e+03 -1.351e+03 -1.359e+03 -1.363e+03 -1.363e+03  1.365e+03\n",
      " -1.369e+03 -1.371e+03  1.388e+03  1.411e+03 -1.419e+03  1.430e+03\n",
      " -1.434e+03 -1.437e+03 -1.448e+03 -1.449e+03 -1.461e+03 -1.467e+03\n",
      " -1.471e+03 -1.474e+03 -1.476e+03 -1.477e+03 -1.492e+03 -1.505e+03\n",
      "  1.508e+03 -1.516e+03 -1.519e+03 -1.522e+03 -1.523e+03 -1.528e+03\n",
      " -1.534e+03 -1.535e+03  1.542e+03 -1.545e+03 -1.545e+03 -1.547e+03\n",
      " -1.548e+03 -1.561e+03  1.563e+03 -1.569e+03 -1.587e+03 -1.604e+03\n",
      " -1.604e+03 -1.611e+03 -1.611e+03 -1.611e+03 -1.612e+03 -1.613e+03\n",
      " -1.614e+03 -1.620e+03 -1.631e+03  1.642e+03 -1.642e+03 -1.644e+03\n",
      " -1.648e+03  1.649e+03 -1.662e+03 -1.673e+03  1.673e+03 -1.682e+03\n",
      " -1.686e+03  1.688e+03 -1.688e+03  1.692e+03  1.694e+03  1.699e+03\n",
      " -1.712e+03 -1.728e+03 -1.742e+03  1.759e+03 -1.780e+03  1.781e+03\n",
      " -1.783e+03  1.793e+03 -1.800e+03  1.812e+03 -1.820e+03 -1.836e+03\n",
      " -1.842e+03 -1.847e+03 -1.853e+03 -1.864e+03 -1.866e+03 -1.871e+03\n",
      " -1.876e+03  1.884e+03 -1.887e+03  1.900e+03 -1.914e+03 -1.919e+03\n",
      "  1.920e+03 -1.926e+03  1.927e+03 -1.928e+03 -1.935e+03 -1.953e+03\n",
      " -1.972e+03 -1.980e+03 -1.988e+03  2.009e+03 -2.012e+03 -2.019e+03\n",
      " -2.031e+03 -2.033e+03 -2.048e+03 -2.072e+03  2.097e+03 -2.109e+03\n",
      " -2.124e+03  2.127e+03 -2.128e+03 -2.136e+03 -2.140e+03 -2.142e+03\n",
      " -2.155e+03 -2.156e+03 -2.161e+03 -2.164e+03 -2.184e+03 -2.190e+03\n",
      " -2.197e+03  2.207e+03 -2.236e+03 -2.248e+03 -2.255e+03 -2.255e+03\n",
      " -2.255e+03 -2.263e+03  2.273e+03 -2.278e+03 -2.281e+03 -2.282e+03\n",
      " -2.288e+03 -2.289e+03  2.296e+03 -2.306e+03 -2.311e+03 -2.329e+03\n",
      " -2.330e+03 -2.335e+03  2.348e+03  2.361e+03 -2.365e+03 -2.372e+03\n",
      " -2.372e+03  2.373e+03 -2.381e+03 -2.385e+03 -2.406e+03  2.417e+03\n",
      " -2.426e+03 -2.442e+03 -2.442e+03  2.469e+03 -2.477e+03  2.483e+03\n",
      " -2.483e+03 -2.486e+03 -2.489e+03 -2.513e+03 -2.515e+03  2.520e+03\n",
      "  2.534e+03 -2.535e+03 -2.541e+03  2.551e+03 -2.558e+03 -2.559e+03\n",
      "  2.573e+03 -2.590e+03 -2.596e+03 -2.596e+03 -2.618e+03 -2.632e+03\n",
      " -2.645e+03 -2.645e+03 -2.650e+03 -2.653e+03 -2.654e+03 -2.695e+03\n",
      " -2.707e+03 -2.709e+03 -2.759e+03  2.763e+03 -2.770e+03 -2.813e+03\n",
      " -2.838e+03 -2.856e+03  2.866e+03  2.911e+03 -2.920e+03 -2.953e+03\n",
      "  2.965e+03  2.965e+03 -2.971e+03 -2.989e+03 -2.991e+03 -2.991e+03\n",
      " -3.001e+03 -3.004e+03 -3.009e+03 -3.011e+03 -3.015e+03 -3.017e+03\n",
      " -3.021e+03 -3.030e+03 -3.035e+03  3.063e+03 -3.072e+03 -3.088e+03\n",
      " -3.091e+03 -3.102e+03 -3.112e+03 -3.121e+03  3.126e+03 -3.128e+03\n",
      " -3.152e+03 -3.172e+03 -3.202e+03 -3.203e+03 -3.204e+03 -3.226e+03\n",
      " -3.247e+03 -3.256e+03  3.262e+03 -3.283e+03 -3.283e+03 -3.287e+03\n",
      " -3.307e+03 -3.316e+03 -3.342e+03 -3.364e+03  3.409e+03  3.418e+03\n",
      " -3.430e+03 -3.456e+03  3.461e+03  3.462e+03  3.472e+03 -3.506e+03\n",
      " -3.519e+03 -3.603e+03 -3.607e+03 -3.660e+03  3.669e+03 -3.709e+03\n",
      "  3.736e+03 -3.807e+03  3.873e+03  3.941e+03 -3.957e+03  3.959e+03\n",
      " -4.005e+03 -4.047e+03 -4.052e+03 -4.080e+03 -4.088e+03 -4.233e+03\n",
      "  4.267e+03 -4.285e+03 -4.354e+03 -4.361e+03  4.456e+03 -4.894e+03\n",
      " -4.929e+03 -5.042e+03 -5.062e+03 -5.156e+03 -5.176e+03 -6.292e+03\n",
      " -7.126e+03  7.455e+03 -7.777e+03 -8.008e+03 -8.391e+03 -8.556e+03\n",
      " -8.605e+03]\n",
      "durations 1.0 7106.0\n",
      "Concordance Index 0.5799930289299408\n",
      "Integrated Brier Score: 0.1804174047425288\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m66.5370\u001b[0m       \u001b[32m64.4246\u001b[0m  0.0644\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m76.2652\u001b[0m       \u001b[32m49.0435\u001b[0m  0.0678\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m155.5762\u001b[0m       \u001b[32m58.8139\u001b[0m  0.0507\n",
      "      2       \u001b[36m57.5256\u001b[0m       \u001b[32m60.1811\u001b[0m  0.0663\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m83.7746\u001b[0m       \u001b[32m89.3985\u001b[0m  0.0769\n",
      "      2      \u001b[36m149.5479\u001b[0m       \u001b[32m55.9437\u001b[0m  0.0359\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m71.3087\u001b[0m       \u001b[32m79.9014\u001b[0m  0.0704\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m161.1121\u001b[0m       \u001b[32m82.3043\u001b[0m  0.0505\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2       \u001b[36m67.2001\u001b[0m       \u001b[32m47.4849\u001b[0m  0.0553\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      3       \u001b[36m57.5228\u001b[0m       60.6062  0.0511\n",
      "      2       \u001b[36m74.1134\u001b[0m       \u001b[32m85.7196\u001b[0m  0.0595\n",
      "      3      \u001b[36m149.2958\u001b[0m       \u001b[32m55.6555\u001b[0m  0.0359\n",
      "      2      \u001b[36m152.8608\u001b[0m       \u001b[32m78.7366\u001b[0m  0.0426\n",
      "      2       \u001b[36m67.7481\u001b[0m       \u001b[32m72.2315\u001b[0m  0.0503\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m141.2457\u001b[0m       \u001b[32m80.3316\u001b[0m  0.0460\n",
      "      4      \u001b[36m146.7251\u001b[0m       56.1410  0.0343\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      3      154.3373       \u001b[32m78.5910\u001b[0m  0.0351\n",
      "      4       57.5422       60.5947  0.0619\n",
      "      3       77.4324       \u001b[32m83.9406\u001b[0m  0.0512\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "      3       \u001b[36m65.9169\u001b[0m       51.2085  0.0732\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m79.0797\u001b[0m       \u001b[32m89.1004\u001b[0m  0.0823\n",
      "      2      \u001b[36m135.4940\u001b[0m       \u001b[32m74.5370\u001b[0m  0.0324\n",
      "      5      \u001b[36m146.4232\u001b[0m       56.2406  0.0360\n",
      "      3       \u001b[36m65.1511\u001b[0m       \u001b[32m68.0209\u001b[0m  0.0614\n",
      "      4      \u001b[36m152.7585\u001b[0m       79.5540  0.0343\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m172.3228\u001b[0m      \u001b[32m115.7775\u001b[0m  0.0421\n",
      "      4       \u001b[36m72.8483\u001b[0m       86.2602  0.0493\n",
      "      3      \u001b[36m133.0452\u001b[0m       \u001b[32m73.2506\u001b[0m  0.0356\n",
      "      5       \u001b[36m56.0163\u001b[0m       62.9717  0.0548\n",
      "      6      148.1006       55.8613  0.0318\n",
      "      2       \u001b[36m69.2111\u001b[0m       \u001b[32m86.2854\u001b[0m  0.0507\n",
      "      4       \u001b[36m61.8120\u001b[0m       49.0117  0.0591\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m183.2904\u001b[0m      \u001b[32m111.7265\u001b[0m  0.0549\n",
      "      5      \u001b[36m150.9881\u001b[0m       80.6220  0.0357\n",
      "      2      \u001b[36m162.0589\u001b[0m      124.2266  0.0318\n",
      "      4       \u001b[36m64.4049\u001b[0m       68.3053  0.0440\n",
      "      4      133.6363       \u001b[32m72.3715\u001b[0m  0.0326\n",
      "      7      \u001b[36m145.8790\u001b[0m       \u001b[32m55.4943\u001b[0m  0.0312\n",
      "      6      \u001b[36m147.9922\u001b[0m       81.0098  0.0288\n",
      "      2      \u001b[36m176.3520\u001b[0m      \u001b[32m105.3485\u001b[0m  0.0363\n",
      "      3      163.3808      116.4760  0.0291\n",
      "      5       \u001b[36m71.5950\u001b[0m       90.8152  0.0513\n",
      "      6       \u001b[36m55.7775\u001b[0m       64.2837  0.0544\n",
      "      3       69.5912       \u001b[32m84.4109\u001b[0m  0.0480\n",
      "      8      \u001b[36m144.6054\u001b[0m       \u001b[32m55.3550\u001b[0m  0.0301\n",
      "      5       \u001b[36m63.5430\u001b[0m       71.1057  0.0451\n",
      "      5       63.3203       47.9710  0.0557\n",
      "      5      134.2793       \u001b[32m71.6301\u001b[0m  0.0413\n",
      "      7      148.9351       80.8413  0.0349\n",
      "      4      \u001b[36m161.5557\u001b[0m      \u001b[32m107.4818\u001b[0m  0.0302\n",
      "      3      177.8194      \u001b[32m104.4177\u001b[0m  0.0317\n",
      "      7       56.4200       62.1037  0.0432\n",
      "      6       \u001b[36m71.3562\u001b[0m       90.2481  0.0509\n",
      "      9      \u001b[36m143.5876\u001b[0m       \u001b[32m55.3518\u001b[0m  0.0358\n",
      "      6      \u001b[36m132.0520\u001b[0m       \u001b[32m71.2231\u001b[0m  0.0280\n",
      "      4       \u001b[36m68.2611\u001b[0m       85.4577  0.0485\n",
      "      4      \u001b[36m175.0053\u001b[0m      106.4291  0.0298\n",
      "      5      \u001b[36m157.9968\u001b[0m      \u001b[32m104.0650\u001b[0m  0.0320\n",
      "      8      148.8590       80.0856  0.0345\n",
      "      6       61.8482       \u001b[32m47.4669\u001b[0m  0.0474\n",
      "      6       65.3413       72.7607  0.0514\n",
      "     10      \u001b[36m142.7066\u001b[0m       55.4072  0.0312\n",
      "Restoring best model from epoch 8.\n",
      "      7      \u001b[36m130.3509\u001b[0m       \u001b[32m70.8437\u001b[0m  0.0355\n",
      "      5      \u001b[36m170.8857\u001b[0m      108.3725  0.0386\n",
      "      7       71.9188       86.9333  0.0530\n",
      "      6      \u001b[36m153.7933\u001b[0m      \u001b[32m102.1270\u001b[0m  0.0428\n",
      "      9      \u001b[36m146.4806\u001b[0m       79.2789  0.0428\n",
      "      8       55.8830       60.6134  0.0574\n",
      "      7       \u001b[36m62.9537\u001b[0m       72.2022  0.0450\n",
      "      5       \u001b[36m66.8504\u001b[0m       88.3184  0.0586\n",
      "      8      \u001b[36m129.9802\u001b[0m       \u001b[32m70.0015\u001b[0m  0.0348\n",
      "      7       63.2715       \u001b[32m47.0944\u001b[0m  0.0589\n",
      "      6      \u001b[36m168.8894\u001b[0m      107.8535  0.0390\n",
      "     10      148.6491       78.9717  0.0339\n",
      "Restoring best model from epoch 3.\n",
      "      7      157.5754      \u001b[32m100.9488\u001b[0m  0.0430\n",
      "      8       \u001b[36m67.3506\u001b[0m       85.9918  0.0495\n",
      "      9       \u001b[36m55.6770\u001b[0m       62.1277  0.0539\n",
      "      8       \u001b[36m62.8311\u001b[0m       71.4913  0.0479\n",
      "      6       \u001b[36m65.7217\u001b[0m       86.7520  0.0468\n",
      "      9      130.4984       \u001b[32m69.6519\u001b[0m  0.0392\n",
      "      7      170.1438      106.2990  0.0324\n",
      "      8      157.0296      101.0246  0.0299\n",
      "      8       \u001b[36m61.2757\u001b[0m       \u001b[32m46.9795\u001b[0m  0.0594\n",
      "      8      170.4300      \u001b[32m104.1863\u001b[0m  0.0277\n",
      "     10       \u001b[36m53.7904\u001b[0m       64.2409  0.0394\n",
      "Restoring best model from epoch 2.\n",
      "      9       71.1056       88.0743  0.0517\n",
      "     10      \u001b[36m128.4361\u001b[0m       \u001b[32m69.4727\u001b[0m  0.0391\n",
      "      7       \u001b[36m64.1182\u001b[0m       \u001b[32m83.9885\u001b[0m  0.0459\n",
      "      9      \u001b[36m151.8359\u001b[0m      102.9608  0.0405\n",
      "      9       \u001b[36m60.5320\u001b[0m       70.9204  0.0604\n",
      "      9       61.5042       \u001b[32m46.9790\u001b[0m  0.0412\n",
      "      9      \u001b[36m167.1893\u001b[0m      \u001b[32m103.4228\u001b[0m  0.0441\n",
      "     10       70.0514       91.0566  0.0404\n",
      "Restoring best model from epoch 3.\n",
      "     10      153.7516      105.4013  0.0309\n",
      "Restoring best model from epoch 7.\n",
      "      8       \u001b[36m61.6460\u001b[0m       \u001b[32m82.5281\u001b[0m  0.0402\n",
      "     10       61.6736       72.8763  0.0482\n",
      "Restoring best model from epoch 3.\n",
      "     10      167.4786      104.0366  0.0280\n",
      "Restoring best model from epoch 9.\n",
      "     10       62.4457       46.9969  0.0455\n",
      "Restoring best model from epoch 8.\n",
      "      9       62.4625       \u001b[32m82.3150\u001b[0m  0.0475\n",
      "     10       63.4393       84.4486  0.0516\n",
      "Restoring best model from epoch 9.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m171.7508\u001b[0m      \u001b[32m102.0168\u001b[0m  0.0309\n",
      "      2      \u001b[36m165.2764\u001b[0m       \u001b[32m97.8663\u001b[0m  0.0285\n",
      "      3      \u001b[36m159.9738\u001b[0m       \u001b[32m95.5876\u001b[0m  0.0287\n",
      "      4      164.5777       \u001b[32m92.4925\u001b[0m  0.0277\n",
      "      5      \u001b[36m158.8994\u001b[0m       \u001b[32m91.2114\u001b[0m  0.0282\n",
      "      6      \u001b[36m156.7537\u001b[0m       \u001b[32m90.9437\u001b[0m  0.0288\n",
      "      7      156.9638       \u001b[32m90.5337\u001b[0m  0.0278\n",
      "      8      \u001b[36m155.3533\u001b[0m       \u001b[32m89.4760\u001b[0m  0.0273\n",
      "      9      \u001b[36m152.8020\u001b[0m       \u001b[32m87.9256\u001b[0m  0.0311\n",
      "     10      156.7779       \u001b[32m87.0144\u001b[0m  0.0304\n",
      "y_train breslow final [-1.000e+00 -5.000e+00 -5.000e+00 -5.000e+00 -7.000e+00 -8.000e+00\n",
      " -9.000e+00 -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01\n",
      " -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01\n",
      " -1.000e+01 -1.000e+01 -1.600e+01 -1.900e+01 -2.400e+01 -3.000e+01\n",
      " -3.000e+01 -3.000e+01 -3.000e+01 -3.100e+01 -3.100e+01 -3.100e+01\n",
      " -3.100e+01 -3.400e+01 -4.000e+01 -4.900e+01 -5.100e+01 -5.400e+01\n",
      " -5.900e+01 -6.400e+01 -7.000e+01 -7.600e+01 -7.800e+01 -7.800e+01\n",
      " -7.800e+01 -8.000e+01 -8.400e+01 -9.000e+01 -9.200e+01  1.160e+02\n",
      " -1.180e+02 -1.490e+02  1.580e+02 -1.600e+02  1.600e+02 -1.620e+02\n",
      " -1.630e+02 -1.700e+02 -1.700e+02 -1.780e+02 -1.860e+02 -1.870e+02\n",
      " -1.960e+02  1.970e+02 -2.100e+02 -2.130e+02 -2.140e+02 -2.150e+02\n",
      " -2.160e+02 -2.170e+02 -2.180e+02 -2.220e+02  2.240e+02  2.270e+02\n",
      " -2.270e+02 -2.310e+02  2.390e+02 -2.420e+02 -2.420e+02 -2.500e+02\n",
      " -2.520e+02  2.550e+02 -2.660e+02 -2.710e+02 -2.730e+02 -2.730e+02\n",
      " -2.740e+02 -2.750e+02 -2.850e+02 -2.870e+02 -2.880e+02 -2.930e+02\n",
      " -2.930e+02  2.950e+02 -2.970e+02 -3.000e+02 -3.020e+02  3.020e+02\n",
      " -3.040e+02 -3.040e+02 -3.040e+02 -3.040e+02  3.040e+02 -3.070e+02\n",
      " -3.130e+02 -3.130e+02 -3.170e+02 -3.170e+02 -3.200e+02  3.220e+02\n",
      " -3.220e+02 -3.260e+02 -3.280e+02 -3.320e+02 -3.340e+02  3.360e+02\n",
      " -3.370e+02 -3.390e+02 -3.430e+02 -3.430e+02 -3.450e+02 -3.450e+02\n",
      " -3.470e+02  3.480e+02 -3.480e+02 -3.520e+02 -3.580e+02 -3.580e+02\n",
      " -3.580e+02  3.620e+02 -3.630e+02 -3.640e+02 -3.650e+02 -3.650e+02\n",
      " -3.650e+02  3.650e+02 -3.650e+02 -3.660e+02 -3.680e+02 -3.700e+02\n",
      " -3.710e+02 -3.730e+02 -3.730e+02 -3.750e+02 -3.750e+02 -3.750e+02\n",
      " -3.750e+02 -3.760e+02  3.770e+02 -3.800e+02 -3.810e+02 -3.810e+02\n",
      " -3.820e+02 -3.830e+02 -3.830e+02  3.850e+02 -3.850e+02 -3.850e+02\n",
      " -3.850e+02  3.850e+02 -3.850e+02 -3.920e+02 -3.930e+02 -3.930e+02\n",
      " -3.940e+02 -3.940e+02 -3.940e+02 -3.950e+02 -3.960e+02 -3.960e+02\n",
      " -3.960e+02 -3.960e+02 -3.960e+02 -3.970e+02 -3.980e+02 -4.020e+02\n",
      " -4.030e+02 -4.030e+02 -4.040e+02 -4.080e+02 -4.090e+02 -4.100e+02\n",
      " -4.100e+02 -4.110e+02 -4.120e+02 -4.140e+02 -4.160e+02 -4.170e+02\n",
      " -4.210e+02 -4.230e+02 -4.240e+02 -4.240e+02 -4.250e+02 -4.260e+02\n",
      "  4.260e+02 -4.260e+02 -4.280e+02 -4.280e+02 -4.300e+02 -4.310e+02\n",
      " -4.310e+02 -4.370e+02 -4.390e+02 -4.390e+02 -4.400e+02 -4.410e+02\n",
      " -4.410e+02 -4.460e+02 -4.470e+02 -4.480e+02 -4.500e+02 -4.500e+02\n",
      " -4.510e+02 -4.540e+02 -4.550e+02 -4.560e+02 -4.570e+02 -4.580e+02\n",
      " -4.610e+02 -4.610e+02 -4.630e+02 -4.630e+02 -4.660e+02 -4.670e+02\n",
      " -4.700e+02 -4.710e+02 -4.720e+02 -4.770e+02 -4.770e+02 -4.770e+02\n",
      " -4.800e+02 -4.820e+02 -4.880e+02 -4.890e+02 -4.920e+02 -4.950e+02\n",
      " -4.960e+02 -4.990e+02 -4.990e+02 -5.010e+02 -5.010e+02 -5.020e+02\n",
      " -5.030e+02 -5.040e+02 -5.040e+02 -5.060e+02 -5.080e+02 -5.090e+02\n",
      " -5.100e+02 -5.110e+02 -5.130e+02 -5.160e+02 -5.180e+02 -5.180e+02\n",
      " -5.190e+02 -5.190e+02 -5.230e+02  5.240e+02 -5.250e+02 -5.260e+02\n",
      " -5.280e+02 -5.280e+02 -5.300e+02 -5.310e+02 -5.320e+02 -5.320e+02\n",
      " -5.330e+02 -5.330e+02 -5.380e+02 -5.380e+02  5.380e+02 -5.410e+02\n",
      " -5.410e+02 -5.450e+02 -5.460e+02 -5.490e+02 -5.510e+02 -5.520e+02\n",
      " -5.520e+02 -5.520e+02 -5.540e+02 -5.540e+02 -5.540e+02 -5.540e+02\n",
      " -5.540e+02  5.580e+02 -5.620e+02 -5.620e+02 -5.630e+02  5.630e+02\n",
      " -5.650e+02 -5.660e+02 -5.670e+02 -5.680e+02  5.710e+02  5.730e+02\n",
      " -5.740e+02 -5.740e+02 -5.750e+02 -5.750e+02 -5.750e+02 -5.750e+02\n",
      " -5.760e+02 -5.770e+02 -5.770e+02 -5.840e+02 -5.840e+02 -5.840e+02\n",
      " -5.850e+02 -5.860e+02 -5.880e+02 -5.880e+02 -5.880e+02 -5.910e+02\n",
      " -5.940e+02 -5.950e+02 -5.960e+02 -6.000e+02 -6.020e+02 -6.060e+02\n",
      " -6.070e+02 -6.070e+02 -6.080e+02 -6.080e+02 -6.110e+02 -6.110e+02\n",
      " -6.120e+02  6.120e+02 -6.120e+02 -6.120e+02 -6.140e+02  6.140e+02\n",
      "  6.160e+02 -6.160e+02 -6.160e+02 -6.180e+02 -6.200e+02 -6.200e+02\n",
      " -6.200e+02 -6.220e+02 -6.240e+02 -6.270e+02 -6.290e+02 -6.300e+02\n",
      " -6.310e+02 -6.350e+02 -6.350e+02 -6.350e+02  6.390e+02 -6.400e+02\n",
      " -6.400e+02 -6.410e+02 -6.430e+02 -6.430e+02 -6.440e+02 -6.460e+02\n",
      " -6.470e+02 -6.480e+02 -6.510e+02 -6.520e+02 -6.550e+02 -6.580e+02\n",
      " -6.620e+02 -6.660e+02 -6.660e+02 -6.720e+02 -6.750e+02 -6.750e+02\n",
      " -6.770e+02 -6.770e+02 -6.790e+02 -6.800e+02 -6.830e+02 -6.940e+02\n",
      " -6.940e+02 -6.980e+02 -7.020e+02 -7.030e+02 -7.070e+02 -7.070e+02\n",
      " -7.090e+02 -7.100e+02 -7.140e+02 -7.140e+02 -7.150e+02 -7.150e+02\n",
      " -7.180e+02 -7.220e+02  7.230e+02 -7.240e+02 -7.250e+02 -7.270e+02\n",
      " -7.270e+02 -7.270e+02 -7.280e+02 -7.340e+02 -7.420e+02 -7.450e+02\n",
      " -7.470e+02 -7.470e+02 -7.470e+02 -7.480e+02  7.490e+02 -7.520e+02\n",
      " -7.520e+02  7.540e+02 -7.540e+02 -7.590e+02 -7.590e+02 -7.600e+02\n",
      " -7.610e+02 -7.620e+02 -7.620e+02 -7.620e+02  7.630e+02 -7.650e+02\n",
      " -7.670e+02 -7.670e+02 -7.690e+02 -7.770e+02 -7.850e+02  7.860e+02\n",
      " -7.880e+02 -7.910e+02  7.920e+02  7.920e+02 -7.950e+02 -7.980e+02\n",
      " -8.030e+02 -8.060e+02  8.110e+02 -8.120e+02 -8.190e+02 -8.200e+02\n",
      "  8.210e+02 -8.220e+02 -8.290e+02 -8.370e+02 -8.390e+02 -8.470e+02\n",
      " -8.490e+02 -8.520e+02 -8.560e+02 -8.600e+02  8.600e+02 -8.650e+02\n",
      " -8.670e+02 -8.750e+02 -8.780e+02  8.790e+02  8.830e+02 -8.890e+02\n",
      " -8.900e+02 -8.990e+02 -9.060e+02 -9.070e+02 -9.080e+02 -9.110e+02\n",
      "  9.120e+02 -9.120e+02 -9.120e+02 -9.150e+02 -9.180e+02 -9.230e+02\n",
      " -9.260e+02 -9.310e+02 -9.310e+02 -9.430e+02  9.430e+02 -9.430e+02\n",
      " -9.430e+02 -9.520e+02 -9.540e+02  9.590e+02  9.670e+02 -9.720e+02\n",
      " -9.730e+02 -9.740e+02 -9.740e+02 -9.750e+02  9.760e+02 -9.840e+02\n",
      " -9.870e+02 -9.890e+02 -9.900e+02  9.910e+02  9.910e+02 -9.960e+02\n",
      " -9.970e+02 -1.000e+03 -1.001e+03 -1.001e+03 -1.001e+03  1.004e+03\n",
      " -1.004e+03 -1.004e+03 -1.006e+03 -1.007e+03 -1.009e+03 -1.010e+03\n",
      " -1.013e+03 -1.015e+03 -1.026e+03 -1.026e+03  1.032e+03  1.034e+03\n",
      "  1.034e+03 -1.034e+03 -1.035e+03 -1.039e+03 -1.042e+03 -1.043e+03\n",
      " -1.043e+03 -1.043e+03 -1.047e+03  1.048e+03 -1.051e+03 -1.051e+03\n",
      " -1.059e+03 -1.062e+03 -1.062e+03 -1.066e+03 -1.074e+03 -1.079e+03\n",
      " -1.088e+03  1.093e+03 -1.099e+03 -1.102e+03  1.104e+03 -1.106e+03\n",
      " -1.112e+03 -1.119e+03 -1.120e+03 -1.120e+03  1.127e+03 -1.132e+03\n",
      " -1.133e+03 -1.138e+03 -1.139e+03 -1.140e+03 -1.141e+03  1.142e+03\n",
      " -1.148e+03  1.148e+03 -1.150e+03  1.152e+03 -1.156e+03 -1.157e+03\n",
      " -1.158e+03 -1.162e+03 -1.163e+03 -1.174e+03  1.174e+03 -1.179e+03\n",
      " -1.189e+03 -1.198e+03 -1.203e+03 -1.206e+03 -1.208e+03 -1.208e+03\n",
      " -1.210e+03 -1.219e+03 -1.220e+03 -1.220e+03 -1.224e+03 -1.229e+03\n",
      " -1.229e+03 -1.232e+03 -1.233e+03 -1.234e+03 -1.234e+03 -1.239e+03\n",
      " -1.247e+03 -1.251e+03 -1.266e+03 -1.270e+03  1.272e+03  1.275e+03\n",
      " -1.277e+03 -1.285e+03  1.286e+03 -1.288e+03 -1.301e+03 -1.308e+03\n",
      " -1.308e+03 -1.308e+03 -1.309e+03 -1.309e+03 -1.318e+03 -1.321e+03\n",
      "  1.324e+03 -1.325e+03 -1.326e+03 -1.330e+03 -1.351e+03 -1.363e+03\n",
      " -1.363e+03  1.365e+03 -1.369e+03 -1.371e+03  1.411e+03 -1.417e+03\n",
      " -1.417e+03 -1.419e+03  1.430e+03 -1.434e+03 -1.437e+03  1.439e+03\n",
      " -1.448e+03 -1.449e+03 -1.463e+03 -1.467e+03 -1.474e+03 -1.474e+03\n",
      " -1.476e+03 -1.477e+03 -1.492e+03 -1.505e+03  1.508e+03 -1.516e+03\n",
      " -1.519e+03 -1.522e+03 -1.523e+03 -1.528e+03 -1.532e+03 -1.535e+03\n",
      "  1.542e+03 -1.542e+03 -1.545e+03 -1.546e+03 -1.548e+03 -1.550e+03\n",
      "  1.556e+03  1.563e+03 -1.563e+03 -1.572e+03 -1.596e+03 -1.604e+03\n",
      " -1.611e+03 -1.611e+03 -1.612e+03 -1.613e+03 -1.614e+03 -1.616e+03\n",
      " -1.620e+03 -1.639e+03 -1.640e+03  1.642e+03 -1.642e+03 -1.662e+03\n",
      " -1.673e+03 -1.682e+03 -1.683e+03 -1.686e+03  1.688e+03  1.692e+03\n",
      " -1.692e+03  1.694e+03  1.699e+03 -1.712e+03 -1.728e+03 -1.728e+03\n",
      " -1.732e+03 -1.742e+03  1.759e+03 -1.780e+03  1.781e+03 -1.783e+03\n",
      "  1.793e+03 -1.800e+03  1.812e+03 -1.842e+03 -1.847e+03 -1.853e+03\n",
      " -1.855e+03 -1.873e+03 -1.882e+03  1.884e+03  1.900e+03 -1.914e+03\n",
      "  1.920e+03 -1.925e+03 -1.926e+03  1.927e+03 -1.928e+03 -1.935e+03\n",
      " -1.935e+03 -1.953e+03 -1.972e+03 -1.980e+03 -1.988e+03  1.993e+03\n",
      " -1.996e+03  2.009e+03 -2.041e+03 -2.048e+03 -2.064e+03 -2.072e+03\n",
      " -2.082e+03 -2.091e+03  2.097e+03 -2.108e+03 -2.109e+03 -2.124e+03\n",
      "  2.127e+03 -2.136e+03 -2.140e+03 -2.142e+03 -2.161e+03 -2.164e+03\n",
      " -2.190e+03 -2.191e+03  2.192e+03 -2.193e+03 -2.197e+03 -2.222e+03\n",
      " -2.231e+03 -2.236e+03 -2.240e+03 -2.246e+03 -2.248e+03 -2.255e+03\n",
      " -2.255e+03 -2.255e+03  2.273e+03 -2.278e+03 -2.281e+03 -2.282e+03\n",
      " -2.289e+03  2.296e+03 -2.306e+03 -2.329e+03 -2.330e+03 -2.335e+03\n",
      "  2.348e+03 -2.362e+03 -2.365e+03 -2.371e+03 -2.372e+03 -2.372e+03\n",
      "  2.373e+03 -2.381e+03 -2.385e+03 -2.403e+03 -2.406e+03  2.417e+03\n",
      " -2.442e+03 -2.442e+03 -2.483e+03 -2.486e+03 -2.489e+03 -2.513e+03\n",
      " -2.515e+03  2.534e+03 -2.535e+03 -2.558e+03  2.573e+03 -2.590e+03\n",
      " -2.596e+03 -2.605e+03 -2.612e+03 -2.618e+03 -2.629e+03 -2.632e+03\n",
      " -2.632e+03  2.636e+03 -2.645e+03 -2.645e+03 -2.654e+03 -2.695e+03\n",
      " -2.707e+03 -2.709e+03  2.712e+03 -2.721e+03 -2.759e+03 -2.767e+03\n",
      " -2.770e+03  2.798e+03  2.854e+03 -2.856e+03 -2.868e+03  2.911e+03\n",
      " -2.920e+03 -2.953e+03 -2.976e+03 -2.989e+03 -2.991e+03 -2.991e+03\n",
      " -3.004e+03 -3.009e+03 -3.011e+03 -3.015e+03 -3.017e+03 -3.021e+03\n",
      " -3.022e+03 -3.030e+03  3.063e+03 -3.072e+03 -3.088e+03 -3.091e+03\n",
      " -3.094e+03 -3.102e+03 -3.112e+03 -3.121e+03  3.126e+03 -3.159e+03\n",
      " -3.202e+03 -3.204e+03 -3.247e+03 -3.248e+03 -3.261e+03  3.262e+03\n",
      " -3.276e+03 -3.283e+03 -3.286e+03 -3.287e+03 -3.296e+03 -3.307e+03\n",
      " -3.316e+03 -3.342e+03 -3.361e+03  3.409e+03  3.418e+03 -3.456e+03\n",
      "  3.472e+03  3.492e+03 -3.506e+03 -3.519e+03 -3.603e+03 -3.607e+03\n",
      " -3.660e+03 -3.709e+03  3.736e+03 -3.807e+03  3.873e+03  3.926e+03\n",
      "  3.941e+03  3.945e+03  3.959e+03 -4.005e+03 -4.047e+03 -4.052e+03\n",
      " -4.088e+03 -4.159e+03 -4.233e+03  4.267e+03 -4.275e+03 -4.354e+03\n",
      " -4.361e+03  4.456e+03 -4.894e+03 -4.929e+03 -5.042e+03 -5.062e+03\n",
      " -5.156e+03 -5.176e+03 -5.739e+03 -6.292e+03  6.456e+03  6.593e+03\n",
      " -7.106e+03 -7.126e+03  7.455e+03 -7.777e+03 -8.008e+03 -8.556e+03\n",
      " -8.605e+03]\n",
      "Concordance Index 0.6724017614360999\n",
      "Integrated Brier Score: 0.1929104959749627\n",
      "y_train breslow final [-1.000e+00 -5.000e+00 -5.000e+00 -5.000e+00 -7.000e+00 -8.000e+00\n",
      " -9.000e+00 -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01\n",
      " -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01\n",
      " -1.000e+01 -1.000e+01 -1.600e+01 -1.900e+01 -2.400e+01 -3.000e+01\n",
      " -3.000e+01 -3.000e+01 -3.000e+01 -3.100e+01 -3.100e+01 -3.100e+01\n",
      " -3.100e+01 -3.400e+01 -4.000e+01 -4.900e+01 -5.100e+01 -5.400e+01\n",
      " -5.900e+01 -6.400e+01 -7.000e+01 -7.600e+01 -7.800e+01 -7.800e+01\n",
      " -7.800e+01 -8.000e+01 -8.400e+01 -9.000e+01 -9.200e+01  1.160e+02\n",
      " -1.180e+02 -1.490e+02  1.580e+02 -1.600e+02  1.600e+02 -1.620e+02\n",
      " -1.630e+02 -1.700e+02 -1.700e+02 -1.780e+02 -1.860e+02 -1.870e+02\n",
      " -1.960e+02  1.970e+02 -2.100e+02 -2.130e+02 -2.140e+02 -2.150e+02\n",
      " -2.160e+02 -2.170e+02 -2.180e+02 -2.220e+02  2.240e+02  2.270e+02\n",
      " -2.270e+02 -2.310e+02  2.390e+02 -2.420e+02 -2.420e+02 -2.500e+02\n",
      " -2.520e+02  2.550e+02 -2.660e+02 -2.710e+02 -2.730e+02 -2.730e+02\n",
      " -2.740e+02 -2.750e+02 -2.850e+02 -2.870e+02 -2.880e+02 -2.930e+02\n",
      " -2.930e+02  2.950e+02 -2.970e+02 -3.000e+02 -3.020e+02  3.020e+02\n",
      " -3.040e+02 -3.040e+02 -3.040e+02 -3.040e+02  3.040e+02 -3.070e+02\n",
      " -3.130e+02 -3.130e+02 -3.170e+02 -3.170e+02 -3.200e+02  3.220e+02\n",
      " -3.220e+02 -3.260e+02 -3.280e+02 -3.320e+02 -3.340e+02  3.360e+02\n",
      " -3.370e+02 -3.390e+02 -3.430e+02 -3.430e+02 -3.450e+02 -3.450e+02\n",
      " -3.470e+02  3.480e+02 -3.480e+02 -3.520e+02 -3.580e+02 -3.580e+02\n",
      " -3.580e+02  3.620e+02 -3.630e+02 -3.640e+02 -3.650e+02 -3.650e+02\n",
      " -3.650e+02  3.650e+02 -3.650e+02 -3.660e+02 -3.680e+02 -3.700e+02\n",
      " -3.710e+02 -3.730e+02 -3.730e+02 -3.750e+02 -3.750e+02 -3.750e+02\n",
      " -3.750e+02 -3.760e+02  3.770e+02 -3.800e+02 -3.810e+02 -3.810e+02\n",
      " -3.820e+02 -3.830e+02 -3.830e+02  3.850e+02 -3.850e+02 -3.850e+02\n",
      " -3.850e+02  3.850e+02 -3.850e+02 -3.920e+02 -3.930e+02 -3.930e+02\n",
      " -3.940e+02 -3.940e+02 -3.940e+02 -3.950e+02 -3.960e+02 -3.960e+02\n",
      " -3.960e+02 -3.960e+02 -3.960e+02 -3.970e+02 -3.980e+02 -4.020e+02\n",
      " -4.030e+02 -4.030e+02 -4.040e+02 -4.080e+02 -4.090e+02 -4.100e+02\n",
      " -4.100e+02 -4.110e+02 -4.120e+02 -4.140e+02 -4.160e+02 -4.170e+02\n",
      " -4.210e+02 -4.230e+02 -4.240e+02 -4.240e+02 -4.250e+02 -4.260e+02\n",
      "  4.260e+02 -4.260e+02 -4.280e+02 -4.280e+02 -4.300e+02 -4.310e+02\n",
      " -4.310e+02 -4.370e+02 -4.390e+02 -4.390e+02 -4.400e+02 -4.410e+02\n",
      " -4.410e+02 -4.460e+02 -4.470e+02 -4.480e+02 -4.500e+02 -4.500e+02\n",
      " -4.510e+02 -4.540e+02 -4.550e+02 -4.560e+02 -4.570e+02 -4.580e+02\n",
      " -4.610e+02 -4.610e+02 -4.630e+02 -4.630e+02 -4.660e+02 -4.670e+02\n",
      " -4.700e+02 -4.710e+02 -4.720e+02 -4.770e+02 -4.770e+02 -4.770e+02\n",
      " -4.800e+02 -4.820e+02 -4.880e+02 -4.890e+02 -4.920e+02 -4.950e+02\n",
      " -4.960e+02 -4.990e+02 -4.990e+02 -5.010e+02 -5.010e+02 -5.020e+02\n",
      " -5.030e+02 -5.040e+02 -5.040e+02 -5.060e+02 -5.080e+02 -5.090e+02\n",
      " -5.100e+02 -5.110e+02 -5.130e+02 -5.160e+02 -5.180e+02 -5.180e+02\n",
      " -5.190e+02 -5.190e+02 -5.230e+02  5.240e+02 -5.250e+02 -5.260e+02\n",
      " -5.280e+02 -5.280e+02 -5.300e+02 -5.310e+02 -5.320e+02 -5.320e+02\n",
      " -5.330e+02 -5.330e+02 -5.380e+02 -5.380e+02  5.380e+02 -5.410e+02\n",
      " -5.410e+02 -5.450e+02 -5.460e+02 -5.490e+02 -5.510e+02 -5.520e+02\n",
      " -5.520e+02 -5.520e+02 -5.540e+02 -5.540e+02 -5.540e+02 -5.540e+02\n",
      " -5.540e+02  5.580e+02 -5.620e+02 -5.620e+02 -5.630e+02  5.630e+02\n",
      " -5.650e+02 -5.660e+02 -5.670e+02 -5.680e+02  5.710e+02  5.730e+02\n",
      " -5.740e+02 -5.740e+02 -5.750e+02 -5.750e+02 -5.750e+02 -5.750e+02\n",
      " -5.760e+02 -5.770e+02 -5.770e+02 -5.840e+02 -5.840e+02 -5.840e+02\n",
      " -5.850e+02 -5.860e+02 -5.880e+02 -5.880e+02 -5.880e+02 -5.910e+02\n",
      " -5.940e+02 -5.950e+02 -5.960e+02 -6.000e+02 -6.020e+02 -6.060e+02\n",
      " -6.070e+02 -6.070e+02 -6.080e+02 -6.080e+02 -6.110e+02 -6.110e+02\n",
      " -6.120e+02  6.120e+02 -6.120e+02 -6.120e+02 -6.140e+02  6.140e+02\n",
      "  6.160e+02 -6.160e+02 -6.160e+02 -6.180e+02 -6.200e+02 -6.200e+02\n",
      " -6.200e+02 -6.220e+02 -6.240e+02 -6.270e+02 -6.290e+02 -6.300e+02\n",
      " -6.310e+02 -6.350e+02 -6.350e+02 -6.350e+02  6.390e+02 -6.400e+02\n",
      " -6.400e+02 -6.410e+02 -6.430e+02 -6.430e+02 -6.440e+02 -6.460e+02\n",
      " -6.470e+02 -6.480e+02 -6.510e+02 -6.520e+02 -6.550e+02 -6.580e+02\n",
      " -6.620e+02 -6.660e+02 -6.660e+02 -6.720e+02 -6.750e+02 -6.750e+02\n",
      " -6.770e+02 -6.770e+02 -6.790e+02 -6.800e+02 -6.830e+02 -6.940e+02\n",
      " -6.940e+02 -6.980e+02 -7.020e+02 -7.030e+02 -7.070e+02 -7.070e+02\n",
      " -7.090e+02 -7.100e+02 -7.140e+02 -7.140e+02 -7.150e+02 -7.150e+02\n",
      " -7.180e+02 -7.220e+02  7.230e+02 -7.240e+02 -7.250e+02 -7.270e+02\n",
      " -7.270e+02 -7.270e+02 -7.280e+02 -7.340e+02 -7.420e+02 -7.450e+02\n",
      " -7.470e+02 -7.470e+02 -7.470e+02 -7.480e+02  7.490e+02 -7.520e+02\n",
      " -7.520e+02  7.540e+02 -7.540e+02 -7.590e+02 -7.590e+02 -7.600e+02\n",
      " -7.610e+02 -7.620e+02 -7.620e+02 -7.620e+02  7.630e+02 -7.650e+02\n",
      " -7.670e+02 -7.670e+02 -7.690e+02 -7.770e+02 -7.850e+02  7.860e+02\n",
      " -7.880e+02 -7.910e+02  7.920e+02  7.920e+02 -7.950e+02 -7.980e+02\n",
      " -8.030e+02 -8.060e+02  8.110e+02 -8.120e+02 -8.190e+02 -8.200e+02\n",
      "  8.210e+02 -8.220e+02 -8.290e+02 -8.370e+02 -8.390e+02 -8.470e+02\n",
      " -8.490e+02 -8.520e+02 -8.560e+02 -8.600e+02  8.600e+02 -8.650e+02\n",
      " -8.670e+02 -8.750e+02 -8.780e+02  8.790e+02  8.830e+02 -8.890e+02\n",
      " -8.900e+02 -8.990e+02 -9.060e+02 -9.070e+02 -9.080e+02 -9.110e+02\n",
      "  9.120e+02 -9.120e+02 -9.120e+02 -9.150e+02 -9.180e+02 -9.230e+02\n",
      " -9.260e+02 -9.310e+02 -9.310e+02 -9.430e+02  9.430e+02 -9.430e+02\n",
      " -9.430e+02 -9.520e+02 -9.540e+02  9.590e+02  9.670e+02 -9.720e+02\n",
      " -9.730e+02 -9.740e+02 -9.740e+02 -9.750e+02  9.760e+02 -9.840e+02\n",
      " -9.870e+02 -9.890e+02 -9.900e+02  9.910e+02  9.910e+02 -9.960e+02\n",
      " -9.970e+02 -1.000e+03 -1.001e+03 -1.001e+03 -1.001e+03  1.004e+03\n",
      " -1.004e+03 -1.004e+03 -1.006e+03 -1.007e+03 -1.009e+03 -1.010e+03\n",
      " -1.013e+03 -1.015e+03 -1.026e+03 -1.026e+03  1.032e+03  1.034e+03\n",
      "  1.034e+03 -1.034e+03 -1.035e+03 -1.039e+03 -1.042e+03 -1.043e+03\n",
      " -1.043e+03 -1.043e+03 -1.047e+03  1.048e+03 -1.051e+03 -1.051e+03\n",
      " -1.059e+03 -1.062e+03 -1.062e+03 -1.066e+03 -1.074e+03 -1.079e+03\n",
      " -1.088e+03  1.093e+03 -1.099e+03 -1.102e+03  1.104e+03 -1.106e+03\n",
      " -1.112e+03 -1.119e+03 -1.120e+03 -1.120e+03  1.127e+03 -1.132e+03\n",
      " -1.133e+03 -1.138e+03 -1.139e+03 -1.140e+03 -1.141e+03  1.142e+03\n",
      " -1.148e+03  1.148e+03 -1.150e+03  1.152e+03 -1.156e+03 -1.157e+03\n",
      " -1.158e+03 -1.162e+03 -1.163e+03 -1.174e+03  1.174e+03 -1.179e+03\n",
      " -1.189e+03 -1.198e+03 -1.203e+03 -1.206e+03 -1.208e+03 -1.208e+03\n",
      " -1.210e+03 -1.219e+03 -1.220e+03 -1.220e+03 -1.224e+03 -1.229e+03\n",
      " -1.229e+03 -1.232e+03 -1.233e+03 -1.234e+03 -1.234e+03 -1.239e+03\n",
      " -1.247e+03 -1.251e+03 -1.266e+03 -1.270e+03  1.272e+03  1.275e+03\n",
      " -1.277e+03 -1.285e+03  1.286e+03 -1.288e+03 -1.301e+03 -1.308e+03\n",
      " -1.308e+03 -1.308e+03 -1.309e+03 -1.309e+03 -1.318e+03 -1.321e+03\n",
      "  1.324e+03 -1.325e+03 -1.326e+03 -1.330e+03 -1.351e+03 -1.363e+03\n",
      " -1.363e+03  1.365e+03 -1.369e+03 -1.371e+03  1.411e+03 -1.417e+03\n",
      " -1.417e+03 -1.419e+03  1.430e+03 -1.434e+03 -1.437e+03  1.439e+03\n",
      " -1.448e+03 -1.449e+03 -1.463e+03 -1.467e+03 -1.474e+03 -1.474e+03\n",
      " -1.476e+03 -1.477e+03 -1.492e+03 -1.505e+03  1.508e+03 -1.516e+03\n",
      " -1.519e+03 -1.522e+03 -1.523e+03 -1.528e+03 -1.532e+03 -1.535e+03\n",
      "  1.542e+03 -1.542e+03 -1.545e+03 -1.546e+03 -1.548e+03 -1.550e+03\n",
      "  1.556e+03  1.563e+03 -1.563e+03 -1.572e+03 -1.596e+03 -1.604e+03\n",
      " -1.611e+03 -1.611e+03 -1.612e+03 -1.613e+03 -1.614e+03 -1.616e+03\n",
      " -1.620e+03 -1.639e+03 -1.640e+03  1.642e+03 -1.642e+03 -1.662e+03\n",
      " -1.673e+03 -1.682e+03 -1.683e+03 -1.686e+03  1.688e+03  1.692e+03\n",
      " -1.692e+03  1.694e+03  1.699e+03 -1.712e+03 -1.728e+03 -1.728e+03\n",
      " -1.732e+03 -1.742e+03  1.759e+03 -1.780e+03  1.781e+03 -1.783e+03\n",
      "  1.793e+03 -1.800e+03  1.812e+03 -1.842e+03 -1.847e+03 -1.853e+03\n",
      " -1.855e+03 -1.873e+03 -1.882e+03  1.884e+03  1.900e+03 -1.914e+03\n",
      "  1.920e+03 -1.925e+03 -1.926e+03  1.927e+03 -1.928e+03 -1.935e+03\n",
      " -1.935e+03 -1.953e+03 -1.972e+03 -1.980e+03 -1.988e+03  1.993e+03\n",
      " -1.996e+03  2.009e+03 -2.041e+03 -2.048e+03 -2.064e+03 -2.072e+03\n",
      " -2.082e+03 -2.091e+03  2.097e+03 -2.108e+03 -2.109e+03 -2.124e+03\n",
      "  2.127e+03 -2.136e+03 -2.140e+03 -2.142e+03 -2.161e+03 -2.164e+03\n",
      " -2.190e+03 -2.191e+03  2.192e+03 -2.193e+03 -2.197e+03 -2.222e+03\n",
      " -2.231e+03 -2.236e+03 -2.240e+03 -2.246e+03 -2.248e+03 -2.255e+03\n",
      " -2.255e+03 -2.255e+03  2.273e+03 -2.278e+03 -2.281e+03 -2.282e+03\n",
      " -2.289e+03  2.296e+03 -2.306e+03 -2.329e+03 -2.330e+03 -2.335e+03\n",
      "  2.348e+03 -2.362e+03 -2.365e+03 -2.371e+03 -2.372e+03 -2.372e+03\n",
      "  2.373e+03 -2.381e+03 -2.385e+03 -2.403e+03 -2.406e+03  2.417e+03\n",
      " -2.442e+03 -2.442e+03 -2.483e+03 -2.486e+03 -2.489e+03 -2.513e+03\n",
      " -2.515e+03  2.534e+03 -2.535e+03 -2.558e+03  2.573e+03 -2.590e+03\n",
      " -2.596e+03 -2.605e+03 -2.612e+03 -2.618e+03 -2.629e+03 -2.632e+03\n",
      " -2.632e+03  2.636e+03 -2.645e+03 -2.645e+03 -2.654e+03 -2.695e+03\n",
      " -2.707e+03 -2.709e+03  2.712e+03 -2.721e+03 -2.759e+03 -2.767e+03\n",
      " -2.770e+03  2.798e+03  2.854e+03 -2.856e+03 -2.868e+03  2.911e+03\n",
      " -2.920e+03 -2.953e+03 -2.976e+03 -2.989e+03 -2.991e+03 -2.991e+03\n",
      " -3.004e+03 -3.009e+03 -3.011e+03 -3.015e+03 -3.017e+03 -3.021e+03\n",
      " -3.022e+03 -3.030e+03  3.063e+03 -3.072e+03 -3.088e+03 -3.091e+03\n",
      " -3.094e+03 -3.102e+03 -3.112e+03 -3.121e+03  3.126e+03 -3.159e+03\n",
      " -3.202e+03 -3.204e+03 -3.247e+03 -3.248e+03 -3.261e+03  3.262e+03\n",
      " -3.276e+03 -3.283e+03 -3.286e+03 -3.287e+03 -3.296e+03 -3.307e+03\n",
      " -3.316e+03 -3.342e+03 -3.361e+03  3.409e+03  3.418e+03 -3.456e+03\n",
      "  3.472e+03  3.492e+03 -3.506e+03 -3.519e+03 -3.603e+03 -3.607e+03\n",
      " -3.660e+03 -3.709e+03  3.736e+03 -3.807e+03  3.873e+03  3.926e+03\n",
      "  3.941e+03  3.945e+03  3.959e+03 -4.005e+03 -4.047e+03 -4.052e+03\n",
      " -4.088e+03 -4.159e+03 -4.233e+03  4.267e+03 -4.275e+03 -4.354e+03\n",
      " -4.361e+03  4.456e+03 -4.894e+03 -4.929e+03 -5.042e+03 -5.062e+03\n",
      " -5.156e+03 -5.176e+03 -5.739e+03 -6.292e+03  6.456e+03  6.593e+03\n",
      " -7.106e+03 -7.126e+03  7.455e+03 -7.777e+03 -8.008e+03 -8.556e+03\n",
      " -8.605e+03]\n",
      "durations 1.0 8391.0\n",
      "Concordance Index 0.5435717076983196\n",
      "Integrated Brier Score: 0.21926749974658102\n",
      "split gex_?|100130426      float32\n",
      "gex_?|100133144      float32\n",
      "gex_?|100134869      float32\n",
      "gex_?|10357          float32\n",
      "gex_?|10431          float32\n",
      "                      ...   \n",
      "gex_ZYG11A|440590    float32\n",
      "gex_ZYG11B|79699     float32\n",
      "gex_ZYX|7791         float32\n",
      "gex_ZZEF1|23140      float32\n",
      "gex_ZZZ3|26009       float32\n",
      "Length: 20531, dtype: object\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m497.8682\u001b[0m      \u001b[32m101.3600\u001b[0m  0.0219\n",
      "      2      \u001b[36m462.6114\u001b[0m       \u001b[32m97.7380\u001b[0m  0.0252\n",
      "      3      \u001b[36m459.8615\u001b[0m       \u001b[32m96.5778\u001b[0m  0.0196\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      4      462.7873       \u001b[32m95.7333\u001b[0m  0.0222\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      5      \u001b[36m459.2295\u001b[0m       \u001b[32m95.6427\u001b[0m  0.0279\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m442.8130\u001b[0m       \u001b[32m71.9221\u001b[0m  0.0553\n",
      "      6      \u001b[36m455.0723\u001b[0m       96.0330  0.0299\n",
      "      2      \u001b[36m412.0978\u001b[0m       \u001b[32m65.2997\u001b[0m  0.0341\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      7      \u001b[36m452.2943\u001b[0m       96.5026  0.0327\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m226.0866\u001b[0m      \u001b[32m134.0328\u001b[0m  0.0707\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m198.0711\u001b[0m       \u001b[32m67.3614\u001b[0m  0.0548\n",
      "      3      \u001b[36m407.5418\u001b[0m       \u001b[32m63.3251\u001b[0m  0.0214\n",
      "      8      \u001b[36m451.2580\u001b[0m       96.6615  0.0217\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m533.9033\u001b[0m      \u001b[32m105.3906\u001b[0m  0.0227\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2      \u001b[36m222.0104\u001b[0m      \u001b[32m132.2170\u001b[0m  0.0293\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m231.8859\u001b[0m      \u001b[32m116.7906\u001b[0m  0.0306\n",
      "      2      \u001b[36m182.2072\u001b[0m       \u001b[32m62.6832\u001b[0m  0.0295\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m531.2939\u001b[0m      \u001b[32m134.6496\u001b[0m  0.0359\n",
      "      9      \u001b[36m447.3824\u001b[0m       96.6687  0.0212\n",
      "      2      \u001b[36m510.4750\u001b[0m      \u001b[32m102.1416\u001b[0m  0.0223\n",
      "      4      408.8406       \u001b[32m62.1118\u001b[0m  0.0343\n",
      "     10      \u001b[36m444.9051\u001b[0m       96.5466  0.0216\n",
      "Restoring best model from epoch 5.\n",
      "      3      \u001b[36m212.2194\u001b[0m      \u001b[32m122.0518\u001b[0m  0.0306\n",
      "      2      \u001b[36m504.8201\u001b[0m      \u001b[32m127.9244\u001b[0m  0.0260\n",
      "      3      \u001b[36m510.2074\u001b[0m      \u001b[32m101.6095\u001b[0m  0.0188\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m213.8140\u001b[0m      \u001b[32m137.2234\u001b[0m  0.0393\n",
      "      5      410.8080       \u001b[32m61.0961\u001b[0m  0.0181\n",
      "      2      \u001b[36m224.7951\u001b[0m       \u001b[32m96.0471\u001b[0m  0.0367\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      3      \u001b[36m178.4151\u001b[0m       64.8679  0.0384\n",
      "      4      \u001b[36m509.3165\u001b[0m      101.6143  0.0204\n",
      "      3      \u001b[36m500.8489\u001b[0m      \u001b[32m124.0202\u001b[0m  0.0255\n",
      "      6      \u001b[36m406.8146\u001b[0m       \u001b[32m60.1168\u001b[0m  0.0265\n",
      "      4      \u001b[36m203.4628\u001b[0m      \u001b[32m121.1503\u001b[0m  0.0309\n",
      "      4      178.6533       68.6614  0.0318\n",
      "      2      \u001b[36m207.6417\u001b[0m      144.4078  0.0411\n",
      "      3      \u001b[36m213.6201\u001b[0m       99.1005  0.0413\n",
      "      5      \u001b[36m502.1527\u001b[0m      \u001b[32m101.1910\u001b[0m  0.0239\n",
      "      4      \u001b[36m496.8527\u001b[0m      \u001b[32m121.5811\u001b[0m  0.0205\n",
      "      7      \u001b[36m401.6454\u001b[0m       \u001b[32m59.6080\u001b[0m  0.0200\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m509.2001\u001b[0m      \u001b[32m143.6270\u001b[0m  0.0501\n",
      "      6      505.0303      \u001b[32m100.5767\u001b[0m  0.0198\n",
      "      5      501.8273      \u001b[32m120.7135\u001b[0m  0.0183\n",
      "      8      406.8080       \u001b[32m59.4395\u001b[0m  0.0166\n",
      "      3      \u001b[36m206.0737\u001b[0m      \u001b[32m132.9658\u001b[0m  0.0280\n",
      "      5      \u001b[36m202.6918\u001b[0m      122.6080  0.0413\n",
      "      5      \u001b[36m175.3439\u001b[0m       66.8165  0.0330\n",
      "      4      215.3910      105.6772  0.0339\n",
      "      2      \u001b[36m486.2190\u001b[0m      \u001b[32m133.7514\u001b[0m  0.0180\n",
      "      6      498.3208      121.1156  0.0179\n",
      "      7      \u001b[36m497.4183\u001b[0m      \u001b[32m100.4103\u001b[0m  0.0185\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      9      \u001b[36m401.1287\u001b[0m       59.5837  0.0169\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      6      \u001b[36m171.1338\u001b[0m       64.2028  0.0243\n",
      "      3      \u001b[36m471.7692\u001b[0m      \u001b[32m131.2845\u001b[0m  0.0182\n",
      "      4      \u001b[36m198.0036\u001b[0m      \u001b[32m122.8856\u001b[0m  0.0319\n",
      "      6      204.8711      122.7016  0.0326\n",
      "      7      \u001b[36m488.9061\u001b[0m      122.0500  0.0230\n",
      "     10      \u001b[36m397.5550\u001b[0m       59.9846  0.0276\n",
      "Restoring best model from epoch 8.\n",
      "      8      \u001b[36m494.3842\u001b[0m      100.5882  0.0306\n",
      "      5      \u001b[36m208.9889\u001b[0m      105.7089  0.0431\n",
      "      4      476.9062      \u001b[32m130.5826\u001b[0m  0.0260\n",
      "      8      493.2916      123.2644  0.0198\n",
      "      7      \u001b[36m170.8121\u001b[0m       63.6663  0.0387\n",
      "      7      203.3077      124.7068  0.0348\n",
      "      9      \u001b[36m492.6059\u001b[0m      100.8969  0.0243\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m216.1612\u001b[0m      \u001b[32m102.1057\u001b[0m  0.0471\n",
      "      5      \u001b[36m197.5510\u001b[0m      \u001b[32m121.3960\u001b[0m  0.0359\n",
      "      9      490.3599      123.6905  0.0181\n",
      "      5      478.3589      \u001b[32m130.1159\u001b[0m  0.0260\n",
      "      6      \u001b[36m208.3991\u001b[0m      104.6349  0.0409\n",
      "     10      \u001b[36m489.4688\u001b[0m      101.1565  0.0225\n",
      "Restoring best model from epoch 7.\n",
      "      8      174.1376       63.8366  0.0294\n",
      "      8      \u001b[36m200.6214\u001b[0m      128.1824  0.0285\n",
      "     10      \u001b[36m487.9856\u001b[0m      123.8952  0.0245\n",
      "Restoring best model from epoch 5.\n",
      "      6      199.3078      \u001b[32m121.1420\u001b[0m  0.0319\n",
      "      2      \u001b[36m200.9138\u001b[0m      \u001b[32m101.0142\u001b[0m  0.0346\n",
      "      6      479.6433      130.4324  0.0380\n",
      "      7      \u001b[36m208.3025\u001b[0m      101.6360  0.0352\n",
      "      9      171.9943       62.9264  0.0376\n",
      "      9      \u001b[36m199.8467\u001b[0m      130.6635  0.0410\n",
      "      7      198.8763      122.3121  0.0350\n",
      "      7      \u001b[36m467.5086\u001b[0m      132.0374  0.0236\n",
      "      3      \u001b[36m196.0404\u001b[0m       \u001b[32m99.5792\u001b[0m  0.0392\n",
      "     10      \u001b[36m170.7588\u001b[0m       62.8290  0.0248\n",
      "Restoring best model from epoch 2.\n",
      "      8      210.1817       98.1134  0.0353\n",
      "     10      204.1177      130.3609  0.0284\n",
      "      8      \u001b[36m465.8910\u001b[0m      134.2120  0.0230\n",
      "Restoring best model from epoch 4.\n",
      "      8      \u001b[36m192.7692\u001b[0m      124.1812  0.0314\n",
      "      4      \u001b[36m193.9402\u001b[0m       \u001b[32m99.1160\u001b[0m  0.0275\n",
      "      9      468.5213      135.4193  0.0193\n",
      "      9      208.3534       96.8229  0.0304\n",
      "      9      \u001b[36m189.3812\u001b[0m      127.7003  0.0262\n",
      "      5      \u001b[36m190.8777\u001b[0m       \u001b[32m97.6695\u001b[0m  0.0295\n",
      "     10      467.4301      134.6756  0.0207\n",
      "Restoring best model from epoch 5.\n",
      "     10      \u001b[36m205.7164\u001b[0m       97.5393  0.0312\n",
      "Restoring best model from epoch 2.\n",
      "     10      196.3048      130.4417  0.0307\n",
      "Restoring best model from epoch 6.\n",
      "      6      191.6030       98.7909  0.0299\n",
      "      7      \u001b[36m186.6669\u001b[0m       97.7454  0.0247\n",
      "      8      190.2662       \u001b[32m95.6439\u001b[0m  0.0259\n",
      "      9      \u001b[36m186.4964\u001b[0m       \u001b[32m94.9921\u001b[0m  0.0285\n",
      "     10      \u001b[36m186.3009\u001b[0m       95.3008  0.0252\n",
      "Restoring best model from epoch 9.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m222.3372\u001b[0m      \u001b[32m147.7221\u001b[0m  0.0269\n",
      "      2      \u001b[36m214.1509\u001b[0m      \u001b[32m140.1844\u001b[0m  0.0244\n",
      "      3      \u001b[36m208.6607\u001b[0m      \u001b[32m137.8964\u001b[0m  0.0251\n",
      "      4      \u001b[36m204.6370\u001b[0m      137.9657  0.0233\n",
      "      5      207.9834      138.8338  0.0244\n",
      "      6      \u001b[36m200.0747\u001b[0m      139.6890  0.0245\n",
      "      7      201.5901      139.2073  0.0234\n",
      "      8      \u001b[36m198.7371\u001b[0m      137.9938  0.0243\n",
      "      9      \u001b[36m194.8595\u001b[0m      \u001b[32m137.7119\u001b[0m  0.0226\n",
      "     10      195.8382      138.2020  0.0229\n",
      "Restoring best model from epoch 9.\n",
      "y_train breslow final [ 2.000e+00  2.000e+00 -1.100e+01  1.100e+01 -1.400e+01  1.400e+01\n",
      "  3.200e+01  5.600e+01 -5.600e+01  6.200e+01  6.400e+01  6.500e+01\n",
      "  6.900e+01 -7.700e+01  8.200e+01  8.900e+01  9.000e+01 -9.200e+01\n",
      "  9.400e+01 -1.020e+02 -1.050e+02  1.070e+02  1.080e+02 -1.110e+02\n",
      "  1.120e+02  1.260e+02  1.290e+02  1.300e+02 -1.340e+02  1.420e+02\n",
      "  1.440e+02  1.510e+02 -1.560e+02  1.600e+02  1.660e+02  1.710e+02\n",
      "  1.730e+02 -1.810e+02  1.820e+02  1.830e+02  1.850e+02 -1.870e+02\n",
      "  1.910e+02  1.940e+02  1.940e+02  1.970e+02  1.980e+02  2.050e+02\n",
      "  2.090e+02  2.150e+02  2.150e+02 -2.160e+02  2.170e+02  2.170e+02\n",
      "  2.180e+02  2.220e+02 -2.250e+02 -2.320e+02 -2.340e+02 -2.420e+02\n",
      "  2.430e+02 -2.530e+02  2.530e+02  2.610e+02  2.680e+02  2.760e+02\n",
      " -2.780e+02  2.790e+02  2.810e+02  2.820e+02  2.840e+02  2.920e+02\n",
      "  2.940e+02  2.950e+02 -3.110e+02  3.170e+02  3.170e+02 -3.180e+02\n",
      "  3.210e+02  3.270e+02  3.270e+02  3.420e+02 -3.450e+02  3.480e+02\n",
      " -3.510e+02  3.510e+02  3.520e+02  3.530e+02 -3.540e+02  3.570e+02\n",
      " -3.580e+02  3.590e+02 -3.600e+02  3.610e+02  3.660e+02 -3.670e+02\n",
      " -3.680e+02  3.710e+02 -3.760e+02  3.770e+02 -3.780e+02  3.790e+02\n",
      " -3.790e+02  3.800e+02 -3.840e+02  3.850e+02 -3.860e+02  3.870e+02\n",
      " -3.870e+02 -3.890e+02 -3.920e+02 -3.920e+02  3.930e+02 -3.930e+02\n",
      "  3.950e+02  3.950e+02  3.970e+02 -4.000e+02 -4.030e+02  4.030e+02\n",
      "  4.030e+02  4.060e+02  4.070e+02 -4.120e+02 -4.140e+02 -4.170e+02\n",
      "  4.210e+02 -4.210e+02 -4.240e+02  4.240e+02  4.360e+02 -4.400e+02\n",
      " -4.410e+02 -4.430e+02  4.460e+02  4.530e+02  4.560e+02 -4.580e+02\n",
      "  4.590e+02 -4.610e+02 -4.660e+02 -4.690e+02 -4.710e+02  4.720e+02\n",
      "  4.790e+02 -4.790e+02  4.800e+02  4.840e+02 -4.850e+02  4.870e+02\n",
      " -4.910e+02  4.930e+02  4.950e+02 -4.990e+02  5.060e+02 -5.090e+02\n",
      " -5.160e+02 -5.180e+02 -5.200e+02  5.210e+02  5.210e+02  5.220e+02\n",
      "  5.260e+02  5.300e+02 -5.390e+02 -5.420e+02 -5.430e+02  5.430e+02\n",
      "  5.450e+02  5.460e+02  5.460e+02 -5.470e+02 -5.490e+02 -5.520e+02\n",
      " -5.590e+02  5.600e+02 -5.680e+02  5.720e+02 -5.730e+02 -5.760e+02\n",
      "  5.770e+02  5.770e+02 -5.780e+02  5.840e+02 -5.860e+02 -5.940e+02\n",
      "  6.010e+02  6.020e+02  6.060e+02 -6.060e+02 -6.160e+02 -6.190e+02\n",
      " -6.220e+02 -6.220e+02 -6.230e+02 -6.230e+02 -6.240e+02  6.250e+02\n",
      " -6.250e+02  6.310e+02 -6.350e+02 -6.390e+02 -6.410e+02 -6.460e+02\n",
      "  6.540e+02 -6.580e+02  6.630e+02  6.660e+02 -6.670e+02 -6.700e+02\n",
      "  6.750e+02 -6.790e+02  6.800e+02 -6.820e+02 -6.850e+02 -6.930e+02\n",
      " -6.950e+02  6.950e+02 -7.010e+02 -7.040e+02 -7.070e+02 -7.100e+02\n",
      " -7.130e+02 -7.140e+02 -7.170e+02 -7.250e+02  7.270e+02  7.390e+02\n",
      " -7.420e+02 -7.500e+02 -7.540e+02 -7.590e+02 -7.590e+02 -7.590e+02\n",
      " -7.590e+02 -7.700e+02 -7.730e+02 -7.820e+02 -7.830e+02  7.890e+02\n",
      " -7.960e+02 -7.970e+02 -7.990e+02  8.040e+02  8.060e+02 -8.170e+02\n",
      "  8.230e+02 -8.270e+02  8.360e+02  8.390e+02 -8.510e+02 -8.520e+02\n",
      " -8.660e+02 -8.750e+02  8.820e+02 -8.930e+02 -8.960e+02 -9.040e+02\n",
      " -9.060e+02 -9.100e+02 -9.110e+02 -9.130e+02 -9.140e+02  9.150e+02\n",
      " -9.180e+02  9.270e+02 -9.300e+02 -9.470e+02 -9.500e+02 -9.540e+02\n",
      " -9.640e+02  9.800e+02  9.850e+02 -9.870e+02  9.880e+02  9.930e+02\n",
      " -9.970e+02  9.980e+02 -1.000e+03 -1.021e+03 -1.022e+03 -1.025e+03\n",
      " -1.027e+03 -1.030e+03 -1.036e+03  1.037e+03 -1.038e+03 -1.045e+03\n",
      " -1.057e+03 -1.065e+03 -1.066e+03 -1.077e+03  1.079e+03  1.081e+03\n",
      "  1.090e+03  1.093e+03 -1.117e+03 -1.120e+03  1.133e+03  1.134e+03\n",
      " -1.138e+03 -1.152e+03 -1.157e+03 -1.168e+03 -1.172e+03 -1.179e+03\n",
      " -1.183e+03 -1.191e+03  1.202e+03 -1.224e+03 -1.245e+03 -1.273e+03\n",
      " -1.278e+03  1.289e+03 -1.309e+03 -1.315e+03 -1.346e+03 -1.353e+03\n",
      " -1.358e+03 -1.368e+03  1.394e+03  1.398e+03 -1.399e+03 -1.409e+03\n",
      " -1.409e+03 -1.413e+03 -1.425e+03 -1.428e+03 -1.430e+03 -1.435e+03\n",
      " -1.440e+03  1.459e+03 -1.461e+03  1.466e+03 -1.472e+03 -1.478e+03\n",
      " -1.483e+03  1.504e+03 -1.506e+03 -1.508e+03 -1.512e+03 -1.521e+03\n",
      " -1.523e+03 -1.527e+03 -1.555e+03 -1.559e+03 -1.560e+03 -1.584e+03\n",
      " -1.584e+03 -1.586e+03  1.591e+03 -1.593e+03 -1.628e+03  1.641e+03\n",
      " -1.660e+03 -1.663e+03 -1.663e+03 -1.665e+03  1.671e+03 -1.686e+03\n",
      " -1.690e+03 -1.699e+03 -1.721e+03 -1.722e+03 -1.724e+03  1.732e+03\n",
      "  1.748e+03 -1.777e+03 -1.778e+03  1.838e+03 -1.840e+03 -1.897e+03\n",
      " -1.899e+03 -1.971e+03  1.972e+03  2.002e+03 -2.016e+03  2.064e+03\n",
      "  2.083e+03  2.120e+03 -2.133e+03 -2.143e+03  2.166e+03 -2.182e+03\n",
      " -2.239e+03 -2.265e+03 -2.298e+03  2.319e+03 -2.327e+03 -2.347e+03\n",
      " -2.359e+03 -2.437e+03  2.570e+03 -2.641e+03  2.703e+03  2.717e+03\n",
      " -2.727e+03  2.741e+03 -2.784e+03 -2.886e+03  3.059e+03 -3.221e+03\n",
      " -3.270e+03  3.314e+03 -3.381e+03 -3.930e+03 -3.981e+03 -4.241e+03\n",
      " -4.282e+03  4.680e+03  4.856e+03  5.152e+03 -5.252e+03 -5.480e+03\n",
      "  6.417e+03]\n",
      "Concordance Index 0.6221321314519964\n",
      "Integrated Brier Score: 0.186779989348572\n",
      "y_train breslow final [ 2.000e+00  2.000e+00 -1.100e+01  1.100e+01 -1.400e+01  1.400e+01\n",
      "  3.200e+01  5.600e+01 -5.600e+01  6.200e+01  6.400e+01  6.500e+01\n",
      "  6.900e+01 -7.700e+01  8.200e+01  8.900e+01  9.000e+01 -9.200e+01\n",
      "  9.400e+01 -1.020e+02 -1.050e+02  1.070e+02  1.080e+02 -1.110e+02\n",
      "  1.120e+02  1.260e+02  1.290e+02  1.300e+02 -1.340e+02  1.420e+02\n",
      "  1.440e+02  1.510e+02 -1.560e+02  1.600e+02  1.660e+02  1.710e+02\n",
      "  1.730e+02 -1.810e+02  1.820e+02  1.830e+02  1.850e+02 -1.870e+02\n",
      "  1.910e+02  1.940e+02  1.940e+02  1.970e+02  1.980e+02  2.050e+02\n",
      "  2.090e+02  2.150e+02  2.150e+02 -2.160e+02  2.170e+02  2.170e+02\n",
      "  2.180e+02  2.220e+02 -2.250e+02 -2.320e+02 -2.340e+02 -2.420e+02\n",
      "  2.430e+02 -2.530e+02  2.530e+02  2.610e+02  2.680e+02  2.760e+02\n",
      " -2.780e+02  2.790e+02  2.810e+02  2.820e+02  2.840e+02  2.920e+02\n",
      "  2.940e+02  2.950e+02 -3.110e+02  3.170e+02  3.170e+02 -3.180e+02\n",
      "  3.210e+02  3.270e+02  3.270e+02  3.420e+02 -3.450e+02  3.480e+02\n",
      " -3.510e+02  3.510e+02  3.520e+02  3.530e+02 -3.540e+02  3.570e+02\n",
      " -3.580e+02  3.590e+02 -3.600e+02  3.610e+02  3.660e+02 -3.670e+02\n",
      " -3.680e+02  3.710e+02 -3.760e+02  3.770e+02 -3.780e+02  3.790e+02\n",
      " -3.790e+02  3.800e+02 -3.840e+02  3.850e+02 -3.860e+02  3.870e+02\n",
      " -3.870e+02 -3.890e+02 -3.920e+02 -3.920e+02  3.930e+02 -3.930e+02\n",
      "  3.950e+02  3.950e+02  3.970e+02 -4.000e+02 -4.030e+02  4.030e+02\n",
      "  4.030e+02  4.060e+02  4.070e+02 -4.120e+02 -4.140e+02 -4.170e+02\n",
      "  4.210e+02 -4.210e+02 -4.240e+02  4.240e+02  4.360e+02 -4.400e+02\n",
      " -4.410e+02 -4.430e+02  4.460e+02  4.530e+02  4.560e+02 -4.580e+02\n",
      "  4.590e+02 -4.610e+02 -4.660e+02 -4.690e+02 -4.710e+02  4.720e+02\n",
      "  4.790e+02 -4.790e+02  4.800e+02  4.840e+02 -4.850e+02  4.870e+02\n",
      " -4.910e+02  4.930e+02  4.950e+02 -4.990e+02  5.060e+02 -5.090e+02\n",
      " -5.160e+02 -5.180e+02 -5.200e+02  5.210e+02  5.210e+02  5.220e+02\n",
      "  5.260e+02  5.300e+02 -5.390e+02 -5.420e+02 -5.430e+02  5.430e+02\n",
      "  5.450e+02  5.460e+02  5.460e+02 -5.470e+02 -5.490e+02 -5.520e+02\n",
      " -5.590e+02  5.600e+02 -5.680e+02  5.720e+02 -5.730e+02 -5.760e+02\n",
      "  5.770e+02  5.770e+02 -5.780e+02  5.840e+02 -5.860e+02 -5.940e+02\n",
      "  6.010e+02  6.020e+02  6.060e+02 -6.060e+02 -6.160e+02 -6.190e+02\n",
      " -6.220e+02 -6.220e+02 -6.230e+02 -6.230e+02 -6.240e+02  6.250e+02\n",
      " -6.250e+02  6.310e+02 -6.350e+02 -6.390e+02 -6.410e+02 -6.460e+02\n",
      "  6.540e+02 -6.580e+02  6.630e+02  6.660e+02 -6.670e+02 -6.700e+02\n",
      "  6.750e+02 -6.790e+02  6.800e+02 -6.820e+02 -6.850e+02 -6.930e+02\n",
      " -6.950e+02  6.950e+02 -7.010e+02 -7.040e+02 -7.070e+02 -7.100e+02\n",
      " -7.130e+02 -7.140e+02 -7.170e+02 -7.250e+02  7.270e+02  7.390e+02\n",
      " -7.420e+02 -7.500e+02 -7.540e+02 -7.590e+02 -7.590e+02 -7.590e+02\n",
      " -7.590e+02 -7.700e+02 -7.730e+02 -7.820e+02 -7.830e+02  7.890e+02\n",
      " -7.960e+02 -7.970e+02 -7.990e+02  8.040e+02  8.060e+02 -8.170e+02\n",
      "  8.230e+02 -8.270e+02  8.360e+02  8.390e+02 -8.510e+02 -8.520e+02\n",
      " -8.660e+02 -8.750e+02  8.820e+02 -8.930e+02 -8.960e+02 -9.040e+02\n",
      " -9.060e+02 -9.100e+02 -9.110e+02 -9.130e+02 -9.140e+02  9.150e+02\n",
      " -9.180e+02  9.270e+02 -9.300e+02 -9.470e+02 -9.500e+02 -9.540e+02\n",
      " -9.640e+02  9.800e+02  9.850e+02 -9.870e+02  9.880e+02  9.930e+02\n",
      " -9.970e+02  9.980e+02 -1.000e+03 -1.021e+03 -1.022e+03 -1.025e+03\n",
      " -1.027e+03 -1.030e+03 -1.036e+03  1.037e+03 -1.038e+03 -1.045e+03\n",
      " -1.057e+03 -1.065e+03 -1.066e+03 -1.077e+03  1.079e+03  1.081e+03\n",
      "  1.090e+03  1.093e+03 -1.117e+03 -1.120e+03  1.133e+03  1.134e+03\n",
      " -1.138e+03 -1.152e+03 -1.157e+03 -1.168e+03 -1.172e+03 -1.179e+03\n",
      " -1.183e+03 -1.191e+03  1.202e+03 -1.224e+03 -1.245e+03 -1.273e+03\n",
      " -1.278e+03  1.289e+03 -1.309e+03 -1.315e+03 -1.346e+03 -1.353e+03\n",
      " -1.358e+03 -1.368e+03  1.394e+03  1.398e+03 -1.399e+03 -1.409e+03\n",
      " -1.409e+03 -1.413e+03 -1.425e+03 -1.428e+03 -1.430e+03 -1.435e+03\n",
      " -1.440e+03  1.459e+03 -1.461e+03  1.466e+03 -1.472e+03 -1.478e+03\n",
      " -1.483e+03  1.504e+03 -1.506e+03 -1.508e+03 -1.512e+03 -1.521e+03\n",
      " -1.523e+03 -1.527e+03 -1.555e+03 -1.559e+03 -1.560e+03 -1.584e+03\n",
      " -1.584e+03 -1.586e+03  1.591e+03 -1.593e+03 -1.628e+03  1.641e+03\n",
      " -1.660e+03 -1.663e+03 -1.663e+03 -1.665e+03  1.671e+03 -1.686e+03\n",
      " -1.690e+03 -1.699e+03 -1.721e+03 -1.722e+03 -1.724e+03  1.732e+03\n",
      "  1.748e+03 -1.777e+03 -1.778e+03  1.838e+03 -1.840e+03 -1.897e+03\n",
      " -1.899e+03 -1.971e+03  1.972e+03  2.002e+03 -2.016e+03  2.064e+03\n",
      "  2.083e+03  2.120e+03 -2.133e+03 -2.143e+03  2.166e+03 -2.182e+03\n",
      " -2.239e+03 -2.265e+03 -2.298e+03  2.319e+03 -2.327e+03 -2.347e+03\n",
      " -2.359e+03 -2.437e+03  2.570e+03 -2.641e+03  2.703e+03  2.717e+03\n",
      " -2.727e+03  2.741e+03 -2.784e+03 -2.886e+03  3.059e+03 -3.221e+03\n",
      " -3.270e+03  3.314e+03 -3.381e+03 -3.930e+03 -3.981e+03 -4.241e+03\n",
      " -4.282e+03  4.680e+03  4.856e+03  5.152e+03 -5.252e+03 -5.480e+03\n",
      "  6.417e+03]\n",
      "durations 23.0 4760.0\n",
      "Concordance Index 0.5740676193795747\n",
      "Integrated Brier Score: 0.2329299819518419\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m119.4586\u001b[0m  0.0122\n",
      "      2      119.4586  0.0032\n",
      "      3      119.4586  0.0026\n",
      "      4      119.4586  0.0044\n",
      "      5      119.4586  0.0079\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      6      119.4586  0.0024\n",
      "      7      119.4586  0.0022\n",
      "      8      119.4586  0.0021\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m141.5646\u001b[0m  0.0049\n",
      "      9      119.4586  0.0045\n",
      "      2      141.5646  0.0046\n",
      "     10      119.4586  0.0043\n",
      "      3      141.5646  0.0023\n",
      "      4      141.5646  0.0019\n",
      "Restoring best model from epoch 1.\n",
      "      5      141.5646  0.0021\n",
      "      6      141.5646  0.0041\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      7      141.5646  0.0022\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      8      141.5646  0.0033\n",
      "      9      141.5646  0.0021\n",
      "     10      141.5646  0.0019\n",
      "Restoring best model from epoch 1.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m107.6794\u001b[0m  0.0055\n",
      "      2      107.6794  0.0038\n",
      "      3      107.6794  0.0029\n",
      "      4      107.6794  0.0039\n",
      "      5      107.6794  0.0034\n",
      "      6      107.6794  0.0027\n",
      "      7      107.6794  0.0026\n",
      "      8      107.6794  0.0026\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      9      107.6794  0.0022\n",
      "     10      107.6794  0.0028\n",
      "Restoring best model from epoch 1.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m152.4867\u001b[0m  0.0034\n",
      "      2      152.4867  0.0035\n",
      "      3      152.4867  0.0023\n",
      "      4      152.4867  0.0020\n",
      "      5      152.4867  0.0052\n",
      "      6      152.4867  0.0027\n",
      "      7      152.4867  0.0019\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      8      152.4867  0.0019\n",
      "      9      152.4867  0.0018\n",
      "     10      152.4867  0.0019\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m262.3289\u001b[0m  0.0089\n",
      "      2      262.3289  0.0037\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      3      262.3289  0.0020\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m107.7587\u001b[0m  0.0043\n",
      "      4      262.3289  0.0027\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m192.4468\u001b[0m  0.0031\n",
      "      2      107.7587  0.0024\n",
      "      5      262.3289  0.0020\n",
      "      2      192.4468  0.0023\n",
      "      3      107.7587  0.0021\n",
      "      6      262.3289  0.0020\n",
      "      7      262.3289  0.0019\n",
      "      3      192.4468  0.0021\n",
      "      4      107.7587  0.0022\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      4      192.4468  0.0021\n",
      "      8      262.3289  0.0023\n",
      "      5      107.7587  0.0025\n",
      "      9      262.3289  0.0020\n",
      "      5      192.4468  0.0021\n",
      "      6      107.7587  0.0022\n",
      "      6      192.4468  0.0020\n",
      "      7      107.7587  0.0021\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m299.7368\u001b[0m  0.0055\n",
      "     10      262.3289  0.0040\n",
      "      7      192.4468  0.0023\n",
      "Restoring best model from epoch 1.\n",
      "      8      107.7587  0.0021\n",
      "      2      299.7368  0.0024\n",
      "      8      192.4468  0.0023\n",
      "      9      107.7587  0.0022\n",
      "      3      299.7368  0.0020\n",
      "      9      192.4468  0.0019\n",
      "     10      107.7587  0.0020\n",
      "Restoring best model from epoch 1.\n",
      "      4      299.7368  0.0020\n",
      "      5      299.7368  0.0019\n",
      "     10      192.4468  0.0041\n",
      "Restoring best model from epoch 1.\n",
      "      6      299.7368  0.0034\n",
      "      7      299.7368  0.0023\n",
      "      8      299.7368  0.0020\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      9      299.7368  0.0026\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "     10      299.7368  0.0035\n",
      "Restoring best model from epoch 1.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m87.8568\u001b[0m  0.0079\n",
      "      2       87.8568  0.0033\n",
      "      3       87.8568  0.0036\n",
      "      4       87.8568  0.0032\n",
      "      5       87.8568  0.0036\n",
      "      6       87.8568  0.0043\n",
      "      7       87.8568  0.0030\n",
      "      8       87.8568  0.0022\n",
      "      9       87.8568  0.0020\n",
      "     10       87.8568  0.0026\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m66.0426\u001b[0m  0.0033\n",
      "      2       66.0426  0.0023\n",
      "      3       66.0426  0.0020\n",
      "      4       66.0426  0.0019\n",
      "      5       66.0426  0.0021\n",
      "      6       66.0426  0.0019\n",
      "      7       66.0426  0.0019\n",
      "      8       66.0426  0.0020\n",
      "      9       66.0426  0.0019\n",
      "     10       66.0426  0.0020\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m153.3701\u001b[0m  0.0036\n",
      "      2      153.3701  0.0033\n",
      "      3      153.3701  0.0032\n",
      "      4      153.3701  0.0031\n",
      "      5      153.3701  0.0035\n",
      "      6      153.3701  0.0034\n",
      "      7      153.3701  0.0033\n",
      "      8      153.3701  0.0032\n",
      "      9      153.3701  0.0036\n",
      "     10      153.3701  0.0035\n",
      "Restoring best model from epoch 1.\n",
      "y_train breslow final [ 2.000e+00  2.000e+00 -1.100e+01  1.100e+01  1.400e+01  2.300e+01\n",
      " -3.000e+01  3.200e+01  5.600e+01  6.200e+01  6.500e+01  6.900e+01\n",
      "  7.600e+01 -7.700e+01  8.200e+01  8.600e+01  9.000e+01 -9.200e+01\n",
      "  9.400e+01  9.500e+01 -1.020e+02 -1.050e+02  1.070e+02 -1.100e+02\n",
      " -1.110e+02  1.120e+02 -1.220e+02  1.260e+02  1.290e+02  1.290e+02\n",
      "  1.300e+02 -1.340e+02  1.420e+02  1.440e+02  1.440e+02 -1.460e+02\n",
      " -1.560e+02  1.600e+02  1.660e+02  1.730e+02 -1.810e+02  1.820e+02\n",
      "  1.850e+02 -1.870e+02  1.940e+02  1.940e+02  1.970e+02  2.050e+02\n",
      "  2.090e+02 -2.100e+02  2.150e+02 -2.160e+02 -2.160e+02  2.170e+02\n",
      "  2.170e+02  2.180e+02  2.220e+02 -2.250e+02 -2.340e+02 -2.420e+02\n",
      "  2.430e+02 -2.530e+02  2.530e+02  2.560e+02  2.590e+02  2.610e+02\n",
      "  2.680e+02  2.740e+02  2.760e+02 -2.780e+02  2.790e+02  2.830e+02\n",
      "  2.840e+02 -2.920e+02  2.940e+02  2.950e+02 -3.110e+02  3.170e+02\n",
      " -3.180e+02 -3.210e+02  3.210e+02  3.270e+02  3.270e+02  3.300e+02\n",
      "  3.340e+02  3.360e+02  3.370e+02  3.410e+02 -3.450e+02  3.480e+02\n",
      "  3.510e+02  3.530e+02 -3.540e+02  3.570e+02  3.600e+02 -3.600e+02\n",
      "  3.610e+02 -3.620e+02 -3.670e+02 -3.680e+02  3.710e+02 -3.760e+02\n",
      "  3.770e+02  3.770e+02 -3.780e+02  3.790e+02 -3.790e+02  3.800e+02\n",
      "  3.840e+02 -3.840e+02  3.850e+02 -3.860e+02 -3.890e+02 -3.920e+02\n",
      " -3.920e+02  3.930e+02 -3.930e+02  3.950e+02  3.950e+02 -4.000e+02\n",
      "  4.030e+02  4.030e+02  4.060e+02  4.060e+02 -4.070e+02 -4.140e+02\n",
      "  4.150e+02  4.210e+02 -4.210e+02 -4.240e+02  4.240e+02  4.270e+02\n",
      "  4.300e+02  4.360e+02 -4.370e+02 -4.410e+02 -4.430e+02  4.460e+02\n",
      "  4.490e+02  4.510e+02  4.530e+02  4.560e+02 -4.580e+02  4.590e+02\n",
      " -4.610e+02  4.620e+02  4.640e+02 -4.660e+02 -4.690e+02 -4.710e+02\n",
      "  4.720e+02  4.790e+02  4.800e+02  4.840e+02  4.870e+02  4.890e+02\n",
      " -4.910e+02  4.950e+02 -4.990e+02  5.060e+02 -5.140e+02 -5.160e+02\n",
      " -5.180e+02  5.210e+02  5.210e+02  5.220e+02  5.260e+02 -5.390e+02\n",
      " -5.430e+02  5.430e+02  5.460e+02 -5.470e+02  5.480e+02  5.600e+02\n",
      "  5.640e+02 -5.680e+02  5.720e+02  5.770e+02 -5.780e+02 -5.790e+02\n",
      "  5.800e+02  5.840e+02 -5.940e+02 -6.000e+02  6.020e+02 -6.030e+02\n",
      "  6.060e+02 -6.060e+02 -6.160e+02 -6.190e+02 -6.230e+02 -6.230e+02\n",
      "  6.240e+02  6.250e+02 -6.250e+02  6.310e+02 -6.350e+02 -6.370e+02\n",
      " -6.390e+02 -6.410e+02 -6.410e+02  6.410e+02 -6.440e+02 -6.450e+02\n",
      " -6.450e+02 -6.460e+02 -6.530e+02  6.540e+02  6.660e+02 -6.700e+02\n",
      "  6.750e+02 -6.790e+02  6.800e+02 -6.820e+02 -6.870e+02 -6.930e+02\n",
      "  6.950e+02 -7.010e+02 -7.010e+02 -7.040e+02 -7.070e+02 -7.130e+02\n",
      " -7.170e+02 -7.220e+02 -7.420e+02 -7.500e+02 -7.540e+02 -7.590e+02\n",
      " -7.590e+02 -7.590e+02  7.630e+02 -7.700e+02  7.730e+02 -7.730e+02\n",
      "  7.890e+02 -7.970e+02 -7.990e+02  8.040e+02 -8.170e+02  8.230e+02\n",
      " -8.330e+02  8.360e+02 -8.360e+02  8.390e+02 -8.500e+02 -8.510e+02\n",
      " -8.520e+02  8.530e+02 -8.550e+02  8.620e+02 -8.660e+02 -8.750e+02\n",
      " -8.890e+02 -8.930e+02 -8.950e+02 -8.960e+02 -8.970e+02 -9.040e+02\n",
      " -9.060e+02 -9.100e+02 -9.110e+02 -9.130e+02 -9.140e+02  9.150e+02\n",
      " -9.180e+02  9.270e+02 -9.330e+02  9.410e+02 -9.500e+02 -9.540e+02\n",
      " -9.640e+02 -9.720e+02  9.800e+02 -9.870e+02  9.880e+02  9.930e+02\n",
      " -9.930e+02 -9.950e+02 -9.970e+02  9.980e+02 -1.021e+03 -1.022e+03\n",
      " -1.025e+03 -1.027e+03 -1.036e+03  1.037e+03 -1.038e+03 -1.045e+03\n",
      " -1.050e+03 -1.057e+03 -1.065e+03 -1.066e+03 -1.075e+03  1.090e+03\n",
      " -1.095e+03 -1.117e+03 -1.120e+03 -1.125e+03 -1.131e+03  1.133e+03\n",
      "  1.134e+03 -1.138e+03 -1.143e+03 -1.147e+03 -1.152e+03 -1.157e+03\n",
      " -1.172e+03 -1.179e+03 -1.190e+03 -1.191e+03  1.202e+03 -1.218e+03\n",
      " -1.224e+03 -1.245e+03 -1.253e+03 -1.265e+03 -1.273e+03 -1.274e+03\n",
      " -1.288e+03 -1.309e+03 -1.311e+03 -1.346e+03 -1.358e+03 -1.368e+03\n",
      "  1.398e+03 -1.399e+03 -1.409e+03 -1.409e+03 -1.413e+03 -1.425e+03\n",
      " -1.428e+03  1.430e+03 -1.430e+03 -1.435e+03 -1.440e+03 -1.443e+03\n",
      " -1.460e+03 -1.461e+03 -1.466e+03  1.466e+03 -1.478e+03 -1.483e+03\n",
      "  1.504e+03 -1.506e+03 -1.508e+03 -1.512e+03 -1.521e+03 -1.523e+03\n",
      " -1.527e+03 -1.560e+03 -1.584e+03 -1.586e+03  1.591e+03 -1.593e+03\n",
      " -1.628e+03  1.641e+03 -1.660e+03 -1.663e+03 -1.663e+03 -1.665e+03\n",
      "  1.671e+03  1.718e+03 -1.722e+03  1.732e+03 -1.737e+03  1.748e+03\n",
      "  1.762e+03 -1.777e+03 -1.778e+03 -1.840e+03 -1.897e+03 -1.899e+03\n",
      " -1.971e+03  1.972e+03 -1.995e+03  2.002e+03 -2.016e+03 -2.016e+03\n",
      "  2.064e+03  2.083e+03  2.120e+03 -2.133e+03 -2.143e+03 -2.161e+03\n",
      "  2.166e+03 -2.169e+03 -2.239e+03 -2.265e+03 -2.298e+03 -2.347e+03\n",
      " -2.359e+03 -2.437e+03 -2.562e+03  2.570e+03 -2.641e+03  2.717e+03\n",
      " -2.727e+03  2.741e+03 -2.784e+03 -2.886e+03  2.900e+03  3.059e+03\n",
      " -3.221e+03 -3.270e+03 -3.381e+03 -3.930e+03 -3.981e+03 -4.241e+03\n",
      "  4.680e+03  4.760e+03  4.856e+03  5.152e+03 -5.252e+03 -5.480e+03\n",
      "  6.417e+03]\n",
      "Concordance Index 0.5510327253218884\n",
      "Integrated Brier Score: 0.19225146058057827\n",
      "y_train breslow final [ 2.000e+00  2.000e+00 -1.100e+01  1.100e+01  1.400e+01  2.300e+01\n",
      " -3.000e+01  3.200e+01  5.600e+01  6.200e+01  6.500e+01  6.900e+01\n",
      "  7.600e+01 -7.700e+01  8.200e+01  8.600e+01  9.000e+01 -9.200e+01\n",
      "  9.400e+01  9.500e+01 -1.020e+02 -1.050e+02  1.070e+02 -1.100e+02\n",
      " -1.110e+02  1.120e+02 -1.220e+02  1.260e+02  1.290e+02  1.290e+02\n",
      "  1.300e+02 -1.340e+02  1.420e+02  1.440e+02  1.440e+02 -1.460e+02\n",
      " -1.560e+02  1.600e+02  1.660e+02  1.730e+02 -1.810e+02  1.820e+02\n",
      "  1.850e+02 -1.870e+02  1.940e+02  1.940e+02  1.970e+02  2.050e+02\n",
      "  2.090e+02 -2.100e+02  2.150e+02 -2.160e+02 -2.160e+02  2.170e+02\n",
      "  2.170e+02  2.180e+02  2.220e+02 -2.250e+02 -2.340e+02 -2.420e+02\n",
      "  2.430e+02 -2.530e+02  2.530e+02  2.560e+02  2.590e+02  2.610e+02\n",
      "  2.680e+02  2.740e+02  2.760e+02 -2.780e+02  2.790e+02  2.830e+02\n",
      "  2.840e+02 -2.920e+02  2.940e+02  2.950e+02 -3.110e+02  3.170e+02\n",
      " -3.180e+02 -3.210e+02  3.210e+02  3.270e+02  3.270e+02  3.300e+02\n",
      "  3.340e+02  3.360e+02  3.370e+02  3.410e+02 -3.450e+02  3.480e+02\n",
      "  3.510e+02  3.530e+02 -3.540e+02  3.570e+02  3.600e+02 -3.600e+02\n",
      "  3.610e+02 -3.620e+02 -3.670e+02 -3.680e+02  3.710e+02 -3.760e+02\n",
      "  3.770e+02  3.770e+02 -3.780e+02  3.790e+02 -3.790e+02  3.800e+02\n",
      "  3.840e+02 -3.840e+02  3.850e+02 -3.860e+02 -3.890e+02 -3.920e+02\n",
      " -3.920e+02  3.930e+02 -3.930e+02  3.950e+02  3.950e+02 -4.000e+02\n",
      "  4.030e+02  4.030e+02  4.060e+02  4.060e+02 -4.070e+02 -4.140e+02\n",
      "  4.150e+02  4.210e+02 -4.210e+02 -4.240e+02  4.240e+02  4.270e+02\n",
      "  4.300e+02  4.360e+02 -4.370e+02 -4.410e+02 -4.430e+02  4.460e+02\n",
      "  4.490e+02  4.510e+02  4.530e+02  4.560e+02 -4.580e+02  4.590e+02\n",
      " -4.610e+02  4.620e+02  4.640e+02 -4.660e+02 -4.690e+02 -4.710e+02\n",
      "  4.720e+02  4.790e+02  4.800e+02  4.840e+02  4.870e+02  4.890e+02\n",
      " -4.910e+02  4.950e+02 -4.990e+02  5.060e+02 -5.140e+02 -5.160e+02\n",
      " -5.180e+02  5.210e+02  5.210e+02  5.220e+02  5.260e+02 -5.390e+02\n",
      " -5.430e+02  5.430e+02  5.460e+02 -5.470e+02  5.480e+02  5.600e+02\n",
      "  5.640e+02 -5.680e+02  5.720e+02  5.770e+02 -5.780e+02 -5.790e+02\n",
      "  5.800e+02  5.840e+02 -5.940e+02 -6.000e+02  6.020e+02 -6.030e+02\n",
      "  6.060e+02 -6.060e+02 -6.160e+02 -6.190e+02 -6.230e+02 -6.230e+02\n",
      "  6.240e+02  6.250e+02 -6.250e+02  6.310e+02 -6.350e+02 -6.370e+02\n",
      " -6.390e+02 -6.410e+02 -6.410e+02  6.410e+02 -6.440e+02 -6.450e+02\n",
      " -6.450e+02 -6.460e+02 -6.530e+02  6.540e+02  6.660e+02 -6.700e+02\n",
      "  6.750e+02 -6.790e+02  6.800e+02 -6.820e+02 -6.870e+02 -6.930e+02\n",
      "  6.950e+02 -7.010e+02 -7.010e+02 -7.040e+02 -7.070e+02 -7.130e+02\n",
      " -7.170e+02 -7.220e+02 -7.420e+02 -7.500e+02 -7.540e+02 -7.590e+02\n",
      " -7.590e+02 -7.590e+02  7.630e+02 -7.700e+02  7.730e+02 -7.730e+02\n",
      "  7.890e+02 -7.970e+02 -7.990e+02  8.040e+02 -8.170e+02  8.230e+02\n",
      " -8.330e+02  8.360e+02 -8.360e+02  8.390e+02 -8.500e+02 -8.510e+02\n",
      " -8.520e+02  8.530e+02 -8.550e+02  8.620e+02 -8.660e+02 -8.750e+02\n",
      " -8.890e+02 -8.930e+02 -8.950e+02 -8.960e+02 -8.970e+02 -9.040e+02\n",
      " -9.060e+02 -9.100e+02 -9.110e+02 -9.130e+02 -9.140e+02  9.150e+02\n",
      " -9.180e+02  9.270e+02 -9.330e+02  9.410e+02 -9.500e+02 -9.540e+02\n",
      " -9.640e+02 -9.720e+02  9.800e+02 -9.870e+02  9.880e+02  9.930e+02\n",
      " -9.930e+02 -9.950e+02 -9.970e+02  9.980e+02 -1.021e+03 -1.022e+03\n",
      " -1.025e+03 -1.027e+03 -1.036e+03  1.037e+03 -1.038e+03 -1.045e+03\n",
      " -1.050e+03 -1.057e+03 -1.065e+03 -1.066e+03 -1.075e+03  1.090e+03\n",
      " -1.095e+03 -1.117e+03 -1.120e+03 -1.125e+03 -1.131e+03  1.133e+03\n",
      "  1.134e+03 -1.138e+03 -1.143e+03 -1.147e+03 -1.152e+03 -1.157e+03\n",
      " -1.172e+03 -1.179e+03 -1.190e+03 -1.191e+03  1.202e+03 -1.218e+03\n",
      " -1.224e+03 -1.245e+03 -1.253e+03 -1.265e+03 -1.273e+03 -1.274e+03\n",
      " -1.288e+03 -1.309e+03 -1.311e+03 -1.346e+03 -1.358e+03 -1.368e+03\n",
      "  1.398e+03 -1.399e+03 -1.409e+03 -1.409e+03 -1.413e+03 -1.425e+03\n",
      " -1.428e+03  1.430e+03 -1.430e+03 -1.435e+03 -1.440e+03 -1.443e+03\n",
      " -1.460e+03 -1.461e+03 -1.466e+03  1.466e+03 -1.478e+03 -1.483e+03\n",
      "  1.504e+03 -1.506e+03 -1.508e+03 -1.512e+03 -1.521e+03 -1.523e+03\n",
      " -1.527e+03 -1.560e+03 -1.584e+03 -1.586e+03  1.591e+03 -1.593e+03\n",
      " -1.628e+03  1.641e+03 -1.660e+03 -1.663e+03 -1.663e+03 -1.665e+03\n",
      "  1.671e+03  1.718e+03 -1.722e+03  1.732e+03 -1.737e+03  1.748e+03\n",
      "  1.762e+03 -1.777e+03 -1.778e+03 -1.840e+03 -1.897e+03 -1.899e+03\n",
      " -1.971e+03  1.972e+03 -1.995e+03  2.002e+03 -2.016e+03 -2.016e+03\n",
      "  2.064e+03  2.083e+03  2.120e+03 -2.133e+03 -2.143e+03 -2.161e+03\n",
      "  2.166e+03 -2.169e+03 -2.239e+03 -2.265e+03 -2.298e+03 -2.347e+03\n",
      " -2.359e+03 -2.437e+03 -2.562e+03  2.570e+03 -2.641e+03  2.717e+03\n",
      " -2.727e+03  2.741e+03 -2.784e+03 -2.886e+03  2.900e+03  3.059e+03\n",
      " -3.221e+03 -3.270e+03 -3.381e+03 -3.930e+03 -3.981e+03 -4.241e+03\n",
      "  4.680e+03  4.760e+03  4.856e+03  5.152e+03 -5.252e+03 -5.480e+03\n",
      "  6.417e+03]\n",
      "durations 14.0 4282.0\n",
      "Concordance Index 0.5189169139465876\n",
      "Integrated Brier Score: 0.19769345197364777\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m87.7420\u001b[0m  0.0028\n",
      "      2       87.7420  0.0037\n",
      "      3       87.7420  0.0027\n",
      "      4       87.7420  0.0020\n",
      "      5       87.7420  0.0019\n",
      "      6       87.7420  0.0022\n",
      "      7       87.7420  0.0021\n",
      "      8       87.7420  0.0022\n",
      "      9       87.7420  0.0022\n",
      "     10       87.7420  0.0020\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m113.3181\u001b[0m  0.0035\n",
      "      2      113.3181  0.0028\n",
      "      3      113.3181  0.0032\n",
      "      4      113.3181  0.0033\n",
      "      5      113.3181  0.0027\n",
      "      6      113.3181  0.0023\n",
      "      7      113.3181  0.0031\n",
      "      8      113.3181  0.0026\n",
      "      9      113.3181  0.0030\n",
      "     10      113.3181  0.0026\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m172.2200\u001b[0m  0.0026\n",
      "      2      172.2200  0.0021\n",
      "      3      172.2200  0.0029\n",
      "      4      172.2200  0.0039\n",
      "      5      172.2200  0.0027\n",
      "      6      172.2200  0.0021\n",
      "      7      172.2200  0.0036\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      8      172.2200  0.0019\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      9      172.2200  0.0019\n",
      "     10      172.2200  0.0024\n",
      "Restoring best model from epoch 1.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m176.1534\u001b[0m  0.0048\n",
      "      2      176.1534  0.0049\n",
      "      3      176.1534  0.0030\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      4      176.1534  0.0037\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      5      176.1534  0.0025\n",
      "      6      176.1534  0.0028\n",
      "      7      176.1534  0.0039\n",
      "      8      176.1534  0.0042\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m95.6578\u001b[0m  0.0080\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      9      176.1534  0.0064\n",
      "      2       95.6578  0.0046\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "     10      176.1534  0.0034\n",
      "Restoring best model from epoch 1.\n",
      "      3       95.6578  0.0035\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m117.6578\u001b[0m  0.0070\n",
      "      4       95.6578  0.0033\n",
      "      5       95.6578  0.0024\n",
      "      2      117.6578  0.0030\n",
      "      6       95.6578  0.0025\n",
      "      3      117.6578  0.0020\n",
      "      4      117.6578  0.0029\n",
      "      7       95.6578  0.0042\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m78.0266\u001b[0m  0.0089\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      5      117.6578  0.0038\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      8       95.6578  0.0032\n",
      "      2       78.0266  0.0032\n",
      "      6      117.6578  0.0038\n",
      "      9       95.6578  0.0038\n",
      "      3       78.0266  0.0036\n",
      "     10       95.6578  0.0023\n",
      "Restoring best model from epoch 1.\n",
      "      4       78.0266  0.0022\n",
      "      7      117.6578  0.0053\n",
      "      5       78.0266  0.0032\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m196.5370\u001b[0m  0.0068\n",
      "      8      117.6578  0.0034\n",
      "      6       78.0266  0.0029\n",
      "      9      117.6578  0.0023\n",
      "      7       78.0266  0.0030\n",
      "      2      196.5370  0.0049\n",
      "     10      117.6578  0.0033\n",
      "      8       78.0266  0.0031\n",
      "      3      196.5370  0.0036\n",
      "Restoring best model from epoch 1.\n",
      "      9       78.0266  0.0037\n",
      "      4      196.5370  0.0083\n",
      "     10       78.0266  0.0055\n",
      "Restoring best model from epoch 1.\n",
      "      5      196.5370  0.0025\n",
      "      6      196.5370  0.0023\n",
      "      7      196.5370  0.0023\n",
      "      8      196.5370  0.0022\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      9      196.5370  0.0034\n",
      "     10      196.5370  0.0023\n",
      "Restoring best model from epoch 1.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m95.5950\u001b[0m  0.0043\n",
      "      2       95.5950  0.0029\n",
      "      3       95.5950  0.0037\n",
      "      4       95.5950  0.0031\n",
      "      5       95.5950  0.0022\n",
      "      6       95.5950  0.0020\n",
      "      7       95.5950  0.0020\n",
      "      8       95.5950  0.0020\n",
      "      9       95.5950  0.0024\n",
      "     10       95.5950  0.0020\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m252.5598\u001b[0m  0.0031\n",
      "      2      252.5598  0.0024\n",
      "      3      252.5598  0.0029\n",
      "      4      252.5598  0.0027\n",
      "      5      252.5598  0.0021\n",
      "      6      252.5598  0.0023\n",
      "      7      252.5598  0.0019\n",
      "      8      252.5598  0.0019\n",
      "      9      252.5598  0.0019\n",
      "     10      252.5598  0.0020\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m130.8282\u001b[0m  0.0044\n",
      "      2      130.8282  0.0037\n",
      "      3      130.8282  0.0037\n",
      "      4      130.8282  0.0035\n",
      "      5      130.8282  0.0040\n",
      "      6      130.8282  0.0034\n",
      "      7      130.8282  0.0037\n",
      "      8      130.8282  0.0057\n",
      "      9      130.8282  0.0055\n",
      "     10      130.8282  0.0090\n",
      "Restoring best model from epoch 1.\n",
      "y_train breslow final [ 2.000e+00  2.000e+00 -1.400e+01  1.400e+01  2.300e+01 -3.000e+01\n",
      "  5.600e+01 -5.600e+01  6.200e+01  6.400e+01  6.500e+01  7.600e+01\n",
      "  8.200e+01  8.600e+01  8.900e+01  9.000e+01 -9.200e+01  9.400e+01\n",
      "  9.500e+01 -1.020e+02 -1.050e+02  1.070e+02  1.080e+02 -1.100e+02\n",
      " -1.110e+02  1.120e+02 -1.220e+02  1.290e+02  1.290e+02 -1.340e+02\n",
      "  1.420e+02  1.440e+02 -1.460e+02  1.510e+02 -1.560e+02  1.600e+02\n",
      "  1.710e+02 -1.810e+02  1.820e+02  1.830e+02  1.910e+02  1.940e+02\n",
      "  1.940e+02  1.980e+02  2.050e+02 -2.100e+02  2.150e+02 -2.160e+02\n",
      "  2.170e+02  2.180e+02 -2.250e+02 -2.320e+02  2.430e+02  2.530e+02\n",
      "  2.560e+02  2.590e+02  2.610e+02  2.680e+02  2.740e+02  2.760e+02\n",
      "  2.760e+02 -2.780e+02  2.790e+02  2.810e+02  2.820e+02  2.830e+02\n",
      "  2.840e+02 -2.920e+02  2.920e+02  2.940e+02  2.950e+02  3.170e+02\n",
      "  3.170e+02 -3.180e+02 -3.210e+02  3.270e+02  3.270e+02  3.300e+02\n",
      "  3.340e+02  3.360e+02  3.370e+02  3.410e+02  3.420e+02 -3.510e+02\n",
      "  3.510e+02  3.520e+02 -3.540e+02  3.570e+02 -3.580e+02  3.590e+02\n",
      "  3.600e+02 -3.600e+02  3.610e+02 -3.620e+02  3.660e+02 -3.670e+02\n",
      " -3.680e+02  3.710e+02  3.770e+02  3.770e+02 -3.780e+02  3.790e+02\n",
      " -3.790e+02  3.800e+02  3.840e+02 -3.840e+02  3.850e+02  3.850e+02\n",
      " -3.860e+02  3.870e+02 -3.870e+02 -3.890e+02 -3.920e+02 -3.920e+02\n",
      " -3.920e+02  3.930e+02  3.970e+02 -4.000e+02 -4.030e+02  4.030e+02\n",
      "  4.060e+02  4.070e+02 -4.070e+02 -4.120e+02 -4.140e+02  4.150e+02\n",
      " -4.170e+02  4.210e+02 -4.210e+02  4.270e+02  4.300e+02  4.360e+02\n",
      " -4.370e+02 -4.400e+02 -4.410e+02 -4.430e+02  4.460e+02  4.490e+02\n",
      "  4.510e+02  4.530e+02  4.560e+02  4.590e+02  4.620e+02  4.640e+02\n",
      " -4.660e+02 -4.690e+02  4.790e+02 -4.790e+02  4.840e+02 -4.850e+02\n",
      "  4.890e+02 -4.910e+02  4.930e+02  4.950e+02 -4.990e+02  5.060e+02\n",
      " -5.090e+02 -5.140e+02 -5.160e+02 -5.180e+02 -5.200e+02  5.210e+02\n",
      "  5.220e+02  5.260e+02  5.300e+02 -5.420e+02  5.430e+02  5.450e+02\n",
      "  5.460e+02  5.460e+02 -5.470e+02  5.480e+02 -5.490e+02 -5.520e+02\n",
      " -5.590e+02  5.640e+02 -5.680e+02  5.720e+02 -5.730e+02 -5.760e+02\n",
      "  5.770e+02  5.770e+02 -5.780e+02 -5.790e+02  5.800e+02  5.840e+02\n",
      " -5.860e+02 -6.000e+02  6.010e+02 -6.030e+02  6.060e+02 -6.060e+02\n",
      " -6.160e+02 -6.160e+02 -6.190e+02 -6.220e+02 -6.220e+02 -6.240e+02\n",
      "  6.240e+02  6.250e+02  6.310e+02 -6.370e+02 -6.390e+02 -6.410e+02\n",
      "  6.410e+02 -6.440e+02 -6.450e+02 -6.450e+02 -6.460e+02 -6.530e+02\n",
      " -6.580e+02  6.630e+02  6.660e+02 -6.670e+02 -6.700e+02  6.750e+02\n",
      " -6.790e+02  6.800e+02 -6.820e+02 -6.850e+02 -6.870e+02 -6.930e+02\n",
      " -6.950e+02  6.950e+02 -7.010e+02 -7.040e+02 -7.070e+02 -7.100e+02\n",
      " -7.140e+02 -7.220e+02 -7.250e+02  7.270e+02  7.390e+02 -7.420e+02\n",
      " -7.590e+02 -7.590e+02 -7.590e+02  7.630e+02  7.730e+02 -7.820e+02\n",
      " -7.830e+02 -7.960e+02 -7.970e+02 -7.990e+02  8.060e+02 -8.170e+02\n",
      "  8.230e+02 -8.270e+02 -8.330e+02  8.360e+02 -8.360e+02 -8.500e+02\n",
      " -8.510e+02 -8.520e+02  8.530e+02 -8.550e+02  8.620e+02 -8.660e+02\n",
      " -8.750e+02  8.820e+02 -8.890e+02 -8.930e+02 -8.950e+02 -8.960e+02\n",
      " -8.970e+02 -9.100e+02  9.150e+02 -9.180e+02 -9.300e+02 -9.330e+02\n",
      "  9.410e+02 -9.470e+02 -9.500e+02 -9.540e+02 -9.720e+02  9.800e+02\n",
      "  9.850e+02 -9.870e+02  9.880e+02  9.930e+02 -9.930e+02 -9.950e+02\n",
      " -9.970e+02  9.980e+02 -1.000e+03 -1.021e+03 -1.022e+03 -1.025e+03\n",
      " -1.027e+03 -1.030e+03 -1.036e+03  1.037e+03 -1.038e+03 -1.045e+03\n",
      " -1.050e+03 -1.057e+03 -1.065e+03 -1.066e+03 -1.075e+03 -1.077e+03\n",
      "  1.079e+03  1.081e+03  1.093e+03 -1.095e+03 -1.117e+03 -1.125e+03\n",
      " -1.131e+03  1.133e+03  1.134e+03 -1.138e+03 -1.143e+03 -1.147e+03\n",
      " -1.152e+03 -1.157e+03 -1.168e+03 -1.172e+03 -1.179e+03 -1.183e+03\n",
      " -1.190e+03 -1.191e+03  1.202e+03 -1.218e+03 -1.224e+03 -1.245e+03\n",
      " -1.253e+03 -1.265e+03 -1.273e+03 -1.274e+03 -1.278e+03 -1.288e+03\n",
      "  1.289e+03 -1.311e+03 -1.315e+03 -1.346e+03 -1.353e+03 -1.358e+03\n",
      " -1.368e+03  1.394e+03 -1.409e+03 -1.409e+03 -1.425e+03  1.430e+03\n",
      " -1.435e+03 -1.440e+03 -1.443e+03  1.459e+03 -1.460e+03 -1.466e+03\n",
      "  1.466e+03 -1.472e+03 -1.478e+03 -1.483e+03  1.504e+03 -1.508e+03\n",
      " -1.512e+03 -1.523e+03 -1.527e+03 -1.555e+03 -1.559e+03 -1.560e+03\n",
      " -1.584e+03  1.591e+03 -1.593e+03 -1.628e+03  1.641e+03 -1.660e+03\n",
      " -1.663e+03 -1.663e+03 -1.665e+03 -1.686e+03 -1.690e+03 -1.699e+03\n",
      "  1.718e+03 -1.721e+03 -1.724e+03  1.732e+03 -1.737e+03  1.748e+03\n",
      "  1.762e+03  1.838e+03 -1.840e+03 -1.897e+03 -1.899e+03 -1.971e+03\n",
      "  1.972e+03 -1.995e+03 -2.016e+03  2.064e+03 -2.143e+03 -2.161e+03\n",
      " -2.169e+03 -2.182e+03 -2.239e+03 -2.265e+03 -2.298e+03  2.319e+03\n",
      " -2.327e+03 -2.359e+03 -2.562e+03  2.570e+03  2.703e+03 -2.727e+03\n",
      "  2.741e+03 -2.784e+03  2.900e+03  3.059e+03 -3.221e+03  3.314e+03\n",
      " -3.381e+03 -3.930e+03 -4.241e+03 -4.282e+03  4.760e+03  5.152e+03\n",
      " -5.480e+03]\n",
      "Concordance Index 0.552373122389121\n",
      "Integrated Brier Score: 0.18525625605790097\n",
      "y_train breslow final [ 2.000e+00  2.000e+00 -1.400e+01  1.400e+01  2.300e+01 -3.000e+01\n",
      "  5.600e+01 -5.600e+01  6.200e+01  6.400e+01  6.500e+01  7.600e+01\n",
      "  8.200e+01  8.600e+01  8.900e+01  9.000e+01 -9.200e+01  9.400e+01\n",
      "  9.500e+01 -1.020e+02 -1.050e+02  1.070e+02  1.080e+02 -1.100e+02\n",
      " -1.110e+02  1.120e+02 -1.220e+02  1.290e+02  1.290e+02 -1.340e+02\n",
      "  1.420e+02  1.440e+02 -1.460e+02  1.510e+02 -1.560e+02  1.600e+02\n",
      "  1.710e+02 -1.810e+02  1.820e+02  1.830e+02  1.910e+02  1.940e+02\n",
      "  1.940e+02  1.980e+02  2.050e+02 -2.100e+02  2.150e+02 -2.160e+02\n",
      "  2.170e+02  2.180e+02 -2.250e+02 -2.320e+02  2.430e+02  2.530e+02\n",
      "  2.560e+02  2.590e+02  2.610e+02  2.680e+02  2.740e+02  2.760e+02\n",
      "  2.760e+02 -2.780e+02  2.790e+02  2.810e+02  2.820e+02  2.830e+02\n",
      "  2.840e+02 -2.920e+02  2.920e+02  2.940e+02  2.950e+02  3.170e+02\n",
      "  3.170e+02 -3.180e+02 -3.210e+02  3.270e+02  3.270e+02  3.300e+02\n",
      "  3.340e+02  3.360e+02  3.370e+02  3.410e+02  3.420e+02 -3.510e+02\n",
      "  3.510e+02  3.520e+02 -3.540e+02  3.570e+02 -3.580e+02  3.590e+02\n",
      "  3.600e+02 -3.600e+02  3.610e+02 -3.620e+02  3.660e+02 -3.670e+02\n",
      " -3.680e+02  3.710e+02  3.770e+02  3.770e+02 -3.780e+02  3.790e+02\n",
      " -3.790e+02  3.800e+02  3.840e+02 -3.840e+02  3.850e+02  3.850e+02\n",
      " -3.860e+02  3.870e+02 -3.870e+02 -3.890e+02 -3.920e+02 -3.920e+02\n",
      " -3.920e+02  3.930e+02  3.970e+02 -4.000e+02 -4.030e+02  4.030e+02\n",
      "  4.060e+02  4.070e+02 -4.070e+02 -4.120e+02 -4.140e+02  4.150e+02\n",
      " -4.170e+02  4.210e+02 -4.210e+02  4.270e+02  4.300e+02  4.360e+02\n",
      " -4.370e+02 -4.400e+02 -4.410e+02 -4.430e+02  4.460e+02  4.490e+02\n",
      "  4.510e+02  4.530e+02  4.560e+02  4.590e+02  4.620e+02  4.640e+02\n",
      " -4.660e+02 -4.690e+02  4.790e+02 -4.790e+02  4.840e+02 -4.850e+02\n",
      "  4.890e+02 -4.910e+02  4.930e+02  4.950e+02 -4.990e+02  5.060e+02\n",
      " -5.090e+02 -5.140e+02 -5.160e+02 -5.180e+02 -5.200e+02  5.210e+02\n",
      "  5.220e+02  5.260e+02  5.300e+02 -5.420e+02  5.430e+02  5.450e+02\n",
      "  5.460e+02  5.460e+02 -5.470e+02  5.480e+02 -5.490e+02 -5.520e+02\n",
      " -5.590e+02  5.640e+02 -5.680e+02  5.720e+02 -5.730e+02 -5.760e+02\n",
      "  5.770e+02  5.770e+02 -5.780e+02 -5.790e+02  5.800e+02  5.840e+02\n",
      " -5.860e+02 -6.000e+02  6.010e+02 -6.030e+02  6.060e+02 -6.060e+02\n",
      " -6.160e+02 -6.160e+02 -6.190e+02 -6.220e+02 -6.220e+02 -6.240e+02\n",
      "  6.240e+02  6.250e+02  6.310e+02 -6.370e+02 -6.390e+02 -6.410e+02\n",
      "  6.410e+02 -6.440e+02 -6.450e+02 -6.450e+02 -6.460e+02 -6.530e+02\n",
      " -6.580e+02  6.630e+02  6.660e+02 -6.670e+02 -6.700e+02  6.750e+02\n",
      " -6.790e+02  6.800e+02 -6.820e+02 -6.850e+02 -6.870e+02 -6.930e+02\n",
      " -6.950e+02  6.950e+02 -7.010e+02 -7.040e+02 -7.070e+02 -7.100e+02\n",
      " -7.140e+02 -7.220e+02 -7.250e+02  7.270e+02  7.390e+02 -7.420e+02\n",
      " -7.590e+02 -7.590e+02 -7.590e+02  7.630e+02  7.730e+02 -7.820e+02\n",
      " -7.830e+02 -7.960e+02 -7.970e+02 -7.990e+02  8.060e+02 -8.170e+02\n",
      "  8.230e+02 -8.270e+02 -8.330e+02  8.360e+02 -8.360e+02 -8.500e+02\n",
      " -8.510e+02 -8.520e+02  8.530e+02 -8.550e+02  8.620e+02 -8.660e+02\n",
      " -8.750e+02  8.820e+02 -8.890e+02 -8.930e+02 -8.950e+02 -8.960e+02\n",
      " -8.970e+02 -9.100e+02  9.150e+02 -9.180e+02 -9.300e+02 -9.330e+02\n",
      "  9.410e+02 -9.470e+02 -9.500e+02 -9.540e+02 -9.720e+02  9.800e+02\n",
      "  9.850e+02 -9.870e+02  9.880e+02  9.930e+02 -9.930e+02 -9.950e+02\n",
      " -9.970e+02  9.980e+02 -1.000e+03 -1.021e+03 -1.022e+03 -1.025e+03\n",
      " -1.027e+03 -1.030e+03 -1.036e+03  1.037e+03 -1.038e+03 -1.045e+03\n",
      " -1.050e+03 -1.057e+03 -1.065e+03 -1.066e+03 -1.075e+03 -1.077e+03\n",
      "  1.079e+03  1.081e+03  1.093e+03 -1.095e+03 -1.117e+03 -1.125e+03\n",
      " -1.131e+03  1.133e+03  1.134e+03 -1.138e+03 -1.143e+03 -1.147e+03\n",
      " -1.152e+03 -1.157e+03 -1.168e+03 -1.172e+03 -1.179e+03 -1.183e+03\n",
      " -1.190e+03 -1.191e+03  1.202e+03 -1.218e+03 -1.224e+03 -1.245e+03\n",
      " -1.253e+03 -1.265e+03 -1.273e+03 -1.274e+03 -1.278e+03 -1.288e+03\n",
      "  1.289e+03 -1.311e+03 -1.315e+03 -1.346e+03 -1.353e+03 -1.358e+03\n",
      " -1.368e+03  1.394e+03 -1.409e+03 -1.409e+03 -1.425e+03  1.430e+03\n",
      " -1.435e+03 -1.440e+03 -1.443e+03  1.459e+03 -1.460e+03 -1.466e+03\n",
      "  1.466e+03 -1.472e+03 -1.478e+03 -1.483e+03  1.504e+03 -1.508e+03\n",
      " -1.512e+03 -1.523e+03 -1.527e+03 -1.555e+03 -1.559e+03 -1.560e+03\n",
      " -1.584e+03  1.591e+03 -1.593e+03 -1.628e+03  1.641e+03 -1.660e+03\n",
      " -1.663e+03 -1.663e+03 -1.665e+03 -1.686e+03 -1.690e+03 -1.699e+03\n",
      "  1.718e+03 -1.721e+03 -1.724e+03  1.732e+03 -1.737e+03  1.748e+03\n",
      "  1.762e+03  1.838e+03 -1.840e+03 -1.897e+03 -1.899e+03 -1.971e+03\n",
      "  1.972e+03 -1.995e+03 -2.016e+03  2.064e+03 -2.143e+03 -2.161e+03\n",
      " -2.169e+03 -2.182e+03 -2.239e+03 -2.265e+03 -2.298e+03  2.319e+03\n",
      " -2.327e+03 -2.359e+03 -2.562e+03  2.570e+03  2.703e+03 -2.727e+03\n",
      "  2.741e+03 -2.784e+03  2.900e+03  3.059e+03 -3.221e+03  3.314e+03\n",
      " -3.381e+03 -3.930e+03 -4.241e+03 -4.282e+03  4.760e+03  5.152e+03\n",
      " -5.480e+03]\n",
      "durations 11.0 6417.0\n",
      "Concordance Index 0.5112781954887218\n",
      "Integrated Brier Score: 0.21270593121875642\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m87.8816\u001b[0m  0.0071\n",
      "      2       87.8816  0.0042\n",
      "      3       87.8816  0.0029\n",
      "      4       87.8816  0.0021\n",
      "      5       87.8816  0.0022\n",
      "      6       87.8816  0.0021\n",
      "      7       87.8816  0.0020\n",
      "      8       87.8816  0.0020\n",
      "      9       87.8816  0.0020\n",
      "     10       87.8816  0.0034\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m102.4996\u001b[0m  0.0041\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m76.1810\u001b[0m  0.0033\n",
      "      2      102.4996  0.0027\n",
      "      2       76.1810  0.0024\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m99.1225\u001b[0m  0.0037\n",
      "      3      102.4996  0.0024\n",
      "      3       76.1810  0.0020\n",
      "      4       76.1810  0.0020\n",
      "      4      102.4996  0.0023\n",
      "      2       99.1225  0.0025\n",
      "      5       76.1810  0.0019\n",
      "      5      102.4996  0.0020\n",
      "      3       99.1225  0.0021\n",
      "      6       76.1810  0.0021\n",
      "      6      102.4996  0.0022\n",
      "      4       99.1225  0.0023\n",
      "      7       76.1810  0.0020\n",
      "      7      102.4996  0.0020\n",
      "      5       99.1225  0.0021\n",
      "      8       76.1810  0.0019\n",
      "      8      102.4996  0.0020\n",
      "      6       99.1225  0.0019\n",
      "      9       76.1810  0.0019\n",
      "      9      102.4996  0.0020\n",
      "      7       99.1225  0.0019\n",
      "     10      102.4996  0.0020\n",
      "     10       76.1810  0.0024\n",
      "Restoring best model from epoch 1.\n",
      "Restoring best model from epoch 1.\n",
      "      8       99.1225  0.0039\n",
      "      9       99.1225  0.0044\n",
      "     10       99.1225  0.0043\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m223.6412\u001b[0m  0.0102\n",
      "      2      223.6412  0.0046\n",
      "      3      223.6412  0.0066\n",
      "      4      223.6412  0.0039\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      5      223.6412  0.0039\n",
      "      6      223.6412  0.0030\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing optimizer.\n",
      "      7      223.6412  0.0020\n",
      "      8      223.6412  0.0020\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m100.4782\u001b[0m  0.0035\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m163.0410\u001b[0m  0.0032\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m115.4507\u001b[0m  0.0038\n",
      "      2      163.0410  0.0021\n",
      "      9      223.6412  0.0042\n",
      "      2      100.4782  0.0029\n",
      "      2      115.4507  0.0024\n",
      "      3      163.0410  0.0020\n",
      "      3      115.4507  0.0023\n",
      "     10      223.6412  0.0035\n",
      "      3      100.4782  0.0026\n",
      "Restoring best model from epoch 1.\n",
      "      4      163.0410  0.0022\n",
      "      4      100.4782  0.0021\n",
      "      4      115.4507  0.0027\n",
      "      5      163.0410  0.0030\n",
      "      5      100.4782  0.0020\n",
      "      5      115.4507  0.0027\n",
      "      6      163.0410  0.0024\n",
      "      6      100.4782  0.0022\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      7      163.0410  0.0021\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      6      115.4507  0.0030\n",
      "      7      100.4782  0.0036\n",
      "      8      163.0410  0.0022\n",
      "      7      115.4507  0.0021\n",
      "      9      163.0410  0.0021\n",
      "      8      100.4782  0.0026\n",
      "      8      115.4507  0.0028\n",
      "     10      163.0410  0.0026\n",
      "Restoring best model from epoch 1.\n",
      "      9      115.4507  0.0028\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m257.2838\u001b[0m  0.0055\n",
      "      9      100.4782  0.0059\n",
      "     10      115.4507  0.0035\n",
      "Restoring best model from epoch 1.\n",
      "      2      257.2838  0.0032\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "     10      100.4782  0.0027\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Restoring best model from epoch 1.\n",
      "      3      257.2838  0.0043\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m243.2712\u001b[0m  0.0037\n",
      "      4      257.2838  0.0022\n",
      "      2      243.2712  0.0027\n",
      "      5      257.2838  0.0024\n",
      "      6      257.2838  0.0026\n",
      "      3      243.2712  0.0031\n",
      "      7      257.2838  0.0022\n",
      "      4      243.2712  0.0023\n",
      "      8      257.2838  0.0020\n",
      "      5      243.2712  0.0024\n",
      "      9      257.2838  0.0019\n",
      "      6      243.2712  0.0023\n",
      "      7      243.2712  0.0021\n",
      "     10      257.2838  0.0044\n",
      "Restoring best model from epoch 1.\n",
      "      8      243.2712  0.0034\n",
      "      9      243.2712  0.0039\n",
      "     10      243.2712  0.0024\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m140.2177\u001b[0m  0.0041\n",
      "      2      140.2177  0.0037\n",
      "      3      140.2177  0.0063\n",
      "      4      140.2177  0.0038\n",
      "      5      140.2177  0.0042\n",
      "      6      140.2177  0.0034\n",
      "      7      140.2177  0.0040\n",
      "      8      140.2177  0.0063\n",
      "      9      140.2177  0.0077\n",
      "     10      140.2177  0.0065\n",
      "Restoring best model from epoch 1.\n",
      "y_train breslow final [  -11.    11.   -14.    14.    23.   -30.    32.   -56.    62.    64.\n",
      "    69.    76.   -77.    82.    86.    89.    90.    95.  -105.   107.\n",
      "   108.  -110.   112.  -122.   126.   129.   129.   130.   142.   144.\n",
      "   144.  -146.   151.  -156.   166.   171.   173.  -181.   182.   183.\n",
      "   185.  -187.   191.   197.   198.   205.   209.  -210.   215.   215.\n",
      "  -216.  -216.   217.   217.   222.  -225.  -232.  -234.  -242.  -253.\n",
      "   253.   256.   259.   261.   274.   276.   276.   279.   281.   282.\n",
      "   283.  -292.   292.   295.  -311.   317.  -321.   321.   327.   330.\n",
      "   334.   336.   337.   341.   342.  -345.   348.  -351.   352.   353.\n",
      "  -354.   357.  -358.   359.   360.  -362.   366.  -367.   371.  -376.\n",
      "   377.  -379.   380.   384.   385.   385.  -386.   387.  -387.  -389.\n",
      "  -392.  -392.  -392.  -393.   395.   395.   397.  -400.  -403.   403.\n",
      "   406.   406.   407.  -407.  -412.  -414.   415.  -417.  -424.   424.\n",
      "   427.   430.   436.  -437.  -440.  -441.   446.   449.   451.   456.\n",
      "  -458.   459.  -461.   462.   464.  -471.   472.  -479.   480.  -485.\n",
      "   487.   489.   493.  -499.   506.  -509.  -514.  -520.   521.   521.\n",
      "   522.   530.  -539.  -542.  -543.   545.   546.   548.  -549.  -552.\n",
      "  -559.   560.   564.  -573.  -576.   577.  -578.  -579.   580.   584.\n",
      "  -586.  -594.  -600.   601.   602.  -603.  -606.  -616.  -616.  -622.\n",
      "  -622.  -623.  -623.  -624.   624.  -625.  -635.  -637.  -639.  -641.\n",
      "  -641.   641.  -644.  -645.  -645.  -646.  -653.   654.  -658.   663.\n",
      "  -667.   675.  -685.  -687.  -695.   695.  -701.  -701.  -704.  -710.\n",
      "  -713.  -714.  -717.  -722.  -725.   727.   739.  -750.  -754.  -759.\n",
      "  -759.  -759.   763.  -770.   773.  -773.  -782.  -783.   789.  -796.\n",
      "   804.   806.   823.  -827.  -833.   836.  -836.   839.  -850.  -852.\n",
      "   853.  -855.   862.  -866.  -875.   882.  -889.  -895.  -896.  -897.\n",
      "  -904.  -906.  -911.  -913.  -914.   915.  -918.   927.  -930.  -933.\n",
      "   941.  -947.  -954.  -964.  -972.   985.  -987.   988.  -993.  -995.\n",
      "   998. -1000. -1022. -1025. -1027. -1030.  1037. -1045. -1050. -1057.\n",
      " -1065. -1066. -1075. -1077.  1079.  1081.  1090.  1093. -1095. -1117.\n",
      " -1120. -1125. -1131.  1134. -1143. -1147. -1152. -1157. -1168. -1183.\n",
      " -1190. -1191. -1218. -1253. -1265. -1274. -1278. -1288.  1289. -1309.\n",
      " -1311. -1315. -1353. -1358.  1394.  1398. -1399. -1413. -1428.  1430.\n",
      " -1430. -1435. -1440. -1443.  1459. -1460. -1461. -1466. -1472. -1478.\n",
      "  1504. -1506. -1512. -1521. -1523. -1555. -1559. -1560. -1584. -1584.\n",
      " -1586.  1591. -1593.  1641. -1660. -1663. -1665.  1671. -1686. -1690.\n",
      " -1699.  1718. -1721. -1722. -1724. -1737.  1748.  1762. -1777. -1778.\n",
      "  1838. -1897. -1899. -1995.  2002. -2016. -2016.  2083.  2120. -2133.\n",
      " -2143. -2161.  2166. -2169. -2182. -2239. -2265. -2298.  2319. -2327.\n",
      " -2347. -2359. -2437. -2562.  2570. -2641.  2703.  2717.  2741. -2784.\n",
      " -2886.  2900.  3059. -3221. -3270.  3314. -3381. -3981. -4282.  4680.\n",
      "  4760.  4856.  5152. -5252.  6417.]\n",
      "Concordance Index 0.5290154264972777\n",
      "Integrated Brier Score: 0.1748608282633937\n",
      "y_train breslow final [  -11.    11.   -14.    14.    23.   -30.    32.   -56.    62.    64.\n",
      "    69.    76.   -77.    82.    86.    89.    90.    95.  -105.   107.\n",
      "   108.  -110.   112.  -122.   126.   129.   129.   130.   142.   144.\n",
      "   144.  -146.   151.  -156.   166.   171.   173.  -181.   182.   183.\n",
      "   185.  -187.   191.   197.   198.   205.   209.  -210.   215.   215.\n",
      "  -216.  -216.   217.   217.   222.  -225.  -232.  -234.  -242.  -253.\n",
      "   253.   256.   259.   261.   274.   276.   276.   279.   281.   282.\n",
      "   283.  -292.   292.   295.  -311.   317.  -321.   321.   327.   330.\n",
      "   334.   336.   337.   341.   342.  -345.   348.  -351.   352.   353.\n",
      "  -354.   357.  -358.   359.   360.  -362.   366.  -367.   371.  -376.\n",
      "   377.  -379.   380.   384.   385.   385.  -386.   387.  -387.  -389.\n",
      "  -392.  -392.  -392.  -393.   395.   395.   397.  -400.  -403.   403.\n",
      "   406.   406.   407.  -407.  -412.  -414.   415.  -417.  -424.   424.\n",
      "   427.   430.   436.  -437.  -440.  -441.   446.   449.   451.   456.\n",
      "  -458.   459.  -461.   462.   464.  -471.   472.  -479.   480.  -485.\n",
      "   487.   489.   493.  -499.   506.  -509.  -514.  -520.   521.   521.\n",
      "   522.   530.  -539.  -542.  -543.   545.   546.   548.  -549.  -552.\n",
      "  -559.   560.   564.  -573.  -576.   577.  -578.  -579.   580.   584.\n",
      "  -586.  -594.  -600.   601.   602.  -603.  -606.  -616.  -616.  -622.\n",
      "  -622.  -623.  -623.  -624.   624.  -625.  -635.  -637.  -639.  -641.\n",
      "  -641.   641.  -644.  -645.  -645.  -646.  -653.   654.  -658.   663.\n",
      "  -667.   675.  -685.  -687.  -695.   695.  -701.  -701.  -704.  -710.\n",
      "  -713.  -714.  -717.  -722.  -725.   727.   739.  -750.  -754.  -759.\n",
      "  -759.  -759.   763.  -770.   773.  -773.  -782.  -783.   789.  -796.\n",
      "   804.   806.   823.  -827.  -833.   836.  -836.   839.  -850.  -852.\n",
      "   853.  -855.   862.  -866.  -875.   882.  -889.  -895.  -896.  -897.\n",
      "  -904.  -906.  -911.  -913.  -914.   915.  -918.   927.  -930.  -933.\n",
      "   941.  -947.  -954.  -964.  -972.   985.  -987.   988.  -993.  -995.\n",
      "   998. -1000. -1022. -1025. -1027. -1030.  1037. -1045. -1050. -1057.\n",
      " -1065. -1066. -1075. -1077.  1079.  1081.  1090.  1093. -1095. -1117.\n",
      " -1120. -1125. -1131.  1134. -1143. -1147. -1152. -1157. -1168. -1183.\n",
      " -1190. -1191. -1218. -1253. -1265. -1274. -1278. -1288.  1289. -1309.\n",
      " -1311. -1315. -1353. -1358.  1394.  1398. -1399. -1413. -1428.  1430.\n",
      " -1430. -1435. -1440. -1443.  1459. -1460. -1461. -1466. -1472. -1478.\n",
      "  1504. -1506. -1512. -1521. -1523. -1555. -1559. -1560. -1584. -1584.\n",
      " -1586.  1591. -1593.  1641. -1660. -1663. -1665.  1671. -1686. -1690.\n",
      " -1699.  1718. -1721. -1722. -1724. -1737.  1748.  1762. -1777. -1778.\n",
      "  1838. -1897. -1899. -1995.  2002. -2016. -2016.  2083.  2120. -2133.\n",
      " -2143. -2161.  2166. -2169. -2182. -2239. -2265. -2298.  2319. -2327.\n",
      " -2347. -2359. -2437. -2562.  2570. -2641.  2703.  2717.  2741. -2784.\n",
      " -2886.  2900.  3059. -3221. -3270.  3314. -3381. -3981. -4282.  4680.\n",
      "  4760.  4856.  5152. -5252.  6417.]\n",
      "durations 2.0 5480.0\n",
      "Concordance Index 0.4785788923719958\n",
      "Integrated Brier Score: 0.2200171825229733\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m70.8828\u001b[0m  0.0032\n",
      "      2       70.8828  0.0031\n",
      "      3       70.8828  0.0022\n",
      "      4       70.8828  0.0020\n",
      "      5       70.8828  0.0045\n",
      "      6       70.8828  0.0037\n",
      "      7       70.8828  0.0019\n",
      "      8       70.8828  0.0019\n",
      "      9       70.8828  0.0059\n",
      "     10       70.8828  0.0038\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m163.6536\u001b[0m  0.0045\n",
      "      2      163.6536  0.0028\n",
      "      3      163.6536  0.0020\n",
      "      4      163.6536  0.0048\n",
      "      5      163.6536  0.0046\n",
      "      6      163.6536  0.0041\n",
      "      7      163.6536  0.0034\n",
      "      8      163.6536  0.0027\n",
      "      9      163.6536  0.0026\n",
      "     10      163.6536  0.0022\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m105.0456\u001b[0m  0.0041\n",
      "      2      105.0456  0.0029\n",
      "      3      105.0456  0.0023\n",
      "      4      105.0456  0.0036\n",
      "      5      105.0456  0.0056\n",
      "      6      105.0456  0.0031\n",
      "      7      105.0456  0.0044\n",
      "      8      105.0456  0.0036\n",
      "      9      105.0456  0.0069\n",
      "     10      105.0456  0.0057\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m103.2281\u001b[0m  0.0043\n",
      "      2      103.2281  0.0025\n",
      "      3      103.2281  0.0031\n",
      "      4      103.2281  0.0026\n",
      "      5      103.2281  0.0022\n",
      "      6      103.2281  0.0026\n",
      "      7      103.2281  0.0023\n",
      "      8      103.2281  0.0022\n",
      "      9      103.2281  0.0065\n",
      "     10      103.2281  0.0026\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m227.8382\u001b[0m  0.0030\n",
      "      2      227.8382  0.0021\n",
      "      3      227.8382  0.0020\n",
      "      4      227.8382  0.0019\n",
      "      5      227.8382  0.0019\n",
      "      6      227.8382  0.0019\n",
      "      7      227.8382  0.0026\n",
      "      8      227.8382  0.0035\n",
      "      9      227.8382  0.0021\n",
      "     10      227.8382  0.0020\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m166.6253\u001b[0m  0.0030\n",
      "      2      166.6253  0.0021\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      3      166.6253  0.0021\n",
      "      4      166.6253  0.0023\n",
      "      5      166.6253  0.0019\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m235.4548\u001b[0m  0.0034\n",
      "      6      166.6253  0.0019\n",
      "      2      235.4548  0.0024\n",
      "      7      166.6253  0.0019\n",
      "      3      235.4548  0.0023\n",
      "      8      166.6253  0.0020\n",
      "      4      235.4548  0.0023\n",
      "      9      166.6253  0.0021\n",
      "      5      235.4548  0.0022\n",
      "     10      166.6253  0.0025\n",
      "Restoring best model from epoch 1.\n",
      "      6      235.4548  0.0021\n",
      "      7      235.4548  0.0020\n",
      "      8      235.4548  0.0019\n",
      "      9      235.4548  0.0019\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "     10      235.4548  0.0019\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m107.5689\u001b[0m  0.0094\n",
      "      2      107.5689  0.0036\n",
      "      3      107.5689  0.0029\n",
      "      4      107.5689  0.0022\n",
      "      5      107.5689  0.0056\n",
      "      6      107.5689  0.0037\n",
      "      7      107.5689  0.0023\n",
      "      8      107.5689  0.0021\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      9      107.5689  0.0035\n",
      "     10      107.5689  0.0023\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Restoring best model from epoch 1.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m199.0015\u001b[0m  0.0035\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2      199.0015  0.0030\n",
      "      3      199.0015  0.0028\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m79.9892\u001b[0m  0.0038\n",
      "      4      199.0015  0.0020\n",
      "      2       79.9892  0.0029\n",
      "      5      199.0015  0.0024\n",
      "      3       79.9892  0.0021\n",
      "      6      199.0015  0.0023\n",
      "      4       79.9892  0.0022\n",
      "      7      199.0015  0.0021\n",
      "      5       79.9892  0.0023\n",
      "      8      199.0015  0.0019\n",
      "      9      199.0015  0.0019\n",
      "      6       79.9892  0.0026\n",
      "     10      199.0015  0.0024\n",
      "Restoring best model from epoch 1.\n",
      "      7       79.9892  0.0023\n",
      "      8       79.9892  0.0022\n",
      "      9       79.9892  0.0020\n",
      "     10       79.9892  0.0019\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m103.1230\u001b[0m  0.0047\n",
      "      2      103.1230  0.0042\n",
      "      3      103.1230  0.0039\n",
      "      4      103.1230  0.0036\n",
      "      5      103.1230  0.0034\n",
      "      6      103.1230  0.0056\n",
      "      7      103.1230  0.0080\n",
      "      8      103.1230  0.0070\n",
      "      9      103.1230  0.0055\n",
      "     10      103.1230  0.0047\n",
      "Restoring best model from epoch 1.\n",
      "y_train breslow final [ 2.000e+00  2.000e+00 -1.100e+01  1.100e+01 -1.400e+01  2.300e+01\n",
      " -3.000e+01  3.200e+01  5.600e+01 -5.600e+01  6.400e+01  6.500e+01\n",
      "  6.900e+01  7.600e+01 -7.700e+01  8.600e+01  8.900e+01 -9.200e+01\n",
      "  9.400e+01  9.500e+01 -1.020e+02  1.080e+02 -1.100e+02 -1.110e+02\n",
      " -1.220e+02  1.260e+02  1.290e+02  1.300e+02 -1.340e+02  1.440e+02\n",
      "  1.440e+02 -1.460e+02  1.510e+02  1.600e+02  1.660e+02  1.710e+02\n",
      "  1.730e+02  1.830e+02  1.850e+02 -1.870e+02  1.910e+02  1.940e+02\n",
      "  1.940e+02  1.970e+02  1.980e+02  2.090e+02 -2.100e+02  2.150e+02\n",
      "  2.150e+02 -2.160e+02 -2.160e+02  2.170e+02  2.180e+02  2.220e+02\n",
      " -2.320e+02 -2.340e+02 -2.420e+02  2.430e+02 -2.530e+02  2.560e+02\n",
      "  2.590e+02  2.680e+02  2.740e+02  2.760e+02  2.760e+02 -2.780e+02\n",
      "  2.810e+02  2.820e+02  2.830e+02  2.840e+02 -2.920e+02  2.920e+02\n",
      "  2.940e+02 -3.110e+02  3.170e+02  3.170e+02 -3.180e+02 -3.210e+02\n",
      "  3.210e+02  3.270e+02  3.300e+02  3.340e+02  3.360e+02  3.370e+02\n",
      "  3.410e+02  3.420e+02 -3.450e+02  3.480e+02 -3.510e+02  3.510e+02\n",
      "  3.520e+02  3.530e+02 -3.580e+02  3.590e+02  3.600e+02 -3.600e+02\n",
      "  3.610e+02 -3.620e+02  3.660e+02 -3.680e+02 -3.760e+02  3.770e+02\n",
      "  3.770e+02 -3.780e+02  3.790e+02  3.840e+02 -3.840e+02  3.850e+02\n",
      "  3.850e+02  3.870e+02 -3.870e+02 -3.920e+02 -3.920e+02  3.930e+02\n",
      " -3.930e+02  3.950e+02  3.950e+02  3.970e+02 -4.030e+02  4.030e+02\n",
      "  4.030e+02  4.060e+02  4.060e+02  4.070e+02 -4.070e+02 -4.120e+02\n",
      "  4.150e+02 -4.170e+02  4.210e+02 -4.210e+02 -4.240e+02  4.240e+02\n",
      "  4.270e+02  4.300e+02 -4.370e+02 -4.400e+02 -4.430e+02  4.490e+02\n",
      "  4.510e+02  4.530e+02 -4.580e+02 -4.610e+02  4.620e+02  4.640e+02\n",
      " -4.660e+02 -4.690e+02 -4.710e+02  4.720e+02  4.790e+02 -4.790e+02\n",
      "  4.800e+02  4.840e+02 -4.850e+02  4.870e+02  4.890e+02 -4.910e+02\n",
      "  4.930e+02  4.950e+02 -5.090e+02 -5.140e+02 -5.160e+02 -5.180e+02\n",
      " -5.200e+02  5.210e+02  5.260e+02  5.300e+02 -5.390e+02 -5.420e+02\n",
      " -5.430e+02  5.430e+02  5.450e+02  5.460e+02  5.460e+02 -5.470e+02\n",
      "  5.480e+02 -5.490e+02 -5.520e+02 -5.590e+02  5.600e+02  5.640e+02\n",
      " -5.680e+02  5.720e+02 -5.730e+02 -5.760e+02  5.770e+02  5.770e+02\n",
      " -5.790e+02  5.800e+02 -5.860e+02 -5.940e+02 -6.000e+02  6.010e+02\n",
      "  6.020e+02 -6.030e+02  6.060e+02 -6.160e+02 -6.160e+02 -6.190e+02\n",
      " -6.220e+02 -6.220e+02 -6.230e+02 -6.230e+02 -6.240e+02  6.240e+02\n",
      "  6.250e+02 -6.250e+02  6.310e+02 -6.350e+02 -6.370e+02 -6.410e+02\n",
      " -6.410e+02  6.410e+02 -6.440e+02 -6.450e+02 -6.450e+02 -6.530e+02\n",
      "  6.540e+02 -6.580e+02  6.630e+02  6.660e+02 -6.670e+02 -6.700e+02\n",
      " -6.790e+02  6.800e+02 -6.820e+02 -6.850e+02 -6.870e+02 -6.930e+02\n",
      " -6.950e+02 -7.010e+02 -7.010e+02 -7.070e+02 -7.100e+02 -7.130e+02\n",
      " -7.140e+02 -7.170e+02 -7.220e+02 -7.250e+02  7.270e+02  7.390e+02\n",
      " -7.420e+02 -7.500e+02 -7.540e+02 -7.590e+02 -7.590e+02 -7.590e+02\n",
      "  7.630e+02 -7.700e+02  7.730e+02 -7.730e+02 -7.820e+02 -7.830e+02\n",
      "  7.890e+02 -7.960e+02 -7.970e+02 -7.990e+02  8.040e+02  8.060e+02\n",
      " -8.170e+02 -8.270e+02 -8.330e+02 -8.360e+02  8.390e+02 -8.500e+02\n",
      " -8.510e+02  8.530e+02 -8.550e+02  8.620e+02  8.820e+02 -8.890e+02\n",
      " -8.930e+02 -8.950e+02 -8.970e+02 -9.040e+02 -9.060e+02 -9.100e+02\n",
      " -9.110e+02 -9.130e+02 -9.140e+02  9.270e+02 -9.300e+02 -9.330e+02\n",
      "  9.410e+02 -9.470e+02 -9.500e+02 -9.640e+02 -9.720e+02  9.800e+02\n",
      "  9.850e+02  9.930e+02 -9.930e+02 -9.950e+02 -9.970e+02 -1.000e+03\n",
      " -1.021e+03 -1.030e+03 -1.036e+03 -1.038e+03 -1.050e+03 -1.075e+03\n",
      " -1.077e+03  1.079e+03  1.081e+03  1.090e+03  1.093e+03 -1.095e+03\n",
      " -1.120e+03 -1.125e+03 -1.131e+03  1.133e+03 -1.138e+03 -1.143e+03\n",
      " -1.147e+03 -1.168e+03 -1.172e+03 -1.179e+03 -1.183e+03 -1.190e+03\n",
      "  1.202e+03 -1.218e+03 -1.224e+03 -1.245e+03 -1.253e+03 -1.265e+03\n",
      " -1.273e+03 -1.274e+03 -1.278e+03 -1.288e+03  1.289e+03 -1.309e+03\n",
      " -1.311e+03 -1.315e+03 -1.346e+03 -1.353e+03 -1.368e+03  1.394e+03\n",
      "  1.398e+03 -1.399e+03 -1.409e+03 -1.409e+03 -1.413e+03 -1.425e+03\n",
      " -1.428e+03  1.430e+03 -1.430e+03 -1.443e+03  1.459e+03 -1.460e+03\n",
      " -1.461e+03 -1.466e+03  1.466e+03 -1.472e+03 -1.483e+03 -1.506e+03\n",
      " -1.508e+03 -1.521e+03 -1.527e+03 -1.555e+03 -1.559e+03 -1.584e+03\n",
      " -1.584e+03 -1.586e+03 -1.628e+03 -1.663e+03  1.671e+03 -1.686e+03\n",
      " -1.690e+03 -1.699e+03  1.718e+03 -1.721e+03 -1.722e+03 -1.724e+03\n",
      "  1.732e+03 -1.737e+03  1.762e+03 -1.777e+03 -1.778e+03  1.838e+03\n",
      " -1.840e+03 -1.971e+03  1.972e+03 -1.995e+03  2.002e+03 -2.016e+03\n",
      " -2.016e+03  2.064e+03  2.083e+03  2.120e+03 -2.133e+03 -2.161e+03\n",
      "  2.166e+03 -2.169e+03 -2.182e+03  2.319e+03 -2.327e+03 -2.347e+03\n",
      " -2.437e+03 -2.562e+03 -2.641e+03  2.703e+03  2.717e+03 -2.727e+03\n",
      " -2.886e+03  2.900e+03 -3.270e+03  3.314e+03 -3.930e+03 -3.981e+03\n",
      " -4.241e+03 -4.282e+03  4.680e+03  4.760e+03  4.856e+03 -5.252e+03\n",
      " -5.480e+03  6.417e+03]\n",
      "Concordance Index 0.5136607122845552\n",
      "Integrated Brier Score: 0.1869276258463748\n",
      "y_train breslow final [ 2.000e+00  2.000e+00 -1.100e+01  1.100e+01 -1.400e+01  2.300e+01\n",
      " -3.000e+01  3.200e+01  5.600e+01 -5.600e+01  6.400e+01  6.500e+01\n",
      "  6.900e+01  7.600e+01 -7.700e+01  8.600e+01  8.900e+01 -9.200e+01\n",
      "  9.400e+01  9.500e+01 -1.020e+02  1.080e+02 -1.100e+02 -1.110e+02\n",
      " -1.220e+02  1.260e+02  1.290e+02  1.300e+02 -1.340e+02  1.440e+02\n",
      "  1.440e+02 -1.460e+02  1.510e+02  1.600e+02  1.660e+02  1.710e+02\n",
      "  1.730e+02  1.830e+02  1.850e+02 -1.870e+02  1.910e+02  1.940e+02\n",
      "  1.940e+02  1.970e+02  1.980e+02  2.090e+02 -2.100e+02  2.150e+02\n",
      "  2.150e+02 -2.160e+02 -2.160e+02  2.170e+02  2.180e+02  2.220e+02\n",
      " -2.320e+02 -2.340e+02 -2.420e+02  2.430e+02 -2.530e+02  2.560e+02\n",
      "  2.590e+02  2.680e+02  2.740e+02  2.760e+02  2.760e+02 -2.780e+02\n",
      "  2.810e+02  2.820e+02  2.830e+02  2.840e+02 -2.920e+02  2.920e+02\n",
      "  2.940e+02 -3.110e+02  3.170e+02  3.170e+02 -3.180e+02 -3.210e+02\n",
      "  3.210e+02  3.270e+02  3.300e+02  3.340e+02  3.360e+02  3.370e+02\n",
      "  3.410e+02  3.420e+02 -3.450e+02  3.480e+02 -3.510e+02  3.510e+02\n",
      "  3.520e+02  3.530e+02 -3.580e+02  3.590e+02  3.600e+02 -3.600e+02\n",
      "  3.610e+02 -3.620e+02  3.660e+02 -3.680e+02 -3.760e+02  3.770e+02\n",
      "  3.770e+02 -3.780e+02  3.790e+02  3.840e+02 -3.840e+02  3.850e+02\n",
      "  3.850e+02  3.870e+02 -3.870e+02 -3.920e+02 -3.920e+02  3.930e+02\n",
      " -3.930e+02  3.950e+02  3.950e+02  3.970e+02 -4.030e+02  4.030e+02\n",
      "  4.030e+02  4.060e+02  4.060e+02  4.070e+02 -4.070e+02 -4.120e+02\n",
      "  4.150e+02 -4.170e+02  4.210e+02 -4.210e+02 -4.240e+02  4.240e+02\n",
      "  4.270e+02  4.300e+02 -4.370e+02 -4.400e+02 -4.430e+02  4.490e+02\n",
      "  4.510e+02  4.530e+02 -4.580e+02 -4.610e+02  4.620e+02  4.640e+02\n",
      " -4.660e+02 -4.690e+02 -4.710e+02  4.720e+02  4.790e+02 -4.790e+02\n",
      "  4.800e+02  4.840e+02 -4.850e+02  4.870e+02  4.890e+02 -4.910e+02\n",
      "  4.930e+02  4.950e+02 -5.090e+02 -5.140e+02 -5.160e+02 -5.180e+02\n",
      " -5.200e+02  5.210e+02  5.260e+02  5.300e+02 -5.390e+02 -5.420e+02\n",
      " -5.430e+02  5.430e+02  5.450e+02  5.460e+02  5.460e+02 -5.470e+02\n",
      "  5.480e+02 -5.490e+02 -5.520e+02 -5.590e+02  5.600e+02  5.640e+02\n",
      " -5.680e+02  5.720e+02 -5.730e+02 -5.760e+02  5.770e+02  5.770e+02\n",
      " -5.790e+02  5.800e+02 -5.860e+02 -5.940e+02 -6.000e+02  6.010e+02\n",
      "  6.020e+02 -6.030e+02  6.060e+02 -6.160e+02 -6.160e+02 -6.190e+02\n",
      " -6.220e+02 -6.220e+02 -6.230e+02 -6.230e+02 -6.240e+02  6.240e+02\n",
      "  6.250e+02 -6.250e+02  6.310e+02 -6.350e+02 -6.370e+02 -6.410e+02\n",
      " -6.410e+02  6.410e+02 -6.440e+02 -6.450e+02 -6.450e+02 -6.530e+02\n",
      "  6.540e+02 -6.580e+02  6.630e+02  6.660e+02 -6.670e+02 -6.700e+02\n",
      " -6.790e+02  6.800e+02 -6.820e+02 -6.850e+02 -6.870e+02 -6.930e+02\n",
      " -6.950e+02 -7.010e+02 -7.010e+02 -7.070e+02 -7.100e+02 -7.130e+02\n",
      " -7.140e+02 -7.170e+02 -7.220e+02 -7.250e+02  7.270e+02  7.390e+02\n",
      " -7.420e+02 -7.500e+02 -7.540e+02 -7.590e+02 -7.590e+02 -7.590e+02\n",
      "  7.630e+02 -7.700e+02  7.730e+02 -7.730e+02 -7.820e+02 -7.830e+02\n",
      "  7.890e+02 -7.960e+02 -7.970e+02 -7.990e+02  8.040e+02  8.060e+02\n",
      " -8.170e+02 -8.270e+02 -8.330e+02 -8.360e+02  8.390e+02 -8.500e+02\n",
      " -8.510e+02  8.530e+02 -8.550e+02  8.620e+02  8.820e+02 -8.890e+02\n",
      " -8.930e+02 -8.950e+02 -8.970e+02 -9.040e+02 -9.060e+02 -9.100e+02\n",
      " -9.110e+02 -9.130e+02 -9.140e+02  9.270e+02 -9.300e+02 -9.330e+02\n",
      "  9.410e+02 -9.470e+02 -9.500e+02 -9.640e+02 -9.720e+02  9.800e+02\n",
      "  9.850e+02  9.930e+02 -9.930e+02 -9.950e+02 -9.970e+02 -1.000e+03\n",
      " -1.021e+03 -1.030e+03 -1.036e+03 -1.038e+03 -1.050e+03 -1.075e+03\n",
      " -1.077e+03  1.079e+03  1.081e+03  1.090e+03  1.093e+03 -1.095e+03\n",
      " -1.120e+03 -1.125e+03 -1.131e+03  1.133e+03 -1.138e+03 -1.143e+03\n",
      " -1.147e+03 -1.168e+03 -1.172e+03 -1.179e+03 -1.183e+03 -1.190e+03\n",
      "  1.202e+03 -1.218e+03 -1.224e+03 -1.245e+03 -1.253e+03 -1.265e+03\n",
      " -1.273e+03 -1.274e+03 -1.278e+03 -1.288e+03  1.289e+03 -1.309e+03\n",
      " -1.311e+03 -1.315e+03 -1.346e+03 -1.353e+03 -1.368e+03  1.394e+03\n",
      "  1.398e+03 -1.399e+03 -1.409e+03 -1.409e+03 -1.413e+03 -1.425e+03\n",
      " -1.428e+03  1.430e+03 -1.430e+03 -1.443e+03  1.459e+03 -1.460e+03\n",
      " -1.461e+03 -1.466e+03  1.466e+03 -1.472e+03 -1.483e+03 -1.506e+03\n",
      " -1.508e+03 -1.521e+03 -1.527e+03 -1.555e+03 -1.559e+03 -1.584e+03\n",
      " -1.584e+03 -1.586e+03 -1.628e+03 -1.663e+03  1.671e+03 -1.686e+03\n",
      " -1.690e+03 -1.699e+03  1.718e+03 -1.721e+03 -1.722e+03 -1.724e+03\n",
      "  1.732e+03 -1.737e+03  1.762e+03 -1.777e+03 -1.778e+03  1.838e+03\n",
      " -1.840e+03 -1.971e+03  1.972e+03 -1.995e+03  2.002e+03 -2.016e+03\n",
      " -2.016e+03  2.064e+03  2.083e+03  2.120e+03 -2.133e+03 -2.161e+03\n",
      "  2.166e+03 -2.169e+03 -2.182e+03  2.319e+03 -2.327e+03 -2.347e+03\n",
      " -2.437e+03 -2.562e+03 -2.641e+03  2.703e+03  2.717e+03 -2.727e+03\n",
      " -2.886e+03  2.900e+03 -3.270e+03  3.314e+03 -3.930e+03 -3.981e+03\n",
      " -4.241e+03 -4.282e+03  4.680e+03  4.760e+03  4.856e+03 -5.252e+03\n",
      " -5.480e+03  6.417e+03]\n",
      "durations 14.0 5152.0\n",
      "Concordance Index 0.5849673202614379\n",
      "Integrated Brier Score: 0.19062248911329635\n",
      "split gex_?|100130426      float32\n",
      "gex_?|100133144      float32\n",
      "gex_?|100134869      float32\n",
      "gex_?|10357          float32\n",
      "gex_?|10431          float32\n",
      "                      ...   \n",
      "gex_ZYG11A|440590    float32\n",
      "gex_ZYG11B|79699     float32\n",
      "gex_ZYX|7791         float32\n",
      "gex_ZZEF1|23140      float32\n",
      "gex_ZZZ3|26009       float32\n",
      "Length: 20531, dtype: object\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m72.0179\u001b[0m  0.0044\n",
      "      2       72.0179  0.0116\n",
      "      3       72.0179  0.0054\n",
      "      4       72.0179  0.0033\n",
      "      5       72.0179  0.0026\n",
      "      6       72.0179  0.0021\n",
      "      7       72.0179  0.0020\n",
      "      8       72.0179  0.0035\n",
      "      9       72.0179  0.0036\n",
      "     10       72.0179  0.0038\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m96.3845\u001b[0m  0.0063\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m84.5898\u001b[0m  0.0035\n",
      "      2       96.3845  0.0023\n",
      "      2       84.5898  0.0025\n",
      "      3       96.3845  0.0021\n",
      "      3       84.5898  0.0022\n",
      "      4       96.3845  0.0020\n",
      "      4       84.5898  0.0021\n",
      "      5       96.3845  0.0020\n",
      "      5       84.5898  0.0020\n",
      "      6       96.3845  0.0030\n",
      "      6       84.5898  0.0045\n",
      "      7       96.3845  0.0024\n",
      "      7       84.5898  0.0022\n",
      "      8       96.3845  0.0021\n",
      "      8       84.5898  0.0021\n",
      "      9       96.3845  0.0024\n",
      "      9       84.5898  0.0025\n",
      "     10       96.3845  0.0027\n",
      "Restoring best model from epoch 1.\n",
      "     10       84.5898  0.0028\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m114.6297\u001b[0m       \u001b[32m87.7460\u001b[0m  0.1440\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m60.9829\u001b[0m  0.0043\n",
      "      2       60.9829  0.0035\n",
      "      3       60.9829  0.0022\n",
      "      4       60.9829  0.0020\n",
      "      5       60.9829  0.0026\n",
      "      6       60.9829  0.0055\n",
      "      7       60.9829  0.0027\n",
      "      8       60.9829  0.0032\n",
      "      9       60.9829  0.0027\n",
      "     10       60.9829  0.0021\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2       \u001b[36m82.1859\u001b[0m      103.8665  0.0879\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      3       \u001b[36m66.7662\u001b[0m       \u001b[32m85.6008\u001b[0m  0.0864\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m95.4518\u001b[0m  0.0031\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m157.3189\u001b[0m           nan  0.1061\n",
      "      2       95.4518  0.0026\n",
      "      3       95.4518  0.0028\n",
      "      4       95.4518  0.0026\n",
      "      5       95.4518  0.0024\n",
      "      6       95.4518  0.0022\n",
      "      7       95.4518  0.0022\n",
      "      8       95.4518  0.0021\n",
      "      9       95.4518  0.0021\n",
      "     10       95.4518  0.0021\n",
      "Restoring best model from epoch 1.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m145.5859\u001b[0m      \u001b[32m310.5548\u001b[0m  0.1155\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m132.0601\u001b[0m      \u001b[32m255.8503\u001b[0m  0.1098\n",
      "      4       \u001b[36m63.6246\u001b[0m       \u001b[32m54.6164\u001b[0m  0.0840\n",
      "      2      \u001b[36m105.6317\u001b[0m      \u001b[32m253.2969\u001b[0m  0.0831\n",
      "      2       \u001b[36m92.8435\u001b[0m      383.3716  0.0761\n",
      "      2       \u001b[36m91.0272\u001b[0m      \u001b[32m130.4572\u001b[0m  0.0980\n",
      "      5       \u001b[36m62.8966\u001b[0m       \u001b[32m42.6407\u001b[0m  0.0759\n",
      "      3       \u001b[36m80.4085\u001b[0m      \u001b[32m138.9944\u001b[0m  0.0779\n",
      "      3       \u001b[36m73.3167\u001b[0m      \u001b[32m228.7507\u001b[0m  0.0760\n",
      "      3       \u001b[36m68.4971\u001b[0m      132.9191  0.0822\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      6       \u001b[36m59.0770\u001b[0m       \u001b[32m33.8821\u001b[0m  0.0756\n",
      "      4       \u001b[36m75.8006\u001b[0m       \u001b[32m90.3700\u001b[0m  0.0783\n",
      "      4       75.4162       \u001b[32m93.1698\u001b[0m  0.0815\n",
      "      4       \u001b[36m67.7122\u001b[0m       \u001b[32m85.8134\u001b[0m  0.0798\n",
      "      7       \u001b[36m58.4075\u001b[0m       35.6035  0.0841\n",
      "      5       \u001b[36m75.6273\u001b[0m       \u001b[32m74.5683\u001b[0m  0.0852\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m127.1663\u001b[0m      \u001b[32m227.4519\u001b[0m  0.1144\n",
      "      5       \u001b[36m67.0094\u001b[0m      101.9129  0.0827\n",
      "      5       \u001b[36m66.0471\u001b[0m       \u001b[32m73.6335\u001b[0m  0.0777\n",
      "      8       \u001b[36m55.5074\u001b[0m       34.9808  0.0730\n",
      "      6       76.0516       \u001b[32m70.4288\u001b[0m  0.0759\n",
      "      2       \u001b[36m76.5604\u001b[0m      \u001b[32m158.3798\u001b[0m  0.0789\n",
      "      6       \u001b[36m66.7723\u001b[0m       \u001b[32m59.8107\u001b[0m  0.0747\n",
      "      6       \u001b[36m62.5463\u001b[0m       \u001b[32m63.5250\u001b[0m  0.0767\n",
      "      9       \u001b[36m55.2728\u001b[0m       \u001b[32m33.2142\u001b[0m  0.0742\n",
      "      7       \u001b[36m72.1944\u001b[0m       70.9923  0.0759\n",
      "      7       \u001b[36m65.0929\u001b[0m       \u001b[32m59.3417\u001b[0m  0.0769\n",
      "      3       \u001b[36m66.0472\u001b[0m       \u001b[32m90.5928\u001b[0m  0.0765\n",
      "      7       63.1848       \u001b[32m61.1270\u001b[0m  0.0739\n",
      "     10       \u001b[36m53.1006\u001b[0m       35.1819  0.0779\n",
      "Restoring best model from epoch 9.\n",
      "      8       72.5163       72.7662  0.0752\n",
      "      8       \u001b[36m64.8088\u001b[0m       \u001b[32m52.6301\u001b[0m  0.0773\n",
      "      4       \u001b[36m60.6524\u001b[0m       \u001b[32m53.5976\u001b[0m  0.0782\n",
      "      8       \u001b[36m61.3867\u001b[0m       61.5238  0.0801\n",
      "      9       \u001b[36m71.2878\u001b[0m       71.5730  0.0728\n",
      "      9       \u001b[36m64.2949\u001b[0m       \u001b[32m51.9941\u001b[0m  0.0757\n",
      "      5       \u001b[36m58.7852\u001b[0m       55.5172  0.0769\n",
      "      9       61.6561       62.4344  0.0719\n",
      "     10       \u001b[36m68.6195\u001b[0m       \u001b[32m69.5090\u001b[0m  0.0733\n",
      "      6       \u001b[36m56.3450\u001b[0m       56.5262  0.0730\n",
      "     10       \u001b[36m64.2234\u001b[0m       53.6338  0.0799\n",
      "Restoring best model from epoch 9.\n",
      "     10       \u001b[36m60.9089\u001b[0m       63.8625  0.0715\n",
      "Restoring best model from epoch 7.\n",
      "      7       \u001b[36m53.5094\u001b[0m       54.4469  0.0713\n",
      "      8       53.6608       54.3271  0.0734\n",
      "      9       \u001b[36m52.7043\u001b[0m       \u001b[32m50.7040\u001b[0m  0.0700\n",
      "     10       \u001b[36m50.6530\u001b[0m       \u001b[32m46.3963\u001b[0m  0.0701\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m100.6625\u001b[0m      \u001b[32m184.1879\u001b[0m  0.0753\n",
      "      2       \u001b[36m80.9653\u001b[0m      184.6409  0.0629\n",
      "      3       \u001b[36m73.2549\u001b[0m      \u001b[32m116.8289\u001b[0m  0.0724\n",
      "      4       \u001b[36m69.7231\u001b[0m       \u001b[32m70.1107\u001b[0m  0.0672\n",
      "      5       \u001b[36m63.7527\u001b[0m       \u001b[32m62.1612\u001b[0m  0.0743\n",
      "      6       \u001b[36m60.1246\u001b[0m       \u001b[32m57.8802\u001b[0m  0.0740\n",
      "      7       \u001b[36m58.3559\u001b[0m       58.5285  0.0632\n",
      "      8       \u001b[36m57.2234\u001b[0m       59.4974  0.0590\n",
      "      9       \u001b[36m55.4995\u001b[0m       59.9787  0.0627\n",
      "     10       \u001b[36m55.2760\u001b[0m       59.2745  0.0713\n",
      "Restoring best model from epoch 6.\n",
      "y_train breslow final [ 2.000e+00 -3.000e+00 -7.000e+00 -1.100e+01 -1.300e+01 -1.600e+01\n",
      " -1.600e+01 -1.800e+01  1.800e+01 -2.600e+01 -2.900e+01 -3.800e+01\n",
      "  4.100e+01  4.200e+01  4.300e+01 -4.300e+01  5.100e+01 -5.300e+01\n",
      "  5.900e+01 -6.100e+01  6.500e+01  6.800e+01  6.900e+01  7.300e+01\n",
      "  7.700e+01  9.300e+01  1.060e+02  1.090e+02 -1.270e+02  1.370e+02\n",
      "  1.390e+02  1.390e+02 -1.410e+02 -1.500e+02 -1.550e+02  1.620e+02\n",
      " -1.660e+02  1.660e+02  1.680e+02 -1.750e+02 -1.770e+02  1.820e+02\n",
      " -1.930e+02 -1.940e+02  2.040e+02 -2.050e+02  2.240e+02  2.380e+02\n",
      "  2.420e+02  2.450e+02 -2.560e+02 -2.930e+02  3.070e+02  3.110e+02\n",
      "  3.130e+02  3.130e+02 -3.190e+02  3.200e+02 -3.230e+02  3.290e+02\n",
      "  3.330e+02  3.340e+02  3.360e+02  3.420e+02  3.440e+02 -3.540e+02\n",
      " -3.560e+02 -3.620e+02 -3.650e+02 -3.650e+02 -3.690e+02 -3.700e+02\n",
      " -3.720e+02 -3.730e+02 -3.740e+02  3.750e+02 -3.850e+02 -3.860e+02\n",
      " -4.000e+02 -4.060e+02 -4.080e+02 -4.110e+02  4.310e+02 -4.310e+02\n",
      " -4.330e+02  4.450e+02  4.460e+02  4.540e+02 -4.540e+02  4.590e+02\n",
      " -4.690e+02  4.780e+02  4.800e+02  4.800e+02  4.850e+02 -4.950e+02\n",
      " -4.980e+02 -5.010e+02 -5.050e+02 -5.070e+02 -5.110e+02 -5.190e+02\n",
      " -5.280e+02 -5.510e+02  5.520e+02  5.610e+02  5.630e+02 -5.630e+02\n",
      " -5.670e+02 -5.740e+02  5.740e+02  5.780e+02  5.870e+02 -5.910e+02\n",
      " -6.030e+02 -6.070e+02 -6.170e+02 -6.300e+02  6.370e+02  6.450e+02\n",
      " -6.480e+02 -6.560e+02 -6.620e+02 -6.650e+02  6.790e+02  6.830e+02\n",
      " -6.850e+02 -6.890e+02 -6.930e+02  7.010e+02  7.090e+02 -7.140e+02\n",
      "  7.220e+02 -7.220e+02  7.350e+02 -7.350e+02 -7.370e+02 -7.490e+02\n",
      " -7.550e+02 -7.620e+02  7.680e+02 -7.850e+02  7.930e+02  8.190e+02\n",
      "  8.220e+02  8.280e+02 -8.300e+02  8.340e+02 -8.400e+02  8.410e+02\n",
      "  8.450e+02 -8.610e+02  8.660e+02 -8.730e+02 -8.740e+02  8.780e+02\n",
      "  8.830e+02  8.850e+02  8.850e+02 -9.090e+02 -9.100e+02 -9.190e+02\n",
      "  9.270e+02  9.320e+02 -9.320e+02 -9.450e+02  9.460e+02 -9.510e+02\n",
      "  9.520e+02 -9.520e+02 -9.670e+02 -9.920e+02  9.920e+02  1.003e+03\n",
      " -1.014e+03 -1.018e+03 -1.018e+03  1.019e+03  1.034e+03  1.045e+03\n",
      " -1.046e+03 -1.063e+03 -1.071e+03  1.075e+03  1.091e+03 -1.092e+03\n",
      "  1.092e+03  1.097e+03 -1.107e+03  1.111e+03 -1.120e+03 -1.124e+03\n",
      " -1.124e+03 -1.126e+03 -1.130e+03 -1.133e+03  1.133e+03 -1.133e+03\n",
      " -1.137e+03 -1.140e+03 -1.143e+03 -1.168e+03 -1.169e+03  1.170e+03\n",
      " -1.175e+03 -1.186e+03  1.191e+03  1.200e+03  1.200e+03 -1.217e+03\n",
      " -1.218e+03 -1.238e+03  1.238e+03 -1.266e+03 -1.274e+03 -1.290e+03\n",
      " -1.314e+03  1.317e+03 -1.329e+03  1.337e+03  1.343e+03 -1.367e+03\n",
      " -1.371e+03  1.371e+03  1.378e+03 -1.380e+03 -1.385e+03 -1.398e+03\n",
      "  1.404e+03 -1.411e+03 -1.413e+03 -1.413e+03 -1.416e+03  1.417e+03\n",
      " -1.423e+03  1.432e+03 -1.433e+03 -1.435e+03 -1.436e+03 -1.450e+03\n",
      " -1.462e+03  1.463e+03 -1.471e+03 -1.476e+03 -1.478e+03 -1.485e+03\n",
      " -1.487e+03 -1.493e+03 -1.496e+03 -1.498e+03 -1.499e+03 -1.502e+03\n",
      " -1.508e+03 -1.516e+03 -1.520e+03 -1.520e+03 -1.521e+03 -1.525e+03\n",
      " -1.528e+03 -1.531e+03 -1.552e+03 -1.560e+03  1.584e+03  1.588e+03\n",
      "  1.589e+03  1.598e+03 -1.608e+03  1.610e+03  1.620e+03 -1.621e+03\n",
      "  1.625e+03  1.625e+03  1.626e+03  1.639e+03  1.657e+03 -1.657e+03\n",
      "  1.661e+03 -1.683e+03  1.696e+03  1.714e+03  1.724e+03 -1.729e+03\n",
      " -1.733e+03 -1.746e+03 -1.755e+03 -1.778e+03 -1.782e+03 -1.785e+03\n",
      " -1.790e+03 -1.794e+03 -1.843e+03 -1.862e+03 -1.871e+03 -1.876e+03\n",
      " -1.879e+03 -1.883e+03 -1.885e+03 -1.886e+03 -1.888e+03 -1.889e+03\n",
      " -1.893e+03 -1.905e+03 -1.906e+03  1.912e+03  1.913e+03 -1.924e+03\n",
      " -1.928e+03 -1.946e+03 -1.955e+03  1.964e+03  1.980e+03  1.986e+03\n",
      " -1.993e+03 -1.997e+03 -2.004e+03 -2.009e+03 -2.014e+03 -2.016e+03\n",
      " -2.017e+03 -2.038e+03 -2.067e+03 -2.080e+03 -2.087e+03  2.090e+03\n",
      " -2.128e+03 -2.133e+03  2.145e+03 -2.150e+03 -2.172e+03 -2.172e+03\n",
      " -2.184e+03 -2.186e+03  2.190e+03 -2.208e+03 -2.217e+03 -2.226e+03\n",
      "  2.227e+03 -2.231e+03  2.241e+03 -2.246e+03  2.256e+03 -2.257e+03\n",
      " -2.263e+03 -2.271e+03 -2.274e+03 -2.283e+03  2.299e+03  2.343e+03\n",
      " -2.372e+03 -2.378e+03  2.386e+03 -2.392e+03 -2.412e+03  2.419e+03\n",
      " -2.422e+03 -2.423e+03 -2.430e+03 -2.439e+03  2.454e+03 -2.470e+03\n",
      " -2.489e+03 -2.504e+03 -2.508e+03 -2.531e+03 -2.552e+03 -2.554e+03\n",
      "  2.564e+03 -2.609e+03 -2.609e+03 -2.630e+03 -2.660e+03 -2.688e+03\n",
      " -2.718e+03 -2.722e+03 -2.746e+03  2.752e+03 -2.754e+03  2.764e+03\n",
      " -2.782e+03 -2.789e+03 -2.799e+03 -2.839e+03 -2.859e+03 -2.865e+03\n",
      " -2.868e+03 -2.873e+03 -2.881e+03 -2.964e+03 -3.074e+03 -3.146e+03\n",
      " -3.205e+03 -3.205e+03 -3.229e+03 -3.267e+03 -3.302e+03 -3.328e+03\n",
      " -3.331e+03 -3.341e+03 -3.343e+03 -3.377e+03 -3.392e+03 -3.409e+03\n",
      " -3.451e+03 -3.480e+03 -3.498e+03 -3.519e+03  3.554e+03 -3.583e+03\n",
      "  3.615e+03 -3.631e+03 -3.639e+03 -3.728e+03 -3.736e+03 -3.744e+03\n",
      " -3.834e+03 -3.841e+03 -3.936e+03 -3.944e+03 -3.974e+03 -3.987e+03\n",
      " -3.989e+03 -4.067e+03 -4.074e+03 -4.537e+03]\n",
      "Concordance Index 0.721002531088368\n",
      "Integrated Brier Score: 0.15771944821115164\n",
      "y_train breslow final [ 2.000e+00 -3.000e+00 -7.000e+00 -1.100e+01 -1.300e+01 -1.600e+01\n",
      " -1.600e+01 -1.800e+01  1.800e+01 -2.600e+01 -2.900e+01 -3.800e+01\n",
      "  4.100e+01  4.200e+01  4.300e+01 -4.300e+01  5.100e+01 -5.300e+01\n",
      "  5.900e+01 -6.100e+01  6.500e+01  6.800e+01  6.900e+01  7.300e+01\n",
      "  7.700e+01  9.300e+01  1.060e+02  1.090e+02 -1.270e+02  1.370e+02\n",
      "  1.390e+02  1.390e+02 -1.410e+02 -1.500e+02 -1.550e+02  1.620e+02\n",
      " -1.660e+02  1.660e+02  1.680e+02 -1.750e+02 -1.770e+02  1.820e+02\n",
      " -1.930e+02 -1.940e+02  2.040e+02 -2.050e+02  2.240e+02  2.380e+02\n",
      "  2.420e+02  2.450e+02 -2.560e+02 -2.930e+02  3.070e+02  3.110e+02\n",
      "  3.130e+02  3.130e+02 -3.190e+02  3.200e+02 -3.230e+02  3.290e+02\n",
      "  3.330e+02  3.340e+02  3.360e+02  3.420e+02  3.440e+02 -3.540e+02\n",
      " -3.560e+02 -3.620e+02 -3.650e+02 -3.650e+02 -3.690e+02 -3.700e+02\n",
      " -3.720e+02 -3.730e+02 -3.740e+02  3.750e+02 -3.850e+02 -3.860e+02\n",
      " -4.000e+02 -4.060e+02 -4.080e+02 -4.110e+02  4.310e+02 -4.310e+02\n",
      " -4.330e+02  4.450e+02  4.460e+02  4.540e+02 -4.540e+02  4.590e+02\n",
      " -4.690e+02  4.780e+02  4.800e+02  4.800e+02  4.850e+02 -4.950e+02\n",
      " -4.980e+02 -5.010e+02 -5.050e+02 -5.070e+02 -5.110e+02 -5.190e+02\n",
      " -5.280e+02 -5.510e+02  5.520e+02  5.610e+02  5.630e+02 -5.630e+02\n",
      " -5.670e+02 -5.740e+02  5.740e+02  5.780e+02  5.870e+02 -5.910e+02\n",
      " -6.030e+02 -6.070e+02 -6.170e+02 -6.300e+02  6.370e+02  6.450e+02\n",
      " -6.480e+02 -6.560e+02 -6.620e+02 -6.650e+02  6.790e+02  6.830e+02\n",
      " -6.850e+02 -6.890e+02 -6.930e+02  7.010e+02  7.090e+02 -7.140e+02\n",
      "  7.220e+02 -7.220e+02  7.350e+02 -7.350e+02 -7.370e+02 -7.490e+02\n",
      " -7.550e+02 -7.620e+02  7.680e+02 -7.850e+02  7.930e+02  8.190e+02\n",
      "  8.220e+02  8.280e+02 -8.300e+02  8.340e+02 -8.400e+02  8.410e+02\n",
      "  8.450e+02 -8.610e+02  8.660e+02 -8.730e+02 -8.740e+02  8.780e+02\n",
      "  8.830e+02  8.850e+02  8.850e+02 -9.090e+02 -9.100e+02 -9.190e+02\n",
      "  9.270e+02  9.320e+02 -9.320e+02 -9.450e+02  9.460e+02 -9.510e+02\n",
      "  9.520e+02 -9.520e+02 -9.670e+02 -9.920e+02  9.920e+02  1.003e+03\n",
      " -1.014e+03 -1.018e+03 -1.018e+03  1.019e+03  1.034e+03  1.045e+03\n",
      " -1.046e+03 -1.063e+03 -1.071e+03  1.075e+03  1.091e+03 -1.092e+03\n",
      "  1.092e+03  1.097e+03 -1.107e+03  1.111e+03 -1.120e+03 -1.124e+03\n",
      " -1.124e+03 -1.126e+03 -1.130e+03 -1.133e+03  1.133e+03 -1.133e+03\n",
      " -1.137e+03 -1.140e+03 -1.143e+03 -1.168e+03 -1.169e+03  1.170e+03\n",
      " -1.175e+03 -1.186e+03  1.191e+03  1.200e+03  1.200e+03 -1.217e+03\n",
      " -1.218e+03 -1.238e+03  1.238e+03 -1.266e+03 -1.274e+03 -1.290e+03\n",
      " -1.314e+03  1.317e+03 -1.329e+03  1.337e+03  1.343e+03 -1.367e+03\n",
      " -1.371e+03  1.371e+03  1.378e+03 -1.380e+03 -1.385e+03 -1.398e+03\n",
      "  1.404e+03 -1.411e+03 -1.413e+03 -1.413e+03 -1.416e+03  1.417e+03\n",
      " -1.423e+03  1.432e+03 -1.433e+03 -1.435e+03 -1.436e+03 -1.450e+03\n",
      " -1.462e+03  1.463e+03 -1.471e+03 -1.476e+03 -1.478e+03 -1.485e+03\n",
      " -1.487e+03 -1.493e+03 -1.496e+03 -1.498e+03 -1.499e+03 -1.502e+03\n",
      " -1.508e+03 -1.516e+03 -1.520e+03 -1.520e+03 -1.521e+03 -1.525e+03\n",
      " -1.528e+03 -1.531e+03 -1.552e+03 -1.560e+03  1.584e+03  1.588e+03\n",
      "  1.589e+03  1.598e+03 -1.608e+03  1.610e+03  1.620e+03 -1.621e+03\n",
      "  1.625e+03  1.625e+03  1.626e+03  1.639e+03  1.657e+03 -1.657e+03\n",
      "  1.661e+03 -1.683e+03  1.696e+03  1.714e+03  1.724e+03 -1.729e+03\n",
      " -1.733e+03 -1.746e+03 -1.755e+03 -1.778e+03 -1.782e+03 -1.785e+03\n",
      " -1.790e+03 -1.794e+03 -1.843e+03 -1.862e+03 -1.871e+03 -1.876e+03\n",
      " -1.879e+03 -1.883e+03 -1.885e+03 -1.886e+03 -1.888e+03 -1.889e+03\n",
      " -1.893e+03 -1.905e+03 -1.906e+03  1.912e+03  1.913e+03 -1.924e+03\n",
      " -1.928e+03 -1.946e+03 -1.955e+03  1.964e+03  1.980e+03  1.986e+03\n",
      " -1.993e+03 -1.997e+03 -2.004e+03 -2.009e+03 -2.014e+03 -2.016e+03\n",
      " -2.017e+03 -2.038e+03 -2.067e+03 -2.080e+03 -2.087e+03  2.090e+03\n",
      " -2.128e+03 -2.133e+03  2.145e+03 -2.150e+03 -2.172e+03 -2.172e+03\n",
      " -2.184e+03 -2.186e+03  2.190e+03 -2.208e+03 -2.217e+03 -2.226e+03\n",
      "  2.227e+03 -2.231e+03  2.241e+03 -2.246e+03  2.256e+03 -2.257e+03\n",
      " -2.263e+03 -2.271e+03 -2.274e+03 -2.283e+03  2.299e+03  2.343e+03\n",
      " -2.372e+03 -2.378e+03  2.386e+03 -2.392e+03 -2.412e+03  2.419e+03\n",
      " -2.422e+03 -2.423e+03 -2.430e+03 -2.439e+03  2.454e+03 -2.470e+03\n",
      " -2.489e+03 -2.504e+03 -2.508e+03 -2.531e+03 -2.552e+03 -2.554e+03\n",
      "  2.564e+03 -2.609e+03 -2.609e+03 -2.630e+03 -2.660e+03 -2.688e+03\n",
      " -2.718e+03 -2.722e+03 -2.746e+03  2.752e+03 -2.754e+03  2.764e+03\n",
      " -2.782e+03 -2.789e+03 -2.799e+03 -2.839e+03 -2.859e+03 -2.865e+03\n",
      " -2.868e+03 -2.873e+03 -2.881e+03 -2.964e+03 -3.074e+03 -3.146e+03\n",
      " -3.205e+03 -3.205e+03 -3.229e+03 -3.267e+03 -3.302e+03 -3.328e+03\n",
      " -3.331e+03 -3.341e+03 -3.343e+03 -3.377e+03 -3.392e+03 -3.409e+03\n",
      " -3.451e+03 -3.480e+03 -3.498e+03 -3.519e+03  3.554e+03 -3.583e+03\n",
      "  3.615e+03 -3.631e+03 -3.639e+03 -3.728e+03 -3.736e+03 -3.744e+03\n",
      " -3.834e+03 -3.841e+03 -3.936e+03 -3.944e+03 -3.974e+03 -3.987e+03\n",
      " -3.989e+03 -4.067e+03 -4.074e+03 -4.537e+03]\n",
      "durations 3.0 3431.0\n",
      "Concordance Index 0.6732334047109207\n",
      "Integrated Brier Score: 0.17355095157170536\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m160.6686\u001b[0m       \u001b[32m43.0601\u001b[0m  0.0280\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m212.2574\u001b[0m  0.0054\n",
      "      2      212.2574  0.0023\n",
      "      3      212.2574  0.0019\n",
      "      4      212.2574  0.0018\n",
      "      2      167.3148       \u001b[32m42.3773\u001b[0m  0.0255\n",
      "      5      212.2574  0.0020\n",
      "      6      212.2574  0.0018\n",
      "      7      212.2574  0.0018\n",
      "      8      212.2574  0.0017\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      9      212.2574  0.0017\n",
      "     10      212.2574  0.0017\n",
      "Restoring best model from epoch 1.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m384.1610\u001b[0m  0.0031\n",
      "      2      384.1610  0.0025\n",
      "      3      384.1610  0.0020\n",
      "      4      384.1610  0.0020\n",
      "      5      384.1610  0.0020\n",
      "      6      384.1610  0.0025\n",
      "      7      384.1610  0.0018\n",
      "      8      384.1610  0.0018\n",
      "      3      \u001b[36m160.3455\u001b[0m       \u001b[32m41.3900\u001b[0m  0.0280\n",
      "      9      384.1610  0.0018\n",
      "     10      384.1610  0.0019\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      4      \u001b[36m159.2169\u001b[0m       \u001b[32m40.5992\u001b[0m  0.0274\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "Re-initializing optimizer.\n",
      "      1      \u001b[36m320.2158\u001b[0m  0.0034\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m220.0913\u001b[0m  0.0038\n",
      "      2      320.2158  0.0025\n",
      "      2      220.0913  0.0021\n",
      "      3      220.0913  0.0018\n",
      "      4      220.0913  0.0017\n",
      "      5      220.0913  0.0017\n",
      "      5      162.6241       40.8157  0.0257\n",
      "      3      320.2158  0.0093\n",
      "      6      220.0913  0.0028\n",
      "      4      320.2158  0.0023\n",
      "      7      220.0913  0.0019\n",
      "      5      320.2158  0.0018\n",
      "      8      220.0913  0.0017\n",
      "      9      220.0913  0.0017\n",
      "      6      320.2158  0.0022\n",
      "     10      220.0913  0.0026\n",
      "      7      320.2158  0.0021\n",
      "Restoring best model from epoch 1.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m181.3175\u001b[0m       \u001b[32m78.8210\u001b[0m  0.0482\n",
      "      8      320.2158  0.0019\n",
      "      9      320.2158  0.0018\n",
      "     10      320.2158  0.0037\n",
      "Restoring best model from epoch 1.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m178.9634\u001b[0m       \u001b[32m92.0731\u001b[0m  0.0266\n",
      "      6      \u001b[36m153.4346\u001b[0m       41.1602  0.0297\n",
      "      2      \u001b[36m178.9935\u001b[0m       \u001b[32m77.8472\u001b[0m  0.0264\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      7      161.5742       41.2431  0.0254\n",
      "      2      180.6262       \u001b[32m90.5160\u001b[0m  0.0353\n",
      "      3      \u001b[36m178.8505\u001b[0m       \u001b[32m77.5558\u001b[0m  0.0230\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m152.9319\u001b[0m  0.0068\n",
      "      2      152.9319  0.0021\n",
      "      8      155.3641       41.6728  0.0239\n",
      "      3      152.9319  0.0017\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m160.5830\u001b[0m       \u001b[32m38.1696\u001b[0m  0.0259\n",
      "      3      182.5688       \u001b[32m88.6683\u001b[0m  0.0244\n",
      "      4      152.9319  0.0017\n",
      "      5      152.9319  0.0017\n",
      "      6      152.9319  0.0024\n",
      "      7      152.9319  0.0027\n",
      "      4      185.1888       77.5936  0.0297\n",
      "      8      152.9319  0.0128\n",
      "      9      154.2137       42.1421  0.0236\n",
      "      4      184.7612       \u001b[32m87.7043\u001b[0m  0.0289\n",
      "      2      \u001b[36m155.5927\u001b[0m       \u001b[32m38.0034\u001b[0m  0.0314\n",
      "      9      152.9319  0.0112\n",
      "     10      152.9319  0.0049\n",
      "      5      182.3396       \u001b[32m77.1433\u001b[0m  0.0257\n",
      "Restoring best model from epoch 1.\n",
      "     10      153.5482       42.3481  0.0270\n",
      "Restoring best model from epoch 4.\n",
      "      5      180.6363       \u001b[32m87.5540\u001b[0m  0.0322\n",
      "      6      181.8020       \u001b[32m77.1411\u001b[0m  0.0305\n",
      "      3      157.2029       \u001b[32m37.8380\u001b[0m  0.0360\n",
      "      6      184.5859       87.6120  0.0284\n",
      "      4      \u001b[36m152.9551\u001b[0m       \u001b[32m37.7864\u001b[0m  0.0256\n",
      "      7      179.8960       77.2266  0.0302\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      7      \u001b[36m173.9529\u001b[0m       87.6720  0.0244\n",
      "      5      153.6299       37.8970  0.0253\n",
      "      8      \u001b[36m177.4531\u001b[0m       77.3968  0.0301\n",
      "      8      177.4308       88.1397  0.0237\n",
      "      6      158.8606       37.9277  0.0233\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m183.0434\u001b[0m       \u001b[32m62.3447\u001b[0m  0.0308\n",
      "      9      179.3122       77.4263  0.0238\n",
      "      9      177.9333       88.3041  0.0234\n",
      "      7      157.3870       37.9102  0.0239\n",
      "      2      183.1980       \u001b[32m62.2944\u001b[0m  0.0308\n",
      "     10      180.4053       \u001b[32m76.9636\u001b[0m  0.0256\n",
      "     10      179.0593       87.9206  0.0241\n",
      "Restoring best model from epoch 5.\n",
      "      8      159.1377       37.9439  0.0237\n",
      "      3      \u001b[36m181.3510\u001b[0m       \u001b[32m61.3295\u001b[0m  0.0242\n",
      "      9      154.1648       37.9386  0.0225\n",
      "      4      192.9418       \u001b[32m60.5278\u001b[0m  0.0265\n",
      "     10      154.4365       38.0563  0.0231\n",
      "Restoring best model from epoch 4.\n",
      "      5      \u001b[36m177.7128\u001b[0m       \u001b[32m59.8669\u001b[0m  0.0257\n",
      "      6      180.7405       \u001b[32m59.7657\u001b[0m  0.0210\n",
      "      7      \u001b[36m174.9145\u001b[0m       \u001b[32m59.7456\u001b[0m  0.0208\n",
      "      8      175.0228       59.8753  0.0209\n",
      "      9      176.0550       59.7817  0.0207\n",
      "     10      176.7206       \u001b[32m59.5095\u001b[0m  0.0204\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m163.5238\u001b[0m      \u001b[32m108.9401\u001b[0m  0.0287\n",
      "      2      163.7306      \u001b[32m108.6543\u001b[0m  0.0247\n",
      "      3      164.1769      \u001b[32m107.8734\u001b[0m  0.0252\n",
      "      4      168.6006      \u001b[32m106.5635\u001b[0m  0.0268\n",
      "      5      \u001b[36m162.4625\u001b[0m      \u001b[32m106.2905\u001b[0m  0.0303\n",
      "      6      163.8085      106.3926  0.0224\n",
      "      7      \u001b[36m162.3896\u001b[0m      106.5593  0.0256\n",
      "      8      \u001b[36m159.4818\u001b[0m      106.5804  0.0225\n",
      "      9      160.3196      106.7265  0.0227\n",
      "     10      \u001b[36m158.2660\u001b[0m      106.5842  0.0231\n",
      "Restoring best model from epoch 5.\n",
      "y_train breslow final [ 2.000e+00 -3.000e+00 -1.300e+01 -1.600e+01 -1.600e+01 -1.600e+01\n",
      "  1.800e+01 -2.600e+01 -2.900e+01 -3.600e+01  4.100e+01  4.200e+01\n",
      "  4.300e+01 -4.300e+01  5.100e+01 -5.300e+01  6.200e+01  6.500e+01\n",
      "  6.900e+01  9.900e+01  1.010e+02  1.060e+02  1.100e+02 -1.180e+02\n",
      " -1.190e+02 -1.270e+02  1.370e+02  1.390e+02  1.390e+02 -1.410e+02\n",
      " -1.500e+02 -1.550e+02  1.620e+02  1.640e+02 -1.820e+02  1.820e+02\n",
      "  1.830e+02 -1.930e+02  2.020e+02  2.040e+02 -2.040e+02 -2.050e+02\n",
      "  2.060e+02 -2.060e+02  2.110e+02 -2.110e+02  2.220e+02  2.380e+02\n",
      "  2.420e+02  2.450e+02 -2.560e+02 -2.930e+02  3.070e+02  3.130e+02\n",
      "  3.130e+02 -3.230e+02  3.290e+02  3.300e+02  3.330e+02  3.340e+02\n",
      "  3.360e+02 -3.400e+02  3.420e+02  3.440e+02 -3.540e+02 -3.550e+02\n",
      "  3.620e+02 -3.620e+02 -3.650e+02 -3.650e+02 -3.690e+02 -3.700e+02\n",
      " -3.730e+02 -3.740e+02  3.750e+02 -3.860e+02 -4.000e+02 -4.060e+02\n",
      " -4.110e+02  4.310e+02 -4.330e+02 -4.350e+02  4.450e+02 -4.540e+02\n",
      "  4.590e+02 -4.680e+02  4.750e+02  4.780e+02  4.800e+02  4.850e+02\n",
      " -4.950e+02 -5.010e+02 -5.050e+02 -5.070e+02  5.100e+02 -5.190e+02\n",
      " -5.230e+02 -5.510e+02  5.520e+02  5.610e+02  5.610e+02  5.620e+02\n",
      "  5.630e+02 -5.630e+02 -5.670e+02  5.710e+02  5.720e+02 -5.740e+02\n",
      "  5.740e+02  5.780e+02  5.870e+02  6.000e+02 -6.030e+02 -6.070e+02\n",
      " -6.200e+02 -6.300e+02  6.370e+02  6.450e+02  6.460e+02 -6.480e+02\n",
      " -6.620e+02  6.790e+02  6.830e+02 -6.850e+02 -6.890e+02 -6.930e+02\n",
      " -6.930e+02 -7.000e+02 -7.140e+02  7.220e+02 -7.220e+02  7.270e+02\n",
      " -7.350e+02 -7.490e+02 -7.500e+02 -7.550e+02 -7.620e+02  7.680e+02\n",
      "  7.700e+02 -7.740e+02 -7.770e+02  7.820e+02 -7.850e+02 -7.880e+02\n",
      "  7.930e+02  8.190e+02  8.220e+02 -8.220e+02  8.280e+02 -8.300e+02\n",
      "  8.410e+02  8.450e+02  8.660e+02  8.780e+02  8.830e+02  8.850e+02\n",
      " -9.090e+02 -9.100e+02 -9.190e+02  9.270e+02  9.320e+02 -9.320e+02\n",
      " -9.320e+02 -9.450e+02  9.520e+02 -9.520e+02  9.530e+02 -9.630e+02\n",
      " -9.670e+02 -9.700e+02 -9.720e+02 -9.920e+02  9.920e+02  1.003e+03\n",
      " -1.011e+03 -1.018e+03 -1.018e+03  1.019e+03  1.045e+03 -1.046e+03\n",
      " -1.063e+03 -1.071e+03  1.075e+03  1.091e+03  1.092e+03 -1.106e+03\n",
      "  1.111e+03  1.111e+03 -1.120e+03  1.121e+03 -1.124e+03 -1.124e+03\n",
      " -1.126e+03 -1.130e+03 -1.130e+03 -1.132e+03 -1.133e+03  1.133e+03\n",
      " -1.133e+03 -1.137e+03 -1.140e+03 -1.165e+03 -1.168e+03 -1.169e+03\n",
      " -1.175e+03 -1.177e+03 -1.186e+03  1.200e+03  1.200e+03  1.230e+03\n",
      " -1.238e+03 -1.266e+03  1.270e+03 -1.274e+03 -1.290e+03 -1.291e+03\n",
      " -1.299e+03 -1.307e+03 -1.308e+03 -1.314e+03 -1.329e+03  1.337e+03\n",
      "  1.343e+03 -1.355e+03 -1.367e+03 -1.371e+03  1.371e+03 -1.373e+03\n",
      "  1.378e+03 -1.380e+03 -1.384e+03 -1.385e+03 -1.385e+03 -1.398e+03\n",
      "  1.404e+03 -1.411e+03 -1.413e+03 -1.413e+03 -1.416e+03  1.417e+03\n",
      " -1.423e+03  1.432e+03 -1.435e+03 -1.450e+03 -1.459e+03  1.463e+03\n",
      " -1.471e+03 -1.476e+03 -1.478e+03 -1.485e+03 -1.485e+03 -1.487e+03\n",
      " -1.489e+03 -1.491e+03 -1.493e+03  1.493e+03 -1.495e+03 -1.496e+03\n",
      " -1.498e+03 -1.499e+03 -1.502e+03 -1.508e+03 -1.516e+03 -1.520e+03\n",
      " -1.521e+03 -1.525e+03 -1.528e+03 -1.531e+03 -1.552e+03 -1.559e+03\n",
      "  1.567e+03  1.588e+03  1.589e+03  1.590e+03  1.598e+03 -1.604e+03\n",
      " -1.608e+03  1.610e+03  1.620e+03 -1.621e+03 -1.624e+03  1.625e+03\n",
      "  1.625e+03 -1.632e+03  1.657e+03 -1.657e+03 -1.666e+03 -1.670e+03\n",
      " -1.683e+03  1.696e+03 -1.729e+03 -1.731e+03 -1.733e+03 -1.746e+03\n",
      " -1.759e+03 -1.778e+03 -1.779e+03 -1.782e+03 -1.785e+03 -1.787e+03\n",
      " -1.789e+03 -1.790e+03 -1.793e+03 -1.834e+03 -1.843e+03 -1.854e+03\n",
      " -1.871e+03 -1.879e+03 -1.883e+03 -1.885e+03 -1.886e+03 -1.888e+03\n",
      " -1.889e+03 -1.893e+03  1.912e+03  1.912e+03 -1.924e+03 -1.928e+03\n",
      " -1.929e+03 -1.935e+03 -1.952e+03 -1.955e+03 -1.955e+03  1.964e+03\n",
      "  1.980e+03 -1.993e+03 -1.997e+03 -2.004e+03 -2.009e+03 -2.014e+03\n",
      " -2.016e+03 -2.017e+03 -2.038e+03 -2.067e+03 -2.080e+03 -2.087e+03\n",
      "  2.090e+03  2.105e+03 -2.133e+03 -2.172e+03 -2.172e+03 -2.186e+03\n",
      "  2.190e+03 -2.208e+03 -2.226e+03  2.227e+03 -2.231e+03  2.241e+03\n",
      " -2.246e+03  2.256e+03 -2.257e+03 -2.259e+03 -2.263e+03 -2.270e+03\n",
      " -2.274e+03 -2.283e+03  2.299e+03  2.343e+03 -2.361e+03 -2.378e+03\n",
      "  2.386e+03 -2.392e+03 -2.412e+03  2.419e+03 -2.423e+03 -2.430e+03\n",
      "  2.454e+03 -2.461e+03 -2.489e+03 -2.504e+03 -2.508e+03 -2.531e+03\n",
      " -2.552e+03 -2.554e+03  2.564e+03  2.601e+03 -2.609e+03 -2.630e+03\n",
      " -2.688e+03 -2.718e+03 -2.722e+03  2.752e+03 -2.754e+03  2.764e+03\n",
      " -2.782e+03 -2.789e+03  2.830e+03 -2.839e+03 -2.859e+03 -2.865e+03\n",
      " -2.868e+03 -2.873e+03 -2.881e+03 -2.964e+03 -3.037e+03 -3.074e+03\n",
      " -3.205e+03 -3.205e+03 -3.222e+03 -3.229e+03 -3.271e+03 -3.302e+03\n",
      " -3.328e+03 -3.341e+03 -3.377e+03 -3.392e+03 -3.409e+03 -3.431e+03\n",
      " -3.451e+03 -3.498e+03 -3.519e+03  3.554e+03  3.615e+03 -3.631e+03\n",
      " -3.639e+03 -3.728e+03 -3.736e+03 -3.744e+03 -3.834e+03 -3.841e+03\n",
      " -3.936e+03 -3.944e+03 -3.987e+03 -3.989e+03 -4.067e+03]\n",
      "Concordance Index 0.7193244860219732\n",
      "Integrated Brier Score: 0.17619746920053658\n",
      "y_train breslow final [ 2.000e+00 -3.000e+00 -1.300e+01 -1.600e+01 -1.600e+01 -1.600e+01\n",
      "  1.800e+01 -2.600e+01 -2.900e+01 -3.600e+01  4.100e+01  4.200e+01\n",
      "  4.300e+01 -4.300e+01  5.100e+01 -5.300e+01  6.200e+01  6.500e+01\n",
      "  6.900e+01  9.900e+01  1.010e+02  1.060e+02  1.100e+02 -1.180e+02\n",
      " -1.190e+02 -1.270e+02  1.370e+02  1.390e+02  1.390e+02 -1.410e+02\n",
      " -1.500e+02 -1.550e+02  1.620e+02  1.640e+02 -1.820e+02  1.820e+02\n",
      "  1.830e+02 -1.930e+02  2.020e+02  2.040e+02 -2.040e+02 -2.050e+02\n",
      "  2.060e+02 -2.060e+02  2.110e+02 -2.110e+02  2.220e+02  2.380e+02\n",
      "  2.420e+02  2.450e+02 -2.560e+02 -2.930e+02  3.070e+02  3.130e+02\n",
      "  3.130e+02 -3.230e+02  3.290e+02  3.300e+02  3.330e+02  3.340e+02\n",
      "  3.360e+02 -3.400e+02  3.420e+02  3.440e+02 -3.540e+02 -3.550e+02\n",
      "  3.620e+02 -3.620e+02 -3.650e+02 -3.650e+02 -3.690e+02 -3.700e+02\n",
      " -3.730e+02 -3.740e+02  3.750e+02 -3.860e+02 -4.000e+02 -4.060e+02\n",
      " -4.110e+02  4.310e+02 -4.330e+02 -4.350e+02  4.450e+02 -4.540e+02\n",
      "  4.590e+02 -4.680e+02  4.750e+02  4.780e+02  4.800e+02  4.850e+02\n",
      " -4.950e+02 -5.010e+02 -5.050e+02 -5.070e+02  5.100e+02 -5.190e+02\n",
      " -5.230e+02 -5.510e+02  5.520e+02  5.610e+02  5.610e+02  5.620e+02\n",
      "  5.630e+02 -5.630e+02 -5.670e+02  5.710e+02  5.720e+02 -5.740e+02\n",
      "  5.740e+02  5.780e+02  5.870e+02  6.000e+02 -6.030e+02 -6.070e+02\n",
      " -6.200e+02 -6.300e+02  6.370e+02  6.450e+02  6.460e+02 -6.480e+02\n",
      " -6.620e+02  6.790e+02  6.830e+02 -6.850e+02 -6.890e+02 -6.930e+02\n",
      " -6.930e+02 -7.000e+02 -7.140e+02  7.220e+02 -7.220e+02  7.270e+02\n",
      " -7.350e+02 -7.490e+02 -7.500e+02 -7.550e+02 -7.620e+02  7.680e+02\n",
      "  7.700e+02 -7.740e+02 -7.770e+02  7.820e+02 -7.850e+02 -7.880e+02\n",
      "  7.930e+02  8.190e+02  8.220e+02 -8.220e+02  8.280e+02 -8.300e+02\n",
      "  8.410e+02  8.450e+02  8.660e+02  8.780e+02  8.830e+02  8.850e+02\n",
      " -9.090e+02 -9.100e+02 -9.190e+02  9.270e+02  9.320e+02 -9.320e+02\n",
      " -9.320e+02 -9.450e+02  9.520e+02 -9.520e+02  9.530e+02 -9.630e+02\n",
      " -9.670e+02 -9.700e+02 -9.720e+02 -9.920e+02  9.920e+02  1.003e+03\n",
      " -1.011e+03 -1.018e+03 -1.018e+03  1.019e+03  1.045e+03 -1.046e+03\n",
      " -1.063e+03 -1.071e+03  1.075e+03  1.091e+03  1.092e+03 -1.106e+03\n",
      "  1.111e+03  1.111e+03 -1.120e+03  1.121e+03 -1.124e+03 -1.124e+03\n",
      " -1.126e+03 -1.130e+03 -1.130e+03 -1.132e+03 -1.133e+03  1.133e+03\n",
      " -1.133e+03 -1.137e+03 -1.140e+03 -1.165e+03 -1.168e+03 -1.169e+03\n",
      " -1.175e+03 -1.177e+03 -1.186e+03  1.200e+03  1.200e+03  1.230e+03\n",
      " -1.238e+03 -1.266e+03  1.270e+03 -1.274e+03 -1.290e+03 -1.291e+03\n",
      " -1.299e+03 -1.307e+03 -1.308e+03 -1.314e+03 -1.329e+03  1.337e+03\n",
      "  1.343e+03 -1.355e+03 -1.367e+03 -1.371e+03  1.371e+03 -1.373e+03\n",
      "  1.378e+03 -1.380e+03 -1.384e+03 -1.385e+03 -1.385e+03 -1.398e+03\n",
      "  1.404e+03 -1.411e+03 -1.413e+03 -1.413e+03 -1.416e+03  1.417e+03\n",
      " -1.423e+03  1.432e+03 -1.435e+03 -1.450e+03 -1.459e+03  1.463e+03\n",
      " -1.471e+03 -1.476e+03 -1.478e+03 -1.485e+03 -1.485e+03 -1.487e+03\n",
      " -1.489e+03 -1.491e+03 -1.493e+03  1.493e+03 -1.495e+03 -1.496e+03\n",
      " -1.498e+03 -1.499e+03 -1.502e+03 -1.508e+03 -1.516e+03 -1.520e+03\n",
      " -1.521e+03 -1.525e+03 -1.528e+03 -1.531e+03 -1.552e+03 -1.559e+03\n",
      "  1.567e+03  1.588e+03  1.589e+03  1.590e+03  1.598e+03 -1.604e+03\n",
      " -1.608e+03  1.610e+03  1.620e+03 -1.621e+03 -1.624e+03  1.625e+03\n",
      "  1.625e+03 -1.632e+03  1.657e+03 -1.657e+03 -1.666e+03 -1.670e+03\n",
      " -1.683e+03  1.696e+03 -1.729e+03 -1.731e+03 -1.733e+03 -1.746e+03\n",
      " -1.759e+03 -1.778e+03 -1.779e+03 -1.782e+03 -1.785e+03 -1.787e+03\n",
      " -1.789e+03 -1.790e+03 -1.793e+03 -1.834e+03 -1.843e+03 -1.854e+03\n",
      " -1.871e+03 -1.879e+03 -1.883e+03 -1.885e+03 -1.886e+03 -1.888e+03\n",
      " -1.889e+03 -1.893e+03  1.912e+03  1.912e+03 -1.924e+03 -1.928e+03\n",
      " -1.929e+03 -1.935e+03 -1.952e+03 -1.955e+03 -1.955e+03  1.964e+03\n",
      "  1.980e+03 -1.993e+03 -1.997e+03 -2.004e+03 -2.009e+03 -2.014e+03\n",
      " -2.016e+03 -2.017e+03 -2.038e+03 -2.067e+03 -2.080e+03 -2.087e+03\n",
      "  2.090e+03  2.105e+03 -2.133e+03 -2.172e+03 -2.172e+03 -2.186e+03\n",
      "  2.190e+03 -2.208e+03 -2.226e+03  2.227e+03 -2.231e+03  2.241e+03\n",
      " -2.246e+03  2.256e+03 -2.257e+03 -2.259e+03 -2.263e+03 -2.270e+03\n",
      " -2.274e+03 -2.283e+03  2.299e+03  2.343e+03 -2.361e+03 -2.378e+03\n",
      "  2.386e+03 -2.392e+03 -2.412e+03  2.419e+03 -2.423e+03 -2.430e+03\n",
      "  2.454e+03 -2.461e+03 -2.489e+03 -2.504e+03 -2.508e+03 -2.531e+03\n",
      " -2.552e+03 -2.554e+03  2.564e+03  2.601e+03 -2.609e+03 -2.630e+03\n",
      " -2.688e+03 -2.718e+03 -2.722e+03  2.752e+03 -2.754e+03  2.764e+03\n",
      " -2.782e+03 -2.789e+03  2.830e+03 -2.839e+03 -2.859e+03 -2.865e+03\n",
      " -2.868e+03 -2.873e+03 -2.881e+03 -2.964e+03 -3.037e+03 -3.074e+03\n",
      " -3.205e+03 -3.205e+03 -3.222e+03 -3.229e+03 -3.271e+03 -3.302e+03\n",
      " -3.328e+03 -3.341e+03 -3.377e+03 -3.392e+03 -3.409e+03 -3.431e+03\n",
      " -3.451e+03 -3.498e+03 -3.519e+03  3.554e+03  3.615e+03 -3.631e+03\n",
      " -3.639e+03 -3.728e+03 -3.736e+03 -3.744e+03 -3.834e+03 -3.841e+03\n",
      " -3.936e+03 -3.944e+03 -3.987e+03 -3.989e+03 -4.067e+03]\n",
      "durations 3.0 4537.0\n",
      "Concordance Index 0.6560364464692483\n",
      "Integrated Brier Score: 0.1855597006132907\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m65.8425\u001b[0m  0.0064\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m85.2064\u001b[0m  0.0048\n",
      "      2       65.8425  0.0037\n",
      "      2       85.2064  0.0031\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m76.5408\u001b[0m  0.0050\n",
      "      3       85.2064  0.0029\n",
      "      3       65.8425  0.0031\n",
      "      4       85.2064  0.0032\n",
      "      4       65.8425  0.0036\n",
      "      2       76.5408  0.0054\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m79.5612\u001b[0m       \u001b[32m55.5711\u001b[0m  0.1072\n",
      "      5       65.8425  0.0039\n",
      "      5       85.2064  0.0053\n",
      "      3       76.5408  0.0046\n",
      "      6       65.8425  0.0043\n",
      "      6       85.2064  0.0048\n",
      "      4       76.5408  0.0047\n",
      "      7       65.8425  0.0045\n",
      "      7       85.2064  0.0047\n",
      "      5       76.5408  0.0045\n",
      "      8       65.8425  0.0031\n",
      "      6       76.5408  0.0030\n",
      "      9       65.8425  0.0031\n",
      "      7       76.5408  0.0036\n",
      "      8       85.2064  0.0077\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m80.2196\u001b[0m       \u001b[32m46.7074\u001b[0m  0.1115\n",
      "     10       65.8425  0.0035\n",
      "Restoring best model from epoch 1.\n",
      "      9       85.2064  0.0031\n",
      "     10       85.2064  0.0030\n",
      "      8       76.5408  0.0066\n",
      "Restoring best model from epoch 1.\n",
      "      9       76.5408  0.0031\n",
      "     10       76.5408  0.0029\n",
      "Restoring best model from epoch 1.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m84.8132\u001b[0m       \u001b[32m64.1648\u001b[0m  0.1255\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m73.5501\u001b[0m       \u001b[32m48.7258\u001b[0m  0.1154\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m150.9027\u001b[0m  0.0098\n",
      "      2      150.9027  0.0075\n",
      "      3      150.9027  0.0030\n",
      "      4      150.9027  0.0029\n",
      "      5      150.9027  0.0030\n",
      "      6      150.9027  0.0038\n",
      "      2       83.4882       61.9063  0.1107\n",
      "      7      150.9027  0.0098\n",
      "      8      150.9027  0.0029\n",
      "      9      150.9027  0.0029\n",
      "     10      150.9027  0.0033\n",
      "Restoring best model from epoch 1.\n",
      "      2       84.7094       52.5474  0.1063\n",
      "      2       96.1464      191.5819  0.0970\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2       84.0664       86.8647  0.1039\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m76.6803\u001b[0m  0.0128\n",
      "      2       76.6803  0.0065\n",
      "      3       76.6803  0.0060\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      4       76.6803  0.0088\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      5       76.6803  0.0066\n",
      "      6       76.6803  0.0035\n",
      "      7       76.6803  0.0031\n",
      "      8       76.6803  0.0029\n",
      "      9       76.6803  0.0032\n",
      "     10       76.6803  0.0029\n",
      "Restoring best model from epoch 1.\n",
      "      3       81.3557       \u001b[32m53.8698\u001b[0m  0.0984\n",
      "      3       86.7233       72.1175  0.0980\n",
      "      3       83.1539       \u001b[32m44.0390\u001b[0m  0.1094\n",
      "      3       76.2884      121.9135  0.0991\n",
      "      4       79.5720       60.6067  0.0845\n",
      "      4       92.6107       73.3789  0.0895\n",
      "      4       84.2275       60.8360  0.0879\n",
      "      4       77.2824       70.1706  0.1146\n",
      "      5       90.5829       56.2641  0.0904\n",
      "      5       \u001b[36m84.7845\u001b[0m       77.7879  0.0857\n",
      "      5       83.7716       87.4464  0.0826\n",
      "      5       \u001b[36m72.2201\u001b[0m       \u001b[32m47.9631\u001b[0m  0.0765\n",
      "      6       \u001b[36m78.1747\u001b[0m       \u001b[32m49.3870\u001b[0m  0.0735\n",
      "      6       \u001b[36m82.9028\u001b[0m       75.4887  0.0742\n",
      "      6       \u001b[36m78.2349\u001b[0m       55.1802  0.0732\n",
      "      6       \u001b[36m71.5409\u001b[0m       63.8232  0.0731\n",
      "      7       78.4976       52.3331  0.0741\n",
      "      7       88.9691       68.2451  0.0745\n",
      "      7       78.3830       44.3553  0.0747\n",
      "      7       75.9101       61.0656  0.0724\n",
      "      8       83.1403       51.7279  0.0726\n",
      "      8       \u001b[36m80.7918\u001b[0m       64.7274  0.0732\n",
      "      8       80.1322       \u001b[32m43.9420\u001b[0m  0.0729\n",
      "      8       72.2255       48.9447  0.0722\n",
      "      9       78.4219       52.6729  0.0737\n",
      "      9       86.3537       65.2471  0.0730\n",
      "      9       83.1493       \u001b[32m43.6037\u001b[0m  0.0739\n",
      "      9       \u001b[36m71.3625\u001b[0m       \u001b[32m47.1652\u001b[0m  0.0724\n",
      "     10       \u001b[36m77.8699\u001b[0m       52.5647  0.0734\n",
      "Restoring best model from epoch 6.\n",
      "     10       87.9875       \u001b[32m63.7022\u001b[0m  0.0730\n",
      "     10       \u001b[36m77.5020\u001b[0m       \u001b[32m41.9525\u001b[0m  0.0732\n",
      "     10       72.8072       47.4501  0.0734\n",
      "Restoring best model from epoch 9.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m141.1963\u001b[0m  0.0045\n",
      "      2      141.1963  0.0038\n",
      "      3      141.1963  0.0038\n",
      "      4      141.1963  0.0037\n",
      "      5      141.1963  0.0036\n",
      "      6      141.1963  0.0059\n",
      "      7      141.1963  0.0054\n",
      "      8      141.1963  0.0048\n",
      "      9      141.1963  0.0041\n",
      "     10      141.1963  0.0040\n",
      "Restoring best model from epoch 1.\n",
      "y_train breslow final [ 2.000e+00 -3.000e+00 -3.000e+00 -7.000e+00 -1.100e+01 -1.300e+01\n",
      " -1.600e+01 -1.600e+01 -1.800e+01  1.800e+01 -2.600e+01 -3.600e+01\n",
      " -3.800e+01  4.200e+01  4.300e+01 -4.300e+01  5.100e+01 -5.300e+01\n",
      "  5.900e+01 -6.100e+01  6.200e+01  6.500e+01  6.800e+01  6.900e+01\n",
      "  7.300e+01  7.700e+01  9.300e+01  9.900e+01  1.010e+02  1.060e+02\n",
      "  1.090e+02  1.100e+02 -1.180e+02 -1.190e+02 -1.270e+02  1.370e+02\n",
      "  1.390e+02  1.390e+02 -1.410e+02 -1.500e+02  1.620e+02  1.640e+02\n",
      " -1.660e+02  1.660e+02  1.680e+02 -1.750e+02 -1.770e+02 -1.820e+02\n",
      "  1.820e+02  1.830e+02 -1.930e+02 -1.940e+02  2.020e+02 -2.040e+02\n",
      "  2.060e+02 -2.060e+02  2.110e+02 -2.110e+02  2.220e+02  2.240e+02\n",
      "  2.380e+02  2.420e+02  2.450e+02  3.070e+02  3.110e+02  3.130e+02\n",
      " -3.190e+02  3.200e+02  3.290e+02  3.300e+02  3.340e+02  3.360e+02\n",
      " -3.400e+02  3.440e+02 -3.540e+02 -3.550e+02 -3.560e+02  3.620e+02\n",
      " -3.620e+02 -3.650e+02 -3.700e+02 -3.720e+02 -3.740e+02  3.750e+02\n",
      " -3.850e+02 -3.860e+02 -4.060e+02 -4.080e+02 -4.110e+02 -4.310e+02\n",
      " -4.350e+02  4.450e+02  4.460e+02  4.540e+02 -4.540e+02 -4.680e+02\n",
      " -4.690e+02  4.750e+02  4.780e+02  4.800e+02  4.850e+02 -4.950e+02\n",
      " -4.980e+02 -5.010e+02 -5.050e+02 -5.070e+02  5.100e+02 -5.110e+02\n",
      " -5.190e+02 -5.230e+02 -5.280e+02 -5.510e+02  5.610e+02  5.620e+02\n",
      " -5.670e+02  5.710e+02  5.720e+02 -5.740e+02  5.780e+02  5.870e+02\n",
      " -5.910e+02  6.000e+02 -6.030e+02 -6.070e+02 -6.170e+02 -6.200e+02\n",
      "  6.370e+02  6.450e+02  6.460e+02 -6.560e+02 -6.620e+02 -6.650e+02\n",
      "  6.790e+02  6.830e+02 -6.890e+02 -6.930e+02 -7.000e+02  7.010e+02\n",
      "  7.090e+02 -7.140e+02  7.270e+02  7.350e+02 -7.370e+02 -7.490e+02\n",
      " -7.500e+02 -7.550e+02 -7.620e+02  7.680e+02  7.700e+02 -7.740e+02\n",
      " -7.770e+02  7.820e+02 -7.850e+02 -7.880e+02  7.930e+02 -8.220e+02\n",
      "  8.340e+02 -8.400e+02 -8.610e+02  8.660e+02 -8.730e+02 -8.740e+02\n",
      "  8.780e+02  8.830e+02  8.850e+02 -9.090e+02 -9.100e+02  9.270e+02\n",
      "  9.320e+02 -9.320e+02 -9.450e+02  9.460e+02 -9.510e+02  9.530e+02\n",
      " -9.630e+02 -9.670e+02 -9.700e+02 -9.720e+02  9.920e+02  1.003e+03\n",
      " -1.011e+03 -1.014e+03 -1.018e+03  1.019e+03  1.034e+03  1.045e+03\n",
      " -1.046e+03 -1.063e+03 -1.071e+03  1.075e+03  1.091e+03 -1.092e+03\n",
      "  1.092e+03  1.097e+03 -1.106e+03 -1.107e+03  1.111e+03  1.111e+03\n",
      "  1.121e+03 -1.124e+03 -1.126e+03 -1.130e+03 -1.132e+03  1.133e+03\n",
      " -1.133e+03 -1.137e+03 -1.140e+03 -1.143e+03 -1.165e+03 -1.168e+03\n",
      " -1.169e+03  1.170e+03 -1.177e+03 -1.186e+03  1.191e+03 -1.217e+03\n",
      " -1.218e+03  1.230e+03 -1.238e+03  1.238e+03 -1.266e+03  1.270e+03\n",
      " -1.274e+03 -1.290e+03 -1.291e+03 -1.299e+03 -1.307e+03 -1.308e+03\n",
      " -1.314e+03  1.317e+03 -1.329e+03  1.343e+03 -1.355e+03 -1.367e+03\n",
      " -1.371e+03  1.371e+03 -1.373e+03 -1.380e+03 -1.384e+03 -1.385e+03\n",
      " -1.385e+03 -1.398e+03 -1.413e+03 -1.416e+03  1.417e+03  1.432e+03\n",
      " -1.433e+03 -1.435e+03 -1.436e+03 -1.459e+03 -1.462e+03 -1.478e+03\n",
      " -1.485e+03 -1.485e+03 -1.487e+03 -1.489e+03 -1.491e+03 -1.493e+03\n",
      "  1.493e+03 -1.495e+03 -1.496e+03 -1.498e+03 -1.502e+03 -1.516e+03\n",
      " -1.520e+03 -1.525e+03 -1.525e+03 -1.528e+03 -1.531e+03 -1.552e+03\n",
      " -1.559e+03 -1.560e+03  1.567e+03  1.584e+03  1.588e+03  1.589e+03\n",
      "  1.590e+03 -1.604e+03 -1.608e+03  1.610e+03  1.620e+03 -1.621e+03\n",
      " -1.624e+03  1.625e+03  1.625e+03  1.626e+03 -1.632e+03  1.639e+03\n",
      "  1.657e+03 -1.657e+03  1.661e+03 -1.666e+03 -1.670e+03 -1.683e+03\n",
      "  1.696e+03  1.714e+03  1.724e+03 -1.729e+03 -1.731e+03 -1.755e+03\n",
      " -1.759e+03 -1.778e+03 -1.779e+03 -1.782e+03 -1.785e+03 -1.787e+03\n",
      " -1.789e+03 -1.793e+03 -1.794e+03 -1.834e+03 -1.854e+03 -1.862e+03\n",
      " -1.876e+03 -1.879e+03 -1.886e+03 -1.888e+03 -1.905e+03 -1.906e+03\n",
      "  1.912e+03  1.913e+03 -1.924e+03 -1.928e+03 -1.929e+03 -1.935e+03\n",
      " -1.946e+03 -1.952e+03 -1.955e+03 -1.955e+03  1.964e+03  1.980e+03\n",
      "  1.986e+03 -1.993e+03 -2.009e+03 -2.038e+03 -2.067e+03 -2.080e+03\n",
      " -2.087e+03  2.090e+03  2.105e+03 -2.128e+03 -2.133e+03  2.145e+03\n",
      " -2.150e+03 -2.172e+03 -2.184e+03 -2.186e+03  2.190e+03 -2.208e+03\n",
      " -2.217e+03  2.227e+03 -2.231e+03  2.241e+03 -2.246e+03 -2.257e+03\n",
      " -2.259e+03 -2.263e+03 -2.270e+03 -2.271e+03 -2.274e+03 -2.283e+03\n",
      "  2.343e+03 -2.361e+03 -2.372e+03 -2.378e+03 -2.392e+03 -2.412e+03\n",
      " -2.422e+03 -2.430e+03 -2.439e+03  2.454e+03 -2.461e+03 -2.470e+03\n",
      " -2.504e+03 -2.531e+03 -2.554e+03  2.564e+03  2.601e+03 -2.609e+03\n",
      " -2.609e+03 -2.660e+03 -2.718e+03 -2.722e+03 -2.746e+03 -2.754e+03\n",
      "  2.764e+03 -2.782e+03 -2.789e+03 -2.799e+03  2.830e+03 -2.865e+03\n",
      " -2.868e+03 -2.873e+03 -2.881e+03 -2.964e+03 -3.037e+03 -3.074e+03\n",
      " -3.146e+03 -3.205e+03 -3.205e+03 -3.222e+03 -3.229e+03 -3.267e+03\n",
      " -3.271e+03 -3.302e+03 -3.331e+03 -3.341e+03 -3.343e+03 -3.377e+03\n",
      " -3.392e+03 -3.409e+03 -3.431e+03 -3.451e+03 -3.480e+03 -3.519e+03\n",
      " -3.583e+03 -3.639e+03 -3.744e+03 -3.834e+03 -3.841e+03 -3.944e+03\n",
      " -3.974e+03 -3.989e+03 -4.067e+03 -4.074e+03 -4.537e+03]\n",
      "Concordance Index 0.6203573742172317\n",
      "Integrated Brier Score: 0.3188012051582324\n",
      "y_train breslow final [ 2.000e+00 -3.000e+00 -3.000e+00 -7.000e+00 -1.100e+01 -1.300e+01\n",
      " -1.600e+01 -1.600e+01 -1.800e+01  1.800e+01 -2.600e+01 -3.600e+01\n",
      " -3.800e+01  4.200e+01  4.300e+01 -4.300e+01  5.100e+01 -5.300e+01\n",
      "  5.900e+01 -6.100e+01  6.200e+01  6.500e+01  6.800e+01  6.900e+01\n",
      "  7.300e+01  7.700e+01  9.300e+01  9.900e+01  1.010e+02  1.060e+02\n",
      "  1.090e+02  1.100e+02 -1.180e+02 -1.190e+02 -1.270e+02  1.370e+02\n",
      "  1.390e+02  1.390e+02 -1.410e+02 -1.500e+02  1.620e+02  1.640e+02\n",
      " -1.660e+02  1.660e+02  1.680e+02 -1.750e+02 -1.770e+02 -1.820e+02\n",
      "  1.820e+02  1.830e+02 -1.930e+02 -1.940e+02  2.020e+02 -2.040e+02\n",
      "  2.060e+02 -2.060e+02  2.110e+02 -2.110e+02  2.220e+02  2.240e+02\n",
      "  2.380e+02  2.420e+02  2.450e+02  3.070e+02  3.110e+02  3.130e+02\n",
      " -3.190e+02  3.200e+02  3.290e+02  3.300e+02  3.340e+02  3.360e+02\n",
      " -3.400e+02  3.440e+02 -3.540e+02 -3.550e+02 -3.560e+02  3.620e+02\n",
      " -3.620e+02 -3.650e+02 -3.700e+02 -3.720e+02 -3.740e+02  3.750e+02\n",
      " -3.850e+02 -3.860e+02 -4.060e+02 -4.080e+02 -4.110e+02 -4.310e+02\n",
      " -4.350e+02  4.450e+02  4.460e+02  4.540e+02 -4.540e+02 -4.680e+02\n",
      " -4.690e+02  4.750e+02  4.780e+02  4.800e+02  4.850e+02 -4.950e+02\n",
      " -4.980e+02 -5.010e+02 -5.050e+02 -5.070e+02  5.100e+02 -5.110e+02\n",
      " -5.190e+02 -5.230e+02 -5.280e+02 -5.510e+02  5.610e+02  5.620e+02\n",
      " -5.670e+02  5.710e+02  5.720e+02 -5.740e+02  5.780e+02  5.870e+02\n",
      " -5.910e+02  6.000e+02 -6.030e+02 -6.070e+02 -6.170e+02 -6.200e+02\n",
      "  6.370e+02  6.450e+02  6.460e+02 -6.560e+02 -6.620e+02 -6.650e+02\n",
      "  6.790e+02  6.830e+02 -6.890e+02 -6.930e+02 -7.000e+02  7.010e+02\n",
      "  7.090e+02 -7.140e+02  7.270e+02  7.350e+02 -7.370e+02 -7.490e+02\n",
      " -7.500e+02 -7.550e+02 -7.620e+02  7.680e+02  7.700e+02 -7.740e+02\n",
      " -7.770e+02  7.820e+02 -7.850e+02 -7.880e+02  7.930e+02 -8.220e+02\n",
      "  8.340e+02 -8.400e+02 -8.610e+02  8.660e+02 -8.730e+02 -8.740e+02\n",
      "  8.780e+02  8.830e+02  8.850e+02 -9.090e+02 -9.100e+02  9.270e+02\n",
      "  9.320e+02 -9.320e+02 -9.450e+02  9.460e+02 -9.510e+02  9.530e+02\n",
      " -9.630e+02 -9.670e+02 -9.700e+02 -9.720e+02  9.920e+02  1.003e+03\n",
      " -1.011e+03 -1.014e+03 -1.018e+03  1.019e+03  1.034e+03  1.045e+03\n",
      " -1.046e+03 -1.063e+03 -1.071e+03  1.075e+03  1.091e+03 -1.092e+03\n",
      "  1.092e+03  1.097e+03 -1.106e+03 -1.107e+03  1.111e+03  1.111e+03\n",
      "  1.121e+03 -1.124e+03 -1.126e+03 -1.130e+03 -1.132e+03  1.133e+03\n",
      " -1.133e+03 -1.137e+03 -1.140e+03 -1.143e+03 -1.165e+03 -1.168e+03\n",
      " -1.169e+03  1.170e+03 -1.177e+03 -1.186e+03  1.191e+03 -1.217e+03\n",
      " -1.218e+03  1.230e+03 -1.238e+03  1.238e+03 -1.266e+03  1.270e+03\n",
      " -1.274e+03 -1.290e+03 -1.291e+03 -1.299e+03 -1.307e+03 -1.308e+03\n",
      " -1.314e+03  1.317e+03 -1.329e+03  1.343e+03 -1.355e+03 -1.367e+03\n",
      " -1.371e+03  1.371e+03 -1.373e+03 -1.380e+03 -1.384e+03 -1.385e+03\n",
      " -1.385e+03 -1.398e+03 -1.413e+03 -1.416e+03  1.417e+03  1.432e+03\n",
      " -1.433e+03 -1.435e+03 -1.436e+03 -1.459e+03 -1.462e+03 -1.478e+03\n",
      " -1.485e+03 -1.485e+03 -1.487e+03 -1.489e+03 -1.491e+03 -1.493e+03\n",
      "  1.493e+03 -1.495e+03 -1.496e+03 -1.498e+03 -1.502e+03 -1.516e+03\n",
      " -1.520e+03 -1.525e+03 -1.525e+03 -1.528e+03 -1.531e+03 -1.552e+03\n",
      " -1.559e+03 -1.560e+03  1.567e+03  1.584e+03  1.588e+03  1.589e+03\n",
      "  1.590e+03 -1.604e+03 -1.608e+03  1.610e+03  1.620e+03 -1.621e+03\n",
      " -1.624e+03  1.625e+03  1.625e+03  1.626e+03 -1.632e+03  1.639e+03\n",
      "  1.657e+03 -1.657e+03  1.661e+03 -1.666e+03 -1.670e+03 -1.683e+03\n",
      "  1.696e+03  1.714e+03  1.724e+03 -1.729e+03 -1.731e+03 -1.755e+03\n",
      " -1.759e+03 -1.778e+03 -1.779e+03 -1.782e+03 -1.785e+03 -1.787e+03\n",
      " -1.789e+03 -1.793e+03 -1.794e+03 -1.834e+03 -1.854e+03 -1.862e+03\n",
      " -1.876e+03 -1.879e+03 -1.886e+03 -1.888e+03 -1.905e+03 -1.906e+03\n",
      "  1.912e+03  1.913e+03 -1.924e+03 -1.928e+03 -1.929e+03 -1.935e+03\n",
      " -1.946e+03 -1.952e+03 -1.955e+03 -1.955e+03  1.964e+03  1.980e+03\n",
      "  1.986e+03 -1.993e+03 -2.009e+03 -2.038e+03 -2.067e+03 -2.080e+03\n",
      " -2.087e+03  2.090e+03  2.105e+03 -2.128e+03 -2.133e+03  2.145e+03\n",
      " -2.150e+03 -2.172e+03 -2.184e+03 -2.186e+03  2.190e+03 -2.208e+03\n",
      " -2.217e+03  2.227e+03 -2.231e+03  2.241e+03 -2.246e+03 -2.257e+03\n",
      " -2.259e+03 -2.263e+03 -2.270e+03 -2.271e+03 -2.274e+03 -2.283e+03\n",
      "  2.343e+03 -2.361e+03 -2.372e+03 -2.378e+03 -2.392e+03 -2.412e+03\n",
      " -2.422e+03 -2.430e+03 -2.439e+03  2.454e+03 -2.461e+03 -2.470e+03\n",
      " -2.504e+03 -2.531e+03 -2.554e+03  2.564e+03  2.601e+03 -2.609e+03\n",
      " -2.609e+03 -2.660e+03 -2.718e+03 -2.722e+03 -2.746e+03 -2.754e+03\n",
      "  2.764e+03 -2.782e+03 -2.789e+03 -2.799e+03  2.830e+03 -2.865e+03\n",
      " -2.868e+03 -2.873e+03 -2.881e+03 -2.964e+03 -3.037e+03 -3.074e+03\n",
      " -3.146e+03 -3.205e+03 -3.205e+03 -3.222e+03 -3.229e+03 -3.267e+03\n",
      " -3.271e+03 -3.302e+03 -3.331e+03 -3.341e+03 -3.343e+03 -3.377e+03\n",
      " -3.392e+03 -3.409e+03 -3.431e+03 -3.451e+03 -3.480e+03 -3.519e+03\n",
      " -3.583e+03 -3.639e+03 -3.744e+03 -3.834e+03 -3.841e+03 -3.944e+03\n",
      " -3.974e+03 -3.989e+03 -4.067e+03 -4.074e+03 -4.537e+03]\n",
      "durations 16.0 3987.0\n",
      "Concordance Index 0.5162505887894488\n",
      "Integrated Brier Score: 0.30233277166406375\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m78.4893\u001b[0m  0.0044\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2       78.4893  0.0104\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      3       78.4893  0.0030\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      4       78.4893  0.0048\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      5       78.4893  0.0063\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m86.6475\u001b[0m  0.0070\n",
      "      6       78.4893  0.0075\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m93.5130\u001b[0m  0.0055\n",
      "      2       86.6475  0.0046\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      7       78.4893  0.0034\n",
      "      3       86.6475  0.0031\n",
      "      2       93.5130  0.0045\n",
      "      8       78.4893  0.0029\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      4       86.6475  0.0030\n",
      "      3       93.5130  0.0031\n",
      "      9       78.4893  0.0028\n",
      "      5       86.6475  0.0029\n",
      "      4       93.5130  0.0029\n",
      "     10       78.4893  0.0032\n",
      "Restoring best model from epoch 1.\n",
      "      6       86.6475  0.0029\n",
      "      5       93.5130  0.0032\n",
      "      7       86.6475  0.0038\n",
      "      8       86.6475  0.0029\n",
      "      6       93.5130  0.0055\n",
      "      9       86.6475  0.0028\n",
      "      7       93.5130  0.0028\n",
      "     10       86.6475  0.0030\n",
      "Restoring best model from epoch 1.\n",
      "      8       93.5130  0.0039\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      9       93.5130  0.0030\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "     10       93.5130  0.0029\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m122.2841\u001b[0m  0.0064\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m79.6784\u001b[0m      \u001b[32m108.7907\u001b[0m  0.1114\n",
      "      2      122.2841  0.0058\n",
      "      3      122.2841  0.0032\n",
      "      4      122.2841  0.0048\n",
      "      5      122.2841  0.0038\n",
      "      6      122.2841  0.0035\n",
      "      7      122.2841  0.0038\n",
      "      8      122.2841  0.0033\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      9      122.2841  0.0031\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "     10      122.2841  0.0031\n",
      "Restoring best model from epoch 1.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m104.4218\u001b[0m  0.0071\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m85.5775\u001b[0m       \u001b[32m84.1507\u001b[0m  0.1043\n",
      "      2      104.4218  0.0036\n",
      "      3      104.4218  0.0033\n",
      "      4      104.4218  0.0033\n",
      "      5      104.4218  0.0030\n",
      "      6      104.4218  0.0030\n",
      "      7      104.4218  0.0030\n",
      "      8      104.4218  0.0029\n",
      "      9      104.4218  0.0029\n",
      "     10      104.4218  0.0031\n",
      "Restoring best model from epoch 1.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m70.3133\u001b[0m       \u001b[32m44.0431\u001b[0m  0.1060\n",
      "      2       84.0480       \u001b[32m81.1675\u001b[0m  0.0951\n",
      "      2       88.0685       \u001b[32m83.5685\u001b[0m  0.0825\n",
      "      2       85.0145      162.2955  0.0887\n",
      "      3       82.5159       \u001b[32m69.3057\u001b[0m  0.0840\n",
      "      3       \u001b[36m78.6844\u001b[0m       \u001b[32m82.5902\u001b[0m  0.0949\n",
      "      3       72.5651       57.1576  0.0792\n",
      "      4       84.7784       73.0973  0.0818\n",
      "      4       87.4282       \u001b[32m79.2636\u001b[0m  0.0766\n",
      "      4       74.4638       87.1649  0.0737\n",
      "      5       \u001b[36m79.3814\u001b[0m       \u001b[32m67.7229\u001b[0m  0.0784\n",
      "      5       85.4937       \u001b[32m77.3107\u001b[0m  0.0751\n",
      "      5       70.5783       97.4339  0.0711\n",
      "      6       79.5917       69.9932  0.0740\n",
      "      6       82.1294       79.5047  0.0745\n",
      "      6       \u001b[36m65.7428\u001b[0m       44.7464  0.0710\n",
      "      7       84.6243       68.9365  0.0730\n",
      "      7       \u001b[36m78.0361\u001b[0m       82.3548  0.0745\n",
      "      7       68.8745       \u001b[32m39.6159\u001b[0m  0.0724\n",
      "      8       \u001b[36m78.0288\u001b[0m       \u001b[32m63.4301\u001b[0m  0.0799\n",
      "      8       \u001b[36m65.3845\u001b[0m       39.8695  0.0777\n",
      "      8       \u001b[36m76.8269\u001b[0m       82.1470  0.0874\n",
      "      9       78.8956       \u001b[32m59.7011\u001b[0m  0.0757\n",
      "      9       70.5915       \u001b[32m38.0727\u001b[0m  0.0712\n",
      "      9       78.0214       82.4777  0.0736\n",
      "     10       79.2461       61.3113  0.0745\n",
      "Restoring best model from epoch 9.\n",
      "     10       67.3687       43.7836  0.0742\n",
      "Restoring best model from epoch 9.\n",
      "     10       80.3202       83.8173  0.0738\n",
      "Restoring best model from epoch 5.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m125.6441\u001b[0m  0.0042\n",
      "      2      125.6441  0.0046\n",
      "      3      125.6441  0.0035\n",
      "      4      125.6441  0.0038\n",
      "      5      125.6441  0.0036\n",
      "      6      125.6441  0.0047\n",
      "      7      125.6441  0.0049\n",
      "      8      125.6441  0.0053\n",
      "      9      125.6441  0.0058\n",
      "     10      125.6441  0.0045\n",
      "Restoring best model from epoch 1.\n",
      "y_train breslow final [ 2.000e+00 -3.000e+00 -3.000e+00 -7.000e+00 -1.100e+01 -1.600e+01\n",
      " -1.600e+01 -1.800e+01  1.800e+01 -2.600e+01 -2.900e+01 -3.600e+01\n",
      " -3.800e+01  4.100e+01  4.300e+01  5.900e+01 -6.100e+01  6.200e+01\n",
      "  6.800e+01  7.300e+01  7.700e+01  9.300e+01  9.900e+01  1.010e+02\n",
      "  1.090e+02  1.100e+02 -1.180e+02 -1.190e+02  1.370e+02  1.390e+02\n",
      " -1.550e+02  1.640e+02 -1.660e+02  1.660e+02  1.680e+02 -1.750e+02\n",
      " -1.770e+02 -1.820e+02  1.830e+02 -1.940e+02  2.020e+02  2.040e+02\n",
      " -2.040e+02 -2.050e+02  2.060e+02 -2.060e+02  2.110e+02 -2.110e+02\n",
      "  2.220e+02  2.240e+02  2.450e+02 -2.560e+02 -2.930e+02  3.070e+02\n",
      "  3.110e+02  3.130e+02 -3.190e+02  3.200e+02 -3.230e+02  3.290e+02\n",
      "  3.300e+02  3.330e+02  3.340e+02  3.360e+02 -3.400e+02  3.420e+02\n",
      " -3.550e+02 -3.560e+02  3.620e+02 -3.620e+02 -3.650e+02 -3.690e+02\n",
      " -3.700e+02 -3.720e+02 -3.730e+02 -3.740e+02  3.750e+02 -3.850e+02\n",
      " -3.860e+02 -4.000e+02 -4.080e+02 -4.110e+02  4.310e+02 -4.310e+02\n",
      " -4.330e+02 -4.350e+02  4.460e+02  4.540e+02  4.590e+02 -4.680e+02\n",
      " -4.690e+02  4.750e+02  4.780e+02  4.800e+02  4.800e+02  4.850e+02\n",
      " -4.980e+02 -5.010e+02 -5.050e+02 -5.070e+02  5.100e+02 -5.110e+02\n",
      " -5.190e+02 -5.230e+02 -5.280e+02  5.520e+02  5.610e+02  5.610e+02\n",
      "  5.620e+02  5.630e+02 -5.630e+02 -5.670e+02  5.710e+02  5.720e+02\n",
      " -5.740e+02  5.740e+02  5.870e+02 -5.910e+02  6.000e+02 -6.170e+02\n",
      " -6.200e+02 -6.300e+02  6.370e+02  6.460e+02 -6.480e+02 -6.560e+02\n",
      " -6.620e+02 -6.650e+02 -6.850e+02 -6.930e+02 -6.930e+02 -7.000e+02\n",
      "  7.010e+02  7.090e+02  7.220e+02 -7.220e+02  7.270e+02  7.350e+02\n",
      " -7.350e+02 -7.370e+02 -7.500e+02  7.700e+02 -7.740e+02 -7.770e+02\n",
      "  7.820e+02 -7.880e+02  7.930e+02  8.190e+02  8.220e+02 -8.220e+02\n",
      "  8.280e+02 -8.300e+02  8.340e+02 -8.400e+02  8.410e+02  8.450e+02\n",
      " -8.610e+02 -8.730e+02 -8.740e+02  8.780e+02  8.830e+02  8.850e+02\n",
      "  8.850e+02 -9.190e+02  9.320e+02 -9.320e+02 -9.320e+02  9.460e+02\n",
      " -9.510e+02  9.520e+02 -9.520e+02  9.530e+02 -9.630e+02 -9.670e+02\n",
      " -9.700e+02 -9.720e+02 -9.920e+02 -1.011e+03 -1.014e+03 -1.018e+03\n",
      " -1.018e+03  1.034e+03  1.045e+03 -1.092e+03  1.097e+03 -1.106e+03\n",
      " -1.107e+03  1.111e+03  1.111e+03 -1.120e+03  1.121e+03 -1.124e+03\n",
      " -1.124e+03 -1.126e+03 -1.130e+03 -1.130e+03 -1.132e+03 -1.133e+03\n",
      "  1.133e+03 -1.133e+03 -1.137e+03 -1.140e+03 -1.143e+03 -1.165e+03\n",
      "  1.170e+03 -1.175e+03 -1.177e+03 -1.186e+03  1.191e+03  1.200e+03\n",
      "  1.200e+03 -1.217e+03 -1.218e+03  1.230e+03  1.238e+03  1.270e+03\n",
      " -1.291e+03 -1.299e+03 -1.307e+03 -1.308e+03  1.317e+03  1.337e+03\n",
      "  1.343e+03 -1.355e+03 -1.371e+03 -1.373e+03  1.378e+03 -1.384e+03\n",
      " -1.385e+03 -1.385e+03 -1.398e+03  1.404e+03 -1.411e+03 -1.413e+03\n",
      " -1.416e+03  1.417e+03 -1.423e+03  1.432e+03 -1.433e+03 -1.435e+03\n",
      " -1.436e+03 -1.450e+03 -1.459e+03 -1.462e+03  1.463e+03 -1.471e+03\n",
      " -1.476e+03 -1.485e+03 -1.485e+03 -1.489e+03 -1.491e+03 -1.493e+03\n",
      "  1.493e+03 -1.495e+03 -1.496e+03 -1.499e+03 -1.502e+03 -1.508e+03\n",
      " -1.516e+03 -1.520e+03 -1.520e+03 -1.521e+03 -1.525e+03 -1.525e+03\n",
      " -1.531e+03 -1.559e+03 -1.560e+03  1.567e+03  1.584e+03  1.588e+03\n",
      "  1.590e+03  1.598e+03 -1.604e+03  1.610e+03 -1.621e+03 -1.624e+03\n",
      "  1.625e+03  1.625e+03  1.626e+03 -1.632e+03  1.639e+03 -1.657e+03\n",
      "  1.661e+03 -1.666e+03 -1.670e+03  1.696e+03  1.714e+03  1.724e+03\n",
      " -1.729e+03 -1.731e+03 -1.733e+03 -1.746e+03 -1.755e+03 -1.759e+03\n",
      " -1.778e+03 -1.779e+03 -1.785e+03 -1.787e+03 -1.789e+03 -1.790e+03\n",
      " -1.793e+03 -1.794e+03 -1.834e+03 -1.843e+03 -1.854e+03 -1.862e+03\n",
      " -1.871e+03 -1.876e+03 -1.879e+03 -1.883e+03 -1.885e+03 -1.889e+03\n",
      " -1.893e+03 -1.905e+03 -1.906e+03  1.912e+03  1.912e+03  1.913e+03\n",
      " -1.924e+03 -1.928e+03 -1.929e+03 -1.935e+03 -1.946e+03 -1.952e+03\n",
      " -1.955e+03 -1.955e+03  1.964e+03  1.986e+03 -1.997e+03 -2.004e+03\n",
      " -2.014e+03 -2.016e+03 -2.017e+03  2.090e+03  2.105e+03 -2.128e+03\n",
      " -2.133e+03  2.145e+03 -2.150e+03 -2.172e+03 -2.172e+03 -2.184e+03\n",
      " -2.186e+03  2.190e+03 -2.208e+03 -2.217e+03 -2.226e+03 -2.231e+03\n",
      "  2.241e+03 -2.246e+03  2.256e+03 -2.257e+03 -2.259e+03 -2.263e+03\n",
      " -2.270e+03 -2.271e+03 -2.283e+03  2.299e+03 -2.361e+03 -2.372e+03\n",
      " -2.378e+03  2.386e+03 -2.392e+03  2.419e+03 -2.422e+03 -2.423e+03\n",
      " -2.439e+03 -2.461e+03 -2.470e+03 -2.489e+03 -2.504e+03 -2.508e+03\n",
      " -2.552e+03 -2.554e+03  2.564e+03  2.601e+03 -2.609e+03 -2.609e+03\n",
      " -2.630e+03 -2.660e+03 -2.688e+03 -2.722e+03 -2.746e+03  2.752e+03\n",
      " -2.782e+03 -2.799e+03  2.830e+03 -2.839e+03 -2.859e+03 -2.868e+03\n",
      " -2.873e+03 -2.964e+03 -3.037e+03 -3.074e+03 -3.146e+03 -3.205e+03\n",
      " -3.222e+03 -3.267e+03 -3.271e+03 -3.302e+03 -3.328e+03 -3.331e+03\n",
      " -3.341e+03 -3.343e+03 -3.377e+03 -3.392e+03 -3.431e+03 -3.451e+03\n",
      " -3.480e+03 -3.498e+03 -3.519e+03  3.554e+03 -3.583e+03  3.615e+03\n",
      " -3.631e+03 -3.639e+03 -3.728e+03 -3.736e+03 -3.834e+03 -3.936e+03\n",
      " -3.944e+03 -3.974e+03 -3.987e+03 -4.074e+03 -4.537e+03]\n",
      "Concordance Index 0.5868099336659175\n",
      "Integrated Brier Score: 0.38573934371533186\n",
      "y_train breslow final [ 2.000e+00 -3.000e+00 -3.000e+00 -7.000e+00 -1.100e+01 -1.600e+01\n",
      " -1.600e+01 -1.800e+01  1.800e+01 -2.600e+01 -2.900e+01 -3.600e+01\n",
      " -3.800e+01  4.100e+01  4.300e+01  5.900e+01 -6.100e+01  6.200e+01\n",
      "  6.800e+01  7.300e+01  7.700e+01  9.300e+01  9.900e+01  1.010e+02\n",
      "  1.090e+02  1.100e+02 -1.180e+02 -1.190e+02  1.370e+02  1.390e+02\n",
      " -1.550e+02  1.640e+02 -1.660e+02  1.660e+02  1.680e+02 -1.750e+02\n",
      " -1.770e+02 -1.820e+02  1.830e+02 -1.940e+02  2.020e+02  2.040e+02\n",
      " -2.040e+02 -2.050e+02  2.060e+02 -2.060e+02  2.110e+02 -2.110e+02\n",
      "  2.220e+02  2.240e+02  2.450e+02 -2.560e+02 -2.930e+02  3.070e+02\n",
      "  3.110e+02  3.130e+02 -3.190e+02  3.200e+02 -3.230e+02  3.290e+02\n",
      "  3.300e+02  3.330e+02  3.340e+02  3.360e+02 -3.400e+02  3.420e+02\n",
      " -3.550e+02 -3.560e+02  3.620e+02 -3.620e+02 -3.650e+02 -3.690e+02\n",
      " -3.700e+02 -3.720e+02 -3.730e+02 -3.740e+02  3.750e+02 -3.850e+02\n",
      " -3.860e+02 -4.000e+02 -4.080e+02 -4.110e+02  4.310e+02 -4.310e+02\n",
      " -4.330e+02 -4.350e+02  4.460e+02  4.540e+02  4.590e+02 -4.680e+02\n",
      " -4.690e+02  4.750e+02  4.780e+02  4.800e+02  4.800e+02  4.850e+02\n",
      " -4.980e+02 -5.010e+02 -5.050e+02 -5.070e+02  5.100e+02 -5.110e+02\n",
      " -5.190e+02 -5.230e+02 -5.280e+02  5.520e+02  5.610e+02  5.610e+02\n",
      "  5.620e+02  5.630e+02 -5.630e+02 -5.670e+02  5.710e+02  5.720e+02\n",
      " -5.740e+02  5.740e+02  5.870e+02 -5.910e+02  6.000e+02 -6.170e+02\n",
      " -6.200e+02 -6.300e+02  6.370e+02  6.460e+02 -6.480e+02 -6.560e+02\n",
      " -6.620e+02 -6.650e+02 -6.850e+02 -6.930e+02 -6.930e+02 -7.000e+02\n",
      "  7.010e+02  7.090e+02  7.220e+02 -7.220e+02  7.270e+02  7.350e+02\n",
      " -7.350e+02 -7.370e+02 -7.500e+02  7.700e+02 -7.740e+02 -7.770e+02\n",
      "  7.820e+02 -7.880e+02  7.930e+02  8.190e+02  8.220e+02 -8.220e+02\n",
      "  8.280e+02 -8.300e+02  8.340e+02 -8.400e+02  8.410e+02  8.450e+02\n",
      " -8.610e+02 -8.730e+02 -8.740e+02  8.780e+02  8.830e+02  8.850e+02\n",
      "  8.850e+02 -9.190e+02  9.320e+02 -9.320e+02 -9.320e+02  9.460e+02\n",
      " -9.510e+02  9.520e+02 -9.520e+02  9.530e+02 -9.630e+02 -9.670e+02\n",
      " -9.700e+02 -9.720e+02 -9.920e+02 -1.011e+03 -1.014e+03 -1.018e+03\n",
      " -1.018e+03  1.034e+03  1.045e+03 -1.092e+03  1.097e+03 -1.106e+03\n",
      " -1.107e+03  1.111e+03  1.111e+03 -1.120e+03  1.121e+03 -1.124e+03\n",
      " -1.124e+03 -1.126e+03 -1.130e+03 -1.130e+03 -1.132e+03 -1.133e+03\n",
      "  1.133e+03 -1.133e+03 -1.137e+03 -1.140e+03 -1.143e+03 -1.165e+03\n",
      "  1.170e+03 -1.175e+03 -1.177e+03 -1.186e+03  1.191e+03  1.200e+03\n",
      "  1.200e+03 -1.217e+03 -1.218e+03  1.230e+03  1.238e+03  1.270e+03\n",
      " -1.291e+03 -1.299e+03 -1.307e+03 -1.308e+03  1.317e+03  1.337e+03\n",
      "  1.343e+03 -1.355e+03 -1.371e+03 -1.373e+03  1.378e+03 -1.384e+03\n",
      " -1.385e+03 -1.385e+03 -1.398e+03  1.404e+03 -1.411e+03 -1.413e+03\n",
      " -1.416e+03  1.417e+03 -1.423e+03  1.432e+03 -1.433e+03 -1.435e+03\n",
      " -1.436e+03 -1.450e+03 -1.459e+03 -1.462e+03  1.463e+03 -1.471e+03\n",
      " -1.476e+03 -1.485e+03 -1.485e+03 -1.489e+03 -1.491e+03 -1.493e+03\n",
      "  1.493e+03 -1.495e+03 -1.496e+03 -1.499e+03 -1.502e+03 -1.508e+03\n",
      " -1.516e+03 -1.520e+03 -1.520e+03 -1.521e+03 -1.525e+03 -1.525e+03\n",
      " -1.531e+03 -1.559e+03 -1.560e+03  1.567e+03  1.584e+03  1.588e+03\n",
      "  1.590e+03  1.598e+03 -1.604e+03  1.610e+03 -1.621e+03 -1.624e+03\n",
      "  1.625e+03  1.625e+03  1.626e+03 -1.632e+03  1.639e+03 -1.657e+03\n",
      "  1.661e+03 -1.666e+03 -1.670e+03  1.696e+03  1.714e+03  1.724e+03\n",
      " -1.729e+03 -1.731e+03 -1.733e+03 -1.746e+03 -1.755e+03 -1.759e+03\n",
      " -1.778e+03 -1.779e+03 -1.785e+03 -1.787e+03 -1.789e+03 -1.790e+03\n",
      " -1.793e+03 -1.794e+03 -1.834e+03 -1.843e+03 -1.854e+03 -1.862e+03\n",
      " -1.871e+03 -1.876e+03 -1.879e+03 -1.883e+03 -1.885e+03 -1.889e+03\n",
      " -1.893e+03 -1.905e+03 -1.906e+03  1.912e+03  1.912e+03  1.913e+03\n",
      " -1.924e+03 -1.928e+03 -1.929e+03 -1.935e+03 -1.946e+03 -1.952e+03\n",
      " -1.955e+03 -1.955e+03  1.964e+03  1.986e+03 -1.997e+03 -2.004e+03\n",
      " -2.014e+03 -2.016e+03 -2.017e+03  2.090e+03  2.105e+03 -2.128e+03\n",
      " -2.133e+03  2.145e+03 -2.150e+03 -2.172e+03 -2.172e+03 -2.184e+03\n",
      " -2.186e+03  2.190e+03 -2.208e+03 -2.217e+03 -2.226e+03 -2.231e+03\n",
      "  2.241e+03 -2.246e+03  2.256e+03 -2.257e+03 -2.259e+03 -2.263e+03\n",
      " -2.270e+03 -2.271e+03 -2.283e+03  2.299e+03 -2.361e+03 -2.372e+03\n",
      " -2.378e+03  2.386e+03 -2.392e+03  2.419e+03 -2.422e+03 -2.423e+03\n",
      " -2.439e+03 -2.461e+03 -2.470e+03 -2.489e+03 -2.504e+03 -2.508e+03\n",
      " -2.552e+03 -2.554e+03  2.564e+03  2.601e+03 -2.609e+03 -2.609e+03\n",
      " -2.630e+03 -2.660e+03 -2.688e+03 -2.722e+03 -2.746e+03  2.752e+03\n",
      " -2.782e+03 -2.799e+03  2.830e+03 -2.839e+03 -2.859e+03 -2.868e+03\n",
      " -2.873e+03 -2.964e+03 -3.037e+03 -3.074e+03 -3.146e+03 -3.205e+03\n",
      " -3.222e+03 -3.267e+03 -3.271e+03 -3.302e+03 -3.328e+03 -3.331e+03\n",
      " -3.341e+03 -3.343e+03 -3.377e+03 -3.392e+03 -3.431e+03 -3.451e+03\n",
      " -3.480e+03 -3.498e+03 -3.519e+03  3.554e+03 -3.583e+03  3.615e+03\n",
      " -3.631e+03 -3.639e+03 -3.728e+03 -3.736e+03 -3.834e+03 -3.936e+03\n",
      " -3.944e+03 -3.974e+03 -3.987e+03 -4.074e+03 -4.537e+03]\n",
      "durations 13.0 4067.0\n",
      "Concordance Index 0.6036235086168803\n",
      "Integrated Brier Score: 0.3607977922614494\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m94.9736\u001b[0m  0.0037\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2       94.9736  0.0031\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      3       94.9736  0.0030\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m106.0289\u001b[0m  0.0038\n",
      "      4       94.9736  0.0030\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m120.2609\u001b[0m  0.0036\n",
      "      2      106.0289  0.0042\n",
      "      5       94.9736  0.0030\n",
      "      2      120.2609  0.0031\n",
      "      6       94.9736  0.0032\n",
      "      3      120.2609  0.0031\n",
      "      7       94.9736  0.0031\n",
      "      4      120.2609  0.0033\n",
      "      8       94.9736  0.0030\n",
      "      5      120.2609  0.0033\n",
      "      9       94.9736  0.0030\n",
      "      3      106.0289  0.0080\n",
      "      6      120.2609  0.0030\n",
      "     10       94.9736  0.0030\n",
      "Restoring best model from epoch 1.\n",
      "      4      106.0289  0.0033\n",
      "      7      120.2609  0.0030\n",
      "      5      106.0289  0.0032\n",
      "      8      120.2609  0.0031\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      9      120.2609  0.0031\n",
      "      6      106.0289  0.0036\n",
      "      7      106.0289  0.0032\n",
      "     10      120.2609  0.0036\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Restoring best model from epoch 1.\n",
      "      8      106.0289  0.0030\n",
      "      9      106.0289  0.0043\n",
      "     10      106.0289  0.0031\n",
      "Restoring best model from epoch 1.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m80.2278\u001b[0m      \u001b[32m108.4730\u001b[0m  0.1081\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m70.4430\u001b[0m      \u001b[32m139.7341\u001b[0m  0.0958\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m71.4134\u001b[0m  0.0082\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m70.1981\u001b[0m       \u001b[32m56.6886\u001b[0m  0.0868\n",
      "      2       71.4134  0.0040\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m109.5450\u001b[0m  0.0051\n",
      "      3       71.4134  0.0031\n",
      "      2      109.5450  0.0033\n",
      "      3      109.5450  0.0039\n",
      "      4       71.4134  0.0060\n",
      "      4      109.5450  0.0061\n",
      "      5       71.4134  0.0092\n",
      "      5      109.5450  0.0031\n",
      "      6      109.5450  0.0038\n",
      "      6       71.4134  0.0052\n",
      "      7      109.5450  0.0032\n",
      "      8      109.5450  0.0031\n",
      "      7       71.4134  0.0053\n",
      "      9      109.5450  0.0030\n",
      "      8       71.4134  0.0083\n",
      "      9       71.4134  0.0035\n",
      "     10       71.4134  0.0038\n",
      "     10      109.5450  0.0126\n",
      "Restoring best model from epoch 1.\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m64.2133\u001b[0m       \u001b[32m77.2340\u001b[0m  0.1170\n",
      "      2       83.9025      139.1991  0.1193\n",
      "      2       76.3015       \u001b[32m99.3761\u001b[0m  0.1283\n",
      "      2       \u001b[36m67.7895\u001b[0m       \u001b[32m54.3912\u001b[0m  0.1243\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m77.7453\u001b[0m      \u001b[32m178.4938\u001b[0m  0.0971\n",
      "      2       65.5793       \u001b[32m70.1083\u001b[0m  0.0852\n",
      "      3       81.2905      189.5563  0.0867\n",
      "      3       74.6094       \u001b[32m89.0574\u001b[0m  0.0927\n",
      "      3       \u001b[36m64.7104\u001b[0m       57.5390  0.0851\n",
      "      2       \u001b[36m77.0361\u001b[0m      289.4438  0.0786\n",
      "      3       \u001b[36m60.0451\u001b[0m       \u001b[32m66.3296\u001b[0m  0.0771\n",
      "      4       \u001b[36m78.9678\u001b[0m      112.1383  0.0786\n",
      "      4       \u001b[36m70.4352\u001b[0m       \u001b[32m87.7110\u001b[0m  0.0754\n",
      "      4       67.9533       55.9127  0.0750\n",
      "      4       67.3524       66.7449  0.0777\n",
      "      3       86.6854      \u001b[32m113.9324\u001b[0m  0.0819\n",
      "      5       80.9244      \u001b[32m105.1021\u001b[0m  0.0774\n",
      "      5       \u001b[36m67.9413\u001b[0m       88.6261  0.0779\n",
      "      5       \u001b[36m61.8791\u001b[0m       \u001b[32m48.3070\u001b[0m  0.0731\n",
      "      5       66.5442       67.0576  0.0767\n",
      "      4       \u001b[36m76.6880\u001b[0m       \u001b[32m94.9980\u001b[0m  0.0818\n",
      "      6       82.1687       \u001b[32m90.2178\u001b[0m  0.0767\n",
      "      6       74.8558       \u001b[32m85.0514\u001b[0m  0.0780\n",
      "      6       65.8271       50.6011  0.0822\n",
      "      6       62.1687       \u001b[32m57.6172\u001b[0m  0.0747\n",
      "      5       \u001b[36m73.8337\u001b[0m      106.7490  0.0760\n",
      "      7       80.0588       97.4476  0.0743\n",
      "      7       72.1499       \u001b[32m82.1589\u001b[0m  0.0742\n",
      "      7       \u001b[36m59.1101\u001b[0m       51.8488  0.0752\n",
      "      7       62.3303       59.4169  0.0759\n",
      "      8       \u001b[36m75.5786\u001b[0m       95.6068  0.0752\n",
      "      6       \u001b[36m72.6146\u001b[0m      101.3821  0.0764\n",
      "      8       68.4374       87.9955  0.0729\n",
      "      8       61.2830       50.1910  0.0770\n",
      "      8       61.7544       59.5818  0.0747\n",
      "      9       \u001b[36m70.8886\u001b[0m       90.2357  0.0740\n",
      "      7       76.8836       \u001b[32m91.4547\u001b[0m  0.0756\n",
      "      9       \u001b[36m65.5428\u001b[0m       \u001b[32m81.3392\u001b[0m  0.0724\n",
      "      9       \u001b[36m58.5933\u001b[0m       49.5785  0.0751\n",
      "      9       \u001b[36m59.2759\u001b[0m       61.2280  0.0751\n",
      "     10       76.3020       \u001b[32m89.1592\u001b[0m  0.0745\n",
      "      8       81.2469       \u001b[32m90.9843\u001b[0m  0.0783\n",
      "     10       68.8710       \u001b[32m76.3683\u001b[0m  0.0731\n",
      "     10       58.6288       49.4648  0.0745\n",
      "Restoring best model from epoch 5.\n",
      "     10       \u001b[36m57.1586\u001b[0m       63.2028  0.0742\n",
      "Restoring best model from epoch 6.\n",
      "      9       72.8255       \u001b[32m90.1336\u001b[0m  0.0775\n",
      "     10       \u001b[36m69.5877\u001b[0m       \u001b[32m89.9615\u001b[0m  0.0740\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m75.3788\u001b[0m       \u001b[32m60.4321\u001b[0m  0.0738\n",
      "      2       \u001b[36m71.1927\u001b[0m       \u001b[32m53.4200\u001b[0m  0.0654\n",
      "      3       71.5996       65.2122  0.0705\n",
      "      4       71.2991       59.5630  0.0609\n",
      "      5       \u001b[36m66.4741\u001b[0m       59.2798  0.0644\n",
      "      6       68.6928       55.0232  0.0726\n",
      "      7       72.2082       56.4379  0.0748\n",
      "      8       72.3104       55.2937  0.0704\n",
      "      9       66.7817       55.5139  0.0641\n",
      "     10       69.4086       56.7835  0.0671\n",
      "Restoring best model from epoch 2.\n",
      "y_train breslow final [-3.000e+00 -3.000e+00 -7.000e+00 -1.100e+01 -1.300e+01 -1.600e+01\n",
      " -1.600e+01 -1.600e+01 -1.800e+01 -2.900e+01 -3.600e+01 -3.800e+01\n",
      "  4.100e+01  4.200e+01 -4.300e+01  5.100e+01 -5.300e+01  5.900e+01\n",
      " -6.100e+01  6.200e+01  6.500e+01  6.800e+01  6.900e+01  7.300e+01\n",
      "  7.700e+01  9.300e+01  9.900e+01  1.010e+02  1.060e+02  1.090e+02\n",
      "  1.100e+02 -1.180e+02 -1.190e+02 -1.270e+02  1.390e+02 -1.410e+02\n",
      " -1.500e+02 -1.550e+02  1.620e+02  1.640e+02 -1.660e+02  1.660e+02\n",
      "  1.680e+02 -1.750e+02 -1.770e+02 -1.820e+02  1.820e+02  1.830e+02\n",
      " -1.930e+02 -1.940e+02  2.020e+02  2.040e+02 -2.040e+02 -2.050e+02\n",
      "  2.060e+02 -2.060e+02  2.110e+02 -2.110e+02  2.220e+02  2.240e+02\n",
      "  2.380e+02  2.420e+02 -2.560e+02 -2.930e+02  3.110e+02  3.130e+02\n",
      "  3.130e+02 -3.190e+02  3.200e+02 -3.230e+02  3.300e+02  3.330e+02\n",
      " -3.400e+02  3.420e+02  3.440e+02 -3.540e+02 -3.550e+02 -3.560e+02\n",
      "  3.620e+02 -3.650e+02 -3.650e+02 -3.690e+02 -3.720e+02 -3.730e+02\n",
      " -3.850e+02 -4.000e+02 -4.060e+02 -4.080e+02  4.310e+02 -4.310e+02\n",
      " -4.330e+02 -4.350e+02  4.450e+02  4.460e+02  4.540e+02 -4.540e+02\n",
      "  4.590e+02 -4.680e+02 -4.690e+02  4.750e+02  4.800e+02  4.800e+02\n",
      " -4.950e+02 -4.980e+02  5.100e+02 -5.110e+02 -5.230e+02 -5.280e+02\n",
      " -5.510e+02  5.520e+02  5.610e+02  5.610e+02  5.620e+02  5.630e+02\n",
      " -5.630e+02  5.710e+02  5.720e+02  5.740e+02  5.780e+02 -5.910e+02\n",
      "  6.000e+02 -6.030e+02 -6.070e+02 -6.170e+02 -6.200e+02 -6.300e+02\n",
      "  6.450e+02  6.460e+02 -6.480e+02 -6.560e+02 -6.650e+02  6.790e+02\n",
      "  6.830e+02 -6.850e+02 -6.890e+02 -6.930e+02 -6.930e+02 -7.000e+02\n",
      "  7.010e+02  7.090e+02 -7.140e+02  7.220e+02 -7.220e+02  7.270e+02\n",
      "  7.350e+02 -7.350e+02 -7.370e+02 -7.490e+02 -7.500e+02 -7.550e+02\n",
      " -7.620e+02  7.680e+02  7.700e+02 -7.740e+02 -7.770e+02  7.820e+02\n",
      " -7.850e+02 -7.880e+02  8.190e+02  8.220e+02 -8.220e+02  8.280e+02\n",
      " -8.300e+02  8.340e+02 -8.400e+02  8.410e+02  8.450e+02 -8.610e+02\n",
      "  8.660e+02 -8.730e+02 -8.740e+02  8.850e+02  8.850e+02 -9.090e+02\n",
      " -9.100e+02 -9.190e+02  9.270e+02 -9.320e+02 -9.320e+02 -9.450e+02\n",
      "  9.460e+02 -9.510e+02  9.520e+02 -9.520e+02  9.530e+02 -9.630e+02\n",
      " -9.700e+02 -9.720e+02 -9.920e+02  9.920e+02  1.003e+03 -1.011e+03\n",
      " -1.014e+03 -1.018e+03  1.019e+03  1.034e+03 -1.046e+03 -1.063e+03\n",
      " -1.071e+03  1.075e+03  1.091e+03 -1.092e+03  1.092e+03  1.097e+03\n",
      " -1.106e+03 -1.107e+03  1.111e+03 -1.120e+03  1.121e+03 -1.124e+03\n",
      " -1.130e+03 -1.130e+03 -1.132e+03 -1.133e+03 -1.143e+03 -1.165e+03\n",
      " -1.168e+03 -1.169e+03  1.170e+03 -1.175e+03 -1.177e+03  1.191e+03\n",
      "  1.200e+03  1.200e+03 -1.217e+03 -1.218e+03  1.230e+03 -1.238e+03\n",
      "  1.238e+03 -1.266e+03  1.270e+03 -1.274e+03 -1.290e+03 -1.291e+03\n",
      " -1.299e+03 -1.307e+03 -1.308e+03 -1.314e+03  1.317e+03 -1.329e+03\n",
      "  1.337e+03 -1.355e+03 -1.367e+03  1.371e+03 -1.373e+03  1.378e+03\n",
      " -1.380e+03 -1.384e+03 -1.385e+03  1.404e+03 -1.411e+03 -1.413e+03\n",
      " -1.413e+03 -1.423e+03 -1.433e+03 -1.436e+03 -1.450e+03 -1.459e+03\n",
      " -1.462e+03  1.463e+03 -1.471e+03 -1.476e+03 -1.478e+03 -1.485e+03\n",
      " -1.487e+03 -1.489e+03 -1.491e+03  1.493e+03 -1.495e+03 -1.498e+03\n",
      " -1.499e+03 -1.508e+03 -1.520e+03 -1.520e+03 -1.521e+03 -1.525e+03\n",
      " -1.525e+03 -1.528e+03 -1.552e+03 -1.559e+03 -1.560e+03  1.567e+03\n",
      "  1.584e+03  1.589e+03  1.590e+03  1.598e+03 -1.604e+03 -1.608e+03\n",
      "  1.620e+03 -1.624e+03  1.626e+03 -1.632e+03  1.639e+03  1.657e+03\n",
      "  1.661e+03 -1.666e+03 -1.670e+03 -1.683e+03  1.714e+03  1.724e+03\n",
      " -1.731e+03 -1.733e+03 -1.746e+03 -1.755e+03 -1.759e+03 -1.779e+03\n",
      " -1.782e+03 -1.787e+03 -1.789e+03 -1.790e+03 -1.793e+03 -1.794e+03\n",
      " -1.834e+03 -1.843e+03 -1.854e+03 -1.862e+03 -1.871e+03 -1.876e+03\n",
      " -1.883e+03 -1.885e+03 -1.886e+03 -1.888e+03 -1.889e+03 -1.893e+03\n",
      " -1.905e+03 -1.906e+03  1.912e+03  1.912e+03  1.913e+03 -1.929e+03\n",
      " -1.935e+03 -1.946e+03 -1.952e+03 -1.955e+03  1.980e+03  1.986e+03\n",
      " -1.993e+03 -1.997e+03 -2.004e+03 -2.009e+03 -2.014e+03 -2.016e+03\n",
      " -2.017e+03 -2.038e+03 -2.067e+03 -2.080e+03 -2.087e+03  2.105e+03\n",
      " -2.128e+03  2.145e+03 -2.150e+03 -2.172e+03 -2.184e+03 -2.217e+03\n",
      " -2.226e+03  2.227e+03  2.256e+03 -2.259e+03 -2.270e+03 -2.271e+03\n",
      " -2.274e+03  2.299e+03  2.343e+03 -2.361e+03 -2.372e+03  2.386e+03\n",
      " -2.412e+03  2.419e+03 -2.422e+03 -2.423e+03 -2.430e+03 -2.439e+03\n",
      "  2.454e+03 -2.461e+03 -2.470e+03 -2.489e+03 -2.508e+03 -2.531e+03\n",
      " -2.552e+03  2.601e+03 -2.609e+03 -2.630e+03 -2.660e+03 -2.688e+03\n",
      " -2.718e+03 -2.746e+03  2.752e+03 -2.754e+03  2.764e+03 -2.789e+03\n",
      " -2.799e+03  2.830e+03 -2.839e+03 -2.859e+03 -2.865e+03 -2.881e+03\n",
      " -3.037e+03 -3.146e+03 -3.205e+03 -3.222e+03 -3.229e+03 -3.267e+03\n",
      " -3.271e+03 -3.328e+03 -3.331e+03 -3.343e+03 -3.409e+03 -3.431e+03\n",
      " -3.480e+03 -3.498e+03  3.554e+03 -3.583e+03  3.615e+03 -3.631e+03\n",
      " -3.728e+03 -3.736e+03 -3.744e+03 -3.841e+03 -3.936e+03 -3.974e+03\n",
      " -3.987e+03 -3.989e+03 -4.067e+03 -4.074e+03 -4.537e+03]\n",
      "Concordance Index 0.6910205109550326\n",
      "Integrated Brier Score: 0.2156189454760904\n",
      "y_train breslow final [-3.000e+00 -3.000e+00 -7.000e+00 -1.100e+01 -1.300e+01 -1.600e+01\n",
      " -1.600e+01 -1.600e+01 -1.800e+01 -2.900e+01 -3.600e+01 -3.800e+01\n",
      "  4.100e+01  4.200e+01 -4.300e+01  5.100e+01 -5.300e+01  5.900e+01\n",
      " -6.100e+01  6.200e+01  6.500e+01  6.800e+01  6.900e+01  7.300e+01\n",
      "  7.700e+01  9.300e+01  9.900e+01  1.010e+02  1.060e+02  1.090e+02\n",
      "  1.100e+02 -1.180e+02 -1.190e+02 -1.270e+02  1.390e+02 -1.410e+02\n",
      " -1.500e+02 -1.550e+02  1.620e+02  1.640e+02 -1.660e+02  1.660e+02\n",
      "  1.680e+02 -1.750e+02 -1.770e+02 -1.820e+02  1.820e+02  1.830e+02\n",
      " -1.930e+02 -1.940e+02  2.020e+02  2.040e+02 -2.040e+02 -2.050e+02\n",
      "  2.060e+02 -2.060e+02  2.110e+02 -2.110e+02  2.220e+02  2.240e+02\n",
      "  2.380e+02  2.420e+02 -2.560e+02 -2.930e+02  3.110e+02  3.130e+02\n",
      "  3.130e+02 -3.190e+02  3.200e+02 -3.230e+02  3.300e+02  3.330e+02\n",
      " -3.400e+02  3.420e+02  3.440e+02 -3.540e+02 -3.550e+02 -3.560e+02\n",
      "  3.620e+02 -3.650e+02 -3.650e+02 -3.690e+02 -3.720e+02 -3.730e+02\n",
      " -3.850e+02 -4.000e+02 -4.060e+02 -4.080e+02  4.310e+02 -4.310e+02\n",
      " -4.330e+02 -4.350e+02  4.450e+02  4.460e+02  4.540e+02 -4.540e+02\n",
      "  4.590e+02 -4.680e+02 -4.690e+02  4.750e+02  4.800e+02  4.800e+02\n",
      " -4.950e+02 -4.980e+02  5.100e+02 -5.110e+02 -5.230e+02 -5.280e+02\n",
      " -5.510e+02  5.520e+02  5.610e+02  5.610e+02  5.620e+02  5.630e+02\n",
      " -5.630e+02  5.710e+02  5.720e+02  5.740e+02  5.780e+02 -5.910e+02\n",
      "  6.000e+02 -6.030e+02 -6.070e+02 -6.170e+02 -6.200e+02 -6.300e+02\n",
      "  6.450e+02  6.460e+02 -6.480e+02 -6.560e+02 -6.650e+02  6.790e+02\n",
      "  6.830e+02 -6.850e+02 -6.890e+02 -6.930e+02 -6.930e+02 -7.000e+02\n",
      "  7.010e+02  7.090e+02 -7.140e+02  7.220e+02 -7.220e+02  7.270e+02\n",
      "  7.350e+02 -7.350e+02 -7.370e+02 -7.490e+02 -7.500e+02 -7.550e+02\n",
      " -7.620e+02  7.680e+02  7.700e+02 -7.740e+02 -7.770e+02  7.820e+02\n",
      " -7.850e+02 -7.880e+02  8.190e+02  8.220e+02 -8.220e+02  8.280e+02\n",
      " -8.300e+02  8.340e+02 -8.400e+02  8.410e+02  8.450e+02 -8.610e+02\n",
      "  8.660e+02 -8.730e+02 -8.740e+02  8.850e+02  8.850e+02 -9.090e+02\n",
      " -9.100e+02 -9.190e+02  9.270e+02 -9.320e+02 -9.320e+02 -9.450e+02\n",
      "  9.460e+02 -9.510e+02  9.520e+02 -9.520e+02  9.530e+02 -9.630e+02\n",
      " -9.700e+02 -9.720e+02 -9.920e+02  9.920e+02  1.003e+03 -1.011e+03\n",
      " -1.014e+03 -1.018e+03  1.019e+03  1.034e+03 -1.046e+03 -1.063e+03\n",
      " -1.071e+03  1.075e+03  1.091e+03 -1.092e+03  1.092e+03  1.097e+03\n",
      " -1.106e+03 -1.107e+03  1.111e+03 -1.120e+03  1.121e+03 -1.124e+03\n",
      " -1.130e+03 -1.130e+03 -1.132e+03 -1.133e+03 -1.143e+03 -1.165e+03\n",
      " -1.168e+03 -1.169e+03  1.170e+03 -1.175e+03 -1.177e+03  1.191e+03\n",
      "  1.200e+03  1.200e+03 -1.217e+03 -1.218e+03  1.230e+03 -1.238e+03\n",
      "  1.238e+03 -1.266e+03  1.270e+03 -1.274e+03 -1.290e+03 -1.291e+03\n",
      " -1.299e+03 -1.307e+03 -1.308e+03 -1.314e+03  1.317e+03 -1.329e+03\n",
      "  1.337e+03 -1.355e+03 -1.367e+03  1.371e+03 -1.373e+03  1.378e+03\n",
      " -1.380e+03 -1.384e+03 -1.385e+03  1.404e+03 -1.411e+03 -1.413e+03\n",
      " -1.413e+03 -1.423e+03 -1.433e+03 -1.436e+03 -1.450e+03 -1.459e+03\n",
      " -1.462e+03  1.463e+03 -1.471e+03 -1.476e+03 -1.478e+03 -1.485e+03\n",
      " -1.487e+03 -1.489e+03 -1.491e+03  1.493e+03 -1.495e+03 -1.498e+03\n",
      " -1.499e+03 -1.508e+03 -1.520e+03 -1.520e+03 -1.521e+03 -1.525e+03\n",
      " -1.525e+03 -1.528e+03 -1.552e+03 -1.559e+03 -1.560e+03  1.567e+03\n",
      "  1.584e+03  1.589e+03  1.590e+03  1.598e+03 -1.604e+03 -1.608e+03\n",
      "  1.620e+03 -1.624e+03  1.626e+03 -1.632e+03  1.639e+03  1.657e+03\n",
      "  1.661e+03 -1.666e+03 -1.670e+03 -1.683e+03  1.714e+03  1.724e+03\n",
      " -1.731e+03 -1.733e+03 -1.746e+03 -1.755e+03 -1.759e+03 -1.779e+03\n",
      " -1.782e+03 -1.787e+03 -1.789e+03 -1.790e+03 -1.793e+03 -1.794e+03\n",
      " -1.834e+03 -1.843e+03 -1.854e+03 -1.862e+03 -1.871e+03 -1.876e+03\n",
      " -1.883e+03 -1.885e+03 -1.886e+03 -1.888e+03 -1.889e+03 -1.893e+03\n",
      " -1.905e+03 -1.906e+03  1.912e+03  1.912e+03  1.913e+03 -1.929e+03\n",
      " -1.935e+03 -1.946e+03 -1.952e+03 -1.955e+03  1.980e+03  1.986e+03\n",
      " -1.993e+03 -1.997e+03 -2.004e+03 -2.009e+03 -2.014e+03 -2.016e+03\n",
      " -2.017e+03 -2.038e+03 -2.067e+03 -2.080e+03 -2.087e+03  2.105e+03\n",
      " -2.128e+03  2.145e+03 -2.150e+03 -2.172e+03 -2.184e+03 -2.217e+03\n",
      " -2.226e+03  2.227e+03  2.256e+03 -2.259e+03 -2.270e+03 -2.271e+03\n",
      " -2.274e+03  2.299e+03  2.343e+03 -2.361e+03 -2.372e+03  2.386e+03\n",
      " -2.412e+03  2.419e+03 -2.422e+03 -2.423e+03 -2.430e+03 -2.439e+03\n",
      "  2.454e+03 -2.461e+03 -2.470e+03 -2.489e+03 -2.508e+03 -2.531e+03\n",
      " -2.552e+03  2.601e+03 -2.609e+03 -2.630e+03 -2.660e+03 -2.688e+03\n",
      " -2.718e+03 -2.746e+03  2.752e+03 -2.754e+03  2.764e+03 -2.789e+03\n",
      " -2.799e+03  2.830e+03 -2.839e+03 -2.859e+03 -2.865e+03 -2.881e+03\n",
      " -3.037e+03 -3.146e+03 -3.205e+03 -3.222e+03 -3.229e+03 -3.267e+03\n",
      " -3.271e+03 -3.328e+03 -3.331e+03 -3.343e+03 -3.409e+03 -3.431e+03\n",
      " -3.480e+03 -3.498e+03  3.554e+03 -3.583e+03  3.615e+03 -3.631e+03\n",
      " -3.728e+03 -3.736e+03 -3.744e+03 -3.841e+03 -3.936e+03 -3.974e+03\n",
      " -3.987e+03 -3.989e+03 -4.067e+03 -4.074e+03 -4.537e+03]\n",
      "durations 2.0 3944.0\n",
      "Concordance Index 0.6515580736543909\n",
      "Integrated Brier Score: 0.18513830856968377\n",
      "split gex_?|100130426      float32\n",
      "gex_?|100133144      float32\n",
      "gex_?|100134869      float32\n",
      "gex_?|10357          float32\n",
      "gex_?|10431          float32\n",
      "                      ...   \n",
      "gex_ZYG11A|440590    float32\n",
      "gex_ZYG11B|79699     float32\n",
      "gex_ZYX|7791         float32\n",
      "gex_ZZEF1|23140      float32\n",
      "gex_ZZZ3|26009       float32\n",
      "Length: 20531, dtype: object\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m79.4129\u001b[0m  0.0046\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      2       79.4129  0.0031\n",
      "      3       79.4129  0.0029\n",
      "      4       79.4129  0.0028\n",
      "      5       79.4129  0.0032\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      6       79.4129  0.0031\n",
      "      7       79.4129  0.0148\n",
      "      8       79.4129  0.0031\n",
      "      9       79.4129  0.0029\n",
      "     10       79.4129  0.0043\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m41.8809\u001b[0m  0.0042\n",
      "      2       41.8809  0.0029\n",
      "      3       41.8809  0.0027\n",
      "      4       41.8809  0.0028\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      5       41.8809  0.0029\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      6       41.8809  0.0029\n",
      "      7       41.8809  0.0027\n",
      "      8       41.8809  0.0027\n",
      "      9       41.8809  0.0035\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "     10       41.8809  0.0042\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m70.0404\u001b[0m  0.0045\n",
      "      2       70.0404  0.0032\n",
      "      3       70.0404  0.0031\n",
      "      4       70.0404  0.0037\n",
      "      5       70.0404  0.0032\n",
      "      6       70.0404  0.0029\n",
      "      7       70.0404  0.0035\n",
      "      8       70.0404  0.0029\n",
      "      9       70.0404  0.0029\n",
      "     10       70.0404  0.0031\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m41.3467\u001b[0m  0.0044\n",
      "      2       41.3467  0.0029\n",
      "      3       41.3467  0.0028\n",
      "      4       41.3467  0.0030\n",
      "      5       41.3467  0.0028\n",
      "      6       41.3467  0.0027\n",
      "      7       41.3467  0.0027\n",
      "      8       41.3467  0.0027\n",
      "      9       41.3467  0.0029\n",
      "     10       41.3467  0.0029\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m81.6915\u001b[0m  0.0045\n",
      "      2       81.6915  0.0032\n",
      "      3       81.6915  0.0035\n",
      "      4       81.6915  0.0030\n",
      "      5       81.6915  0.0029\n",
      "      6       81.6915  0.0029\n",
      "      7       81.6915  0.0029\n",
      "      8       81.6915  0.0029\n",
      "      9       81.6915  0.0028\n",
      "     10       81.6915  0.0029\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m35.8106\u001b[0m  0.0058\n",
      "      2       35.8106  0.0044\n",
      "      3       35.8106  0.0036\n",
      "      4       35.8106  0.0039\n",
      "      5       35.8106  0.0038\n",
      "      6       35.8106  0.0034\n",
      "      7       35.8106  0.0033\n",
      "      8       35.8106  0.0048\n",
      "      9       35.8106  0.0031\n",
      "     10       35.8106  0.0050\n",
      "Restoring best model from epoch 1.\n",
      "y_train breslow final [-1.000e+00 -2.000e+00 -3.000e+00 -3.000e+00 -3.000e+00 -3.000e+00\n",
      " -3.000e+00 -3.000e+00 -3.000e+00 -4.000e+00 -4.000e+00 -5.000e+00\n",
      " -6.000e+00 -7.000e+00 -7.000e+00 -7.000e+00 -7.000e+00  7.000e+00\n",
      " -8.000e+00 -1.400e+01 -1.700e+01  2.300e+01 -2.300e+01  3.700e+01\n",
      " -3.900e+01 -5.500e+01 -6.200e+01 -6.300e+01 -7.000e+01 -7.200e+01\n",
      " -7.300e+01 -7.400e+01 -8.200e+01 -8.400e+01 -9.000e+01  9.600e+01\n",
      " -1.050e+02  1.110e+02 -1.120e+02  1.130e+02 -1.390e+02 -1.530e+02\n",
      " -1.620e+02 -1.660e+02 -1.740e+02  1.780e+02 -1.790e+02 -1.820e+02\n",
      " -1.840e+02 -1.900e+02 -1.940e+02 -1.940e+02  1.940e+02  1.990e+02\n",
      " -1.990e+02 -2.030e+02  2.050e+02 -2.100e+02  2.140e+02  2.280e+02\n",
      " -2.300e+02 -2.310e+02  2.340e+02  2.400e+02  2.410e+02 -2.420e+02\n",
      " -2.430e+02  2.450e+02  2.690e+02 -2.740e+02 -2.790e+02 -2.860e+02\n",
      " -2.920e+02 -3.010e+02 -3.130e+02  3.160e+02 -3.170e+02 -3.200e+02\n",
      " -3.260e+02 -3.280e+02 -3.330e+02 -3.360e+02 -3.370e+02 -3.420e+02\n",
      " -3.430e+02  3.470e+02  3.490e+02 -3.540e+02 -3.720e+02  3.720e+02\n",
      " -3.720e+02  3.780e+02  3.980e+02 -4.030e+02 -4.080e+02 -4.110e+02\n",
      " -4.140e+02 -4.140e+02 -4.160e+02 -4.170e+02 -4.180e+02 -4.190e+02\n",
      " -4.280e+02 -4.310e+02 -4.330e+02 -4.340e+02  4.350e+02  4.380e+02\n",
      " -4.380e+02 -4.380e+02 -4.420e+02 -4.430e+02 -4.490e+02 -4.510e+02\n",
      " -4.540e+02 -4.540e+02 -4.550e+02 -4.550e+02  4.560e+02 -4.570e+02\n",
      " -4.580e+02 -4.610e+02 -4.610e+02 -4.670e+02 -4.670e+02 -4.710e+02\n",
      " -4.780e+02 -4.820e+02 -4.860e+02 -4.870e+02 -4.910e+02  4.920e+02\n",
      " -4.920e+02 -4.940e+02 -4.940e+02 -5.000e+02 -5.020e+02 -5.080e+02\n",
      " -5.090e+02  5.120e+02 -5.150e+02 -5.160e+02 -5.220e+02 -5.230e+02\n",
      " -5.230e+02 -5.260e+02 -5.300e+02  5.310e+02 -5.310e+02 -5.320e+02\n",
      " -5.330e+02 -5.330e+02  5.370e+02  5.380e+02 -5.420e+02 -5.440e+02\n",
      " -5.440e+02  5.470e+02  5.590e+02  5.610e+02 -5.630e+02 -5.640e+02\n",
      " -5.660e+02 -5.660e+02 -5.660e+02 -5.670e+02 -5.710e+02 -5.710e+02\n",
      "  5.760e+02 -5.760e+02  5.780e+02 -5.820e+02 -5.840e+02 -5.850e+02\n",
      " -5.880e+02 -5.960e+02 -5.990e+02 -6.040e+02  6.050e+02  6.070e+02\n",
      " -6.080e+02 -6.090e+02 -6.090e+02 -6.110e+02 -6.110e+02 -6.150e+02\n",
      " -6.220e+02 -6.230e+02 -6.270e+02 -6.290e+02 -6.290e+02 -6.300e+02\n",
      " -6.330e+02  6.390e+02  6.480e+02  6.480e+02 -6.510e+02 -6.510e+02\n",
      " -6.550e+02 -6.560e+02 -6.710e+02 -6.770e+02 -6.780e+02  6.820e+02\n",
      " -6.850e+02 -6.860e+02 -7.050e+02 -7.060e+02 -7.060e+02 -7.140e+02\n",
      " -7.150e+02 -7.180e+02 -7.210e+02  7.220e+02 -7.240e+02  7.270e+02\n",
      " -7.350e+02 -7.360e+02 -7.380e+02  7.420e+02 -7.480e+02 -7.540e+02\n",
      "  7.580e+02 -7.580e+02 -7.600e+02  7.750e+02  7.750e+02 -7.770e+02\n",
      " -7.850e+02 -7.870e+02  7.880e+02 -7.920e+02 -7.950e+02 -7.960e+02\n",
      " -8.000e+02  8.140e+02  8.140e+02  8.190e+02  8.210e+02 -8.260e+02\n",
      " -8.350e+02 -8.370e+02 -8.460e+02 -8.460e+02 -8.540e+02 -8.540e+02\n",
      " -8.600e+02 -8.620e+02 -8.680e+02  8.860e+02 -8.890e+02 -9.000e+02\n",
      " -9.080e+02 -9.080e+02 -9.140e+02  9.190e+02 -9.260e+02  9.330e+02\n",
      " -9.330e+02 -9.350e+02 -9.390e+02 -9.460e+02 -9.490e+02  9.540e+02\n",
      " -9.560e+02  9.610e+02 -9.620e+02  9.620e+02 -9.640e+02 -9.640e+02\n",
      "  9.840e+02  9.870e+02 -9.920e+02 -9.920e+02 -9.930e+02 -1.004e+03\n",
      "  1.011e+03 -1.021e+03 -1.026e+03 -1.032e+03  1.033e+03 -1.040e+03\n",
      " -1.058e+03 -1.069e+03 -1.076e+03 -1.078e+03 -1.079e+03  1.106e+03\n",
      " -1.116e+03 -1.120e+03  1.120e+03  1.120e+03 -1.127e+03  1.137e+03\n",
      " -1.137e+03 -1.139e+03 -1.147e+03  1.152e+03 -1.164e+03 -1.173e+03\n",
      "  1.183e+03 -1.189e+03 -1.191e+03 -1.201e+03  1.209e+03 -1.210e+03\n",
      " -1.222e+03 -1.229e+03 -1.236e+03 -1.245e+03 -1.250e+03 -1.257e+03\n",
      " -1.277e+03 -1.279e+03 -1.294e+03 -1.294e+03 -1.300e+03 -1.301e+03\n",
      " -1.314e+03 -1.320e+03  1.335e+03 -1.337e+03  1.339e+03 -1.341e+03\n",
      "  1.351e+03 -1.354e+03 -1.359e+03 -1.387e+03 -1.393e+03 -1.397e+03\n",
      " -1.399e+03  1.401e+03 -1.401e+03 -1.426e+03 -1.428e+03 -1.458e+03\n",
      " -1.469e+03  1.481e+03 -1.494e+03 -1.500e+03 -1.540e+03  1.547e+03\n",
      " -1.553e+03 -1.567e+03 -1.568e+03 -1.588e+03 -1.631e+03 -1.650e+03\n",
      "  1.666e+03 -1.706e+03 -1.721e+03 -1.752e+03  1.762e+03 -1.796e+03\n",
      " -1.834e+03 -1.868e+03  1.886e+03  1.891e+03  1.915e+03  1.933e+03\n",
      " -1.943e+03 -1.989e+03  2.000e+03 -2.078e+03 -2.219e+03  2.235e+03\n",
      "  2.282e+03 -2.287e+03 -2.289e+03  2.379e+03 -2.381e+03  2.433e+03\n",
      " -2.493e+03 -2.565e+03 -2.585e+03 -2.602e+03 -2.650e+03 -2.702e+03\n",
      " -2.761e+03 -2.772e+03  2.835e+03 -2.860e+03 -2.869e+03  2.875e+03\n",
      " -2.893e+03  2.907e+03 -2.918e+03 -2.918e+03  2.988e+03 -3.000e+03\n",
      " -3.013e+03  3.200e+03 -3.253e+03  3.571e+03 -3.574e+03 -3.725e+03\n",
      " -3.761e+03  3.978e+03  4.068e+03  4.084e+03  4.229e+03  4.412e+03\n",
      "  4.445e+03 -4.752e+03  5.166e+03 -5.255e+03 -5.546e+03 -6.423e+03]\n",
      "Concordance Index 0.6317709484855185\n",
      "Integrated Brier Score: 0.2920227544007943\n",
      "y_train breslow final [-1.000e+00 -2.000e+00 -3.000e+00 -3.000e+00 -3.000e+00 -3.000e+00\n",
      " -3.000e+00 -3.000e+00 -3.000e+00 -4.000e+00 -4.000e+00 -5.000e+00\n",
      " -6.000e+00 -7.000e+00 -7.000e+00 -7.000e+00 -7.000e+00  7.000e+00\n",
      " -8.000e+00 -1.400e+01 -1.700e+01  2.300e+01 -2.300e+01  3.700e+01\n",
      " -3.900e+01 -5.500e+01 -6.200e+01 -6.300e+01 -7.000e+01 -7.200e+01\n",
      " -7.300e+01 -7.400e+01 -8.200e+01 -8.400e+01 -9.000e+01  9.600e+01\n",
      " -1.050e+02  1.110e+02 -1.120e+02  1.130e+02 -1.390e+02 -1.530e+02\n",
      " -1.620e+02 -1.660e+02 -1.740e+02  1.780e+02 -1.790e+02 -1.820e+02\n",
      " -1.840e+02 -1.900e+02 -1.940e+02 -1.940e+02  1.940e+02  1.990e+02\n",
      " -1.990e+02 -2.030e+02  2.050e+02 -2.100e+02  2.140e+02  2.280e+02\n",
      " -2.300e+02 -2.310e+02  2.340e+02  2.400e+02  2.410e+02 -2.420e+02\n",
      " -2.430e+02  2.450e+02  2.690e+02 -2.740e+02 -2.790e+02 -2.860e+02\n",
      " -2.920e+02 -3.010e+02 -3.130e+02  3.160e+02 -3.170e+02 -3.200e+02\n",
      " -3.260e+02 -3.280e+02 -3.330e+02 -3.360e+02 -3.370e+02 -3.420e+02\n",
      " -3.430e+02  3.470e+02  3.490e+02 -3.540e+02 -3.720e+02  3.720e+02\n",
      " -3.720e+02  3.780e+02  3.980e+02 -4.030e+02 -4.080e+02 -4.110e+02\n",
      " -4.140e+02 -4.140e+02 -4.160e+02 -4.170e+02 -4.180e+02 -4.190e+02\n",
      " -4.280e+02 -4.310e+02 -4.330e+02 -4.340e+02  4.350e+02  4.380e+02\n",
      " -4.380e+02 -4.380e+02 -4.420e+02 -4.430e+02 -4.490e+02 -4.510e+02\n",
      " -4.540e+02 -4.540e+02 -4.550e+02 -4.550e+02  4.560e+02 -4.570e+02\n",
      " -4.580e+02 -4.610e+02 -4.610e+02 -4.670e+02 -4.670e+02 -4.710e+02\n",
      " -4.780e+02 -4.820e+02 -4.860e+02 -4.870e+02 -4.910e+02  4.920e+02\n",
      " -4.920e+02 -4.940e+02 -4.940e+02 -5.000e+02 -5.020e+02 -5.080e+02\n",
      " -5.090e+02  5.120e+02 -5.150e+02 -5.160e+02 -5.220e+02 -5.230e+02\n",
      " -5.230e+02 -5.260e+02 -5.300e+02  5.310e+02 -5.310e+02 -5.320e+02\n",
      " -5.330e+02 -5.330e+02  5.370e+02  5.380e+02 -5.420e+02 -5.440e+02\n",
      " -5.440e+02  5.470e+02  5.590e+02  5.610e+02 -5.630e+02 -5.640e+02\n",
      " -5.660e+02 -5.660e+02 -5.660e+02 -5.670e+02 -5.710e+02 -5.710e+02\n",
      "  5.760e+02 -5.760e+02  5.780e+02 -5.820e+02 -5.840e+02 -5.850e+02\n",
      " -5.880e+02 -5.960e+02 -5.990e+02 -6.040e+02  6.050e+02  6.070e+02\n",
      " -6.080e+02 -6.090e+02 -6.090e+02 -6.110e+02 -6.110e+02 -6.150e+02\n",
      " -6.220e+02 -6.230e+02 -6.270e+02 -6.290e+02 -6.290e+02 -6.300e+02\n",
      " -6.330e+02  6.390e+02  6.480e+02  6.480e+02 -6.510e+02 -6.510e+02\n",
      " -6.550e+02 -6.560e+02 -6.710e+02 -6.770e+02 -6.780e+02  6.820e+02\n",
      " -6.850e+02 -6.860e+02 -7.050e+02 -7.060e+02 -7.060e+02 -7.140e+02\n",
      " -7.150e+02 -7.180e+02 -7.210e+02  7.220e+02 -7.240e+02  7.270e+02\n",
      " -7.350e+02 -7.360e+02 -7.380e+02  7.420e+02 -7.480e+02 -7.540e+02\n",
      "  7.580e+02 -7.580e+02 -7.600e+02  7.750e+02  7.750e+02 -7.770e+02\n",
      " -7.850e+02 -7.870e+02  7.880e+02 -7.920e+02 -7.950e+02 -7.960e+02\n",
      " -8.000e+02  8.140e+02  8.140e+02  8.190e+02  8.210e+02 -8.260e+02\n",
      " -8.350e+02 -8.370e+02 -8.460e+02 -8.460e+02 -8.540e+02 -8.540e+02\n",
      " -8.600e+02 -8.620e+02 -8.680e+02  8.860e+02 -8.890e+02 -9.000e+02\n",
      " -9.080e+02 -9.080e+02 -9.140e+02  9.190e+02 -9.260e+02  9.330e+02\n",
      " -9.330e+02 -9.350e+02 -9.390e+02 -9.460e+02 -9.490e+02  9.540e+02\n",
      " -9.560e+02  9.610e+02 -9.620e+02  9.620e+02 -9.640e+02 -9.640e+02\n",
      "  9.840e+02  9.870e+02 -9.920e+02 -9.920e+02 -9.930e+02 -1.004e+03\n",
      "  1.011e+03 -1.021e+03 -1.026e+03 -1.032e+03  1.033e+03 -1.040e+03\n",
      " -1.058e+03 -1.069e+03 -1.076e+03 -1.078e+03 -1.079e+03  1.106e+03\n",
      " -1.116e+03 -1.120e+03  1.120e+03  1.120e+03 -1.127e+03  1.137e+03\n",
      " -1.137e+03 -1.139e+03 -1.147e+03  1.152e+03 -1.164e+03 -1.173e+03\n",
      "  1.183e+03 -1.189e+03 -1.191e+03 -1.201e+03  1.209e+03 -1.210e+03\n",
      " -1.222e+03 -1.229e+03 -1.236e+03 -1.245e+03 -1.250e+03 -1.257e+03\n",
      " -1.277e+03 -1.279e+03 -1.294e+03 -1.294e+03 -1.300e+03 -1.301e+03\n",
      " -1.314e+03 -1.320e+03  1.335e+03 -1.337e+03  1.339e+03 -1.341e+03\n",
      "  1.351e+03 -1.354e+03 -1.359e+03 -1.387e+03 -1.393e+03 -1.397e+03\n",
      " -1.399e+03  1.401e+03 -1.401e+03 -1.426e+03 -1.428e+03 -1.458e+03\n",
      " -1.469e+03  1.481e+03 -1.494e+03 -1.500e+03 -1.540e+03  1.547e+03\n",
      " -1.553e+03 -1.567e+03 -1.568e+03 -1.588e+03 -1.631e+03 -1.650e+03\n",
      "  1.666e+03 -1.706e+03 -1.721e+03 -1.752e+03  1.762e+03 -1.796e+03\n",
      " -1.834e+03 -1.868e+03  1.886e+03  1.891e+03  1.915e+03  1.933e+03\n",
      " -1.943e+03 -1.989e+03  2.000e+03 -2.078e+03 -2.219e+03  2.235e+03\n",
      "  2.282e+03 -2.287e+03 -2.289e+03  2.379e+03 -2.381e+03  2.433e+03\n",
      " -2.493e+03 -2.565e+03 -2.585e+03 -2.602e+03 -2.650e+03 -2.702e+03\n",
      " -2.761e+03 -2.772e+03  2.835e+03 -2.860e+03 -2.869e+03  2.875e+03\n",
      " -2.893e+03  2.907e+03 -2.918e+03 -2.918e+03  2.988e+03 -3.000e+03\n",
      " -3.013e+03  3.200e+03 -3.253e+03  3.571e+03 -3.574e+03 -3.725e+03\n",
      " -3.761e+03  3.978e+03  4.068e+03  4.084e+03  4.229e+03  4.412e+03\n",
      "  4.445e+03 -4.752e+03  5.166e+03 -5.255e+03 -5.546e+03 -6.423e+03]\n",
      "durations 3.0 4695.0\n",
      "Concordance Index 0.5705824284304047\n",
      "Integrated Brier Score: 0.27941860633833004\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m304.7208\u001b[0m       \u001b[32m52.5646\u001b[0m  0.0414\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m295.0052\u001b[0m       \u001b[32m91.1413\u001b[0m  0.0184\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m255.7170\u001b[0m       \u001b[32m60.5453\u001b[0m  0.0177\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2      \u001b[36m287.2098\u001b[0m       \u001b[32m81.0462\u001b[0m  0.0174\n",
      "      2      \u001b[36m234.0378\u001b[0m       61.9533  0.0182\n",
      "      2      305.8200       \u001b[32m51.7635\u001b[0m  0.0335\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m294.1860\u001b[0m       \u001b[32m81.2273\u001b[0m  0.0227\n",
      "      3      292.2438       \u001b[32m72.7930\u001b[0m  0.0228\n",
      "      3      234.3233       62.4849  0.0194\n",
      "      2      \u001b[36m271.9681\u001b[0m       \u001b[32m79.9505\u001b[0m  0.0186\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m308.5206\u001b[0m       \u001b[32m55.7313\u001b[0m  0.0456\n",
      "      4      \u001b[36m233.7416\u001b[0m       \u001b[32m59.1908\u001b[0m  0.0152\n",
      "      4      \u001b[36m285.2092\u001b[0m       \u001b[32m68.0310\u001b[0m  0.0188\n",
      "      3      312.3458       52.4522  0.0333\n",
      "      3      274.0356       \u001b[32m78.5930\u001b[0m  0.0158\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m283.7347\u001b[0m       \u001b[32m62.4660\u001b[0m  0.0608\n",
      "      5      233.8646       \u001b[32m55.8995\u001b[0m  0.0148\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m310.9779\u001b[0m       \u001b[32m54.8556\u001b[0m  0.0208\n",
      "      5      285.8224       \u001b[32m65.3564\u001b[0m  0.0172\n",
      "      4      \u001b[36m269.1288\u001b[0m       \u001b[32m76.0607\u001b[0m  0.0163\n",
      "      6      \u001b[36m221.0741\u001b[0m       \u001b[32m53.3862\u001b[0m  0.0160\n",
      "      2      \u001b[36m296.0523\u001b[0m       57.2639  0.0202\n",
      "      2      323.1052       \u001b[32m54.4511\u001b[0m  0.0345\n",
      "      6      \u001b[36m279.6259\u001b[0m       \u001b[32m64.2669\u001b[0m  0.0163\n",
      "      5      \u001b[36m267.8598\u001b[0m       \u001b[32m72.1856\u001b[0m  0.0159\n",
      "      4      \u001b[36m291.5710\u001b[0m       54.4555  0.0406\n",
      "      2      291.4144       \u001b[32m61.3193\u001b[0m  0.0341\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m240.3015\u001b[0m       \u001b[32m50.8829\u001b[0m  0.0438\n",
      "      7      \u001b[36m221.0649\u001b[0m       \u001b[32m51.7582\u001b[0m  0.0157\n",
      "      3      \u001b[36m293.6528\u001b[0m       59.0995  0.0157\n",
      "      7      \u001b[36m279.0596\u001b[0m       \u001b[32m64.0733\u001b[0m  0.0157\n",
      "      6      \u001b[36m260.7962\u001b[0m       \u001b[32m69.4438\u001b[0m  0.0155\n",
      "      8      \u001b[36m220.6822\u001b[0m       \u001b[32m50.8597\u001b[0m  0.0151\n",
      "      4      \u001b[36m284.8959\u001b[0m       59.3485  0.0155\n",
      "      8      \u001b[36m272.4774\u001b[0m       \u001b[32m64.0473\u001b[0m  0.0154\n",
      "      3      319.6830       54.7824  0.0381\n",
      "      5      304.0725       55.4451  0.0330\n",
      "      7      \u001b[36m260.6340\u001b[0m       \u001b[32m67.7576\u001b[0m  0.0185\n",
      "      9      \u001b[36m217.2887\u001b[0m       \u001b[32m50.4311\u001b[0m  0.0152\n",
      "      2      251.1837       \u001b[32m50.2017\u001b[0m  0.0342\n",
      "      5      285.2124       59.6351  0.0156\n",
      "      9      273.6867       64.1356  0.0158\n",
      "      3      \u001b[36m283.0215\u001b[0m       \u001b[32m61.0980\u001b[0m  0.0459\n",
      "     10      219.0337       \u001b[32m50.3478\u001b[0m  0.0154\n",
      "      8      \u001b[36m256.6747\u001b[0m       \u001b[32m67.0298\u001b[0m  0.0193\n",
      "      6      \u001b[36m282.7630\u001b[0m       60.1607  0.0185\n",
      "     10      \u001b[36m267.0262\u001b[0m       64.3728  0.0187\n",
      "Restoring best model from epoch 8.\n",
      "      6      310.4925       55.9868  0.0347\n",
      "      9      \u001b[36m254.0027\u001b[0m       67.1935  0.0172\n",
      "      4      \u001b[36m300.5870\u001b[0m       56.5921  0.0454\n",
      "      3      \u001b[36m237.1688\u001b[0m       52.0435  0.0416\n",
      "      7      \u001b[36m279.8569\u001b[0m       60.1575  0.0227\n",
      "     10      \u001b[36m252.4213\u001b[0m       67.7220  0.0165\n",
      "Restoring best model from epoch 8.\n",
      "      4      \u001b[36m277.9787\u001b[0m       62.3518  0.0437\n",
      "      8      283.7394       60.1131  0.0177\n",
      "      7      \u001b[36m289.4119\u001b[0m       56.7953  0.0339\n",
      "      4      \u001b[36m226.3032\u001b[0m       58.8219  0.0342\n",
      "      9      \u001b[36m278.7387\u001b[0m       60.1789  0.0230\n",
      "      5      309.5764       59.3023  0.0464\n",
      "      5      \u001b[36m274.6773\u001b[0m       63.5991  0.0429\n",
      "      8      290.9001       57.9299  0.0347\n",
      "     10      \u001b[36m274.0525\u001b[0m       60.3415  0.0160\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      5      265.7914       58.5294  0.0401\n",
      "      6      317.7191       60.5141  0.0336\n",
      "      9      \u001b[36m285.2865\u001b[0m       60.5957  0.0340\n",
      "      6      \u001b[36m271.4918\u001b[0m       64.8445  0.0379\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m297.8549\u001b[0m       \u001b[32m61.9017\u001b[0m  0.0412\n",
      "      6      242.1250       55.8995  0.0385\n",
      "      7      303.9755       60.7741  0.0392\n",
      "     10      \u001b[36m284.6587\u001b[0m       65.1031  0.0345\n",
      "Restoring best model from epoch 2.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m315.5978\u001b[0m       \u001b[32m69.3026\u001b[0m  0.0261\n",
      "      7      \u001b[36m269.5268\u001b[0m       67.1635  0.0365\n",
      "      7      235.6194       53.6266  0.0356\n",
      "      2      \u001b[36m307.8005\u001b[0m       \u001b[32m65.5818\u001b[0m  0.0210\n",
      "      2      301.0867       \u001b[32m61.5085\u001b[0m  0.0424\n",
      "      8      312.8282       61.4772  0.0363\n",
      "      3      \u001b[36m303.3003\u001b[0m       \u001b[32m62.7861\u001b[0m  0.0191\n",
      "      8      269.8849       69.5183  0.0416\n",
      "      8      235.2730       54.0189  0.0365\n",
      "      3      315.7383       \u001b[32m59.8825\u001b[0m  0.0347\n",
      "      4      308.2562       \u001b[32m61.0517\u001b[0m  0.0185\n",
      "      9      303.9839       62.1813  0.0359\n",
      "      9      271.7255       72.7957  0.0329\n",
      "      5      \u001b[36m302.4211\u001b[0m       \u001b[32m61.0159\u001b[0m  0.0169\n",
      "      4      299.8586       59.9324  0.0320\n",
      "      9      232.7006       58.0558  0.0355\n",
      "     10      306.2123       63.9168  0.0318\n",
      "      6      \u001b[36m295.9891\u001b[0m       61.1900  0.0161\n",
      "Restoring best model from epoch 2.\n",
      "     10      \u001b[36m266.1034\u001b[0m       75.8991  0.0337\n",
      "      7      \u001b[36m294.5229\u001b[0m       61.4734  0.0160\n",
      "Restoring best model from epoch 3.\n",
      "      5      \u001b[36m293.3293\u001b[0m       61.8558  0.0352\n",
      "     10      226.8387       64.0234  0.0340\n",
      "Restoring best model from epoch 2.\n",
      "      8      \u001b[36m288.4388\u001b[0m       61.9449  0.0197\n",
      "      9      \u001b[36m287.9058\u001b[0m       62.3673  0.0166\n",
      "      6      \u001b[36m283.5663\u001b[0m       64.9999  0.0331\n",
      "     10      289.5648       62.7404  0.0159\n",
      "Restoring best model from epoch 5.\n",
      "      7      293.1217       68.3771  0.0324\n",
      "      8      285.4458       69.1158  0.0360\n",
      "      9      \u001b[36m281.3806\u001b[0m       69.2844  0.0327\n",
      "     10      \u001b[36m277.4897\u001b[0m       68.2852  0.0383\n",
      "Restoring best model from epoch 3.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m286.7832\u001b[0m       \u001b[32m82.3825\u001b[0m  0.0309\n",
      "      2      \u001b[36m283.7719\u001b[0m       \u001b[32m81.6693\u001b[0m  0.0281\n",
      "      3      296.2969       82.1913  0.0257\n",
      "      4      \u001b[36m266.8550\u001b[0m       82.2389  0.0365\n",
      "      5      284.0021       81.7648  0.0302\n",
      "      6      268.3279       82.9280  0.0306\n",
      "      7      268.0801       87.8846  0.0239\n",
      "      8      \u001b[36m265.7218\u001b[0m       94.6916  0.0261\n",
      "      9      \u001b[36m260.7380\u001b[0m       94.8593  0.0266\n",
      "     10      262.0309       94.7909  0.0313\n",
      "Restoring best model from epoch 2.\n",
      "y_train breslow final [-1.000e+00 -2.000e+00 -3.000e+00 -3.000e+00 -3.000e+00 -3.000e+00\n",
      " -3.000e+00 -4.000e+00 -4.000e+00 -6.000e+00 -7.000e+00 -7.000e+00\n",
      " -7.000e+00 -7.000e+00 -7.000e+00 -7.000e+00  7.000e+00 -8.000e+00\n",
      " -1.000e+01 -1.400e+01 -1.500e+01 -1.700e+01  2.300e+01 -2.300e+01\n",
      "  3.700e+01 -3.900e+01 -5.000e+01 -5.500e+01 -5.800e+01 -6.200e+01\n",
      " -7.000e+01 -7.200e+01 -7.300e+01 -7.400e+01 -7.600e+01 -7.700e+01\n",
      " -8.200e+01  9.600e+01  1.110e+02 -1.120e+02 -1.140e+02 -1.220e+02\n",
      " -1.340e+02 -1.390e+02  1.550e+02  1.620e+02 -1.660e+02 -1.690e+02\n",
      " -1.740e+02  1.780e+02 -1.790e+02 -1.820e+02 -1.840e+02 -1.900e+02\n",
      " -1.940e+02 -1.940e+02  1.940e+02  1.990e+02 -2.030e+02 -2.040e+02\n",
      "  2.050e+02 -2.070e+02  2.140e+02  2.280e+02 -2.300e+02  2.340e+02\n",
      "  2.400e+02  2.410e+02 -2.420e+02  2.420e+02 -2.420e+02 -2.490e+02\n",
      " -2.570e+02 -2.790e+02 -2.860e+02 -2.870e+02 -2.920e+02 -3.010e+02\n",
      " -3.130e+02  3.150e+02 -3.200e+02 -3.260e+02 -3.330e+02 -3.360e+02\n",
      " -3.370e+02 -3.420e+02 -3.430e+02  3.490e+02  3.510e+02 -3.540e+02\n",
      "  3.540e+02 -3.680e+02 -3.720e+02  3.720e+02 -3.720e+02 -3.840e+02\n",
      "  3.880e+02 -3.950e+02 -4.030e+02 -4.050e+02 -4.070e+02 -4.110e+02\n",
      " -4.140e+02 -4.160e+02 -4.170e+02 -4.180e+02 -4.190e+02 -4.270e+02\n",
      " -4.310e+02 -4.340e+02  4.380e+02 -4.380e+02 -4.420e+02 -4.420e+02\n",
      " -4.420e+02 -4.430e+02  4.440e+02 -4.490e+02 -4.540e+02 -4.540e+02\n",
      " -4.550e+02 -4.570e+02 -4.580e+02 -4.610e+02  4.660e+02 -4.670e+02\n",
      " -4.710e+02 -4.780e+02 -4.820e+02 -4.860e+02 -4.870e+02 -4.870e+02\n",
      " -4.910e+02  4.920e+02 -4.920e+02  4.920e+02 -4.940e+02 -4.940e+02\n",
      " -4.970e+02 -5.020e+02 -5.030e+02 -5.080e+02 -5.090e+02  5.120e+02\n",
      " -5.120e+02 -5.150e+02 -5.230e+02 -5.230e+02 -5.260e+02 -5.300e+02\n",
      "  5.310e+02 -5.310e+02 -5.320e+02 -5.330e+02  5.380e+02 -5.440e+02\n",
      "  5.470e+02 -5.480e+02  5.610e+02 -5.660e+02 -5.670e+02 -5.670e+02\n",
      " -5.690e+02 -5.690e+02 -5.730e+02  5.760e+02 -5.760e+02  5.780e+02\n",
      " -5.820e+02 -5.840e+02 -5.850e+02 -5.880e+02  5.920e+02 -6.040e+02\n",
      "  6.050e+02 -6.090e+02 -6.090e+02 -6.110e+02 -6.110e+02 -6.150e+02\n",
      " -6.220e+02 -6.230e+02 -6.270e+02 -6.290e+02 -6.290e+02 -6.300e+02\n",
      " -6.380e+02  6.390e+02 -6.500e+02 -6.510e+02 -6.510e+02 -6.550e+02\n",
      " -6.710e+02 -6.770e+02 -6.780e+02 -6.860e+02 -7.050e+02 -7.060e+02\n",
      " -7.060e+02  7.090e+02 -7.140e+02 -7.150e+02 -7.180e+02  7.220e+02\n",
      "  7.270e+02 -7.350e+02 -7.360e+02 -7.380e+02  7.420e+02 -7.430e+02\n",
      " -7.480e+02  7.580e+02 -7.580e+02 -7.600e+02 -7.720e+02  7.750e+02\n",
      "  7.750e+02 -7.850e+02 -7.870e+02  7.880e+02 -7.920e+02 -7.950e+02\n",
      " -7.960e+02 -8.050e+02  8.140e+02  8.190e+02  8.210e+02 -8.260e+02\n",
      " -8.320e+02 -8.370e+02 -8.460e+02 -8.540e+02 -8.600e+02 -8.620e+02\n",
      " -8.630e+02 -8.680e+02 -8.780e+02 -8.890e+02 -9.080e+02 -9.080e+02\n",
      " -9.080e+02 -9.140e+02 -9.140e+02 -9.160e+02  9.190e+02 -9.190e+02\n",
      " -9.260e+02  9.330e+02 -9.350e+02 -9.390e+02 -9.460e+02  9.540e+02\n",
      " -9.550e+02 -9.560e+02 -9.620e+02  9.620e+02 -9.640e+02 -9.640e+02\n",
      " -9.680e+02  9.840e+02  9.870e+02 -9.920e+02 -9.920e+02 -9.930e+02\n",
      " -1.004e+03  1.011e+03 -1.012e+03 -1.021e+03 -1.026e+03  1.033e+03\n",
      " -1.040e+03 -1.058e+03 -1.069e+03 -1.076e+03 -1.079e+03 -1.099e+03\n",
      "  1.106e+03 -1.112e+03 -1.115e+03 -1.116e+03 -1.120e+03  1.120e+03\n",
      "  1.120e+03 -1.127e+03 -1.130e+03 -1.137e+03 -1.139e+03 -1.147e+03\n",
      " -1.164e+03 -1.173e+03  1.183e+03 -1.189e+03 -1.201e+03  1.209e+03\n",
      " -1.213e+03 -1.217e+03 -1.219e+03  1.220e+03 -1.222e+03 -1.227e+03\n",
      " -1.229e+03 -1.236e+03  1.242e+03 -1.245e+03 -1.250e+03  1.251e+03\n",
      " -1.257e+03  1.262e+03 -1.277e+03 -1.279e+03 -1.294e+03 -1.294e+03\n",
      " -1.300e+03 -1.301e+03 -1.314e+03 -1.320e+03  1.335e+03 -1.337e+03\n",
      "  1.339e+03 -1.341e+03  1.351e+03 -1.354e+03 -1.359e+03 -1.382e+03\n",
      " -1.387e+03 -1.393e+03 -1.397e+03 -1.399e+03  1.401e+03 -1.412e+03\n",
      " -1.421e+03 -1.426e+03 -1.428e+03 -1.453e+03 -1.458e+03 -1.470e+03\n",
      "  1.481e+03  1.491e+03 -1.494e+03 -1.500e+03 -1.519e+03  1.525e+03\n",
      " -1.540e+03 -1.567e+03 -1.568e+03  1.578e+03  1.585e+03 -1.588e+03\n",
      " -1.631e+03  1.666e+03 -1.706e+03 -1.721e+03 -1.752e+03 -1.796e+03\n",
      " -1.806e+03 -1.828e+03 -1.834e+03 -1.868e+03  1.886e+03  1.891e+03\n",
      "  1.933e+03 -1.943e+03 -1.989e+03  2.000e+03  2.052e+03 -2.078e+03\n",
      " -2.107e+03 -2.218e+03 -2.219e+03  2.235e+03  2.282e+03  2.286e+03\n",
      " -2.287e+03 -2.289e+03 -2.289e+03  2.379e+03 -2.381e+03  2.433e+03\n",
      " -2.493e+03 -2.565e+03 -2.585e+03 -2.602e+03 -2.650e+03  2.660e+03\n",
      " -2.702e+03 -2.761e+03  2.835e+03  2.875e+03 -2.893e+03  2.907e+03\n",
      " -2.918e+03  2.988e+03 -3.013e+03  3.200e+03 -3.253e+03  3.470e+03\n",
      "  3.571e+03 -3.725e+03 -3.733e+03 -3.761e+03  4.068e+03 -4.113e+03\n",
      "  4.229e+03  4.412e+03  4.445e+03  4.695e+03  5.166e+03 -5.255e+03\n",
      " -5.546e+03]\n",
      "Concordance Index 0.7501702610669694\n",
      "Integrated Brier Score: 0.15299701494399198\n",
      "y_train breslow final [-1.000e+00 -2.000e+00 -3.000e+00 -3.000e+00 -3.000e+00 -3.000e+00\n",
      " -3.000e+00 -4.000e+00 -4.000e+00 -6.000e+00 -7.000e+00 -7.000e+00\n",
      " -7.000e+00 -7.000e+00 -7.000e+00 -7.000e+00  7.000e+00 -8.000e+00\n",
      " -1.000e+01 -1.400e+01 -1.500e+01 -1.700e+01  2.300e+01 -2.300e+01\n",
      "  3.700e+01 -3.900e+01 -5.000e+01 -5.500e+01 -5.800e+01 -6.200e+01\n",
      " -7.000e+01 -7.200e+01 -7.300e+01 -7.400e+01 -7.600e+01 -7.700e+01\n",
      " -8.200e+01  9.600e+01  1.110e+02 -1.120e+02 -1.140e+02 -1.220e+02\n",
      " -1.340e+02 -1.390e+02  1.550e+02  1.620e+02 -1.660e+02 -1.690e+02\n",
      " -1.740e+02  1.780e+02 -1.790e+02 -1.820e+02 -1.840e+02 -1.900e+02\n",
      " -1.940e+02 -1.940e+02  1.940e+02  1.990e+02 -2.030e+02 -2.040e+02\n",
      "  2.050e+02 -2.070e+02  2.140e+02  2.280e+02 -2.300e+02  2.340e+02\n",
      "  2.400e+02  2.410e+02 -2.420e+02  2.420e+02 -2.420e+02 -2.490e+02\n",
      " -2.570e+02 -2.790e+02 -2.860e+02 -2.870e+02 -2.920e+02 -3.010e+02\n",
      " -3.130e+02  3.150e+02 -3.200e+02 -3.260e+02 -3.330e+02 -3.360e+02\n",
      " -3.370e+02 -3.420e+02 -3.430e+02  3.490e+02  3.510e+02 -3.540e+02\n",
      "  3.540e+02 -3.680e+02 -3.720e+02  3.720e+02 -3.720e+02 -3.840e+02\n",
      "  3.880e+02 -3.950e+02 -4.030e+02 -4.050e+02 -4.070e+02 -4.110e+02\n",
      " -4.140e+02 -4.160e+02 -4.170e+02 -4.180e+02 -4.190e+02 -4.270e+02\n",
      " -4.310e+02 -4.340e+02  4.380e+02 -4.380e+02 -4.420e+02 -4.420e+02\n",
      " -4.420e+02 -4.430e+02  4.440e+02 -4.490e+02 -4.540e+02 -4.540e+02\n",
      " -4.550e+02 -4.570e+02 -4.580e+02 -4.610e+02  4.660e+02 -4.670e+02\n",
      " -4.710e+02 -4.780e+02 -4.820e+02 -4.860e+02 -4.870e+02 -4.870e+02\n",
      " -4.910e+02  4.920e+02 -4.920e+02  4.920e+02 -4.940e+02 -4.940e+02\n",
      " -4.970e+02 -5.020e+02 -5.030e+02 -5.080e+02 -5.090e+02  5.120e+02\n",
      " -5.120e+02 -5.150e+02 -5.230e+02 -5.230e+02 -5.260e+02 -5.300e+02\n",
      "  5.310e+02 -5.310e+02 -5.320e+02 -5.330e+02  5.380e+02 -5.440e+02\n",
      "  5.470e+02 -5.480e+02  5.610e+02 -5.660e+02 -5.670e+02 -5.670e+02\n",
      " -5.690e+02 -5.690e+02 -5.730e+02  5.760e+02 -5.760e+02  5.780e+02\n",
      " -5.820e+02 -5.840e+02 -5.850e+02 -5.880e+02  5.920e+02 -6.040e+02\n",
      "  6.050e+02 -6.090e+02 -6.090e+02 -6.110e+02 -6.110e+02 -6.150e+02\n",
      " -6.220e+02 -6.230e+02 -6.270e+02 -6.290e+02 -6.290e+02 -6.300e+02\n",
      " -6.380e+02  6.390e+02 -6.500e+02 -6.510e+02 -6.510e+02 -6.550e+02\n",
      " -6.710e+02 -6.770e+02 -6.780e+02 -6.860e+02 -7.050e+02 -7.060e+02\n",
      " -7.060e+02  7.090e+02 -7.140e+02 -7.150e+02 -7.180e+02  7.220e+02\n",
      "  7.270e+02 -7.350e+02 -7.360e+02 -7.380e+02  7.420e+02 -7.430e+02\n",
      " -7.480e+02  7.580e+02 -7.580e+02 -7.600e+02 -7.720e+02  7.750e+02\n",
      "  7.750e+02 -7.850e+02 -7.870e+02  7.880e+02 -7.920e+02 -7.950e+02\n",
      " -7.960e+02 -8.050e+02  8.140e+02  8.190e+02  8.210e+02 -8.260e+02\n",
      " -8.320e+02 -8.370e+02 -8.460e+02 -8.540e+02 -8.600e+02 -8.620e+02\n",
      " -8.630e+02 -8.680e+02 -8.780e+02 -8.890e+02 -9.080e+02 -9.080e+02\n",
      " -9.080e+02 -9.140e+02 -9.140e+02 -9.160e+02  9.190e+02 -9.190e+02\n",
      " -9.260e+02  9.330e+02 -9.350e+02 -9.390e+02 -9.460e+02  9.540e+02\n",
      " -9.550e+02 -9.560e+02 -9.620e+02  9.620e+02 -9.640e+02 -9.640e+02\n",
      " -9.680e+02  9.840e+02  9.870e+02 -9.920e+02 -9.920e+02 -9.930e+02\n",
      " -1.004e+03  1.011e+03 -1.012e+03 -1.021e+03 -1.026e+03  1.033e+03\n",
      " -1.040e+03 -1.058e+03 -1.069e+03 -1.076e+03 -1.079e+03 -1.099e+03\n",
      "  1.106e+03 -1.112e+03 -1.115e+03 -1.116e+03 -1.120e+03  1.120e+03\n",
      "  1.120e+03 -1.127e+03 -1.130e+03 -1.137e+03 -1.139e+03 -1.147e+03\n",
      " -1.164e+03 -1.173e+03  1.183e+03 -1.189e+03 -1.201e+03  1.209e+03\n",
      " -1.213e+03 -1.217e+03 -1.219e+03  1.220e+03 -1.222e+03 -1.227e+03\n",
      " -1.229e+03 -1.236e+03  1.242e+03 -1.245e+03 -1.250e+03  1.251e+03\n",
      " -1.257e+03  1.262e+03 -1.277e+03 -1.279e+03 -1.294e+03 -1.294e+03\n",
      " -1.300e+03 -1.301e+03 -1.314e+03 -1.320e+03  1.335e+03 -1.337e+03\n",
      "  1.339e+03 -1.341e+03  1.351e+03 -1.354e+03 -1.359e+03 -1.382e+03\n",
      " -1.387e+03 -1.393e+03 -1.397e+03 -1.399e+03  1.401e+03 -1.412e+03\n",
      " -1.421e+03 -1.426e+03 -1.428e+03 -1.453e+03 -1.458e+03 -1.470e+03\n",
      "  1.481e+03  1.491e+03 -1.494e+03 -1.500e+03 -1.519e+03  1.525e+03\n",
      " -1.540e+03 -1.567e+03 -1.568e+03  1.578e+03  1.585e+03 -1.588e+03\n",
      " -1.631e+03  1.666e+03 -1.706e+03 -1.721e+03 -1.752e+03 -1.796e+03\n",
      " -1.806e+03 -1.828e+03 -1.834e+03 -1.868e+03  1.886e+03  1.891e+03\n",
      "  1.933e+03 -1.943e+03 -1.989e+03  2.000e+03  2.052e+03 -2.078e+03\n",
      " -2.107e+03 -2.218e+03 -2.219e+03  2.235e+03  2.282e+03  2.286e+03\n",
      " -2.287e+03 -2.289e+03 -2.289e+03  2.379e+03 -2.381e+03  2.433e+03\n",
      " -2.493e+03 -2.565e+03 -2.585e+03 -2.602e+03 -2.650e+03  2.660e+03\n",
      " -2.702e+03 -2.761e+03  2.835e+03  2.875e+03 -2.893e+03  2.907e+03\n",
      " -2.918e+03  2.988e+03 -3.013e+03  3.200e+03 -3.253e+03  3.470e+03\n",
      "  3.571e+03 -3.725e+03 -3.733e+03 -3.761e+03  4.068e+03 -4.113e+03\n",
      "  4.229e+03  4.412e+03  4.445e+03  4.695e+03  5.166e+03 -5.255e+03\n",
      " -5.546e+03]\n",
      "durations 3.0 6423.0\n",
      "Concordance Index 0.7900262467191601\n",
      "Integrated Brier Score: 0.21232466022383634\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m281.9138\u001b[0m       \u001b[32m37.4212\u001b[0m  0.0166\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2      \u001b[36m266.8640\u001b[0m       39.6981  0.0296\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      3      \u001b[36m259.4031\u001b[0m       41.4051  0.0188\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m333.6614\u001b[0m       \u001b[32m34.5210\u001b[0m  0.0317\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m298.8596\u001b[0m       \u001b[32m39.2470\u001b[0m  0.0536\n",
      "      4      261.9504       40.6771  0.0219\n",
      "      2      \u001b[36m317.9318\u001b[0m       37.3440  0.0264\n",
      "      5      \u001b[36m257.9617\u001b[0m       39.1480  0.0221\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m310.2913\u001b[0m       \u001b[32m55.2969\u001b[0m  0.0608\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      3      \u001b[36m301.7883\u001b[0m       40.4402  0.0236\n",
      "      6      \u001b[36m256.4911\u001b[0m       37.4796  0.0237\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2      \u001b[36m294.8813\u001b[0m       39.4097  0.0528\n",
      "      7      \u001b[36m251.7775\u001b[0m       \u001b[32m35.8668\u001b[0m  0.0212\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      4      \u001b[36m300.7210\u001b[0m       40.8687  0.0242\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2      311.8649       \u001b[32m55.2656\u001b[0m  0.0445\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      5      300.7277       39.2181  0.0177\n",
      "      8      \u001b[36m250.8088\u001b[0m       \u001b[32m34.2660\u001b[0m  0.0201\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m311.6055\u001b[0m       \u001b[32m57.1805\u001b[0m  0.0420\n",
      "      3      316.7008       \u001b[32m39.2112\u001b[0m  0.0457\n",
      "      6      \u001b[36m299.3726\u001b[0m       37.2347  0.0181\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m318.7769\u001b[0m       \u001b[32m31.1281\u001b[0m  0.0604\n",
      "      9      \u001b[36m241.6511\u001b[0m       \u001b[32m33.2698\u001b[0m  0.0208\n",
      "      2      \u001b[36m298.7340\u001b[0m       62.0259  0.0205\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m282.8721\u001b[0m       \u001b[32m29.8063\u001b[0m  0.0400\n",
      "      3      318.1941       \u001b[32m54.1647\u001b[0m  0.0367\n",
      "     10      246.2707       \u001b[32m32.7647\u001b[0m  0.0165\n",
      "      3      304.2736       65.6669  0.0189\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m315.0806\u001b[0m       \u001b[32m42.7097\u001b[0m  0.0260\n",
      "      7      301.0640       35.5535  0.0323\n",
      "      4      298.4264       39.5425  0.0456\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m304.4489\u001b[0m       \u001b[32m50.0963\u001b[0m  0.0489\n",
      "      2      \u001b[36m314.6760\u001b[0m       \u001b[32m31.0595\u001b[0m  0.0394\n",
      "      4      \u001b[36m296.5084\u001b[0m       67.8642  0.0184\n",
      "      4      \u001b[36m302.4290\u001b[0m       56.0739  0.0347\n",
      "      2      \u001b[36m291.9597\u001b[0m       46.6243  0.0212\n",
      "      2      \u001b[36m271.6689\u001b[0m       30.7611  0.0372\n",
      "      8      \u001b[36m295.0519\u001b[0m       \u001b[32m34.2112\u001b[0m  0.0202\n",
      "      5      \u001b[36m290.1528\u001b[0m       70.1919  0.0210\n",
      "      3      \u001b[36m284.2267\u001b[0m       50.2753  0.0198\n",
      "      9      297.3621       \u001b[32m33.4594\u001b[0m  0.0201\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      3      342.3747       31.4954  0.0412\n",
      "      6      \u001b[36m284.5574\u001b[0m       70.8100  0.0174\n",
      "      5      \u001b[36m293.8457\u001b[0m       39.8208  0.0504\n",
      "      2      \u001b[36m294.2480\u001b[0m       \u001b[32m49.5893\u001b[0m  0.0484\n",
      "      5      302.6015       61.2195  0.0378\n",
      "      4      \u001b[36m280.0392\u001b[0m       50.8463  0.0186\n",
      "      3      279.8605       33.3906  0.0409\n",
      "     10      \u001b[36m292.6948\u001b[0m       \u001b[32m33.0248\u001b[0m  0.0195\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m294.7024\u001b[0m       \u001b[32m63.3505\u001b[0m  0.0217\n",
      "      7      \u001b[36m282.9170\u001b[0m       70.5431  0.0198\n",
      "      5      282.3275       49.2283  0.0175\n",
      "      2      \u001b[36m282.0066\u001b[0m       \u001b[32m60.2531\u001b[0m  0.0180\n",
      "      6      303.1556       65.3175  0.0331\n",
      "      4      328.3008       31.4704  0.0370\n",
      "      6      280.1398       47.7650  0.0154\n",
      "      8      \u001b[36m278.4778\u001b[0m       69.7424  0.0188\n",
      "      6      \u001b[36m285.5688\u001b[0m       40.2629  0.0425\n",
      "      3      311.8595       \u001b[32m48.1196\u001b[0m  0.0409\n",
      "      4      279.6125       33.6723  0.0427\n",
      "      3      283.9646       \u001b[32m56.3140\u001b[0m  0.0196\n",
      "      7      \u001b[36m275.7691\u001b[0m       46.3432  0.0156\n",
      "      9      280.5895       68.4956  0.0151\n",
      "      8      \u001b[36m272.4132\u001b[0m       45.5977  0.0164\n",
      "     10      283.2715       67.0630  0.0174\n",
      "Restoring best model from epoch 1.\n",
      "      5      324.8690       31.5093  0.0385\n",
      "      4      \u001b[36m279.3990\u001b[0m       \u001b[32m53.9883\u001b[0m  0.0219\n",
      "      7      \u001b[36m292.2295\u001b[0m       65.2645  0.0407\n",
      "      4      \u001b[36m289.0527\u001b[0m       \u001b[32m47.0299\u001b[0m  0.0332\n",
      "      5      288.8905       34.1132  0.0318\n",
      "      7      295.0113       40.7775  0.0375\n",
      "      9      274.7251       45.1771  0.0141\n",
      "     10      275.6894       44.7464  0.0147\n",
      "Restoring best model from epoch 1.\n",
      "      5      \u001b[36m276.8145\u001b[0m       \u001b[32m52.5848\u001b[0m  0.0235\n",
      "      8      299.2374       63.3886  0.0312\n",
      "      6      \u001b[36m309.9448\u001b[0m       32.1975  0.0326\n",
      "      5      294.6633       47.1763  0.0317\n",
      "      6      \u001b[36m265.5041\u001b[0m       35.0290  0.0300\n",
      "      8      \u001b[36m285.5410\u001b[0m       41.3543  0.0367\n",
      "      6      278.0220       \u001b[32m52.2220\u001b[0m  0.0240\n",
      "      9      293.1026       62.7345  0.0314\n",
      "      7      \u001b[36m274.7314\u001b[0m       52.3836  0.0167\n",
      "      7      268.8679       33.2821  0.0308\n",
      "      7      315.9295       33.4556  0.0382\n",
      "      6      291.8757       48.3898  0.0379\n",
      "      9      \u001b[36m277.4949\u001b[0m       42.1242  0.0385\n",
      "     10      \u001b[36m291.3026\u001b[0m       62.6157  0.0337\n",
      "Restoring best model from epoch 3.\n",
      "      8      265.8654       32.7685  0.0323\n",
      "      8      \u001b[36m307.6908\u001b[0m       35.2645  0.0319\n",
      "      7      \u001b[36m284.3652\u001b[0m       50.1164  0.0344\n",
      "      8      \u001b[36m263.5633\u001b[0m       52.5917  0.0455\n",
      "     10      288.1909       42.0613  0.0363\n",
      "Restoring best model from epoch 3.\n",
      "      9      270.7897       52.7138  0.0161\n",
      "      9      \u001b[36m258.9778\u001b[0m       33.6464  0.0316\n",
      "      9      \u001b[36m297.1552\u001b[0m       37.6604  0.0335\n",
      "      8      \u001b[36m272.5557\u001b[0m       52.7655  0.0350\n",
      "     10      266.4547       52.6642  0.0155\n",
      "Restoring best model from epoch 6.\n",
      "     10      260.9403       34.2598  0.0317\n",
      "Restoring best model from epoch 1.\n",
      "     10      298.7528       40.1827  0.0342\n",
      "Restoring best model from epoch 2.\n",
      "      9      276.0231       56.1682  0.0325\n",
      "     10      278.2365       59.0163  0.0307\n",
      "Restoring best model from epoch 4.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m268.7900\u001b[0m       \u001b[32m61.4325\u001b[0m  0.0264\n",
      "      2      \u001b[36m266.6418\u001b[0m       61.4989  0.0264\n",
      "      3      273.8399       63.0853  0.0323\n",
      "      4      \u001b[36m263.7018\u001b[0m       63.0999  0.0274\n",
      "      5      \u001b[36m247.7236\u001b[0m       \u001b[32m60.0933\u001b[0m  0.0257\n",
      "      6      253.3917       61.3167  0.0260\n",
      "      7      263.8598       \u001b[32m59.3402\u001b[0m  0.0266\n",
      "      8      256.8646       \u001b[32m57.4098\u001b[0m  0.0312\n",
      "      9      255.2772       \u001b[32m56.2344\u001b[0m  0.0263\n",
      "     10      260.3735       \u001b[32m55.9334\u001b[0m  0.0256\n",
      "y_train breslow final [-1.000e+00 -2.000e+00 -3.000e+00 -3.000e+00 -3.000e+00 -3.000e+00\n",
      " -3.000e+00 -3.000e+00 -3.000e+00 -3.000e+00 -3.000e+00 -4.000e+00\n",
      " -4.000e+00 -5.000e+00 -6.000e+00 -7.000e+00 -7.000e+00 -7.000e+00\n",
      " -7.000e+00  7.000e+00 -1.000e+01 -1.500e+01  3.700e+01 -3.900e+01\n",
      " -5.000e+01 -5.500e+01 -5.800e+01 -6.200e+01 -6.200e+01 -6.300e+01\n",
      " -7.200e+01 -7.200e+01 -7.300e+01 -7.400e+01 -7.600e+01 -7.700e+01\n",
      " -8.200e+01 -8.400e+01 -9.000e+01 -1.050e+02  1.110e+02 -1.120e+02\n",
      "  1.130e+02 -1.140e+02 -1.220e+02 -1.340e+02 -1.390e+02 -1.530e+02\n",
      "  1.550e+02 -1.620e+02  1.620e+02 -1.660e+02 -1.690e+02  1.780e+02\n",
      " -1.790e+02 -1.820e+02 -1.840e+02 -1.900e+02 -1.940e+02 -1.940e+02\n",
      "  1.990e+02 -1.990e+02 -2.030e+02 -2.040e+02 -2.070e+02 -2.100e+02\n",
      "  2.140e+02  2.280e+02 -2.300e+02 -2.310e+02  2.340e+02  2.400e+02\n",
      "  2.410e+02 -2.420e+02  2.420e+02 -2.420e+02 -2.430e+02  2.450e+02\n",
      " -2.490e+02 -2.570e+02  2.690e+02 -2.740e+02 -2.790e+02 -2.870e+02\n",
      " -2.920e+02 -3.010e+02 -3.130e+02  3.150e+02  3.160e+02 -3.170e+02\n",
      " -3.260e+02 -3.280e+02 -3.360e+02 -3.370e+02 -3.420e+02 -3.430e+02\n",
      "  3.470e+02  3.490e+02  3.510e+02 -3.540e+02  3.540e+02 -3.680e+02\n",
      " -3.720e+02  3.780e+02 -3.840e+02  3.880e+02 -3.950e+02  3.980e+02\n",
      " -4.050e+02 -4.070e+02 -4.080e+02 -4.110e+02 -4.140e+02 -4.140e+02\n",
      " -4.170e+02 -4.180e+02 -4.190e+02 -4.270e+02 -4.280e+02 -4.310e+02\n",
      " -4.330e+02 -4.340e+02  4.350e+02  4.380e+02 -4.380e+02 -4.380e+02\n",
      " -4.420e+02 -4.420e+02 -4.420e+02  4.440e+02 -4.490e+02 -4.510e+02\n",
      " -4.540e+02 -4.540e+02 -4.550e+02 -4.550e+02  4.560e+02 -4.570e+02\n",
      " -4.580e+02 -4.610e+02 -4.610e+02  4.660e+02 -4.670e+02 -4.710e+02\n",
      " -4.820e+02 -4.870e+02 -4.870e+02 -4.910e+02  4.920e+02 -4.920e+02\n",
      "  4.920e+02 -4.940e+02 -4.970e+02 -5.000e+02 -5.020e+02 -5.030e+02\n",
      " -5.080e+02 -5.090e+02  5.120e+02 -5.120e+02 -5.160e+02 -5.220e+02\n",
      " -5.260e+02  5.310e+02 -5.330e+02  5.370e+02  5.380e+02 -5.420e+02\n",
      " -5.440e+02 -5.440e+02 -5.480e+02  5.590e+02  5.610e+02 -5.630e+02\n",
      " -5.640e+02 -5.660e+02 -5.660e+02 -5.660e+02 -5.670e+02 -5.690e+02\n",
      " -5.690e+02 -5.710e+02 -5.710e+02 -5.730e+02  5.760e+02  5.780e+02\n",
      " -5.850e+02 -5.880e+02  5.920e+02 -5.960e+02 -5.990e+02  6.050e+02\n",
      "  6.070e+02 -6.080e+02 -6.090e+02 -6.110e+02 -6.110e+02 -6.150e+02\n",
      " -6.220e+02 -6.290e+02 -6.290e+02 -6.300e+02 -6.330e+02 -6.380e+02\n",
      "  6.480e+02  6.480e+02 -6.500e+02 -6.510e+02 -6.560e+02 -6.770e+02\n",
      " -6.780e+02  6.820e+02 -6.850e+02 -6.860e+02 -7.050e+02 -7.060e+02\n",
      " -7.060e+02  7.090e+02 -7.140e+02 -7.180e+02 -7.210e+02  7.220e+02\n",
      " -7.240e+02 -7.380e+02  7.420e+02 -7.430e+02 -7.540e+02 -7.600e+02\n",
      " -7.720e+02  7.750e+02  7.750e+02 -7.770e+02 -7.850e+02  7.880e+02\n",
      " -7.950e+02 -7.960e+02 -8.000e+02 -8.050e+02  8.140e+02  8.140e+02\n",
      "  8.190e+02  8.210e+02 -8.260e+02 -8.320e+02 -8.350e+02 -8.370e+02\n",
      " -8.460e+02 -8.460e+02 -8.540e+02 -8.540e+02 -8.620e+02 -8.630e+02\n",
      " -8.680e+02 -8.780e+02  8.860e+02 -8.890e+02 -9.000e+02 -9.080e+02\n",
      " -9.080e+02 -9.140e+02 -9.160e+02 -9.190e+02 -9.330e+02 -9.350e+02\n",
      " -9.390e+02 -9.490e+02  9.540e+02 -9.550e+02 -9.560e+02  9.610e+02\n",
      " -9.620e+02 -9.640e+02 -9.680e+02  9.840e+02 -9.920e+02 -9.930e+02\n",
      " -1.004e+03  1.011e+03 -1.012e+03 -1.021e+03 -1.026e+03 -1.032e+03\n",
      " -1.058e+03 -1.078e+03 -1.099e+03 -1.112e+03 -1.115e+03 -1.116e+03\n",
      "  1.120e+03  1.120e+03 -1.127e+03 -1.130e+03  1.137e+03 -1.137e+03\n",
      " -1.147e+03  1.152e+03 -1.164e+03  1.183e+03 -1.189e+03 -1.191e+03\n",
      " -1.201e+03 -1.201e+03  1.209e+03 -1.210e+03 -1.213e+03 -1.217e+03\n",
      " -1.219e+03  1.220e+03 -1.227e+03 -1.229e+03 -1.236e+03  1.242e+03\n",
      " -1.250e+03  1.251e+03 -1.257e+03  1.262e+03 -1.277e+03 -1.294e+03\n",
      " -1.300e+03 -1.301e+03 -1.314e+03 -1.320e+03  1.335e+03 -1.337e+03\n",
      "  1.339e+03 -1.341e+03  1.351e+03 -1.382e+03 -1.397e+03 -1.399e+03\n",
      "  1.401e+03 -1.401e+03 -1.412e+03 -1.421e+03 -1.426e+03 -1.426e+03\n",
      " -1.453e+03 -1.458e+03 -1.469e+03 -1.470e+03  1.491e+03 -1.494e+03\n",
      " -1.500e+03 -1.519e+03  1.525e+03 -1.540e+03  1.547e+03 -1.553e+03\n",
      " -1.567e+03  1.578e+03  1.585e+03 -1.588e+03 -1.631e+03 -1.650e+03\n",
      "  1.666e+03 -1.706e+03 -1.721e+03 -1.752e+03  1.762e+03 -1.796e+03\n",
      " -1.806e+03 -1.828e+03  1.891e+03  1.915e+03  1.933e+03 -1.943e+03\n",
      "  2.000e+03  2.052e+03 -2.107e+03 -2.218e+03 -2.219e+03  2.282e+03\n",
      "  2.286e+03 -2.287e+03 -2.289e+03 -2.289e+03  2.379e+03 -2.381e+03\n",
      " -2.565e+03 -2.602e+03 -2.650e+03  2.660e+03 -2.702e+03 -2.761e+03\n",
      " -2.772e+03  2.835e+03 -2.860e+03 -2.869e+03  2.875e+03 -2.893e+03\n",
      " -2.918e+03 -2.918e+03  2.988e+03 -3.000e+03 -3.013e+03  3.470e+03\n",
      " -3.574e+03 -3.725e+03 -3.733e+03  3.978e+03  4.084e+03 -4.113e+03\n",
      "  4.229e+03  4.412e+03  4.695e+03 -4.752e+03 -5.255e+03 -5.546e+03\n",
      " -6.423e+03]\n",
      "Concordance Index 0.8298012517843417\n",
      "Integrated Brier Score: 0.19093646885379534\n",
      "y_train breslow final [-1.000e+00 -2.000e+00 -3.000e+00 -3.000e+00 -3.000e+00 -3.000e+00\n",
      " -3.000e+00 -3.000e+00 -3.000e+00 -3.000e+00 -3.000e+00 -4.000e+00\n",
      " -4.000e+00 -5.000e+00 -6.000e+00 -7.000e+00 -7.000e+00 -7.000e+00\n",
      " -7.000e+00  7.000e+00 -1.000e+01 -1.500e+01  3.700e+01 -3.900e+01\n",
      " -5.000e+01 -5.500e+01 -5.800e+01 -6.200e+01 -6.200e+01 -6.300e+01\n",
      " -7.200e+01 -7.200e+01 -7.300e+01 -7.400e+01 -7.600e+01 -7.700e+01\n",
      " -8.200e+01 -8.400e+01 -9.000e+01 -1.050e+02  1.110e+02 -1.120e+02\n",
      "  1.130e+02 -1.140e+02 -1.220e+02 -1.340e+02 -1.390e+02 -1.530e+02\n",
      "  1.550e+02 -1.620e+02  1.620e+02 -1.660e+02 -1.690e+02  1.780e+02\n",
      " -1.790e+02 -1.820e+02 -1.840e+02 -1.900e+02 -1.940e+02 -1.940e+02\n",
      "  1.990e+02 -1.990e+02 -2.030e+02 -2.040e+02 -2.070e+02 -2.100e+02\n",
      "  2.140e+02  2.280e+02 -2.300e+02 -2.310e+02  2.340e+02  2.400e+02\n",
      "  2.410e+02 -2.420e+02  2.420e+02 -2.420e+02 -2.430e+02  2.450e+02\n",
      " -2.490e+02 -2.570e+02  2.690e+02 -2.740e+02 -2.790e+02 -2.870e+02\n",
      " -2.920e+02 -3.010e+02 -3.130e+02  3.150e+02  3.160e+02 -3.170e+02\n",
      " -3.260e+02 -3.280e+02 -3.360e+02 -3.370e+02 -3.420e+02 -3.430e+02\n",
      "  3.470e+02  3.490e+02  3.510e+02 -3.540e+02  3.540e+02 -3.680e+02\n",
      " -3.720e+02  3.780e+02 -3.840e+02  3.880e+02 -3.950e+02  3.980e+02\n",
      " -4.050e+02 -4.070e+02 -4.080e+02 -4.110e+02 -4.140e+02 -4.140e+02\n",
      " -4.170e+02 -4.180e+02 -4.190e+02 -4.270e+02 -4.280e+02 -4.310e+02\n",
      " -4.330e+02 -4.340e+02  4.350e+02  4.380e+02 -4.380e+02 -4.380e+02\n",
      " -4.420e+02 -4.420e+02 -4.420e+02  4.440e+02 -4.490e+02 -4.510e+02\n",
      " -4.540e+02 -4.540e+02 -4.550e+02 -4.550e+02  4.560e+02 -4.570e+02\n",
      " -4.580e+02 -4.610e+02 -4.610e+02  4.660e+02 -4.670e+02 -4.710e+02\n",
      " -4.820e+02 -4.870e+02 -4.870e+02 -4.910e+02  4.920e+02 -4.920e+02\n",
      "  4.920e+02 -4.940e+02 -4.970e+02 -5.000e+02 -5.020e+02 -5.030e+02\n",
      " -5.080e+02 -5.090e+02  5.120e+02 -5.120e+02 -5.160e+02 -5.220e+02\n",
      " -5.260e+02  5.310e+02 -5.330e+02  5.370e+02  5.380e+02 -5.420e+02\n",
      " -5.440e+02 -5.440e+02 -5.480e+02  5.590e+02  5.610e+02 -5.630e+02\n",
      " -5.640e+02 -5.660e+02 -5.660e+02 -5.660e+02 -5.670e+02 -5.690e+02\n",
      " -5.690e+02 -5.710e+02 -5.710e+02 -5.730e+02  5.760e+02  5.780e+02\n",
      " -5.850e+02 -5.880e+02  5.920e+02 -5.960e+02 -5.990e+02  6.050e+02\n",
      "  6.070e+02 -6.080e+02 -6.090e+02 -6.110e+02 -6.110e+02 -6.150e+02\n",
      " -6.220e+02 -6.290e+02 -6.290e+02 -6.300e+02 -6.330e+02 -6.380e+02\n",
      "  6.480e+02  6.480e+02 -6.500e+02 -6.510e+02 -6.560e+02 -6.770e+02\n",
      " -6.780e+02  6.820e+02 -6.850e+02 -6.860e+02 -7.050e+02 -7.060e+02\n",
      " -7.060e+02  7.090e+02 -7.140e+02 -7.180e+02 -7.210e+02  7.220e+02\n",
      " -7.240e+02 -7.380e+02  7.420e+02 -7.430e+02 -7.540e+02 -7.600e+02\n",
      " -7.720e+02  7.750e+02  7.750e+02 -7.770e+02 -7.850e+02  7.880e+02\n",
      " -7.950e+02 -7.960e+02 -8.000e+02 -8.050e+02  8.140e+02  8.140e+02\n",
      "  8.190e+02  8.210e+02 -8.260e+02 -8.320e+02 -8.350e+02 -8.370e+02\n",
      " -8.460e+02 -8.460e+02 -8.540e+02 -8.540e+02 -8.620e+02 -8.630e+02\n",
      " -8.680e+02 -8.780e+02  8.860e+02 -8.890e+02 -9.000e+02 -9.080e+02\n",
      " -9.080e+02 -9.140e+02 -9.160e+02 -9.190e+02 -9.330e+02 -9.350e+02\n",
      " -9.390e+02 -9.490e+02  9.540e+02 -9.550e+02 -9.560e+02  9.610e+02\n",
      " -9.620e+02 -9.640e+02 -9.680e+02  9.840e+02 -9.920e+02 -9.930e+02\n",
      " -1.004e+03  1.011e+03 -1.012e+03 -1.021e+03 -1.026e+03 -1.032e+03\n",
      " -1.058e+03 -1.078e+03 -1.099e+03 -1.112e+03 -1.115e+03 -1.116e+03\n",
      "  1.120e+03  1.120e+03 -1.127e+03 -1.130e+03  1.137e+03 -1.137e+03\n",
      " -1.147e+03  1.152e+03 -1.164e+03  1.183e+03 -1.189e+03 -1.191e+03\n",
      " -1.201e+03 -1.201e+03  1.209e+03 -1.210e+03 -1.213e+03 -1.217e+03\n",
      " -1.219e+03  1.220e+03 -1.227e+03 -1.229e+03 -1.236e+03  1.242e+03\n",
      " -1.250e+03  1.251e+03 -1.257e+03  1.262e+03 -1.277e+03 -1.294e+03\n",
      " -1.300e+03 -1.301e+03 -1.314e+03 -1.320e+03  1.335e+03 -1.337e+03\n",
      "  1.339e+03 -1.341e+03  1.351e+03 -1.382e+03 -1.397e+03 -1.399e+03\n",
      "  1.401e+03 -1.401e+03 -1.412e+03 -1.421e+03 -1.426e+03 -1.426e+03\n",
      " -1.453e+03 -1.458e+03 -1.469e+03 -1.470e+03  1.491e+03 -1.494e+03\n",
      " -1.500e+03 -1.519e+03  1.525e+03 -1.540e+03  1.547e+03 -1.553e+03\n",
      " -1.567e+03  1.578e+03  1.585e+03 -1.588e+03 -1.631e+03 -1.650e+03\n",
      "  1.666e+03 -1.706e+03 -1.721e+03 -1.752e+03  1.762e+03 -1.796e+03\n",
      " -1.806e+03 -1.828e+03  1.891e+03  1.915e+03  1.933e+03 -1.943e+03\n",
      "  2.000e+03  2.052e+03 -2.107e+03 -2.218e+03 -2.219e+03  2.282e+03\n",
      "  2.286e+03 -2.287e+03 -2.289e+03 -2.289e+03  2.379e+03 -2.381e+03\n",
      " -2.565e+03 -2.602e+03 -2.650e+03  2.660e+03 -2.702e+03 -2.761e+03\n",
      " -2.772e+03  2.835e+03 -2.860e+03 -2.869e+03  2.875e+03 -2.893e+03\n",
      " -2.918e+03 -2.918e+03  2.988e+03 -3.000e+03 -3.013e+03  3.470e+03\n",
      " -3.574e+03 -3.725e+03 -3.733e+03  3.978e+03  4.084e+03 -4.113e+03\n",
      "  4.229e+03  4.412e+03  4.695e+03 -4.752e+03 -5.255e+03 -5.546e+03\n",
      " -6.423e+03]\n",
      "durations 4.0 5166.0\n",
      "Concordance Index 0.693939393939394\n",
      "Integrated Brier Score: 0.1901732538104823\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m296.6620\u001b[0m       \u001b[32m61.0815\u001b[0m  0.0183\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m322.7235\u001b[0m       \u001b[32m54.2901\u001b[0m  0.0177\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      2      \u001b[36m280.8637\u001b[0m       64.5277  0.0216\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2      \u001b[36m306.7453\u001b[0m       57.1687  0.0164\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      3      \u001b[36m278.7928\u001b[0m       71.7672  0.0164\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m294.4927\u001b[0m       \u001b[32m34.3038\u001b[0m  0.0225\n",
      "      3      \u001b[36m297.3498\u001b[0m       62.7811  0.0176\n",
      "      4      \u001b[36m277.7374\u001b[0m       73.1530  0.0158\n",
      "      2      \u001b[36m270.9352\u001b[0m       37.8311  0.0159\n",
      "      4      \u001b[36m296.8681\u001b[0m       64.2017  0.0156\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m314.5132\u001b[0m       \u001b[32m44.9476\u001b[0m  0.0389\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m314.3935\u001b[0m       \u001b[32m68.1542\u001b[0m  0.0237\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m314.9076\u001b[0m       \u001b[32m46.3057\u001b[0m  0.0409\n",
      "      5      280.2386       69.4431  0.0233\n",
      "      3      \u001b[36m259.8002\u001b[0m       40.4809  0.0213\n",
      "      5      \u001b[36m294.6154\u001b[0m       62.5807  0.0205\n",
      "      2      \u001b[36m304.1792\u001b[0m       \u001b[32m60.1114\u001b[0m  0.0176\n",
      "      4      \u001b[36m257.3942\u001b[0m       39.9796  0.0151\n",
      "      6      \u001b[36m268.1517\u001b[0m       64.4243  0.0221\n",
      "      3      305.3885       \u001b[32m55.5371\u001b[0m  0.0158\n",
      "      6      \u001b[36m288.7068\u001b[0m       59.6812  0.0192\n",
      "      2      328.1547       45.1841  0.0409\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      2      \u001b[36m311.1781\u001b[0m       \u001b[32m46.0855\u001b[0m  0.0326\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      7      276.0267       \u001b[32m59.0181\u001b[0m  0.0150\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      4      307.2072       \u001b[32m51.0362\u001b[0m  0.0160\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      7      293.9347       55.9925  0.0162\n",
      "      5      \u001b[36m247.6918\u001b[0m       39.0796  0.0279\n",
      "      8      \u001b[36m267.6758\u001b[0m       \u001b[32m55.4256\u001b[0m  0.0151\n",
      "      3      316.9718       47.3078  0.0316\n",
      "      5      \u001b[36m294.6452\u001b[0m       \u001b[32m48.4096\u001b[0m  0.0197\n",
      "      8      289.7942       \u001b[32m51.9130\u001b[0m  0.0211\n",
      "      6      \u001b[36m244.5179\u001b[0m       37.9332  0.0186\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m327.8753\u001b[0m       \u001b[32m59.7747\u001b[0m  0.0236\n",
      "      9      \u001b[36m260.8582\u001b[0m       \u001b[32m53.3863\u001b[0m  0.0220\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m286.5614\u001b[0m       \u001b[32m51.3446\u001b[0m  0.0450\n",
      "      6      295.8387       \u001b[32m47.1426\u001b[0m  0.0194\n",
      "      9      \u001b[36m282.2798\u001b[0m       \u001b[32m49.5408\u001b[0m  0.0218\n",
      "      7      253.6448       35.8946  0.0183\n",
      "      3      329.1399       \u001b[32m44.7434\u001b[0m  0.0554\n",
      "      2      \u001b[36m311.3496\u001b[0m       61.2218  0.0271\n",
      "     10      262.4114       \u001b[32m52.2771\u001b[0m  0.0257\n",
      "      7      296.2811       \u001b[32m46.4747\u001b[0m  0.0198\n",
      "      4      \u001b[36m308.7866\u001b[0m       49.8432  0.0452\n",
      "     10      \u001b[36m279.3295\u001b[0m       \u001b[32m48.1821\u001b[0m  0.0176\n",
      "      8      251.6568       \u001b[32m33.2411\u001b[0m  0.0189\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      3      \u001b[36m297.1117\u001b[0m       63.6081  0.0184\n",
      "      8      \u001b[36m289.6674\u001b[0m       \u001b[32m46.2301\u001b[0m  0.0159\n",
      "      9      245.3439       \u001b[32m31.2441\u001b[0m  0.0150\n",
      "      2      \u001b[36m279.1547\u001b[0m       \u001b[32m51.1524\u001b[0m  0.0486\n",
      "      4      301.9676       60.7007  0.0184\n",
      "      4      329.6984       44.9504  0.0488\n",
      "     10      248.5759       \u001b[32m30.3375\u001b[0m  0.0167\n",
      "      9      291.2519       46.2780  0.0224\n",
      "      5      313.7761       50.2378  0.0467\n",
      "      5      299.5859       \u001b[32m56.4097\u001b[0m  0.0193\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m310.7908\u001b[0m       \u001b[32m45.9862\u001b[0m  0.0451\n",
      "     10      294.3340       46.3497  0.0172\n",
      "Restoring best model from epoch 8.\n",
      "      6      \u001b[36m293.1790\u001b[0m       \u001b[32m53.0588\u001b[0m  0.0156\n",
      "      5      316.8522       45.4241  0.0392\n",
      "      3      293.2205       52.8962  0.0472\n",
      "      7      294.9462       \u001b[32m49.9678\u001b[0m  0.0202\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      2      \u001b[36m304.1550\u001b[0m       \u001b[32m45.8913\u001b[0m  0.0386\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      6      341.6154       51.0858  0.0580\n",
      "      6      320.0408       48.2965  0.0317\n",
      "      3      328.7849       48.0329  0.0343\n",
      "      4      315.4343       52.0384  0.0566\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m280.4248\u001b[0m       \u001b[32m33.1545\u001b[0m  0.0530\n",
      "      7      318.9771       50.6817  0.0486\n",
      "      8      293.7052       \u001b[32m48.2320\u001b[0m  0.0676\n",
      "      7      317.5929       48.4971  0.0540\n",
      "      4      337.4815       46.7801  0.0337\n",
      "      9      \u001b[36m291.9304\u001b[0m       \u001b[32m47.2207\u001b[0m  0.0158\n",
      "      5      284.7925       \u001b[32m50.5755\u001b[0m  0.0494\n",
      "      2      \u001b[36m266.2764\u001b[0m       \u001b[32m30.9155\u001b[0m  0.0316\n",
      "     10      \u001b[36m290.1258\u001b[0m       \u001b[32m46.7829\u001b[0m  0.0148\n",
      "      8      316.3834       50.7317  0.0345\n",
      "      8      308.8701       46.7462  0.0333\n",
      "      5      320.8499       46.0283  0.0319\n",
      "      6      295.0319       \u001b[32m49.6260\u001b[0m  0.0317\n",
      "      3      289.7849       \u001b[32m30.5677\u001b[0m  0.0303\n",
      "      9      \u001b[36m309.9560\u001b[0m       49.3784  0.0293\n",
      "      9      \u001b[36m297.7748\u001b[0m       46.2960  0.0306\n",
      "      6      310.9373       46.6246  0.0307\n",
      "     10      \u001b[36m299.2502\u001b[0m       48.0413  0.0288\n",
      "      7      286.6859       \u001b[32m48.9065\u001b[0m  0.0306\n",
      "Restoring best model from epoch 3.\n",
      "      4      306.4554       \u001b[32m29.8010\u001b[0m  0.0305\n",
      "     10      313.2151       46.6431  0.0316\n",
      "Restoring best model from epoch 1.\n",
      "      7      309.9928       47.6606  0.0308\n",
      "      8      279.8620       49.3722  0.0305\n",
      "      5      280.4480       29.9340  0.0299\n",
      "      8      \u001b[36m299.1354\u001b[0m       49.9682  0.0297\n",
      "      6      273.4179       30.3180  0.0300\n",
      "      9      281.1421       50.7813  0.0332\n",
      "      9      305.8485       52.8896  0.0304\n",
      "      7      268.0335       31.0522  0.0295\n",
      "     10      279.5854       53.1128  0.0306\n",
      "Restoring best model from epoch 7.\n",
      "     10      \u001b[36m292.6815\u001b[0m       56.4146  0.0303\n",
      "Restoring best model from epoch 2.\n",
      "      8      \u001b[36m258.2922\u001b[0m       31.1614  0.0306\n",
      "      9      258.5411       31.2563  0.0308\n",
      "     10      270.1947       31.0307  0.0318\n",
      "Restoring best model from epoch 4.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m313.3059\u001b[0m       \u001b[32m58.9460\u001b[0m  0.0163\n",
      "      2      \u001b[36m292.6972\u001b[0m       59.9874  0.0162\n",
      "      3      \u001b[36m289.9590\u001b[0m       62.2384  0.0153\n",
      "      4      \u001b[36m287.0759\u001b[0m       61.0914  0.0155\n",
      "      5      290.4034       \u001b[32m58.4855\u001b[0m  0.0182\n",
      "      6      \u001b[36m276.8840\u001b[0m       \u001b[32m56.5361\u001b[0m  0.0166\n",
      "      7      281.2873       \u001b[32m55.2650\u001b[0m  0.0161\n",
      "      8      280.6227       \u001b[32m54.8571\u001b[0m  0.0156\n",
      "      9      \u001b[36m274.8106\u001b[0m       \u001b[32m54.7452\u001b[0m  0.0176\n",
      "     10      \u001b[36m271.1913\u001b[0m       \u001b[32m54.7229\u001b[0m  0.0160\n",
      "y_train breslow final [-1.000e+00 -3.000e+00 -3.000e+00 -3.000e+00 -3.000e+00 -3.000e+00\n",
      " -3.000e+00 -3.000e+00 -4.000e+00 -4.000e+00 -4.000e+00 -5.000e+00\n",
      " -7.000e+00 -7.000e+00 -7.000e+00 -7.000e+00 -7.000e+00  7.000e+00\n",
      " -8.000e+00 -1.000e+01 -1.400e+01 -1.500e+01 -1.700e+01  2.300e+01\n",
      " -2.300e+01  3.700e+01 -5.000e+01 -5.500e+01 -5.800e+01 -6.200e+01\n",
      " -6.200e+01 -6.300e+01 -7.000e+01 -7.200e+01 -7.200e+01 -7.300e+01\n",
      " -7.600e+01 -7.700e+01 -8.200e+01 -8.400e+01 -9.000e+01  9.600e+01\n",
      " -1.050e+02 -1.120e+02  1.130e+02 -1.140e+02 -1.220e+02 -1.340e+02\n",
      " -1.390e+02 -1.530e+02  1.550e+02 -1.620e+02  1.620e+02 -1.690e+02\n",
      " -1.740e+02 -1.790e+02 -1.820e+02 -1.840e+02 -1.940e+02  1.940e+02\n",
      "  1.990e+02 -1.990e+02 -2.030e+02 -2.040e+02  2.050e+02 -2.070e+02\n",
      " -2.100e+02  2.140e+02  2.280e+02 -2.310e+02  2.340e+02 -2.420e+02\n",
      "  2.420e+02 -2.420e+02 -2.430e+02  2.450e+02 -2.490e+02 -2.570e+02\n",
      "  2.690e+02 -2.740e+02 -2.790e+02 -2.860e+02 -2.870e+02 -3.010e+02\n",
      " -3.130e+02  3.150e+02  3.160e+02 -3.170e+02 -3.200e+02 -3.260e+02\n",
      " -3.280e+02 -3.330e+02 -3.420e+02  3.470e+02  3.490e+02  3.510e+02\n",
      " -3.540e+02  3.540e+02 -3.680e+02 -3.720e+02  3.720e+02 -3.720e+02\n",
      "  3.780e+02 -3.840e+02  3.880e+02 -3.950e+02  3.980e+02 -4.030e+02\n",
      " -4.050e+02 -4.070e+02 -4.080e+02 -4.110e+02 -4.140e+02 -4.140e+02\n",
      " -4.160e+02 -4.180e+02 -4.270e+02 -4.280e+02 -4.330e+02 -4.340e+02\n",
      "  4.350e+02 -4.380e+02 -4.380e+02 -4.420e+02 -4.420e+02 -4.420e+02\n",
      " -4.430e+02  4.440e+02 -4.490e+02 -4.510e+02 -4.550e+02  4.560e+02\n",
      " -4.580e+02 -4.610e+02 -4.610e+02  4.660e+02 -4.670e+02 -4.670e+02\n",
      " -4.710e+02 -4.780e+02 -4.820e+02 -4.860e+02 -4.870e+02  4.920e+02\n",
      " -4.940e+02 -4.970e+02 -5.000e+02 -5.030e+02 -5.090e+02 -5.120e+02\n",
      " -5.150e+02 -5.160e+02 -5.220e+02 -5.230e+02 -5.230e+02 -5.300e+02\n",
      " -5.310e+02 -5.320e+02 -5.330e+02 -5.330e+02  5.370e+02 -5.420e+02\n",
      " -5.440e+02  5.470e+02 -5.480e+02  5.590e+02 -5.630e+02 -5.640e+02\n",
      " -5.660e+02 -5.660e+02 -5.670e+02 -5.670e+02 -5.690e+02 -5.690e+02\n",
      " -5.710e+02 -5.710e+02 -5.730e+02 -5.760e+02  5.780e+02 -5.820e+02\n",
      " -5.840e+02  5.920e+02 -5.960e+02 -5.990e+02 -6.040e+02  6.050e+02\n",
      "  6.070e+02 -6.080e+02 -6.090e+02 -6.090e+02 -6.110e+02 -6.110e+02\n",
      " -6.230e+02 -6.270e+02 -6.330e+02 -6.380e+02  6.390e+02  6.480e+02\n",
      "  6.480e+02 -6.500e+02 -6.510e+02 -6.510e+02 -6.550e+02 -6.560e+02\n",
      " -6.710e+02  6.820e+02 -6.850e+02 -6.860e+02 -7.060e+02 -7.060e+02\n",
      "  7.090e+02 -7.140e+02 -7.150e+02 -7.210e+02 -7.240e+02  7.270e+02\n",
      " -7.350e+02 -7.360e+02 -7.380e+02  7.420e+02 -7.430e+02 -7.480e+02\n",
      " -7.540e+02  7.580e+02 -7.580e+02 -7.600e+02 -7.720e+02  7.750e+02\n",
      " -7.770e+02 -7.850e+02 -7.870e+02  7.880e+02 -7.920e+02 -7.950e+02\n",
      " -7.960e+02 -8.000e+02 -8.050e+02  8.140e+02  8.140e+02  8.190e+02\n",
      "  8.210e+02 -8.260e+02 -8.320e+02 -8.350e+02 -8.370e+02 -8.460e+02\n",
      " -8.540e+02 -8.600e+02 -8.630e+02 -8.680e+02 -8.780e+02  8.860e+02\n",
      " -9.000e+02 -9.080e+02 -9.080e+02 -9.140e+02 -9.140e+02 -9.160e+02\n",
      "  9.190e+02 -9.190e+02 -9.260e+02  9.330e+02 -9.330e+02 -9.350e+02\n",
      " -9.460e+02 -9.490e+02 -9.550e+02  9.610e+02  9.620e+02 -9.640e+02\n",
      " -9.640e+02 -9.680e+02  9.840e+02  9.870e+02 -9.920e+02 -1.004e+03\n",
      " -1.012e+03 -1.026e+03 -1.032e+03  1.033e+03 -1.040e+03 -1.058e+03\n",
      " -1.069e+03 -1.076e+03 -1.078e+03 -1.079e+03 -1.099e+03  1.106e+03\n",
      " -1.112e+03 -1.115e+03 -1.120e+03  1.120e+03 -1.127e+03 -1.130e+03\n",
      "  1.137e+03 -1.139e+03  1.152e+03 -1.173e+03 -1.189e+03 -1.191e+03\n",
      " -1.201e+03 -1.201e+03 -1.210e+03 -1.213e+03 -1.217e+03 -1.219e+03\n",
      "  1.220e+03 -1.222e+03 -1.227e+03 -1.229e+03 -1.236e+03  1.242e+03\n",
      " -1.245e+03  1.251e+03 -1.257e+03  1.262e+03 -1.279e+03 -1.294e+03\n",
      " -1.300e+03 -1.314e+03 -1.320e+03  1.335e+03  1.351e+03 -1.354e+03\n",
      " -1.359e+03 -1.382e+03 -1.387e+03 -1.393e+03 -1.397e+03 -1.401e+03\n",
      " -1.412e+03 -1.421e+03 -1.426e+03 -1.426e+03 -1.428e+03 -1.453e+03\n",
      " -1.469e+03 -1.470e+03  1.481e+03  1.491e+03 -1.519e+03  1.525e+03\n",
      " -1.540e+03  1.547e+03 -1.553e+03 -1.568e+03  1.578e+03  1.585e+03\n",
      " -1.588e+03 -1.631e+03 -1.650e+03  1.666e+03 -1.721e+03 -1.752e+03\n",
      "  1.762e+03 -1.806e+03 -1.828e+03 -1.834e+03 -1.868e+03  1.886e+03\n",
      "  1.915e+03 -1.943e+03 -1.989e+03  2.052e+03 -2.078e+03 -2.107e+03\n",
      " -2.218e+03 -2.219e+03  2.235e+03  2.282e+03  2.286e+03 -2.289e+03\n",
      "  2.379e+03 -2.381e+03  2.433e+03 -2.493e+03 -2.585e+03 -2.650e+03\n",
      "  2.660e+03 -2.772e+03 -2.860e+03 -2.869e+03  2.875e+03 -2.893e+03\n",
      "  2.907e+03 -2.918e+03 -2.918e+03  2.988e+03 -3.000e+03 -3.013e+03\n",
      "  3.200e+03 -3.253e+03  3.470e+03  3.571e+03 -3.574e+03 -3.725e+03\n",
      " -3.733e+03 -3.761e+03  3.978e+03  4.068e+03  4.084e+03 -4.113e+03\n",
      "  4.412e+03  4.445e+03  4.695e+03 -4.752e+03  5.166e+03 -5.546e+03\n",
      " -6.423e+03]\n",
      "Concordance Index 0.7782943978226355\n",
      "Integrated Brier Score: 0.16649168382355736\n",
      "y_train breslow final [-1.000e+00 -3.000e+00 -3.000e+00 -3.000e+00 -3.000e+00 -3.000e+00\n",
      " -3.000e+00 -3.000e+00 -4.000e+00 -4.000e+00 -4.000e+00 -5.000e+00\n",
      " -7.000e+00 -7.000e+00 -7.000e+00 -7.000e+00 -7.000e+00  7.000e+00\n",
      " -8.000e+00 -1.000e+01 -1.400e+01 -1.500e+01 -1.700e+01  2.300e+01\n",
      " -2.300e+01  3.700e+01 -5.000e+01 -5.500e+01 -5.800e+01 -6.200e+01\n",
      " -6.200e+01 -6.300e+01 -7.000e+01 -7.200e+01 -7.200e+01 -7.300e+01\n",
      " -7.600e+01 -7.700e+01 -8.200e+01 -8.400e+01 -9.000e+01  9.600e+01\n",
      " -1.050e+02 -1.120e+02  1.130e+02 -1.140e+02 -1.220e+02 -1.340e+02\n",
      " -1.390e+02 -1.530e+02  1.550e+02 -1.620e+02  1.620e+02 -1.690e+02\n",
      " -1.740e+02 -1.790e+02 -1.820e+02 -1.840e+02 -1.940e+02  1.940e+02\n",
      "  1.990e+02 -1.990e+02 -2.030e+02 -2.040e+02  2.050e+02 -2.070e+02\n",
      " -2.100e+02  2.140e+02  2.280e+02 -2.310e+02  2.340e+02 -2.420e+02\n",
      "  2.420e+02 -2.420e+02 -2.430e+02  2.450e+02 -2.490e+02 -2.570e+02\n",
      "  2.690e+02 -2.740e+02 -2.790e+02 -2.860e+02 -2.870e+02 -3.010e+02\n",
      " -3.130e+02  3.150e+02  3.160e+02 -3.170e+02 -3.200e+02 -3.260e+02\n",
      " -3.280e+02 -3.330e+02 -3.420e+02  3.470e+02  3.490e+02  3.510e+02\n",
      " -3.540e+02  3.540e+02 -3.680e+02 -3.720e+02  3.720e+02 -3.720e+02\n",
      "  3.780e+02 -3.840e+02  3.880e+02 -3.950e+02  3.980e+02 -4.030e+02\n",
      " -4.050e+02 -4.070e+02 -4.080e+02 -4.110e+02 -4.140e+02 -4.140e+02\n",
      " -4.160e+02 -4.180e+02 -4.270e+02 -4.280e+02 -4.330e+02 -4.340e+02\n",
      "  4.350e+02 -4.380e+02 -4.380e+02 -4.420e+02 -4.420e+02 -4.420e+02\n",
      " -4.430e+02  4.440e+02 -4.490e+02 -4.510e+02 -4.550e+02  4.560e+02\n",
      " -4.580e+02 -4.610e+02 -4.610e+02  4.660e+02 -4.670e+02 -4.670e+02\n",
      " -4.710e+02 -4.780e+02 -4.820e+02 -4.860e+02 -4.870e+02  4.920e+02\n",
      " -4.940e+02 -4.970e+02 -5.000e+02 -5.030e+02 -5.090e+02 -5.120e+02\n",
      " -5.150e+02 -5.160e+02 -5.220e+02 -5.230e+02 -5.230e+02 -5.300e+02\n",
      " -5.310e+02 -5.320e+02 -5.330e+02 -5.330e+02  5.370e+02 -5.420e+02\n",
      " -5.440e+02  5.470e+02 -5.480e+02  5.590e+02 -5.630e+02 -5.640e+02\n",
      " -5.660e+02 -5.660e+02 -5.670e+02 -5.670e+02 -5.690e+02 -5.690e+02\n",
      " -5.710e+02 -5.710e+02 -5.730e+02 -5.760e+02  5.780e+02 -5.820e+02\n",
      " -5.840e+02  5.920e+02 -5.960e+02 -5.990e+02 -6.040e+02  6.050e+02\n",
      "  6.070e+02 -6.080e+02 -6.090e+02 -6.090e+02 -6.110e+02 -6.110e+02\n",
      " -6.230e+02 -6.270e+02 -6.330e+02 -6.380e+02  6.390e+02  6.480e+02\n",
      "  6.480e+02 -6.500e+02 -6.510e+02 -6.510e+02 -6.550e+02 -6.560e+02\n",
      " -6.710e+02  6.820e+02 -6.850e+02 -6.860e+02 -7.060e+02 -7.060e+02\n",
      "  7.090e+02 -7.140e+02 -7.150e+02 -7.210e+02 -7.240e+02  7.270e+02\n",
      " -7.350e+02 -7.360e+02 -7.380e+02  7.420e+02 -7.430e+02 -7.480e+02\n",
      " -7.540e+02  7.580e+02 -7.580e+02 -7.600e+02 -7.720e+02  7.750e+02\n",
      " -7.770e+02 -7.850e+02 -7.870e+02  7.880e+02 -7.920e+02 -7.950e+02\n",
      " -7.960e+02 -8.000e+02 -8.050e+02  8.140e+02  8.140e+02  8.190e+02\n",
      "  8.210e+02 -8.260e+02 -8.320e+02 -8.350e+02 -8.370e+02 -8.460e+02\n",
      " -8.540e+02 -8.600e+02 -8.630e+02 -8.680e+02 -8.780e+02  8.860e+02\n",
      " -9.000e+02 -9.080e+02 -9.080e+02 -9.140e+02 -9.140e+02 -9.160e+02\n",
      "  9.190e+02 -9.190e+02 -9.260e+02  9.330e+02 -9.330e+02 -9.350e+02\n",
      " -9.460e+02 -9.490e+02 -9.550e+02  9.610e+02  9.620e+02 -9.640e+02\n",
      " -9.640e+02 -9.680e+02  9.840e+02  9.870e+02 -9.920e+02 -1.004e+03\n",
      " -1.012e+03 -1.026e+03 -1.032e+03  1.033e+03 -1.040e+03 -1.058e+03\n",
      " -1.069e+03 -1.076e+03 -1.078e+03 -1.079e+03 -1.099e+03  1.106e+03\n",
      " -1.112e+03 -1.115e+03 -1.120e+03  1.120e+03 -1.127e+03 -1.130e+03\n",
      "  1.137e+03 -1.139e+03  1.152e+03 -1.173e+03 -1.189e+03 -1.191e+03\n",
      " -1.201e+03 -1.201e+03 -1.210e+03 -1.213e+03 -1.217e+03 -1.219e+03\n",
      "  1.220e+03 -1.222e+03 -1.227e+03 -1.229e+03 -1.236e+03  1.242e+03\n",
      " -1.245e+03  1.251e+03 -1.257e+03  1.262e+03 -1.279e+03 -1.294e+03\n",
      " -1.300e+03 -1.314e+03 -1.320e+03  1.335e+03  1.351e+03 -1.354e+03\n",
      " -1.359e+03 -1.382e+03 -1.387e+03 -1.393e+03 -1.397e+03 -1.401e+03\n",
      " -1.412e+03 -1.421e+03 -1.426e+03 -1.426e+03 -1.428e+03 -1.453e+03\n",
      " -1.469e+03 -1.470e+03  1.481e+03  1.491e+03 -1.519e+03  1.525e+03\n",
      " -1.540e+03  1.547e+03 -1.553e+03 -1.568e+03  1.578e+03  1.585e+03\n",
      " -1.588e+03 -1.631e+03 -1.650e+03  1.666e+03 -1.721e+03 -1.752e+03\n",
      "  1.762e+03 -1.806e+03 -1.828e+03 -1.834e+03 -1.868e+03  1.886e+03\n",
      "  1.915e+03 -1.943e+03 -1.989e+03  2.052e+03 -2.078e+03 -2.107e+03\n",
      " -2.218e+03 -2.219e+03  2.235e+03  2.282e+03  2.286e+03 -2.289e+03\n",
      "  2.379e+03 -2.381e+03  2.433e+03 -2.493e+03 -2.585e+03 -2.650e+03\n",
      "  2.660e+03 -2.772e+03 -2.860e+03 -2.869e+03  2.875e+03 -2.893e+03\n",
      "  2.907e+03 -2.918e+03 -2.918e+03  2.988e+03 -3.000e+03 -3.013e+03\n",
      "  3.200e+03 -3.253e+03  3.470e+03  3.571e+03 -3.574e+03 -3.725e+03\n",
      " -3.733e+03 -3.761e+03  3.978e+03  4.068e+03  4.084e+03 -4.113e+03\n",
      "  4.412e+03  4.445e+03  4.695e+03 -4.752e+03  5.166e+03 -5.546e+03\n",
      " -6.423e+03]\n",
      "durations 2.0 5255.0\n",
      "Concordance Index 0.7480586712683348\n",
      "Integrated Brier Score: 0.2572629896680649\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m318.3043\u001b[0m       \u001b[32m48.6001\u001b[0m  0.0247\n",
      "      2      \u001b[36m308.6640\u001b[0m       52.3736  0.0183\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m300.1771\u001b[0m       \u001b[32m56.0442\u001b[0m  0.0404\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      3      \u001b[36m300.8819\u001b[0m       53.1274  0.0180\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m320.9962\u001b[0m       \u001b[32m53.3545\u001b[0m  0.0191\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      4      301.0423       51.3230  0.0176\n",
      "      2      \u001b[36m294.0026\u001b[0m       \u001b[32m55.9455\u001b[0m  0.0392\n",
      "      2      \u001b[36m304.4691\u001b[0m       55.1059  0.0248\n",
      "      5      \u001b[36m295.7060\u001b[0m       49.3155  0.0225\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m279.0018\u001b[0m       \u001b[32m65.2798\u001b[0m  0.0280\n",
      "      3      \u001b[36m303.1859\u001b[0m       \u001b[32m51.7010\u001b[0m  0.0252\n",
      "      6      298.4922       \u001b[32m47.4330\u001b[0m  0.0226\n",
      "      2      \u001b[36m267.2525\u001b[0m       66.3924  0.0279\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      4      \u001b[36m294.1727\u001b[0m       \u001b[32m48.3022\u001b[0m  0.0222\n",
      "      7      \u001b[36m294.5839\u001b[0m       \u001b[32m46.0396\u001b[0m  0.0164\n",
      "      3      303.7996       \u001b[32m55.4102\u001b[0m  0.0579\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      3      269.4791       65.5733  0.0165\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      8      \u001b[36m291.2444\u001b[0m       \u001b[32m44.9562\u001b[0m  0.0172\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      5      \u001b[36m287.2241\u001b[0m       \u001b[32m46.0550\u001b[0m  0.0249\n",
      "      4      \u001b[36m263.9122\u001b[0m       \u001b[32m63.7554\u001b[0m  0.0223\n",
      "Re-initializing criterion.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m276.0334\u001b[0m       \u001b[32m35.3526\u001b[0m  0.0226\n",
      "      9      293.8919       \u001b[32m44.1239\u001b[0m  0.0162\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      4      \u001b[36m291.7277\u001b[0m       55.8926  0.0425\n",
      "      6      288.9971       \u001b[32m44.3019\u001b[0m  0.0190\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m278.5595\u001b[0m       \u001b[32m53.3014\u001b[0m  0.0455\n",
      "      5      \u001b[36m259.2560\u001b[0m       \u001b[32m62.2260\u001b[0m  0.0240\n",
      "     10      \u001b[36m288.2061\u001b[0m       \u001b[32m43.5424\u001b[0m  0.0167\n",
      "      2      \u001b[36m258.4037\u001b[0m       35.7972  0.0220\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m299.3241\u001b[0m       \u001b[32m66.0167\u001b[0m  0.0292\n",
      "      7      \u001b[36m284.6226\u001b[0m       \u001b[32m43.4818\u001b[0m  0.0212\n",
      "      6      \u001b[36m258.9880\u001b[0m       \u001b[32m60.2813\u001b[0m  0.0243\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      5      295.2678       57.5975  0.0445\n",
      "      8      \u001b[36m283.2364\u001b[0m       \u001b[32m42.7869\u001b[0m  0.0220\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      3      \u001b[36m249.6215\u001b[0m       38.0967  0.0365\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m306.1967\u001b[0m       \u001b[32m40.5855\u001b[0m  0.0600\n",
      "      2      \u001b[36m287.0804\u001b[0m       \u001b[32m65.8413\u001b[0m  0.0410\n",
      "      7      \u001b[36m257.9433\u001b[0m       \u001b[32m59.2558\u001b[0m  0.0299\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m270.7758\u001b[0m       \u001b[32m32.1199\u001b[0m  0.0678\n",
      "      9      285.9771       \u001b[32m42.4275\u001b[0m  0.0184\n",
      "      4      \u001b[36m241.2680\u001b[0m       40.6888  0.0187\n",
      "      2      283.5812       \u001b[32m52.4223\u001b[0m  0.0651\n",
      "      8      \u001b[36m254.1703\u001b[0m       \u001b[32m58.7081\u001b[0m  0.0187\n",
      "     10      \u001b[36m276.1175\u001b[0m       \u001b[32m42.2358\u001b[0m  0.0171\n",
      "      6      \u001b[36m286.2527\u001b[0m       59.7392  0.0395\n",
      "      5      \u001b[36m240.1893\u001b[0m       42.8161  0.0176\n",
      "      3      \u001b[36m286.2492\u001b[0m       \u001b[32m64.3098\u001b[0m  0.0284\n",
      "      2      \u001b[36m299.4616\u001b[0m       40.6225  0.0456\n",
      "      2      \u001b[36m258.2225\u001b[0m       \u001b[32m31.7223\u001b[0m  0.0338\n",
      "      9      254.4220       \u001b[32m58.4981\u001b[0m  0.0186\n",
      "      6      241.4545       42.8326  0.0161\n",
      "      4      \u001b[36m281.7458\u001b[0m       \u001b[32m62.1369\u001b[0m  0.0148\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m317.3459\u001b[0m       \u001b[32m44.8641\u001b[0m  0.0588\n",
      "     10      \u001b[36m254.0497\u001b[0m       \u001b[32m58.3565\u001b[0m  0.0158\n",
      "      7      \u001b[36m227.9968\u001b[0m       42.9678  0.0157\n",
      "      3      285.2231       55.0833  0.0455\n",
      "      5      \u001b[36m280.4817\u001b[0m       \u001b[32m59.6115\u001b[0m  0.0167\n",
      "      7      289.4154       61.9987  0.0408\n",
      "      3      324.4206       42.2192  0.0317\n",
      "      3      269.9467       32.9686  0.0424\n",
      "      8      231.9625       42.1474  0.0228\n",
      "      6      \u001b[36m275.7192\u001b[0m       \u001b[32m57.3741\u001b[0m  0.0259\n",
      "      2      \u001b[36m300.8158\u001b[0m       \u001b[32m44.8407\u001b[0m  0.0369\n",
      "      4      \u001b[36m272.1632\u001b[0m       57.9422  0.0326\n",
      "      8      \u001b[36m279.3582\u001b[0m       64.3021  0.0339\n",
      "      4      317.5638       43.7474  0.0351\n",
      "      9      231.2826       40.4613  0.0222\n",
      "      7      276.4807       \u001b[32m56.0096\u001b[0m  0.0284\n",
      "     10      228.6669       39.2486  0.0158\n",
      "Restoring best model from epoch 1.\n",
      "      4      \u001b[36m257.3812\u001b[0m       35.2035  0.0439\n",
      "      3      323.7857       \u001b[32m44.7208\u001b[0m  0.0350\n",
      "      5      276.3324       60.1487  0.0387\n",
      "      9      281.7123       66.4345  0.0346\n",
      "      8      276.5365       \u001b[32m55.2837\u001b[0m  0.0224\n",
      "      5      301.0577       46.2675  0.0396\n",
      "      9      \u001b[36m272.6441\u001b[0m       \u001b[32m55.0865\u001b[0m  0.0156\n",
      "     10      280.0192       69.1511  0.0355\n",
      "      5      264.3093       33.7252  0.0441\n",
      "Restoring best model from epoch 3.\n",
      "      6      \u001b[36m263.7979\u001b[0m       62.7408  0.0405\n",
      "     10      273.8440       55.1283  0.0180\n",
      "      4      337.5938       \u001b[32m44.4342\u001b[0m  0.0468\n",
      "Restoring best model from epoch 9.\n",
      "      6      327.1858       44.1798  0.0400\n",
      "      6      \u001b[36m249.5008\u001b[0m       34.1156  0.0324\n",
      "      7      268.9257       64.5012  0.0334\n",
      "      5      311.1432       \u001b[32m44.3221\u001b[0m  0.0320\n",
      "      7      \u001b[36m296.6220\u001b[0m       43.7255  0.0307\n",
      "      7      \u001b[36m247.5111\u001b[0m       36.0221  0.0308\n",
      "      8      266.4593       68.3748  0.0315\n",
      "      6      301.7818       45.3621  0.0319\n",
      "      8      297.5233       44.5662  0.0318\n",
      "      8      265.0013       37.6648  0.0300\n",
      "      9      \u001b[36m260.5343\u001b[0m       72.4170  0.0308\n",
      "      7      \u001b[36m298.8134\u001b[0m       47.1760  0.0315\n",
      "      9      \u001b[36m288.1953\u001b[0m       45.7194  0.0325\n",
      "      9      \u001b[36m243.8889\u001b[0m       39.0765  0.0292\n",
      "     10      261.5859       78.2393  0.0299\n",
      "Restoring best model from epoch 2.\n",
      "      8      305.4134       49.9469  0.0291\n",
      "     10      293.6507       45.9867  0.0287\n",
      "Restoring best model from epoch 1.\n",
      "     10      248.7488       39.2137  0.0283\n",
      "Restoring best model from epoch 2.\n",
      "      9      299.8306       52.7202  0.0291\n",
      "     10      301.8152       54.4930  0.0282\n",
      "Restoring best model from epoch 5.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m288.2824\u001b[0m       \u001b[32m54.8925\u001b[0m  0.0349\n",
      "      2      \u001b[36m284.5228\u001b[0m       \u001b[32m54.4504\u001b[0m  0.0299\n",
      "      3      298.4055       \u001b[32m52.8440\u001b[0m  0.0348\n",
      "      4      303.8228       53.3970  0.0274\n",
      "      5      293.0258       54.2894  0.0278\n",
      "      6      \u001b[36m261.0721\u001b[0m       54.3083  0.0448\n",
      "      7      278.3136       \u001b[32m52.0302\u001b[0m  0.0622\n",
      "      8      283.9834       \u001b[32m49.9033\u001b[0m  0.0304\n",
      "      9      268.1412       \u001b[32m49.3290\u001b[0m  0.0436\n",
      "     10      273.6880       49.5049  0.0289\n",
      "Restoring best model from epoch 9.\n",
      "y_train breslow final [-2.000e+00 -3.000e+00 -3.000e+00 -3.000e+00 -3.000e+00 -3.000e+00\n",
      " -3.000e+00 -3.000e+00 -3.000e+00 -4.000e+00 -4.000e+00 -4.000e+00\n",
      " -5.000e+00 -6.000e+00 -7.000e+00 -7.000e+00 -7.000e+00 -7.000e+00\n",
      " -7.000e+00 -8.000e+00 -1.000e+01 -1.400e+01 -1.500e+01 -1.700e+01\n",
      "  2.300e+01 -2.300e+01 -3.900e+01 -5.000e+01 -5.800e+01 -6.200e+01\n",
      " -6.200e+01 -6.300e+01 -7.000e+01 -7.200e+01 -7.200e+01 -7.400e+01\n",
      " -7.600e+01 -7.700e+01 -8.400e+01 -9.000e+01  9.600e+01 -1.050e+02\n",
      "  1.110e+02  1.130e+02 -1.140e+02 -1.220e+02 -1.340e+02 -1.530e+02\n",
      "  1.550e+02 -1.620e+02  1.620e+02 -1.660e+02 -1.690e+02 -1.740e+02\n",
      "  1.780e+02 -1.900e+02 -1.940e+02  1.940e+02 -1.990e+02 -2.040e+02\n",
      "  2.050e+02 -2.070e+02 -2.100e+02 -2.300e+02 -2.310e+02  2.400e+02\n",
      "  2.410e+02  2.420e+02 -2.420e+02 -2.430e+02  2.450e+02 -2.490e+02\n",
      " -2.570e+02  2.690e+02 -2.740e+02 -2.860e+02 -2.870e+02 -2.920e+02\n",
      "  3.150e+02  3.160e+02 -3.170e+02 -3.200e+02 -3.280e+02 -3.330e+02\n",
      " -3.360e+02 -3.370e+02 -3.430e+02  3.470e+02  3.510e+02  3.540e+02\n",
      " -3.680e+02  3.720e+02 -3.720e+02  3.780e+02 -3.840e+02  3.880e+02\n",
      " -3.950e+02  3.980e+02 -4.030e+02 -4.050e+02 -4.070e+02 -4.080e+02\n",
      " -4.140e+02 -4.160e+02 -4.170e+02 -4.190e+02 -4.270e+02 -4.280e+02\n",
      " -4.310e+02 -4.330e+02  4.350e+02  4.380e+02 -4.380e+02 -4.420e+02\n",
      " -4.420e+02 -4.430e+02  4.440e+02 -4.510e+02 -4.540e+02 -4.540e+02\n",
      " -4.550e+02 -4.550e+02  4.560e+02 -4.570e+02 -4.610e+02  4.660e+02\n",
      " -4.670e+02 -4.670e+02 -4.780e+02 -4.860e+02 -4.870e+02 -4.870e+02\n",
      " -4.910e+02  4.920e+02 -4.920e+02  4.920e+02 -4.940e+02 -4.940e+02\n",
      " -4.970e+02 -5.000e+02 -5.020e+02 -5.030e+02 -5.080e+02  5.120e+02\n",
      " -5.120e+02 -5.150e+02 -5.160e+02 -5.220e+02 -5.230e+02 -5.230e+02\n",
      " -5.260e+02 -5.300e+02  5.310e+02 -5.310e+02 -5.320e+02 -5.330e+02\n",
      " -5.330e+02  5.370e+02  5.380e+02 -5.420e+02 -5.440e+02 -5.440e+02\n",
      "  5.470e+02 -5.480e+02  5.590e+02  5.610e+02 -5.630e+02 -5.640e+02\n",
      " -5.660e+02 -5.660e+02 -5.660e+02 -5.670e+02 -5.670e+02 -5.690e+02\n",
      " -5.690e+02 -5.710e+02 -5.710e+02 -5.730e+02  5.760e+02 -5.760e+02\n",
      " -5.820e+02 -5.840e+02 -5.850e+02 -5.880e+02  5.920e+02 -5.960e+02\n",
      " -5.990e+02 -6.040e+02  6.070e+02 -6.080e+02 -6.090e+02 -6.150e+02\n",
      " -6.220e+02 -6.230e+02 -6.270e+02 -6.290e+02 -6.290e+02 -6.300e+02\n",
      " -6.330e+02 -6.380e+02  6.390e+02  6.480e+02  6.480e+02 -6.500e+02\n",
      " -6.510e+02 -6.550e+02 -6.560e+02 -6.710e+02 -6.770e+02 -6.780e+02\n",
      "  6.820e+02 -6.850e+02 -7.050e+02  7.090e+02 -7.150e+02 -7.180e+02\n",
      " -7.210e+02  7.220e+02 -7.240e+02  7.270e+02 -7.350e+02 -7.360e+02\n",
      " -7.430e+02 -7.480e+02 -7.540e+02  7.580e+02 -7.580e+02 -7.720e+02\n",
      "  7.750e+02 -7.770e+02 -7.870e+02 -7.920e+02 -8.000e+02 -8.050e+02\n",
      "  8.140e+02 -8.320e+02 -8.350e+02 -8.460e+02 -8.460e+02 -8.540e+02\n",
      " -8.540e+02 -8.600e+02 -8.620e+02 -8.630e+02 -8.780e+02  8.860e+02\n",
      " -8.890e+02 -9.000e+02 -9.080e+02 -9.080e+02 -9.080e+02 -9.140e+02\n",
      " -9.140e+02 -9.160e+02  9.190e+02 -9.190e+02 -9.260e+02  9.330e+02\n",
      " -9.330e+02 -9.390e+02 -9.460e+02 -9.490e+02  9.540e+02 -9.550e+02\n",
      " -9.560e+02  9.610e+02 -9.620e+02  9.620e+02 -9.640e+02 -9.680e+02\n",
      "  9.870e+02 -9.920e+02 -9.920e+02 -9.930e+02  1.011e+03 -1.012e+03\n",
      " -1.021e+03 -1.032e+03  1.033e+03 -1.040e+03 -1.069e+03 -1.076e+03\n",
      " -1.078e+03 -1.079e+03 -1.099e+03  1.106e+03 -1.112e+03 -1.115e+03\n",
      " -1.116e+03 -1.120e+03  1.120e+03 -1.130e+03  1.137e+03 -1.137e+03\n",
      " -1.139e+03 -1.147e+03  1.152e+03 -1.164e+03 -1.173e+03  1.183e+03\n",
      " -1.191e+03 -1.201e+03 -1.201e+03  1.209e+03 -1.210e+03 -1.213e+03\n",
      " -1.217e+03 -1.219e+03  1.220e+03 -1.222e+03 -1.227e+03  1.242e+03\n",
      " -1.245e+03 -1.250e+03  1.251e+03  1.262e+03 -1.277e+03 -1.279e+03\n",
      " -1.294e+03 -1.294e+03 -1.301e+03 -1.337e+03  1.339e+03 -1.341e+03\n",
      " -1.354e+03 -1.359e+03 -1.382e+03 -1.387e+03 -1.393e+03 -1.399e+03\n",
      "  1.401e+03 -1.401e+03 -1.412e+03 -1.421e+03 -1.426e+03 -1.426e+03\n",
      " -1.428e+03 -1.453e+03 -1.458e+03 -1.469e+03 -1.470e+03  1.481e+03\n",
      "  1.491e+03 -1.494e+03 -1.500e+03 -1.519e+03  1.525e+03  1.547e+03\n",
      " -1.553e+03 -1.567e+03 -1.568e+03  1.578e+03  1.585e+03 -1.650e+03\n",
      " -1.706e+03  1.762e+03 -1.796e+03 -1.806e+03 -1.828e+03 -1.834e+03\n",
      " -1.868e+03  1.886e+03  1.891e+03  1.915e+03  1.933e+03 -1.989e+03\n",
      "  2.000e+03  2.052e+03 -2.078e+03 -2.107e+03 -2.218e+03  2.235e+03\n",
      "  2.286e+03 -2.287e+03 -2.289e+03 -2.289e+03  2.433e+03 -2.493e+03\n",
      " -2.565e+03 -2.585e+03 -2.602e+03  2.660e+03 -2.702e+03 -2.761e+03\n",
      " -2.772e+03  2.835e+03 -2.860e+03 -2.869e+03  2.907e+03 -2.918e+03\n",
      " -3.000e+03  3.200e+03 -3.253e+03  3.470e+03  3.571e+03 -3.574e+03\n",
      " -3.733e+03 -3.761e+03  3.978e+03  4.068e+03  4.084e+03 -4.113e+03\n",
      "  4.229e+03  4.445e+03  4.695e+03 -4.752e+03  5.166e+03 -5.255e+03\n",
      " -6.423e+03]\n",
      "Concordance Index 0.791435682769544\n",
      "Integrated Brier Score: 0.24657128405266168\n",
      "y_train breslow final [-2.000e+00 -3.000e+00 -3.000e+00 -3.000e+00 -3.000e+00 -3.000e+00\n",
      " -3.000e+00 -3.000e+00 -3.000e+00 -4.000e+00 -4.000e+00 -4.000e+00\n",
      " -5.000e+00 -6.000e+00 -7.000e+00 -7.000e+00 -7.000e+00 -7.000e+00\n",
      " -7.000e+00 -8.000e+00 -1.000e+01 -1.400e+01 -1.500e+01 -1.700e+01\n",
      "  2.300e+01 -2.300e+01 -3.900e+01 -5.000e+01 -5.800e+01 -6.200e+01\n",
      " -6.200e+01 -6.300e+01 -7.000e+01 -7.200e+01 -7.200e+01 -7.400e+01\n",
      " -7.600e+01 -7.700e+01 -8.400e+01 -9.000e+01  9.600e+01 -1.050e+02\n",
      "  1.110e+02  1.130e+02 -1.140e+02 -1.220e+02 -1.340e+02 -1.530e+02\n",
      "  1.550e+02 -1.620e+02  1.620e+02 -1.660e+02 -1.690e+02 -1.740e+02\n",
      "  1.780e+02 -1.900e+02 -1.940e+02  1.940e+02 -1.990e+02 -2.040e+02\n",
      "  2.050e+02 -2.070e+02 -2.100e+02 -2.300e+02 -2.310e+02  2.400e+02\n",
      "  2.410e+02  2.420e+02 -2.420e+02 -2.430e+02  2.450e+02 -2.490e+02\n",
      " -2.570e+02  2.690e+02 -2.740e+02 -2.860e+02 -2.870e+02 -2.920e+02\n",
      "  3.150e+02  3.160e+02 -3.170e+02 -3.200e+02 -3.280e+02 -3.330e+02\n",
      " -3.360e+02 -3.370e+02 -3.430e+02  3.470e+02  3.510e+02  3.540e+02\n",
      " -3.680e+02  3.720e+02 -3.720e+02  3.780e+02 -3.840e+02  3.880e+02\n",
      " -3.950e+02  3.980e+02 -4.030e+02 -4.050e+02 -4.070e+02 -4.080e+02\n",
      " -4.140e+02 -4.160e+02 -4.170e+02 -4.190e+02 -4.270e+02 -4.280e+02\n",
      " -4.310e+02 -4.330e+02  4.350e+02  4.380e+02 -4.380e+02 -4.420e+02\n",
      " -4.420e+02 -4.430e+02  4.440e+02 -4.510e+02 -4.540e+02 -4.540e+02\n",
      " -4.550e+02 -4.550e+02  4.560e+02 -4.570e+02 -4.610e+02  4.660e+02\n",
      " -4.670e+02 -4.670e+02 -4.780e+02 -4.860e+02 -4.870e+02 -4.870e+02\n",
      " -4.910e+02  4.920e+02 -4.920e+02  4.920e+02 -4.940e+02 -4.940e+02\n",
      " -4.970e+02 -5.000e+02 -5.020e+02 -5.030e+02 -5.080e+02  5.120e+02\n",
      " -5.120e+02 -5.150e+02 -5.160e+02 -5.220e+02 -5.230e+02 -5.230e+02\n",
      " -5.260e+02 -5.300e+02  5.310e+02 -5.310e+02 -5.320e+02 -5.330e+02\n",
      " -5.330e+02  5.370e+02  5.380e+02 -5.420e+02 -5.440e+02 -5.440e+02\n",
      "  5.470e+02 -5.480e+02  5.590e+02  5.610e+02 -5.630e+02 -5.640e+02\n",
      " -5.660e+02 -5.660e+02 -5.660e+02 -5.670e+02 -5.670e+02 -5.690e+02\n",
      " -5.690e+02 -5.710e+02 -5.710e+02 -5.730e+02  5.760e+02 -5.760e+02\n",
      " -5.820e+02 -5.840e+02 -5.850e+02 -5.880e+02  5.920e+02 -5.960e+02\n",
      " -5.990e+02 -6.040e+02  6.070e+02 -6.080e+02 -6.090e+02 -6.150e+02\n",
      " -6.220e+02 -6.230e+02 -6.270e+02 -6.290e+02 -6.290e+02 -6.300e+02\n",
      " -6.330e+02 -6.380e+02  6.390e+02  6.480e+02  6.480e+02 -6.500e+02\n",
      " -6.510e+02 -6.550e+02 -6.560e+02 -6.710e+02 -6.770e+02 -6.780e+02\n",
      "  6.820e+02 -6.850e+02 -7.050e+02  7.090e+02 -7.150e+02 -7.180e+02\n",
      " -7.210e+02  7.220e+02 -7.240e+02  7.270e+02 -7.350e+02 -7.360e+02\n",
      " -7.430e+02 -7.480e+02 -7.540e+02  7.580e+02 -7.580e+02 -7.720e+02\n",
      "  7.750e+02 -7.770e+02 -7.870e+02 -7.920e+02 -8.000e+02 -8.050e+02\n",
      "  8.140e+02 -8.320e+02 -8.350e+02 -8.460e+02 -8.460e+02 -8.540e+02\n",
      " -8.540e+02 -8.600e+02 -8.620e+02 -8.630e+02 -8.780e+02  8.860e+02\n",
      " -8.890e+02 -9.000e+02 -9.080e+02 -9.080e+02 -9.080e+02 -9.140e+02\n",
      " -9.140e+02 -9.160e+02  9.190e+02 -9.190e+02 -9.260e+02  9.330e+02\n",
      " -9.330e+02 -9.390e+02 -9.460e+02 -9.490e+02  9.540e+02 -9.550e+02\n",
      " -9.560e+02  9.610e+02 -9.620e+02  9.620e+02 -9.640e+02 -9.680e+02\n",
      "  9.870e+02 -9.920e+02 -9.920e+02 -9.930e+02  1.011e+03 -1.012e+03\n",
      " -1.021e+03 -1.032e+03  1.033e+03 -1.040e+03 -1.069e+03 -1.076e+03\n",
      " -1.078e+03 -1.079e+03 -1.099e+03  1.106e+03 -1.112e+03 -1.115e+03\n",
      " -1.116e+03 -1.120e+03  1.120e+03 -1.130e+03  1.137e+03 -1.137e+03\n",
      " -1.139e+03 -1.147e+03  1.152e+03 -1.164e+03 -1.173e+03  1.183e+03\n",
      " -1.191e+03 -1.201e+03 -1.201e+03  1.209e+03 -1.210e+03 -1.213e+03\n",
      " -1.217e+03 -1.219e+03  1.220e+03 -1.222e+03 -1.227e+03  1.242e+03\n",
      " -1.245e+03 -1.250e+03  1.251e+03  1.262e+03 -1.277e+03 -1.279e+03\n",
      " -1.294e+03 -1.294e+03 -1.301e+03 -1.337e+03  1.339e+03 -1.341e+03\n",
      " -1.354e+03 -1.359e+03 -1.382e+03 -1.387e+03 -1.393e+03 -1.399e+03\n",
      "  1.401e+03 -1.401e+03 -1.412e+03 -1.421e+03 -1.426e+03 -1.426e+03\n",
      " -1.428e+03 -1.453e+03 -1.458e+03 -1.469e+03 -1.470e+03  1.481e+03\n",
      "  1.491e+03 -1.494e+03 -1.500e+03 -1.519e+03  1.525e+03  1.547e+03\n",
      " -1.553e+03 -1.567e+03 -1.568e+03  1.578e+03  1.585e+03 -1.650e+03\n",
      " -1.706e+03  1.762e+03 -1.796e+03 -1.806e+03 -1.828e+03 -1.834e+03\n",
      " -1.868e+03  1.886e+03  1.891e+03  1.915e+03  1.933e+03 -1.989e+03\n",
      "  2.000e+03  2.052e+03 -2.078e+03 -2.107e+03 -2.218e+03  2.235e+03\n",
      "  2.286e+03 -2.287e+03 -2.289e+03 -2.289e+03  2.433e+03 -2.493e+03\n",
      " -2.565e+03 -2.585e+03 -2.602e+03  2.660e+03 -2.702e+03 -2.761e+03\n",
      " -2.772e+03  2.835e+03 -2.860e+03 -2.869e+03  2.907e+03 -2.918e+03\n",
      " -3.000e+03  3.200e+03 -3.253e+03  3.470e+03  3.571e+03 -3.574e+03\n",
      " -3.733e+03 -3.761e+03  3.978e+03  4.068e+03  4.084e+03 -4.113e+03\n",
      "  4.229e+03  4.445e+03  4.695e+03 -4.752e+03  5.166e+03 -5.255e+03\n",
      " -6.423e+03]\n",
      "durations 1.0 5546.0\n",
      "Concordance Index 0.7370689655172413\n",
      "Integrated Brier Score: 0.19089521928669276\n",
      "split gex_?|100130426      float32\n",
      "gex_?|100133144      float32\n",
      "gex_?|100134869      float32\n",
      "gex_?|10357          float32\n",
      "gex_?|10431          float32\n",
      "                      ...   \n",
      "gex_ZYG11A|440590    float32\n",
      "gex_ZYG11B|79699     float32\n",
      "gex_ZYX|7791         float32\n",
      "gex_ZZEF1|23140      float32\n",
      "gex_ZZZ3|26009       float32\n",
      "Length: 20531, dtype: object\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m108.6649\u001b[0m  0.0031\n",
      "      2      108.6649  0.0020\n",
      "      3      108.6649  0.0015\n",
      "      4      108.6649  0.0018\n",
      "      5      108.6649  0.0016\n",
      "      6      108.6649  0.0019\n",
      "      7      108.6649  0.0017\n",
      "      8      108.6649  0.0046\n",
      "      9      108.6649  0.0024\n",
      "     10      108.6649  0.0028\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m227.0142\u001b[0m  0.0029\n",
      "      2      227.0142  0.0017\n",
      "      3      227.0142  0.0014\n",
      "      4      227.0142  0.0013\n",
      "      5      227.0142  0.0013\n",
      "      6      227.0142  0.0013\n",
      "      7      227.0142  0.0012\n",
      "      8      227.0142  0.0034\n",
      "      9      227.0142  0.0013\n",
      "     10      227.0142  0.0014\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m43.2787\u001b[0m  0.0031\n",
      "      2       43.2787  0.0023\n",
      "      3       43.2787  0.0021\n",
      "      4       43.2787  0.0021\n",
      "      5       43.2787  0.0020\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      6       43.2787  0.0022\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m134.9939\u001b[0m  0.0024\n",
      "      7       43.2787  0.0035\n",
      "      2      134.9939  0.0014\n",
      "      3      134.9939  0.0015\n",
      "      8       43.2787  0.0022\n",
      "      4      134.9939  0.0013\n",
      "      9       43.2787  0.0023\n",
      "      5      134.9939  0.0013\n",
      "      6      134.9939  0.0014\n",
      "     10       43.2787  0.0022\n",
      "      7      134.9939  0.0013\n",
      "Restoring best model from epoch 1.\n",
      "      8      134.9939  0.0013\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.      9      134.9939  0.0016\n",
      "\n",
      "     10      134.9939  0.0016\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m38.9687\u001b[0m  0.0078\n",
      "      2       38.9687  0.0057\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m39.1040\u001b[0m  0.0072\n",
      "      3       38.9687  0.0028\n",
      "      2       39.1040  0.0037\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m104.0364\u001b[0m  0.0101\n",
      "      4       38.9687  0.0037\n",
      "      2      104.0364  0.0026\n",
      "      3       39.1040  0.0028\n",
      "      5       38.9687  0.0032\n",
      "      3      104.0364  0.0021\n",
      "      4       39.1040  0.0024\n",
      "      4      104.0364  0.0017\n",
      "      6       38.9687  0.0026\n",
      "      5      104.0364  0.0019\n",
      "      5       39.1040  0.0034\n",
      "      7       38.9687  0.0020\n",
      "      6      104.0364  0.0027\n",
      "      6       39.1040  0.0028\n",
      "      8       38.9687  0.0037\n",
      "      7      104.0364  0.0013\n",
      "      7       39.1040  0.0029\n",
      "      8      104.0364  0.0018\n",
      "      9       38.9687  0.0027\n",
      "      8       39.1040  0.0021\n",
      "      9      104.0364  0.0018\n",
      "     10       38.9687  0.0025\n",
      "      9       39.1040  0.0020\n",
      "Restoring best model from epoch 1.\n",
      "     10      104.0364  0.0023\n",
      "Restoring best model from epoch 1.\n",
      "     10       39.1040  0.0020\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m105.6685\u001b[0m  0.0028\n",
      "      2      105.6685  0.0023\n",
      "      3      105.6685  0.0016\n",
      "      4      105.6685  0.0016\n",
      "      5      105.6685  0.0016\n",
      "      6      105.6685  0.0014\n",
      "      7      105.6685  0.0013\n",
      "      8      105.6685  0.0013\n",
      "      9      105.6685  0.0012\n",
      "     10      105.6685  0.0012\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m43.1511\u001b[0m  0.0095\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m28.1560\u001b[0m  0.0088\n",
      "      2       43.1511  0.0032\n",
      "      2       28.1560  0.0026\n",
      "      3       43.1511  0.0031\n",
      "      3       28.1560  0.0029\n",
      "      4       43.1511  0.0027\n",
      "      4       28.1560  0.0025\n",
      "      5       43.1511  0.0023\n",
      "      5       28.1560  0.0021\n",
      "      6       43.1511  0.0021\n",
      "      6       28.1560  0.0020\n",
      "      7       43.1511  0.0020\n",
      "      7       28.1560  0.0024\n",
      "      8       28.1560  0.0021\n",
      "      8       43.1511  0.0030\n",
      "      9       28.1560  0.0019\n",
      "      9       43.1511  0.0023\n",
      "     10       28.1560  0.0019\n",
      "Restoring best model from epoch 1.\n",
      "     10       43.1511  0.0021\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m72.0151\u001b[0m  0.0035\n",
      "      2       72.0151  0.0034\n",
      "      3       72.0151  0.0031\n",
      "      4       72.0151  0.0027\n",
      "      5       72.0151  0.0028\n",
      "      6       72.0151  0.0030\n",
      "      7       72.0151  0.0128\n",
      "      8       72.0151  0.0053\n",
      "      9       72.0151  0.0030\n",
      "     10       72.0151  0.0037\n",
      "Restoring best model from epoch 1.\n",
      "y_train breslow final [-1.000e+00 -6.000e+00 -6.000e+00 -8.000e+00 -9.000e+00 -9.000e+00\n",
      "  9.000e+00 -1.000e+01  1.100e+01  1.200e+01 -1.200e+01  1.400e+01\n",
      "  1.600e+01 -2.000e+01 -2.000e+01 -2.100e+01 -2.300e+01  2.700e+01\n",
      " -3.000e+01  3.100e+01  3.400e+01  4.600e+01  5.600e+01  5.600e+01\n",
      "  6.500e+01  6.500e+01  6.700e+01 -7.900e+01  8.700e+01  9.100e+01\n",
      " -9.400e+01  9.700e+01  1.010e+02  1.020e+02  1.030e+02  1.070e+02\n",
      "  1.150e+02  1.290e+02  1.290e+02  1.400e+02 -1.410e+02  1.530e+02\n",
      "  1.710e+02  1.710e+02 -1.800e+02 -1.830e+02  1.940e+02 -2.110e+02\n",
      "  2.140e+02  2.170e+02  2.230e+02 -2.290e+02  2.330e+02  2.470e+02\n",
      " -2.480e+02 -2.500e+02  2.620e+02  2.780e+02  2.790e+02  2.830e+02\n",
      " -2.910e+02  2.960e+02  2.990e+02  3.000e+02  3.000e+02 -3.030e+02\n",
      "  3.030e+02  3.040e+02  3.080e+02  3.150e+02 -3.220e+02 -3.270e+02\n",
      " -3.280e+02 -3.300e+02  3.440e+02 -3.450e+02 -3.470e+02  3.470e+02\n",
      "  3.490e+02 -3.540e+02 -3.570e+02 -3.590e+02  3.590e+02 -3.610e+02\n",
      " -3.610e+02 -3.630e+02 -3.630e+02 -3.650e+02  3.650e+02  3.660e+02\n",
      " -3.660e+02 -3.720e+02  3.810e+02 -3.820e+02 -3.870e+02 -3.870e+02\n",
      " -3.900e+02  3.940e+02 -3.950e+02 -3.950e+02 -3.960e+02 -3.990e+02\n",
      " -4.060e+02 -4.060e+02 -4.080e+02 -4.090e+02  4.100e+02  4.120e+02\n",
      " -4.120e+02  4.150e+02 -4.150e+02  4.190e+02 -4.230e+02 -4.250e+02\n",
      "  4.250e+02 -4.270e+02 -4.300e+02  4.380e+02 -4.440e+02 -4.470e+02\n",
      " -4.490e+02  4.520e+02 -4.580e+02 -4.680e+02 -4.720e+02 -4.740e+02\n",
      " -4.760e+02 -4.780e+02 -4.800e+02 -4.860e+02 -5.200e+02 -5.380e+02\n",
      "  5.470e+02 -5.520e+02 -5.540e+02 -5.550e+02 -5.550e+02  5.550e+02\n",
      "  5.560e+02  5.580e+02 -5.620e+02 -5.660e+02 -5.700e+02 -5.740e+02\n",
      " -5.750e+02  5.810e+02 -5.850e+02 -5.870e+02 -5.880e+02  5.960e+02\n",
      "  6.010e+02 -6.020e+02 -6.080e+02  6.120e+02 -6.150e+02 -6.210e+02\n",
      "  6.270e+02 -6.300e+02 -6.310e+02 -6.320e+02 -6.360e+02 -6.380e+02\n",
      " -6.400e+02  6.430e+02  6.490e+02 -6.550e+02 -6.580e+02  6.600e+02\n",
      " -6.710e+02 -6.720e+02 -6.730e+02  6.880e+02  6.930e+02 -6.970e+02\n",
      " -6.980e+02 -7.060e+02  7.110e+02 -7.220e+02  7.240e+02 -7.290e+02\n",
      " -7.440e+02 -7.470e+02 -7.560e+02  7.570e+02 -7.630e+02 -7.800e+02\n",
      " -7.820e+02 -7.840e+02  8.020e+02 -8.100e+02 -8.190e+02  8.270e+02\n",
      "  8.370e+02 -8.480e+02 -8.480e+02 -8.490e+02 -8.600e+02 -8.790e+02\n",
      " -8.980e+02 -8.980e+02  8.990e+02 -9.100e+02 -9.170e+02 -9.280e+02\n",
      "  9.310e+02 -9.420e+02 -9.510e+02 -9.890e+02  1.005e+03 -1.008e+03\n",
      " -1.030e+03 -1.049e+03 -1.066e+03 -1.067e+03 -1.091e+03 -1.098e+03\n",
      " -1.115e+03  1.135e+03 -1.145e+03  1.147e+03 -1.168e+03  1.210e+03\n",
      " -1.219e+03  1.229e+03 -1.241e+03 -1.242e+03 -1.295e+03 -1.339e+03\n",
      " -1.345e+03 -1.351e+03 -1.363e+03  1.386e+03  1.397e+03  1.423e+03\n",
      " -1.424e+03 -1.450e+03 -1.452e+03  1.490e+03 -1.495e+03 -1.516e+03\n",
      " -1.531e+03 -1.531e+03 -1.553e+03 -1.562e+03 -1.567e+03 -1.618e+03\n",
      "  1.622e+03  1.624e+03 -1.633e+03 -1.636e+03  1.685e+03  1.694e+03\n",
      " -1.711e+03 -1.718e+03 -1.731e+03 -1.804e+03 -1.823e+03 -1.855e+03\n",
      " -1.876e+03 -1.900e+03 -1.939e+03 -1.970e+03 -2.015e+03 -2.017e+03\n",
      " -2.102e+03  2.116e+03  2.131e+03 -2.184e+03 -2.202e+03 -2.245e+03\n",
      " -2.301e+03 -2.301e+03 -2.317e+03 -2.324e+03 -2.398e+03 -2.412e+03\n",
      " -2.415e+03 -2.425e+03 -2.442e+03 -2.455e+03  2.456e+03  2.486e+03\n",
      "  2.532e+03  2.542e+03 -2.728e+03 -2.746e+03 -2.752e+03  3.125e+03\n",
      "  3.258e+03 -3.437e+03 -3.478e+03 -3.675e+03]\n",
      "Concordance Index 0.42395982783357244\n",
      "Integrated Brier Score: 0.2138522210894207\n",
      "y_train breslow final [-1.000e+00 -6.000e+00 -6.000e+00 -8.000e+00 -9.000e+00 -9.000e+00\n",
      "  9.000e+00 -1.000e+01  1.100e+01  1.200e+01 -1.200e+01  1.400e+01\n",
      "  1.600e+01 -2.000e+01 -2.000e+01 -2.100e+01 -2.300e+01  2.700e+01\n",
      " -3.000e+01  3.100e+01  3.400e+01  4.600e+01  5.600e+01  5.600e+01\n",
      "  6.500e+01  6.500e+01  6.700e+01 -7.900e+01  8.700e+01  9.100e+01\n",
      " -9.400e+01  9.700e+01  1.010e+02  1.020e+02  1.030e+02  1.070e+02\n",
      "  1.150e+02  1.290e+02  1.290e+02  1.400e+02 -1.410e+02  1.530e+02\n",
      "  1.710e+02  1.710e+02 -1.800e+02 -1.830e+02  1.940e+02 -2.110e+02\n",
      "  2.140e+02  2.170e+02  2.230e+02 -2.290e+02  2.330e+02  2.470e+02\n",
      " -2.480e+02 -2.500e+02  2.620e+02  2.780e+02  2.790e+02  2.830e+02\n",
      " -2.910e+02  2.960e+02  2.990e+02  3.000e+02  3.000e+02 -3.030e+02\n",
      "  3.030e+02  3.040e+02  3.080e+02  3.150e+02 -3.220e+02 -3.270e+02\n",
      " -3.280e+02 -3.300e+02  3.440e+02 -3.450e+02 -3.470e+02  3.470e+02\n",
      "  3.490e+02 -3.540e+02 -3.570e+02 -3.590e+02  3.590e+02 -3.610e+02\n",
      " -3.610e+02 -3.630e+02 -3.630e+02 -3.650e+02  3.650e+02  3.660e+02\n",
      " -3.660e+02 -3.720e+02  3.810e+02 -3.820e+02 -3.870e+02 -3.870e+02\n",
      " -3.900e+02  3.940e+02 -3.950e+02 -3.950e+02 -3.960e+02 -3.990e+02\n",
      " -4.060e+02 -4.060e+02 -4.080e+02 -4.090e+02  4.100e+02  4.120e+02\n",
      " -4.120e+02  4.150e+02 -4.150e+02  4.190e+02 -4.230e+02 -4.250e+02\n",
      "  4.250e+02 -4.270e+02 -4.300e+02  4.380e+02 -4.440e+02 -4.470e+02\n",
      " -4.490e+02  4.520e+02 -4.580e+02 -4.680e+02 -4.720e+02 -4.740e+02\n",
      " -4.760e+02 -4.780e+02 -4.800e+02 -4.860e+02 -5.200e+02 -5.380e+02\n",
      "  5.470e+02 -5.520e+02 -5.540e+02 -5.550e+02 -5.550e+02  5.550e+02\n",
      "  5.560e+02  5.580e+02 -5.620e+02 -5.660e+02 -5.700e+02 -5.740e+02\n",
      " -5.750e+02  5.810e+02 -5.850e+02 -5.870e+02 -5.880e+02  5.960e+02\n",
      "  6.010e+02 -6.020e+02 -6.080e+02  6.120e+02 -6.150e+02 -6.210e+02\n",
      "  6.270e+02 -6.300e+02 -6.310e+02 -6.320e+02 -6.360e+02 -6.380e+02\n",
      " -6.400e+02  6.430e+02  6.490e+02 -6.550e+02 -6.580e+02  6.600e+02\n",
      " -6.710e+02 -6.720e+02 -6.730e+02  6.880e+02  6.930e+02 -6.970e+02\n",
      " -6.980e+02 -7.060e+02  7.110e+02 -7.220e+02  7.240e+02 -7.290e+02\n",
      " -7.440e+02 -7.470e+02 -7.560e+02  7.570e+02 -7.630e+02 -7.800e+02\n",
      " -7.820e+02 -7.840e+02  8.020e+02 -8.100e+02 -8.190e+02  8.270e+02\n",
      "  8.370e+02 -8.480e+02 -8.480e+02 -8.490e+02 -8.600e+02 -8.790e+02\n",
      " -8.980e+02 -8.980e+02  8.990e+02 -9.100e+02 -9.170e+02 -9.280e+02\n",
      "  9.310e+02 -9.420e+02 -9.510e+02 -9.890e+02  1.005e+03 -1.008e+03\n",
      " -1.030e+03 -1.049e+03 -1.066e+03 -1.067e+03 -1.091e+03 -1.098e+03\n",
      " -1.115e+03  1.135e+03 -1.145e+03  1.147e+03 -1.168e+03  1.210e+03\n",
      " -1.219e+03  1.229e+03 -1.241e+03 -1.242e+03 -1.295e+03 -1.339e+03\n",
      " -1.345e+03 -1.351e+03 -1.363e+03  1.386e+03  1.397e+03  1.423e+03\n",
      " -1.424e+03 -1.450e+03 -1.452e+03  1.490e+03 -1.495e+03 -1.516e+03\n",
      " -1.531e+03 -1.531e+03 -1.553e+03 -1.562e+03 -1.567e+03 -1.618e+03\n",
      "  1.622e+03  1.624e+03 -1.633e+03 -1.636e+03  1.685e+03  1.694e+03\n",
      " -1.711e+03 -1.718e+03 -1.731e+03 -1.804e+03 -1.823e+03 -1.855e+03\n",
      " -1.876e+03 -1.900e+03 -1.939e+03 -1.970e+03 -2.015e+03 -2.017e+03\n",
      " -2.102e+03  2.116e+03  2.131e+03 -2.184e+03 -2.202e+03 -2.245e+03\n",
      " -2.301e+03 -2.301e+03 -2.317e+03 -2.324e+03 -2.398e+03 -2.412e+03\n",
      " -2.415e+03 -2.425e+03 -2.442e+03 -2.455e+03  2.456e+03  2.486e+03\n",
      "  2.532e+03  2.542e+03 -2.728e+03 -2.746e+03 -2.752e+03  3.125e+03\n",
      "  3.258e+03 -3.437e+03 -3.478e+03 -3.675e+03]\n",
      "durations 6.0 3308.0\n",
      "Concordance Index 0.40293809024134314\n",
      "Integrated Brier Score: 0.23239838944936206\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m94.2737\u001b[0m       \u001b[32m61.1214\u001b[0m  0.0282\n",
      "      2      103.8769       72.2923  0.0213\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m76.1220\u001b[0m  0.0019\n",
      "      3       95.9607       82.2049  0.0190\n",
      "      2       76.1220  0.0015\n",
      "      3       76.1220  0.0015\n",
      "      4       76.1220  0.0015\n",
      "      5       76.1220  0.0015\n",
      "      6       76.1220  0.0015\n",
      "      7       76.1220  0.0016\n",
      "      8       76.1220  0.0015\n",
      "      9       76.1220  0.0015\n",
      "     10       76.1220  0.0015\n",
      "Restoring best model from epoch 1.\n",
      "      4       \u001b[36m91.2805\u001b[0m       70.9381  0.0194\n",
      "      5       \u001b[36m87.0519\u001b[0m       63.9872  0.0195\n",
      "      6       \u001b[36m84.5073\u001b[0m       67.6952  0.0241\n",
      "      7       88.8223       69.9458  0.0218\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      8       \u001b[36m82.1128\u001b[0m       76.3580  0.0238\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m50.6735\u001b[0m  0.0030\n",
      "      2       50.6735  0.0018\n",
      "      9       \u001b[36m79.6457\u001b[0m       79.6250  0.0229\n",
      "      3       50.6735  0.0016\n",
      "      4       50.6735  0.0016\n",
      "      5       50.6735  0.0015\n",
      "      6       50.6735  0.0017\n",
      "      7       50.6735  0.0016\n",
      "      8       50.6735  0.0016\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m96.4630\u001b[0m       \u001b[32m64.7343\u001b[0m  0.0355\n",
      "      9       50.6735  0.0015\n",
      "     10       50.6735  0.0082\n",
      "Restoring best model from epoch 1.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m85.3198\u001b[0m       \u001b[32m37.6900\u001b[0m  0.0234\n",
      "     10       80.7385       81.2338  0.0218\n",
      "Restoring best model from epoch 1.\n",
      "      2      105.1557       67.1533  0.0286\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2       92.8787       56.4595  0.0709\n",
      "      3       \u001b[36m93.6251\u001b[0m       66.7123  0.0539\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m79.8812\u001b[0m  0.0068\n",
      "      2       79.8812  0.0044\n",
      "      3       79.8812  0.0030\n",
      "      4       79.8812  0.0026\n",
      "      5       79.8812  0.0018\n",
      "      6       79.8812  0.0029\n",
      "      7       79.8812  0.0038\n",
      "      8       79.8812  0.0032\n",
      "      9       79.8812  0.0026\n",
      "     10       79.8812  0.0034\n",
      "Restoring best model from epoch 1.\n",
      "      4       \u001b[36m86.7165\u001b[0m       69.2996  0.0481\n",
      "      3       \u001b[36m82.7526\u001b[0m       56.3737  0.0513\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m93.8668\u001b[0m       \u001b[32m65.8543\u001b[0m  0.0689\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m103.8310\u001b[0m  0.0032\n",
      "      2      103.8310  0.0024\n",
      "      3      103.8310  0.0018\n",
      "      4      103.8310  0.0032\n",
      "      5       \u001b[36m82.0666\u001b[0m       65.0222  0.0293\n",
      "      4       84.2267       44.8853  0.0312\n",
      "      5      103.8310  0.0071\n",
      "      6      103.8310  0.0022\n",
      "      7      103.8310  0.0040\n",
      "      8      103.8310  0.0031\n",
      "      9      103.8310  0.0019\n",
      "     10      103.8310  0.0036\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      2       \u001b[36m93.7988\u001b[0m       \u001b[32m64.7265\u001b[0m  0.0357\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      5       \u001b[36m73.3532\u001b[0m       52.3148  0.0244\n",
      "      6       \u001b[36m77.3634\u001b[0m       69.0879  0.0292\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m91.4503\u001b[0m  0.0044\n",
      "      2       91.4503  0.0037\n",
      "      3       91.4503  0.0040\n",
      "      4       91.4503  0.0057\n",
      "      5       91.4503  0.0026\n",
      "      3       97.9326       83.8345  0.0311\n",
      "      6       \u001b[36m72.7204\u001b[0m       58.4083  0.0290\n",
      "      6       91.4503  0.0037\n",
      "      7       91.4503  0.0020\n",
      "      7       78.6516       73.4796  0.0319\n",
      "      8       91.4503  0.0027\n",
      "      9       91.4503  0.0029\n",
      "     10       91.4503  0.0039\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      7       \u001b[36m70.2738\u001b[0m       62.0922  0.0225\n",
      "      8       78.0704       78.5951  0.0229\n",
      "      4       \u001b[36m86.3446\u001b[0m       66.3579  0.0276\n",
      "      8       \u001b[36m68.7432\u001b[0m       56.7071  0.0231\n",
      "      9       80.9421       82.0414  0.0239\n",
      "      5       \u001b[36m84.7396\u001b[0m       69.8963  0.0245\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m66.6878\u001b[0m       \u001b[32m34.9824\u001b[0m  0.0298\n",
      "      9       69.5897       50.6400  0.0210\n",
      "      6       \u001b[36m82.0910\u001b[0m       67.6900  0.0194\n",
      "     10       79.3756       79.1616  0.0205\n",
      "Restoring best model from epoch 1.\n",
      "      2       77.2961       \u001b[32m34.2271\u001b[0m  0.0208\n",
      "     10       69.2707       44.1218  0.0192\n",
      "Restoring best model from epoch 1.\n",
      "      7       \u001b[36m78.7118\u001b[0m       73.8065  0.0192\n",
      "      3       76.4438       \u001b[32m31.9912\u001b[0m  0.0192\n",
      "      8       78.8460       74.8468  0.0196\n",
      "      4       \u001b[36m66.5023\u001b[0m       34.2109  0.0193\n",
      "      9       83.9028       70.7008  0.0185\n",
      "      5       \u001b[36m62.1910\u001b[0m       33.7436  0.0192\n",
      "     10       \u001b[36m76.0155\u001b[0m       70.2429  0.0192\n",
      "Restoring best model from epoch 2.\n",
      "      6       \u001b[36m61.4838\u001b[0m       \u001b[32m30.7991\u001b[0m  0.0199\n",
      "      7       \u001b[36m60.0991\u001b[0m       \u001b[32m27.7879\u001b[0m  0.0208\n",
      "      8       \u001b[36m56.5216\u001b[0m       30.0803  0.0188\n",
      "      9       \u001b[36m56.1621\u001b[0m       30.9762  0.0179\n",
      "     10       59.7571       29.5569  0.0171\n",
      "Restoring best model from epoch 7.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m90.6719\u001b[0m       \u001b[32m86.0494\u001b[0m  0.0296\n",
      "      2       \u001b[36m89.3168\u001b[0m       \u001b[32m74.5653\u001b[0m  0.0263\n",
      "      3       \u001b[36m76.9509\u001b[0m       75.8184  0.0265\n",
      "      4       \u001b[36m75.4425\u001b[0m       76.0317  0.0339\n",
      "      5       \u001b[36m74.9530\u001b[0m       75.9685  0.0328\n",
      "      6       \u001b[36m71.5097\u001b[0m       77.4808  0.0289\n",
      "      7       \u001b[36m70.5835\u001b[0m       74.9090  0.0415\n",
      "      8       \u001b[36m67.7622\u001b[0m       \u001b[32m74.5455\u001b[0m  0.0316\n",
      "      9       68.7601       76.6213  0.0306\n",
      "     10       71.3713       75.4778  0.0448\n",
      "Restoring best model from epoch 8.\n",
      "y_train breslow final [   -6.    -6.    -6.    -8.    -9.    -9.     9.    11.    12.   -12.\n",
      "    14.   -15.    16.    19.   -20.   -21.   -22.   -23.   -30.    31.\n",
      "    34.    36.   -44.    46.    52.    56.    56.    65.    65.    67.\n",
      "   -79.    87.    91.    91.   -94.    97.   101.   103.   107.   115.\n",
      "   129.   129.  -137.  -141.   153.  -170.   171.   171.  -180.  -183.\n",
      "   194.   195.  -211.   217.  -219.   233.   247.  -248.  -260.   262.\n",
      "   262.   272.   278.   283.  -291.   296.   299.   300.   304.   308.\n",
      "  -314.  -322.  -327.  -328.  -330.  -341.   344.  -345.  -347.   347.\n",
      "  -354.  -357.  -359.  -361.  -363.  -365.   365.   366.  -366.  -372.\n",
      "   373.  -382.  -387.  -387.  -390.   394.  -395.  -395.  -396.  -400.\n",
      "  -406.  -406.  -408.  -409.   410.   412.  -412.   415.   416.   419.\n",
      "  -423.  -425.   425.   432.  -436.   438.  -444.  -447.   452.  -453.\n",
      "  -468.   469.  -472.  -474.  -478.  -480.  -486.  -500.  -507.  -512.\n",
      "  -519.  -520.  -520.   535.  -538.   547.  -552.  -555.  -555.   555.\n",
      "   556.  -564.  -566.  -570.  -575.  -579.   581.  -585.  -587.  -588.\n",
      "  -594.  -601.   601.  -608.  -608.   612.  -615.  -621.   627.  -630.\n",
      "   633.  -636.  -638.   639.  -640.   643.  -644.  -655.  -662.  -671.\n",
      "  -672.  -673.  -680.   688.  -693.   693.  -697.  -698.  -698.  -701.\n",
      "  -706.   711.  -719.  -722.   724.  -728.  -729.  -747.  -756.   757.\n",
      "  -763.   765.   768.   770.  -782.  -799.   802.  -810.  -816.  -819.\n",
      "   827.  -829.   837.   837.  -848.  -848.   848.  -854.  -860.  -898.\n",
      "  -898.  -906.  -910.  -917.  -925.  -928.  -951.  -989.  1005. -1008.\n",
      " -1030. -1066. -1085. -1085.  1088. -1091. -1145.  1147.  1149. -1168.\n",
      "  1210.  1229. -1231. -1233. -1242.  1271. -1295. -1302. -1339. -1351.\n",
      " -1363.  1372.  1386.  1423. -1424. -1450. -1516. -1531. -1553.  1560.\n",
      " -1562. -1567. -1618.  1624. -1633. -1636.  1685.  1694. -1779.  1791.\n",
      " -1823.  1852. -1855. -1876. -1900. -1939. -1970. -1989. -2015. -2017.\n",
      " -2028.  2116.  2131. -2184. -2202. -2245. -2301. -2301. -2317. -2398.\n",
      " -2415. -2425. -2442. -2455. -2513.  2542. -2728. -2752.  3258. -3308.\n",
      " -3478. -3675.]\n",
      "Concordance Index 0.6807735426008968\n",
      "Integrated Brier Score: 0.19392011029563777\n",
      "y_train breslow final [   -6.    -6.    -6.    -8.    -9.    -9.     9.    11.    12.   -12.\n",
      "    14.   -15.    16.    19.   -20.   -21.   -22.   -23.   -30.    31.\n",
      "    34.    36.   -44.    46.    52.    56.    56.    65.    65.    67.\n",
      "   -79.    87.    91.    91.   -94.    97.   101.   103.   107.   115.\n",
      "   129.   129.  -137.  -141.   153.  -170.   171.   171.  -180.  -183.\n",
      "   194.   195.  -211.   217.  -219.   233.   247.  -248.  -260.   262.\n",
      "   262.   272.   278.   283.  -291.   296.   299.   300.   304.   308.\n",
      "  -314.  -322.  -327.  -328.  -330.  -341.   344.  -345.  -347.   347.\n",
      "  -354.  -357.  -359.  -361.  -363.  -365.   365.   366.  -366.  -372.\n",
      "   373.  -382.  -387.  -387.  -390.   394.  -395.  -395.  -396.  -400.\n",
      "  -406.  -406.  -408.  -409.   410.   412.  -412.   415.   416.   419.\n",
      "  -423.  -425.   425.   432.  -436.   438.  -444.  -447.   452.  -453.\n",
      "  -468.   469.  -472.  -474.  -478.  -480.  -486.  -500.  -507.  -512.\n",
      "  -519.  -520.  -520.   535.  -538.   547.  -552.  -555.  -555.   555.\n",
      "   556.  -564.  -566.  -570.  -575.  -579.   581.  -585.  -587.  -588.\n",
      "  -594.  -601.   601.  -608.  -608.   612.  -615.  -621.   627.  -630.\n",
      "   633.  -636.  -638.   639.  -640.   643.  -644.  -655.  -662.  -671.\n",
      "  -672.  -673.  -680.   688.  -693.   693.  -697.  -698.  -698.  -701.\n",
      "  -706.   711.  -719.  -722.   724.  -728.  -729.  -747.  -756.   757.\n",
      "  -763.   765.   768.   770.  -782.  -799.   802.  -810.  -816.  -819.\n",
      "   827.  -829.   837.   837.  -848.  -848.   848.  -854.  -860.  -898.\n",
      "  -898.  -906.  -910.  -917.  -925.  -928.  -951.  -989.  1005. -1008.\n",
      " -1030. -1066. -1085. -1085.  1088. -1091. -1145.  1147.  1149. -1168.\n",
      "  1210.  1229. -1231. -1233. -1242.  1271. -1295. -1302. -1339. -1351.\n",
      " -1363.  1372.  1386.  1423. -1424. -1450. -1516. -1531. -1553.  1560.\n",
      " -1562. -1567. -1618.  1624. -1633. -1636.  1685.  1694. -1779.  1791.\n",
      " -1823.  1852. -1855. -1876. -1900. -1939. -1970. -1989. -2015. -2017.\n",
      " -2028.  2116.  2131. -2184. -2202. -2245. -2301. -2301. -2317. -2398.\n",
      " -2415. -2425. -2442. -2455. -2513.  2542. -2728. -2752.  3258. -3308.\n",
      " -3478. -3675.]\n",
      "durations 1.0 3437.0\n",
      "Concordance Index 0.5038535645472062\n",
      "Integrated Brier Score: 0.26039842556165516\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m91.5263\u001b[0m       \u001b[32m45.9286\u001b[0m  0.0323\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m87.7755\u001b[0m       \u001b[32m56.7340\u001b[0m  0.0235\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2       98.6869       \u001b[32m45.8067\u001b[0m  0.0238\n",
      "      2       96.7904       \u001b[32m52.6623\u001b[0m  0.0230\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m94.9881\u001b[0m       \u001b[32m61.4736\u001b[0m  0.0361\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m87.3506\u001b[0m       \u001b[32m51.8882\u001b[0m  0.0262\n",
      "      3       \u001b[36m87.4198\u001b[0m       \u001b[32m44.3054\u001b[0m  0.0253\n",
      "      2      120.8801       \u001b[32m58.6708\u001b[0m  0.0325\n",
      "      3       \u001b[36m79.4037\u001b[0m       53.0329  0.0406\n",
      "      2      113.2243       52.1927  0.0196\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m74.5557\u001b[0m       \u001b[32m41.9619\u001b[0m  0.0320\n",
      "      3       \u001b[36m90.7183\u001b[0m       \u001b[32m58.1428\u001b[0m  0.0207\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      4       \u001b[36m77.4159\u001b[0m       44.6590  0.0373\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m48.8760\u001b[0m  0.0021\n",
      "      4       \u001b[36m70.7908\u001b[0m       56.4234  0.0248\n",
      "      2       48.8760  0.0016\n",
      "      3       48.8760  0.0028\n",
      "      3       \u001b[36m76.8629\u001b[0m       \u001b[32m51.2101\u001b[0m  0.0264\n",
      "      4       48.8760  0.0042\n",
      "      5       48.8760  0.0035\n",
      "      6       48.8760  0.0017\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      7       48.8760  0.0015\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2       78.3964       \u001b[32m41.3462\u001b[0m  0.0228\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      8       48.8760  0.0015\n",
      "      4       \u001b[36m84.1875\u001b[0m       \u001b[32m55.6250\u001b[0m  0.0267\n",
      "      9       48.8760  0.0015\n",
      "      5       \u001b[36m69.9741\u001b[0m       62.7523  0.0200\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m73.8013\u001b[0m  0.0021\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m54.8693\u001b[0m  0.0027\n",
      "     10       48.8760  0.0015\n",
      "Restoring best model from epoch 1.\n",
      "      5       \u001b[36m73.6767\u001b[0m       46.3402  0.0240\n",
      "      2       73.8013  0.0017\n",
      "      2       54.8693  0.0024\n",
      "      3       73.8013  0.0016\n",
      "      3       54.8693  0.0017\n",
      "      4       73.8013  0.0016\n",
      "      4       54.8693  0.0016\n",
      "      5       73.8013  0.0016\n",
      "      5       54.8693  0.0015\n",
      "      6       73.8013  0.0016\n",
      "      6       54.8693  0.0015\n",
      "      7       73.8013  0.0015\n",
      "      4       79.8356       \u001b[32m50.8957\u001b[0m  0.0208\n",
      "      7       54.8693  0.0015\n",
      "      8       73.8013  0.0015\n",
      "      8       54.8693  0.0015\n",
      "      9       73.8013  0.0015\n",
      "      9       54.8693  0.0015\n",
      "     10       73.8013  0.0015\n",
      "Restoring best model from epoch 1.\n",
      "     10       54.8693  0.0017\n",
      "Restoring best model from epoch 1.\n",
      "      5       \u001b[36m81.6594\u001b[0m       61.3545  0.0188\n",
      "      3       \u001b[36m74.5115\u001b[0m       \u001b[32m40.7213\u001b[0m  0.0212\n",
      "      6       \u001b[36m68.4907\u001b[0m       68.7211  0.0290\n",
      "      5       \u001b[36m73.9506\u001b[0m       51.9323  0.0202\n",
      "      6       76.1582       \u001b[32m43.8809\u001b[0m  0.0299\n",
      "      4       \u001b[36m67.7899\u001b[0m       47.6441  0.0276\n",
      "      7       72.8057       70.6841  0.0188\n",
      "      6       81.8970       62.0672  0.0312\n",
      "      6       75.7695       54.1422  0.0189\n",
      "      7       75.6137       46.4858  0.0225\n",
      "      5       \u001b[36m64.0759\u001b[0m       43.3642  0.0187\n",
      "      8       72.0602       68.9589  0.0192\n",
      "      7       \u001b[36m81.4591\u001b[0m       60.7365  0.0188\n",
      "      7       \u001b[36m73.6012\u001b[0m       61.1029  0.0262\n",
      "      8       \u001b[36m72.5037\u001b[0m       50.7155  0.0265\n",
      "      8       \u001b[36m81.4449\u001b[0m       61.1345  0.0185\n",
      "      9       68.5133       65.7975  0.0218\n",
      "      6       64.4300       \u001b[32m40.5251\u001b[0m  0.0299\n",
      "      8       73.6895       65.3010  0.0196\n",
      "      9       74.0057       52.4253  0.0198\n",
      "      9       \u001b[36m79.2055\u001b[0m       62.4115  0.0185\n",
      "     10       70.6895       61.6603  0.0227\n",
      "Restoring best model from epoch 2.\n",
      "      7       66.2531       43.3039  0.0219\n",
      "     10       \u001b[36m71.2107\u001b[0m       52.3971  0.0189\n",
      "Restoring best model from epoch 6.\n",
      "      9       \u001b[36m71.6240\u001b[0m       63.5874  0.0273\n",
      "     10       79.7816       61.7578  0.0184\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Restoring best model from epoch 4.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m58.7365\u001b[0m  0.0027\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m66.0023\u001b[0m  0.0022\n",
      "      2       58.7365  0.0017\n",
      "      2       66.0023  0.0018\n",
      "      3       58.7365  0.0016\n",
      "      3       66.0023  0.0019\n",
      "      4       58.7365  0.0015\n",
      "      4       66.0023  0.0017\n",
      "      5       58.7365  0.0015\n",
      "      8       \u001b[36m60.8606\u001b[0m       47.2550  0.0191\n",
      "      5       66.0023  0.0016\n",
      "      6       58.7365  0.0015\n",
      "      6       66.0023  0.0016\n",
      "      7       58.7365  0.0017\n",
      "      7       66.0023  0.0015\n",
      "      8       58.7365  0.0015\n",
      "     10       \u001b[36m68.9859\u001b[0m       61.3709  0.0188\n",
      "Restoring best model from epoch 4.\n",
      "      8       66.0023  0.0016\n",
      "      9       58.7365  0.0015\n",
      "      9       66.0023  0.0016\n",
      "     10       58.7365  0.0016\n",
      "Restoring best model from epoch 1.\n",
      "     10       66.0023  0.0016\n",
      "Restoring best model from epoch 1.\n",
      "      9       \u001b[36m58.5132\u001b[0m       52.7539  0.0187\n",
      "     10       \u001b[36m58.1347\u001b[0m       55.6038  0.0185\n",
      "Restoring best model from epoch 6.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m87.2111\u001b[0m      \u001b[32m115.6963\u001b[0m  0.0255\n",
      "      2       \u001b[36m75.9286\u001b[0m       \u001b[32m82.6274\u001b[0m  0.0351\n",
      "      3       \u001b[36m68.0291\u001b[0m       86.0409  0.0262\n",
      "      4       \u001b[36m62.5814\u001b[0m       93.9815  0.0264\n",
      "      5       62.8815       97.0042  0.0301\n",
      "      6       \u001b[36m62.2715\u001b[0m       97.3583  0.0275\n",
      "      7       \u001b[36m61.7191\u001b[0m       96.4469  0.0896\n",
      "      8       \u001b[36m59.8906\u001b[0m      100.2536  0.0413\n",
      "      9       60.5728       99.2482  0.0257\n",
      "     10       \u001b[36m57.1725\u001b[0m      101.0965  0.0244\n",
      "Restoring best model from epoch 2.\n",
      "y_train breslow final [-1.000e+00 -6.000e+00 -6.000e+00 -6.000e+00 -9.000e+00  9.000e+00\n",
      " -1.000e+01  1.100e+01  1.200e+01 -1.200e+01 -1.500e+01  1.600e+01\n",
      "  1.900e+01 -2.000e+01 -2.000e+01 -2.100e+01 -2.200e+01 -2.300e+01\n",
      "  2.700e+01 -3.000e+01  3.100e+01  3.600e+01 -4.400e+01  4.600e+01\n",
      "  5.200e+01  5.600e+01  6.500e+01  6.500e+01  6.700e+01  8.700e+01\n",
      "  9.100e+01 -9.400e+01  9.700e+01  1.010e+02  1.020e+02  1.030e+02\n",
      "  1.070e+02  1.290e+02  1.290e+02 -1.370e+02  1.400e+02 -1.410e+02\n",
      " -1.700e+02  1.710e+02  1.710e+02 -1.830e+02  1.940e+02  1.950e+02\n",
      " -2.110e+02  2.140e+02  2.170e+02 -2.190e+02  2.230e+02 -2.290e+02\n",
      "  2.330e+02  2.470e+02 -2.480e+02 -2.500e+02 -2.600e+02  2.620e+02\n",
      "  2.620e+02  2.720e+02  2.790e+02 -2.910e+02  2.990e+02  3.000e+02\n",
      "  3.000e+02 -3.030e+02  3.030e+02  3.040e+02 -3.140e+02  3.150e+02\n",
      " -3.280e+02 -3.300e+02 -3.410e+02 -3.450e+02 -3.470e+02  3.470e+02\n",
      "  3.490e+02 -3.540e+02 -3.570e+02  3.590e+02 -3.610e+02 -3.610e+02\n",
      " -3.630e+02 -3.630e+02  3.650e+02 -3.660e+02 -3.720e+02  3.730e+02\n",
      "  3.810e+02 -3.820e+02 -3.870e+02 -3.870e+02 -3.900e+02 -3.960e+02\n",
      " -3.990e+02 -4.000e+02 -4.060e+02 -4.080e+02  4.100e+02 -4.120e+02\n",
      " -4.150e+02  4.160e+02 -4.230e+02  4.250e+02 -4.270e+02 -4.300e+02\n",
      "  4.320e+02 -4.360e+02  4.380e+02 -4.490e+02  4.520e+02 -4.530e+02\n",
      " -4.580e+02  4.690e+02 -4.720e+02 -4.740e+02 -4.760e+02 -4.800e+02\n",
      " -5.000e+02 -5.070e+02 -5.120e+02 -5.190e+02 -5.200e+02  5.350e+02\n",
      " -5.520e+02 -5.540e+02 -5.550e+02 -5.550e+02  5.560e+02  5.580e+02\n",
      " -5.620e+02 -5.640e+02 -5.700e+02 -5.740e+02 -5.790e+02 -5.850e+02\n",
      " -5.870e+02 -5.880e+02 -5.940e+02  5.960e+02 -6.010e+02  6.010e+02\n",
      " -6.020e+02 -6.080e+02  6.270e+02 -6.300e+02 -6.310e+02 -6.320e+02\n",
      "  6.330e+02 -6.360e+02 -6.380e+02  6.390e+02  6.430e+02 -6.440e+02\n",
      "  6.490e+02 -6.580e+02  6.600e+02 -6.620e+02 -6.720e+02 -6.800e+02\n",
      " -6.930e+02 -6.970e+02 -6.980e+02 -6.980e+02 -7.010e+02 -7.060e+02\n",
      "  7.110e+02 -7.190e+02  7.240e+02 -7.280e+02 -7.290e+02 -7.440e+02\n",
      " -7.560e+02 -7.630e+02  7.650e+02  7.680e+02  7.700e+02 -7.800e+02\n",
      " -7.820e+02 -7.840e+02 -7.990e+02  8.020e+02 -8.100e+02 -8.160e+02\n",
      "  8.270e+02 -8.290e+02  8.370e+02  8.370e+02 -8.480e+02 -8.480e+02\n",
      "  8.480e+02 -8.490e+02 -8.540e+02 -8.600e+02 -8.790e+02 -8.980e+02\n",
      "  8.990e+02 -9.060e+02 -9.100e+02 -9.170e+02 -9.250e+02  9.310e+02\n",
      " -9.420e+02 -9.890e+02 -1.008e+03 -1.030e+03 -1.049e+03 -1.066e+03\n",
      " -1.067e+03 -1.085e+03 -1.085e+03  1.088e+03 -1.098e+03 -1.115e+03\n",
      "  1.135e+03 -1.145e+03  1.147e+03  1.149e+03 -1.168e+03 -1.219e+03\n",
      "  1.229e+03 -1.231e+03 -1.233e+03 -1.241e+03 -1.242e+03  1.271e+03\n",
      " -1.302e+03 -1.339e+03 -1.345e+03 -1.351e+03  1.372e+03  1.386e+03\n",
      "  1.397e+03  1.423e+03 -1.424e+03 -1.450e+03 -1.452e+03  1.490e+03\n",
      " -1.495e+03 -1.531e+03 -1.531e+03 -1.553e+03  1.560e+03 -1.562e+03\n",
      " -1.567e+03 -1.618e+03  1.622e+03  1.624e+03 -1.633e+03 -1.636e+03\n",
      "  1.694e+03 -1.711e+03 -1.718e+03 -1.731e+03 -1.779e+03  1.791e+03\n",
      " -1.804e+03 -1.823e+03  1.852e+03 -1.855e+03 -1.876e+03 -1.900e+03\n",
      " -1.939e+03 -1.989e+03 -2.015e+03 -2.017e+03 -2.028e+03 -2.102e+03\n",
      "  2.116e+03  2.131e+03 -2.184e+03 -2.245e+03 -2.324e+03 -2.398e+03\n",
      " -2.412e+03 -2.415e+03 -2.425e+03 -2.455e+03  2.456e+03  2.486e+03\n",
      " -2.513e+03  2.532e+03  2.542e+03 -2.728e+03 -2.746e+03  3.125e+03\n",
      "  3.258e+03 -3.308e+03 -3.437e+03 -3.478e+03]\n",
      "Concordance Index 0.637225981838163\n",
      "Integrated Brier Score: 0.1945932569709806\n",
      "y_train breslow final [-1.000e+00 -6.000e+00 -6.000e+00 -6.000e+00 -9.000e+00  9.000e+00\n",
      " -1.000e+01  1.100e+01  1.200e+01 -1.200e+01 -1.500e+01  1.600e+01\n",
      "  1.900e+01 -2.000e+01 -2.000e+01 -2.100e+01 -2.200e+01 -2.300e+01\n",
      "  2.700e+01 -3.000e+01  3.100e+01  3.600e+01 -4.400e+01  4.600e+01\n",
      "  5.200e+01  5.600e+01  6.500e+01  6.500e+01  6.700e+01  8.700e+01\n",
      "  9.100e+01 -9.400e+01  9.700e+01  1.010e+02  1.020e+02  1.030e+02\n",
      "  1.070e+02  1.290e+02  1.290e+02 -1.370e+02  1.400e+02 -1.410e+02\n",
      " -1.700e+02  1.710e+02  1.710e+02 -1.830e+02  1.940e+02  1.950e+02\n",
      " -2.110e+02  2.140e+02  2.170e+02 -2.190e+02  2.230e+02 -2.290e+02\n",
      "  2.330e+02  2.470e+02 -2.480e+02 -2.500e+02 -2.600e+02  2.620e+02\n",
      "  2.620e+02  2.720e+02  2.790e+02 -2.910e+02  2.990e+02  3.000e+02\n",
      "  3.000e+02 -3.030e+02  3.030e+02  3.040e+02 -3.140e+02  3.150e+02\n",
      " -3.280e+02 -3.300e+02 -3.410e+02 -3.450e+02 -3.470e+02  3.470e+02\n",
      "  3.490e+02 -3.540e+02 -3.570e+02  3.590e+02 -3.610e+02 -3.610e+02\n",
      " -3.630e+02 -3.630e+02  3.650e+02 -3.660e+02 -3.720e+02  3.730e+02\n",
      "  3.810e+02 -3.820e+02 -3.870e+02 -3.870e+02 -3.900e+02 -3.960e+02\n",
      " -3.990e+02 -4.000e+02 -4.060e+02 -4.080e+02  4.100e+02 -4.120e+02\n",
      " -4.150e+02  4.160e+02 -4.230e+02  4.250e+02 -4.270e+02 -4.300e+02\n",
      "  4.320e+02 -4.360e+02  4.380e+02 -4.490e+02  4.520e+02 -4.530e+02\n",
      " -4.580e+02  4.690e+02 -4.720e+02 -4.740e+02 -4.760e+02 -4.800e+02\n",
      " -5.000e+02 -5.070e+02 -5.120e+02 -5.190e+02 -5.200e+02  5.350e+02\n",
      " -5.520e+02 -5.540e+02 -5.550e+02 -5.550e+02  5.560e+02  5.580e+02\n",
      " -5.620e+02 -5.640e+02 -5.700e+02 -5.740e+02 -5.790e+02 -5.850e+02\n",
      " -5.870e+02 -5.880e+02 -5.940e+02  5.960e+02 -6.010e+02  6.010e+02\n",
      " -6.020e+02 -6.080e+02  6.270e+02 -6.300e+02 -6.310e+02 -6.320e+02\n",
      "  6.330e+02 -6.360e+02 -6.380e+02  6.390e+02  6.430e+02 -6.440e+02\n",
      "  6.490e+02 -6.580e+02  6.600e+02 -6.620e+02 -6.720e+02 -6.800e+02\n",
      " -6.930e+02 -6.970e+02 -6.980e+02 -6.980e+02 -7.010e+02 -7.060e+02\n",
      "  7.110e+02 -7.190e+02  7.240e+02 -7.280e+02 -7.290e+02 -7.440e+02\n",
      " -7.560e+02 -7.630e+02  7.650e+02  7.680e+02  7.700e+02 -7.800e+02\n",
      " -7.820e+02 -7.840e+02 -7.990e+02  8.020e+02 -8.100e+02 -8.160e+02\n",
      "  8.270e+02 -8.290e+02  8.370e+02  8.370e+02 -8.480e+02 -8.480e+02\n",
      "  8.480e+02 -8.490e+02 -8.540e+02 -8.600e+02 -8.790e+02 -8.980e+02\n",
      "  8.990e+02 -9.060e+02 -9.100e+02 -9.170e+02 -9.250e+02  9.310e+02\n",
      " -9.420e+02 -9.890e+02 -1.008e+03 -1.030e+03 -1.049e+03 -1.066e+03\n",
      " -1.067e+03 -1.085e+03 -1.085e+03  1.088e+03 -1.098e+03 -1.115e+03\n",
      "  1.135e+03 -1.145e+03  1.147e+03  1.149e+03 -1.168e+03 -1.219e+03\n",
      "  1.229e+03 -1.231e+03 -1.233e+03 -1.241e+03 -1.242e+03  1.271e+03\n",
      " -1.302e+03 -1.339e+03 -1.345e+03 -1.351e+03  1.372e+03  1.386e+03\n",
      "  1.397e+03  1.423e+03 -1.424e+03 -1.450e+03 -1.452e+03  1.490e+03\n",
      " -1.495e+03 -1.531e+03 -1.531e+03 -1.553e+03  1.560e+03 -1.562e+03\n",
      " -1.567e+03 -1.618e+03  1.622e+03  1.624e+03 -1.633e+03 -1.636e+03\n",
      "  1.694e+03 -1.711e+03 -1.718e+03 -1.731e+03 -1.779e+03  1.791e+03\n",
      " -1.804e+03 -1.823e+03  1.852e+03 -1.855e+03 -1.876e+03 -1.900e+03\n",
      " -1.939e+03 -1.989e+03 -2.015e+03 -2.017e+03 -2.028e+03 -2.102e+03\n",
      "  2.116e+03  2.131e+03 -2.184e+03 -2.245e+03 -2.324e+03 -2.398e+03\n",
      " -2.412e+03 -2.415e+03 -2.425e+03 -2.455e+03  2.456e+03  2.486e+03\n",
      " -2.513e+03  2.532e+03  2.542e+03 -2.728e+03 -2.746e+03  3.125e+03\n",
      "  3.258e+03 -3.308e+03 -3.437e+03 -3.478e+03]\n",
      "durations 8.0 3675.0\n",
      "Concordance Index 0.6073253833049403\n",
      "Integrated Brier Score: 0.3481800414570151\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m88.1864\u001b[0m       \u001b[32m34.5227\u001b[0m  0.0212\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m94.5338\u001b[0m       \u001b[32m49.7107\u001b[0m  0.0323\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2       \u001b[36m86.8064\u001b[0m       41.4669  0.0205\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m43.0469\u001b[0m  0.0021\n",
      "      2       97.6491       53.8235  0.0205\n",
      "      2       43.0469  0.0016\n",
      "      3       43.0469  0.0016\n",
      "      4       43.0469  0.0015\n",
      "      5       43.0469  0.0022\n",
      "      6       43.0469  0.0051\n",
      "      7       43.0469  0.0016\n",
      "      8       43.0469  0.0015\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      9       43.0469  0.0015\n",
      "     10       43.0469  0.0015\n",
      "Restoring best model from epoch 1.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m87.2827\u001b[0m       \u001b[32m54.8223\u001b[0m  0.0204\n",
      "      3       \u001b[36m83.1017\u001b[0m       51.0113  0.0254\n",
      "      3      106.7271       65.2859  0.0250\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m38.6365\u001b[0m  0.0019\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      2       38.6365  0.0016\n",
      "      3       38.6365  0.0015\n",
      "      4       38.6365  0.0015\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      5       38.6365  0.0018\n",
      "      6       38.6365  0.0025\n",
      "      7       38.6365  0.0016\n",
      "      8       38.6365  0.0015\n",
      "      9       38.6365  0.0015\n",
      "      4       \u001b[36m73.1451\u001b[0m       41.3343  0.0226\n",
      "     10       38.6365  0.0015\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2       99.0125       \u001b[32m54.0909\u001b[0m  0.0233\n",
      "      4       \u001b[36m87.2227\u001b[0m       68.7970  0.0237\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m55.1507\u001b[0m  0.0021\n",
      "      2       55.1507  0.0018\n",
      "      3       55.1507  0.0017\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      4       55.1507  0.0018\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      5       55.1507  0.0019\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m57.6925\u001b[0m  0.0019\n",
      "      6       55.1507  0.0026\n",
      "      2       57.6925  0.0020\n",
      "      7       55.1507  0.0018\n",
      "      3       57.6925  0.0016\n",
      "      8       55.1507  0.0017\n",
      "      4       57.6925  0.0016\n",
      "      9       55.1507  0.0019\n",
      "      5       57.6925  0.0015\n",
      "     10       55.1507  0.0018\n",
      "Restoring best model from epoch 1.\n",
      "      5       \u001b[36m72.0274\u001b[0m       38.1514  0.0265\n",
      "      6       57.6925  0.0015\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      3       \u001b[36m78.0239\u001b[0m       \u001b[32m51.1858\u001b[0m  0.0249\n",
      "      5       \u001b[36m83.6444\u001b[0m       62.5464  0.0274\n",
      "      7       57.6925  0.0054\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      8       57.6925  0.0017\n",
      "      9       57.6925  0.0016\n",
      "     10       57.6925  0.0016\n",
      "Restoring best model from epoch 1.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m66.4293\u001b[0m       \u001b[32m32.0925\u001b[0m  0.0395\n",
      "      6       73.4577       47.1101  0.0249\n",
      "      4       \u001b[36m74.1893\u001b[0m       \u001b[32m50.2001\u001b[0m  0.0262\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m83.5044\u001b[0m       \u001b[32m44.1831\u001b[0m  0.0229\n",
      "      6       \u001b[36m81.3227\u001b[0m       56.9850  0.0274\n",
      "      2       77.9687       \u001b[32m31.4234\u001b[0m  0.0196\n",
      "      2       \u001b[36m81.3647\u001b[0m       45.6047  0.0197\n",
      "      7       \u001b[36m68.2042\u001b[0m       62.0815  0.0271\n",
      "      5       \u001b[36m73.9740\u001b[0m       54.9355  0.0208\n",
      "      7       81.6690       58.0754  0.0207\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m53.3164\u001b[0m  0.0034\n",
      "      2       53.3164  0.0019\n",
      "      3       53.3164  0.0016\n",
      "      4       53.3164  0.0016\n",
      "      8       69.9513       65.6202  0.0189\n",
      "      5       53.3164  0.0015\n",
      "      3       75.8397       34.6875  0.0371\n",
      "      6       74.0621       55.9304  0.0200\n",
      "      6       53.3164  0.0015\n",
      "      3       86.9452       50.3430  0.0222\n",
      "      7       53.3164  0.0016\n",
      "      8       \u001b[36m78.2118\u001b[0m       66.0580  0.0225\n",
      "      8       53.3164  0.0015\n",
      "      9       53.3164  0.0015\n",
      "     10       53.3164  0.0015\n",
      "Restoring best model from epoch 1.\n",
      "      9       \u001b[36m67.7397\u001b[0m       64.1041  0.0192\n",
      "      7       \u001b[36m72.7318\u001b[0m       55.1020  0.0191\n",
      "      4       \u001b[36m61.6209\u001b[0m       37.3671  0.0248\n",
      "      9       \u001b[36m77.1074\u001b[0m       67.4703  0.0261\n",
      "      4       \u001b[36m70.4929\u001b[0m       52.2739  0.0282\n",
      "     10       \u001b[36m67.5690\u001b[0m       58.7327  0.0198\n",
      "Restoring best model from epoch 1.\n",
      "      5       \u001b[36m60.2671\u001b[0m       37.1569  0.0189\n",
      "      8       73.5243       54.7967  0.0244\n",
      "      5       \u001b[36m70.1441\u001b[0m       48.8551  0.0192\n",
      "     10       \u001b[36m74.2004\u001b[0m       63.8617  0.0194\n",
      "Restoring best model from epoch 1.\n",
      "      6       \u001b[36m59.9370\u001b[0m       37.7667  0.0186\n",
      "      9       \u001b[36m72.6699\u001b[0m       54.6376  0.0197\n",
      "      6       \u001b[36m67.9635\u001b[0m       51.8976  0.0181\n",
      "      7       \u001b[36m57.4156\u001b[0m       43.2356  0.0191\n",
      "     10       \u001b[36m69.4915\u001b[0m       53.2097  0.0184\n",
      "Restoring best model from epoch 4.\n",
      "      7       \u001b[36m67.0213\u001b[0m       56.5088  0.0178\n",
      "      8       57.4454       45.1230  0.0177\n",
      "      8       \u001b[36m62.7614\u001b[0m       64.0785  0.0178\n",
      "      9       \u001b[36m57.2101\u001b[0m       46.7757  0.0188\n",
      "      9       71.7151       60.8627  0.0178\n",
      "     10       \u001b[36m54.6874\u001b[0m       48.4428  0.0178\n",
      "Restoring best model from epoch 2.\n",
      "     10       65.5638       54.2031  0.0174\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m81.6346\u001b[0m       \u001b[32m85.8323\u001b[0m  0.0254\n",
      "      2       95.7766       \u001b[32m79.3715\u001b[0m  0.0259\n",
      "      3       \u001b[36m75.3850\u001b[0m       89.0641  0.0316\n",
      "      4       \u001b[36m74.9882\u001b[0m       80.8888  0.0272\n",
      "      5       75.9001       79.6680  0.0278\n",
      "      6       \u001b[36m73.8643\u001b[0m       83.4989  0.0315\n",
      "      7       \u001b[36m73.0395\u001b[0m       89.9369  0.0271\n",
      "      8       \u001b[36m72.6105\u001b[0m       86.5405  0.0255\n",
      "      9       \u001b[36m69.1581\u001b[0m       82.5690  0.0319\n",
      "     10       69.6094       83.8091  0.0260\n",
      "Restoring best model from epoch 2.\n",
      "y_train breslow final [-1.000e+00 -6.000e+00 -6.000e+00 -8.000e+00 -9.000e+00 -9.000e+00\n",
      "  9.000e+00 -1.000e+01  1.100e+01  1.200e+01 -1.200e+01  1.400e+01\n",
      " -1.500e+01  1.600e+01  1.900e+01 -2.000e+01 -2.000e+01 -2.100e+01\n",
      " -2.200e+01 -2.300e+01  2.700e+01  3.400e+01  3.600e+01 -4.400e+01\n",
      "  4.600e+01  5.200e+01  5.600e+01  5.600e+01  6.500e+01 -7.900e+01\n",
      "  8.700e+01  9.100e+01  9.100e+01 -9.400e+01  1.020e+02  1.030e+02\n",
      "  1.150e+02  1.290e+02 -1.370e+02  1.400e+02 -1.410e+02  1.530e+02\n",
      " -1.700e+02  1.710e+02  1.710e+02 -1.800e+02  1.940e+02  1.950e+02\n",
      " -2.110e+02  2.140e+02  2.170e+02 -2.190e+02  2.230e+02 -2.290e+02\n",
      " -2.500e+02 -2.600e+02  2.620e+02  2.720e+02  2.780e+02  2.790e+02\n",
      "  2.830e+02  2.960e+02  3.000e+02  3.000e+02 -3.030e+02  3.030e+02\n",
      "  3.080e+02 -3.140e+02  3.150e+02 -3.220e+02 -3.270e+02 -3.410e+02\n",
      "  3.440e+02  3.490e+02 -3.570e+02 -3.590e+02  3.590e+02 -3.610e+02\n",
      " -3.610e+02 -3.630e+02 -3.630e+02 -3.650e+02  3.650e+02  3.660e+02\n",
      "  3.730e+02  3.810e+02 -3.870e+02 -3.900e+02  3.940e+02 -3.950e+02\n",
      " -3.950e+02 -3.990e+02 -4.000e+02 -4.060e+02 -4.090e+02  4.100e+02\n",
      "  4.120e+02 -4.120e+02  4.150e+02 -4.150e+02  4.160e+02  4.190e+02\n",
      " -4.230e+02 -4.250e+02  4.250e+02 -4.270e+02 -4.300e+02  4.320e+02\n",
      " -4.360e+02  4.380e+02 -4.440e+02 -4.470e+02 -4.490e+02 -4.530e+02\n",
      " -4.580e+02 -4.680e+02  4.690e+02 -4.760e+02 -4.780e+02 -4.800e+02\n",
      " -4.860e+02 -5.000e+02 -5.070e+02 -5.120e+02 -5.190e+02 -5.200e+02\n",
      " -5.200e+02  5.350e+02 -5.380e+02  5.470e+02 -5.520e+02 -5.540e+02\n",
      "  5.550e+02  5.580e+02 -5.620e+02 -5.640e+02 -5.660e+02 -5.740e+02\n",
      " -5.750e+02 -5.790e+02  5.810e+02 -5.870e+02 -5.940e+02  5.960e+02\n",
      " -6.010e+02  6.010e+02 -6.020e+02 -6.080e+02 -6.080e+02  6.120e+02\n",
      " -6.150e+02 -6.210e+02  6.270e+02 -6.310e+02 -6.320e+02  6.330e+02\n",
      "  6.390e+02 -6.400e+02 -6.440e+02  6.490e+02 -6.550e+02 -6.580e+02\n",
      "  6.600e+02 -6.620e+02 -6.710e+02 -6.720e+02 -6.730e+02 -6.800e+02\n",
      "  6.880e+02 -6.930e+02  6.930e+02 -6.970e+02 -6.980e+02 -7.010e+02\n",
      " -7.190e+02 -7.220e+02  7.240e+02 -7.280e+02 -7.440e+02 -7.470e+02\n",
      " -7.560e+02  7.570e+02  7.650e+02  7.680e+02  7.700e+02 -7.800e+02\n",
      " -7.840e+02 -7.990e+02 -8.160e+02 -8.190e+02  8.270e+02 -8.290e+02\n",
      "  8.370e+02  8.370e+02 -8.480e+02  8.480e+02 -8.490e+02 -8.540e+02\n",
      " -8.600e+02 -8.790e+02 -8.980e+02  8.990e+02 -9.060e+02 -9.250e+02\n",
      " -9.280e+02  9.310e+02 -9.420e+02 -9.510e+02 -9.890e+02  1.005e+03\n",
      " -1.008e+03 -1.030e+03 -1.049e+03 -1.067e+03 -1.085e+03 -1.085e+03\n",
      "  1.088e+03 -1.091e+03 -1.098e+03 -1.115e+03  1.135e+03 -1.145e+03\n",
      "  1.147e+03  1.149e+03 -1.168e+03  1.210e+03 -1.219e+03 -1.231e+03\n",
      " -1.233e+03 -1.241e+03 -1.242e+03  1.271e+03 -1.295e+03 -1.302e+03\n",
      " -1.345e+03 -1.351e+03 -1.363e+03  1.372e+03  1.397e+03 -1.424e+03\n",
      " -1.450e+03 -1.452e+03  1.490e+03 -1.495e+03 -1.516e+03 -1.531e+03\n",
      " -1.531e+03  1.560e+03 -1.562e+03 -1.567e+03 -1.618e+03  1.622e+03\n",
      " -1.633e+03  1.685e+03 -1.711e+03 -1.718e+03 -1.731e+03 -1.779e+03\n",
      "  1.791e+03 -1.804e+03 -1.823e+03  1.852e+03 -1.876e+03 -1.939e+03\n",
      " -1.970e+03 -1.989e+03 -2.017e+03 -2.028e+03 -2.102e+03 -2.202e+03\n",
      " -2.301e+03 -2.301e+03 -2.317e+03 -2.324e+03 -2.398e+03 -2.412e+03\n",
      " -2.415e+03 -2.425e+03 -2.442e+03 -2.455e+03  2.456e+03  2.486e+03\n",
      " -2.513e+03  2.532e+03  2.542e+03 -2.728e+03 -2.746e+03 -2.752e+03\n",
      "  3.125e+03 -3.308e+03 -3.437e+03 -3.675e+03]\n",
      "Concordance Index 0.6126000111900632\n",
      "Integrated Brier Score: 0.21881497385023083\n",
      "y_train breslow final [-1.000e+00 -6.000e+00 -6.000e+00 -8.000e+00 -9.000e+00 -9.000e+00\n",
      "  9.000e+00 -1.000e+01  1.100e+01  1.200e+01 -1.200e+01  1.400e+01\n",
      " -1.500e+01  1.600e+01  1.900e+01 -2.000e+01 -2.000e+01 -2.100e+01\n",
      " -2.200e+01 -2.300e+01  2.700e+01  3.400e+01  3.600e+01 -4.400e+01\n",
      "  4.600e+01  5.200e+01  5.600e+01  5.600e+01  6.500e+01 -7.900e+01\n",
      "  8.700e+01  9.100e+01  9.100e+01 -9.400e+01  1.020e+02  1.030e+02\n",
      "  1.150e+02  1.290e+02 -1.370e+02  1.400e+02 -1.410e+02  1.530e+02\n",
      " -1.700e+02  1.710e+02  1.710e+02 -1.800e+02  1.940e+02  1.950e+02\n",
      " -2.110e+02  2.140e+02  2.170e+02 -2.190e+02  2.230e+02 -2.290e+02\n",
      " -2.500e+02 -2.600e+02  2.620e+02  2.720e+02  2.780e+02  2.790e+02\n",
      "  2.830e+02  2.960e+02  3.000e+02  3.000e+02 -3.030e+02  3.030e+02\n",
      "  3.080e+02 -3.140e+02  3.150e+02 -3.220e+02 -3.270e+02 -3.410e+02\n",
      "  3.440e+02  3.490e+02 -3.570e+02 -3.590e+02  3.590e+02 -3.610e+02\n",
      " -3.610e+02 -3.630e+02 -3.630e+02 -3.650e+02  3.650e+02  3.660e+02\n",
      "  3.730e+02  3.810e+02 -3.870e+02 -3.900e+02  3.940e+02 -3.950e+02\n",
      " -3.950e+02 -3.990e+02 -4.000e+02 -4.060e+02 -4.090e+02  4.100e+02\n",
      "  4.120e+02 -4.120e+02  4.150e+02 -4.150e+02  4.160e+02  4.190e+02\n",
      " -4.230e+02 -4.250e+02  4.250e+02 -4.270e+02 -4.300e+02  4.320e+02\n",
      " -4.360e+02  4.380e+02 -4.440e+02 -4.470e+02 -4.490e+02 -4.530e+02\n",
      " -4.580e+02 -4.680e+02  4.690e+02 -4.760e+02 -4.780e+02 -4.800e+02\n",
      " -4.860e+02 -5.000e+02 -5.070e+02 -5.120e+02 -5.190e+02 -5.200e+02\n",
      " -5.200e+02  5.350e+02 -5.380e+02  5.470e+02 -5.520e+02 -5.540e+02\n",
      "  5.550e+02  5.580e+02 -5.620e+02 -5.640e+02 -5.660e+02 -5.740e+02\n",
      " -5.750e+02 -5.790e+02  5.810e+02 -5.870e+02 -5.940e+02  5.960e+02\n",
      " -6.010e+02  6.010e+02 -6.020e+02 -6.080e+02 -6.080e+02  6.120e+02\n",
      " -6.150e+02 -6.210e+02  6.270e+02 -6.310e+02 -6.320e+02  6.330e+02\n",
      "  6.390e+02 -6.400e+02 -6.440e+02  6.490e+02 -6.550e+02 -6.580e+02\n",
      "  6.600e+02 -6.620e+02 -6.710e+02 -6.720e+02 -6.730e+02 -6.800e+02\n",
      "  6.880e+02 -6.930e+02  6.930e+02 -6.970e+02 -6.980e+02 -7.010e+02\n",
      " -7.190e+02 -7.220e+02  7.240e+02 -7.280e+02 -7.440e+02 -7.470e+02\n",
      " -7.560e+02  7.570e+02  7.650e+02  7.680e+02  7.700e+02 -7.800e+02\n",
      " -7.840e+02 -7.990e+02 -8.160e+02 -8.190e+02  8.270e+02 -8.290e+02\n",
      "  8.370e+02  8.370e+02 -8.480e+02  8.480e+02 -8.490e+02 -8.540e+02\n",
      " -8.600e+02 -8.790e+02 -8.980e+02  8.990e+02 -9.060e+02 -9.250e+02\n",
      " -9.280e+02  9.310e+02 -9.420e+02 -9.510e+02 -9.890e+02  1.005e+03\n",
      " -1.008e+03 -1.030e+03 -1.049e+03 -1.067e+03 -1.085e+03 -1.085e+03\n",
      "  1.088e+03 -1.091e+03 -1.098e+03 -1.115e+03  1.135e+03 -1.145e+03\n",
      "  1.147e+03  1.149e+03 -1.168e+03  1.210e+03 -1.219e+03 -1.231e+03\n",
      " -1.233e+03 -1.241e+03 -1.242e+03  1.271e+03 -1.295e+03 -1.302e+03\n",
      " -1.345e+03 -1.351e+03 -1.363e+03  1.372e+03  1.397e+03 -1.424e+03\n",
      " -1.450e+03 -1.452e+03  1.490e+03 -1.495e+03 -1.516e+03 -1.531e+03\n",
      " -1.531e+03  1.560e+03 -1.562e+03 -1.567e+03 -1.618e+03  1.622e+03\n",
      " -1.633e+03  1.685e+03 -1.711e+03 -1.718e+03 -1.731e+03 -1.779e+03\n",
      "  1.791e+03 -1.804e+03 -1.823e+03  1.852e+03 -1.876e+03 -1.939e+03\n",
      " -1.970e+03 -1.989e+03 -2.017e+03 -2.028e+03 -2.102e+03 -2.202e+03\n",
      " -2.301e+03 -2.301e+03 -2.317e+03 -2.324e+03 -2.398e+03 -2.412e+03\n",
      " -2.415e+03 -2.425e+03 -2.442e+03 -2.455e+03  2.456e+03  2.486e+03\n",
      " -2.513e+03  2.532e+03  2.542e+03 -2.728e+03 -2.746e+03 -2.752e+03\n",
      "  3.125e+03 -3.308e+03 -3.437e+03 -3.675e+03]\n",
      "durations 6.0 3478.0\n",
      "Concordance Index 0.6135496183206107\n",
      "Integrated Brier Score: 0.23100105582354022\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m87.3648\u001b[0m       \u001b[32m57.0719\u001b[0m  0.0287\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m78.6338\u001b[0m       \u001b[32m49.0361\u001b[0m  0.0238\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m91.4023\u001b[0m       \u001b[32m43.5801\u001b[0m  0.0380\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m85.4792\u001b[0m       \u001b[32m57.9818\u001b[0m  0.0243\n",
      "      2       97.4728       57.0726  0.0200\n",
      "      2       \u001b[36m72.4663\u001b[0m       54.7904  0.0225\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m81.0048\u001b[0m       \u001b[32m30.7022\u001b[0m  0.0238\n",
      "      2       \u001b[36m85.6734\u001b[0m       46.2364  0.0204\n",
      "      2      106.0620       \u001b[32m42.6553\u001b[0m  0.0203\n",
      "      3       \u001b[36m72.1194\u001b[0m       49.4562  0.0207\n",
      "      3       \u001b[36m80.4421\u001b[0m       58.0147  0.0259\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      2       \u001b[36m76.5426\u001b[0m       \u001b[32m30.3486\u001b[0m  0.0292\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      4       \u001b[36m68.5459\u001b[0m       54.8313  0.0205\n",
      "      3       \u001b[36m84.5541\u001b[0m       51.6048  0.0282\n",
      "      4       \u001b[36m71.0251\u001b[0m       61.0023  0.0195\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m45.1769\u001b[0m  0.0074\n",
      "      2       45.1769  0.0016\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m60.6944\u001b[0m  0.0023\n",
      "      3       45.1769  0.0015\n",
      "      2       60.6944  0.0018\n",
      "      3       87.3132       \u001b[32m40.8301\u001b[0m  0.0324\n",
      "      4       45.1769  0.0015\n",
      "      3       60.6944  0.0016\n",
      "      5       45.1769  0.0015\n",
      "      4       60.6944  0.0015\n",
      "      6       45.1769  0.0016\n",
      "      5       60.6944  0.0019\n",
      "      7       45.1769  0.0015\n",
      "      8       45.1769  0.0015\n",
      "      6       60.6944  0.0021\n",
      "      9       45.1769  0.0015\n",
      "     10       45.1769  0.0015\n",
      "Restoring best model from epoch 1.\n",
      "      5       \u001b[36m63.1693\u001b[0m       65.4513  0.0200\n",
      "      4       \u001b[36m79.9909\u001b[0m       58.4660  0.0199\n",
      "      3      103.1621       35.3856  0.0274\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      7       60.6944  0.0042\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m75.4047\u001b[0m  0.0020\n",
      "      8       60.6944  0.0046\n",
      "      2       75.4047  0.0016\n",
      "      9       60.6944  0.0016\n",
      "      3       75.4047  0.0016\n",
      "     10       60.6944  0.0016\n",
      "Restoring best model from epoch 1.\n",
      "      4       75.4047  0.0016\n",
      "      5       75.4047  0.0016\n",
      "      5       73.2788       66.4246  0.0320\n",
      "      6       75.4047  0.0015\n",
      "      7       75.4047  0.0016\n",
      "      8       75.4047  0.0016\n",
      "      9       75.4047  0.0017\n",
      "      6       66.1126       70.8612  0.0194\n",
      "      5       \u001b[36m77.3834\u001b[0m       69.7214  0.0202\n",
      "     10       75.4047  0.0016\n",
      "Restoring best model from epoch 1.\n",
      "      4       \u001b[36m78.9011\u001b[0m       52.2689  0.0263\n",
      "      4       \u001b[36m66.8954\u001b[0m       32.1523  0.0207\n",
      "      6       75.3565       70.4105  0.0196\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m49.7565\u001b[0m  0.0020\n",
      "      5       \u001b[36m66.3055\u001b[0m       34.7061  0.0183\n",
      "      5       \u001b[36m76.8157\u001b[0m       49.2310  0.0192\n",
      "      2       49.7565  0.0021\n",
      "      6       \u001b[36m74.6032\u001b[0m       71.8051  0.0238\n",
      "      7       \u001b[36m61.3942\u001b[0m       67.9548  0.0247\n",
      "      3       49.7565  0.0068\n",
      "      4       49.7565  0.0019\n",
      "      5       49.7565  0.0017\n",
      "      7       71.0296       73.5804  0.0212\n",
      "      6       49.7565  0.0016\n",
      "      7       49.7565  0.0016\n",
      "      8       49.7565  0.0015\n",
      "      6       \u001b[36m65.7529\u001b[0m       42.7860  0.0183\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m54.3401\u001b[0m  0.0168\n",
      "      9       49.7565  0.0015\n",
      "      2       54.3401  0.0018\n",
      "     10       49.7565  0.0015\n",
      "Restoring best model from epoch 1.\n",
      "      6       \u001b[36m76.3052\u001b[0m       46.7969  0.0209\n",
      "      3       54.3401  0.0015\n",
      "      4       54.3401  0.0015\n",
      "      5       54.3401  0.0015\n",
      "      8       \u001b[36m57.5774\u001b[0m       68.2539  0.0222\n",
      "      6       54.3401  0.0015\n",
      "      7       54.3401  0.0017\n",
      "      7       \u001b[36m71.8591\u001b[0m       72.9864  0.0260\n",
      "      8       54.3401  0.0014\n",
      "      9       54.3401  0.0014\n",
      "     10       54.3401  0.0014\n",
      "      8       \u001b[36m70.6445\u001b[0m       76.7043  0.0234\n",
      "Restoring best model from epoch 1.\n",
      "      7       \u001b[36m65.7009\u001b[0m       44.8774  0.0208\n",
      "      7       \u001b[36m73.6970\u001b[0m       47.6990  0.0234\n",
      "      8       72.6466       78.7322  0.0248\n",
      "      9       \u001b[36m70.5915\u001b[0m       76.8254  0.0187\n",
      "      9       60.0264       68.5052  0.0351\n",
      "      8       \u001b[36m72.9575\u001b[0m       48.4951  0.0184      8       \u001b[36m64.3259\u001b[0m       40.0914  0.0238\n",
      "\n",
      "     10       \u001b[36m66.6079\u001b[0m       77.1060  0.0193\n",
      "Restoring best model from epoch 1.\n",
      "      9       \u001b[36m69.1274\u001b[0m       87.9682  0.0216\n",
      "     10       59.4307       69.8250  0.0189\n",
      "Restoring best model from epoch 1.\n",
      "      9       \u001b[36m72.0941\u001b[0m       48.7326  0.0184\n",
      "      9       \u001b[36m60.2900\u001b[0m       38.9036  0.0287\n",
      "     10       69.4857       93.2234  0.0190\n",
      "Restoring best model from epoch 1.\n",
      "     10       \u001b[36m70.8649\u001b[0m       47.7876  0.0189\n",
      "Restoring best model from epoch 3.\n",
      "     10       62.8850       41.5600  0.0186\n",
      "Restoring best model from epoch 2.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m89.8284\u001b[0m       \u001b[32m55.9932\u001b[0m  0.0262\n",
      "      2       \u001b[36m86.9492\u001b[0m       58.1476  0.0306\n",
      "      3       \u001b[36m76.8322\u001b[0m       \u001b[32m54.9231\u001b[0m  0.0291\n",
      "      4       \u001b[36m76.6607\u001b[0m       59.7144  0.0264\n",
      "      5       \u001b[36m74.2142\u001b[0m       57.2657  0.0268\n",
      "      6       \u001b[36m70.1827\u001b[0m       55.2283  0.0272\n",
      "      7       71.2592       \u001b[32m53.7724\u001b[0m  0.0271\n",
      "      8       \u001b[36m68.6630\u001b[0m       \u001b[32m52.8778\u001b[0m  0.0494\n",
      "      9       69.1566       52.9262  0.0267\n",
      "     10       69.1414       \u001b[32m51.8647\u001b[0m  0.0264\n",
      "y_train breslow final [-1.000e+00 -6.000e+00 -6.000e+00 -8.000e+00 -9.000e+00 -1.000e+01\n",
      "  1.400e+01 -1.500e+01  1.900e+01 -2.000e+01 -2.200e+01  2.700e+01\n",
      " -3.000e+01  3.100e+01  3.400e+01  3.600e+01 -4.400e+01  5.200e+01\n",
      "  5.600e+01  6.500e+01  6.700e+01 -7.900e+01  9.100e+01  9.100e+01\n",
      "  9.700e+01  1.010e+02  1.020e+02  1.070e+02  1.150e+02  1.290e+02\n",
      " -1.370e+02  1.400e+02  1.530e+02 -1.700e+02 -1.800e+02 -1.830e+02\n",
      "  1.950e+02  2.140e+02 -2.190e+02  2.230e+02 -2.290e+02  2.330e+02\n",
      "  2.470e+02 -2.480e+02 -2.500e+02 -2.600e+02  2.620e+02  2.620e+02\n",
      "  2.720e+02  2.780e+02  2.790e+02  2.830e+02 -2.910e+02  2.960e+02\n",
      "  2.990e+02  3.000e+02 -3.030e+02  3.030e+02  3.040e+02  3.080e+02\n",
      " -3.140e+02  3.150e+02 -3.220e+02 -3.270e+02 -3.280e+02 -3.300e+02\n",
      " -3.410e+02  3.440e+02 -3.450e+02 -3.470e+02  3.470e+02  3.490e+02\n",
      " -3.540e+02 -3.590e+02  3.590e+02 -3.610e+02 -3.630e+02 -3.650e+02\n",
      "  3.660e+02 -3.660e+02 -3.720e+02  3.730e+02  3.810e+02 -3.820e+02\n",
      " -3.870e+02  3.940e+02 -3.950e+02 -3.950e+02 -3.960e+02 -3.990e+02\n",
      " -4.000e+02 -4.060e+02 -4.060e+02 -4.080e+02 -4.090e+02  4.120e+02\n",
      "  4.150e+02 -4.150e+02  4.160e+02  4.190e+02 -4.250e+02 -4.270e+02\n",
      " -4.300e+02  4.320e+02 -4.360e+02 -4.440e+02 -4.470e+02 -4.490e+02\n",
      "  4.520e+02 -4.530e+02 -4.580e+02 -4.680e+02  4.690e+02 -4.720e+02\n",
      " -4.740e+02 -4.760e+02 -4.780e+02 -4.860e+02 -5.000e+02 -5.070e+02\n",
      " -5.120e+02 -5.190e+02 -5.200e+02 -5.200e+02  5.350e+02 -5.380e+02\n",
      "  5.470e+02 -5.540e+02 -5.550e+02 -5.550e+02  5.550e+02  5.560e+02\n",
      "  5.580e+02 -5.620e+02 -5.640e+02 -5.660e+02 -5.700e+02 -5.740e+02\n",
      " -5.750e+02 -5.790e+02  5.810e+02 -5.850e+02 -5.880e+02 -5.940e+02\n",
      "  5.960e+02 -6.010e+02 -6.020e+02 -6.080e+02 -6.080e+02  6.120e+02\n",
      " -6.150e+02 -6.210e+02 -6.300e+02 -6.310e+02 -6.320e+02  6.330e+02\n",
      " -6.360e+02 -6.380e+02  6.390e+02 -6.400e+02  6.430e+02 -6.440e+02\n",
      "  6.490e+02 -6.550e+02 -6.580e+02  6.600e+02 -6.620e+02 -6.710e+02\n",
      " -6.730e+02 -6.800e+02  6.880e+02 -6.930e+02  6.930e+02 -6.980e+02\n",
      " -6.980e+02 -7.010e+02 -7.060e+02  7.110e+02 -7.190e+02 -7.220e+02\n",
      " -7.280e+02 -7.290e+02 -7.440e+02 -7.470e+02  7.570e+02 -7.630e+02\n",
      "  7.650e+02  7.680e+02  7.700e+02 -7.800e+02 -7.820e+02 -7.840e+02\n",
      " -7.990e+02  8.020e+02 -8.100e+02 -8.160e+02 -8.190e+02 -8.290e+02\n",
      "  8.370e+02 -8.480e+02  8.480e+02 -8.490e+02 -8.540e+02 -8.790e+02\n",
      " -8.980e+02 -8.980e+02  8.990e+02 -9.060e+02 -9.100e+02 -9.170e+02\n",
      " -9.250e+02 -9.280e+02  9.310e+02 -9.420e+02 -9.510e+02  1.005e+03\n",
      " -1.049e+03 -1.066e+03 -1.067e+03 -1.085e+03 -1.085e+03  1.088e+03\n",
      " -1.091e+03 -1.098e+03 -1.115e+03  1.135e+03  1.149e+03  1.210e+03\n",
      " -1.219e+03  1.229e+03 -1.231e+03 -1.233e+03 -1.241e+03  1.271e+03\n",
      " -1.295e+03 -1.302e+03 -1.339e+03 -1.345e+03 -1.363e+03  1.372e+03\n",
      "  1.386e+03  1.397e+03  1.423e+03 -1.452e+03  1.490e+03 -1.495e+03\n",
      " -1.516e+03 -1.531e+03 -1.553e+03  1.560e+03  1.622e+03  1.624e+03\n",
      " -1.636e+03  1.685e+03  1.694e+03 -1.711e+03 -1.718e+03 -1.731e+03\n",
      " -1.779e+03  1.791e+03 -1.804e+03  1.852e+03 -1.855e+03 -1.900e+03\n",
      " -1.970e+03 -1.989e+03 -2.015e+03 -2.028e+03 -2.102e+03  2.116e+03\n",
      "  2.131e+03 -2.184e+03 -2.202e+03 -2.245e+03 -2.301e+03 -2.301e+03\n",
      " -2.317e+03 -2.324e+03 -2.412e+03 -2.442e+03  2.456e+03  2.486e+03\n",
      " -2.513e+03  2.532e+03 -2.746e+03 -2.752e+03  3.125e+03  3.258e+03\n",
      " -3.308e+03 -3.437e+03 -3.478e+03 -3.675e+03]\n",
      "Concordance Index 0.6504265960576641\n",
      "Integrated Brier Score: 0.23376135153022182\n",
      "y_train breslow final [-1.000e+00 -6.000e+00 -6.000e+00 -8.000e+00 -9.000e+00 -1.000e+01\n",
      "  1.400e+01 -1.500e+01  1.900e+01 -2.000e+01 -2.200e+01  2.700e+01\n",
      " -3.000e+01  3.100e+01  3.400e+01  3.600e+01 -4.400e+01  5.200e+01\n",
      "  5.600e+01  6.500e+01  6.700e+01 -7.900e+01  9.100e+01  9.100e+01\n",
      "  9.700e+01  1.010e+02  1.020e+02  1.070e+02  1.150e+02  1.290e+02\n",
      " -1.370e+02  1.400e+02  1.530e+02 -1.700e+02 -1.800e+02 -1.830e+02\n",
      "  1.950e+02  2.140e+02 -2.190e+02  2.230e+02 -2.290e+02  2.330e+02\n",
      "  2.470e+02 -2.480e+02 -2.500e+02 -2.600e+02  2.620e+02  2.620e+02\n",
      "  2.720e+02  2.780e+02  2.790e+02  2.830e+02 -2.910e+02  2.960e+02\n",
      "  2.990e+02  3.000e+02 -3.030e+02  3.030e+02  3.040e+02  3.080e+02\n",
      " -3.140e+02  3.150e+02 -3.220e+02 -3.270e+02 -3.280e+02 -3.300e+02\n",
      " -3.410e+02  3.440e+02 -3.450e+02 -3.470e+02  3.470e+02  3.490e+02\n",
      " -3.540e+02 -3.590e+02  3.590e+02 -3.610e+02 -3.630e+02 -3.650e+02\n",
      "  3.660e+02 -3.660e+02 -3.720e+02  3.730e+02  3.810e+02 -3.820e+02\n",
      " -3.870e+02  3.940e+02 -3.950e+02 -3.950e+02 -3.960e+02 -3.990e+02\n",
      " -4.000e+02 -4.060e+02 -4.060e+02 -4.080e+02 -4.090e+02  4.120e+02\n",
      "  4.150e+02 -4.150e+02  4.160e+02  4.190e+02 -4.250e+02 -4.270e+02\n",
      " -4.300e+02  4.320e+02 -4.360e+02 -4.440e+02 -4.470e+02 -4.490e+02\n",
      "  4.520e+02 -4.530e+02 -4.580e+02 -4.680e+02  4.690e+02 -4.720e+02\n",
      " -4.740e+02 -4.760e+02 -4.780e+02 -4.860e+02 -5.000e+02 -5.070e+02\n",
      " -5.120e+02 -5.190e+02 -5.200e+02 -5.200e+02  5.350e+02 -5.380e+02\n",
      "  5.470e+02 -5.540e+02 -5.550e+02 -5.550e+02  5.550e+02  5.560e+02\n",
      "  5.580e+02 -5.620e+02 -5.640e+02 -5.660e+02 -5.700e+02 -5.740e+02\n",
      " -5.750e+02 -5.790e+02  5.810e+02 -5.850e+02 -5.880e+02 -5.940e+02\n",
      "  5.960e+02 -6.010e+02 -6.020e+02 -6.080e+02 -6.080e+02  6.120e+02\n",
      " -6.150e+02 -6.210e+02 -6.300e+02 -6.310e+02 -6.320e+02  6.330e+02\n",
      " -6.360e+02 -6.380e+02  6.390e+02 -6.400e+02  6.430e+02 -6.440e+02\n",
      "  6.490e+02 -6.550e+02 -6.580e+02  6.600e+02 -6.620e+02 -6.710e+02\n",
      " -6.730e+02 -6.800e+02  6.880e+02 -6.930e+02  6.930e+02 -6.980e+02\n",
      " -6.980e+02 -7.010e+02 -7.060e+02  7.110e+02 -7.190e+02 -7.220e+02\n",
      " -7.280e+02 -7.290e+02 -7.440e+02 -7.470e+02  7.570e+02 -7.630e+02\n",
      "  7.650e+02  7.680e+02  7.700e+02 -7.800e+02 -7.820e+02 -7.840e+02\n",
      " -7.990e+02  8.020e+02 -8.100e+02 -8.160e+02 -8.190e+02 -8.290e+02\n",
      "  8.370e+02 -8.480e+02  8.480e+02 -8.490e+02 -8.540e+02 -8.790e+02\n",
      " -8.980e+02 -8.980e+02  8.990e+02 -9.060e+02 -9.100e+02 -9.170e+02\n",
      " -9.250e+02 -9.280e+02  9.310e+02 -9.420e+02 -9.510e+02  1.005e+03\n",
      " -1.049e+03 -1.066e+03 -1.067e+03 -1.085e+03 -1.085e+03  1.088e+03\n",
      " -1.091e+03 -1.098e+03 -1.115e+03  1.135e+03  1.149e+03  1.210e+03\n",
      " -1.219e+03  1.229e+03 -1.231e+03 -1.233e+03 -1.241e+03  1.271e+03\n",
      " -1.295e+03 -1.302e+03 -1.339e+03 -1.345e+03 -1.363e+03  1.372e+03\n",
      "  1.386e+03  1.397e+03  1.423e+03 -1.452e+03  1.490e+03 -1.495e+03\n",
      " -1.516e+03 -1.531e+03 -1.553e+03  1.560e+03  1.622e+03  1.624e+03\n",
      " -1.636e+03  1.685e+03  1.694e+03 -1.711e+03 -1.718e+03 -1.731e+03\n",
      " -1.779e+03  1.791e+03 -1.804e+03  1.852e+03 -1.855e+03 -1.900e+03\n",
      " -1.970e+03 -1.989e+03 -2.015e+03 -2.028e+03 -2.102e+03  2.116e+03\n",
      "  2.131e+03 -2.184e+03 -2.202e+03 -2.245e+03 -2.301e+03 -2.301e+03\n",
      " -2.317e+03 -2.324e+03 -2.412e+03 -2.442e+03  2.456e+03  2.486e+03\n",
      " -2.513e+03  2.532e+03 -2.746e+03 -2.752e+03  3.125e+03  3.258e+03\n",
      " -3.308e+03 -3.437e+03 -3.478e+03 -3.675e+03]\n",
      "durations 6.0 2728.0\n",
      "Concordance Index 0.46197874080130824\n",
      "Integrated Brier Score: 0.2587434531376844\n",
      "split gex_?|100130426      float32\n",
      "gex_?|100133144      float32\n",
      "gex_?|100134869      float32\n",
      "gex_?|10357          float32\n",
      "gex_?|10431          float32\n",
      "                      ...   \n",
      "gex_ZYG11A|440590    float32\n",
      "gex_ZYG11B|79699     float32\n",
      "gex_ZYX|7791         float32\n",
      "gex_ZZEF1|23140      float32\n",
      "gex_ZZZ3|26009       float32\n",
      "Length: 20531, dtype: object\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m78.4935\u001b[0m       \u001b[32m65.2231\u001b[0m  0.0539\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2       \u001b[36m77.7246\u001b[0m       72.7132  0.0410\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m86.0741\u001b[0m      \u001b[32m104.8977\u001b[0m  0.0677\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      3       \u001b[36m65.3192\u001b[0m       72.2444  0.0393\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m123.1672\u001b[0m  0.0032\n",
      "      2      123.1672  0.0022\n",
      "      2       91.3622       \u001b[32m90.1479\u001b[0m  0.0407\n",
      "      3      123.1672  0.0023\n",
      "      4      123.1672  0.0021\n",
      "      5      123.1672  0.0029\n",
      "      6      123.1672  0.0022\n",
      "      7      123.1672  0.0021\n",
      "      8      123.1672  0.0023\n",
      "      9      123.1672  0.0030\n",
      "     10      123.1672  0.0033\n",
      "Restoring best model from epoch 1.\n",
      "      4       67.6041       \u001b[32m60.6803\u001b[0m  0.0402\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      3       \u001b[36m78.1194\u001b[0m       94.4173  0.0483\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m126.3815\u001b[0m  0.0033\n",
      "      2      126.3815  0.0023\n",
      "      3      126.3815  0.0022\n",
      "      4      126.3815  0.0021\n",
      "      5      126.3815  0.0021\n",
      "      6      126.3815  0.0021\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      7      126.3815  0.0023\n",
      "      5       \u001b[36m64.5945\u001b[0m       60.8919  0.0362\n",
      "      8      126.3815  0.0023\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m80.4462\u001b[0m  0.0030\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m125.6218\u001b[0m  0.0032\n",
      "      9      126.3815  0.0023\n",
      "      2       80.4462  0.0024\n",
      "      2      125.6218  0.0026\n",
      "     10      126.3815  0.0025\n",
      "Restoring best model from epoch 1.\n",
      "      3       80.4462  0.0021\n",
      "      3      125.6218  0.0022\n",
      "      4       80.4462  0.0020\n",
      "      4      125.6218  0.0022\n",
      "      5       80.4462  0.0019\n",
      "      5      125.6218  0.0021\n",
      "      6      125.6218  0.0021\n",
      "      6       80.4462  0.0038\n",
      "      7      125.6218  0.0021\n",
      "      4       \u001b[36m76.5230\u001b[0m       93.5324  0.0365\n",
      "      7       80.4462  0.0023\n",
      "      8      125.6218  0.0021\n",
      "      8       80.4462  0.0020\n",
      "      9      125.6218  0.0021\n",
      "      9       80.4462  0.0019\n",
      "     10      125.6218  0.0021\n",
      "Restoring best model from epoch 1.\n",
      "     10       80.4462  0.0019\n",
      "Restoring best model from epoch 1.\n",
      "      6       \u001b[36m62.0000\u001b[0m       61.9176  0.0357\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m114.7610\u001b[0m  0.0032\n",
      "      2      114.7610  0.0022\n",
      "      3      114.7610  0.0021\n",
      "      4      114.7610  0.0020\n",
      "      5       \u001b[36m76.2381\u001b[0m       92.6502  0.0381\n",
      "      5      114.7610  0.0023\n",
      "      6      114.7610  0.0020\n",
      "      7      114.7610  0.0020\n",
      "      8      114.7610  0.0020\n",
      "      9      114.7610  0.0021\n",
      "     10      114.7610  0.0024\n",
      "Restoring best model from epoch 1.\n",
      "      7       \u001b[36m61.5020\u001b[0m       64.0410  0.0359\n",
      "      6       \u001b[36m73.2318\u001b[0m       93.4207  0.0362\n",
      "      8       \u001b[36m60.3643\u001b[0m       64.0837  0.0340\n",
      "      7       \u001b[36m72.6310\u001b[0m       94.7481  0.0382\n",
      "      9       61.4115       61.9762  0.0329\n",
      "      8       \u001b[36m72.5930\u001b[0m       93.7042  0.0347\n",
      "     10       \u001b[36m60.0989\u001b[0m       61.9289  0.0320\n",
      "Restoring best model from epoch 4.\n",
      "      9       \u001b[36m70.3215\u001b[0m       95.5987  0.0333\n",
      "     10       \u001b[36m69.4385\u001b[0m       99.6681  0.0337\n",
      "Restoring best model from epoch 2.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m132.4423\u001b[0m  0.0035\n",
      "      2      132.4423  0.0035\n",
      "      3      132.4423  0.0029\n",
      "      4      132.4423  0.0029\n",
      "      5      132.4423  0.0034\n",
      "      6      132.4423  0.0032\n",
      "      7      132.4423  0.0030\n",
      "      8      132.4423  0.0029\n",
      "      9      132.4423  0.0033\n",
      "     10      132.4423  0.0038\n",
      "Restoring best model from epoch 1.\n",
      "y_train breslow final [  -11.   -15.    18.    19.    22.   -24.   -28.    33.   -35.   -44.\n",
      "   -44.   -50.   -60.   -62.    62.    74.   -79.   -84.    87.    91.\n",
      "    97.    99.   116.   118.  -119.   121.   124.  -129.  -131.  -133.\n",
      "  -134.   139.  -139.  -141.  -151.   154.   161.   164.  -165.   167.\n",
      "  -174.   176.   179.  -179.  -182.  -184.  -186.   189.   193.  -202.\n",
      "   210.  -225.   237.   243.   244.   244.  -256.   257.   258.   260.\n",
      "  -264.   275.   281.   282.  -285.  -287.   291.   300.   303.  -307.\n",
      "   307.   308.  -310.   321.   321.  -323.   336.   339.   340.   343.\n",
      "  -351.  -353.   354.  -354.  -365.   370.  -372.   375.  -377.  -385.\n",
      "   385.  -385.  -408.   409.  -409.  -411.   414.  -414.  -415.  -417.\n",
      "  -418.  -422.  -423.  -424.  -426.  -426.  -427.   428.   434.  -435.\n",
      "  -435.  -440.   442.  -442.   444.   444.  -446.  -448.  -455.   460.\n",
      "  -466.   468.  -476.  -476.  -476.  -477.  -478.  -481.  -486.  -487.\n",
      "   488.  -499.   500.   503.  -515.  -520.  -522.  -526.  -534.  -536.\n",
      "  -537.  -539.  -540.  -541.  -545.  -546.   550.  -551.  -553.   561.\n",
      "  -564.  -565.  -567.  -568.  -568.  -573.   574.  -578.  -578.  -578.\n",
      "   586.  -591.   593.   594.  -595.  -596.   598.  -599.  -600.  -601.\n",
      "  -603.  -603.  -603.   607.  -608.  -609.  -610.  -617.  -624.   624.\n",
      "   625.  -626.   626.  -626.  -629.  -630.  -634.  -651.  -652.  -652.\n",
      "  -653.   656.  -658.   666.  -669.  -670.  -670.  -674.   677.  -677.\n",
      "  -683.  -689.  -690.   694.   697.  -701.   701.   702.  -704.  -704.\n",
      "  -705.   711.  -719.   719.  -724.   731.   737.  -741.  -747.   760.\n",
      "   761.  -761.  -775.   777.  -791.  -791.   800.  -805.   807.  -813.\n",
      "  -824.   826.  -829.  -830.  -839.  -842.  -845.  -852.   855.  -862.\n",
      "  -863.  -864.   864.  -866.   869.  -872.   879.  -882.  -882.  -889.\n",
      "   896.  -896.   896.  -904.   905.  -910.  -912.  -913.   922.  -930.\n",
      "  -938.  -944.  -947.  -949.   949.   952.   976.  -977.  -988.  -993.\n",
      "   995.  -997. -1013.  1026. -1036. -1040.  1043.  1046. -1060. -1071.\n",
      " -1072.  1073. -1079.  1081. -1097.  1115. -1118. -1119. -1125. -1126.\n",
      " -1126. -1130. -1148. -1157. -1159. -1163.  1167.  1171. -1175. -1178.\n",
      " -1189.  1194.  1209.  1215. -1216.  1229. -1233.  1235. -1246.  1258.\n",
      "  1265.  1268. -1272. -1280.  1288. -1289.  1293. -1301. -1305. -1324.\n",
      " -1351.  1357. -1367. -1369.  1379. -1400.  1421. -1429. -1431. -1432.\n",
      " -1442.  1454. -1479.  1498.  1499.  1501.  1516. -1523.  1528.  1531.\n",
      " -1559. -1617. -1621.  1622.  1632.  1653. -1683. -1700.  1725. -1728.\n",
      " -1750.  1778.  1790.  1798.  1830. -1847. -1864. -1870. -1893. -1932.\n",
      " -1965. -1974.  2027. -2065. -2067. -2109. -2137. -2161.  2174. -2199.\n",
      " -2224. -2248. -2261.  2318. -2368. -2449. -2488. -2515. -2595. -2616.\n",
      "  2617.  2620. -2696. -2823. -2832. -2973. -3059. -3094.  3169. -3261.\n",
      " -3305.  3361. -3674. -3759. -3940. -4765.  4961. -4992. -6732. -7062.\n",
      " -7248.]\n",
      "Concordance Index 0.44903106343581395\n",
      "Integrated Brier Score: 0.34839459487483165\n",
      "y_train breslow final [  -11.   -15.    18.    19.    22.   -24.   -28.    33.   -35.   -44.\n",
      "   -44.   -50.   -60.   -62.    62.    74.   -79.   -84.    87.    91.\n",
      "    97.    99.   116.   118.  -119.   121.   124.  -129.  -131.  -133.\n",
      "  -134.   139.  -139.  -141.  -151.   154.   161.   164.  -165.   167.\n",
      "  -174.   176.   179.  -179.  -182.  -184.  -186.   189.   193.  -202.\n",
      "   210.  -225.   237.   243.   244.   244.  -256.   257.   258.   260.\n",
      "  -264.   275.   281.   282.  -285.  -287.   291.   300.   303.  -307.\n",
      "   307.   308.  -310.   321.   321.  -323.   336.   339.   340.   343.\n",
      "  -351.  -353.   354.  -354.  -365.   370.  -372.   375.  -377.  -385.\n",
      "   385.  -385.  -408.   409.  -409.  -411.   414.  -414.  -415.  -417.\n",
      "  -418.  -422.  -423.  -424.  -426.  -426.  -427.   428.   434.  -435.\n",
      "  -435.  -440.   442.  -442.   444.   444.  -446.  -448.  -455.   460.\n",
      "  -466.   468.  -476.  -476.  -476.  -477.  -478.  -481.  -486.  -487.\n",
      "   488.  -499.   500.   503.  -515.  -520.  -522.  -526.  -534.  -536.\n",
      "  -537.  -539.  -540.  -541.  -545.  -546.   550.  -551.  -553.   561.\n",
      "  -564.  -565.  -567.  -568.  -568.  -573.   574.  -578.  -578.  -578.\n",
      "   586.  -591.   593.   594.  -595.  -596.   598.  -599.  -600.  -601.\n",
      "  -603.  -603.  -603.   607.  -608.  -609.  -610.  -617.  -624.   624.\n",
      "   625.  -626.   626.  -626.  -629.  -630.  -634.  -651.  -652.  -652.\n",
      "  -653.   656.  -658.   666.  -669.  -670.  -670.  -674.   677.  -677.\n",
      "  -683.  -689.  -690.   694.   697.  -701.   701.   702.  -704.  -704.\n",
      "  -705.   711.  -719.   719.  -724.   731.   737.  -741.  -747.   760.\n",
      "   761.  -761.  -775.   777.  -791.  -791.   800.  -805.   807.  -813.\n",
      "  -824.   826.  -829.  -830.  -839.  -842.  -845.  -852.   855.  -862.\n",
      "  -863.  -864.   864.  -866.   869.  -872.   879.  -882.  -882.  -889.\n",
      "   896.  -896.   896.  -904.   905.  -910.  -912.  -913.   922.  -930.\n",
      "  -938.  -944.  -947.  -949.   949.   952.   976.  -977.  -988.  -993.\n",
      "   995.  -997. -1013.  1026. -1036. -1040.  1043.  1046. -1060. -1071.\n",
      " -1072.  1073. -1079.  1081. -1097.  1115. -1118. -1119. -1125. -1126.\n",
      " -1126. -1130. -1148. -1157. -1159. -1163.  1167.  1171. -1175. -1178.\n",
      " -1189.  1194.  1209.  1215. -1216.  1229. -1233.  1235. -1246.  1258.\n",
      "  1265.  1268. -1272. -1280.  1288. -1289.  1293. -1301. -1305. -1324.\n",
      " -1351.  1357. -1367. -1369.  1379. -1400.  1421. -1429. -1431. -1432.\n",
      " -1442.  1454. -1479.  1498.  1499.  1501.  1516. -1523.  1528.  1531.\n",
      " -1559. -1617. -1621.  1622.  1632.  1653. -1683. -1700.  1725. -1728.\n",
      " -1750.  1778.  1790.  1798.  1830. -1847. -1864. -1870. -1893. -1932.\n",
      " -1965. -1974.  2027. -2065. -2067. -2109. -2137. -2161.  2174. -2199.\n",
      " -2224. -2248. -2261.  2318. -2368. -2449. -2488. -2515. -2595. -2616.\n",
      "  2617.  2620. -2696. -2823. -2832. -2973. -3059. -3094.  3169. -3261.\n",
      " -3305.  3361. -3674. -3759. -3940. -4765.  4961. -4992. -6732. -7062.\n",
      " -7248.]\n",
      "durations 4.0 3635.0\n",
      "Concordance Index 0.32077393075356414\n",
      "Integrated Brier Score: 0.4152810230831341\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m240.5898\u001b[0m  0.0024\n",
      "      2      240.5898  0.0019\n",
      "      3      240.5898  0.0019\n",
      "      4      240.5898  0.0019\n",
      "      5      240.5898  0.0018\n",
      "      6      240.5898  0.0018\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      7      240.5898  0.0033\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m220.3230\u001b[0m  0.0022\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2      220.3230  0.0020\n",
      "      3      220.3230  0.0019\n",
      "      8      240.5898  0.0054\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m281.2724\u001b[0m  0.0023\n",
      "      4      220.3230  0.0018\n",
      "      2      281.2724  0.0019\n",
      "      5      220.3230  0.0020\n",
      "      3      281.2724  0.0018\n",
      "      9      240.5898  0.0052\n",
      "      6      220.3230  0.0018\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      4      281.2724  0.0018\n",
      "     10      240.5898  0.0020\n",
      "Restoring best model from epoch 1.\n",
      "      7      220.3230  0.0018\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      5      281.2724  0.0018\n",
      "      8      220.3230  0.0018\n",
      "      6      281.2724  0.0017\n",
      "      9      220.3230  0.0018\n",
      "      7      281.2724  0.0018\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m80.0431\u001b[0m  0.0030\n",
      "     10      220.3230  0.0018\n",
      "Restoring best model from epoch 1.\n",
      "      8      281.2724  0.0018\n",
      "      9      281.2724  0.0017\n",
      "     10      281.2724  0.0017\n",
      "Restoring best model from epoch 1.\n",
      "      2       80.0431  0.0068\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      3       80.0431  0.0029\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      4       80.0431  0.0027\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m93.1200\u001b[0m  0.0030\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      5       80.0431  0.0027\n",
      "      2       93.1200  0.0027\n",
      "      6       80.0431  0.0026\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m73.4860\u001b[0m  0.0029\n",
      "      3       93.1200  0.0027\n",
      "      2       73.4860  0.0027\n",
      "      4       93.1200  0.0027\n",
      "      7       80.0431  0.0050\n",
      "      3       73.4860  0.0026\n",
      "      5       93.1200  0.0027\n",
      "      4       73.4860  0.0029\n",
      "      6       93.1200  0.0026\n",
      "      8       80.0431  0.0065\n",
      "      5       73.4860  0.0027\n",
      "      7       93.1200  0.0026\n",
      "      9       80.0431  0.0028\n",
      "      6       73.4860  0.0026\n",
      "      8       93.1200  0.0028\n",
      "     10       80.0431  0.0027\n",
      "Restoring best model from epoch 1.\n",
      "      9       93.1200  0.0026\n",
      "      7       73.4860  0.0028\n",
      "     10       93.1200  0.0026\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      8       73.4860  0.0027\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      9       73.4860  0.0029\n",
      "     10       73.4860  0.0026\n",
      "Restoring best model from epoch 1.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m88.2628\u001b[0m  0.0029\n",
      "      2       88.2628  0.0033\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      3       88.2628  0.0027\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      4       88.2628  0.0027\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m227.6306\u001b[0m  0.0027\n",
      "      2      227.6306  0.0020\n",
      "      5       88.2628  0.0029\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m92.3360\u001b[0m  0.0029\n",
      "      3      227.6306  0.0018\n",
      "      6       88.2628  0.0032\n",
      "      4      227.6306  0.0018\n",
      "      2       92.3360  0.0030\n",
      "      5      227.6306  0.0017\n",
      "      7       88.2628  0.0027\n",
      "      6      227.6306  0.0018\n",
      "      8       88.2628  0.0026\n",
      "      3       92.3360  0.0045\n",
      "      7      227.6306  0.0017\n",
      "      9       88.2628  0.0030\n",
      "      8      227.6306  0.0022\n",
      "     10       88.2628  0.0029\n",
      "Restoring best model from epoch 1.\n",
      "      4       92.3360  0.0074\n",
      "      9      227.6306  0.0047\n",
      "     10      227.6306  0.0020\n",
      "Restoring best model from epoch 1.\n",
      "      5       92.3360  0.0027\n",
      "      6       92.3360  0.0027\n",
      "      7       92.3360  0.0026\n",
      "      8       92.3360  0.0043\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      9       92.3360  0.0050\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m242.5562\u001b[0m  0.0026\n",
      "     10       92.3360  0.0031\n",
      "      2      242.5562  0.0017\n",
      "Restoring best model from epoch 1.\n",
      "      3      242.5562  0.0017\n",
      "      4      242.5562  0.0017\n",
      "      5      242.5562  0.0017\n",
      "      6      242.5562  0.0017\n",
      "      7      242.5562  0.0020\n",
      "      8      242.5562  0.0017\n",
      "      9      242.5562  0.0017\n",
      "     10      242.5562  0.0017\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m88.5232\u001b[0m  0.0031\n",
      "      2       88.5232  0.0034\n",
      "      3       88.5232  0.0038\n",
      "      4       88.5232  0.0036\n",
      "      5       88.5232  0.0037\n",
      "      6       88.5232  0.0038\n",
      "      7       88.5232  0.0036\n",
      "      8       88.5232  0.0034\n",
      "      9       88.5232  0.0053\n",
      "     10       88.5232  0.0050\n",
      "Restoring best model from epoch 1.\n",
      "y_train breslow final [ 4.000e+00 -1.100e+01 -1.300e+01 -1.400e+01 -1.500e+01  1.900e+01\n",
      "  2.200e+01 -2.800e+01  3.300e+01 -3.600e+01  3.800e+01 -4.800e+01\n",
      "  5.800e+01 -6.200e+01  7.400e+01 -7.900e+01 -8.400e+01 -8.400e+01\n",
      "  8.700e+01  9.100e+01  9.700e+01  1.160e+02  1.190e+02 -1.190e+02\n",
      "  1.240e+02 -1.290e+02 -1.310e+02 -1.340e+02  1.390e+02 -1.390e+02\n",
      " -1.410e+02 -1.510e+02  1.610e+02  1.640e+02  1.670e+02  1.710e+02\n",
      "  1.730e+02 -1.740e+02  1.760e+02 -1.820e+02 -1.840e+02 -1.860e+02\n",
      "  1.870e+02  1.890e+02  1.930e+02  2.100e+02 -2.180e+02 -2.240e+02\n",
      " -2.250e+02 -2.300e+02  2.430e+02  2.440e+02  2.500e+02 -2.560e+02\n",
      "  2.570e+02 -2.600e+02 -2.640e+02  2.680e+02  2.740e+02  2.810e+02\n",
      "  2.820e+02  2.910e+02  3.000e+02  3.030e+02 -3.070e+02  3.080e+02\n",
      " -3.100e+02  3.210e+02  3.210e+02 -3.230e+02  3.360e+02  3.360e+02\n",
      "  3.390e+02  3.430e+02 -3.510e+02  3.540e+02 -3.540e+02 -3.650e+02\n",
      "  3.700e+02  3.750e+02  3.760e+02 -3.850e+02  3.850e+02 -3.850e+02\n",
      "  4.090e+02 -4.090e+02 -4.110e+02  4.140e+02 -4.150e+02 -4.150e+02\n",
      " -4.160e+02 -4.170e+02 -4.180e+02 -4.220e+02 -4.260e+02 -4.260e+02\n",
      " -4.270e+02  4.280e+02  4.290e+02  4.340e+02  4.340e+02 -4.350e+02\n",
      " -4.350e+02 -4.350e+02  4.420e+02  4.440e+02  4.440e+02 -4.460e+02\n",
      " -4.480e+02 -4.550e+02 -4.570e+02  4.570e+02  4.600e+02 -4.620e+02\n",
      "  4.640e+02 -4.660e+02 -4.670e+02  4.680e+02  4.680e+02  4.690e+02\n",
      " -4.760e+02 -4.760e+02 -4.780e+02  4.780e+02 -4.810e+02 -4.840e+02\n",
      " -4.860e+02 -4.870e+02  4.880e+02 -4.920e+02 -4.990e+02  5.000e+02\n",
      "  5.030e+02 -5.050e+02 -5.130e+02 -5.150e+02 -5.200e+02 -5.220e+02\n",
      " -5.260e+02 -5.310e+02 -5.360e+02 -5.370e+02 -5.390e+02 -5.400e+02\n",
      " -5.410e+02 -5.460e+02 -5.470e+02  5.500e+02 -5.520e+02 -5.530e+02\n",
      "  5.570e+02  5.610e+02 -5.640e+02 -5.650e+02 -5.670e+02 -5.680e+02\n",
      " -5.680e+02 -5.680e+02 -5.730e+02  5.740e+02 -5.780e+02 -5.780e+02\n",
      " -5.780e+02  5.820e+02  5.860e+02 -5.910e+02 -5.920e+02 -5.960e+02\n",
      " -6.000e+02 -6.030e+02 -6.030e+02 -6.050e+02 -6.050e+02 -6.070e+02\n",
      "  6.070e+02 -6.080e+02 -6.090e+02 -6.100e+02 -6.100e+02 -6.100e+02\n",
      " -6.140e+02  6.140e+02 -6.170e+02 -6.170e+02 -6.240e+02  6.250e+02\n",
      " -6.260e+02  6.260e+02 -6.260e+02  6.280e+02 -6.290e+02 -6.510e+02\n",
      " -6.520e+02 -6.520e+02 -6.530e+02  6.530e+02  6.560e+02 -6.570e+02\n",
      " -6.580e+02 -6.580e+02 -6.640e+02  6.660e+02 -6.690e+02 -6.700e+02\n",
      " -6.740e+02  6.770e+02 -6.770e+02 -6.890e+02 -6.900e+02 -6.910e+02\n",
      "  6.940e+02 -7.010e+02  7.010e+02  7.020e+02 -7.040e+02 -7.040e+02\n",
      " -7.050e+02  7.110e+02 -7.180e+02 -7.190e+02  7.190e+02 -7.240e+02\n",
      " -7.280e+02 -7.300e+02  7.310e+02 -7.390e+02 -7.400e+02 -7.410e+02\n",
      " -7.470e+02  7.600e+02 -7.610e+02  7.610e+02 -7.730e+02 -7.750e+02\n",
      "  7.770e+02 -7.910e+02 -7.910e+02 -7.910e+02 -7.910e+02 -8.000e+02\n",
      " -8.050e+02 -8.060e+02  8.080e+02 -8.240e+02 -8.240e+02 -8.270e+02\n",
      " -8.390e+02 -8.420e+02 -8.450e+02 -8.520e+02  8.550e+02 -8.620e+02\n",
      " -8.630e+02 -8.640e+02  8.640e+02 -8.660e+02 -8.720e+02  8.790e+02\n",
      " -8.820e+02 -8.820e+02 -8.880e+02 -8.890e+02 -8.890e+02  8.960e+02\n",
      "  8.960e+02 -9.040e+02  9.050e+02 -9.100e+02 -9.120e+02 -9.130e+02\n",
      "  9.290e+02 -9.300e+02 -9.380e+02 -9.440e+02  9.490e+02  9.500e+02\n",
      "  9.520e+02 -9.770e+02  9.870e+02 -9.880e+02  9.950e+02  9.950e+02\n",
      " -9.970e+02  9.990e+02 -1.013e+03  1.026e+03 -1.036e+03 -1.040e+03\n",
      "  1.043e+03  1.046e+03 -1.060e+03  1.073e+03 -1.079e+03  1.081e+03\n",
      " -1.097e+03  1.115e+03 -1.118e+03 -1.119e+03 -1.126e+03 -1.130e+03\n",
      "  1.135e+03  1.147e+03 -1.157e+03 -1.159e+03 -1.163e+03  1.171e+03\n",
      " -1.175e+03 -1.178e+03 -1.189e+03 -1.189e+03  1.194e+03  1.197e+03\n",
      "  1.215e+03 -1.216e+03  1.229e+03 -1.233e+03  1.235e+03 -1.239e+03\n",
      " -1.246e+03  1.258e+03 -1.272e+03 -1.280e+03 -1.285e+03  1.288e+03\n",
      " -1.289e+03 -1.301e+03 -1.305e+03 -1.324e+03 -1.333e+03  1.357e+03\n",
      " -1.367e+03 -1.369e+03 -1.400e+03 -1.431e+03 -1.432e+03 -1.442e+03\n",
      "  1.454e+03 -1.474e+03  1.492e+03  1.499e+03  1.501e+03 -1.523e+03\n",
      "  1.528e+03  1.531e+03 -1.559e+03  1.600e+03 -1.617e+03 -1.621e+03\n",
      "  1.632e+03  1.653e+03 -1.683e+03 -1.700e+03  1.725e+03 -1.728e+03\n",
      "  1.778e+03  1.790e+03  1.798e+03  1.830e+03 -1.847e+03 -1.870e+03\n",
      " -1.893e+03 -1.932e+03 -1.974e+03  2.027e+03 -2.065e+03 -2.109e+03\n",
      " -2.137e+03 -2.161e+03  2.174e+03 -2.199e+03 -2.224e+03 -2.248e+03\n",
      " -2.261e+03  2.318e+03 -2.360e+03 -2.368e+03  2.393e+03 -2.488e+03\n",
      " -2.515e+03 -2.590e+03 -2.595e+03 -2.616e+03  2.620e+03 -2.676e+03\n",
      "  2.681e+03 -2.696e+03 -2.823e+03 -2.973e+03 -3.094e+03  3.169e+03\n",
      " -3.261e+03 -3.305e+03  3.361e+03 -3.635e+03 -3.759e+03 -3.940e+03\n",
      "  4.961e+03 -4.992e+03 -6.732e+03 -7.062e+03 -7.248e+03]\n",
      "Concordance Index 0.5331231278574807\n",
      "Integrated Brier Score: 0.198614059458693\n",
      "y_train breslow final [ 4.000e+00 -1.100e+01 -1.300e+01 -1.400e+01 -1.500e+01  1.900e+01\n",
      "  2.200e+01 -2.800e+01  3.300e+01 -3.600e+01  3.800e+01 -4.800e+01\n",
      "  5.800e+01 -6.200e+01  7.400e+01 -7.900e+01 -8.400e+01 -8.400e+01\n",
      "  8.700e+01  9.100e+01  9.700e+01  1.160e+02  1.190e+02 -1.190e+02\n",
      "  1.240e+02 -1.290e+02 -1.310e+02 -1.340e+02  1.390e+02 -1.390e+02\n",
      " -1.410e+02 -1.510e+02  1.610e+02  1.640e+02  1.670e+02  1.710e+02\n",
      "  1.730e+02 -1.740e+02  1.760e+02 -1.820e+02 -1.840e+02 -1.860e+02\n",
      "  1.870e+02  1.890e+02  1.930e+02  2.100e+02 -2.180e+02 -2.240e+02\n",
      " -2.250e+02 -2.300e+02  2.430e+02  2.440e+02  2.500e+02 -2.560e+02\n",
      "  2.570e+02 -2.600e+02 -2.640e+02  2.680e+02  2.740e+02  2.810e+02\n",
      "  2.820e+02  2.910e+02  3.000e+02  3.030e+02 -3.070e+02  3.080e+02\n",
      " -3.100e+02  3.210e+02  3.210e+02 -3.230e+02  3.360e+02  3.360e+02\n",
      "  3.390e+02  3.430e+02 -3.510e+02  3.540e+02 -3.540e+02 -3.650e+02\n",
      "  3.700e+02  3.750e+02  3.760e+02 -3.850e+02  3.850e+02 -3.850e+02\n",
      "  4.090e+02 -4.090e+02 -4.110e+02  4.140e+02 -4.150e+02 -4.150e+02\n",
      " -4.160e+02 -4.170e+02 -4.180e+02 -4.220e+02 -4.260e+02 -4.260e+02\n",
      " -4.270e+02  4.280e+02  4.290e+02  4.340e+02  4.340e+02 -4.350e+02\n",
      " -4.350e+02 -4.350e+02  4.420e+02  4.440e+02  4.440e+02 -4.460e+02\n",
      " -4.480e+02 -4.550e+02 -4.570e+02  4.570e+02  4.600e+02 -4.620e+02\n",
      "  4.640e+02 -4.660e+02 -4.670e+02  4.680e+02  4.680e+02  4.690e+02\n",
      " -4.760e+02 -4.760e+02 -4.780e+02  4.780e+02 -4.810e+02 -4.840e+02\n",
      " -4.860e+02 -4.870e+02  4.880e+02 -4.920e+02 -4.990e+02  5.000e+02\n",
      "  5.030e+02 -5.050e+02 -5.130e+02 -5.150e+02 -5.200e+02 -5.220e+02\n",
      " -5.260e+02 -5.310e+02 -5.360e+02 -5.370e+02 -5.390e+02 -5.400e+02\n",
      " -5.410e+02 -5.460e+02 -5.470e+02  5.500e+02 -5.520e+02 -5.530e+02\n",
      "  5.570e+02  5.610e+02 -5.640e+02 -5.650e+02 -5.670e+02 -5.680e+02\n",
      " -5.680e+02 -5.680e+02 -5.730e+02  5.740e+02 -5.780e+02 -5.780e+02\n",
      " -5.780e+02  5.820e+02  5.860e+02 -5.910e+02 -5.920e+02 -5.960e+02\n",
      " -6.000e+02 -6.030e+02 -6.030e+02 -6.050e+02 -6.050e+02 -6.070e+02\n",
      "  6.070e+02 -6.080e+02 -6.090e+02 -6.100e+02 -6.100e+02 -6.100e+02\n",
      " -6.140e+02  6.140e+02 -6.170e+02 -6.170e+02 -6.240e+02  6.250e+02\n",
      " -6.260e+02  6.260e+02 -6.260e+02  6.280e+02 -6.290e+02 -6.510e+02\n",
      " -6.520e+02 -6.520e+02 -6.530e+02  6.530e+02  6.560e+02 -6.570e+02\n",
      " -6.580e+02 -6.580e+02 -6.640e+02  6.660e+02 -6.690e+02 -6.700e+02\n",
      " -6.740e+02  6.770e+02 -6.770e+02 -6.890e+02 -6.900e+02 -6.910e+02\n",
      "  6.940e+02 -7.010e+02  7.010e+02  7.020e+02 -7.040e+02 -7.040e+02\n",
      " -7.050e+02  7.110e+02 -7.180e+02 -7.190e+02  7.190e+02 -7.240e+02\n",
      " -7.280e+02 -7.300e+02  7.310e+02 -7.390e+02 -7.400e+02 -7.410e+02\n",
      " -7.470e+02  7.600e+02 -7.610e+02  7.610e+02 -7.730e+02 -7.750e+02\n",
      "  7.770e+02 -7.910e+02 -7.910e+02 -7.910e+02 -7.910e+02 -8.000e+02\n",
      " -8.050e+02 -8.060e+02  8.080e+02 -8.240e+02 -8.240e+02 -8.270e+02\n",
      " -8.390e+02 -8.420e+02 -8.450e+02 -8.520e+02  8.550e+02 -8.620e+02\n",
      " -8.630e+02 -8.640e+02  8.640e+02 -8.660e+02 -8.720e+02  8.790e+02\n",
      " -8.820e+02 -8.820e+02 -8.880e+02 -8.890e+02 -8.890e+02  8.960e+02\n",
      "  8.960e+02 -9.040e+02  9.050e+02 -9.100e+02 -9.120e+02 -9.130e+02\n",
      "  9.290e+02 -9.300e+02 -9.380e+02 -9.440e+02  9.490e+02  9.500e+02\n",
      "  9.520e+02 -9.770e+02  9.870e+02 -9.880e+02  9.950e+02  9.950e+02\n",
      " -9.970e+02  9.990e+02 -1.013e+03  1.026e+03 -1.036e+03 -1.040e+03\n",
      "  1.043e+03  1.046e+03 -1.060e+03  1.073e+03 -1.079e+03  1.081e+03\n",
      " -1.097e+03  1.115e+03 -1.118e+03 -1.119e+03 -1.126e+03 -1.130e+03\n",
      "  1.135e+03  1.147e+03 -1.157e+03 -1.159e+03 -1.163e+03  1.171e+03\n",
      " -1.175e+03 -1.178e+03 -1.189e+03 -1.189e+03  1.194e+03  1.197e+03\n",
      "  1.215e+03 -1.216e+03  1.229e+03 -1.233e+03  1.235e+03 -1.239e+03\n",
      " -1.246e+03  1.258e+03 -1.272e+03 -1.280e+03 -1.285e+03  1.288e+03\n",
      " -1.289e+03 -1.301e+03 -1.305e+03 -1.324e+03 -1.333e+03  1.357e+03\n",
      " -1.367e+03 -1.369e+03 -1.400e+03 -1.431e+03 -1.432e+03 -1.442e+03\n",
      "  1.454e+03 -1.474e+03  1.492e+03  1.499e+03  1.501e+03 -1.523e+03\n",
      "  1.528e+03  1.531e+03 -1.559e+03  1.600e+03 -1.617e+03 -1.621e+03\n",
      "  1.632e+03  1.653e+03 -1.683e+03 -1.700e+03  1.725e+03 -1.728e+03\n",
      "  1.778e+03  1.790e+03  1.798e+03  1.830e+03 -1.847e+03 -1.870e+03\n",
      " -1.893e+03 -1.932e+03 -1.974e+03  2.027e+03 -2.065e+03 -2.109e+03\n",
      " -2.137e+03 -2.161e+03  2.174e+03 -2.199e+03 -2.224e+03 -2.248e+03\n",
      " -2.261e+03  2.318e+03 -2.360e+03 -2.368e+03  2.393e+03 -2.488e+03\n",
      " -2.515e+03 -2.590e+03 -2.595e+03 -2.616e+03  2.620e+03 -2.676e+03\n",
      "  2.681e+03 -2.696e+03 -2.823e+03 -2.973e+03 -3.094e+03  3.169e+03\n",
      " -3.261e+03 -3.305e+03  3.361e+03 -3.635e+03 -3.759e+03 -3.940e+03\n",
      "  4.961e+03 -4.992e+03 -6.732e+03 -7.062e+03 -7.248e+03]\n",
      "durations 18.0 4765.0\n",
      "Concordance Index 0.4452164617851416\n",
      "Integrated Brier Score: 0.20887635731409174\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m217.4252\u001b[0m  0.0025\n",
      "      2      217.4252  0.0019\n",
      "      3      217.4252  0.0018\n",
      "      4      217.4252  0.0017\n",
      "      5      217.4252  0.0028\n",
      "      6      217.4252  0.0050\n",
      "      7      217.4252  0.0037\n",
      "      8      217.4252  0.0020\n",
      "      9      217.4252  0.0018\n",
      "     10      217.4252  0.0017\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m169.4287\u001b[0m  0.0029\n",
      "      2      169.4287  0.0021\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      3      169.4287  0.0020\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      4      169.4287  0.0019\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      5      169.4287  0.0018\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m380.2364\u001b[0m  0.0031\n",
      "      6      169.4287  0.0017\n",
      "      2      380.2364  0.0019\n",
      "      7      169.4287  0.0017\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m63.2013\u001b[0m  0.0035\n",
      "      3      380.2364  0.0018\n",
      "      8      169.4287  0.0023\n",
      "      4      380.2364  0.0020\n",
      "      2       63.2013  0.0031\n",
      "      9      169.4287  0.0018\n",
      "      5      380.2364  0.0021\n",
      "     10      169.4287  0.0017\n",
      "Restoring best model from epoch 1.\n",
      "      6      380.2364  0.0022\n",
      "      3       63.2013  0.0065\n",
      "      7      380.2364  0.0044\n",
      "      4       63.2013  0.0028\n",
      "      8      380.2364  0.0032\n",
      "      5       63.2013  0.0027\n",
      "      9      380.2364  0.0018\n",
      "      6       63.2013  0.0027\n",
      "     10      380.2364  0.0019\n",
      "Restoring best model from epoch 1.\n",
      "      7       63.2013  0.0027\n",
      "      8       63.2013  0.0027\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      9       63.2013  0.0027\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "     10       63.2013  0.0027\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m85.7124\u001b[0m  0.0086\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m167.3575\u001b[0m  0.0075\n",
      "      2       85.7124  0.0046\n",
      "      2      167.3575  0.0033\n",
      "      3       85.7124  0.0029\n",
      "      3      167.3575  0.0018\n",
      "      4       85.7124  0.0027\n",
      "      4      167.3575  0.0018\n",
      "      5      167.3575  0.0017\n",
      "      5       85.7124  0.0027\n",
      "      6      167.3575  0.0017\n",
      "      6       85.7124  0.0027\n",
      "      7      167.3575  0.0017\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      8      167.3575  0.0017\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      7       85.7124  0.0027\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      9      167.3575  0.0018\n",
      "      8       85.7124  0.0027\n",
      "     10      167.3575  0.0017\n",
      "Restoring best model from epoch 1.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m171.4089\u001b[0m  0.0028\n",
      "      9       85.7124  0.0027\n",
      "      2      171.4089  0.0020\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m79.3446\u001b[0m  0.0037\n",
      "      3      171.4089  0.0020\n",
      "     10       85.7124  0.0026\n",
      "Restoring best model from epoch 1.\n",
      "      4      171.4089  0.0017\n",
      "      2       79.3446  0.0028\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      5      171.4089  0.0016\n",
      "      3       79.3446  0.0027\n",
      "      6      171.4089  0.0016\n",
      "      7      171.4089  0.0016\n",
      "      4       79.3446  0.0026\n",
      "      8      171.4089  0.0016\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      9      171.4089  0.0016\n",
      "     10      171.4089  0.0016\n",
      "Restoring best model from epoch 1.\n",
      "      5       79.3446  0.0066\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m68.0304\u001b[0m  0.0042\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      6       79.3446  0.0035\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2       68.0304  0.0030\n",
      "      3       68.0304  0.0027\n",
      "      7       79.3446  0.0057\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m68.2078\u001b[0m  0.0041\n",
      "      4       68.0304  0.0026\n",
      "      8       79.3446  0.0030\n",
      "      2       68.2078  0.0027\n",
      "      5       68.0304  0.0026\n",
      "      3       68.2078  0.0026\n",
      "      6       68.0304  0.0026\n",
      "      9       79.3446  0.0046\n",
      "      7       68.0304  0.0026\n",
      "      4       68.2078  0.0026\n",
      "      5       68.2078  0.0026\n",
      "     10       79.3446  0.0051\n",
      "Restoring best model from epoch 1.\n",
      "      6       68.2078  0.0026\n",
      "      8       68.0304  0.0025\n",
      "      7       68.2078  0.0026\n",
      "      9       68.0304  0.0029\n",
      "      8       68.2078  0.0026\n",
      "     10       68.0304  0.0026\n",
      "Restoring best model from epoch 1.\n",
      "      9       68.2078  0.0026\n",
      "     10       68.2078  0.0026\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m103.1231\u001b[0m  0.0038\n",
      "      2      103.1231  0.0035\n",
      "      3      103.1231  0.0036\n",
      "      4      103.1231  0.0032\n",
      "      5      103.1231  0.0034\n",
      "      6      103.1231  0.0042\n",
      "      7      103.1231  0.0054\n",
      "      8      103.1231  0.0058\n",
      "      9      103.1231  0.0186\n",
      "     10      103.1231  0.0058\n",
      "Restoring best model from epoch 1.\n",
      "y_train breslow final [ 4.000e+00 -1.300e+01 -1.400e+01  1.800e+01  1.900e+01 -2.400e+01\n",
      " -2.800e+01  3.300e+01 -3.500e+01 -3.600e+01  3.800e+01 -4.400e+01\n",
      " -4.400e+01 -4.800e+01 -5.000e+01  5.800e+01 -6.000e+01 -6.200e+01\n",
      "  6.200e+01  7.400e+01 -8.400e+01  9.700e+01  9.900e+01  1.160e+02\n",
      "  1.180e+02  1.190e+02  1.210e+02  1.240e+02 -1.330e+02 -1.340e+02\n",
      " -1.390e+02 -1.410e+02  1.540e+02  1.610e+02  1.640e+02 -1.650e+02\n",
      "  1.670e+02  1.710e+02  1.730e+02  1.760e+02  1.790e+02 -1.790e+02\n",
      " -1.820e+02 -1.840e+02 -1.860e+02 -1.860e+02  1.870e+02  1.890e+02\n",
      "  1.930e+02 -2.020e+02  2.100e+02 -2.180e+02 -2.240e+02 -2.250e+02\n",
      " -2.300e+02  2.370e+02  2.440e+02  2.500e+02 -2.560e+02  2.570e+02\n",
      "  2.580e+02  2.600e+02 -2.600e+02 -2.640e+02  2.680e+02  2.740e+02\n",
      "  2.750e+02  2.810e+02 -2.850e+02 -2.870e+02  2.910e+02  3.030e+02\n",
      " -3.070e+02  3.070e+02  3.080e+02  3.210e+02 -3.230e+02  3.360e+02\n",
      "  3.360e+02  3.400e+02  3.430e+02 -3.510e+02 -3.530e+02  3.540e+02\n",
      " -3.540e+02 -3.720e+02  3.750e+02  3.760e+02 -3.770e+02 -3.850e+02\n",
      "  3.850e+02 -4.080e+02 -4.090e+02 -4.110e+02  4.140e+02 -4.140e+02\n",
      " -4.150e+02 -4.150e+02 -4.160e+02 -4.170e+02 -4.180e+02 -4.230e+02\n",
      " -4.240e+02  4.280e+02  4.290e+02  4.340e+02 -4.350e+02 -4.350e+02\n",
      " -4.400e+02  4.420e+02 -4.420e+02  4.440e+02 -4.480e+02 -4.480e+02\n",
      " -4.550e+02 -4.570e+02  4.570e+02 -4.620e+02  4.640e+02 -4.660e+02\n",
      " -4.670e+02  4.680e+02  4.680e+02  4.690e+02 -4.760e+02 -4.760e+02\n",
      " -4.760e+02 -4.770e+02 -4.780e+02  4.780e+02 -4.810e+02 -4.840e+02\n",
      " -4.920e+02 -4.990e+02  5.000e+02  5.030e+02 -5.050e+02 -5.130e+02\n",
      " -5.150e+02 -5.150e+02 -5.200e+02 -5.260e+02 -5.310e+02 -5.340e+02\n",
      " -5.360e+02 -5.370e+02 -5.400e+02 -5.410e+02 -5.450e+02 -5.470e+02\n",
      "  5.500e+02 -5.510e+02 -5.520e+02 -5.530e+02  5.570e+02  5.610e+02\n",
      " -5.650e+02 -5.680e+02 -5.680e+02 -5.680e+02 -5.730e+02  5.740e+02\n",
      "  5.820e+02  5.860e+02 -5.920e+02  5.930e+02  5.940e+02 -5.950e+02\n",
      "  5.980e+02 -5.990e+02 -6.000e+02 -6.010e+02 -6.030e+02 -6.030e+02\n",
      " -6.050e+02 -6.050e+02 -6.070e+02  6.070e+02 -6.080e+02 -6.090e+02\n",
      " -6.100e+02 -6.100e+02 -6.100e+02 -6.140e+02  6.140e+02 -6.170e+02\n",
      " -6.170e+02  6.240e+02  6.250e+02 -6.260e+02  6.260e+02 -6.260e+02\n",
      "  6.280e+02 -6.290e+02 -6.300e+02 -6.340e+02 -6.510e+02 -6.510e+02\n",
      " -6.520e+02 -6.530e+02  6.530e+02  6.560e+02 -6.570e+02 -6.580e+02\n",
      " -6.580e+02 -6.640e+02 -6.690e+02 -6.700e+02 -6.700e+02 -6.740e+02\n",
      " -6.830e+02 -6.900e+02 -6.910e+02  6.970e+02 -7.010e+02  7.010e+02\n",
      " -7.040e+02 -7.040e+02 -7.050e+02 -7.050e+02 -7.180e+02 -7.190e+02\n",
      "  7.190e+02 -7.240e+02 -7.280e+02 -7.300e+02  7.310e+02  7.370e+02\n",
      " -7.390e+02 -7.400e+02 -7.410e+02 -7.470e+02 -7.610e+02  7.610e+02\n",
      " -7.610e+02 -7.730e+02 -7.750e+02  7.770e+02 -7.910e+02 -7.910e+02\n",
      " -7.910e+02 -7.910e+02  8.000e+02 -8.000e+02 -8.060e+02  8.070e+02\n",
      "  8.080e+02 -8.130e+02 -8.240e+02  8.260e+02 -8.270e+02 -8.290e+02\n",
      " -8.300e+02 -8.390e+02 -8.450e+02 -8.520e+02  8.550e+02 -8.630e+02\n",
      "  8.640e+02  8.690e+02 -8.720e+02  8.790e+02 -8.820e+02 -8.820e+02\n",
      " -8.880e+02 -8.890e+02  8.960e+02 -8.960e+02  8.960e+02 -9.040e+02\n",
      " -9.130e+02  9.220e+02  9.290e+02 -9.380e+02 -9.440e+02 -9.470e+02\n",
      " -9.490e+02  9.490e+02  9.500e+02  9.520e+02  9.760e+02 -9.770e+02\n",
      "  9.870e+02 -9.930e+02  9.950e+02  9.990e+02 -1.013e+03  1.026e+03\n",
      " -1.036e+03 -1.040e+03 -1.060e+03 -1.071e+03 -1.072e+03  1.073e+03\n",
      "  1.081e+03 -1.118e+03 -1.119e+03 -1.125e+03 -1.126e+03 -1.126e+03\n",
      "  1.135e+03  1.147e+03 -1.148e+03 -1.157e+03 -1.159e+03 -1.163e+03\n",
      "  1.167e+03  1.171e+03 -1.178e+03 -1.189e+03 -1.189e+03  1.194e+03\n",
      "  1.197e+03  1.209e+03  1.215e+03 -1.216e+03  1.229e+03 -1.233e+03\n",
      "  1.235e+03 -1.239e+03 -1.246e+03  1.258e+03  1.265e+03  1.268e+03\n",
      " -1.272e+03 -1.280e+03 -1.285e+03 -1.289e+03  1.293e+03 -1.305e+03\n",
      " -1.324e+03 -1.333e+03 -1.351e+03  1.357e+03 -1.367e+03 -1.369e+03\n",
      "  1.379e+03 -1.400e+03  1.421e+03 -1.429e+03 -1.432e+03 -1.474e+03\n",
      " -1.479e+03  1.492e+03  1.498e+03  1.499e+03  1.501e+03  1.516e+03\n",
      " -1.523e+03  1.528e+03  1.531e+03 -1.559e+03  1.600e+03 -1.621e+03\n",
      "  1.622e+03  1.632e+03  1.653e+03 -1.700e+03  1.725e+03 -1.728e+03\n",
      " -1.750e+03  1.798e+03 -1.847e+03 -1.864e+03 -1.893e+03 -1.932e+03\n",
      " -1.965e+03 -1.974e+03 -2.065e+03 -2.067e+03 -2.137e+03 -2.161e+03\n",
      "  2.174e+03 -2.199e+03 -2.224e+03 -2.248e+03 -2.261e+03  2.318e+03\n",
      " -2.360e+03 -2.368e+03  2.393e+03 -2.449e+03 -2.488e+03 -2.515e+03\n",
      " -2.590e+03 -2.595e+03  2.617e+03 -2.676e+03  2.681e+03 -2.696e+03\n",
      " -2.832e+03 -2.973e+03 -3.059e+03 -3.094e+03 -3.261e+03 -3.305e+03\n",
      "  3.361e+03 -3.635e+03 -3.674e+03 -3.940e+03 -4.765e+03 -6.732e+03]\n",
      "Concordance Index 0.5396232339089482\n",
      "Integrated Brier Score: 0.21176809709646272\n",
      "y_train breslow final [ 4.000e+00 -1.300e+01 -1.400e+01  1.800e+01  1.900e+01 -2.400e+01\n",
      " -2.800e+01  3.300e+01 -3.500e+01 -3.600e+01  3.800e+01 -4.400e+01\n",
      " -4.400e+01 -4.800e+01 -5.000e+01  5.800e+01 -6.000e+01 -6.200e+01\n",
      "  6.200e+01  7.400e+01 -8.400e+01  9.700e+01  9.900e+01  1.160e+02\n",
      "  1.180e+02  1.190e+02  1.210e+02  1.240e+02 -1.330e+02 -1.340e+02\n",
      " -1.390e+02 -1.410e+02  1.540e+02  1.610e+02  1.640e+02 -1.650e+02\n",
      "  1.670e+02  1.710e+02  1.730e+02  1.760e+02  1.790e+02 -1.790e+02\n",
      " -1.820e+02 -1.840e+02 -1.860e+02 -1.860e+02  1.870e+02  1.890e+02\n",
      "  1.930e+02 -2.020e+02  2.100e+02 -2.180e+02 -2.240e+02 -2.250e+02\n",
      " -2.300e+02  2.370e+02  2.440e+02  2.500e+02 -2.560e+02  2.570e+02\n",
      "  2.580e+02  2.600e+02 -2.600e+02 -2.640e+02  2.680e+02  2.740e+02\n",
      "  2.750e+02  2.810e+02 -2.850e+02 -2.870e+02  2.910e+02  3.030e+02\n",
      " -3.070e+02  3.070e+02  3.080e+02  3.210e+02 -3.230e+02  3.360e+02\n",
      "  3.360e+02  3.400e+02  3.430e+02 -3.510e+02 -3.530e+02  3.540e+02\n",
      " -3.540e+02 -3.720e+02  3.750e+02  3.760e+02 -3.770e+02 -3.850e+02\n",
      "  3.850e+02 -4.080e+02 -4.090e+02 -4.110e+02  4.140e+02 -4.140e+02\n",
      " -4.150e+02 -4.150e+02 -4.160e+02 -4.170e+02 -4.180e+02 -4.230e+02\n",
      " -4.240e+02  4.280e+02  4.290e+02  4.340e+02 -4.350e+02 -4.350e+02\n",
      " -4.400e+02  4.420e+02 -4.420e+02  4.440e+02 -4.480e+02 -4.480e+02\n",
      " -4.550e+02 -4.570e+02  4.570e+02 -4.620e+02  4.640e+02 -4.660e+02\n",
      " -4.670e+02  4.680e+02  4.680e+02  4.690e+02 -4.760e+02 -4.760e+02\n",
      " -4.760e+02 -4.770e+02 -4.780e+02  4.780e+02 -4.810e+02 -4.840e+02\n",
      " -4.920e+02 -4.990e+02  5.000e+02  5.030e+02 -5.050e+02 -5.130e+02\n",
      " -5.150e+02 -5.150e+02 -5.200e+02 -5.260e+02 -5.310e+02 -5.340e+02\n",
      " -5.360e+02 -5.370e+02 -5.400e+02 -5.410e+02 -5.450e+02 -5.470e+02\n",
      "  5.500e+02 -5.510e+02 -5.520e+02 -5.530e+02  5.570e+02  5.610e+02\n",
      " -5.650e+02 -5.680e+02 -5.680e+02 -5.680e+02 -5.730e+02  5.740e+02\n",
      "  5.820e+02  5.860e+02 -5.920e+02  5.930e+02  5.940e+02 -5.950e+02\n",
      "  5.980e+02 -5.990e+02 -6.000e+02 -6.010e+02 -6.030e+02 -6.030e+02\n",
      " -6.050e+02 -6.050e+02 -6.070e+02  6.070e+02 -6.080e+02 -6.090e+02\n",
      " -6.100e+02 -6.100e+02 -6.100e+02 -6.140e+02  6.140e+02 -6.170e+02\n",
      " -6.170e+02  6.240e+02  6.250e+02 -6.260e+02  6.260e+02 -6.260e+02\n",
      "  6.280e+02 -6.290e+02 -6.300e+02 -6.340e+02 -6.510e+02 -6.510e+02\n",
      " -6.520e+02 -6.530e+02  6.530e+02  6.560e+02 -6.570e+02 -6.580e+02\n",
      " -6.580e+02 -6.640e+02 -6.690e+02 -6.700e+02 -6.700e+02 -6.740e+02\n",
      " -6.830e+02 -6.900e+02 -6.910e+02  6.970e+02 -7.010e+02  7.010e+02\n",
      " -7.040e+02 -7.040e+02 -7.050e+02 -7.050e+02 -7.180e+02 -7.190e+02\n",
      "  7.190e+02 -7.240e+02 -7.280e+02 -7.300e+02  7.310e+02  7.370e+02\n",
      " -7.390e+02 -7.400e+02 -7.410e+02 -7.470e+02 -7.610e+02  7.610e+02\n",
      " -7.610e+02 -7.730e+02 -7.750e+02  7.770e+02 -7.910e+02 -7.910e+02\n",
      " -7.910e+02 -7.910e+02  8.000e+02 -8.000e+02 -8.060e+02  8.070e+02\n",
      "  8.080e+02 -8.130e+02 -8.240e+02  8.260e+02 -8.270e+02 -8.290e+02\n",
      " -8.300e+02 -8.390e+02 -8.450e+02 -8.520e+02  8.550e+02 -8.630e+02\n",
      "  8.640e+02  8.690e+02 -8.720e+02  8.790e+02 -8.820e+02 -8.820e+02\n",
      " -8.880e+02 -8.890e+02  8.960e+02 -8.960e+02  8.960e+02 -9.040e+02\n",
      " -9.130e+02  9.220e+02  9.290e+02 -9.380e+02 -9.440e+02 -9.470e+02\n",
      " -9.490e+02  9.490e+02  9.500e+02  9.520e+02  9.760e+02 -9.770e+02\n",
      "  9.870e+02 -9.930e+02  9.950e+02  9.990e+02 -1.013e+03  1.026e+03\n",
      " -1.036e+03 -1.040e+03 -1.060e+03 -1.071e+03 -1.072e+03  1.073e+03\n",
      "  1.081e+03 -1.118e+03 -1.119e+03 -1.125e+03 -1.126e+03 -1.126e+03\n",
      "  1.135e+03  1.147e+03 -1.148e+03 -1.157e+03 -1.159e+03 -1.163e+03\n",
      "  1.167e+03  1.171e+03 -1.178e+03 -1.189e+03 -1.189e+03  1.194e+03\n",
      "  1.197e+03  1.209e+03  1.215e+03 -1.216e+03  1.229e+03 -1.233e+03\n",
      "  1.235e+03 -1.239e+03 -1.246e+03  1.258e+03  1.265e+03  1.268e+03\n",
      " -1.272e+03 -1.280e+03 -1.285e+03 -1.289e+03  1.293e+03 -1.305e+03\n",
      " -1.324e+03 -1.333e+03 -1.351e+03  1.357e+03 -1.367e+03 -1.369e+03\n",
      "  1.379e+03 -1.400e+03  1.421e+03 -1.429e+03 -1.432e+03 -1.474e+03\n",
      " -1.479e+03  1.492e+03  1.498e+03  1.499e+03  1.501e+03  1.516e+03\n",
      " -1.523e+03  1.528e+03  1.531e+03 -1.559e+03  1.600e+03 -1.621e+03\n",
      "  1.622e+03  1.632e+03  1.653e+03 -1.700e+03  1.725e+03 -1.728e+03\n",
      " -1.750e+03  1.798e+03 -1.847e+03 -1.864e+03 -1.893e+03 -1.932e+03\n",
      " -1.965e+03 -1.974e+03 -2.065e+03 -2.067e+03 -2.137e+03 -2.161e+03\n",
      "  2.174e+03 -2.199e+03 -2.224e+03 -2.248e+03 -2.261e+03  2.318e+03\n",
      " -2.360e+03 -2.368e+03  2.393e+03 -2.449e+03 -2.488e+03 -2.515e+03\n",
      " -2.590e+03 -2.595e+03  2.617e+03 -2.676e+03  2.681e+03 -2.696e+03\n",
      " -2.832e+03 -2.973e+03 -3.059e+03 -3.094e+03 -3.261e+03 -3.305e+03\n",
      "  3.361e+03 -3.635e+03 -3.674e+03 -3.940e+03 -4.765e+03 -6.732e+03]\n",
      "durations 11.0 7248.0\n",
      "Concordance Index 0.42637362637362636\n",
      "Integrated Brier Score: 0.2060413750774309\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m269.3496\u001b[0m  0.0026\n",
      "      2      269.3496  0.0020\n",
      "      3      269.3496  0.0019\n",
      "      4      269.3496  0.0018\n",
      "      5      269.3496  0.0030\n",
      "      6      269.3496  0.0053\n",
      "      7      269.3496  0.0052\n",
      "      8      269.3496  0.0022\n",
      "      9      269.3496  0.0019\n",
      "     10      269.3496  0.0018\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m202.6476\u001b[0m  0.0032\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      2      202.6476  0.0021\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      3      202.6476  0.0028\n",
      "      4      202.6476  0.0019\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m269.7728\u001b[0m  0.0035\n",
      "      5      202.6476  0.0018\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m295.9162\u001b[0m  0.0058\n",
      "      2      269.7728  0.0026\n",
      "      6      202.6476  0.0018\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      3      269.7728  0.0019\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      7      202.6476  0.0024\n",
      "      4      269.7728  0.0018\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      2      295.9162  0.0055\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      5      269.7728  0.0018\n",
      "      3      295.9162  0.0019\n",
      "      6      269.7728  0.0017\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m79.5009\u001b[0m  0.0038\n",
      "      4      295.9162  0.0018\n",
      "      8      202.6476  0.0055\n",
      "      5      295.9162  0.0018\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m236.6923\u001b[0m  0.0037\n",
      "      2       79.5009  0.0028\n",
      "      2      236.6923  0.0020\n",
      "      9      202.6476  0.0038\n",
      "      6      295.9162  0.0025\n",
      "      7      269.7728  0.0055\n",
      "      3       79.5009  0.0027\n",
      "      3      236.6923  0.0019\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "     10      202.6476  0.0027\n",
      "Restoring best model from epoch 1.\n",
      "      4      236.6923  0.0018\n",
      "      4       79.5009  0.0029\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      5      236.6923  0.0018\n",
      "      8      269.7728  0.0053\n",
      "      7      295.9162  0.0051\n",
      "      5       79.5009  0.0027\n",
      "      6      236.6923  0.0018\n",
      "      9      269.7728  0.0019\n",
      "      7      236.6923  0.0018\n",
      "      6       79.5009  0.0029\n",
      "     10      269.7728  0.0018\n",
      "      8      295.9162  0.0040\n",
      "Restoring best model from epoch 1.\n",
      "      8      236.6923  0.0019\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m85.7973\u001b[0m  0.0043\n",
      "      7       79.5009  0.0027\n",
      "      9      236.6923  0.0023\n",
      "      9      295.9162  0.0036\n",
      "      2       85.7973  0.0029\n",
      "      8       79.5009  0.0026\n",
      "     10      236.6923  0.0019\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      9       79.5009  0.0027\n",
      "      3       85.7973  0.0028\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "     10      295.9162  0.0038\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "     10       79.5009  0.0026\n",
      "      4       85.7973  0.0027\n",
      "Restoring best model from epoch 1.\n",
      "      5       85.7973  0.0027\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m89.6978\u001b[0m  0.0038\n",
      "      6       85.7973  0.0026\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m106.0212\u001b[0m  0.0056\n",
      "      7       85.7973  0.0028\n",
      "      2       89.6978  0.0030\n",
      "      2      106.0212  0.0030\n",
      "      3       89.6978  0.0027\n",
      "      8       85.7973  0.0028\n",
      "      9       85.7973  0.0027\n",
      "      4       89.6978  0.0028\n",
      "      3      106.0212  0.0046\n",
      "     10       85.7973  0.0027\n",
      "Restoring best model from epoch 1.\n",
      "      5       89.6978  0.0037\n",
      "      4      106.0212  0.0044\n",
      "      6       89.6978  0.0031\n",
      "      5      106.0212  0.0028\n",
      "      7       89.6978  0.0029\n",
      "      8       89.6978  0.0028\n",
      "      6      106.0212  0.0057\n",
      "      7      106.0212  0.0029\n",
      "      9       89.6978  0.0071\n",
      "      8      106.0212  0.0029\n",
      "      9      106.0212  0.0027\n",
      "     10       89.6978  0.0040\n",
      "Restoring best model from epoch 1.\n",
      "     10      106.0212  0.0027\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m88.8809\u001b[0m  0.0038\n",
      "      2       88.8809  0.0028\n",
      "      3       88.8809  0.0027\n",
      "      4       88.8809  0.0026\n",
      "      5       88.8809  0.0026\n",
      "      6       88.8809  0.0026\n",
      "      7       88.8809  0.0026\n",
      "      8       88.8809  0.0026\n",
      "      9       88.8809  0.0026\n",
      "     10       88.8809  0.0026\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m112.7514\u001b[0m  0.0046\n",
      "      2      112.7514  0.0035\n",
      "      3      112.7514  0.0034\n",
      "      4      112.7514  0.0045\n",
      "      5      112.7514  0.0036\n",
      "      6      112.7514  0.0042\n",
      "      7      112.7514  0.0065\n",
      "      8      112.7514  0.0053\n",
      "      9      112.7514  0.0048\n",
      "     10      112.7514  0.0058\n",
      "Restoring best model from epoch 1.\n",
      "y_train breslow final [ 4.000e+00 -1.100e+01 -1.300e+01 -1.400e+01 -1.500e+01  1.800e+01\n",
      "  1.900e+01  2.200e+01 -2.400e+01  3.300e+01 -3.500e+01 -3.600e+01\n",
      "  3.800e+01 -4.400e+01 -4.400e+01 -4.800e+01 -5.000e+01  5.800e+01\n",
      " -6.000e+01  6.200e+01 -7.900e+01 -8.400e+01 -8.400e+01  8.700e+01\n",
      "  9.100e+01  9.700e+01  9.900e+01  1.180e+02  1.190e+02 -1.190e+02\n",
      "  1.210e+02 -1.290e+02 -1.310e+02 -1.330e+02 -1.340e+02  1.390e+02\n",
      " -1.410e+02 -1.510e+02  1.540e+02  1.610e+02 -1.650e+02  1.710e+02\n",
      "  1.730e+02 -1.740e+02  1.790e+02 -1.790e+02 -1.820e+02 -1.860e+02\n",
      " -1.860e+02  1.870e+02  1.890e+02 -2.020e+02  2.100e+02 -2.180e+02\n",
      " -2.240e+02 -2.300e+02  2.370e+02  2.430e+02  2.440e+02  2.440e+02\n",
      "  2.500e+02  2.580e+02  2.600e+02 -2.600e+02 -2.640e+02  2.680e+02\n",
      "  2.740e+02  2.750e+02  2.820e+02 -2.850e+02 -2.870e+02  3.000e+02\n",
      " -3.070e+02  3.070e+02 -3.100e+02  3.210e+02  3.210e+02  3.360e+02\n",
      "  3.360e+02  3.390e+02  3.400e+02  3.430e+02 -3.510e+02 -3.530e+02\n",
      "  3.540e+02 -3.650e+02  3.700e+02 -3.720e+02  3.750e+02  3.760e+02\n",
      " -3.770e+02 -3.850e+02  3.850e+02 -3.850e+02 -4.080e+02  4.090e+02\n",
      " -4.140e+02 -4.150e+02 -4.150e+02 -4.160e+02 -4.180e+02 -4.220e+02\n",
      " -4.230e+02 -4.240e+02 -4.260e+02 -4.260e+02 -4.270e+02  4.290e+02\n",
      "  4.340e+02  4.340e+02 -4.350e+02 -4.350e+02 -4.350e+02 -4.400e+02\n",
      "  4.420e+02 -4.420e+02  4.440e+02 -4.460e+02 -4.480e+02 -4.480e+02\n",
      " -4.570e+02  4.570e+02  4.600e+02 -4.620e+02  4.640e+02 -4.670e+02\n",
      "  4.680e+02  4.690e+02 -4.760e+02 -4.760e+02 -4.760e+02 -4.770e+02\n",
      "  4.780e+02 -4.810e+02 -4.840e+02 -4.860e+02 -4.870e+02  4.880e+02\n",
      " -4.920e+02 -5.050e+02 -5.130e+02 -5.150e+02 -5.150e+02 -5.200e+02\n",
      " -5.220e+02 -5.310e+02 -5.340e+02 -5.360e+02 -5.390e+02 -5.400e+02\n",
      " -5.410e+02 -5.450e+02 -5.460e+02 -5.470e+02 -5.510e+02 -5.520e+02\n",
      " -5.530e+02  5.570e+02  5.610e+02 -5.640e+02 -5.650e+02 -5.670e+02\n",
      " -5.680e+02  5.740e+02 -5.780e+02 -5.780e+02 -5.780e+02  5.820e+02\n",
      " -5.910e+02 -5.920e+02  5.930e+02  5.940e+02 -5.950e+02 -5.960e+02\n",
      "  5.980e+02 -5.990e+02 -6.000e+02 -6.010e+02 -6.030e+02 -6.030e+02\n",
      " -6.030e+02 -6.050e+02 -6.050e+02 -6.070e+02 -6.090e+02 -6.100e+02\n",
      " -6.100e+02 -6.100e+02 -6.140e+02  6.140e+02 -6.170e+02 -6.170e+02\n",
      " -6.240e+02  6.240e+02  6.280e+02 -6.290e+02 -6.300e+02 -6.340e+02\n",
      " -6.510e+02 -6.510e+02 -6.520e+02  6.530e+02 -6.570e+02 -6.580e+02\n",
      " -6.640e+02  6.660e+02 -6.700e+02 -6.700e+02  6.770e+02 -6.770e+02\n",
      " -6.830e+02 -6.890e+02 -6.900e+02 -6.910e+02  6.940e+02  6.970e+02\n",
      " -7.010e+02  7.020e+02 -7.040e+02 -7.050e+02 -7.050e+02  7.110e+02\n",
      " -7.180e+02  7.190e+02 -7.240e+02 -7.280e+02 -7.300e+02  7.310e+02\n",
      "  7.370e+02 -7.390e+02 -7.400e+02  7.600e+02 -7.610e+02 -7.610e+02\n",
      " -7.730e+02 -7.750e+02  7.770e+02 -7.910e+02 -7.910e+02  8.000e+02\n",
      " -8.000e+02 -8.050e+02 -8.060e+02  8.070e+02  8.080e+02 -8.130e+02\n",
      " -8.240e+02 -8.240e+02  8.260e+02 -8.270e+02 -8.290e+02 -8.300e+02\n",
      " -8.390e+02 -8.420e+02 -8.620e+02 -8.640e+02 -8.660e+02  8.690e+02\n",
      "  8.790e+02 -8.820e+02 -8.880e+02 -8.890e+02 -8.890e+02  8.960e+02\n",
      " -8.960e+02 -9.040e+02  9.050e+02 -9.100e+02 -9.120e+02  9.220e+02\n",
      "  9.290e+02 -9.300e+02 -9.470e+02 -9.490e+02  9.490e+02  9.500e+02\n",
      "  9.520e+02  9.760e+02 -9.770e+02  9.870e+02 -9.880e+02 -9.930e+02\n",
      "  9.950e+02  9.950e+02 -9.970e+02  9.990e+02 -1.040e+03  1.043e+03\n",
      "  1.046e+03 -1.071e+03 -1.072e+03  1.073e+03 -1.079e+03  1.081e+03\n",
      " -1.097e+03  1.115e+03 -1.119e+03 -1.125e+03 -1.126e+03 -1.130e+03\n",
      "  1.135e+03  1.147e+03 -1.148e+03 -1.157e+03 -1.159e+03 -1.163e+03\n",
      "  1.167e+03  1.171e+03 -1.175e+03 -1.189e+03 -1.189e+03  1.197e+03\n",
      "  1.209e+03  1.215e+03 -1.216e+03 -1.233e+03  1.235e+03 -1.239e+03\n",
      " -1.246e+03  1.258e+03  1.265e+03  1.268e+03 -1.280e+03 -1.285e+03\n",
      "  1.288e+03  1.293e+03 -1.301e+03 -1.324e+03 -1.333e+03 -1.351e+03\n",
      "  1.357e+03 -1.367e+03 -1.369e+03  1.379e+03 -1.400e+03  1.421e+03\n",
      " -1.429e+03 -1.431e+03 -1.432e+03 -1.442e+03  1.454e+03 -1.474e+03\n",
      " -1.479e+03  1.492e+03  1.498e+03  1.499e+03  1.516e+03  1.528e+03\n",
      "  1.531e+03 -1.559e+03  1.600e+03 -1.617e+03  1.622e+03  1.632e+03\n",
      " -1.683e+03 -1.700e+03  1.725e+03 -1.728e+03 -1.750e+03  1.778e+03\n",
      "  1.790e+03  1.830e+03 -1.864e+03 -1.870e+03 -1.932e+03 -1.965e+03\n",
      " -1.974e+03  2.027e+03 -2.067e+03 -2.109e+03 -2.224e+03 -2.261e+03\n",
      "  2.318e+03 -2.360e+03 -2.368e+03  2.393e+03 -2.449e+03 -2.590e+03\n",
      " -2.616e+03  2.617e+03  2.620e+03 -2.676e+03  2.681e+03 -2.696e+03\n",
      " -2.823e+03 -2.832e+03 -2.973e+03 -3.059e+03 -3.094e+03  3.169e+03\n",
      " -3.261e+03 -3.305e+03  3.361e+03 -3.635e+03 -3.674e+03 -3.759e+03\n",
      " -4.765e+03  4.961e+03 -4.992e+03 -6.732e+03 -7.062e+03 -7.248e+03]\n",
      "Concordance Index 0.5507056550640165\n",
      "Integrated Brier Score: 0.1949739406615247\n",
      "y_train breslow final [ 4.000e+00 -1.100e+01 -1.300e+01 -1.400e+01 -1.500e+01  1.800e+01\n",
      "  1.900e+01  2.200e+01 -2.400e+01  3.300e+01 -3.500e+01 -3.600e+01\n",
      "  3.800e+01 -4.400e+01 -4.400e+01 -4.800e+01 -5.000e+01  5.800e+01\n",
      " -6.000e+01  6.200e+01 -7.900e+01 -8.400e+01 -8.400e+01  8.700e+01\n",
      "  9.100e+01  9.700e+01  9.900e+01  1.180e+02  1.190e+02 -1.190e+02\n",
      "  1.210e+02 -1.290e+02 -1.310e+02 -1.330e+02 -1.340e+02  1.390e+02\n",
      " -1.410e+02 -1.510e+02  1.540e+02  1.610e+02 -1.650e+02  1.710e+02\n",
      "  1.730e+02 -1.740e+02  1.790e+02 -1.790e+02 -1.820e+02 -1.860e+02\n",
      " -1.860e+02  1.870e+02  1.890e+02 -2.020e+02  2.100e+02 -2.180e+02\n",
      " -2.240e+02 -2.300e+02  2.370e+02  2.430e+02  2.440e+02  2.440e+02\n",
      "  2.500e+02  2.580e+02  2.600e+02 -2.600e+02 -2.640e+02  2.680e+02\n",
      "  2.740e+02  2.750e+02  2.820e+02 -2.850e+02 -2.870e+02  3.000e+02\n",
      " -3.070e+02  3.070e+02 -3.100e+02  3.210e+02  3.210e+02  3.360e+02\n",
      "  3.360e+02  3.390e+02  3.400e+02  3.430e+02 -3.510e+02 -3.530e+02\n",
      "  3.540e+02 -3.650e+02  3.700e+02 -3.720e+02  3.750e+02  3.760e+02\n",
      " -3.770e+02 -3.850e+02  3.850e+02 -3.850e+02 -4.080e+02  4.090e+02\n",
      " -4.140e+02 -4.150e+02 -4.150e+02 -4.160e+02 -4.180e+02 -4.220e+02\n",
      " -4.230e+02 -4.240e+02 -4.260e+02 -4.260e+02 -4.270e+02  4.290e+02\n",
      "  4.340e+02  4.340e+02 -4.350e+02 -4.350e+02 -4.350e+02 -4.400e+02\n",
      "  4.420e+02 -4.420e+02  4.440e+02 -4.460e+02 -4.480e+02 -4.480e+02\n",
      " -4.570e+02  4.570e+02  4.600e+02 -4.620e+02  4.640e+02 -4.670e+02\n",
      "  4.680e+02  4.690e+02 -4.760e+02 -4.760e+02 -4.760e+02 -4.770e+02\n",
      "  4.780e+02 -4.810e+02 -4.840e+02 -4.860e+02 -4.870e+02  4.880e+02\n",
      " -4.920e+02 -5.050e+02 -5.130e+02 -5.150e+02 -5.150e+02 -5.200e+02\n",
      " -5.220e+02 -5.310e+02 -5.340e+02 -5.360e+02 -5.390e+02 -5.400e+02\n",
      " -5.410e+02 -5.450e+02 -5.460e+02 -5.470e+02 -5.510e+02 -5.520e+02\n",
      " -5.530e+02  5.570e+02  5.610e+02 -5.640e+02 -5.650e+02 -5.670e+02\n",
      " -5.680e+02  5.740e+02 -5.780e+02 -5.780e+02 -5.780e+02  5.820e+02\n",
      " -5.910e+02 -5.920e+02  5.930e+02  5.940e+02 -5.950e+02 -5.960e+02\n",
      "  5.980e+02 -5.990e+02 -6.000e+02 -6.010e+02 -6.030e+02 -6.030e+02\n",
      " -6.030e+02 -6.050e+02 -6.050e+02 -6.070e+02 -6.090e+02 -6.100e+02\n",
      " -6.100e+02 -6.100e+02 -6.140e+02  6.140e+02 -6.170e+02 -6.170e+02\n",
      " -6.240e+02  6.240e+02  6.280e+02 -6.290e+02 -6.300e+02 -6.340e+02\n",
      " -6.510e+02 -6.510e+02 -6.520e+02  6.530e+02 -6.570e+02 -6.580e+02\n",
      " -6.640e+02  6.660e+02 -6.700e+02 -6.700e+02  6.770e+02 -6.770e+02\n",
      " -6.830e+02 -6.890e+02 -6.900e+02 -6.910e+02  6.940e+02  6.970e+02\n",
      " -7.010e+02  7.020e+02 -7.040e+02 -7.050e+02 -7.050e+02  7.110e+02\n",
      " -7.180e+02  7.190e+02 -7.240e+02 -7.280e+02 -7.300e+02  7.310e+02\n",
      "  7.370e+02 -7.390e+02 -7.400e+02  7.600e+02 -7.610e+02 -7.610e+02\n",
      " -7.730e+02 -7.750e+02  7.770e+02 -7.910e+02 -7.910e+02  8.000e+02\n",
      " -8.000e+02 -8.050e+02 -8.060e+02  8.070e+02  8.080e+02 -8.130e+02\n",
      " -8.240e+02 -8.240e+02  8.260e+02 -8.270e+02 -8.290e+02 -8.300e+02\n",
      " -8.390e+02 -8.420e+02 -8.620e+02 -8.640e+02 -8.660e+02  8.690e+02\n",
      "  8.790e+02 -8.820e+02 -8.880e+02 -8.890e+02 -8.890e+02  8.960e+02\n",
      " -8.960e+02 -9.040e+02  9.050e+02 -9.100e+02 -9.120e+02  9.220e+02\n",
      "  9.290e+02 -9.300e+02 -9.470e+02 -9.490e+02  9.490e+02  9.500e+02\n",
      "  9.520e+02  9.760e+02 -9.770e+02  9.870e+02 -9.880e+02 -9.930e+02\n",
      "  9.950e+02  9.950e+02 -9.970e+02  9.990e+02 -1.040e+03  1.043e+03\n",
      "  1.046e+03 -1.071e+03 -1.072e+03  1.073e+03 -1.079e+03  1.081e+03\n",
      " -1.097e+03  1.115e+03 -1.119e+03 -1.125e+03 -1.126e+03 -1.130e+03\n",
      "  1.135e+03  1.147e+03 -1.148e+03 -1.157e+03 -1.159e+03 -1.163e+03\n",
      "  1.167e+03  1.171e+03 -1.175e+03 -1.189e+03 -1.189e+03  1.197e+03\n",
      "  1.209e+03  1.215e+03 -1.216e+03 -1.233e+03  1.235e+03 -1.239e+03\n",
      " -1.246e+03  1.258e+03  1.265e+03  1.268e+03 -1.280e+03 -1.285e+03\n",
      "  1.288e+03  1.293e+03 -1.301e+03 -1.324e+03 -1.333e+03 -1.351e+03\n",
      "  1.357e+03 -1.367e+03 -1.369e+03  1.379e+03 -1.400e+03  1.421e+03\n",
      " -1.429e+03 -1.431e+03 -1.432e+03 -1.442e+03  1.454e+03 -1.474e+03\n",
      " -1.479e+03  1.492e+03  1.498e+03  1.499e+03  1.516e+03  1.528e+03\n",
      "  1.531e+03 -1.559e+03  1.600e+03 -1.617e+03  1.622e+03  1.632e+03\n",
      " -1.683e+03 -1.700e+03  1.725e+03 -1.728e+03 -1.750e+03  1.778e+03\n",
      "  1.790e+03  1.830e+03 -1.864e+03 -1.870e+03 -1.932e+03 -1.965e+03\n",
      " -1.974e+03  2.027e+03 -2.067e+03 -2.109e+03 -2.224e+03 -2.261e+03\n",
      "  2.318e+03 -2.360e+03 -2.368e+03  2.393e+03 -2.449e+03 -2.590e+03\n",
      " -2.616e+03  2.617e+03  2.620e+03 -2.676e+03  2.681e+03 -2.696e+03\n",
      " -2.823e+03 -2.832e+03 -2.973e+03 -3.059e+03 -3.094e+03  3.169e+03\n",
      " -3.261e+03 -3.305e+03  3.361e+03 -3.635e+03 -3.674e+03 -3.759e+03\n",
      " -4.765e+03  4.961e+03 -4.992e+03 -6.732e+03 -7.062e+03 -7.248e+03]\n",
      "durations 28.0 3940.0\n",
      "Concordance Index 0.5995370370370371\n",
      "Integrated Brier Score: 0.21360030064729016\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m145.6896\u001b[0m  0.0021\n",
      "      2      145.6896  0.0018\n",
      "      3      145.6896  0.0017\n",
      "      4      145.6896  0.0018\n",
      "      5      145.6896  0.0018\n",
      "      6      145.6896  0.0018\n",
      "      7      145.6896  0.0017\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      8      145.6896  0.0017\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      9      145.6896  0.0017\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "     10      145.6896  0.0016\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m54.9228\u001b[0m  0.0031\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m291.3726\u001b[0m  0.0022\n",
      "      2       54.9228  0.0038\n",
      "      2      291.3726  0.0019\n",
      "      3       54.9228  0.0027\n",
      "      3      291.3726  0.0018\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      4      291.3726  0.0018\n",
      "      4       54.9228  0.0026\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m240.9685\u001b[0m  0.0022\n",
      "      5      291.3726  0.0041\n",
      "      5       54.9228  0.0040\n",
      "      2      240.9685  0.0021\n",
      "      6      291.3726  0.0019\n",
      "      3      240.9685  0.0018\n",
      "      6       54.9228  0.0027\n",
      "      7      291.3726  0.0018\n",
      "      4      240.9685  0.0017\n",
      "      7       54.9228  0.0026\n",
      "      5      240.9685  0.0017\n",
      "      8      291.3726  0.0018\n",
      "      9      291.3726  0.0019\n",
      "      8       54.9228  0.0026\n",
      "      6      240.9685  0.0039\n",
      "     10      291.3726  0.0018\n",
      "Restoring best model from epoch 1.\n",
      "      7      240.9685  0.0020\n",
      "      8      240.9685  0.0018\n",
      "      9       54.9228  0.0060\n",
      "      9      240.9685  0.0018\n",
      "     10      240.9685  0.0017\n",
      "Restoring best model from epoch 1.\n",
      "     10       54.9228  0.0058\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m80.6819\u001b[0m  0.0038\n",
      "      2       80.6819  0.0029\n",
      "      3       80.6819  0.0027\n",
      "      4       80.6819  0.0071\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      5       80.6819  0.0028\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      6       80.6819  0.0036\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      7       80.6819  0.0041\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m64.8211\u001b[0m  0.0053\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m98.3683\u001b[0m  0.0032\n",
      "      8       80.6819  0.0027\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      2       98.3683  0.0030\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      9       80.6819  0.0038\n",
      "      2       64.8211  0.0071\n",
      "      3       98.3683  0.0051\n",
      "      3       64.8211  0.0032\n",
      "     10       80.6819  0.0051\n",
      "Restoring best model from epoch 1.\n",
      "      4       98.3683  0.0035\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m123.0941\u001b[0m  0.0073\n",
      "      4       64.8211  0.0043\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      5       98.3683  0.0043\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2      123.0941  0.0020\n",
      "      3      123.0941  0.0017\n",
      "      5       64.8211  0.0035\n",
      "      6       98.3683  0.0040\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m100.0883\u001b[0m  0.0031\n",
      "      4      123.0941  0.0024\n",
      "      6       64.8211  0.0027\n",
      "      2      100.0883  0.0028\n",
      "      7       64.8211  0.0026\n",
      "      7       98.3683  0.0040\n",
      "      5      123.0941  0.0045\n",
      "      8       64.8211  0.0027\n",
      "      3      100.0883  0.0036\n",
      "      6      123.0941  0.0021\n",
      "      8       98.3683  0.0042\n",
      "      9       64.8211  0.0026\n",
      "      9       98.3683  0.0030\n",
      "      4      100.0883  0.0046\n",
      "      7      123.0941  0.0054\n",
      "     10       64.8211  0.0046\n",
      "     10       98.3683  0.0032\n",
      "Restoring best model from epoch 1.\n",
      "      8      123.0941  0.0017\n",
      "Restoring best model from epoch 1.\n",
      "      5      100.0883  0.0035\n",
      "      9      123.0941  0.0016\n",
      "     10      123.0941  0.0016\n",
      "      6      100.0883  0.0030\n",
      "Restoring best model from epoch 1.\n",
      "      7      100.0883  0.0031\n",
      "      8      100.0883  0.0032\n",
      "      9      100.0883  0.0028\n",
      "     10      100.0883  0.0031\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m314.5165\u001b[0m  0.0067\n",
      "      2      314.5165  0.0027\n",
      "      3      314.5165  0.0020\n",
      "      4      314.5165  0.0018\n",
      "      5      314.5165  0.0018\n",
      "      6      314.5165  0.0018\n",
      "      7      314.5165  0.0018\n",
      "      8      314.5165  0.0017\n",
      "      9      314.5165  0.0017\n",
      "     10      314.5165  0.0017\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m115.1936\u001b[0m  0.0036\n",
      "      2      115.1936  0.0037\n",
      "      3      115.1936  0.0039\n",
      "      4      115.1936  0.0044\n",
      "      5      115.1936  0.0039\n",
      "      6      115.1936  0.0044\n",
      "      7      115.1936  0.0069\n",
      "      8      115.1936  0.0060\n",
      "      9      115.1936  0.0060\n",
      "     10      115.1936  0.0039\n",
      "Restoring best model from epoch 1.\n",
      "y_train breslow final [ 4.000e+00 -1.100e+01 -1.300e+01 -1.400e+01 -1.500e+01  1.800e+01\n",
      "  2.200e+01 -2.400e+01 -2.800e+01 -3.500e+01 -3.600e+01  3.800e+01\n",
      " -4.400e+01 -4.400e+01 -4.800e+01 -5.000e+01  5.800e+01 -6.000e+01\n",
      " -6.200e+01  6.200e+01  7.400e+01 -7.900e+01 -8.400e+01 -8.400e+01\n",
      "  8.700e+01  9.100e+01  9.900e+01  1.160e+02  1.180e+02  1.190e+02\n",
      " -1.190e+02  1.210e+02  1.240e+02 -1.290e+02 -1.310e+02 -1.330e+02\n",
      "  1.390e+02 -1.390e+02 -1.510e+02  1.540e+02  1.640e+02 -1.650e+02\n",
      "  1.670e+02  1.710e+02  1.730e+02 -1.740e+02  1.760e+02  1.790e+02\n",
      " -1.790e+02 -1.840e+02 -1.860e+02 -1.860e+02  1.870e+02  1.930e+02\n",
      " -2.020e+02 -2.180e+02 -2.240e+02 -2.250e+02 -2.300e+02  2.370e+02\n",
      "  2.430e+02  2.440e+02  2.440e+02  2.500e+02 -2.560e+02  2.570e+02\n",
      "  2.580e+02  2.600e+02 -2.600e+02  2.680e+02  2.740e+02  2.750e+02\n",
      "  2.810e+02  2.820e+02 -2.850e+02 -2.870e+02  2.910e+02  3.000e+02\n",
      "  3.030e+02  3.070e+02  3.080e+02 -3.100e+02  3.210e+02 -3.230e+02\n",
      "  3.360e+02  3.390e+02  3.400e+02 -3.530e+02 -3.540e+02 -3.650e+02\n",
      "  3.700e+02 -3.720e+02  3.760e+02 -3.770e+02 -3.850e+02 -4.080e+02\n",
      "  4.090e+02 -4.090e+02 -4.110e+02  4.140e+02 -4.140e+02 -4.150e+02\n",
      " -4.160e+02 -4.170e+02 -4.220e+02 -4.230e+02 -4.240e+02 -4.260e+02\n",
      " -4.260e+02 -4.270e+02  4.280e+02  4.290e+02  4.340e+02  4.340e+02\n",
      " -4.350e+02 -4.350e+02 -4.400e+02 -4.420e+02  4.440e+02  4.440e+02\n",
      " -4.460e+02 -4.480e+02 -4.480e+02 -4.550e+02 -4.570e+02  4.570e+02\n",
      "  4.600e+02 -4.620e+02  4.640e+02 -4.660e+02 -4.670e+02  4.680e+02\n",
      "  4.680e+02  4.690e+02 -4.760e+02 -4.770e+02 -4.780e+02  4.780e+02\n",
      " -4.840e+02 -4.860e+02 -4.870e+02  4.880e+02 -4.920e+02 -4.990e+02\n",
      "  5.000e+02  5.030e+02 -5.050e+02 -5.130e+02 -5.150e+02 -5.150e+02\n",
      " -5.220e+02 -5.260e+02 -5.310e+02 -5.340e+02 -5.370e+02 -5.390e+02\n",
      " -5.450e+02 -5.460e+02 -5.470e+02  5.500e+02 -5.510e+02 -5.520e+02\n",
      "  5.570e+02 -5.640e+02 -5.670e+02 -5.680e+02 -5.680e+02 -5.680e+02\n",
      " -5.730e+02 -5.780e+02 -5.780e+02 -5.780e+02  5.820e+02  5.860e+02\n",
      " -5.910e+02 -5.920e+02  5.930e+02  5.940e+02 -5.950e+02 -5.960e+02\n",
      "  5.980e+02 -5.990e+02 -6.010e+02 -6.030e+02 -6.030e+02 -6.050e+02\n",
      " -6.050e+02 -6.070e+02  6.070e+02 -6.080e+02 -6.100e+02 -6.100e+02\n",
      " -6.140e+02  6.140e+02 -6.170e+02 -6.240e+02  6.240e+02  6.250e+02\n",
      " -6.260e+02  6.260e+02 -6.260e+02  6.280e+02 -6.300e+02 -6.340e+02\n",
      " -6.510e+02 -6.510e+02 -6.520e+02 -6.520e+02 -6.530e+02  6.530e+02\n",
      "  6.560e+02 -6.570e+02 -6.580e+02 -6.580e+02 -6.640e+02  6.660e+02\n",
      " -6.690e+02 -6.700e+02 -6.740e+02  6.770e+02 -6.770e+02 -6.830e+02\n",
      " -6.890e+02 -6.910e+02  6.940e+02  6.970e+02  7.010e+02  7.020e+02\n",
      " -7.040e+02 -7.050e+02 -7.050e+02  7.110e+02 -7.180e+02 -7.190e+02\n",
      " -7.280e+02 -7.300e+02  7.370e+02 -7.390e+02 -7.400e+02 -7.410e+02\n",
      " -7.470e+02  7.600e+02 -7.610e+02  7.610e+02 -7.610e+02 -7.730e+02\n",
      " -7.910e+02 -7.910e+02 -7.910e+02 -7.910e+02  8.000e+02 -8.000e+02\n",
      " -8.050e+02 -8.060e+02  8.070e+02  8.080e+02 -8.130e+02 -8.240e+02\n",
      " -8.240e+02  8.260e+02 -8.270e+02 -8.290e+02 -8.300e+02 -8.420e+02\n",
      " -8.450e+02 -8.520e+02  8.550e+02 -8.620e+02 -8.630e+02 -8.640e+02\n",
      "  8.640e+02 -8.660e+02  8.690e+02 -8.720e+02 -8.820e+02 -8.880e+02\n",
      " -8.890e+02 -8.890e+02 -8.960e+02  8.960e+02  9.050e+02 -9.100e+02\n",
      " -9.120e+02 -9.130e+02  9.220e+02  9.290e+02 -9.300e+02 -9.380e+02\n",
      " -9.440e+02 -9.470e+02 -9.490e+02  9.500e+02  9.760e+02  9.870e+02\n",
      " -9.880e+02 -9.930e+02  9.950e+02  9.950e+02 -9.970e+02  9.990e+02\n",
      " -1.013e+03  1.026e+03 -1.036e+03  1.043e+03  1.046e+03 -1.060e+03\n",
      " -1.071e+03 -1.072e+03 -1.079e+03 -1.097e+03  1.115e+03 -1.118e+03\n",
      " -1.125e+03 -1.126e+03 -1.126e+03 -1.130e+03  1.135e+03  1.147e+03\n",
      " -1.148e+03  1.167e+03 -1.175e+03 -1.178e+03 -1.189e+03  1.194e+03\n",
      "  1.197e+03  1.209e+03  1.229e+03 -1.239e+03  1.265e+03  1.268e+03\n",
      " -1.272e+03 -1.285e+03  1.288e+03 -1.289e+03  1.293e+03 -1.301e+03\n",
      " -1.305e+03 -1.333e+03 -1.351e+03  1.379e+03  1.421e+03 -1.429e+03\n",
      " -1.431e+03 -1.442e+03  1.454e+03 -1.474e+03 -1.479e+03  1.492e+03\n",
      "  1.498e+03  1.501e+03  1.516e+03 -1.523e+03  1.600e+03 -1.617e+03\n",
      " -1.621e+03  1.622e+03  1.653e+03 -1.683e+03 -1.750e+03  1.778e+03\n",
      "  1.790e+03  1.798e+03  1.830e+03 -1.847e+03 -1.864e+03 -1.870e+03\n",
      " -1.893e+03 -1.965e+03  2.027e+03 -2.065e+03 -2.067e+03 -2.109e+03\n",
      " -2.137e+03 -2.161e+03  2.174e+03 -2.199e+03 -2.248e+03 -2.360e+03\n",
      "  2.393e+03 -2.449e+03 -2.488e+03 -2.515e+03 -2.590e+03 -2.595e+03\n",
      " -2.616e+03  2.617e+03  2.620e+03 -2.676e+03  2.681e+03 -2.823e+03\n",
      " -2.832e+03 -3.059e+03  3.169e+03 -3.635e+03 -3.674e+03 -3.759e+03\n",
      " -3.940e+03 -4.765e+03  4.961e+03 -4.992e+03 -7.062e+03 -7.248e+03]\n",
      "Concordance Index 0.42617073717230564\n",
      "Integrated Brier Score: 0.19247649330686062\n",
      "y_train breslow final [ 4.000e+00 -1.100e+01 -1.300e+01 -1.400e+01 -1.500e+01  1.800e+01\n",
      "  2.200e+01 -2.400e+01 -2.800e+01 -3.500e+01 -3.600e+01  3.800e+01\n",
      " -4.400e+01 -4.400e+01 -4.800e+01 -5.000e+01  5.800e+01 -6.000e+01\n",
      " -6.200e+01  6.200e+01  7.400e+01 -7.900e+01 -8.400e+01 -8.400e+01\n",
      "  8.700e+01  9.100e+01  9.900e+01  1.160e+02  1.180e+02  1.190e+02\n",
      " -1.190e+02  1.210e+02  1.240e+02 -1.290e+02 -1.310e+02 -1.330e+02\n",
      "  1.390e+02 -1.390e+02 -1.510e+02  1.540e+02  1.640e+02 -1.650e+02\n",
      "  1.670e+02  1.710e+02  1.730e+02 -1.740e+02  1.760e+02  1.790e+02\n",
      " -1.790e+02 -1.840e+02 -1.860e+02 -1.860e+02  1.870e+02  1.930e+02\n",
      " -2.020e+02 -2.180e+02 -2.240e+02 -2.250e+02 -2.300e+02  2.370e+02\n",
      "  2.430e+02  2.440e+02  2.440e+02  2.500e+02 -2.560e+02  2.570e+02\n",
      "  2.580e+02  2.600e+02 -2.600e+02  2.680e+02  2.740e+02  2.750e+02\n",
      "  2.810e+02  2.820e+02 -2.850e+02 -2.870e+02  2.910e+02  3.000e+02\n",
      "  3.030e+02  3.070e+02  3.080e+02 -3.100e+02  3.210e+02 -3.230e+02\n",
      "  3.360e+02  3.390e+02  3.400e+02 -3.530e+02 -3.540e+02 -3.650e+02\n",
      "  3.700e+02 -3.720e+02  3.760e+02 -3.770e+02 -3.850e+02 -4.080e+02\n",
      "  4.090e+02 -4.090e+02 -4.110e+02  4.140e+02 -4.140e+02 -4.150e+02\n",
      " -4.160e+02 -4.170e+02 -4.220e+02 -4.230e+02 -4.240e+02 -4.260e+02\n",
      " -4.260e+02 -4.270e+02  4.280e+02  4.290e+02  4.340e+02  4.340e+02\n",
      " -4.350e+02 -4.350e+02 -4.400e+02 -4.420e+02  4.440e+02  4.440e+02\n",
      " -4.460e+02 -4.480e+02 -4.480e+02 -4.550e+02 -4.570e+02  4.570e+02\n",
      "  4.600e+02 -4.620e+02  4.640e+02 -4.660e+02 -4.670e+02  4.680e+02\n",
      "  4.680e+02  4.690e+02 -4.760e+02 -4.770e+02 -4.780e+02  4.780e+02\n",
      " -4.840e+02 -4.860e+02 -4.870e+02  4.880e+02 -4.920e+02 -4.990e+02\n",
      "  5.000e+02  5.030e+02 -5.050e+02 -5.130e+02 -5.150e+02 -5.150e+02\n",
      " -5.220e+02 -5.260e+02 -5.310e+02 -5.340e+02 -5.370e+02 -5.390e+02\n",
      " -5.450e+02 -5.460e+02 -5.470e+02  5.500e+02 -5.510e+02 -5.520e+02\n",
      "  5.570e+02 -5.640e+02 -5.670e+02 -5.680e+02 -5.680e+02 -5.680e+02\n",
      " -5.730e+02 -5.780e+02 -5.780e+02 -5.780e+02  5.820e+02  5.860e+02\n",
      " -5.910e+02 -5.920e+02  5.930e+02  5.940e+02 -5.950e+02 -5.960e+02\n",
      "  5.980e+02 -5.990e+02 -6.010e+02 -6.030e+02 -6.030e+02 -6.050e+02\n",
      " -6.050e+02 -6.070e+02  6.070e+02 -6.080e+02 -6.100e+02 -6.100e+02\n",
      " -6.140e+02  6.140e+02 -6.170e+02 -6.240e+02  6.240e+02  6.250e+02\n",
      " -6.260e+02  6.260e+02 -6.260e+02  6.280e+02 -6.300e+02 -6.340e+02\n",
      " -6.510e+02 -6.510e+02 -6.520e+02 -6.520e+02 -6.530e+02  6.530e+02\n",
      "  6.560e+02 -6.570e+02 -6.580e+02 -6.580e+02 -6.640e+02  6.660e+02\n",
      " -6.690e+02 -6.700e+02 -6.740e+02  6.770e+02 -6.770e+02 -6.830e+02\n",
      " -6.890e+02 -6.910e+02  6.940e+02  6.970e+02  7.010e+02  7.020e+02\n",
      " -7.040e+02 -7.050e+02 -7.050e+02  7.110e+02 -7.180e+02 -7.190e+02\n",
      " -7.280e+02 -7.300e+02  7.370e+02 -7.390e+02 -7.400e+02 -7.410e+02\n",
      " -7.470e+02  7.600e+02 -7.610e+02  7.610e+02 -7.610e+02 -7.730e+02\n",
      " -7.910e+02 -7.910e+02 -7.910e+02 -7.910e+02  8.000e+02 -8.000e+02\n",
      " -8.050e+02 -8.060e+02  8.070e+02  8.080e+02 -8.130e+02 -8.240e+02\n",
      " -8.240e+02  8.260e+02 -8.270e+02 -8.290e+02 -8.300e+02 -8.420e+02\n",
      " -8.450e+02 -8.520e+02  8.550e+02 -8.620e+02 -8.630e+02 -8.640e+02\n",
      "  8.640e+02 -8.660e+02  8.690e+02 -8.720e+02 -8.820e+02 -8.880e+02\n",
      " -8.890e+02 -8.890e+02 -8.960e+02  8.960e+02  9.050e+02 -9.100e+02\n",
      " -9.120e+02 -9.130e+02  9.220e+02  9.290e+02 -9.300e+02 -9.380e+02\n",
      " -9.440e+02 -9.470e+02 -9.490e+02  9.500e+02  9.760e+02  9.870e+02\n",
      " -9.880e+02 -9.930e+02  9.950e+02  9.950e+02 -9.970e+02  9.990e+02\n",
      " -1.013e+03  1.026e+03 -1.036e+03  1.043e+03  1.046e+03 -1.060e+03\n",
      " -1.071e+03 -1.072e+03 -1.079e+03 -1.097e+03  1.115e+03 -1.118e+03\n",
      " -1.125e+03 -1.126e+03 -1.126e+03 -1.130e+03  1.135e+03  1.147e+03\n",
      " -1.148e+03  1.167e+03 -1.175e+03 -1.178e+03 -1.189e+03  1.194e+03\n",
      "  1.197e+03  1.209e+03  1.229e+03 -1.239e+03  1.265e+03  1.268e+03\n",
      " -1.272e+03 -1.285e+03  1.288e+03 -1.289e+03  1.293e+03 -1.301e+03\n",
      " -1.305e+03 -1.333e+03 -1.351e+03  1.379e+03  1.421e+03 -1.429e+03\n",
      " -1.431e+03 -1.442e+03  1.454e+03 -1.474e+03 -1.479e+03  1.492e+03\n",
      "  1.498e+03  1.501e+03  1.516e+03 -1.523e+03  1.600e+03 -1.617e+03\n",
      " -1.621e+03  1.622e+03  1.653e+03 -1.683e+03 -1.750e+03  1.778e+03\n",
      "  1.790e+03  1.798e+03  1.830e+03 -1.847e+03 -1.864e+03 -1.870e+03\n",
      " -1.893e+03 -1.965e+03  2.027e+03 -2.065e+03 -2.067e+03 -2.109e+03\n",
      " -2.137e+03 -2.161e+03  2.174e+03 -2.199e+03 -2.248e+03 -2.360e+03\n",
      "  2.393e+03 -2.449e+03 -2.488e+03 -2.515e+03 -2.590e+03 -2.595e+03\n",
      " -2.616e+03  2.617e+03  2.620e+03 -2.676e+03  2.681e+03 -2.823e+03\n",
      " -2.832e+03 -3.059e+03  3.169e+03 -3.635e+03 -3.674e+03 -3.759e+03\n",
      " -3.940e+03 -4.765e+03  4.961e+03 -4.992e+03 -7.062e+03 -7.248e+03]\n",
      "durations 19.0 6732.0\n",
      "Concordance Index 0.4679226069246436\n",
      "Integrated Brier Score: 0.18435247712338215\n",
      "split gex_?|100130426      float32\n",
      "gex_?|100133144      float32\n",
      "gex_?|100134869      float32\n",
      "gex_?|10357          float32\n",
      "gex_?|10431          float32\n",
      "                      ...   \n",
      "gex_ZYG11A|440590    float32\n",
      "gex_ZYG11B|79699     float32\n",
      "gex_ZYX|7791         float32\n",
      "gex_ZZEF1|23140      float32\n",
      "gex_ZZZ3|26009       float32\n",
      "Length: 20531, dtype: object\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m278.0762\u001b[0m  0.0027\n",
      "      2      278.0762  0.0020\n",
      "      3      278.0762  0.0018\n",
      "      4      278.0762  0.0018\n",
      "      5      278.0762  0.0024\n",
      "      6      278.0762  0.0018\n",
      "      7      278.0762  0.0066\n",
      "      8      278.0762  0.0032\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m265.8808\u001b[0m  0.0029\n",
      "      9      278.0762  0.0065\n",
      "      2      265.8808  0.0020\n",
      "     10      278.0762  0.0018\n",
      "Restoring best model from epoch 1.\n",
      "      3      265.8808  0.0018\n",
      "      4      265.8808  0.0031\n",
      "      5      265.8808  0.0028\n",
      "      6      265.8808  0.0018\n",
      "      7      265.8808  0.0018\n",
      "      8      265.8808  0.0018\n",
      "      9      265.8808  0.0018\n",
      "     10      265.8808  0.0020\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m470.2995\u001b[0m  0.0031\n",
      "      2      470.2995  0.0019\n",
      "      3      470.2995  0.0019\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m249.9177\u001b[0m  0.0032  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "\n",
      "      1       \u001b[36m93.5572\u001b[0m  0.0041\n",
      "      2      249.9177  0.0027\n",
      "      4      470.2995  0.0034\n",
      "      2       93.5572  0.0042\n",
      "      5      470.2995  0.0026\n",
      "      3      249.9177  0.0042\n",
      "      6      470.2995  0.0020\n",
      "      3       93.5572  0.0038\n",
      "      4      249.9177  0.0020\n",
      "      7      470.2995  0.0020\n",
      "      5      249.9177  0.0019\n",
      "      4       93.5572  0.0032\n",
      "      6      249.9177  0.0018\n",
      "      8      470.2995  0.0031\n",
      "      5       93.5572  0.0028\n",
      "      7      249.9177  0.0018\n",
      "      9      470.2995  0.0020\n",
      "      8      249.9177  0.0017\n",
      "      6       93.5572  0.0027\n",
      "     10      470.2995  0.0019\n",
      "Restoring best model from epoch 1.\n",
      "      9      249.9177  0.0017\n",
      "      7       93.5572  0.0027\n",
      "      8       93.5572  0.0028\n",
      "     10      249.9177  0.0051\n",
      "Restoring best model from epoch 1.\n",
      "      9       93.5572  0.0029\n",
      "     10       93.5572  0.0028\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m492.6743\u001b[0m  0.0028\n",
      "      2      492.6743  0.0019\n",
      "      3      492.6743  0.0018\n",
      "      4      492.6743  0.0017\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      5      492.6743  0.0017\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      6      492.6743  0.0024\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      7      492.6743  0.0050\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m97.7922\u001b[0m  0.0053\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      8      492.6743  0.0039\n",
      "      2       97.7922  0.0029\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      3       97.7922  0.0027\n",
      "      9      492.6743  0.0033\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m109.6825\u001b[0m  0.0070\n",
      "     10      492.6743  0.0018\n",
      "      4       97.7922  0.0026\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m105.7274\u001b[0m  0.0069\n",
      "Restoring best model from epoch 1.\n",
      "      2      109.6825  0.0028\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m108.6187\u001b[0m  0.0040\n",
      "      5       97.7922  0.0026\n",
      "      3      109.6825  0.0027\n",
      "      2      105.7274  0.0029\n",
      "      2      108.6187  0.0028\n",
      "      6       97.7922  0.0030\n",
      "      3      105.7274  0.0027\n",
      "      4      109.6825  0.0027\n",
      "      3      108.6187  0.0028\n",
      "      4      105.7274  0.0027\n",
      "      7       97.7922  0.0044\n",
      "      4      108.6187  0.0028\n",
      "      5      105.7274  0.0027\n",
      "      5      108.6187  0.0027\n",
      "      8       97.7922  0.0029\n",
      "      5      109.6825  0.0073\n",
      "      6      105.7274  0.0027\n",
      "      6      108.6187  0.0027\n",
      "      9       97.7922  0.0027\n",
      "      7      105.7274  0.0027\n",
      "     10       97.7922  0.0026\n",
      "      6      109.6825  0.0040\n",
      "      7      108.6187  0.0027\n",
      "Restoring best model from epoch 1.\n",
      "      8      105.7274  0.0026\n",
      "      8      108.6187  0.0027\n",
      "      7      109.6825  0.0027\n",
      "      9      105.7274  0.0027\n",
      "      9      108.6187  0.0027\n",
      "      8      109.6825  0.0027\n",
      "     10      105.7274  0.0027\n",
      "Restoring best model from epoch 1.\n",
      "      9      109.6825  0.0026\n",
      "     10      108.6187  0.0027\n",
      "Restoring best model from epoch 1.\n",
      "     10      109.6825  0.0026\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m151.2519\u001b[0m  0.0034\n",
      "      2      151.2519  0.0041\n",
      "      3      151.2519  0.0037\n",
      "      4      151.2519  0.0036\n",
      "      5      151.2519  0.0035\n",
      "      6      151.2519  0.0035\n",
      "      7      151.2519  0.0056\n",
      "      8      151.2519  0.0049\n",
      "      9      151.2519  0.0053\n",
      "     10      151.2519  0.0044\n",
      "Restoring best model from epoch 1.\n",
      "y_train breslow final [ 2.000e+00  3.000e+00 -4.000e+00  5.000e+00  5.000e+00  6.000e+00\n",
      " -8.000e+00  9.000e+00 -1.200e+01  1.200e+01 -1.200e+01 -1.300e+01\n",
      " -1.300e+01 -1.500e+01  1.700e+01  2.300e+01  2.400e+01 -2.800e+01\n",
      " -3.000e+01 -3.000e+01 -3.000e+01 -3.100e+01 -3.100e+01  3.400e+01\n",
      " -3.500e+01 -4.200e+01  4.700e+01  5.300e+01  5.900e+01 -6.000e+01\n",
      " -6.100e+01 -6.100e+01 -6.100e+01 -6.100e+01 -6.100e+01 -6.100e+01\n",
      " -6.100e+01 -7.000e+01  8.000e+01 -8.300e+01  8.400e+01  8.500e+01\n",
      "  8.800e+01 -8.900e+01  8.900e+01  9.200e+01  9.400e+01 -9.700e+01\n",
      " -1.030e+02 -1.050e+02 -1.060e+02  1.090e+02 -1.110e+02 -1.110e+02\n",
      "  1.160e+02 -1.210e+02 -1.220e+02 -1.220e+02 -1.220e+02 -1.220e+02\n",
      "  1.230e+02 -1.280e+02  1.310e+02  1.360e+02  1.380e+02  1.430e+02\n",
      "  1.460e+02 -1.500e+02  1.510e+02 -1.530e+02  1.530e+02 -1.600e+02\n",
      "  1.610e+02  1.660e+02  1.660e+02 -1.730e+02  1.800e+02  1.880e+02\n",
      "  1.950e+02  1.980e+02  1.980e+02 -2.020e+02 -2.070e+02 -2.110e+02\n",
      "  2.110e+02  2.120e+02  2.150e+02  2.230e+02 -2.270e+02 -2.360e+02\n",
      " -2.380e+02 -2.420e+02 -2.440e+02  2.660e+02  2.740e+02 -2.890e+02\n",
      "  2.910e+02  2.940e+02  2.990e+02  3.020e+02  3.070e+02  3.110e+02\n",
      "  3.150e+02  3.220e+02 -3.240e+02  3.290e+02  3.400e+02  3.400e+02\n",
      "  3.450e+02 -3.530e+02 -3.580e+02 -3.580e+02  3.600e+02  3.650e+02\n",
      "  3.660e+02 -3.660e+02 -3.670e+02 -3.700e+02  3.710e+02  3.720e+02\n",
      " -3.770e+02 -3.780e+02  3.830e+02  3.830e+02  3.970e+02  3.990e+02\n",
      " -4.010e+02  4.020e+02 -4.050e+02 -4.060e+02 -4.070e+02  4.080e+02\n",
      "  4.120e+02  4.180e+02 -4.200e+02 -4.230e+02 -4.280e+02  4.290e+02\n",
      " -4.300e+02 -4.320e+02 -4.400e+02  4.420e+02  4.450e+02 -4.480e+02\n",
      "  4.480e+02  4.540e+02  4.550e+02  4.560e+02 -4.810e+02 -4.820e+02\n",
      "  4.900e+02 -4.920e+02 -4.980e+02  5.010e+02 -5.100e+02 -5.100e+02\n",
      " -5.110e+02  5.150e+02  5.160e+02 -5.170e+02  5.390e+02  5.430e+02\n",
      "  5.520e+02 -5.550e+02 -5.570e+02  5.590e+02 -5.590e+02  5.620e+02\n",
      " -5.650e+02 -5.700e+02 -5.760e+02 -5.780e+02 -5.780e+02 -5.790e+02\n",
      " -5.810e+02 -5.850e+02 -5.890e+02 -5.960e+02 -6.000e+02 -6.010e+02\n",
      "  6.010e+02  6.040e+02 -6.080e+02 -6.150e+02 -6.180e+02  6.230e+02\n",
      " -6.300e+02  6.320e+02 -6.370e+02 -6.400e+02 -6.410e+02 -6.420e+02\n",
      "  6.450e+02 -6.600e+02 -6.660e+02  6.690e+02  6.780e+02  6.840e+02\n",
      "  6.870e+02 -6.880e+02 -6.990e+02 -7.000e+02  7.080e+02  7.160e+02\n",
      " -7.170e+02  7.340e+02 -7.340e+02  7.400e+02 -7.410e+02 -7.570e+02\n",
      " -7.580e+02 -7.620e+02 -7.650e+02 -7.690e+02  7.700e+02 -7.900e+02\n",
      "  8.030e+02 -8.040e+02 -8.040e+02 -8.160e+02 -8.180e+02 -8.220e+02\n",
      "  8.220e+02  8.260e+02 -8.260e+02 -8.330e+02  8.350e+02  8.400e+02\n",
      " -8.420e+02 -8.490e+02 -8.670e+02  8.810e+02 -9.080e+02 -9.100e+02\n",
      " -9.110e+02 -9.110e+02  9.160e+02  9.210e+02  9.270e+02  9.330e+02\n",
      "  9.370e+02 -9.370e+02 -9.420e+02  9.510e+02 -9.520e+02 -9.600e+02\n",
      "  9.650e+02  9.730e+02  9.740e+02 -9.830e+02 -9.950e+02  1.000e+03\n",
      "  1.001e+03  1.006e+03 -1.007e+03 -1.032e+03  1.045e+03 -1.050e+03\n",
      "  1.057e+03 -1.058e+03 -1.072e+03  1.075e+03 -1.092e+03  1.097e+03\n",
      " -1.100e+03 -1.106e+03  1.107e+03 -1.111e+03 -1.113e+03  1.114e+03\n",
      "  1.143e+03  1.150e+03 -1.160e+03 -1.176e+03 -1.180e+03 -1.182e+03\n",
      "  1.189e+03  1.190e+03 -1.196e+03 -1.212e+03 -1.217e+03 -1.224e+03\n",
      " -1.259e+03 -1.260e+03 -1.268e+03 -1.280e+03 -1.297e+03 -1.311e+03\n",
      "  1.315e+03 -1.336e+03  1.338e+03  1.344e+03  1.346e+03 -1.361e+03\n",
      " -1.386e+03  1.423e+03  1.426e+03  1.470e+03 -1.475e+03  1.485e+03\n",
      " -1.492e+03 -1.499e+03 -1.505e+03 -1.519e+03 -1.531e+03 -1.541e+03\n",
      " -1.602e+03 -1.639e+03  1.640e+03 -1.645e+03  1.655e+03  1.656e+03\n",
      "  1.679e+03 -1.682e+03 -1.690e+03  1.695e+03  1.713e+03 -1.723e+03\n",
      " -1.731e+03  1.736e+03 -1.743e+03 -1.784e+03 -1.824e+03 -1.834e+03\n",
      "  1.841e+03 -1.845e+03  1.856e+03 -1.884e+03  1.912e+03 -1.927e+03\n",
      "  1.933e+03  1.953e+03 -1.963e+03  1.975e+03  1.984e+03 -1.992e+03\n",
      " -1.997e+03 -2.023e+03 -2.024e+03 -2.073e+03 -2.133e+03  2.133e+03\n",
      " -2.134e+03 -2.148e+03  2.160e+03 -2.167e+03  2.224e+03 -2.227e+03\n",
      " -2.271e+03  2.284e+03  2.304e+03 -2.336e+03  2.378e+03 -2.381e+03\n",
      "  2.409e+03 -2.419e+03 -2.447e+03 -2.471e+03 -2.542e+03 -2.589e+03\n",
      "  2.625e+03  2.639e+03 -2.645e+03  2.680e+03  2.803e+03 -2.811e+03\n",
      " -2.820e+03  2.945e+03  2.979e+03 -3.016e+03 -3.108e+03 -3.123e+03\n",
      " -3.166e+03 -3.189e+03  3.253e+03  3.376e+03 -3.387e+03 -3.576e+03\n",
      "  3.600e+03 -3.636e+03 -3.644e+03 -3.724e+03 -3.747e+03 -3.850e+03\n",
      "  3.924e+03 -4.026e+03 -4.053e+03 -4.068e+03 -4.261e+03 -4.570e+03\n",
      "  4.601e+03 -4.694e+03 -4.765e+03  5.287e+03]\n",
      "Concordance Index 0.539204913644944\n",
      "Integrated Brier Score: 0.19272873148231495\n",
      "y_train breslow final [ 2.000e+00  3.000e+00 -4.000e+00  5.000e+00  5.000e+00  6.000e+00\n",
      " -8.000e+00  9.000e+00 -1.200e+01  1.200e+01 -1.200e+01 -1.300e+01\n",
      " -1.300e+01 -1.500e+01  1.700e+01  2.300e+01  2.400e+01 -2.800e+01\n",
      " -3.000e+01 -3.000e+01 -3.000e+01 -3.100e+01 -3.100e+01  3.400e+01\n",
      " -3.500e+01 -4.200e+01  4.700e+01  5.300e+01  5.900e+01 -6.000e+01\n",
      " -6.100e+01 -6.100e+01 -6.100e+01 -6.100e+01 -6.100e+01 -6.100e+01\n",
      " -6.100e+01 -7.000e+01  8.000e+01 -8.300e+01  8.400e+01  8.500e+01\n",
      "  8.800e+01 -8.900e+01  8.900e+01  9.200e+01  9.400e+01 -9.700e+01\n",
      " -1.030e+02 -1.050e+02 -1.060e+02  1.090e+02 -1.110e+02 -1.110e+02\n",
      "  1.160e+02 -1.210e+02 -1.220e+02 -1.220e+02 -1.220e+02 -1.220e+02\n",
      "  1.230e+02 -1.280e+02  1.310e+02  1.360e+02  1.380e+02  1.430e+02\n",
      "  1.460e+02 -1.500e+02  1.510e+02 -1.530e+02  1.530e+02 -1.600e+02\n",
      "  1.610e+02  1.660e+02  1.660e+02 -1.730e+02  1.800e+02  1.880e+02\n",
      "  1.950e+02  1.980e+02  1.980e+02 -2.020e+02 -2.070e+02 -2.110e+02\n",
      "  2.110e+02  2.120e+02  2.150e+02  2.230e+02 -2.270e+02 -2.360e+02\n",
      " -2.380e+02 -2.420e+02 -2.440e+02  2.660e+02  2.740e+02 -2.890e+02\n",
      "  2.910e+02  2.940e+02  2.990e+02  3.020e+02  3.070e+02  3.110e+02\n",
      "  3.150e+02  3.220e+02 -3.240e+02  3.290e+02  3.400e+02  3.400e+02\n",
      "  3.450e+02 -3.530e+02 -3.580e+02 -3.580e+02  3.600e+02  3.650e+02\n",
      "  3.660e+02 -3.660e+02 -3.670e+02 -3.700e+02  3.710e+02  3.720e+02\n",
      " -3.770e+02 -3.780e+02  3.830e+02  3.830e+02  3.970e+02  3.990e+02\n",
      " -4.010e+02  4.020e+02 -4.050e+02 -4.060e+02 -4.070e+02  4.080e+02\n",
      "  4.120e+02  4.180e+02 -4.200e+02 -4.230e+02 -4.280e+02  4.290e+02\n",
      " -4.300e+02 -4.320e+02 -4.400e+02  4.420e+02  4.450e+02 -4.480e+02\n",
      "  4.480e+02  4.540e+02  4.550e+02  4.560e+02 -4.810e+02 -4.820e+02\n",
      "  4.900e+02 -4.920e+02 -4.980e+02  5.010e+02 -5.100e+02 -5.100e+02\n",
      " -5.110e+02  5.150e+02  5.160e+02 -5.170e+02  5.390e+02  5.430e+02\n",
      "  5.520e+02 -5.550e+02 -5.570e+02  5.590e+02 -5.590e+02  5.620e+02\n",
      " -5.650e+02 -5.700e+02 -5.760e+02 -5.780e+02 -5.780e+02 -5.790e+02\n",
      " -5.810e+02 -5.850e+02 -5.890e+02 -5.960e+02 -6.000e+02 -6.010e+02\n",
      "  6.010e+02  6.040e+02 -6.080e+02 -6.150e+02 -6.180e+02  6.230e+02\n",
      " -6.300e+02  6.320e+02 -6.370e+02 -6.400e+02 -6.410e+02 -6.420e+02\n",
      "  6.450e+02 -6.600e+02 -6.660e+02  6.690e+02  6.780e+02  6.840e+02\n",
      "  6.870e+02 -6.880e+02 -6.990e+02 -7.000e+02  7.080e+02  7.160e+02\n",
      " -7.170e+02  7.340e+02 -7.340e+02  7.400e+02 -7.410e+02 -7.570e+02\n",
      " -7.580e+02 -7.620e+02 -7.650e+02 -7.690e+02  7.700e+02 -7.900e+02\n",
      "  8.030e+02 -8.040e+02 -8.040e+02 -8.160e+02 -8.180e+02 -8.220e+02\n",
      "  8.220e+02  8.260e+02 -8.260e+02 -8.330e+02  8.350e+02  8.400e+02\n",
      " -8.420e+02 -8.490e+02 -8.670e+02  8.810e+02 -9.080e+02 -9.100e+02\n",
      " -9.110e+02 -9.110e+02  9.160e+02  9.210e+02  9.270e+02  9.330e+02\n",
      "  9.370e+02 -9.370e+02 -9.420e+02  9.510e+02 -9.520e+02 -9.600e+02\n",
      "  9.650e+02  9.730e+02  9.740e+02 -9.830e+02 -9.950e+02  1.000e+03\n",
      "  1.001e+03  1.006e+03 -1.007e+03 -1.032e+03  1.045e+03 -1.050e+03\n",
      "  1.057e+03 -1.058e+03 -1.072e+03  1.075e+03 -1.092e+03  1.097e+03\n",
      " -1.100e+03 -1.106e+03  1.107e+03 -1.111e+03 -1.113e+03  1.114e+03\n",
      "  1.143e+03  1.150e+03 -1.160e+03 -1.176e+03 -1.180e+03 -1.182e+03\n",
      "  1.189e+03  1.190e+03 -1.196e+03 -1.212e+03 -1.217e+03 -1.224e+03\n",
      " -1.259e+03 -1.260e+03 -1.268e+03 -1.280e+03 -1.297e+03 -1.311e+03\n",
      "  1.315e+03 -1.336e+03  1.338e+03  1.344e+03  1.346e+03 -1.361e+03\n",
      " -1.386e+03  1.423e+03  1.426e+03  1.470e+03 -1.475e+03  1.485e+03\n",
      " -1.492e+03 -1.499e+03 -1.505e+03 -1.519e+03 -1.531e+03 -1.541e+03\n",
      " -1.602e+03 -1.639e+03  1.640e+03 -1.645e+03  1.655e+03  1.656e+03\n",
      "  1.679e+03 -1.682e+03 -1.690e+03  1.695e+03  1.713e+03 -1.723e+03\n",
      " -1.731e+03  1.736e+03 -1.743e+03 -1.784e+03 -1.824e+03 -1.834e+03\n",
      "  1.841e+03 -1.845e+03  1.856e+03 -1.884e+03  1.912e+03 -1.927e+03\n",
      "  1.933e+03  1.953e+03 -1.963e+03  1.975e+03  1.984e+03 -1.992e+03\n",
      " -1.997e+03 -2.023e+03 -2.024e+03 -2.073e+03 -2.133e+03  2.133e+03\n",
      " -2.134e+03 -2.148e+03  2.160e+03 -2.167e+03  2.224e+03 -2.227e+03\n",
      " -2.271e+03  2.284e+03  2.304e+03 -2.336e+03  2.378e+03 -2.381e+03\n",
      "  2.409e+03 -2.419e+03 -2.447e+03 -2.471e+03 -2.542e+03 -2.589e+03\n",
      "  2.625e+03  2.639e+03 -2.645e+03  2.680e+03  2.803e+03 -2.811e+03\n",
      " -2.820e+03  2.945e+03  2.979e+03 -3.016e+03 -3.108e+03 -3.123e+03\n",
      " -3.166e+03 -3.189e+03  3.253e+03  3.376e+03 -3.387e+03 -3.576e+03\n",
      "  3.600e+03 -3.636e+03 -3.644e+03 -3.724e+03 -3.747e+03 -3.850e+03\n",
      "  3.924e+03 -4.026e+03 -4.053e+03 -4.068e+03 -4.261e+03 -4.570e+03\n",
      "  4.601e+03 -4.694e+03 -4.765e+03  5.287e+03]\n",
      "durations 1.0 3838.0\n",
      "Concordance Index 0.44848225737494657\n",
      "Integrated Brier Score: 0.19718916458333732\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m297.8870\u001b[0m  0.0038\n",
      "      2      297.8870  0.0072\n",
      "      3      297.8870  0.0031\n",
      "      4      297.8870  0.0049\n",
      "      5      297.8870  0.0033\n",
      "      6      297.8870  0.0019\n",
      "      7      297.8870  0.0017\n",
      "      8      297.8870  0.0017\n",
      "      9      297.8870  0.0017\n",
      "     10      297.8870  0.0017\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m247.4434\u001b[0m  0.0023\n",
      "      2      247.4434  0.0019\n",
      "      3      247.4434  0.0018\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      4      247.4434  0.0018\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m78.2312\u001b[0m  0.0039\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m223.7235\u001b[0m  0.0020\n",
      "      2       78.2312  0.0029\n",
      "      2      223.7235  0.0018\n",
      "      5      247.4434  0.0054\n",
      "      3       78.2312  0.0027      3      223.7235  0.0019\n",
      "\n",
      "      6      247.4434  0.0018\n",
      "      4      223.7235  0.0017\n",
      "      4       78.2312  0.0027\n",
      "      7      247.4434  0.0018\n",
      "      5      223.7235  0.0016\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      6      223.7235  0.0016\n",
      "      8      247.4434  0.0019\n",
      "      5       78.2312  0.0026\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      7      223.7235  0.0016\n",
      "      9      247.4434  0.0020\n",
      "      6       78.2312  0.0026\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      8      223.7235  0.0016\n",
      "     10      247.4434  0.0018\n",
      "Restoring best model from epoch 1.\n",
      "      7       78.2312  0.0026\n",
      "      9      223.7235  0.0020\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m91.6354\u001b[0m  0.0053\n",
      "     10      223.7235  0.0020\n",
      "Restoring best model from epoch 1.\n",
      "      8       78.2312  0.0026\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2       91.6354  0.0029\n",
      "      9       78.2312  0.0026\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m82.5905\u001b[0m  0.0072\n",
      "      3       91.6354  0.0027\n",
      "     10       78.2312  0.0026\n",
      "Restoring best model from epoch 1.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m206.0622\u001b[0m  0.0026\n",
      "      2       82.5905  0.0029\n",
      "      2      206.0622  0.0018\n",
      "      4       91.6354  0.0026\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      3      206.0622  0.0017\n",
      "      3       82.5905  0.0026\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      4      206.0622  0.0017\n",
      "      4       82.5905  0.0026\n",
      "      5       91.6354  0.0053\n",
      "      5      206.0622  0.0017\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      6      206.0622  0.0017\n",
      "      5       82.5905  0.0026\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m98.5682\u001b[0m  0.0032\n",
      "      6       91.6354  0.0028\n",
      "      7      206.0622  0.0023\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m266.3711\u001b[0m  0.0026\n",
      "      2       98.5682  0.0027\n",
      "      6       82.5905  0.0029\n",
      "      7       91.6354  0.0026\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      8      206.0622  0.0018\n",
      "      2      266.3711  0.0018\n",
      "      7       82.5905  0.0026\n",
      "      3       98.5682  0.0028\n",
      "      8       91.6354  0.0026\n",
      "      9      206.0622  0.0018\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      3      266.3711  0.0017\n",
      "     10      206.0622  0.0017\n",
      "Restoring best model from epoch 1.\n",
      "      4      266.3711  0.0019\n",
      "      8       82.5905  0.0026\n",
      "      9       91.6354  0.0026\n",
      "      4       98.5682  0.0040\n",
      "      9       82.5905  0.0025\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m86.6718\u001b[0m  0.0030\n",
      "     10       91.6354  0.0026\n",
      "Restoring best model from epoch 1.\n",
      "      5       98.5682  0.0027\n",
      "      5      266.3711  0.0039\n",
      "     10       82.5905  0.0025\n",
      "Restoring best model from epoch 1.\n",
      "      2       86.6718  0.0028\n",
      "      6       98.5682  0.0026\n",
      "      6      266.3711  0.0031\n",
      "      3       86.6718  0.0027\n",
      "      7      266.3711  0.0017\n",
      "      7       98.5682  0.0027\n",
      "      8      266.3711  0.0017\n",
      "      4       86.6718  0.0027\n",
      "      8       98.5682  0.0027\n",
      "      9      266.3711  0.0016\n",
      "      5       86.6718  0.0027\n",
      "     10      266.3711  0.0016\n",
      "Restoring best model from epoch 1.\n",
      "      9       98.5682  0.0026\n",
      "      6       86.6718  0.0026\n",
      "      7       86.6718  0.0026\n",
      "      8       86.6718  0.0026\n",
      "      9       86.6718  0.0031\n",
      "     10       98.5682  0.0109\n",
      "Restoring best model from epoch 1.\n",
      "     10       86.6718  0.0026\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m111.3153\u001b[0m  0.0034\n",
      "      2      111.3153  0.0036\n",
      "      3      111.3153  0.0036\n",
      "      4      111.3153  0.0032\n",
      "      5      111.3153  0.0036\n",
      "      6      111.3153  0.0036\n",
      "      7      111.3153  0.0060\n",
      "      8      111.3153  0.0055\n",
      "      9      111.3153  0.0051\n",
      "     10      111.3153  0.0039\n",
      "Restoring best model from epoch 1.\n",
      "y_train breslow final [ 1.000e+00  2.000e+00  3.000e+00  6.000e+00 -1.200e+01  1.200e+01\n",
      " -1.200e+01  1.200e+01 -1.300e+01 -1.500e+01  1.700e+01  2.300e+01\n",
      "  2.400e+01 -3.000e+01 -3.000e+01 -3.100e+01 -3.100e+01  3.400e+01\n",
      " -3.700e+01 -4.100e+01  5.200e+01  5.300e+01 -5.500e+01  5.900e+01\n",
      " -6.000e+01 -6.100e+01 -6.100e+01 -6.100e+01 -6.100e+01 -6.100e+01\n",
      " -6.100e+01 -6.100e+01  6.100e+01  8.000e+01 -8.200e+01 -8.300e+01\n",
      "  8.400e+01 -8.900e+01  8.900e+01  9.200e+01  9.400e+01 -9.700e+01\n",
      " -1.030e+02 -1.050e+02 -1.060e+02  1.090e+02 -1.110e+02 -1.110e+02\n",
      "  1.130e+02  1.160e+02 -1.210e+02 -1.220e+02 -1.220e+02  1.230e+02\n",
      "  1.230e+02 -1.280e+02  1.310e+02 -1.350e+02  1.360e+02  1.380e+02\n",
      " -1.500e+02  1.510e+02  1.530e+02  1.610e+02  1.660e+02  1.660e+02\n",
      " -1.730e+02  1.800e+02  1.880e+02  1.950e+02  1.980e+02  1.980e+02\n",
      " -2.020e+02  2.150e+02  2.230e+02 -2.270e+02 -2.360e+02  2.360e+02\n",
      " -2.420e+02 -2.470e+02  2.610e+02  2.660e+02  2.740e+02  2.760e+02\n",
      "  2.840e+02 -2.890e+02  2.910e+02  2.940e+02 -2.960e+02  3.060e+02\n",
      "  3.070e+02  3.150e+02  3.220e+02 -3.240e+02  3.290e+02  3.400e+02\n",
      "  3.450e+02  3.450e+02  3.510e+02 -3.530e+02  3.570e+02  3.580e+02\n",
      " -3.580e+02 -3.580e+02  3.580e+02  3.600e+02  3.650e+02  3.660e+02\n",
      " -3.700e+02  3.720e+02 -3.750e+02 -3.770e+02 -3.780e+02  3.830e+02\n",
      "  3.870e+02 -3.920e+02 -3.960e+02  3.970e+02 -4.010e+02  4.020e+02\n",
      "  4.030e+02 -4.050e+02  4.080e+02  4.120e+02  4.180e+02  4.280e+02\n",
      " -4.280e+02 -4.280e+02 -4.300e+02 -4.320e+02 -4.400e+02  4.450e+02\n",
      " -4.480e+02  4.480e+02  4.540e+02 -4.550e+02  4.560e+02 -4.650e+02\n",
      " -4.680e+02  4.740e+02 -4.810e+02 -4.820e+02  4.900e+02 -4.910e+02\n",
      " -4.930e+02 -4.980e+02  5.060e+02 -5.100e+02 -5.100e+02 -5.110e+02\n",
      "  5.150e+02  5.160e+02  5.190e+02 -5.310e+02  5.320e+02 -5.380e+02\n",
      "  5.390e+02  5.430e+02  5.440e+02  5.500e+02  5.520e+02 -5.550e+02\n",
      " -5.570e+02  5.590e+02  5.620e+02 -5.700e+02  5.730e+02 -5.780e+02\n",
      " -5.780e+02 -5.790e+02 -5.810e+02 -5.850e+02  5.870e+02 -5.960e+02\n",
      " -6.000e+02  6.010e+02  6.040e+02 -6.080e+02 -6.160e+02  6.230e+02\n",
      " -6.300e+02  6.320e+02 -6.390e+02 -6.400e+02 -6.400e+02 -6.410e+02\n",
      "  6.450e+02 -6.490e+02 -6.520e+02 -6.530e+02 -6.660e+02  6.670e+02\n",
      " -6.710e+02  6.780e+02 -6.830e+02  6.840e+02  6.870e+02 -6.880e+02\n",
      "  6.920e+02 -6.990e+02 -6.990e+02 -6.990e+02 -7.000e+02  7.080e+02\n",
      "  7.160e+02 -7.170e+02 -7.340e+02  7.400e+02 -7.410e+02 -7.570e+02\n",
      " -7.590e+02 -7.620e+02 -7.650e+02 -7.690e+02 -7.900e+02 -8.040e+02\n",
      " -8.100e+02 -8.150e+02 -8.160e+02 -8.220e+02  8.220e+02  8.260e+02\n",
      "  8.260e+02 -8.260e+02  8.270e+02  8.350e+02  8.400e+02 -8.420e+02\n",
      " -8.490e+02 -8.620e+02 -8.670e+02  8.990e+02 -9.080e+02 -9.100e+02\n",
      " -9.100e+02 -9.110e+02 -9.110e+02 -9.110e+02 -9.130e+02  9.160e+02\n",
      " -9.230e+02 -9.250e+02  9.270e+02 -9.320e+02  9.330e+02 -9.370e+02\n",
      " -9.420e+02  9.510e+02  9.620e+02  9.650e+02  9.740e+02 -9.790e+02\n",
      " -9.830e+02 -9.900e+02 -9.950e+02  1.000e+03  1.001e+03  1.006e+03\n",
      " -1.007e+03 -1.011e+03 -1.031e+03 -1.032e+03 -1.038e+03 -1.052e+03\n",
      "  1.058e+03 -1.058e+03 -1.063e+03  1.067e+03 -1.072e+03  1.075e+03\n",
      " -1.092e+03  1.097e+03 -1.100e+03 -1.106e+03 -1.111e+03 -1.113e+03\n",
      "  1.114e+03  1.143e+03  1.150e+03  1.154e+03 -1.160e+03  1.161e+03\n",
      " -1.176e+03 -1.180e+03 -1.182e+03  1.190e+03 -1.196e+03 -1.212e+03\n",
      " -1.217e+03 -1.223e+03 -1.224e+03 -1.244e+03 -1.260e+03 -1.268e+03\n",
      " -1.280e+03 -1.297e+03 -1.311e+03  1.315e+03  1.335e+03 -1.336e+03\n",
      "  1.338e+03  1.346e+03  1.423e+03  1.426e+03  1.470e+03 -1.475e+03\n",
      " -1.492e+03 -1.505e+03 -1.519e+03 -1.531e+03 -1.541e+03 -1.645e+03\n",
      "  1.655e+03  1.679e+03 -1.682e+03 -1.690e+03  1.695e+03  1.713e+03\n",
      " -1.723e+03 -1.743e+03 -1.784e+03 -1.824e+03 -1.834e+03 -1.841e+03\n",
      " -1.845e+03  1.856e+03  1.874e+03 -1.884e+03 -1.927e+03  1.933e+03\n",
      "  1.953e+03 -1.963e+03  1.975e+03  1.984e+03 -1.992e+03 -1.997e+03\n",
      " -2.023e+03 -2.024e+03 -2.026e+03 -2.073e+03 -2.080e+03  2.086e+03\n",
      " -2.133e+03  2.133e+03 -2.134e+03 -2.142e+03 -2.148e+03  2.160e+03\n",
      " -2.165e+03 -2.167e+03  2.170e+03 -2.271e+03  2.284e+03  2.304e+03\n",
      " -2.336e+03  2.378e+03  2.409e+03 -2.447e+03 -2.471e+03 -2.510e+03\n",
      " -2.589e+03  2.625e+03  2.639e+03 -2.645e+03  2.680e+03 -2.716e+03\n",
      " -2.811e+03 -2.820e+03  2.945e+03  2.979e+03 -3.016e+03 -3.108e+03\n",
      " -3.123e+03  3.149e+03 -3.166e+03 -3.189e+03  3.253e+03 -3.387e+03\n",
      " -3.576e+03  3.600e+03 -3.636e+03 -3.724e+03 -3.747e+03  3.838e+03\n",
      " -3.850e+03  3.924e+03 -4.053e+03 -4.068e+03 -4.261e+03 -4.570e+03\n",
      "  4.601e+03 -4.694e+03 -4.765e+03  5.287e+03]\n",
      "Concordance Index 0.5434966793298532\n",
      "Integrated Brier Score: 0.18527250435322287\n",
      "y_train breslow final [ 1.000e+00  2.000e+00  3.000e+00  6.000e+00 -1.200e+01  1.200e+01\n",
      " -1.200e+01  1.200e+01 -1.300e+01 -1.500e+01  1.700e+01  2.300e+01\n",
      "  2.400e+01 -3.000e+01 -3.000e+01 -3.100e+01 -3.100e+01  3.400e+01\n",
      " -3.700e+01 -4.100e+01  5.200e+01  5.300e+01 -5.500e+01  5.900e+01\n",
      " -6.000e+01 -6.100e+01 -6.100e+01 -6.100e+01 -6.100e+01 -6.100e+01\n",
      " -6.100e+01 -6.100e+01  6.100e+01  8.000e+01 -8.200e+01 -8.300e+01\n",
      "  8.400e+01 -8.900e+01  8.900e+01  9.200e+01  9.400e+01 -9.700e+01\n",
      " -1.030e+02 -1.050e+02 -1.060e+02  1.090e+02 -1.110e+02 -1.110e+02\n",
      "  1.130e+02  1.160e+02 -1.210e+02 -1.220e+02 -1.220e+02  1.230e+02\n",
      "  1.230e+02 -1.280e+02  1.310e+02 -1.350e+02  1.360e+02  1.380e+02\n",
      " -1.500e+02  1.510e+02  1.530e+02  1.610e+02  1.660e+02  1.660e+02\n",
      " -1.730e+02  1.800e+02  1.880e+02  1.950e+02  1.980e+02  1.980e+02\n",
      " -2.020e+02  2.150e+02  2.230e+02 -2.270e+02 -2.360e+02  2.360e+02\n",
      " -2.420e+02 -2.470e+02  2.610e+02  2.660e+02  2.740e+02  2.760e+02\n",
      "  2.840e+02 -2.890e+02  2.910e+02  2.940e+02 -2.960e+02  3.060e+02\n",
      "  3.070e+02  3.150e+02  3.220e+02 -3.240e+02  3.290e+02  3.400e+02\n",
      "  3.450e+02  3.450e+02  3.510e+02 -3.530e+02  3.570e+02  3.580e+02\n",
      " -3.580e+02 -3.580e+02  3.580e+02  3.600e+02  3.650e+02  3.660e+02\n",
      " -3.700e+02  3.720e+02 -3.750e+02 -3.770e+02 -3.780e+02  3.830e+02\n",
      "  3.870e+02 -3.920e+02 -3.960e+02  3.970e+02 -4.010e+02  4.020e+02\n",
      "  4.030e+02 -4.050e+02  4.080e+02  4.120e+02  4.180e+02  4.280e+02\n",
      " -4.280e+02 -4.280e+02 -4.300e+02 -4.320e+02 -4.400e+02  4.450e+02\n",
      " -4.480e+02  4.480e+02  4.540e+02 -4.550e+02  4.560e+02 -4.650e+02\n",
      " -4.680e+02  4.740e+02 -4.810e+02 -4.820e+02  4.900e+02 -4.910e+02\n",
      " -4.930e+02 -4.980e+02  5.060e+02 -5.100e+02 -5.100e+02 -5.110e+02\n",
      "  5.150e+02  5.160e+02  5.190e+02 -5.310e+02  5.320e+02 -5.380e+02\n",
      "  5.390e+02  5.430e+02  5.440e+02  5.500e+02  5.520e+02 -5.550e+02\n",
      " -5.570e+02  5.590e+02  5.620e+02 -5.700e+02  5.730e+02 -5.780e+02\n",
      " -5.780e+02 -5.790e+02 -5.810e+02 -5.850e+02  5.870e+02 -5.960e+02\n",
      " -6.000e+02  6.010e+02  6.040e+02 -6.080e+02 -6.160e+02  6.230e+02\n",
      " -6.300e+02  6.320e+02 -6.390e+02 -6.400e+02 -6.400e+02 -6.410e+02\n",
      "  6.450e+02 -6.490e+02 -6.520e+02 -6.530e+02 -6.660e+02  6.670e+02\n",
      " -6.710e+02  6.780e+02 -6.830e+02  6.840e+02  6.870e+02 -6.880e+02\n",
      "  6.920e+02 -6.990e+02 -6.990e+02 -6.990e+02 -7.000e+02  7.080e+02\n",
      "  7.160e+02 -7.170e+02 -7.340e+02  7.400e+02 -7.410e+02 -7.570e+02\n",
      " -7.590e+02 -7.620e+02 -7.650e+02 -7.690e+02 -7.900e+02 -8.040e+02\n",
      " -8.100e+02 -8.150e+02 -8.160e+02 -8.220e+02  8.220e+02  8.260e+02\n",
      "  8.260e+02 -8.260e+02  8.270e+02  8.350e+02  8.400e+02 -8.420e+02\n",
      " -8.490e+02 -8.620e+02 -8.670e+02  8.990e+02 -9.080e+02 -9.100e+02\n",
      " -9.100e+02 -9.110e+02 -9.110e+02 -9.110e+02 -9.130e+02  9.160e+02\n",
      " -9.230e+02 -9.250e+02  9.270e+02 -9.320e+02  9.330e+02 -9.370e+02\n",
      " -9.420e+02  9.510e+02  9.620e+02  9.650e+02  9.740e+02 -9.790e+02\n",
      " -9.830e+02 -9.900e+02 -9.950e+02  1.000e+03  1.001e+03  1.006e+03\n",
      " -1.007e+03 -1.011e+03 -1.031e+03 -1.032e+03 -1.038e+03 -1.052e+03\n",
      "  1.058e+03 -1.058e+03 -1.063e+03  1.067e+03 -1.072e+03  1.075e+03\n",
      " -1.092e+03  1.097e+03 -1.100e+03 -1.106e+03 -1.111e+03 -1.113e+03\n",
      "  1.114e+03  1.143e+03  1.150e+03  1.154e+03 -1.160e+03  1.161e+03\n",
      " -1.176e+03 -1.180e+03 -1.182e+03  1.190e+03 -1.196e+03 -1.212e+03\n",
      " -1.217e+03 -1.223e+03 -1.224e+03 -1.244e+03 -1.260e+03 -1.268e+03\n",
      " -1.280e+03 -1.297e+03 -1.311e+03  1.315e+03  1.335e+03 -1.336e+03\n",
      "  1.338e+03  1.346e+03  1.423e+03  1.426e+03  1.470e+03 -1.475e+03\n",
      " -1.492e+03 -1.505e+03 -1.519e+03 -1.531e+03 -1.541e+03 -1.645e+03\n",
      "  1.655e+03  1.679e+03 -1.682e+03 -1.690e+03  1.695e+03  1.713e+03\n",
      " -1.723e+03 -1.743e+03 -1.784e+03 -1.824e+03 -1.834e+03 -1.841e+03\n",
      " -1.845e+03  1.856e+03  1.874e+03 -1.884e+03 -1.927e+03  1.933e+03\n",
      "  1.953e+03 -1.963e+03  1.975e+03  1.984e+03 -1.992e+03 -1.997e+03\n",
      " -2.023e+03 -2.024e+03 -2.026e+03 -2.073e+03 -2.080e+03  2.086e+03\n",
      " -2.133e+03  2.133e+03 -2.134e+03 -2.142e+03 -2.148e+03  2.160e+03\n",
      " -2.165e+03 -2.167e+03  2.170e+03 -2.271e+03  2.284e+03  2.304e+03\n",
      " -2.336e+03  2.378e+03  2.409e+03 -2.447e+03 -2.471e+03 -2.510e+03\n",
      " -2.589e+03  2.625e+03  2.639e+03 -2.645e+03  2.680e+03 -2.716e+03\n",
      " -2.811e+03 -2.820e+03  2.945e+03  2.979e+03 -3.016e+03 -3.108e+03\n",
      " -3.123e+03  3.149e+03 -3.166e+03 -3.189e+03  3.253e+03 -3.387e+03\n",
      " -3.576e+03  3.600e+03 -3.636e+03 -3.724e+03 -3.747e+03  3.838e+03\n",
      " -3.850e+03  3.924e+03 -4.053e+03 -4.068e+03 -4.261e+03 -4.570e+03\n",
      "  4.601e+03 -4.694e+03 -4.765e+03  5.287e+03]\n",
      "durations 4.0 4026.0\n",
      "Concordance Index 0.4574938574938575\n",
      "Integrated Brier Score: 0.2003813492438247\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m238.2901\u001b[0m  0.0021\n",
      "      2      238.2901  0.0018\n",
      "      3      238.2901  0.0017\n",
      "      4      238.2901  0.0017\n",
      "      5      238.2901  0.0017\n",
      "      6      238.2901  0.0016\n",
      "      7      238.2901  0.0016\n",
      "      8      238.2901  0.0016\n",
      "      9      238.2901  0.0016\n",
      "     10      238.2901  0.0016\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m166.5340\u001b[0m  0.0021\n",
      "      2      166.5340  0.0018\n",
      "      3      166.5340  0.0017\n",
      "      4      166.5340  0.0017\n",
      "      5      166.5340  0.0016\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      6      166.5340  0.0017\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      7      166.5340  0.0017\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m221.2088\u001b[0m  0.0022\n",
      "      8      166.5340  0.0016\n",
      "      9      166.5340  0.0016\n",
      "      2      221.2088  0.0019\n",
      "     10      166.5340  0.0016\n",
      "Restoring best model from epoch 1.\n",
      "      3      221.2088  0.0017\n",
      "      4      221.2088  0.0017\n",
      "      5      221.2088  0.0017\n",
      "      6      221.2088  0.0017\n",
      "      7      221.2088  0.0017\n",
      "      8      221.2088  0.0017\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      9      221.2088  0.0017Re-initializing criterion.\n",
      "\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m262.3083\u001b[0m  0.0025\n",
      "     10      221.2088  0.0045\n",
      "Restoring best model from epoch 1.\n",
      "      2      262.3083  0.0018\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      3      262.3083  0.0018\n",
      "      4      262.3083  0.0018\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      5      262.3083  0.0017\n",
      "      6      262.3083  0.0017\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m71.1638\u001b[0m  0.0029\n",
      "      7      262.3083  0.0018\n",
      "      8      262.3083  0.0017\n",
      "      2       71.1638  0.0032\n",
      "      9      262.3083  0.0017\n",
      "      3       71.1638  0.0028\n",
      "     10      262.3083  0.0017\n",
      "Restoring best model from epoch 1.\n",
      "      4       71.1638  0.0025\n",
      "      5       71.1638  0.0026\n",
      "      6       71.1638  0.0027\n",
      "      7       71.1638  0.0026\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      8       71.1638  0.0025\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      9       71.1638  0.0025\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m93.4060\u001b[0m  0.0029\n",
      "      2       93.4060  0.0027\n",
      "     10       71.1638  0.0063\n",
      "Restoring best model from epoch 1.\n",
      "      3       93.4060  0.0027\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m79.5770\u001b[0m  0.0070\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2       79.5770  0.0027\n",
      "      4       93.4060  0.0057\n",
      "      3       79.5770  0.0025\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m69.1095\u001b[0m  0.0028\n",
      "      2       69.1095  0.0027\n",
      "      4       79.5770  0.0029\n",
      "      5       93.4060  0.0057\n",
      "      5       79.5770  0.0027\n",
      "      3       69.1095  0.0047\n",
      "      6       93.4060  0.0028\n",
      "      6       79.5770  0.0026\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      7       93.4060  0.0027\n",
      "      7       79.5770  0.0026\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      8       79.5770  0.0025\n",
      "      8       93.4060  0.0027\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m83.9120\u001b[0m  0.0032\n",
      "      9       79.5770  0.0025\n",
      "      4       69.1095  0.0085\n",
      "      9       93.4060  0.0028\n",
      "     10       79.5770  0.0025\n",
      "      2       83.9120  0.0029\n",
      "      5       69.1095  0.0026\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "     10       93.4060  0.0026\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      3       83.9120  0.0026\n",
      "      6       69.1095  0.0025\n",
      "      7       69.1095  0.0025\n",
      "      4       83.9120  0.0026\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m231.0837\u001b[0m  0.0048\n",
      "      8       69.1095  0.0026\n",
      "      5       83.9120  0.0026\n",
      "      9       69.1095  0.0025\n",
      "      6       83.9120  0.0026\n",
      "      2      231.0837  0.0035\n",
      "     10       69.1095  0.0024\n",
      "Restoring best model from epoch 1.\n",
      "      7       83.9120  0.0026\n",
      "      3      231.0837  0.0020\n",
      "      4      231.0837  0.0018\n",
      "      8       83.9120  0.0026\n",
      "      5      231.0837  0.0017\n",
      "      9       83.9120  0.0026\n",
      "      6      231.0837  0.0017\n",
      "      7      231.0837  0.0019\n",
      "     10       83.9120  0.0029\n",
      "Restoring best model from epoch 1.\n",
      "      8      231.0837  0.0019\n",
      "      9      231.0837  0.0018\n",
      "     10      231.0837  0.0017\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m99.3288\u001b[0m  0.0034\n",
      "      2       99.3288  0.0046\n",
      "      3       99.3288  0.0035\n",
      "      4       99.3288  0.0033\n",
      "      5       99.3288  0.0036\n",
      "      6       99.3288  0.0036\n",
      "      7       99.3288  0.0036\n",
      "      8       99.3288  0.0030\n",
      "      9       99.3288  0.0031\n",
      "     10       99.3288  0.0051\n",
      "Restoring best model from epoch 1.\n",
      "y_train breslow final [ 1.000e+00  2.000e+00  3.000e+00 -4.000e+00  5.000e+00  5.000e+00\n",
      "  6.000e+00 -8.000e+00  9.000e+00 -1.200e+01  1.200e+01  1.200e+01\n",
      " -1.300e+01 -1.500e+01  2.300e+01  2.400e+01 -2.800e+01 -3.000e+01\n",
      " -3.000e+01 -3.000e+01 -3.100e+01 -3.100e+01  3.400e+01 -3.500e+01\n",
      " -3.700e+01 -4.100e+01 -4.200e+01  4.700e+01  5.200e+01  5.300e+01\n",
      " -5.500e+01  5.900e+01 -6.000e+01 -6.100e+01 -6.100e+01 -6.100e+01\n",
      " -6.100e+01 -6.100e+01 -6.100e+01  6.100e+01 -7.000e+01  8.000e+01\n",
      " -8.200e+01 -8.300e+01  8.500e+01  8.800e+01  8.900e+01  9.200e+01\n",
      "  9.400e+01 -9.700e+01 -1.030e+02 -1.060e+02 -1.060e+02  1.090e+02\n",
      " -1.110e+02  1.130e+02 -1.210e+02 -1.220e+02 -1.220e+02 -1.220e+02\n",
      " -1.220e+02  1.230e+02  1.230e+02  1.310e+02 -1.350e+02  1.430e+02\n",
      "  1.460e+02 -1.500e+02  1.510e+02 -1.530e+02  1.530e+02 -1.600e+02\n",
      "  1.660e+02 -1.730e+02  1.880e+02  1.950e+02 -2.070e+02 -2.110e+02\n",
      "  2.110e+02  2.120e+02 -2.270e+02 -2.360e+02  2.360e+02 -2.380e+02\n",
      " -2.440e+02 -2.470e+02  2.610e+02  2.660e+02  2.760e+02  2.840e+02\n",
      " -2.890e+02  2.910e+02 -2.960e+02  2.990e+02  3.020e+02  3.060e+02\n",
      "  3.070e+02  3.110e+02  3.220e+02  3.290e+02  3.400e+02  3.450e+02\n",
      "  3.450e+02  3.510e+02 -3.530e+02  3.570e+02  3.580e+02 -3.580e+02\n",
      " -3.580e+02  3.580e+02  3.600e+02  3.650e+02  3.660e+02 -3.660e+02\n",
      " -3.670e+02 -3.700e+02  3.710e+02 -3.750e+02 -3.770e+02 -3.780e+02\n",
      "  3.830e+02  3.830e+02  3.870e+02 -3.920e+02 -3.960e+02  3.970e+02\n",
      "  3.990e+02 -4.010e+02  4.030e+02 -4.050e+02 -4.050e+02 -4.060e+02\n",
      " -4.070e+02  4.080e+02  4.180e+02 -4.200e+02 -4.230e+02  4.280e+02\n",
      " -4.280e+02  4.290e+02 -4.300e+02 -4.320e+02  4.420e+02  4.450e+02\n",
      " -4.550e+02  4.550e+02  4.560e+02 -4.650e+02 -4.680e+02  4.740e+02\n",
      " -4.810e+02 -4.820e+02 -4.910e+02 -4.920e+02 -4.930e+02 -4.980e+02\n",
      "  5.010e+02  5.060e+02 -5.100e+02 -5.110e+02  5.150e+02 -5.170e+02\n",
      "  5.190e+02 -5.310e+02  5.320e+02 -5.380e+02  5.390e+02  5.430e+02\n",
      "  5.440e+02  5.500e+02  5.520e+02 -5.570e+02  5.590e+02 -5.590e+02\n",
      "  5.620e+02 -5.650e+02 -5.700e+02  5.730e+02 -5.760e+02 -5.780e+02\n",
      " -5.780e+02 -5.810e+02 -5.850e+02  5.870e+02 -5.890e+02 -6.010e+02\n",
      "  6.040e+02 -6.080e+02 -6.150e+02 -6.160e+02 -6.180e+02  6.230e+02\n",
      " -6.300e+02  6.320e+02 -6.370e+02 -6.390e+02 -6.400e+02 -6.400e+02\n",
      " -6.410e+02 -6.420e+02 -6.490e+02 -6.520e+02 -6.530e+02 -6.600e+02\n",
      " -6.660e+02  6.670e+02  6.690e+02 -6.710e+02 -6.830e+02  6.840e+02\n",
      "  6.870e+02  6.920e+02 -6.990e+02 -6.990e+02 -6.990e+02  7.080e+02\n",
      "  7.160e+02 -7.170e+02  7.340e+02  7.400e+02 -7.580e+02 -7.590e+02\n",
      " -7.690e+02  7.700e+02 -7.900e+02  8.030e+02 -8.040e+02 -8.040e+02\n",
      " -8.100e+02 -8.150e+02 -8.160e+02 -8.180e+02 -8.220e+02  8.220e+02\n",
      "  8.260e+02  8.270e+02 -8.330e+02  8.350e+02  8.400e+02 -8.490e+02\n",
      " -8.620e+02 -8.670e+02  8.810e+02  8.990e+02 -9.080e+02 -9.100e+02\n",
      " -9.100e+02 -9.110e+02 -9.110e+02 -9.130e+02  9.160e+02  9.210e+02\n",
      " -9.230e+02 -9.250e+02 -9.320e+02  9.330e+02  9.370e+02 -9.370e+02\n",
      " -9.420e+02  9.510e+02 -9.520e+02 -9.600e+02  9.620e+02  9.730e+02\n",
      "  9.740e+02 -9.790e+02 -9.830e+02 -9.900e+02 -9.950e+02  1.000e+03\n",
      "  1.001e+03  1.006e+03 -1.007e+03 -1.011e+03 -1.031e+03 -1.032e+03\n",
      " -1.038e+03  1.045e+03 -1.050e+03 -1.052e+03  1.057e+03  1.058e+03\n",
      " -1.063e+03  1.067e+03 -1.100e+03 -1.106e+03  1.107e+03 -1.111e+03\n",
      " -1.113e+03  1.114e+03  1.143e+03  1.154e+03  1.161e+03 -1.176e+03\n",
      "  1.189e+03  1.190e+03 -1.196e+03 -1.212e+03 -1.217e+03 -1.223e+03\n",
      " -1.244e+03 -1.259e+03 -1.260e+03 -1.280e+03 -1.311e+03  1.315e+03\n",
      "  1.335e+03 -1.336e+03  1.338e+03  1.344e+03  1.346e+03 -1.361e+03\n",
      " -1.386e+03  1.423e+03  1.426e+03  1.470e+03 -1.475e+03  1.485e+03\n",
      " -1.499e+03 -1.519e+03 -1.531e+03 -1.541e+03 -1.602e+03 -1.639e+03\n",
      "  1.640e+03  1.656e+03  1.679e+03 -1.690e+03  1.695e+03  1.713e+03\n",
      " -1.723e+03 -1.731e+03  1.736e+03 -1.784e+03 -1.841e+03  1.841e+03\n",
      "  1.874e+03 -1.884e+03  1.912e+03  1.933e+03  1.953e+03 -1.963e+03\n",
      "  1.975e+03  1.984e+03 -1.992e+03 -1.997e+03 -2.023e+03 -2.024e+03\n",
      " -2.026e+03 -2.080e+03  2.086e+03 -2.133e+03  2.133e+03 -2.142e+03\n",
      " -2.148e+03  2.160e+03 -2.165e+03  2.170e+03  2.224e+03 -2.227e+03\n",
      " -2.271e+03  2.284e+03  2.304e+03 -2.336e+03  2.378e+03 -2.381e+03\n",
      " -2.419e+03 -2.447e+03 -2.471e+03 -2.510e+03 -2.542e+03 -2.716e+03\n",
      "  2.803e+03 -2.820e+03  2.979e+03 -3.016e+03 -3.123e+03  3.149e+03\n",
      " -3.189e+03  3.253e+03  3.376e+03 -3.387e+03 -3.636e+03 -3.644e+03\n",
      " -3.747e+03  3.838e+03 -3.850e+03 -4.026e+03 -4.053e+03 -4.261e+03\n",
      " -4.570e+03  4.601e+03 -4.694e+03 -4.765e+03]\n",
      "Concordance Index 0.4926478905101233\n",
      "Integrated Brier Score: 0.1932948382239094\n",
      "y_train breslow final [ 1.000e+00  2.000e+00  3.000e+00 -4.000e+00  5.000e+00  5.000e+00\n",
      "  6.000e+00 -8.000e+00  9.000e+00 -1.200e+01  1.200e+01  1.200e+01\n",
      " -1.300e+01 -1.500e+01  2.300e+01  2.400e+01 -2.800e+01 -3.000e+01\n",
      " -3.000e+01 -3.000e+01 -3.100e+01 -3.100e+01  3.400e+01 -3.500e+01\n",
      " -3.700e+01 -4.100e+01 -4.200e+01  4.700e+01  5.200e+01  5.300e+01\n",
      " -5.500e+01  5.900e+01 -6.000e+01 -6.100e+01 -6.100e+01 -6.100e+01\n",
      " -6.100e+01 -6.100e+01 -6.100e+01  6.100e+01 -7.000e+01  8.000e+01\n",
      " -8.200e+01 -8.300e+01  8.500e+01  8.800e+01  8.900e+01  9.200e+01\n",
      "  9.400e+01 -9.700e+01 -1.030e+02 -1.060e+02 -1.060e+02  1.090e+02\n",
      " -1.110e+02  1.130e+02 -1.210e+02 -1.220e+02 -1.220e+02 -1.220e+02\n",
      " -1.220e+02  1.230e+02  1.230e+02  1.310e+02 -1.350e+02  1.430e+02\n",
      "  1.460e+02 -1.500e+02  1.510e+02 -1.530e+02  1.530e+02 -1.600e+02\n",
      "  1.660e+02 -1.730e+02  1.880e+02  1.950e+02 -2.070e+02 -2.110e+02\n",
      "  2.110e+02  2.120e+02 -2.270e+02 -2.360e+02  2.360e+02 -2.380e+02\n",
      " -2.440e+02 -2.470e+02  2.610e+02  2.660e+02  2.760e+02  2.840e+02\n",
      " -2.890e+02  2.910e+02 -2.960e+02  2.990e+02  3.020e+02  3.060e+02\n",
      "  3.070e+02  3.110e+02  3.220e+02  3.290e+02  3.400e+02  3.450e+02\n",
      "  3.450e+02  3.510e+02 -3.530e+02  3.570e+02  3.580e+02 -3.580e+02\n",
      " -3.580e+02  3.580e+02  3.600e+02  3.650e+02  3.660e+02 -3.660e+02\n",
      " -3.670e+02 -3.700e+02  3.710e+02 -3.750e+02 -3.770e+02 -3.780e+02\n",
      "  3.830e+02  3.830e+02  3.870e+02 -3.920e+02 -3.960e+02  3.970e+02\n",
      "  3.990e+02 -4.010e+02  4.030e+02 -4.050e+02 -4.050e+02 -4.060e+02\n",
      " -4.070e+02  4.080e+02  4.180e+02 -4.200e+02 -4.230e+02  4.280e+02\n",
      " -4.280e+02  4.290e+02 -4.300e+02 -4.320e+02  4.420e+02  4.450e+02\n",
      " -4.550e+02  4.550e+02  4.560e+02 -4.650e+02 -4.680e+02  4.740e+02\n",
      " -4.810e+02 -4.820e+02 -4.910e+02 -4.920e+02 -4.930e+02 -4.980e+02\n",
      "  5.010e+02  5.060e+02 -5.100e+02 -5.110e+02  5.150e+02 -5.170e+02\n",
      "  5.190e+02 -5.310e+02  5.320e+02 -5.380e+02  5.390e+02  5.430e+02\n",
      "  5.440e+02  5.500e+02  5.520e+02 -5.570e+02  5.590e+02 -5.590e+02\n",
      "  5.620e+02 -5.650e+02 -5.700e+02  5.730e+02 -5.760e+02 -5.780e+02\n",
      " -5.780e+02 -5.810e+02 -5.850e+02  5.870e+02 -5.890e+02 -6.010e+02\n",
      "  6.040e+02 -6.080e+02 -6.150e+02 -6.160e+02 -6.180e+02  6.230e+02\n",
      " -6.300e+02  6.320e+02 -6.370e+02 -6.390e+02 -6.400e+02 -6.400e+02\n",
      " -6.410e+02 -6.420e+02 -6.490e+02 -6.520e+02 -6.530e+02 -6.600e+02\n",
      " -6.660e+02  6.670e+02  6.690e+02 -6.710e+02 -6.830e+02  6.840e+02\n",
      "  6.870e+02  6.920e+02 -6.990e+02 -6.990e+02 -6.990e+02  7.080e+02\n",
      "  7.160e+02 -7.170e+02  7.340e+02  7.400e+02 -7.580e+02 -7.590e+02\n",
      " -7.690e+02  7.700e+02 -7.900e+02  8.030e+02 -8.040e+02 -8.040e+02\n",
      " -8.100e+02 -8.150e+02 -8.160e+02 -8.180e+02 -8.220e+02  8.220e+02\n",
      "  8.260e+02  8.270e+02 -8.330e+02  8.350e+02  8.400e+02 -8.490e+02\n",
      " -8.620e+02 -8.670e+02  8.810e+02  8.990e+02 -9.080e+02 -9.100e+02\n",
      " -9.100e+02 -9.110e+02 -9.110e+02 -9.130e+02  9.160e+02  9.210e+02\n",
      " -9.230e+02 -9.250e+02 -9.320e+02  9.330e+02  9.370e+02 -9.370e+02\n",
      " -9.420e+02  9.510e+02 -9.520e+02 -9.600e+02  9.620e+02  9.730e+02\n",
      "  9.740e+02 -9.790e+02 -9.830e+02 -9.900e+02 -9.950e+02  1.000e+03\n",
      "  1.001e+03  1.006e+03 -1.007e+03 -1.011e+03 -1.031e+03 -1.032e+03\n",
      " -1.038e+03  1.045e+03 -1.050e+03 -1.052e+03  1.057e+03  1.058e+03\n",
      " -1.063e+03  1.067e+03 -1.100e+03 -1.106e+03  1.107e+03 -1.111e+03\n",
      " -1.113e+03  1.114e+03  1.143e+03  1.154e+03  1.161e+03 -1.176e+03\n",
      "  1.189e+03  1.190e+03 -1.196e+03 -1.212e+03 -1.217e+03 -1.223e+03\n",
      " -1.244e+03 -1.259e+03 -1.260e+03 -1.280e+03 -1.311e+03  1.315e+03\n",
      "  1.335e+03 -1.336e+03  1.338e+03  1.344e+03  1.346e+03 -1.361e+03\n",
      " -1.386e+03  1.423e+03  1.426e+03  1.470e+03 -1.475e+03  1.485e+03\n",
      " -1.499e+03 -1.519e+03 -1.531e+03 -1.541e+03 -1.602e+03 -1.639e+03\n",
      "  1.640e+03  1.656e+03  1.679e+03 -1.690e+03  1.695e+03  1.713e+03\n",
      " -1.723e+03 -1.731e+03  1.736e+03 -1.784e+03 -1.841e+03  1.841e+03\n",
      "  1.874e+03 -1.884e+03  1.912e+03  1.933e+03  1.953e+03 -1.963e+03\n",
      "  1.975e+03  1.984e+03 -1.992e+03 -1.997e+03 -2.023e+03 -2.024e+03\n",
      " -2.026e+03 -2.080e+03  2.086e+03 -2.133e+03  2.133e+03 -2.142e+03\n",
      " -2.148e+03  2.160e+03 -2.165e+03  2.170e+03  2.224e+03 -2.227e+03\n",
      " -2.271e+03  2.284e+03  2.304e+03 -2.336e+03  2.378e+03 -2.381e+03\n",
      " -2.419e+03 -2.447e+03 -2.471e+03 -2.510e+03 -2.542e+03 -2.716e+03\n",
      "  2.803e+03 -2.820e+03  2.979e+03 -3.016e+03 -3.123e+03  3.149e+03\n",
      " -3.189e+03  3.253e+03  3.376e+03 -3.387e+03 -3.636e+03 -3.644e+03\n",
      " -3.747e+03  3.838e+03 -3.850e+03 -4.026e+03 -4.053e+03 -4.261e+03\n",
      " -4.570e+03  4.601e+03 -4.694e+03 -4.765e+03]\n",
      "durations 12.0 5287.0\n",
      "Concordance Index 0.50709067468844\n",
      "Integrated Brier Score: 0.18994193636499657\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m382.0060\u001b[0m  0.0026\n",
      "      2      382.0060  0.0020\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      3      382.0060  0.0020\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      4      382.0060  0.0019\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m106.7526\u001b[0m  0.0031\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      5      382.0060  0.0044\n",
      "      2      106.7526  0.0028\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m266.5128\u001b[0m  0.0026\n",
      "      3      106.7526  0.0028\n",
      "  epoch    valid_loss     dur      6      382.0060  0.0052\n",
      "\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m178.5278\u001b[0m  0.0024\n",
      "      2      266.5128  0.0020\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m269.4631\u001b[0m  0.0023\n",
      "      4      106.7526  0.0028\n",
      "      3      266.5128  0.0019\n",
      "      7      382.0060  0.0022\n",
      "      2      269.4631  0.0019\n",
      "      4      266.5128  0.0018\n",
      "      8      382.0060  0.0019\n",
      "      3      269.4631  0.0018\n",
      "      5      106.7526  0.0029\n",
      "      2      178.5278  0.0051\n",
      "      5      266.5128  0.0019\n",
      "      4      269.4631  0.0018\n",
      "      9      382.0060  0.0018\n",
      "      6      266.5128  0.0018\n",
      "      5      269.4631  0.0018\n",
      "     10      382.0060  0.0019\n",
      "Restoring best model from epoch 1.\n",
      "      7      266.5128  0.0018\n",
      "      6      269.4631  0.0018\n",
      "      3      178.5278  0.0046\n",
      "      8      266.5128  0.0018\n",
      "      7      269.4631  0.0017\n",
      "      4      178.5278  0.0020\n",
      "      6      106.7526  0.0074\n",
      "      9      266.5128  0.0018\n",
      "      8      269.4631  0.0017\n",
      "      5      178.5278  0.0018\n",
      "     10      266.5128  0.0018\n",
      "Restoring best model from epoch 1.\n",
      "      9      269.4631  0.0017\n",
      "      6      178.5278  0.0018\n",
      "      7      106.7526  0.0038\n",
      "     10      269.4631  0.0017\n",
      "Restoring best model from epoch 1.\n",
      "      7      178.5278  0.0018\n",
      "      8      106.7526  0.0028\n",
      "      8      178.5278  0.0018\n",
      "      9      178.5278  0.0018\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      9      106.7526  0.0028\n",
      "     10      178.5278  0.0018\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing criterion.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing optimizer.\n",
      "     10      106.7526  0.0027\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m87.3122\u001b[0m  0.0031\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m264.6351\u001b[0m  0.0023\n",
      "      2       87.3122  0.0027\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m116.3670\u001b[0m  0.0033\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m119.3021\u001b[0m  0.0030\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m94.8276\u001b[0m  0.0032      3       87.3122  0.0029\n",
      "      2      116.3670  0.0028\n",
      "      2      264.6351  0.0044\n",
      "\n",
      "      2      119.3021  0.0028\n",
      "      3      264.6351  0.0019\n",
      "      4       87.3122  0.0028\n",
      "      2       94.8276  0.0028\n",
      "      3      116.3670  0.0044\n",
      "      5       87.3122  0.0028\n",
      "      4      264.6351  0.0027\n",
      "      3       94.8276  0.0026\n",
      "      4      116.3670  0.0027\n",
      "      6       87.3122  0.0027\n",
      "      4       94.8276  0.0032\n",
      "      3      119.3021  0.0077\n",
      "      5      116.3670  0.0026\n",
      "      7       87.3122  0.0027\n",
      "      5       94.8276  0.0027\n",
      "      6      116.3670  0.0026\n",
      "      4      119.3021  0.0039\n",
      "      8       87.3122  0.0026\n",
      "      5      264.6351  0.0086\n",
      "      6       94.8276  0.0026\n",
      "      6      264.6351  0.0020\n",
      "      5      119.3021  0.0027\n",
      "      9       87.3122  0.0026\n",
      "      7      116.3670  0.0039\n",
      "      7       94.8276  0.0026\n",
      "      7      264.6351  0.0019\n",
      "      6      119.3021  0.0027\n",
      "     10       87.3122  0.0025\n",
      "      8      116.3670  0.0028\n",
      "Restoring best model from epoch 1.\n",
      "      8      264.6351  0.0019\n",
      "      8       94.8276  0.0026\n",
      "      9      264.6351  0.0018      9      116.3670  0.0027\n",
      "\n",
      "      7      119.3021  0.0031\n",
      "      9       94.8276  0.0026\n",
      "     10      264.6351  0.0022\n",
      "Restoring best model from epoch 1.\n",
      "      8      119.3021  0.0026\n",
      "     10       94.8276  0.0026\n",
      "     10      116.3670  0.0036\n",
      "Restoring best model from epoch 1.\n",
      "Restoring best model from epoch 1.\n",
      "      9      119.3021  0.0026\n",
      "     10      119.3021  0.0026\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m135.1295\u001b[0m  0.0034\n",
      "      2      135.1295  0.0039\n",
      "      3      135.1295  0.0037\n",
      "      4      135.1295  0.0043\n",
      "      5      135.1295  0.0048\n",
      "      6      135.1295  0.0041\n",
      "      7      135.1295  0.0080\n",
      "      8      135.1295  0.0076\n",
      "      9      135.1295  0.0039\n",
      "     10      135.1295  0.0044\n",
      "Restoring best model from epoch 1.\n",
      "y_train breslow final [ 1.000e+00  2.000e+00 -4.000e+00  5.000e+00  5.000e+00  6.000e+00\n",
      " -8.000e+00  9.000e+00 -1.200e+01  1.200e+01 -1.200e+01  1.200e+01\n",
      " -1.300e+01 -1.300e+01  1.700e+01  2.400e+01 -2.800e+01 -3.000e+01\n",
      " -3.000e+01 -3.000e+01 -3.500e+01 -3.700e+01 -4.100e+01 -4.200e+01\n",
      "  4.700e+01  5.200e+01  5.300e+01 -5.500e+01 -6.100e+01 -6.100e+01\n",
      " -6.100e+01  6.100e+01 -7.000e+01  8.000e+01 -8.200e+01  8.400e+01\n",
      "  8.500e+01  8.800e+01 -8.900e+01  9.200e+01 -9.700e+01 -1.050e+02\n",
      " -1.060e+02 -1.060e+02 -1.110e+02  1.130e+02  1.160e+02 -1.210e+02\n",
      " -1.220e+02 -1.220e+02  1.230e+02  1.230e+02 -1.280e+02 -1.350e+02\n",
      "  1.360e+02  1.380e+02  1.430e+02  1.460e+02 -1.530e+02  1.530e+02\n",
      " -1.600e+02  1.610e+02  1.660e+02  1.660e+02  1.800e+02  1.950e+02\n",
      "  1.980e+02  1.980e+02 -2.020e+02 -2.070e+02 -2.110e+02  2.110e+02\n",
      "  2.120e+02  2.150e+02  2.230e+02 -2.270e+02 -2.360e+02  2.360e+02\n",
      " -2.380e+02 -2.420e+02 -2.440e+02 -2.470e+02  2.610e+02  2.660e+02\n",
      "  2.740e+02  2.760e+02  2.840e+02 -2.890e+02  2.910e+02  2.940e+02\n",
      " -2.960e+02  2.990e+02  3.020e+02  3.060e+02  3.070e+02  3.110e+02\n",
      "  3.150e+02 -3.240e+02  3.290e+02  3.400e+02  3.400e+02  3.450e+02\n",
      "  3.450e+02  3.510e+02 -3.530e+02  3.570e+02  3.580e+02  3.580e+02\n",
      "  3.650e+02 -3.660e+02 -3.670e+02  3.710e+02  3.720e+02 -3.750e+02\n",
      " -3.780e+02  3.830e+02  3.830e+02  3.870e+02 -3.920e+02 -3.960e+02\n",
      "  3.970e+02  3.990e+02 -4.010e+02  4.020e+02  4.030e+02 -4.050e+02\n",
      " -4.050e+02 -4.060e+02 -4.070e+02  4.120e+02 -4.200e+02 -4.230e+02\n",
      "  4.280e+02 -4.280e+02 -4.280e+02  4.290e+02 -4.320e+02 -4.400e+02\n",
      "  4.420e+02 -4.480e+02  4.480e+02  4.540e+02 -4.550e+02  4.550e+02\n",
      " -4.650e+02 -4.680e+02  4.740e+02 -4.810e+02  4.900e+02 -4.910e+02\n",
      " -4.920e+02 -4.930e+02  5.010e+02  5.060e+02 -5.100e+02 -5.100e+02\n",
      "  5.160e+02 -5.170e+02  5.190e+02 -5.310e+02  5.320e+02 -5.380e+02\n",
      "  5.390e+02  5.440e+02  5.500e+02  5.520e+02 -5.550e+02 -5.570e+02\n",
      "  5.590e+02 -5.590e+02 -5.650e+02  5.730e+02 -5.760e+02 -5.780e+02\n",
      " -5.790e+02  5.870e+02 -5.890e+02 -5.960e+02 -6.000e+02 -6.010e+02\n",
      "  6.010e+02 -6.080e+02 -6.150e+02 -6.160e+02 -6.180e+02  6.230e+02\n",
      " -6.370e+02 -6.390e+02 -6.400e+02 -6.420e+02  6.450e+02 -6.490e+02\n",
      " -6.520e+02 -6.530e+02 -6.600e+02 -6.660e+02  6.670e+02  6.690e+02\n",
      " -6.710e+02  6.780e+02 -6.830e+02  6.870e+02 -6.880e+02  6.920e+02\n",
      " -6.990e+02 -6.990e+02 -6.990e+02 -7.000e+02  7.080e+02  7.340e+02\n",
      " -7.340e+02 -7.410e+02 -7.570e+02 -7.580e+02 -7.590e+02 -7.620e+02\n",
      " -7.650e+02  7.700e+02  8.030e+02 -8.040e+02 -8.100e+02 -8.150e+02\n",
      " -8.180e+02 -8.220e+02  8.260e+02  8.260e+02 -8.260e+02  8.270e+02\n",
      " -8.330e+02 -8.420e+02 -8.620e+02 -8.670e+02  8.810e+02  8.990e+02\n",
      " -9.080e+02 -9.100e+02 -9.110e+02 -9.110e+02 -9.130e+02  9.160e+02\n",
      "  9.210e+02 -9.230e+02 -9.250e+02  9.270e+02 -9.320e+02  9.370e+02\n",
      " -9.370e+02 -9.520e+02 -9.600e+02  9.620e+02  9.650e+02  9.730e+02\n",
      " -9.790e+02 -9.830e+02 -9.900e+02 -9.950e+02  1.000e+03  1.006e+03\n",
      " -1.011e+03 -1.031e+03 -1.038e+03  1.045e+03 -1.050e+03 -1.052e+03\n",
      "  1.057e+03  1.058e+03 -1.058e+03 -1.063e+03  1.067e+03 -1.072e+03\n",
      "  1.075e+03 -1.092e+03  1.097e+03 -1.100e+03  1.107e+03 -1.111e+03\n",
      " -1.113e+03  1.114e+03  1.143e+03  1.150e+03  1.154e+03 -1.160e+03\n",
      "  1.161e+03 -1.180e+03 -1.182e+03  1.189e+03  1.190e+03 -1.212e+03\n",
      " -1.217e+03 -1.223e+03 -1.224e+03 -1.244e+03 -1.259e+03 -1.260e+03\n",
      " -1.268e+03 -1.297e+03 -1.311e+03  1.335e+03  1.344e+03  1.346e+03\n",
      " -1.361e+03 -1.386e+03  1.426e+03  1.470e+03 -1.475e+03  1.485e+03\n",
      " -1.492e+03 -1.499e+03 -1.505e+03 -1.519e+03 -1.541e+03 -1.602e+03\n",
      " -1.639e+03  1.640e+03 -1.645e+03  1.655e+03  1.656e+03  1.679e+03\n",
      " -1.682e+03  1.695e+03 -1.723e+03 -1.731e+03  1.736e+03 -1.743e+03\n",
      " -1.784e+03 -1.824e+03 -1.834e+03 -1.841e+03  1.841e+03 -1.845e+03\n",
      "  1.856e+03  1.874e+03 -1.884e+03  1.912e+03 -1.927e+03  1.953e+03\n",
      "  1.975e+03 -1.992e+03 -1.997e+03 -2.023e+03 -2.024e+03 -2.026e+03\n",
      " -2.073e+03 -2.080e+03  2.086e+03 -2.134e+03 -2.142e+03 -2.148e+03\n",
      "  2.160e+03 -2.165e+03 -2.167e+03  2.170e+03  2.224e+03 -2.227e+03\n",
      " -2.271e+03 -2.336e+03 -2.381e+03  2.409e+03 -2.419e+03 -2.447e+03\n",
      " -2.471e+03 -2.510e+03 -2.542e+03 -2.589e+03  2.625e+03  2.639e+03\n",
      " -2.645e+03  2.680e+03 -2.716e+03  2.803e+03 -2.811e+03  2.945e+03\n",
      "  2.979e+03 -3.016e+03 -3.108e+03 -3.123e+03  3.149e+03 -3.166e+03\n",
      " -3.189e+03  3.253e+03  3.376e+03 -3.576e+03  3.600e+03 -3.644e+03\n",
      " -3.724e+03 -3.747e+03  3.838e+03 -3.850e+03  3.924e+03 -4.026e+03\n",
      " -4.053e+03 -4.068e+03  4.601e+03 -4.694e+03  5.287e+03]\n",
      "Concordance Index 0.5320161996497373\n",
      "Integrated Brier Score: 0.17791216964965256\n",
      "y_train breslow final [ 1.000e+00  2.000e+00 -4.000e+00  5.000e+00  5.000e+00  6.000e+00\n",
      " -8.000e+00  9.000e+00 -1.200e+01  1.200e+01 -1.200e+01  1.200e+01\n",
      " -1.300e+01 -1.300e+01  1.700e+01  2.400e+01 -2.800e+01 -3.000e+01\n",
      " -3.000e+01 -3.000e+01 -3.500e+01 -3.700e+01 -4.100e+01 -4.200e+01\n",
      "  4.700e+01  5.200e+01  5.300e+01 -5.500e+01 -6.100e+01 -6.100e+01\n",
      " -6.100e+01  6.100e+01 -7.000e+01  8.000e+01 -8.200e+01  8.400e+01\n",
      "  8.500e+01  8.800e+01 -8.900e+01  9.200e+01 -9.700e+01 -1.050e+02\n",
      " -1.060e+02 -1.060e+02 -1.110e+02  1.130e+02  1.160e+02 -1.210e+02\n",
      " -1.220e+02 -1.220e+02  1.230e+02  1.230e+02 -1.280e+02 -1.350e+02\n",
      "  1.360e+02  1.380e+02  1.430e+02  1.460e+02 -1.530e+02  1.530e+02\n",
      " -1.600e+02  1.610e+02  1.660e+02  1.660e+02  1.800e+02  1.950e+02\n",
      "  1.980e+02  1.980e+02 -2.020e+02 -2.070e+02 -2.110e+02  2.110e+02\n",
      "  2.120e+02  2.150e+02  2.230e+02 -2.270e+02 -2.360e+02  2.360e+02\n",
      " -2.380e+02 -2.420e+02 -2.440e+02 -2.470e+02  2.610e+02  2.660e+02\n",
      "  2.740e+02  2.760e+02  2.840e+02 -2.890e+02  2.910e+02  2.940e+02\n",
      " -2.960e+02  2.990e+02  3.020e+02  3.060e+02  3.070e+02  3.110e+02\n",
      "  3.150e+02 -3.240e+02  3.290e+02  3.400e+02  3.400e+02  3.450e+02\n",
      "  3.450e+02  3.510e+02 -3.530e+02  3.570e+02  3.580e+02  3.580e+02\n",
      "  3.650e+02 -3.660e+02 -3.670e+02  3.710e+02  3.720e+02 -3.750e+02\n",
      " -3.780e+02  3.830e+02  3.830e+02  3.870e+02 -3.920e+02 -3.960e+02\n",
      "  3.970e+02  3.990e+02 -4.010e+02  4.020e+02  4.030e+02 -4.050e+02\n",
      " -4.050e+02 -4.060e+02 -4.070e+02  4.120e+02 -4.200e+02 -4.230e+02\n",
      "  4.280e+02 -4.280e+02 -4.280e+02  4.290e+02 -4.320e+02 -4.400e+02\n",
      "  4.420e+02 -4.480e+02  4.480e+02  4.540e+02 -4.550e+02  4.550e+02\n",
      " -4.650e+02 -4.680e+02  4.740e+02 -4.810e+02  4.900e+02 -4.910e+02\n",
      " -4.920e+02 -4.930e+02  5.010e+02  5.060e+02 -5.100e+02 -5.100e+02\n",
      "  5.160e+02 -5.170e+02  5.190e+02 -5.310e+02  5.320e+02 -5.380e+02\n",
      "  5.390e+02  5.440e+02  5.500e+02  5.520e+02 -5.550e+02 -5.570e+02\n",
      "  5.590e+02 -5.590e+02 -5.650e+02  5.730e+02 -5.760e+02 -5.780e+02\n",
      " -5.790e+02  5.870e+02 -5.890e+02 -5.960e+02 -6.000e+02 -6.010e+02\n",
      "  6.010e+02 -6.080e+02 -6.150e+02 -6.160e+02 -6.180e+02  6.230e+02\n",
      " -6.370e+02 -6.390e+02 -6.400e+02 -6.420e+02  6.450e+02 -6.490e+02\n",
      " -6.520e+02 -6.530e+02 -6.600e+02 -6.660e+02  6.670e+02  6.690e+02\n",
      " -6.710e+02  6.780e+02 -6.830e+02  6.870e+02 -6.880e+02  6.920e+02\n",
      " -6.990e+02 -6.990e+02 -6.990e+02 -7.000e+02  7.080e+02  7.340e+02\n",
      " -7.340e+02 -7.410e+02 -7.570e+02 -7.580e+02 -7.590e+02 -7.620e+02\n",
      " -7.650e+02  7.700e+02  8.030e+02 -8.040e+02 -8.100e+02 -8.150e+02\n",
      " -8.180e+02 -8.220e+02  8.260e+02  8.260e+02 -8.260e+02  8.270e+02\n",
      " -8.330e+02 -8.420e+02 -8.620e+02 -8.670e+02  8.810e+02  8.990e+02\n",
      " -9.080e+02 -9.100e+02 -9.110e+02 -9.110e+02 -9.130e+02  9.160e+02\n",
      "  9.210e+02 -9.230e+02 -9.250e+02  9.270e+02 -9.320e+02  9.370e+02\n",
      " -9.370e+02 -9.520e+02 -9.600e+02  9.620e+02  9.650e+02  9.730e+02\n",
      " -9.790e+02 -9.830e+02 -9.900e+02 -9.950e+02  1.000e+03  1.006e+03\n",
      " -1.011e+03 -1.031e+03 -1.038e+03  1.045e+03 -1.050e+03 -1.052e+03\n",
      "  1.057e+03  1.058e+03 -1.058e+03 -1.063e+03  1.067e+03 -1.072e+03\n",
      "  1.075e+03 -1.092e+03  1.097e+03 -1.100e+03  1.107e+03 -1.111e+03\n",
      " -1.113e+03  1.114e+03  1.143e+03  1.150e+03  1.154e+03 -1.160e+03\n",
      "  1.161e+03 -1.180e+03 -1.182e+03  1.189e+03  1.190e+03 -1.212e+03\n",
      " -1.217e+03 -1.223e+03 -1.224e+03 -1.244e+03 -1.259e+03 -1.260e+03\n",
      " -1.268e+03 -1.297e+03 -1.311e+03  1.335e+03  1.344e+03  1.346e+03\n",
      " -1.361e+03 -1.386e+03  1.426e+03  1.470e+03 -1.475e+03  1.485e+03\n",
      " -1.492e+03 -1.499e+03 -1.505e+03 -1.519e+03 -1.541e+03 -1.602e+03\n",
      " -1.639e+03  1.640e+03 -1.645e+03  1.655e+03  1.656e+03  1.679e+03\n",
      " -1.682e+03  1.695e+03 -1.723e+03 -1.731e+03  1.736e+03 -1.743e+03\n",
      " -1.784e+03 -1.824e+03 -1.834e+03 -1.841e+03  1.841e+03 -1.845e+03\n",
      "  1.856e+03  1.874e+03 -1.884e+03  1.912e+03 -1.927e+03  1.953e+03\n",
      "  1.975e+03 -1.992e+03 -1.997e+03 -2.023e+03 -2.024e+03 -2.026e+03\n",
      " -2.073e+03 -2.080e+03  2.086e+03 -2.134e+03 -2.142e+03 -2.148e+03\n",
      "  2.160e+03 -2.165e+03 -2.167e+03  2.170e+03  2.224e+03 -2.227e+03\n",
      " -2.271e+03 -2.336e+03 -2.381e+03  2.409e+03 -2.419e+03 -2.447e+03\n",
      " -2.471e+03 -2.510e+03 -2.542e+03 -2.589e+03  2.625e+03  2.639e+03\n",
      " -2.645e+03  2.680e+03 -2.716e+03  2.803e+03 -2.811e+03  2.945e+03\n",
      "  2.979e+03 -3.016e+03 -3.108e+03 -3.123e+03  3.149e+03 -3.166e+03\n",
      " -3.189e+03  3.253e+03  3.376e+03 -3.576e+03  3.600e+03 -3.644e+03\n",
      " -3.724e+03 -3.747e+03  3.838e+03 -3.850e+03  3.924e+03 -4.026e+03\n",
      " -4.053e+03 -4.068e+03  4.601e+03 -4.694e+03  5.287e+03]\n",
      "durations 3.0 4765.0\n",
      "Concordance Index 0.5825621042377009\n",
      "Integrated Brier Score: 0.19109648565934462\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m221.5611\u001b[0m  0.0024\n",
      "      2      221.5611  0.0020\n",
      "      3      221.5611  0.0019\n",
      "      4      221.5611  0.0018\n",
      "      5      221.5611  0.0024\n",
      "      6      221.5611  0.0020\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      7      221.5611  0.0019\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "\n",
      "Re-initializing optimizer.\n",
      "      8      221.5611  0.0019\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m313.8798\u001b[0m  0.0024\n",
      "      9      221.5611  0.0018\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2      313.8798  0.0019\n",
      "     10      221.5611  0.0018\n",
      "Restoring best model from epoch 1.\n",
      "      3      313.8798  0.0018\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m237.4372\u001b[0m  0.0051\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m111.4244\u001b[0m  0.0031\n",
      "      4      313.8798  0.0018\n",
      "      2      237.4372  0.0020\n",
      "      2      111.4244  0.0028\n",
      "      5      313.8798  0.0019\n",
      "      3      237.4372  0.0019\n",
      "      6      313.8798  0.0019\n",
      "      3      111.4244  0.0028\n",
      "      4      237.4372  0.0019\n",
      "      7      313.8798  0.0017\n",
      "      5      237.4372  0.0018\n",
      "      4      111.4244  0.0027\n",
      "      8      313.8798  0.0017\n",
      "      6      237.4372  0.0018\n",
      "      9      313.8798  0.0017\n",
      "      5      111.4244  0.0026\n",
      "      7      237.4372  0.0017\n",
      "     10      313.8798  0.0017\n",
      "Restoring best model from epoch 1.\n",
      "      8      237.4372  0.0020\n",
      "      9      237.4372  0.0021      6      111.4244  0.0041\n",
      "\n",
      "     10      237.4372  0.0018\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      7      111.4244  0.0086\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m98.6412\u001b[0m  0.0032\n",
      "      8      111.4244  0.0030\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m109.9196\u001b[0m  0.0032\n",
      "      2       98.6412  0.0027\n",
      "      9      111.4244  0.0027\n",
      "      2      109.9196  0.0029\n",
      "      3       98.6412  0.0027\n",
      "     10      111.4244  0.0026\n",
      "Restoring best model from epoch 1.\n",
      "      3      109.9196  0.0027\n",
      "      4       98.6412  0.0027\n",
      "      4      109.9196  0.0027\n",
      "      5       98.6412  0.0026\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      5      109.9196  0.0027\n",
      "      6       98.6412  0.0026\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      7       98.6412  0.0026\n",
      "      8       98.6412  0.0026\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m99.6905\u001b[0m  0.0030\n",
      "      6      109.9196  0.0062\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      9       98.6412  0.0026\n",
      "      2       99.6905  0.0027\n",
      "     10       98.6412  0.0025\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Restoring best model from epoch 1.\n",
      "      3       99.6905  0.0026\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m246.9321\u001b[0m  0.0025\n",
      "      7      109.9196  0.0057\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2      246.9321  0.0018\n",
      "      4       99.6905  0.0026\n",
      "      3      246.9321  0.0017\n",
      "      8      109.9196  0.0028\n",
      "      5       99.6905  0.0026\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m92.1767\u001b[0m  0.0031\n",
      "      4      246.9321  0.0018\n",
      "      9      109.9196  0.0027\n",
      "      5      246.9321  0.0018\n",
      "      6       99.6905  0.0026\n",
      "      2       92.1767  0.0028\n",
      "     10      109.9196  0.0027\n",
      "Restoring best model from epoch 1.\n",
      "      7       99.6905  0.0026\n",
      "      6      246.9321  0.0028\n",
      "      3       92.1767  0.0026\n",
      "      8       99.6905  0.0026\n",
      "      4       92.1767  0.0026\n",
      "      7      246.9321  0.0048\n",
      "      9       99.6905  0.0026\n",
      "      5       92.1767  0.0026\n",
      "     10       99.6905  0.0026\n",
      "Restoring best model from epoch 1.\n",
      "      6       92.1767  0.0026\n",
      "      8      246.9321  0.0035\n",
      "      7       92.1767  0.0026\n",
      "      9      246.9321  0.0020\n",
      "     10      246.9321  0.0018\n",
      "Restoring best model from epoch 1.\n",
      "      8       92.1767  0.0026\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      9       92.1767  0.0039\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m260.5679\u001b[0m  0.0023\n",
      "      2      260.5679  0.0018\n",
      "      3      260.5679  0.0039\n",
      "     10       92.1767  0.0069\n",
      "Restoring best model from epoch 1.\n",
      "      4      260.5679  0.0020\n",
      "      5      260.5679  0.0017\n",
      "      6      260.5679  0.0017\n",
      "      7      260.5679  0.0017\n",
      "      8      260.5679  0.0032\n",
      "      9      260.5679  0.0044\n",
      "     10      260.5679  0.0018\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m107.2824\u001b[0m  0.0031\n",
      "      2      107.2824  0.0037\n",
      "      3      107.2824  0.0035\n",
      "      4      107.2824  0.0035\n",
      "      5      107.2824  0.0035\n",
      "      6      107.2824  0.0047\n",
      "      7      107.2824  0.0055\n",
      "      8      107.2824  0.0047\n",
      "      9      107.2824  0.0084\n",
      "     10      107.2824  0.0105\n",
      "Restoring best model from epoch 1.\n",
      "y_train breslow final [ 1.000e+00  3.000e+00 -4.000e+00  5.000e+00  5.000e+00 -8.000e+00\n",
      "  9.000e+00 -1.200e+01  1.200e+01 -1.300e+01 -1.300e+01 -1.500e+01\n",
      "  1.700e+01  2.300e+01 -2.800e+01 -3.000e+01 -3.100e+01 -3.100e+01\n",
      "  3.400e+01 -3.500e+01 -3.700e+01 -4.100e+01 -4.200e+01  4.700e+01\n",
      "  5.200e+01 -5.500e+01  5.900e+01 -6.000e+01 -6.100e+01 -6.100e+01\n",
      " -6.100e+01 -6.100e+01 -6.100e+01  6.100e+01 -7.000e+01 -8.200e+01\n",
      " -8.300e+01  8.400e+01  8.500e+01  8.800e+01 -8.900e+01  8.900e+01\n",
      "  9.400e+01 -1.030e+02 -1.050e+02 -1.060e+02 -1.060e+02  1.090e+02\n",
      " -1.110e+02 -1.110e+02  1.130e+02  1.160e+02 -1.220e+02 -1.220e+02\n",
      " -1.220e+02 -1.220e+02  1.230e+02 -1.280e+02  1.310e+02 -1.350e+02\n",
      "  1.360e+02  1.380e+02  1.430e+02  1.460e+02 -1.500e+02  1.510e+02\n",
      " -1.530e+02 -1.600e+02  1.610e+02  1.660e+02 -1.730e+02  1.800e+02\n",
      "  1.880e+02  1.980e+02  1.980e+02 -2.020e+02 -2.070e+02 -2.110e+02\n",
      "  2.110e+02  2.120e+02  2.150e+02  2.230e+02  2.360e+02 -2.380e+02\n",
      " -2.420e+02 -2.440e+02 -2.470e+02  2.610e+02  2.740e+02  2.760e+02\n",
      "  2.840e+02  2.940e+02 -2.960e+02  2.990e+02  3.020e+02  3.060e+02\n",
      "  3.110e+02  3.150e+02  3.220e+02 -3.240e+02  3.400e+02  3.400e+02\n",
      "  3.450e+02  3.510e+02  3.570e+02  3.580e+02 -3.580e+02 -3.580e+02\n",
      "  3.580e+02  3.600e+02  3.660e+02 -3.660e+02 -3.670e+02 -3.700e+02\n",
      "  3.710e+02  3.720e+02 -3.750e+02 -3.770e+02  3.830e+02  3.870e+02\n",
      " -3.920e+02 -3.960e+02  3.990e+02  4.020e+02  4.030e+02 -4.050e+02\n",
      " -4.050e+02 -4.060e+02 -4.070e+02  4.080e+02  4.120e+02  4.180e+02\n",
      " -4.200e+02 -4.230e+02  4.280e+02 -4.280e+02 -4.280e+02  4.290e+02\n",
      " -4.300e+02 -4.400e+02  4.420e+02  4.450e+02 -4.480e+02  4.480e+02\n",
      "  4.540e+02 -4.550e+02  4.550e+02  4.560e+02 -4.650e+02 -4.680e+02\n",
      "  4.740e+02 -4.820e+02  4.900e+02 -4.910e+02 -4.920e+02 -4.930e+02\n",
      " -4.980e+02  5.010e+02  5.060e+02 -5.100e+02 -5.110e+02  5.150e+02\n",
      "  5.160e+02 -5.170e+02  5.190e+02 -5.310e+02  5.320e+02 -5.380e+02\n",
      "  5.430e+02  5.440e+02  5.500e+02 -5.550e+02 -5.590e+02  5.620e+02\n",
      " -5.650e+02 -5.700e+02  5.730e+02 -5.760e+02 -5.780e+02 -5.790e+02\n",
      " -5.810e+02 -5.850e+02  5.870e+02 -5.890e+02 -5.960e+02 -6.000e+02\n",
      " -6.010e+02  6.010e+02  6.040e+02 -6.150e+02 -6.160e+02 -6.180e+02\n",
      " -6.300e+02  6.320e+02 -6.370e+02 -6.390e+02 -6.400e+02 -6.400e+02\n",
      " -6.410e+02 -6.420e+02  6.450e+02 -6.490e+02 -6.520e+02 -6.530e+02\n",
      " -6.600e+02  6.670e+02  6.690e+02 -6.710e+02  6.780e+02 -6.830e+02\n",
      "  6.840e+02 -6.880e+02  6.920e+02 -6.990e+02 -6.990e+02 -7.000e+02\n",
      "  7.160e+02 -7.170e+02  7.340e+02 -7.340e+02  7.400e+02 -7.410e+02\n",
      " -7.570e+02 -7.580e+02 -7.590e+02 -7.620e+02 -7.650e+02 -7.690e+02\n",
      "  7.700e+02 -7.900e+02  8.030e+02 -8.040e+02 -8.040e+02 -8.100e+02\n",
      " -8.150e+02 -8.160e+02 -8.180e+02  8.220e+02  8.260e+02  8.260e+02\n",
      " -8.260e+02  8.270e+02 -8.330e+02  8.350e+02  8.400e+02 -8.420e+02\n",
      " -8.490e+02 -8.620e+02  8.810e+02  8.990e+02 -9.100e+02 -9.100e+02\n",
      " -9.110e+02 -9.110e+02 -9.110e+02 -9.130e+02  9.210e+02 -9.230e+02\n",
      " -9.250e+02  9.270e+02 -9.320e+02  9.330e+02  9.370e+02 -9.420e+02\n",
      "  9.510e+02 -9.520e+02 -9.600e+02  9.620e+02  9.650e+02  9.730e+02\n",
      "  9.740e+02 -9.790e+02 -9.900e+02  1.001e+03 -1.007e+03 -1.011e+03\n",
      " -1.031e+03 -1.032e+03 -1.038e+03  1.045e+03 -1.050e+03 -1.052e+03\n",
      "  1.057e+03  1.058e+03 -1.058e+03 -1.063e+03  1.067e+03 -1.072e+03\n",
      "  1.075e+03 -1.092e+03  1.097e+03 -1.106e+03  1.107e+03  1.150e+03\n",
      "  1.154e+03 -1.160e+03  1.161e+03 -1.176e+03 -1.180e+03 -1.182e+03\n",
      "  1.189e+03 -1.196e+03 -1.223e+03 -1.224e+03 -1.244e+03 -1.259e+03\n",
      " -1.268e+03 -1.280e+03 -1.297e+03  1.315e+03  1.335e+03 -1.336e+03\n",
      "  1.338e+03  1.344e+03 -1.361e+03 -1.386e+03  1.423e+03  1.485e+03\n",
      " -1.492e+03 -1.499e+03 -1.505e+03 -1.531e+03 -1.602e+03 -1.639e+03\n",
      "  1.640e+03 -1.645e+03  1.655e+03  1.656e+03 -1.682e+03 -1.690e+03\n",
      "  1.713e+03 -1.731e+03  1.736e+03 -1.743e+03 -1.824e+03 -1.834e+03\n",
      " -1.841e+03  1.841e+03 -1.845e+03  1.856e+03  1.874e+03  1.912e+03\n",
      " -1.927e+03  1.933e+03 -1.963e+03  1.984e+03 -2.026e+03 -2.073e+03\n",
      " -2.080e+03  2.086e+03 -2.133e+03  2.133e+03 -2.134e+03 -2.142e+03\n",
      " -2.165e+03 -2.167e+03  2.170e+03  2.224e+03 -2.227e+03  2.284e+03\n",
      "  2.304e+03  2.378e+03 -2.381e+03  2.409e+03 -2.419e+03 -2.510e+03\n",
      " -2.542e+03 -2.589e+03  2.625e+03  2.639e+03 -2.645e+03  2.680e+03\n",
      " -2.716e+03  2.803e+03 -2.811e+03 -2.820e+03  2.945e+03 -3.108e+03\n",
      "  3.149e+03 -3.166e+03  3.376e+03 -3.387e+03 -3.576e+03  3.600e+03\n",
      " -3.636e+03 -3.644e+03 -3.724e+03  3.838e+03  3.924e+03 -4.026e+03\n",
      " -4.068e+03 -4.261e+03 -4.570e+03 -4.765e+03  5.287e+03]\n",
      "Concordance Index 0.5364797146173664\n",
      "Integrated Brier Score: 0.1780845053962166\n",
      "y_train breslow final [ 1.000e+00  3.000e+00 -4.000e+00  5.000e+00  5.000e+00 -8.000e+00\n",
      "  9.000e+00 -1.200e+01  1.200e+01 -1.300e+01 -1.300e+01 -1.500e+01\n",
      "  1.700e+01  2.300e+01 -2.800e+01 -3.000e+01 -3.100e+01 -3.100e+01\n",
      "  3.400e+01 -3.500e+01 -3.700e+01 -4.100e+01 -4.200e+01  4.700e+01\n",
      "  5.200e+01 -5.500e+01  5.900e+01 -6.000e+01 -6.100e+01 -6.100e+01\n",
      " -6.100e+01 -6.100e+01 -6.100e+01  6.100e+01 -7.000e+01 -8.200e+01\n",
      " -8.300e+01  8.400e+01  8.500e+01  8.800e+01 -8.900e+01  8.900e+01\n",
      "  9.400e+01 -1.030e+02 -1.050e+02 -1.060e+02 -1.060e+02  1.090e+02\n",
      " -1.110e+02 -1.110e+02  1.130e+02  1.160e+02 -1.220e+02 -1.220e+02\n",
      " -1.220e+02 -1.220e+02  1.230e+02 -1.280e+02  1.310e+02 -1.350e+02\n",
      "  1.360e+02  1.380e+02  1.430e+02  1.460e+02 -1.500e+02  1.510e+02\n",
      " -1.530e+02 -1.600e+02  1.610e+02  1.660e+02 -1.730e+02  1.800e+02\n",
      "  1.880e+02  1.980e+02  1.980e+02 -2.020e+02 -2.070e+02 -2.110e+02\n",
      "  2.110e+02  2.120e+02  2.150e+02  2.230e+02  2.360e+02 -2.380e+02\n",
      " -2.420e+02 -2.440e+02 -2.470e+02  2.610e+02  2.740e+02  2.760e+02\n",
      "  2.840e+02  2.940e+02 -2.960e+02  2.990e+02  3.020e+02  3.060e+02\n",
      "  3.110e+02  3.150e+02  3.220e+02 -3.240e+02  3.400e+02  3.400e+02\n",
      "  3.450e+02  3.510e+02  3.570e+02  3.580e+02 -3.580e+02 -3.580e+02\n",
      "  3.580e+02  3.600e+02  3.660e+02 -3.660e+02 -3.670e+02 -3.700e+02\n",
      "  3.710e+02  3.720e+02 -3.750e+02 -3.770e+02  3.830e+02  3.870e+02\n",
      " -3.920e+02 -3.960e+02  3.990e+02  4.020e+02  4.030e+02 -4.050e+02\n",
      " -4.050e+02 -4.060e+02 -4.070e+02  4.080e+02  4.120e+02  4.180e+02\n",
      " -4.200e+02 -4.230e+02  4.280e+02 -4.280e+02 -4.280e+02  4.290e+02\n",
      " -4.300e+02 -4.400e+02  4.420e+02  4.450e+02 -4.480e+02  4.480e+02\n",
      "  4.540e+02 -4.550e+02  4.550e+02  4.560e+02 -4.650e+02 -4.680e+02\n",
      "  4.740e+02 -4.820e+02  4.900e+02 -4.910e+02 -4.920e+02 -4.930e+02\n",
      " -4.980e+02  5.010e+02  5.060e+02 -5.100e+02 -5.110e+02  5.150e+02\n",
      "  5.160e+02 -5.170e+02  5.190e+02 -5.310e+02  5.320e+02 -5.380e+02\n",
      "  5.430e+02  5.440e+02  5.500e+02 -5.550e+02 -5.590e+02  5.620e+02\n",
      " -5.650e+02 -5.700e+02  5.730e+02 -5.760e+02 -5.780e+02 -5.790e+02\n",
      " -5.810e+02 -5.850e+02  5.870e+02 -5.890e+02 -5.960e+02 -6.000e+02\n",
      " -6.010e+02  6.010e+02  6.040e+02 -6.150e+02 -6.160e+02 -6.180e+02\n",
      " -6.300e+02  6.320e+02 -6.370e+02 -6.390e+02 -6.400e+02 -6.400e+02\n",
      " -6.410e+02 -6.420e+02  6.450e+02 -6.490e+02 -6.520e+02 -6.530e+02\n",
      " -6.600e+02  6.670e+02  6.690e+02 -6.710e+02  6.780e+02 -6.830e+02\n",
      "  6.840e+02 -6.880e+02  6.920e+02 -6.990e+02 -6.990e+02 -7.000e+02\n",
      "  7.160e+02 -7.170e+02  7.340e+02 -7.340e+02  7.400e+02 -7.410e+02\n",
      " -7.570e+02 -7.580e+02 -7.590e+02 -7.620e+02 -7.650e+02 -7.690e+02\n",
      "  7.700e+02 -7.900e+02  8.030e+02 -8.040e+02 -8.040e+02 -8.100e+02\n",
      " -8.150e+02 -8.160e+02 -8.180e+02  8.220e+02  8.260e+02  8.260e+02\n",
      " -8.260e+02  8.270e+02 -8.330e+02  8.350e+02  8.400e+02 -8.420e+02\n",
      " -8.490e+02 -8.620e+02  8.810e+02  8.990e+02 -9.100e+02 -9.100e+02\n",
      " -9.110e+02 -9.110e+02 -9.110e+02 -9.130e+02  9.210e+02 -9.230e+02\n",
      " -9.250e+02  9.270e+02 -9.320e+02  9.330e+02  9.370e+02 -9.420e+02\n",
      "  9.510e+02 -9.520e+02 -9.600e+02  9.620e+02  9.650e+02  9.730e+02\n",
      "  9.740e+02 -9.790e+02 -9.900e+02  1.001e+03 -1.007e+03 -1.011e+03\n",
      " -1.031e+03 -1.032e+03 -1.038e+03  1.045e+03 -1.050e+03 -1.052e+03\n",
      "  1.057e+03  1.058e+03 -1.058e+03 -1.063e+03  1.067e+03 -1.072e+03\n",
      "  1.075e+03 -1.092e+03  1.097e+03 -1.106e+03  1.107e+03  1.150e+03\n",
      "  1.154e+03 -1.160e+03  1.161e+03 -1.176e+03 -1.180e+03 -1.182e+03\n",
      "  1.189e+03 -1.196e+03 -1.223e+03 -1.224e+03 -1.244e+03 -1.259e+03\n",
      " -1.268e+03 -1.280e+03 -1.297e+03  1.315e+03  1.335e+03 -1.336e+03\n",
      "  1.338e+03  1.344e+03 -1.361e+03 -1.386e+03  1.423e+03  1.485e+03\n",
      " -1.492e+03 -1.499e+03 -1.505e+03 -1.531e+03 -1.602e+03 -1.639e+03\n",
      "  1.640e+03 -1.645e+03  1.655e+03  1.656e+03 -1.682e+03 -1.690e+03\n",
      "  1.713e+03 -1.731e+03  1.736e+03 -1.743e+03 -1.824e+03 -1.834e+03\n",
      " -1.841e+03  1.841e+03 -1.845e+03  1.856e+03  1.874e+03  1.912e+03\n",
      " -1.927e+03  1.933e+03 -1.963e+03  1.984e+03 -2.026e+03 -2.073e+03\n",
      " -2.080e+03  2.086e+03 -2.133e+03  2.133e+03 -2.134e+03 -2.142e+03\n",
      " -2.165e+03 -2.167e+03  2.170e+03  2.224e+03 -2.227e+03  2.284e+03\n",
      "  2.304e+03  2.378e+03 -2.381e+03  2.409e+03 -2.419e+03 -2.510e+03\n",
      " -2.542e+03 -2.589e+03  2.625e+03  2.639e+03 -2.645e+03  2.680e+03\n",
      " -2.716e+03  2.803e+03 -2.811e+03 -2.820e+03  2.945e+03 -3.108e+03\n",
      "  3.149e+03 -3.166e+03  3.376e+03 -3.387e+03 -3.576e+03  3.600e+03\n",
      " -3.636e+03 -3.644e+03 -3.724e+03  3.838e+03  3.924e+03 -4.026e+03\n",
      " -4.068e+03 -4.261e+03 -4.570e+03 -4.765e+03  5.287e+03]\n",
      "durations 2.0 4694.0\n",
      "Concordance Index 0.4883127921801955\n",
      "Integrated Brier Score: 0.2152095568137767\n",
      "split gex_?|100133144      float32\n",
      "gex_?|100134869      float32\n",
      "gex_?|10357          float32\n",
      "gex_?|10431          float32\n",
      "gex_?|155060         float32\n",
      "                      ...   \n",
      "gex_ZYG11A|440590    float32\n",
      "gex_ZYG11B|79699     float32\n",
      "gex_ZYX|7791         float32\n",
      "gex_ZZEF1|23140      float32\n",
      "gex_ZZZ3|26009       float32\n",
      "Length: 19076, dtype: object\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m285.8373\u001b[0m  0.0018\n",
      "      2      285.8373  0.0015\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      3      285.8373  0.0014\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      4      285.8373  0.0016\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m206.0049\u001b[0m  0.0018\n",
      "      5      285.8373  0.0013\n",
      "      6      285.8373  0.0013\n",
      "      2      206.0049  0.0014\n",
      "      3      206.0049  0.0012      7      285.8373  0.0012\n",
      "\n",
      "      4      206.0049  0.0012\n",
      "      8      285.8373  0.0012\n",
      "      5      206.0049  0.0012\n",
      "      9      285.8373  0.0012\n",
      "      6      206.0049  0.0012\n",
      "     10      285.8373  0.0012\n",
      "Restoring best model from epoch 1.\n",
      "      7      206.0049  0.0012\n",
      "      8      206.0049  0.0012\n",
      "      9      206.0049  0.0012\n",
      "     10      206.0049  0.0012\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m64.3283\u001b[0m  0.0024\n",
      "      2       64.3283  0.0020\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m228.8663\u001b[0m  0.0038\n",
      "      2      228.8663  0.0023\n",
      "      3       64.3283  0.0031\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      3      228.8663  0.0014\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      4      228.8663  0.0012\n",
      "      5      228.8663  0.0012\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m213.1623\u001b[0m  0.0021\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m229.2789\u001b[0m  0.0015\n",
      "      6      228.8663  0.0012\n",
      "      2      213.1623  0.0013\n",
      "      4       64.3283  0.0053\n",
      "      2      229.2789  0.0013\n",
      "      7      228.8663  0.0012\n",
      "      3      229.2789  0.0012\n",
      "      8      228.8663  0.0012\n",
      "      4      229.2789  0.0012\n",
      "      5       64.3283  0.0034\n",
      "      9      228.8663  0.0016\n",
      "      3      213.1623  0.0036\n",
      "      5      229.2789  0.0012\n",
      "     10      228.8663  0.0013\n",
      "Restoring best model from epoch 1.\n",
      "      6      229.2789  0.0012\n",
      "      6       64.3283  0.0020\n",
      "      7      229.2789  0.0012\n",
      "      4      213.1623  0.0033\n",
      "      7       64.3283  0.0019\n",
      "      8      229.2789  0.0012\n",
      "      8       64.3283  0.0018\n",
      "      9      229.2789  0.0016\n",
      "      5      213.1623  0.0031\n",
      "      9       64.3283  0.0019\n",
      "      6      213.1623  0.0016\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "     10       64.3283  0.0018\n",
      "      7      213.1623  0.0012\n",
      "Restoring best model from epoch 1.\n",
      "     10      229.2789  0.0037\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      8      213.1623  0.0012\n",
      "      9      213.1623  0.0012\n",
      "     10      213.1623  0.0012\n",
      "Restoring best model from epoch 1.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m64.6868\u001b[0m  0.0022\n",
      "      2       64.6868  0.0020\n",
      "      3       64.6868  0.0018\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      4       64.6868  0.0018\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      5       64.6868  0.0018\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      6       64.6868  0.0026\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m50.6923\u001b[0m  0.0021\n",
      "      2       50.6923  0.0018\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m65.0720\u001b[0m  0.0021\n",
      "      3       50.6923  0.0018\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      7       64.6868  0.0052\n",
      "      4       50.6923  0.0018\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m54.0541\u001b[0m  0.0020\n",
      "      5       50.6923  0.0017\n",
      "      2       65.0720  0.0051\n",
      "      8       64.6868  0.0036\n",
      "      2       54.0541  0.0018\n",
      "      6       50.6923  0.0017\n",
      "      3       65.0720  0.0021\n",
      "      9       64.6868  0.0019\n",
      "      3       54.0541  0.0018\n",
      "      7       50.6923  0.0020\n",
      "      4       65.0720  0.0019\n",
      "     10       64.6868  0.0018\n",
      "Restoring best model from epoch 1.\n",
      "      4       54.0541  0.0018\n",
      "      8       50.6923  0.0017\n",
      "      5       65.0720  0.0018\n",
      "      5       54.0541  0.0018\n",
      "      9       50.6923  0.0017\n",
      "      6       65.0720  0.0018\n",
      "      6       54.0541  0.0018\n",
      "     10       50.6923  0.0017\n",
      "Restoring best model from epoch 1.\n",
      "      7       54.0541  0.0018\n",
      "      7       65.0720  0.0030\n",
      "      8       54.0541  0.0042\n",
      "      8       65.0720  0.0049\n",
      "      9       65.0720  0.0036\n",
      "      9       54.0541  0.0052\n",
      "     10       65.0720  0.0020\n",
      "Restoring best model from epoch 1.\n",
      "     10       54.0541  0.0024\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m88.3661\u001b[0m  0.0033\n",
      "      2       88.3661  0.0025\n",
      "      3       88.3661  0.0026\n",
      "      4       88.3661  0.0025\n",
      "      5       88.3661  0.0025\n",
      "      6       88.3661  0.0026\n",
      "      7       88.3661  0.0027\n",
      "      8       88.3661  0.0039\n",
      "      9       88.3661  0.0046\n",
      "     10       88.3661  0.0041\n",
      "Restoring best model from epoch 1.\n",
      "y_train breslow final [    9.    11.   -16.    24.    24.    31.    36.    53.    61.   -61.\n",
      "    65.    74.    75.    90.   -92.    92.   -98.  -106.  -113.  -122.\n",
      "  -130.  -133.  -147.  -148.  -165.  -168.  -168.  -183.  -186.   186.\n",
      "  -190.  -192.  -194.   197.  -204.  -238.  -239.  -243.  -253.  -255.\n",
      "   260.  -260.  -268.  -277.   286.   304.  -343.   346.  -360.  -361.\n",
      "   365.  -379.   395.   396.  -420.  -441.  -442.  -454.   455.   457.\n",
      "   479.   493.   506.  -508.   518.   524.   528.  -529.   547.  -547.\n",
      "   555.   562.   563.   565.   568.  -571.  -576.  -581.   594.   608.\n",
      "   624.   627.   629.   637.  -643.  -655.   663.   676.   676.   679.\n",
      "  -686.   695.   731.   737.  -741.  -751.   760.  -761.  -763.  -783.\n",
      "   787.  -816.   820.  -837.   840.  -847.  -848.   857.   863.  -875.\n",
      "   883.  -885.   887.  -918.  -928.  -932.  -932.   949.  -967.   976.\n",
      "  1004.  1018.  1024.  1032.  1033.  1039.  1046. -1053.  1058.  1059.\n",
      "  1064.  1069.  1088.  1089.  1091.  1106.  1123. -1127.  1155.  1157.\n",
      "  1161.  1162.  1163.  1187.  1189. -1207.  1213.  1229.  1249.  1249.\n",
      "  1259.  1278.  1279.  1321.  1324.  1329.  1341. -1342.  1348.  1354.\n",
      "  1354. -1357.  1364. -1364.  1366. -1372.  1384. -1386.  1446.  1448.\n",
      "  1451.  1484.  1484.  1492.  1516.  1562. -1573.  1579.  1583.  1620.\n",
      "  1646. -1684.  1688.  1699.  1720.  1721. -1722.  1736.  1746. -1752.\n",
      "  1757.  1767.  1769. -1785.  1799.  1815.  1875.  1891. -1900. -1919.\n",
      "  1955.  1977. -1977. -1993.  1993. -1998.  2009.  2012.  2012. -2032.\n",
      " -2078.  2089. -2143.  2148. -2182.  2182.  2218.  2342.  2345. -2369.\n",
      "  2400.  2467. -2535. -2614.  2621.  2634.  2648. -2661. -2661.  2692.\n",
      "  2717. -3096.  3224. -3325.  3337. -3525. -3532. -3825. -3871.  4624.\n",
      " -5481.]\n",
      "Concordance Index 0.5112287748767573\n",
      "Integrated Brier Score: 0.12593525890414564\n",
      "y_train breslow final [    9.    11.   -16.    24.    24.    31.    36.    53.    61.   -61.\n",
      "    65.    74.    75.    90.   -92.    92.   -98.  -106.  -113.  -122.\n",
      "  -130.  -133.  -147.  -148.  -165.  -168.  -168.  -183.  -186.   186.\n",
      "  -190.  -192.  -194.   197.  -204.  -238.  -239.  -243.  -253.  -255.\n",
      "   260.  -260.  -268.  -277.   286.   304.  -343.   346.  -360.  -361.\n",
      "   365.  -379.   395.   396.  -420.  -441.  -442.  -454.   455.   457.\n",
      "   479.   493.   506.  -508.   518.   524.   528.  -529.   547.  -547.\n",
      "   555.   562.   563.   565.   568.  -571.  -576.  -581.   594.   608.\n",
      "   624.   627.   629.   637.  -643.  -655.   663.   676.   676.   679.\n",
      "  -686.   695.   731.   737.  -741.  -751.   760.  -761.  -763.  -783.\n",
      "   787.  -816.   820.  -837.   840.  -847.  -848.   857.   863.  -875.\n",
      "   883.  -885.   887.  -918.  -928.  -932.  -932.   949.  -967.   976.\n",
      "  1004.  1018.  1024.  1032.  1033.  1039.  1046. -1053.  1058.  1059.\n",
      "  1064.  1069.  1088.  1089.  1091.  1106.  1123. -1127.  1155.  1157.\n",
      "  1161.  1162.  1163.  1187.  1189. -1207.  1213.  1229.  1249.  1249.\n",
      "  1259.  1278.  1279.  1321.  1324.  1329.  1341. -1342.  1348.  1354.\n",
      "  1354. -1357.  1364. -1364.  1366. -1372.  1384. -1386.  1446.  1448.\n",
      "  1451.  1484.  1484.  1492.  1516.  1562. -1573.  1579.  1583.  1620.\n",
      "  1646. -1684.  1688.  1699.  1720.  1721. -1722.  1736.  1746. -1752.\n",
      "  1757.  1767.  1769. -1785.  1799.  1815.  1875.  1891. -1900. -1919.\n",
      "  1955.  1977. -1977. -1993.  1993. -1998.  2009.  2012.  2012. -2032.\n",
      " -2078.  2089. -2143.  2148. -2182.  2182.  2218.  2342.  2345. -2369.\n",
      "  2400.  2467. -2535. -2614.  2621.  2634.  2648. -2661. -2661.  2692.\n",
      "  2717. -3096.  3224. -3325.  3337. -3525. -3532. -3825. -3871.  4624.\n",
      " -5481.]\n",
      "durations 8.0 4424.0\n",
      "Concordance Index 0.4504331087584216\n",
      "Integrated Brier Score: 0.1369150026528061\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      1       \u001b[36m92.2424\u001b[0m  0.0019\n",
      "      2       92.2424  0.0016\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m99.1561\u001b[0m  0.0019\n",
      "      3       92.2424  0.0015\n",
      "      2       99.1561  0.0015\n",
      "      3       99.1561  0.0015\n",
      "      4       99.1561  0.0015\n",
      "      4       92.2424  0.0045\n",
      "      5       99.1561  0.0014\n",
      "      6       99.1561  0.0015\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m118.4719\u001b[0m       \u001b[32m59.4361\u001b[0m  0.0195\n",
      "      5       92.2424  0.0026\n",
      "      7       99.1561  0.0015\n",
      "      6       92.2424  0.0020\n",
      "      8       99.1561  0.0024\n",
      "      7       92.2424  0.0016\n",
      "      9       99.1561  0.0016\n",
      "      8       92.2424  0.0015\n",
      "     10       99.1561  0.0015\n",
      "Restoring best model from epoch 1.\n",
      "      9       92.2424  0.0015\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m126.3319\u001b[0m       \u001b[32m56.8191\u001b[0m  0.0227\n",
      "     10       92.2424  0.0015\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2      121.1934       \u001b[32m59.0241\u001b[0m  0.0177\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2      \u001b[36m123.7240\u001b[0m       56.8264  0.0187\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m100.1616\u001b[0m  0.0018\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m158.6947\u001b[0m  0.0019\n",
      "      2      100.1616  0.0018\n",
      "      3      100.1616  0.0015\n",
      "      2      158.6947  0.0031\n",
      "      4      100.1616  0.0015\n",
      "      3      158.6947  0.0015\n",
      "      5      100.1616  0.0016\n",
      "      4      158.6947  0.0015\n",
      "      5      158.6947  0.0015\n",
      "      3      124.6728       59.5369  0.0210\n",
      "      6      100.1616  0.0028\n",
      "      6      158.6947  0.0015\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m134.1572\u001b[0m       \u001b[32m69.5116\u001b[0m  0.0216\n",
      "      7      100.1616  0.0016\n",
      "      7      158.6947  0.0015\n",
      "      8      100.1616  0.0015\n",
      "      8      158.6947  0.0017\n",
      "      3      126.6631       56.8381  0.0177\n",
      "      9      100.1616  0.0014\n",
      "      9      158.6947  0.0015\n",
      "     10      100.1616  0.0014\n",
      "Restoring best model from epoch 1.\n",
      "     10      158.6947  0.0015\n",
      "Restoring best model from epoch 1.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m153.2639\u001b[0m       \u001b[32m62.1304\u001b[0m  0.0291\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m116.0390\u001b[0m       \u001b[32m59.5515\u001b[0m  0.0275\n",
      "      4      119.9361       60.5005  0.0180\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2      \u001b[36m132.8178\u001b[0m       70.3646  0.0183\n",
      "      4      \u001b[36m122.5079\u001b[0m       \u001b[32m56.6205\u001b[0m  0.0167\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m81.8221\u001b[0m  0.0019\n",
      "      2       81.8221  0.0015\n",
      "      3       81.8221  0.0014\n",
      "      4       81.8221  0.0014\n",
      "      5       81.8221  0.0014\n",
      "      2      \u001b[36m148.9755\u001b[0m       62.7136  0.0172\n",
      "      6       81.8221  0.0015\n",
      "      7       81.8221  0.0015\n",
      "      8       81.8221  0.0014\n",
      "      2      121.3279       59.6068  0.0173\n",
      "      3      \u001b[36m130.6193\u001b[0m       71.5120  0.0150\n",
      "      9       81.8221  0.0014\n",
      "     10       81.8221  0.0014\n",
      "      5      118.4725       60.5643  0.0198\n",
      "Restoring best model from epoch 1.\n",
      "      5      \u001b[36m116.3440\u001b[0m       \u001b[32m56.5112\u001b[0m  0.0158\n",
      "      3      \u001b[36m144.4933\u001b[0m       63.7660  0.0182\n",
      "      4      \u001b[36m125.8904\u001b[0m       70.9864  0.0186\n",
      "      6      \u001b[36m118.0115\u001b[0m       60.0361  0.0171\n",
      "      3      116.9353       59.6836  0.0245\n",
      "      6      \u001b[36m114.7520\u001b[0m       56.6597  0.0200\n",
      "      4      145.4314       64.0239  0.0160\n",
      "      5      131.5942       70.1187  0.0139\n",
      "      7      \u001b[36m114.7052\u001b[0m       59.5511  0.0147\n",
      "      4      \u001b[36m113.6904\u001b[0m       60.1643  0.0179\n",
      "      7      118.3229       57.5784  0.0227\n",
      "      8      \u001b[36m112.8365\u001b[0m       59.6205  0.0153\n",
      "      6      126.0068       69.9834  0.0185\n",
      "      5      \u001b[36m140.6613\u001b[0m       63.9663  0.0226\n",
      "      5      \u001b[36m113.2397\u001b[0m       60.0157  0.0154\n",
      "      8      117.2998       59.0188  0.0141\n",
      "      9      \u001b[36m111.5585\u001b[0m       59.8625  0.0155\n",
      "      7      \u001b[36m124.0760\u001b[0m       69.9730  0.0160\n",
      "      6      \u001b[36m135.7023\u001b[0m       63.5268  0.0158\n",
      "      6      \u001b[36m113.1491\u001b[0m       59.9107  0.0154\n",
      "      9      117.8957       59.7450  0.0139\n",
      "     10      \u001b[36m111.0127\u001b[0m       60.5650  0.0146\n",
      "Restoring best model from epoch 2.\n",
      "      8      126.7894       70.0349  0.0156\n",
      "      7      140.8382       63.1534  0.0158\n",
      "      7      115.3875       59.9489  0.0147\n",
      "     10      117.4707       60.0513  0.0137\n",
      "Restoring best model from epoch 5.\n",
      "      9      127.1114       70.1227  0.0145\n",
      "      8      139.4206       63.6843  0.0152\n",
      "      8      \u001b[36m109.0696\u001b[0m       60.1747  0.0141\n",
      "     10      128.2009       70.2458  0.0138\n",
      "Restoring best model from epoch 1.\n",
      "      9      137.2264       64.3071  0.0150\n",
      "      9      111.2429       60.4985  0.0142\n",
      "     10      \u001b[36m134.8715\u001b[0m       65.3740  0.0151\n",
      "Restoring best model from epoch 1.\n",
      "     10      111.4275       60.8924  0.0141\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m135.2347\u001b[0m       \u001b[32m79.8057\u001b[0m  0.0209\n",
      "      2      \u001b[36m134.7177\u001b[0m       80.6520  0.0222\n",
      "      3      \u001b[36m132.9444\u001b[0m       80.4747  0.0324\n",
      "      4      133.4091       80.8618  0.0248\n",
      "      5      133.2246       81.0759  0.0274\n",
      "      6      \u001b[36m130.1185\u001b[0m       80.8026  0.0248\n",
      "      7      \u001b[36m128.7290\u001b[0m       80.3571  0.0247\n",
      "      8      130.3871       81.1404  0.0286\n",
      "      9      \u001b[36m128.3670\u001b[0m       80.8378  0.0237\n",
      "     10      128.6961       79.9033  0.0254\n",
      "Restoring best model from epoch 1.\n",
      "y_train breslow final [    8.    11.    24.   -24.    24.    25.    31.    36.    53.    61.\n",
      "    65.   -67.    75.   -81.    91.   -98.  -106.  -113.  -130.   138.\n",
      "  -145.  -147.  -148.  -163.  -165.  -168.  -176.  -176.  -181.  -183.\n",
      "  -186.   186.  -190.  -192.  -194.  -220.  -238.  -239.  -243.  -253.\n",
      "  -255.   260.  -260.  -268.   286.   304.   304.   336.  -338.  -343.\n",
      "   346.  -360.  -361.   394.   395.   396.  -420.  -441.   455.   479.\n",
      "   493.   515.   518.   531.   542.  -547.  -547.   555.   562.   563.\n",
      "   565.   568.  -581.   608.   624.   627.   629.   637.   637.  -643.\n",
      "  -655.   676.   676.   679.  -684.  -686.   695.   728.   731.   737.\n",
      "   760.  -763.  -772.  -783.   787.  -816.   820.   820.   821.  -837.\n",
      "   840.  -847.  -848.   857.   863.   868.   883.  -885.   887.   914.\n",
      "  -915.  -918.   919.  -928.  -932.  -932.   949.   951.  -952.  -956.\n",
      "   962.  -967.   976.  1004.  1018.  1024.  1024.  1032.  1033.  1035.\n",
      "  1039. -1053.  1058.  1069.  1082.  1089.  1091.  1102.  1104.  1106.\n",
      "  1123. -1127.  1155.  1157.  1162.  1163.  1187.  1189. -1212.  1213.\n",
      "  1249.  1249.  1259.  1319.  1324.  1329. -1342.  1348.  1354.  1354.\n",
      " -1357.  1364. -1364.  1369. -1372.  1384. -1386.  1399.  1446.  1451.\n",
      "  1470.  1483.  1484.  1484.  1492.  1516.  1562. -1573.  1579.  1579.\n",
      "  1583.  1620.  1646.  1650. -1684.  1699.  1720.  1721. -1722.  1736.\n",
      " -1752.  1767. -1785.  1799. -1844. -1882.  1891. -1900. -1919.  1947.\n",
      "  1977. -1977. -1993.  1993. -1998.  2012.  2012. -2032.  2049. -2078.\n",
      " -2143.  2148. -2182.  2182.  2218. -2329. -2338.  2342.  2345.  2400.\n",
      " -2464.  2467. -2535. -2614.  2621.  2634.  2648. -2661. -2661.  2688.\n",
      "  2717.  2742. -3096.  3224. -3325.  3337. -3525. -3825. -3871. -4424.\n",
      "  4624.]\n",
      "Concordance Index 0.557712924359738\n",
      "Integrated Brier Score: 0.13808939641899534\n",
      "y_train breslow final [    8.    11.    24.   -24.    24.    25.    31.    36.    53.    61.\n",
      "    65.   -67.    75.   -81.    91.   -98.  -106.  -113.  -130.   138.\n",
      "  -145.  -147.  -148.  -163.  -165.  -168.  -176.  -176.  -181.  -183.\n",
      "  -186.   186.  -190.  -192.  -194.  -220.  -238.  -239.  -243.  -253.\n",
      "  -255.   260.  -260.  -268.   286.   304.   304.   336.  -338.  -343.\n",
      "   346.  -360.  -361.   394.   395.   396.  -420.  -441.   455.   479.\n",
      "   493.   515.   518.   531.   542.  -547.  -547.   555.   562.   563.\n",
      "   565.   568.  -581.   608.   624.   627.   629.   637.   637.  -643.\n",
      "  -655.   676.   676.   679.  -684.  -686.   695.   728.   731.   737.\n",
      "   760.  -763.  -772.  -783.   787.  -816.   820.   820.   821.  -837.\n",
      "   840.  -847.  -848.   857.   863.   868.   883.  -885.   887.   914.\n",
      "  -915.  -918.   919.  -928.  -932.  -932.   949.   951.  -952.  -956.\n",
      "   962.  -967.   976.  1004.  1018.  1024.  1024.  1032.  1033.  1035.\n",
      "  1039. -1053.  1058.  1069.  1082.  1089.  1091.  1102.  1104.  1106.\n",
      "  1123. -1127.  1155.  1157.  1162.  1163.  1187.  1189. -1212.  1213.\n",
      "  1249.  1249.  1259.  1319.  1324.  1329. -1342.  1348.  1354.  1354.\n",
      " -1357.  1364. -1364.  1369. -1372.  1384. -1386.  1399.  1446.  1451.\n",
      "  1470.  1483.  1484.  1484.  1492.  1516.  1562. -1573.  1579.  1579.\n",
      "  1583.  1620.  1646.  1650. -1684.  1699.  1720.  1721. -1722.  1736.\n",
      " -1752.  1767. -1785.  1799. -1844. -1882.  1891. -1900. -1919.  1947.\n",
      "  1977. -1977. -1993.  1993. -1998.  2012.  2012. -2032.  2049. -2078.\n",
      " -2143.  2148. -2182.  2182.  2218. -2329. -2338.  2342.  2345.  2400.\n",
      " -2464.  2467. -2535. -2614.  2621.  2634.  2648. -2661. -2661.  2688.\n",
      "  2717.  2742. -3096.  3224. -3325.  3337. -3525. -3825. -3871. -4424.\n",
      "  4624.]\n",
      "durations 9.0 5481.0\n",
      "Concordance Index 0.46394984326018807\n",
      "Integrated Brier Score: 0.12205484132759518\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m125.9997\u001b[0m       \u001b[32m62.8889\u001b[0m  0.0172\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      2      129.9440       \u001b[32m62.6899\u001b[0m  0.0153\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m89.7164\u001b[0m  0.0020\n",
      "      2       89.7164  0.0019\n",
      "      3       89.7164  0.0016\n",
      "      4       89.7164  0.0022\n",
      "      5       89.7164  0.0015\n",
      "      6       89.7164  0.0015\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      7       89.7164  0.0015\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      8       89.7164  0.0015\n",
      "      9       89.7164  0.0014\n",
      "     10       89.7164  0.0014\n",
      "Restoring best model from epoch 1.\n",
      "      3      127.3050       62.9077  0.0195\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m146.0988\u001b[0m       \u001b[32m68.7135\u001b[0m  0.0225\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m93.5786\u001b[0m  0.0025\n",
      "      2       93.5786  0.0020\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      4      127.5517       63.2087  0.0175\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      3       93.5786  0.0015\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m113.5559\u001b[0m       \u001b[32m55.8220\u001b[0m  0.0230\n",
      "      4       93.5786  0.0014\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m115.5190\u001b[0m  0.0018\n",
      "      5       93.5786  0.0014\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      2      115.5190  0.0015\n",
      "      2      147.3405       \u001b[32m68.3167\u001b[0m  0.0166\n",
      "      6       93.5786  0.0014\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      3      115.5190  0.0015\n",
      "      7       93.5786  0.0014\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      4      115.5190  0.0014\n",
      "      5      115.5190  0.0014\n",
      "      6      115.5190  0.0014\n",
      "      8       93.5786  0.0041\n",
      "      7      115.5190  0.0016\n",
      "      8      115.5190  0.0017\n",
      "      9       93.5786  0.0035\n",
      "      5      \u001b[36m122.8862\u001b[0m       63.7026  0.0161\n",
      "      9      115.5190  0.0015\n",
      "     10      115.5190  0.0015\n",
      "      2      120.8912       \u001b[32m55.7609\u001b[0m  0.0156\n",
      "Restoring best model from epoch 1.\n",
      "     10       93.5786  0.0034\n",
      "Restoring best model from epoch 1.\n",
      "      3      \u001b[36m142.6236\u001b[0m       \u001b[32m67.7503\u001b[0m  0.0167\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m130.8852\u001b[0m       \u001b[32m67.0344\u001b[0m  0.0171\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m121.0109\u001b[0m       \u001b[32m67.4750\u001b[0m  0.0217\n",
      "      6      \u001b[36m120.5396\u001b[0m       63.6329  0.0180\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m99.1665\u001b[0m  0.0019\n",
      "      2      131.9957       67.6920  0.0173\n",
      "      2       99.1665  0.0015\n",
      "      3      \u001b[36m113.4632\u001b[0m       56.3533  0.0239\n",
      "      3       99.1665  0.0014\n",
      "      4      142.6787       67.8386  0.0213\n",
      "      4       99.1665  0.0039\n",
      "      5       99.1665  0.0015\n",
      "      2      128.3091       \u001b[32m67.1575\u001b[0m  0.0167\n",
      "      6       99.1665  0.0015\n",
      "      7       99.1665  0.0014\n",
      "      8       99.1665  0.0014\n",
      "      7      121.8127       63.6424  0.0206\n",
      "      9       99.1665  0.0014\n",
      "     10       99.1665  0.0014\n",
      "Restoring best model from epoch 1.\n",
      "      3      134.9913       69.1051  0.0171\n",
      "      5      \u001b[36m140.2601\u001b[0m       68.3950  0.0156\n",
      "      4      \u001b[36m112.6148\u001b[0m       56.0343  0.0222\n",
      "      3      \u001b[36m116.8292\u001b[0m       \u001b[32m66.8643\u001b[0m  0.0170\n",
      "      8      123.1415       63.4651  0.0146\n",
      "      6      \u001b[36m138.0845\u001b[0m       68.4732  0.0149\n",
      "      4      135.4915       68.7991  0.0178\n",
      "      9      120.8688       63.4704  0.0139\n",
      "      4      118.6030       66.9793  0.0165\n",
      "      5      \u001b[36m108.2288\u001b[0m       \u001b[32m55.6743\u001b[0m  0.0194\n",
      "      5      \u001b[36m127.7508\u001b[0m       68.2731  0.0139\n",
      "      7      140.2068       67.9940  0.0155\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m93.5177\u001b[0m  0.0023\n",
      "     10      \u001b[36m119.8311\u001b[0m       63.6163  0.0154\n",
      "Restoring best model from epoch 2.\n",
      "      6      \u001b[36m105.5608\u001b[0m       \u001b[32m55.3724\u001b[0m  0.0143\n",
      "      2       93.5177  0.0015\n",
      "      5      \u001b[36m113.3683\u001b[0m       67.1737  0.0173\n",
      "      3       93.5177  0.0015\n",
      "      4       93.5177  0.0014\n",
      "      5       93.5177  0.0015\n",
      "      6      \u001b[36m126.8871\u001b[0m       \u001b[32m66.2140\u001b[0m  0.0151\n",
      "      6       93.5177  0.0014\n",
      "      8      139.4381       \u001b[32m67.3634\u001b[0m  0.0146\n",
      "      7       93.5177  0.0014\n",
      "      8       93.5177  0.0014\n",
      "      9       93.5177  0.0016\n",
      "     10       93.5177  0.0014\n",
      "Restoring best model from epoch 1.\n",
      "      7      106.6883       \u001b[32m55.1339\u001b[0m  0.0149\n",
      "      6      \u001b[36m109.5641\u001b[0m       67.0064  0.0155\n",
      "      9      \u001b[36m136.9314\u001b[0m       \u001b[32m66.7450\u001b[0m  0.0149\n",
      "      7      \u001b[36m123.8860\u001b[0m       \u001b[32m64.6706\u001b[0m  0.0201\n",
      "      8      107.3076       55.2568  0.0153\n",
      "      7      114.2591       66.8844  0.0166\n",
      "     10      140.5327       \u001b[32m66.2092\u001b[0m  0.0143\n",
      "      8      \u001b[36m122.6456\u001b[0m       \u001b[32m64.3584\u001b[0m  0.0141\n",
      "      9      110.3511       55.9460  0.0146\n",
      "      8      116.9201       66.9310  0.0146\n",
      "      9      126.8768       64.6737  0.0141\n",
      "     10      \u001b[36m103.9116\u001b[0m       56.1089  0.0142\n",
      "Restoring best model from epoch 7.\n",
      "      9      111.7356       67.1520  0.0145\n",
      "     10      123.6602       65.5080  0.0177\n",
      "Restoring best model from epoch 8.\n",
      "     10      114.1425       67.2080  0.0166\n",
      "Restoring best model from epoch 3.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m132.2568\u001b[0m       \u001b[32m94.6259\u001b[0m  0.0231\n",
      "      2      \u001b[36m126.3354\u001b[0m       96.4886  0.0256\n",
      "      3      \u001b[36m121.2341\u001b[0m       98.7458  0.0256\n",
      "      4      \u001b[36m119.2772\u001b[0m      100.2612  0.0251\n",
      "      5      120.0181      100.6239  0.0252\n",
      "      6      122.4993       98.2865  0.0245\n",
      "      7      \u001b[36m118.5593\u001b[0m       96.4409  0.0263\n",
      "      8      121.5658       96.3031  0.0267\n",
      "      9      119.2566       98.8388  0.0260\n",
      "     10      \u001b[36m118.2216\u001b[0m      101.6828  0.0246\n",
      "Restoring best model from epoch 1.\n",
      "y_train breslow final [    8.     9.    11.   -16.    24.   -24.    24.    25.    31.    31.\n",
      "    36.   -61.    65.   -67.    74.    75.   -81.    90.    91.   -92.\n",
      "    92.   -98.  -113.  -122.  -130.  -133.   138.  -145.  -148.  -163.\n",
      "  -168.  -168.  -176.  -176.  -181.  -183.  -186.  -192.   197.  -204.\n",
      "  -220.  -238.  -243.  -253.   260.  -260.  -260.  -277.   286.   304.\n",
      "   304.   336.  -338.  -343.   346.  -360.  -361.   365.  -379.   394.\n",
      "   395.   396.  -420.  -442.  -454.   457.   479.   493.   506.  -508.\n",
      "   515.   518.   524.   528.  -529.   531.   542.   547.  -547.   555.\n",
      "   563.  -571.  -576.  -581.   594.   608.   629.   637.   637.  -643.\n",
      "  -655.   663.   676.   676.  -684.  -686.   695.   728.   731.   737.\n",
      "  -741.  -751.   760.  -761.  -763.  -772.  -783.   820.   820.   821.\n",
      "  -837.  -848.   857.   863.   868.  -875.   883.  -885.   887.   914.\n",
      "  -915.  -918.   919.  -928.  -932.  -932.   951.  -952.  -956.   962.\n",
      "  -967.  1004.  1024.  1024.  1032.  1033.  1035.  1039.  1046.  1059.\n",
      "  1064.  1069.  1082.  1088.  1089.  1091.  1102.  1104.  1106. -1127.\n",
      "  1161.  1162.  1189. -1207. -1212.  1213.  1229.  1249.  1259.  1278.\n",
      "  1279.  1319.  1321.  1324.  1329.  1341. -1342.  1348.  1354.  1354.\n",
      " -1357.  1366.  1369.  1384. -1386.  1399.  1446.  1448.  1470.  1483.\n",
      "  1516.  1562.  1579.  1583.  1583.  1620.  1646.  1650. -1684.  1688.\n",
      "  1699.  1720.  1721. -1722.  1736.  1746.  1757.  1769. -1785.  1799.\n",
      "  1815. -1844.  1875. -1882. -1900.  1947.  1955.  1977. -1977. -1993.\n",
      "  1993. -1998.  2009.  2012.  2012. -2032.  2049. -2078.  2089.  2148.\n",
      " -2182.  2182.  2218. -2329. -2338. -2369.  2400. -2464. -2535.  2634.\n",
      "  2648.  2688.  2692.  2742. -3096. -3325.  3337. -3525. -3532. -3871.\n",
      " -4424. -5481.]\n",
      "Concordance Index 0.504730713245997\n",
      "Integrated Brier Score: 0.12513580362848592\n",
      "y_train breslow final [    8.     9.    11.   -16.    24.   -24.    24.    25.    31.    31.\n",
      "    36.   -61.    65.   -67.    74.    75.   -81.    90.    91.   -92.\n",
      "    92.   -98.  -113.  -122.  -130.  -133.   138.  -145.  -148.  -163.\n",
      "  -168.  -168.  -176.  -176.  -181.  -183.  -186.  -192.   197.  -204.\n",
      "  -220.  -238.  -243.  -253.   260.  -260.  -260.  -277.   286.   304.\n",
      "   304.   336.  -338.  -343.   346.  -360.  -361.   365.  -379.   394.\n",
      "   395.   396.  -420.  -442.  -454.   457.   479.   493.   506.  -508.\n",
      "   515.   518.   524.   528.  -529.   531.   542.   547.  -547.   555.\n",
      "   563.  -571.  -576.  -581.   594.   608.   629.   637.   637.  -643.\n",
      "  -655.   663.   676.   676.  -684.  -686.   695.   728.   731.   737.\n",
      "  -741.  -751.   760.  -761.  -763.  -772.  -783.   820.   820.   821.\n",
      "  -837.  -848.   857.   863.   868.  -875.   883.  -885.   887.   914.\n",
      "  -915.  -918.   919.  -928.  -932.  -932.   951.  -952.  -956.   962.\n",
      "  -967.  1004.  1024.  1024.  1032.  1033.  1035.  1039.  1046.  1059.\n",
      "  1064.  1069.  1082.  1088.  1089.  1091.  1102.  1104.  1106. -1127.\n",
      "  1161.  1162.  1189. -1207. -1212.  1213.  1229.  1249.  1259.  1278.\n",
      "  1279.  1319.  1321.  1324.  1329.  1341. -1342.  1348.  1354.  1354.\n",
      " -1357.  1366.  1369.  1384. -1386.  1399.  1446.  1448.  1470.  1483.\n",
      "  1516.  1562.  1579.  1583.  1583.  1620.  1646.  1650. -1684.  1688.\n",
      "  1699.  1720.  1721. -1722.  1736.  1746.  1757.  1769. -1785.  1799.\n",
      "  1815. -1844.  1875. -1882. -1900.  1947.  1955.  1977. -1977. -1993.\n",
      "  1993. -1998.  2009.  2012.  2012. -2032.  2049. -2078.  2089.  2148.\n",
      " -2182.  2182.  2218. -2329. -2338. -2369.  2400. -2464. -2535.  2634.\n",
      "  2648.  2688.  2692.  2742. -3096. -3325.  3337. -3525. -3532. -3871.\n",
      " -4424. -5481.]\n",
      "durations 53.0 4624.0\n",
      "Concordance Index 0.5986394557823129\n",
      "Integrated Brier Score: 0.14867696884507353\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m114.6016\u001b[0m  0.0019\n",
      "      2      114.6016  0.0016\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      3      114.6016  0.0015\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      4      114.6016  0.0015\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      5      114.6016  0.0014\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m122.1499\u001b[0m  0.0028\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m85.6513\u001b[0m  0.0018\n",
      "      6      114.6016  0.0019\n",
      "      2      122.1499  0.0017\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2       85.6513  0.0015\n",
      "      7      114.6016  0.0016\n",
      "      3      122.1499  0.0015\n",
      "      3       85.6513  0.0014\n",
      "      8      114.6016  0.0015\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      4      122.1499  0.0015\n",
      "      4       85.6513  0.0014\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      9      114.6016  0.0014\n",
      "      5      122.1499  0.0015\n",
      "      5       85.6513  0.0014\n",
      "     10      114.6016  0.0014\n",
      "Restoring best model from epoch 1.\n",
      "      6       85.6513  0.0013\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m92.6744\u001b[0m  0.0017\n",
      "      6      122.1499  0.0015\n",
      "      7       85.6513  0.0013\n",
      "      7      122.1499  0.0015\n",
      "      2       92.6744  0.0015\n",
      "      8       85.6513  0.0013\n",
      "      8      122.1499  0.0015\n",
      "      3       92.6744  0.0015\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m106.7299\u001b[0m  0.0088\n",
      "      9       85.6513  0.0013\n",
      "      4       92.6744  0.0014\n",
      "      9      122.1499  0.0015\n",
      "      2      106.7299  0.0017\n",
      "     10       85.6513  0.0013\n",
      "Restoring best model from epoch 1.\n",
      "      5       92.6744  0.0014\n",
      "     10      122.1499  0.0015\n",
      "Restoring best model from epoch 1.\n",
      "      3      106.7299  0.0016\n",
      "      6       92.6744  0.0014\n",
      "      4      106.7299  0.0015\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m131.7001\u001b[0m       \u001b[32m66.0897\u001b[0m  0.0189\n",
      "      7       92.6744  0.0014\n",
      "      5      106.7299  0.0019\n",
      "      8       92.6744  0.0014\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      9       92.6744  0.0015\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "     10       92.6744  0.0014\n",
      "Restoring best model from epoch 1.\n",
      "      6      106.7299  0.0034\n",
      "      7      106.7299  0.0016\n",
      "      8      106.7299  0.0015\n",
      "      9      106.7299  0.0015\n",
      "     10      106.7299  0.0014\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2      \u001b[36m125.2952\u001b[0m       \u001b[32m66.0557\u001b[0m  0.0165\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m119.0860\u001b[0m       \u001b[32m56.3334\u001b[0m  0.0178\n",
      "      3      \u001b[36m124.2541\u001b[0m       \u001b[32m65.8447\u001b[0m  0.0148\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m149.2146\u001b[0m       \u001b[32m81.7028\u001b[0m  0.0218\n",
      "      2      125.9342       57.0196  0.0154\n",
      "      4      125.5329       65.9722  0.0252\n",
      "      3      124.5739       57.4301  0.0176\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      2      \u001b[36m146.3984\u001b[0m       \u001b[32m81.5918\u001b[0m  0.0312\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      4      120.9782       56.7012  0.0176\n",
      "      5      \u001b[36m120.2838\u001b[0m       \u001b[32m65.6648\u001b[0m  0.0187\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m131.3354\u001b[0m       \u001b[32m68.0468\u001b[0m  0.0180\n",
      "      3      \u001b[36m143.4162\u001b[0m       82.0683  0.0177\n",
      "      6      \u001b[36m119.2651\u001b[0m       \u001b[32m65.1017\u001b[0m  0.0159\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m126.5384\u001b[0m       \u001b[32m68.8408\u001b[0m  0.0216\n",
      "      2      \u001b[36m130.0367\u001b[0m       68.2432  0.0220\n",
      "      5      \u001b[36m115.0592\u001b[0m       56.6849  0.0259\n",
      "      4      \u001b[36m136.3241\u001b[0m       82.4483  0.0179\n",
      "      7      119.4609       \u001b[32m64.8436\u001b[0m  0.0180\n",
      "      2      129.8699       \u001b[32m68.0651\u001b[0m  0.0190\n",
      "      6      \u001b[36m112.6655\u001b[0m       56.5339  0.0175\n",
      "      3      132.3979       68.5443  0.0179\n",
      "      5      \u001b[36m134.6179\u001b[0m       82.7759  0.0188\n",
      "      3      128.4809       \u001b[32m67.9140\u001b[0m  0.0153\n",
      "      8      123.9889       64.8658  0.0195\n",
      "      7      114.4547       \u001b[32m56.0040\u001b[0m  0.0136\n",
      "      4      \u001b[36m127.9231\u001b[0m       68.6009  0.0152\n",
      "      6      \u001b[36m134.5989\u001b[0m       83.0037  0.0162\n",
      "      4      \u001b[36m122.6986\u001b[0m       68.1650  0.0153\n",
      "      9      124.2307       65.1138  0.0146\n",
      "      8      114.8128       56.1458  0.0133\n",
      "      5      \u001b[36m123.0377\u001b[0m       68.7139  0.0154\n",
      "      7      135.8584       83.3356  0.0145\n",
      "      9      115.6994       56.3241  0.0131\n",
      "      5      \u001b[36m119.8525\u001b[0m       68.2251  0.0158\n",
      "     10      \u001b[36m118.7027\u001b[0m       65.0492  0.0148\n",
      "Restoring best model from epoch 7.\n",
      "      6      \u001b[36m122.3582\u001b[0m       68.6905  0.0152\n",
      "      8      \u001b[36m131.8470\u001b[0m       83.8517  0.0138\n",
      "     10      113.1349       56.8144  0.0131\n",
      "Restoring best model from epoch 7.\n",
      "      6      120.9922       68.4771  0.0143\n",
      "      7      \u001b[36m120.0026\u001b[0m       68.7173  0.0140\n",
      "      9      \u001b[36m130.7938\u001b[0m       84.5253  0.0137\n",
      "      7      122.3187       68.7368  0.0144\n",
      "      8      \u001b[36m117.5450\u001b[0m       68.6517  0.0136\n",
      "     10      133.9637       84.5463  0.0137\n",
      "Restoring best model from epoch 2.\n",
      "      8      \u001b[36m116.3710\u001b[0m       68.9392  0.0135\n",
      "      9      119.1195       68.7708  0.0145\n",
      "      9      116.4900       69.0230  0.0148\n",
      "     10      118.1747       68.9016  0.0141\n",
      "Restoring best model from epoch 1.\n",
      "     10      120.3522       68.6544  0.0143\n",
      "Restoring best model from epoch 3.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m126.1997\u001b[0m       \u001b[32m95.2681\u001b[0m  0.0220\n",
      "      2      \u001b[36m123.4166\u001b[0m       95.9735  0.0219\n",
      "      3      127.7623       96.8527  0.0248\n",
      "      4      \u001b[36m123.3723\u001b[0m       97.5123  0.0286\n",
      "      5      \u001b[36m122.1125\u001b[0m       96.5691  0.0249\n",
      "      6      \u001b[36m120.0233\u001b[0m       95.9038  0.0260\n",
      "      7      121.5213       95.8076  0.0261\n",
      "      8      \u001b[36m119.3352\u001b[0m       95.8768  0.0253\n",
      "      9      119.9030       95.8691  0.0264\n",
      "     10      \u001b[36m118.9732\u001b[0m       95.8860  0.0263\n",
      "Restoring best model from epoch 1.\n",
      "y_train breslow final [    8.     9.    11.   -16.   -24.    24.    25.    31.    31.    53.\n",
      "    61.   -61.    65.   -67.    74.    75.   -81.    90.    91.   -92.\n",
      "    92.   -98.  -106.  -113.  -122.  -133.   138.  -145.  -147.  -148.\n",
      "  -163.  -165.  -168.  -168.  -176.  -176.  -181.  -186.   186.  -190.\n",
      "  -194.   197.  -204.  -220.  -239.  -255.   260.  -260.  -260.  -268.\n",
      "  -277.   304.   336.  -338.  -343.  -361.   365.  -379.   394.   395.\n",
      "  -441.  -442.  -454.   455.   457.   506.  -508.   515.   518.   524.\n",
      "   528.  -529.   531.   542.   547.  -547.  -547.   562.   565.   568.\n",
      "  -571.  -576.   594.   624.   627.   629.   637.   663.   676.   676.\n",
      "   679.  -684.  -686.   695.   728.   731.   737.  -741.  -751.  -761.\n",
      "  -763.  -772.   787.  -816.   820.   821.   840.  -847.   868.  -875.\n",
      "   883.   914.  -915.  -918.   919.  -932.   949.   951.  -952.  -956.\n",
      "   962.  -967.   976.  1004.  1018.  1024.  1035.  1046. -1053.  1058.\n",
      "  1059.  1064.  1069.  1082.  1088.  1102.  1104.  1123. -1127.  1155.\n",
      "  1157.  1161.  1163.  1187.  1189. -1207. -1212.  1213.  1229.  1249.\n",
      "  1259.  1278.  1279.  1319.  1321.  1341. -1342.  1348.  1364. -1364.\n",
      "  1366.  1369. -1372.  1384.  1399.  1446.  1448.  1451.  1470.  1483.\n",
      "  1484.  1484.  1492.  1516.  1562. -1573.  1579.  1579.  1583.  1583.\n",
      "  1620.  1650. -1684.  1688.  1699.  1721. -1722.  1736.  1746. -1752.\n",
      "  1757.  1767.  1769. -1785.  1799.  1815. -1844.  1875. -1882.  1891.\n",
      " -1919.  1947.  1955.  1977. -1993. -1998.  2009.  2012.  2012. -2032.\n",
      "  2049.  2089. -2143.  2148. -2182.  2218. -2329. -2338.  2342.  2345.\n",
      " -2369. -2464.  2467. -2535. -2614.  2621.  2648. -2661. -2661.  2688.\n",
      "  2692.  2717.  2742. -3096.  3224.  3337. -3525. -3532. -3825. -4424.\n",
      "  4624. -5481.]\n",
      "Concordance Index 0.4993367900639093\n",
      "Integrated Brier Score: 0.12382283162537086\n",
      "y_train breslow final [    8.     9.    11.   -16.   -24.    24.    25.    31.    31.    53.\n",
      "    61.   -61.    65.   -67.    74.    75.   -81.    90.    91.   -92.\n",
      "    92.   -98.  -106.  -113.  -122.  -133.   138.  -145.  -147.  -148.\n",
      "  -163.  -165.  -168.  -168.  -176.  -176.  -181.  -186.   186.  -190.\n",
      "  -194.   197.  -204.  -220.  -239.  -255.   260.  -260.  -260.  -268.\n",
      "  -277.   304.   336.  -338.  -343.  -361.   365.  -379.   394.   395.\n",
      "  -441.  -442.  -454.   455.   457.   506.  -508.   515.   518.   524.\n",
      "   528.  -529.   531.   542.   547.  -547.  -547.   562.   565.   568.\n",
      "  -571.  -576.   594.   624.   627.   629.   637.   663.   676.   676.\n",
      "   679.  -684.  -686.   695.   728.   731.   737.  -741.  -751.  -761.\n",
      "  -763.  -772.   787.  -816.   820.   821.   840.  -847.   868.  -875.\n",
      "   883.   914.  -915.  -918.   919.  -932.   949.   951.  -952.  -956.\n",
      "   962.  -967.   976.  1004.  1018.  1024.  1035.  1046. -1053.  1058.\n",
      "  1059.  1064.  1069.  1082.  1088.  1102.  1104.  1123. -1127.  1155.\n",
      "  1157.  1161.  1163.  1187.  1189. -1207. -1212.  1213.  1229.  1249.\n",
      "  1259.  1278.  1279.  1319.  1321.  1341. -1342.  1348.  1364. -1364.\n",
      "  1366.  1369. -1372.  1384.  1399.  1446.  1448.  1451.  1470.  1483.\n",
      "  1484.  1484.  1492.  1516.  1562. -1573.  1579.  1579.  1583.  1583.\n",
      "  1620.  1650. -1684.  1688.  1699.  1721. -1722.  1736.  1746. -1752.\n",
      "  1757.  1767.  1769. -1785.  1799.  1815. -1844.  1875. -1882.  1891.\n",
      " -1919.  1947.  1955.  1977. -1993. -1998.  2009.  2012.  2012. -2032.\n",
      "  2049.  2089. -2143.  2148. -2182.  2218. -2329. -2338.  2342.  2345.\n",
      " -2369. -2464.  2467. -2535. -2614.  2621.  2648. -2661. -2661.  2688.\n",
      "  2692.  2717.  2742. -3096.  3224.  3337. -3525. -3532. -3825. -4424.\n",
      "  4624. -5481.]\n",
      "durations 24.0 3871.0\n",
      "Concordance Index 0.5556660039761432\n",
      "Integrated Brier Score: 0.1483175205283391\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m85.7843\u001b[0m  0.0019\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2       85.7843  0.0017\n",
      "      3       85.7843  0.0016\n",
      "      4       85.7843  0.0016\n",
      "      5       85.7843  0.0016\n",
      "      6       85.7843  0.0015\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      7       85.7843  0.0015\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m130.2763\u001b[0m       \u001b[32m71.0165\u001b[0m  0.0162\n",
      "      8       85.7843  0.0015\n",
      "      9       85.7843  0.0015\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m112.8716\u001b[0m  0.0018\n",
      "     10       85.7843  0.0015\n",
      "Restoring best model from epoch 1.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m122.9347\u001b[0m       \u001b[32m69.7345\u001b[0m  0.0160\n",
      "      2      112.8716  0.0038\n",
      "      3      112.8716  0.0015\n",
      "      4      112.8716  0.0015\n",
      "      5      112.8716  0.0015\n",
      "      6      112.8716  0.0014\n",
      "      7      112.8716  0.0014\n",
      "      8      112.8716  0.0014\n",
      "      9      112.8716  0.0014\n",
      "     10      112.8716  0.0014\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2      130.4636       71.1063  0.0219\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m97.8365\u001b[0m  0.0017\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      2       97.8365  0.0015\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      3       97.8365  0.0014\n",
      "      2      127.2070       70.3407  0.0234\n",
      "      4       97.8365  0.0014\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m105.5291\u001b[0m  0.0019\n",
      "      5       97.8365  0.0014\n",
      "      2      105.5291  0.0015\n",
      "      6       97.8365  0.0014\n",
      "      3      105.5291  0.0014\n",
      "      7       97.8365  0.0014\n",
      "      4      105.5291  0.0014\n",
      "      8       97.8365  0.0014\n",
      "      5      105.5291  0.0014\n",
      "      9       97.8365  0.0018\n",
      "     10       97.8365  0.0014\n",
      "Restoring best model from epoch 1.\n",
      "      6      105.5291  0.0029\n",
      "      3      \u001b[36m127.4506\u001b[0m       \u001b[32m70.8462\u001b[0m  0.0153\n",
      "      7      105.5291  0.0015\n",
      "      8      105.5291  0.0014\n",
      "      9      105.5291  0.0014\n",
      "     10      105.5291  0.0014\n",
      "Restoring best model from epoch 1.\n",
      "      3      \u001b[36m117.8725\u001b[0m       \u001b[32m69.3222\u001b[0m  0.0197\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      4      \u001b[36m127.2819\u001b[0m       \u001b[32m70.3955\u001b[0m  0.0154\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      4      117.8751       \u001b[32m69.1004\u001b[0m  0.0143\n",
      "      5      \u001b[36m124.8343\u001b[0m       70.6378  0.0147\n",
      "      5      \u001b[36m114.5832\u001b[0m       69.5341  0.0163\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m144.2834\u001b[0m       \u001b[32m79.3210\u001b[0m  0.0235\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2      152.4972       79.5292  0.0168\n",
      "      6      \u001b[36m111.9461\u001b[0m       70.3762  0.0199\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      6      127.1179       70.4362  0.0264\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m118.5857\u001b[0m       \u001b[32m59.3570\u001b[0m  0.0155\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m93.1003\u001b[0m  0.0037\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2       93.1003  0.0016\n",
      "      3       93.1003  0.0017\n",
      "      4       93.1003  0.0015\n",
      "      3      146.6706       \u001b[32m78.6886\u001b[0m  0.0151\n",
      "      5       93.1003  0.0014\n",
      "      7      113.0348       71.0794  0.0162\n",
      "      6       93.1003  0.0014\n",
      "      2      \u001b[36m118.0629\u001b[0m       59.8759  0.0145\n",
      "      7       93.1003  0.0014\n",
      "      8       93.1003  0.0014\n",
      "      9       93.1003  0.0014\n",
      "     10       93.1003  0.0014\n",
      "Restoring best model from epoch 1.\n",
      "      7      \u001b[36m122.6684\u001b[0m       \u001b[32m70.3671\u001b[0m  0.0211\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m118.6800\u001b[0m       \u001b[32m74.7528\u001b[0m  0.0177\n",
      "      4      \u001b[36m142.6069\u001b[0m       \u001b[32m78.4630\u001b[0m  0.0148\n",
      "      8      114.2787       72.1414  0.0157\n",
      "      3      121.1626       \u001b[32m59.2103\u001b[0m  0.0152\n",
      "      8      124.2869       70.8165  0.0152\n",
      "      2      131.1586       75.3068  0.0147\n",
      "      4      119.7960       \u001b[32m58.8488\u001b[0m  0.0141\n",
      "      9      \u001b[36m111.8376\u001b[0m       73.0254  0.0161\n",
      "      5      \u001b[36m141.6480\u001b[0m       \u001b[32m78.4551\u001b[0m  0.0179\n",
      "      9      \u001b[36m121.5718\u001b[0m       71.2501  0.0148\n",
      "      3      121.8401       75.2708  0.0138\n",
      "      5      \u001b[36m116.3147\u001b[0m       59.1418  0.0137\n",
      "     10      113.0088       73.9198  0.0144\n",
      "Restoring best model from epoch 4.\n",
      "      6      \u001b[36m137.7687\u001b[0m       78.4761  0.0162\n",
      "     10      \u001b[36m120.1392\u001b[0m       71.5189  0.0138\n",
      "Restoring best model from epoch 7.\n",
      "      4      \u001b[36m115.3653\u001b[0m       75.2646  0.0133\n",
      "      6      \u001b[36m113.7816\u001b[0m       59.5298  0.0141\n",
      "      7      138.5373       78.5292  0.0160\n",
      "      5      \u001b[36m114.3657\u001b[0m       75.8127  0.0138\n",
      "      7      \u001b[36m110.2226\u001b[0m       59.7767  0.0138\n",
      "      8      142.9631       78.4665  0.0150\n",
      "      6      \u001b[36m112.5063\u001b[0m       75.8037  0.0135\n",
      "      8      111.4365       59.5977  0.0140\n",
      "      7      112.8354       76.3328  0.0134\n",
      "      9      \u001b[36m137.4767\u001b[0m       \u001b[32m78.4243\u001b[0m  0.0160\n",
      "      9      \u001b[36m110.1230\u001b[0m       59.2453  0.0152\n",
      "      8      114.4610       77.0940  0.0137\n",
      "     10      140.5675       \u001b[32m78.3797\u001b[0m  0.0155\n",
      "     10      111.8156       58.9046  0.0142\n",
      "Restoring best model from epoch 4.\n",
      "      9      113.7365       78.1079  0.0143\n",
      "     10      \u001b[36m109.9453\u001b[0m       79.2999  0.0136\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m134.4720\u001b[0m       \u001b[32m98.1383\u001b[0m  0.0227\n",
      "      2      \u001b[36m133.6069\u001b[0m       98.4257  0.0241\n",
      "      3      \u001b[36m127.8532\u001b[0m       98.6853  0.0273\n",
      "      4      \u001b[36m127.6586\u001b[0m       \u001b[32m97.4660\u001b[0m  0.0257\n",
      "      5      \u001b[36m126.5004\u001b[0m       97.7175  0.0245\n",
      "      6      \u001b[36m124.4548\u001b[0m       97.8862  0.0249\n",
      "      7      \u001b[36m124.2302\u001b[0m       98.0215  0.0253\n",
      "      8      \u001b[36m124.1744\u001b[0m       98.2436  0.0243\n",
      "      9      \u001b[36m121.7308\u001b[0m       98.4714  0.0258\n",
      "     10      125.3679       98.4511  0.0238\n",
      "Restoring best model from epoch 4.\n",
      "y_train breslow final [    8.     9.   -16.    24.   -24.    25.    31.    31.    36.    53.\n",
      "    61.   -61.   -67.    74.   -81.    90.    91.   -92.    92.  -106.\n",
      "  -122.  -130.  -133.   138.  -145.  -147.  -163.  -165.  -168.  -176.\n",
      "  -176.  -181.  -183.   186.  -190.  -192.  -194.   197.  -204.  -220.\n",
      "  -238.  -239.  -243.  -253.  -255.  -260.  -260.  -268.  -277.   286.\n",
      "   304.   304.   336.  -338.   346.  -360.   365.  -379.   394.   396.\n",
      "  -420.  -441.  -442.  -454.   455.   457.   479.   493.   506.  -508.\n",
      "   515.   524.   528.  -529.   531.   542.   547.  -547.  -547.   555.\n",
      "   562.   563.   565.   568.  -571.  -576.  -581.   594.   608.   624.\n",
      "   627.   637.   637.  -643.  -655.   663.   679.  -684.   728.  -741.\n",
      "  -751.   760.  -761.  -772.  -783.   787.  -816.   820.   820.   821.\n",
      "  -837.   840.  -847.  -848.   857.   863.   868.  -875.  -885.   887.\n",
      "   914.  -915.   919.  -928.  -932.   949.   951.  -952.  -956.   962.\n",
      "   976.  1018.  1024.  1024.  1032.  1033.  1035.  1039.  1046. -1053.\n",
      "  1058.  1059.  1064.  1082.  1088.  1089.  1091.  1102.  1104.  1106.\n",
      "  1123.  1155.  1157.  1161.  1162.  1163.  1187. -1207. -1212.  1229.\n",
      "  1249.  1249.  1278.  1279.  1319.  1321.  1324.  1329.  1341.  1354.\n",
      "  1354. -1357.  1364. -1364.  1366.  1369. -1372. -1386.  1399.  1448.\n",
      "  1451.  1470.  1483.  1484.  1484.  1492. -1573.  1579.  1579.  1583.\n",
      "  1583.  1646.  1650.  1688.  1720.  1746. -1752.  1757.  1767.  1769.\n",
      "  1815. -1844.  1875. -1882.  1891. -1900. -1919.  1947.  1955. -1977.\n",
      "  1993.  2009.  2049. -2078.  2089. -2143.  2182. -2329. -2338.  2342.\n",
      "  2345. -2369.  2400. -2464.  2467. -2614.  2621.  2634. -2661. -2661.\n",
      "  2688.  2692.  2717.  2742.  3224. -3325. -3532. -3825. -3871. -4424.\n",
      "  4624. -5481.]\n",
      "Concordance Index 0.5784681636419563\n",
      "Integrated Brier Score: 0.115815105435137\n",
      "y_train breslow final [    8.     9.   -16.    24.   -24.    25.    31.    31.    36.    53.\n",
      "    61.   -61.   -67.    74.   -81.    90.    91.   -92.    92.  -106.\n",
      "  -122.  -130.  -133.   138.  -145.  -147.  -163.  -165.  -168.  -176.\n",
      "  -176.  -181.  -183.   186.  -190.  -192.  -194.   197.  -204.  -220.\n",
      "  -238.  -239.  -243.  -253.  -255.  -260.  -260.  -268.  -277.   286.\n",
      "   304.   304.   336.  -338.   346.  -360.   365.  -379.   394.   396.\n",
      "  -420.  -441.  -442.  -454.   455.   457.   479.   493.   506.  -508.\n",
      "   515.   524.   528.  -529.   531.   542.   547.  -547.  -547.   555.\n",
      "   562.   563.   565.   568.  -571.  -576.  -581.   594.   608.   624.\n",
      "   627.   637.   637.  -643.  -655.   663.   679.  -684.   728.  -741.\n",
      "  -751.   760.  -761.  -772.  -783.   787.  -816.   820.   820.   821.\n",
      "  -837.   840.  -847.  -848.   857.   863.   868.  -875.  -885.   887.\n",
      "   914.  -915.   919.  -928.  -932.   949.   951.  -952.  -956.   962.\n",
      "   976.  1018.  1024.  1024.  1032.  1033.  1035.  1039.  1046. -1053.\n",
      "  1058.  1059.  1064.  1082.  1088.  1089.  1091.  1102.  1104.  1106.\n",
      "  1123.  1155.  1157.  1161.  1162.  1163.  1187. -1207. -1212.  1229.\n",
      "  1249.  1249.  1278.  1279.  1319.  1321.  1324.  1329.  1341.  1354.\n",
      "  1354. -1357.  1364. -1364.  1366.  1369. -1372. -1386.  1399.  1448.\n",
      "  1451.  1470.  1483.  1484.  1484.  1492. -1573.  1579.  1579.  1583.\n",
      "  1583.  1646.  1650.  1688.  1720.  1746. -1752.  1757.  1767.  1769.\n",
      "  1815. -1844.  1875. -1882.  1891. -1900. -1919.  1947.  1955. -1977.\n",
      "  1993.  2009.  2049. -2078.  2089. -2143.  2182. -2329. -2338.  2342.\n",
      "  2345. -2369.  2400. -2464.  2467. -2614.  2621.  2634. -2661. -2661.\n",
      "  2688.  2692.  2717.  2742.  3224. -3325. -3532. -3825. -3871. -4424.\n",
      "  4624. -5481.]\n",
      "durations 11.0 3525.0\n",
      "Concordance Index 0.46441947565543074\n",
      "Integrated Brier Score: 0.16736086653943127\n",
      "split gex_?|100133144      float32\n",
      "gex_?|100134869      float32\n",
      "gex_?|10357          float32\n",
      "gex_?|10431          float32\n",
      "gex_?|155060         float32\n",
      "                      ...   \n",
      "gex_ZYG11A|440590    float32\n",
      "gex_ZYG11B|79699     float32\n",
      "gex_ZYX|7791         float32\n",
      "gex_ZZEF1|23140      float32\n",
      "gex_ZZZ3|26009       float32\n",
      "Length: 19076, dtype: object\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m103.1867\u001b[0m       \u001b[32m70.7945\u001b[0m  0.0265  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m93.0851\u001b[0m  0.0029\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m65.7177\u001b[0m  0.0029\n",
      "      2       93.0851  0.0019\n",
      "\n",
      "      2       65.7177  0.0018\n",
      "      3       93.0851  0.0017\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      3       65.7177  0.0017\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      4       93.0851  0.0017\n",
      "      4       65.7177  0.0016\n",
      "      5       93.0851  0.0017\n",
      "      5       65.7177  0.0016\n",
      "      6       93.0851  0.0016\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      6       65.7177  0.0016\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      7       93.0851  0.0016\n",
      "      7       65.7177  0.0018\n",
      "      8       93.0851  0.0016\n",
      "      8       65.7177  0.0016\n",
      "      9       93.0851  0.0018\n",
      "      9       65.7177  0.0016\n",
      "     10       93.0851  0.0016\n",
      "Restoring best model from epoch 1.\n",
      "     10       65.7177  0.0016\n",
      "Restoring best model from epoch 1.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m72.6665\u001b[0m       \u001b[32m61.2078\u001b[0m  0.0345\n",
      "      2       \u001b[36m94.0598\u001b[0m       71.1716  0.0246\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m79.8742\u001b[0m       \u001b[32m58.7206\u001b[0m  0.0313\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2       76.2402       61.4541  0.0212\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m93.3092\u001b[0m       \u001b[32m59.4480\u001b[0m  0.0400\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m99.6256\u001b[0m  0.0048\n",
      "      2       99.6256  0.0022\n",
      "      3       94.3410       \u001b[32m70.3652\u001b[0m  0.0291\n",
      "      3       99.6256  0.0053\n",
      "      4       99.6256  0.0059\n",
      "      5       99.6256  0.0023\n",
      "      2       \u001b[36m77.0694\u001b[0m       \u001b[32m57.7184\u001b[0m  0.0262\n",
      "      6       99.6256  0.0023\n",
      "      7       99.6256  0.0020\n",
      "      3       76.2971       61.9463  0.0314\n",
      "      8       99.6256  0.0018\n",
      "      9       99.6256  0.0018\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "     10       99.6256  0.0018\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2       \u001b[36m90.8967\u001b[0m       60.0605  0.0377\n",
      "      4       \u001b[36m92.2331\u001b[0m       \u001b[32m69.9882\u001b[0m  0.0331\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m88.2670\u001b[0m  0.0054\n",
      "      3       78.4815       57.8651  0.0266\n",
      "      2       88.2670  0.0041\n",
      "      4       \u001b[36m72.1217\u001b[0m       \u001b[32m60.8191\u001b[0m  0.0289\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      3       88.2670  0.0062\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      4       88.2670  0.0027\n",
      "      3       \u001b[36m85.5500\u001b[0m       59.5662  0.0251\n",
      "      5       88.2670  0.0040\n",
      "      6       88.2670  0.0019\n",
      "      5       92.3820       70.2303  0.0255\n",
      "      7       88.2670  0.0017\n",
      "      8       88.2670  0.0017\n",
      "      9       88.2670  0.0032\n",
      "      4       78.4955       \u001b[32m57.6297\u001b[0m  0.0264\n",
      "     10       88.2670  0.0038\n",
      "Restoring best model from epoch 1.\n",
      "      5       \u001b[36m70.7367\u001b[0m       \u001b[32m59.9935\u001b[0m  0.0275\n",
      "      4       88.0093       59.4945  0.0230\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m84.7424\u001b[0m       \u001b[32m49.6530\u001b[0m  0.0240\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      6       92.5855       70.6692  0.0236\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m70.1510\u001b[0m  0.0029\n",
      "      2       70.1510  0.0018\n",
      "      3       70.1510  0.0017\n",
      "      4       70.1510  0.0024\n",
      "      5       70.1510  0.0020\n",
      "      5       \u001b[36m73.4695\u001b[0m       57.9073  0.0297\n",
      "      6       70.1510  0.0038\n",
      "      7       70.1510  0.0018\n",
      "      5       89.9795       59.6716  0.0234\n",
      "      6       \u001b[36m69.0216\u001b[0m       60.1445  0.0243\n",
      "      8       70.1510  0.0017\n",
      "      2       87.5891       \u001b[32m49.5916\u001b[0m  0.0249\n",
      "      9       70.1510  0.0029\n",
      "     10       70.1510  0.0018\n",
      "Restoring best model from epoch 1.\n",
      "      7       \u001b[36m91.8067\u001b[0m       70.9722  0.0286\n",
      "      7       \u001b[36m67.6747\u001b[0m       60.9628  0.0224\n",
      "      6       87.4272       59.7611  0.0242\n",
      "      3       \u001b[36m80.7720\u001b[0m       49.8893  0.0228\n",
      "      6       75.9037       58.1180  0.0356\n",
      "      8       92.4673       70.9046  0.0208\n",
      "      7       \u001b[36m84.3912\u001b[0m       60.0977  0.0231\n",
      "      8       68.6842       61.6860  0.0274\n",
      "      4       82.2651       49.7196  0.0275\n",
      "      9       \u001b[36m89.5949\u001b[0m       70.8955  0.0245\n",
      "      7       75.0853       58.5544  0.0296\n",
      "      8       85.6857       60.5883  0.0220\n",
      "      9       68.0825       62.6734  0.0232\n",
      "      8       74.7814       58.7416  0.0209\n",
      "     10       90.0377       70.8310  0.0278\n",
      "Restoring best model from epoch 4.\n",
      "      9       85.4119       60.7456  0.0195\n",
      "      5       81.3825       \u001b[32m49.4483\u001b[0m  0.0380\n",
      "     10       70.4328       62.7393  0.0203\n",
      "Restoring best model from epoch 5.\n",
      "      9       \u001b[36m72.9465\u001b[0m       58.5914  0.0251\n",
      "     10       85.5664       61.3400  0.0197\n",
      "Restoring best model from epoch 1.\n",
      "      6       \u001b[36m79.4060\u001b[0m       \u001b[32m49.0713\u001b[0m  0.0196\n",
      "     10       \u001b[36m72.0705\u001b[0m       58.7426  0.0191\n",
      "Restoring best model from epoch 4.\n",
      "      7       81.2684       49.2749  0.0189\n",
      "      8       79.6388       49.4868  0.0197\n",
      "      9       \u001b[36m78.3424\u001b[0m       49.2962  0.0182\n",
      "     10       \u001b[36m78.0995\u001b[0m       49.4732  0.0196\n",
      "Restoring best model from epoch 6.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m85.1418\u001b[0m       \u001b[32m78.9404\u001b[0m  0.0223\n",
      "      2       85.5113       \u001b[32m78.2037\u001b[0m  0.0213\n",
      "      3       87.9863       \u001b[32m77.3304\u001b[0m  0.0230\n",
      "      4       86.6684       77.5062  0.0263\n",
      "      5       \u001b[36m82.3903\u001b[0m       77.9687  0.0266\n",
      "      6       \u001b[36m80.2688\u001b[0m       78.3505  0.0688\n",
      "      7       81.0428       78.5923  0.0218\n",
      "      8       80.3628       78.1551  0.0217\n",
      "      9       \u001b[36m80.0253\u001b[0m       78.3339  0.0259\n",
      "     10       \u001b[36m77.8434\u001b[0m       79.1132  0.0252\n",
      "Restoring best model from epoch 3.\n",
      "y_train breslow final [ 3.000e+00  8.000e+00 -8.000e+00 -1.400e+01 -1.600e+01 -1.700e+01\n",
      " -2.100e+01 -2.100e+01  2.200e+01 -2.300e+01  2.400e+01 -2.900e+01\n",
      "  3.000e+01 -3.000e+01 -3.100e+01 -3.500e+01  4.500e+01 -4.600e+01\n",
      "  5.200e+01  5.700e+01  6.100e+01 -6.400e+01  7.600e+01 -7.700e+01\n",
      "  8.100e+01 -9.900e+01  1.050e+02  1.050e+02  1.060e+02  1.130e+02\n",
      " -1.130e+02  1.220e+02 -1.310e+02  1.410e+02  1.530e+02  1.560e+02\n",
      " -1.560e+02  1.660e+02  1.680e+02 -1.700e+02  1.740e+02 -1.760e+02\n",
      "  1.800e+02  1.800e+02 -1.830e+02  1.850e+02  1.850e+02  1.880e+02\n",
      "  1.910e+02  1.920e+02 -1.980e+02 -1.980e+02  2.000e+02  2.010e+02\n",
      "  2.050e+02  2.120e+02  2.150e+02  2.180e+02 -2.250e+02 -2.290e+02\n",
      "  2.350e+02  2.430e+02 -2.450e+02  2.450e+02  2.500e+02  2.550e+02\n",
      "  2.620e+02  2.720e+02  2.730e+02 -2.730e+02  2.750e+02  2.760e+02\n",
      "  2.790e+02 -2.800e+02  2.810e+02  2.840e+02 -2.870e+02 -2.880e+02\n",
      "  2.890e+02  2.910e+02  2.940e+02  2.960e+02  3.000e+02  3.000e+02\n",
      " -3.230e+02 -3.250e+02 -3.350e+02  3.360e+02  3.410e+02 -3.440e+02\n",
      " -3.440e+02 -3.460e+02  3.480e+02  3.480e+02  3.530e+02  3.540e+02\n",
      "  3.560e+02 -3.560e+02 -3.580e+02  3.580e+02  3.590e+02 -3.650e+02\n",
      "  3.660e+02 -3.670e+02 -3.680e+02  3.700e+02 -3.710e+02 -3.740e+02\n",
      " -3.750e+02  3.760e+02  3.770e+02  3.780e+02 -3.780e+02 -3.780e+02\n",
      " -3.790e+02 -3.810e+02 -3.830e+02 -3.830e+02 -3.830e+02 -3.880e+02\n",
      "  3.890e+02 -3.890e+02 -3.890e+02 -3.900e+02 -3.920e+02 -3.940e+02\n",
      " -3.960e+02  3.960e+02  3.980e+02 -4.000e+02  4.010e+02  4.030e+02\n",
      "  4.060e+02 -4.080e+02 -4.110e+02 -4.110e+02 -4.130e+02 -4.160e+02\n",
      " -4.190e+02  4.220e+02  4.220e+02  4.260e+02 -4.270e+02 -4.280e+02\n",
      "  4.280e+02 -4.310e+02 -4.340e+02  4.390e+02 -4.400e+02  4.460e+02\n",
      " -4.500e+02  4.570e+02 -4.630e+02  4.740e+02  4.760e+02  4.770e+02\n",
      " -4.790e+02 -4.860e+02  4.910e+02  4.910e+02 -4.910e+02 -4.940e+02\n",
      "  4.960e+02 -5.000e+02  5.070e+02  5.130e+02 -5.190e+02 -5.210e+02\n",
      " -5.230e+02 -5.250e+02  5.430e+02 -5.430e+02  5.490e+02  5.540e+02\n",
      "  5.580e+02 -5.590e+02  5.600e+02 -5.660e+02  5.700e+02 -5.730e+02\n",
      " -5.770e+02 -5.790e+02 -5.800e+02 -5.820e+02  5.880e+02 -5.930e+02\n",
      " -5.940e+02 -5.950e+02 -6.000e+02 -6.070e+02 -6.130e+02 -6.150e+02\n",
      "  6.180e+02 -6.210e+02 -6.250e+02 -6.280e+02  6.330e+02  6.350e+02\n",
      " -6.360e+02  6.400e+02 -6.440e+02 -6.470e+02 -6.500e+02  6.520e+02\n",
      "  6.610e+02 -6.640e+02 -6.660e+02  6.690e+02 -6.750e+02  6.750e+02\n",
      " -6.780e+02 -6.790e+02 -6.940e+02 -6.990e+02  7.120e+02 -7.240e+02\n",
      " -7.250e+02 -7.360e+02 -7.380e+02 -7.390e+02 -7.420e+02 -7.520e+02\n",
      " -7.540e+02  7.620e+02  7.790e+02  7.820e+02 -7.850e+02  7.920e+02\n",
      "  7.940e+02  8.010e+02 -8.120e+02 -8.130e+02 -8.190e+02 -8.250e+02\n",
      "  8.320e+02 -8.380e+02 -8.380e+02 -8.560e+02 -8.620e+02 -8.640e+02\n",
      "  8.690e+02  8.740e+02 -8.810e+02  8.810e+02 -8.820e+02 -9.000e+02\n",
      " -9.120e+02 -9.120e+02 -9.280e+02  9.400e+02 -9.400e+02 -9.420e+02\n",
      " -9.420e+02 -9.460e+02 -9.490e+02 -9.510e+02 -9.720e+02 -9.810e+02\n",
      " -9.890e+02 -9.910e+02 -1.000e+03 -1.002e+03 -1.010e+03 -1.016e+03\n",
      " -1.023e+03 -1.038e+03  1.043e+03 -1.055e+03 -1.072e+03  1.095e+03\n",
      " -1.100e+03 -1.106e+03 -1.108e+03 -1.124e+03 -1.132e+03 -1.133e+03\n",
      " -1.138e+03 -1.145e+03  1.153e+03 -1.160e+03 -1.184e+03 -1.190e+03\n",
      " -1.200e+03 -1.210e+03 -1.223e+03 -1.236e+03  1.294e+03 -1.297e+03\n",
      " -1.319e+03 -1.328e+03 -1.367e+03 -1.431e+03 -1.484e+03 -1.588e+03\n",
      " -1.645e+03  1.686e+03 -1.690e+03  1.747e+03 -1.765e+03 -1.851e+03\n",
      " -1.862e+03 -1.918e+03 -1.935e+03 -1.964e+03 -2.032e+03  2.100e+03\n",
      "  2.197e+03 -2.267e+03 -3.196e+03 -3.519e+03 -3.720e+03]\n",
      "Concordance Index 0.6547366635569503\n",
      "Integrated Brier Score: 0.2069823157694685\n",
      "y_train breslow final [ 3.000e+00  8.000e+00 -8.000e+00 -1.400e+01 -1.600e+01 -1.700e+01\n",
      " -2.100e+01 -2.100e+01  2.200e+01 -2.300e+01  2.400e+01 -2.900e+01\n",
      "  3.000e+01 -3.000e+01 -3.100e+01 -3.500e+01  4.500e+01 -4.600e+01\n",
      "  5.200e+01  5.700e+01  6.100e+01 -6.400e+01  7.600e+01 -7.700e+01\n",
      "  8.100e+01 -9.900e+01  1.050e+02  1.050e+02  1.060e+02  1.130e+02\n",
      " -1.130e+02  1.220e+02 -1.310e+02  1.410e+02  1.530e+02  1.560e+02\n",
      " -1.560e+02  1.660e+02  1.680e+02 -1.700e+02  1.740e+02 -1.760e+02\n",
      "  1.800e+02  1.800e+02 -1.830e+02  1.850e+02  1.850e+02  1.880e+02\n",
      "  1.910e+02  1.920e+02 -1.980e+02 -1.980e+02  2.000e+02  2.010e+02\n",
      "  2.050e+02  2.120e+02  2.150e+02  2.180e+02 -2.250e+02 -2.290e+02\n",
      "  2.350e+02  2.430e+02 -2.450e+02  2.450e+02  2.500e+02  2.550e+02\n",
      "  2.620e+02  2.720e+02  2.730e+02 -2.730e+02  2.750e+02  2.760e+02\n",
      "  2.790e+02 -2.800e+02  2.810e+02  2.840e+02 -2.870e+02 -2.880e+02\n",
      "  2.890e+02  2.910e+02  2.940e+02  2.960e+02  3.000e+02  3.000e+02\n",
      " -3.230e+02 -3.250e+02 -3.350e+02  3.360e+02  3.410e+02 -3.440e+02\n",
      " -3.440e+02 -3.460e+02  3.480e+02  3.480e+02  3.530e+02  3.540e+02\n",
      "  3.560e+02 -3.560e+02 -3.580e+02  3.580e+02  3.590e+02 -3.650e+02\n",
      "  3.660e+02 -3.670e+02 -3.680e+02  3.700e+02 -3.710e+02 -3.740e+02\n",
      " -3.750e+02  3.760e+02  3.770e+02  3.780e+02 -3.780e+02 -3.780e+02\n",
      " -3.790e+02 -3.810e+02 -3.830e+02 -3.830e+02 -3.830e+02 -3.880e+02\n",
      "  3.890e+02 -3.890e+02 -3.890e+02 -3.900e+02 -3.920e+02 -3.940e+02\n",
      " -3.960e+02  3.960e+02  3.980e+02 -4.000e+02  4.010e+02  4.030e+02\n",
      "  4.060e+02 -4.080e+02 -4.110e+02 -4.110e+02 -4.130e+02 -4.160e+02\n",
      " -4.190e+02  4.220e+02  4.220e+02  4.260e+02 -4.270e+02 -4.280e+02\n",
      "  4.280e+02 -4.310e+02 -4.340e+02  4.390e+02 -4.400e+02  4.460e+02\n",
      " -4.500e+02  4.570e+02 -4.630e+02  4.740e+02  4.760e+02  4.770e+02\n",
      " -4.790e+02 -4.860e+02  4.910e+02  4.910e+02 -4.910e+02 -4.940e+02\n",
      "  4.960e+02 -5.000e+02  5.070e+02  5.130e+02 -5.190e+02 -5.210e+02\n",
      " -5.230e+02 -5.250e+02  5.430e+02 -5.430e+02  5.490e+02  5.540e+02\n",
      "  5.580e+02 -5.590e+02  5.600e+02 -5.660e+02  5.700e+02 -5.730e+02\n",
      " -5.770e+02 -5.790e+02 -5.800e+02 -5.820e+02  5.880e+02 -5.930e+02\n",
      " -5.940e+02 -5.950e+02 -6.000e+02 -6.070e+02 -6.130e+02 -6.150e+02\n",
      "  6.180e+02 -6.210e+02 -6.250e+02 -6.280e+02  6.330e+02  6.350e+02\n",
      " -6.360e+02  6.400e+02 -6.440e+02 -6.470e+02 -6.500e+02  6.520e+02\n",
      "  6.610e+02 -6.640e+02 -6.660e+02  6.690e+02 -6.750e+02  6.750e+02\n",
      " -6.780e+02 -6.790e+02 -6.940e+02 -6.990e+02  7.120e+02 -7.240e+02\n",
      " -7.250e+02 -7.360e+02 -7.380e+02 -7.390e+02 -7.420e+02 -7.520e+02\n",
      " -7.540e+02  7.620e+02  7.790e+02  7.820e+02 -7.850e+02  7.920e+02\n",
      "  7.940e+02  8.010e+02 -8.120e+02 -8.130e+02 -8.190e+02 -8.250e+02\n",
      "  8.320e+02 -8.380e+02 -8.380e+02 -8.560e+02 -8.620e+02 -8.640e+02\n",
      "  8.690e+02  8.740e+02 -8.810e+02  8.810e+02 -8.820e+02 -9.000e+02\n",
      " -9.120e+02 -9.120e+02 -9.280e+02  9.400e+02 -9.400e+02 -9.420e+02\n",
      " -9.420e+02 -9.460e+02 -9.490e+02 -9.510e+02 -9.720e+02 -9.810e+02\n",
      " -9.890e+02 -9.910e+02 -1.000e+03 -1.002e+03 -1.010e+03 -1.016e+03\n",
      " -1.023e+03 -1.038e+03  1.043e+03 -1.055e+03 -1.072e+03  1.095e+03\n",
      " -1.100e+03 -1.106e+03 -1.108e+03 -1.124e+03 -1.132e+03 -1.133e+03\n",
      " -1.138e+03 -1.145e+03  1.153e+03 -1.160e+03 -1.184e+03 -1.190e+03\n",
      " -1.200e+03 -1.210e+03 -1.223e+03 -1.236e+03  1.294e+03 -1.297e+03\n",
      " -1.319e+03 -1.328e+03 -1.367e+03 -1.431e+03 -1.484e+03 -1.588e+03\n",
      " -1.645e+03  1.686e+03 -1.690e+03  1.747e+03 -1.765e+03 -1.851e+03\n",
      " -1.862e+03 -1.918e+03 -1.935e+03 -1.964e+03 -2.032e+03  2.100e+03\n",
      "  2.197e+03 -2.267e+03 -3.196e+03 -3.519e+03 -3.720e+03]\n",
      "durations 7.0 3540.0\n",
      "Concordance Index 0.432580424366872\n",
      "Integrated Brier Score: 0.2598385022937182\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m59.2621\u001b[0m  0.0018\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2       59.2621  0.0016\n",
      "      3       59.2621  0.0014\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m79.6638\u001b[0m  0.0017\n",
      "      4       59.2621  0.0014\n",
      "      2       79.6638  0.0015\n",
      "      5       59.2621  0.0014\n",
      "      3       79.6638  0.0014\n",
      "      6       59.2621  0.0014\n",
      "      4       79.6638  0.0014\n",
      "      7       59.2621  0.0015\n",
      "      5       79.6638  0.0014\n",
      "      8       59.2621  0.0014\n",
      "      6       79.6638  0.0014\n",
      "      9       59.2621  0.0014\n",
      "      7       79.6638  0.0014\n",
      "     10       59.2621  0.0014\n",
      "Restoring best model from epoch 1.\n",
      "      8       79.6638  0.0014\n",
      "      9       79.6638  0.0014\n",
      "     10       79.6638  0.0014\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m200.9013\u001b[0m  0.0019\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m203.7951\u001b[0m  0.0020\n",
      "      2      200.9013  0.0016\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1           nan  0.0017\n",
      "      2      203.7951  0.0016\n",
      "      3      200.9013  0.0015\n",
      "      2           nan  0.0015\n",
      "      3      203.7951  0.0019\n",
      "      4      200.9013  0.0015\n",
      "      3           nan  0.0015\n",
      "      5      200.9013  0.0014\n",
      "      4      203.7951  0.0016\n",
      "      4           nan  0.0014\n",
      "      6      200.9013  0.0014\n",
      "      5      203.7951  0.0016\n",
      "      5           nan  0.0014\n",
      "      7      200.9013  0.0014\n",
      "      6      203.7951  0.0015\n",
      "      6           nan  0.0014\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      8      200.9013  0.0014\n",
      "      7      203.7951  0.0015\n",
      "      7           nan  0.0014\n",
      "      9      200.9013  0.0014\n",
      "      8           nan  0.0014\n",
      "      8      203.7951  0.0015\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m70.4469\u001b[0m  0.0016\n",
      "     10      200.9013  0.0014\n",
      "Restoring best model from epoch 1.\n",
      "      9           nan  0.0014\n",
      "      9      203.7951  0.0016\n",
      "      2       70.4469  0.0015\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "      3       70.4469  0.0014\n",
      "     10      203.7951  0.0015\n",
      "Restoring best model from epoch 1.\n",
      "      4       70.4469  0.0014\n",
      "      5       70.4469  0.0014\n",
      "      6       70.4469  0.0025\n",
      "      7       70.4469  0.0046\n",
      "      8       70.4469  0.0036\n",
      "      9       70.4469  0.0016\n",
      "     10       70.4469  0.0014\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m114.5243\u001b[0m  0.0018\n",
      "      2      114.5243  0.0028\n",
      "      3      114.5243  0.0044\n",
      "      4      114.5243  0.0016\n",
      "      5      114.5243  0.0014\n",
      "      6      114.5243  0.0014\n",
      "      7      114.5243  0.0039\n",
      "      8      114.5243  0.0017\n",
      "      9      114.5243  0.0014\n",
      "     10      114.5243  0.0014\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m140.7403\u001b[0m  0.0018\n",
      "      2      140.7403  0.0016\n",
      "      3      140.7403  0.0016\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      4      140.7403  0.0015\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m150.1609\u001b[0m  0.0017\n",
      "      5      140.7403  0.0022\n",
      "      2      150.1609  0.0029\n",
      "      6      140.7403  0.0024\n",
      "      3      150.1609  0.0015\n",
      "      4      150.1609  0.0014\n",
      "      7      140.7403  0.0032\n",
      "      5      150.1609  0.0014\n",
      "      8      140.7403  0.0016\n",
      "      6      150.1609  0.0014\n",
      "      9      140.7403  0.0016\n",
      "      7      150.1609  0.0014\n",
      "     10      140.7403  0.0015\n",
      "Restoring best model from epoch 1.\n",
      "      8      150.1609  0.0014\n",
      "      9      150.1609  0.0014\n",
      "     10      150.1609  0.0014\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m222.9811\u001b[0m  0.0023\n",
      "      2      222.9811  0.0015\n",
      "      3      222.9811  0.0014\n",
      "      4      222.9811  0.0016\n",
      "      5      222.9811  0.0014\n",
      "      6      222.9811  0.0014\n",
      "      7      222.9811  0.0014\n",
      "      8      222.9811  0.0014\n",
      "      9      222.9811  0.0013\n",
      "     10      222.9811  0.0014\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m138.4464\u001b[0m  0.0023\n",
      "      2      138.4464  0.0025\n",
      "      3      138.4464  0.0026\n",
      "      4      138.4464  0.0025\n",
      "      5      138.4464  0.0021\n",
      "      6      138.4464  0.0023\n",
      "      7      138.4464  0.0024\n",
      "      8      138.4464  0.0026\n",
      "      9      138.4464  0.0025\n",
      "     10      138.4464  0.0025\n",
      "Restoring best model from epoch 1.\n",
      "y_train breslow final [ 3.000e+00 -7.000e+00  8.000e+00 -8.000e+00 -2.000e+01 -2.100e+01\n",
      " -2.100e+01  2.200e+01 -2.300e+01  2.400e+01 -2.900e+01  3.000e+01\n",
      "  3.100e+01 -3.100e+01 -3.400e+01 -3.500e+01  4.500e+01  5.200e+01\n",
      "  5.700e+01  6.100e+01 -6.400e+01  6.700e+01  7.600e+01  8.100e+01\n",
      "  8.100e+01  8.200e+01 -9.200e+01 -9.900e+01  1.030e+02  1.050e+02\n",
      "  1.050e+02  1.060e+02  1.130e+02 -1.130e+02  1.220e+02  1.240e+02\n",
      "  1.240e+02  1.320e+02  1.380e+02  1.410e+02  1.530e+02 -1.560e+02\n",
      " -1.640e+02  1.680e+02 -1.700e+02 -1.760e+02  1.800e+02 -1.830e+02\n",
      "  1.850e+02  1.850e+02  1.880e+02 -1.890e+02  1.910e+02  1.920e+02\n",
      " -1.980e+02 -2.000e+02 -2.000e+02  2.000e+02  2.010e+02  2.050e+02\n",
      "  2.120e+02  2.180e+02 -2.240e+02 -2.290e+02  2.290e+02  2.350e+02\n",
      " -2.430e+02  2.430e+02  2.430e+02  2.500e+02  2.550e+02  2.590e+02\n",
      "  2.620e+02  2.720e+02 -2.730e+02  2.740e+02  2.760e+02  2.790e+02\n",
      " -2.800e+02 -2.800e+02  2.810e+02  2.820e+02  2.840e+02  2.840e+02\n",
      " -2.880e+02  2.890e+02  2.910e+02  2.920e+02  2.940e+02  2.960e+02\n",
      "  3.000e+02  3.120e+02 -3.230e+02 -3.250e+02 -3.370e+02  3.410e+02\n",
      "  3.420e+02 -3.420e+02 -3.440e+02  3.440e+02  3.480e+02  3.530e+02\n",
      "  3.540e+02 -3.560e+02  3.560e+02 -3.560e+02 -3.580e+02  3.580e+02\n",
      " -3.650e+02  3.660e+02 -3.670e+02 -3.680e+02 -3.710e+02 -3.740e+02\n",
      " -3.740e+02 -3.750e+02 -3.750e+02  3.760e+02 -3.770e+02  3.780e+02\n",
      " -3.780e+02 -3.790e+02 -3.830e+02 -3.840e+02 -3.850e+02 -3.880e+02\n",
      "  3.890e+02 -3.890e+02 -3.890e+02 -3.920e+02 -3.940e+02 -3.960e+02\n",
      "  3.980e+02  4.010e+02  4.060e+02  4.060e+02 -4.080e+02 -4.110e+02\n",
      " -4.110e+02 -4.130e+02 -4.160e+02 -4.190e+02  4.220e+02 -4.270e+02\n",
      " -4.280e+02  4.280e+02 -4.340e+02  4.390e+02 -4.400e+02 -4.490e+02\n",
      " -4.630e+02  4.660e+02 -4.680e+02  4.760e+02 -4.760e+02  4.770e+02\n",
      " -4.790e+02 -4.850e+02 -4.850e+02 -4.860e+02  4.910e+02  4.910e+02\n",
      " -4.910e+02 -4.940e+02  4.960e+02 -5.000e+02  5.070e+02 -5.110e+02\n",
      "  5.130e+02 -5.230e+02 -5.230e+02  5.260e+02  5.330e+02 -5.430e+02\n",
      "  5.490e+02  5.520e+02  5.540e+02  5.580e+02 -5.590e+02  5.600e+02\n",
      "  5.620e+02 -5.640e+02 -5.660e+02  5.700e+02 -5.720e+02 -5.730e+02\n",
      " -5.790e+02 -5.800e+02 -5.820e+02  5.880e+02 -5.930e+02 -5.940e+02\n",
      " -5.940e+02 -6.000e+02 -6.070e+02  6.070e+02 -6.130e+02 -6.150e+02\n",
      " -6.160e+02  6.180e+02 -6.210e+02 -6.250e+02 -6.280e+02  6.330e+02\n",
      "  6.350e+02 -6.360e+02 -6.410e+02 -6.430e+02 -6.470e+02 -6.500e+02\n",
      "  6.520e+02 -6.640e+02  6.690e+02 -6.750e+02 -6.780e+02 -6.780e+02\n",
      " -6.790e+02 -6.900e+02 -6.920e+02 -6.940e+02 -6.990e+02 -7.250e+02\n",
      " -7.360e+02 -7.380e+02 -7.390e+02 -7.420e+02 -7.520e+02  7.620e+02\n",
      "  7.660e+02  7.820e+02 -7.850e+02  7.920e+02  8.010e+02  8.050e+02\n",
      " -8.120e+02 -8.130e+02 -8.380e+02 -8.380e+02 -8.560e+02 -8.620e+02\n",
      " -8.640e+02  8.690e+02  8.740e+02 -8.810e+02  8.810e+02 -8.950e+02\n",
      " -8.990e+02 -9.000e+02 -9.120e+02 -9.280e+02  9.400e+02 -9.400e+02\n",
      " -9.420e+02 -9.460e+02 -9.490e+02 -9.510e+02 -9.720e+02 -9.910e+02\n",
      " -9.970e+02 -1.000e+03 -1.010e+03 -1.016e+03 -1.023e+03 -1.038e+03\n",
      "  1.043e+03 -1.055e+03 -1.072e+03 -1.083e+03 -1.090e+03  1.095e+03\n",
      " -1.106e+03 -1.108e+03 -1.124e+03 -1.132e+03 -1.133e+03 -1.138e+03\n",
      " -1.145e+03  1.153e+03 -1.184e+03 -1.190e+03 -1.210e+03 -1.236e+03\n",
      "  1.294e+03 -1.297e+03 -1.319e+03 -1.328e+03 -1.367e+03 -1.389e+03\n",
      "  1.407e+03 -1.431e+03 -1.484e+03 -1.588e+03 -1.645e+03 -1.646e+03\n",
      "  1.686e+03 -1.690e+03  1.747e+03 -1.765e+03  1.811e+03 -1.851e+03\n",
      " -1.862e+03 -1.935e+03 -2.032e+03  2.100e+03 -2.171e+03  2.197e+03\n",
      " -2.351e+03 -3.196e+03 -3.519e+03 -3.540e+03 -3.720e+03]\n",
      "Concordance Index 0.48521303258145365\n",
      "Integrated Brier Score: 0.43741644608263525\n",
      "y_train breslow final [ 3.000e+00 -7.000e+00  8.000e+00 -8.000e+00 -2.000e+01 -2.100e+01\n",
      " -2.100e+01  2.200e+01 -2.300e+01  2.400e+01 -2.900e+01  3.000e+01\n",
      "  3.100e+01 -3.100e+01 -3.400e+01 -3.500e+01  4.500e+01  5.200e+01\n",
      "  5.700e+01  6.100e+01 -6.400e+01  6.700e+01  7.600e+01  8.100e+01\n",
      "  8.100e+01  8.200e+01 -9.200e+01 -9.900e+01  1.030e+02  1.050e+02\n",
      "  1.050e+02  1.060e+02  1.130e+02 -1.130e+02  1.220e+02  1.240e+02\n",
      "  1.240e+02  1.320e+02  1.380e+02  1.410e+02  1.530e+02 -1.560e+02\n",
      " -1.640e+02  1.680e+02 -1.700e+02 -1.760e+02  1.800e+02 -1.830e+02\n",
      "  1.850e+02  1.850e+02  1.880e+02 -1.890e+02  1.910e+02  1.920e+02\n",
      " -1.980e+02 -2.000e+02 -2.000e+02  2.000e+02  2.010e+02  2.050e+02\n",
      "  2.120e+02  2.180e+02 -2.240e+02 -2.290e+02  2.290e+02  2.350e+02\n",
      " -2.430e+02  2.430e+02  2.430e+02  2.500e+02  2.550e+02  2.590e+02\n",
      "  2.620e+02  2.720e+02 -2.730e+02  2.740e+02  2.760e+02  2.790e+02\n",
      " -2.800e+02 -2.800e+02  2.810e+02  2.820e+02  2.840e+02  2.840e+02\n",
      " -2.880e+02  2.890e+02  2.910e+02  2.920e+02  2.940e+02  2.960e+02\n",
      "  3.000e+02  3.120e+02 -3.230e+02 -3.250e+02 -3.370e+02  3.410e+02\n",
      "  3.420e+02 -3.420e+02 -3.440e+02  3.440e+02  3.480e+02  3.530e+02\n",
      "  3.540e+02 -3.560e+02  3.560e+02 -3.560e+02 -3.580e+02  3.580e+02\n",
      " -3.650e+02  3.660e+02 -3.670e+02 -3.680e+02 -3.710e+02 -3.740e+02\n",
      " -3.740e+02 -3.750e+02 -3.750e+02  3.760e+02 -3.770e+02  3.780e+02\n",
      " -3.780e+02 -3.790e+02 -3.830e+02 -3.840e+02 -3.850e+02 -3.880e+02\n",
      "  3.890e+02 -3.890e+02 -3.890e+02 -3.920e+02 -3.940e+02 -3.960e+02\n",
      "  3.980e+02  4.010e+02  4.060e+02  4.060e+02 -4.080e+02 -4.110e+02\n",
      " -4.110e+02 -4.130e+02 -4.160e+02 -4.190e+02  4.220e+02 -4.270e+02\n",
      " -4.280e+02  4.280e+02 -4.340e+02  4.390e+02 -4.400e+02 -4.490e+02\n",
      " -4.630e+02  4.660e+02 -4.680e+02  4.760e+02 -4.760e+02  4.770e+02\n",
      " -4.790e+02 -4.850e+02 -4.850e+02 -4.860e+02  4.910e+02  4.910e+02\n",
      " -4.910e+02 -4.940e+02  4.960e+02 -5.000e+02  5.070e+02 -5.110e+02\n",
      "  5.130e+02 -5.230e+02 -5.230e+02  5.260e+02  5.330e+02 -5.430e+02\n",
      "  5.490e+02  5.520e+02  5.540e+02  5.580e+02 -5.590e+02  5.600e+02\n",
      "  5.620e+02 -5.640e+02 -5.660e+02  5.700e+02 -5.720e+02 -5.730e+02\n",
      " -5.790e+02 -5.800e+02 -5.820e+02  5.880e+02 -5.930e+02 -5.940e+02\n",
      " -5.940e+02 -6.000e+02 -6.070e+02  6.070e+02 -6.130e+02 -6.150e+02\n",
      " -6.160e+02  6.180e+02 -6.210e+02 -6.250e+02 -6.280e+02  6.330e+02\n",
      "  6.350e+02 -6.360e+02 -6.410e+02 -6.430e+02 -6.470e+02 -6.500e+02\n",
      "  6.520e+02 -6.640e+02  6.690e+02 -6.750e+02 -6.780e+02 -6.780e+02\n",
      " -6.790e+02 -6.900e+02 -6.920e+02 -6.940e+02 -6.990e+02 -7.250e+02\n",
      " -7.360e+02 -7.380e+02 -7.390e+02 -7.420e+02 -7.520e+02  7.620e+02\n",
      "  7.660e+02  7.820e+02 -7.850e+02  7.920e+02  8.010e+02  8.050e+02\n",
      " -8.120e+02 -8.130e+02 -8.380e+02 -8.380e+02 -8.560e+02 -8.620e+02\n",
      " -8.640e+02  8.690e+02  8.740e+02 -8.810e+02  8.810e+02 -8.950e+02\n",
      " -8.990e+02 -9.000e+02 -9.120e+02 -9.280e+02  9.400e+02 -9.400e+02\n",
      " -9.420e+02 -9.460e+02 -9.490e+02 -9.510e+02 -9.720e+02 -9.910e+02\n",
      " -9.970e+02 -1.000e+03 -1.010e+03 -1.016e+03 -1.023e+03 -1.038e+03\n",
      "  1.043e+03 -1.055e+03 -1.072e+03 -1.083e+03 -1.090e+03  1.095e+03\n",
      " -1.106e+03 -1.108e+03 -1.124e+03 -1.132e+03 -1.133e+03 -1.138e+03\n",
      " -1.145e+03  1.153e+03 -1.184e+03 -1.190e+03 -1.210e+03 -1.236e+03\n",
      "  1.294e+03 -1.297e+03 -1.319e+03 -1.328e+03 -1.367e+03 -1.389e+03\n",
      "  1.407e+03 -1.431e+03 -1.484e+03 -1.588e+03 -1.645e+03 -1.646e+03\n",
      "  1.686e+03 -1.690e+03  1.747e+03 -1.765e+03  1.811e+03 -1.851e+03\n",
      " -1.862e+03 -1.935e+03 -2.032e+03  2.100e+03 -2.171e+03  2.197e+03\n",
      " -2.351e+03 -3.196e+03 -3.519e+03 -3.540e+03 -3.720e+03]\n",
      "durations 14.0 2267.0\n",
      "Concordance Index 0.49336283185840707\n",
      "Integrated Brier Score: 0.33808183663286206\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m278.6340\u001b[0m  0.0020\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1           nan  0.0019\n",
      "      2      278.6340  0.0016\n",
      "      2           nan  0.0016\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      3      278.6340  0.0020\n",
      "      4      278.6340  0.0017\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m67.6404\u001b[0m  0.0020\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      5      278.6340  0.0015\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2       67.6404  0.0016\n",
      "      6      278.6340  0.0015\n",
      "      3       67.6404  0.0015\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m105.9626\u001b[0m  0.0017\n",
      "      7      278.6340  0.0014\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      3           nan  0.0029\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      4       67.6404  0.0015\n",
      "      2      105.9626  0.0016\n",
      "      8      278.6340  0.0016\n",
      "      4           nan  0.0018\n",
      "      5       67.6404  0.0016\n",
      "      3      105.9626  0.0015\n",
      "      9      278.6340  0.0015\n",
      "      5           nan  0.0017\n",
      "      6       67.6404  0.0017\n",
      "      4      105.9626  0.0016\n",
      "     10      278.6340  0.0015\n",
      "Restoring best model from epoch 1.\n",
      "      6           nan  0.0015\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m90.6806\u001b[0m  0.0034\n",
      "      7       67.6404  0.0016      5      105.9626  0.0015\n",
      "\n",
      "      7           nan  0.0015\n",
      "      8       67.6404  0.0015\n",
      "      6      105.9626  0.0014\n",
      "      8           nan  0.0015\n",
      "      2       90.6806  0.0032\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      9       67.6404  0.0015\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      7      105.9626  0.0017\n",
      "      3       90.6806  0.0015\n",
      "     10       67.6404  0.0015\n",
      "Restoring best model from epoch 1.\n",
      "      8      105.9626  0.0015\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m110.5265\u001b[0m  0.0018\n",
      "      4       90.6806  0.0015\n",
      "      9      105.9626  0.0015\n",
      "      5       90.6806  0.0015\n",
      "      2      110.5265  0.0016\n",
      "     10      105.9626  0.0015\n",
      "Restoring best model from epoch 1.\n",
      "      6       90.6806  0.0015\n",
      "      3      110.5265  0.0016\n",
      "      7       90.6806  0.0014\n",
      "      4      110.5265  0.0015\n",
      "      8       90.6806  0.0014\n",
      "      5      110.5265  0.0015\n",
      "      9       90.6806  0.0014\n",
      "      6      110.5265  0.0015\n",
      "     10       90.6806  0.0014\n",
      "Restoring best model from epoch 1.\n",
      "      9           nan  0.0146\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "      7      110.5265  0.0036\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m407.3492\u001b[0m  0.0018\n",
      "      8      110.5265  0.0043\n",
      "      2      407.3492  0.0016\n",
      "      3      407.3492  0.0015\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      9      110.5265  0.0026\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      4      407.3492  0.0015\n",
      "     10      110.5265  0.0016\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Restoring best model from epoch 1.\n",
      "      5      407.3492  0.0015\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m124.5032\u001b[0m  0.0018\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m406.0663\u001b[0m  0.0017\n",
      "      6      407.3492  0.0015\n",
      "      2      406.0663  0.0015\n",
      "      7      407.3492  0.0015\n",
      "      3      406.0663  0.0015\n",
      "      2      124.5032  0.0033\n",
      "      8      407.3492  0.0015\n",
      "      4      406.0663  0.0015\n",
      "      9      407.3492  0.0015\n",
      "      3      124.5032  0.0028\n",
      "      5      406.0663  0.0014\n",
      "     10      407.3492  0.0015\n",
      "Restoring best model from epoch 1.\n",
      "      6      406.0663  0.0014\n",
      "      7      406.0663  0.0016\n",
      "      4      124.5032  0.0030\n",
      "      8      406.0663  0.0016\n",
      "      9      406.0663  0.0015\n",
      "     10      406.0663  0.0014\n",
      "Restoring best model from epoch 1.\n",
      "      5      124.5032  0.0047\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      6      124.5032  0.0053\n",
      "      7      124.5032  0.0025\n",
      "      8      124.5032  0.0015\n",
      "      9      124.5032  0.0015\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m422.5891\u001b[0m  0.0053\n",
      "     10      124.5032  0.0015\n",
      "Restoring best model from epoch 1.\n",
      "      2      422.5891  0.0019\n",
      "      3      422.5891  0.0016\n",
      "      4      422.5891  0.0015\n",
      "      5      422.5891  0.0015\n",
      "      6      422.5891  0.0015\n",
      "      7      422.5891  0.0015\n",
      "      8      422.5891  0.0015\n",
      "      9      422.5891  0.0040\n",
      "     10      422.5891  0.0034\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m142.6711\u001b[0m  0.0021\n",
      "      2      142.6711  0.0024\n",
      "      3      142.6711  0.0026\n",
      "      4      142.6711  0.0023\n",
      "      5      142.6711  0.0023\n",
      "      6      142.6711  0.0023\n",
      "      7      142.6711  0.0026\n",
      "      8      142.6711  0.0026\n",
      "      9      142.6711  0.0020\n",
      "     10      142.6711  0.0038\n",
      "Restoring best model from epoch 1.\n",
      "y_train breslow final [ 3.000e+00 -7.000e+00 -8.000e+00 -1.400e+01 -1.600e+01 -1.700e+01\n",
      " -2.000e+01 -2.100e+01 -2.100e+01 -2.300e+01  3.000e+01 -3.000e+01\n",
      "  3.100e+01 -3.100e+01 -3.400e+01 -3.500e+01  4.500e+01 -4.600e+01\n",
      "  5.200e+01  6.100e+01 -6.400e+01  6.700e+01 -7.700e+01  8.100e+01\n",
      "  8.100e+01  8.200e+01 -9.200e+01 -9.900e+01  1.030e+02  1.050e+02\n",
      "  1.050e+02  1.060e+02 -1.130e+02  1.220e+02  1.240e+02  1.240e+02\n",
      " -1.310e+02  1.320e+02  1.380e+02  1.410e+02  1.560e+02 -1.640e+02\n",
      "  1.660e+02  1.680e+02 -1.700e+02  1.740e+02 -1.760e+02  1.800e+02\n",
      "  1.800e+02  1.850e+02  1.850e+02  1.880e+02 -1.890e+02  1.910e+02\n",
      " -1.980e+02 -2.000e+02 -2.000e+02  2.000e+02  2.000e+02  2.010e+02\n",
      "  2.120e+02  2.150e+02 -2.240e+02 -2.250e+02  2.290e+02  2.350e+02\n",
      " -2.430e+02  2.430e+02 -2.450e+02  2.450e+02  2.550e+02  2.590e+02\n",
      "  2.620e+02  2.720e+02  2.730e+02  2.740e+02  2.750e+02  2.790e+02\n",
      " -2.800e+02 -2.800e+02  2.820e+02  2.840e+02  2.840e+02 -2.870e+02\n",
      " -2.880e+02  2.890e+02  2.910e+02  2.920e+02  2.940e+02  3.000e+02\n",
      "  3.120e+02 -3.250e+02 -3.350e+02  3.360e+02 -3.370e+02  3.410e+02\n",
      "  3.420e+02 -3.420e+02 -3.440e+02  3.440e+02 -3.460e+02  3.480e+02\n",
      "  3.480e+02  3.540e+02 -3.560e+02  3.560e+02 -3.580e+02  3.580e+02\n",
      "  3.590e+02  3.660e+02 -3.670e+02  3.700e+02 -3.710e+02 -3.740e+02\n",
      " -3.750e+02 -3.750e+02  3.770e+02 -3.770e+02  3.780e+02 -3.780e+02\n",
      " -3.780e+02 -3.790e+02 -3.810e+02 -3.830e+02 -3.830e+02 -3.840e+02\n",
      " -3.850e+02  3.890e+02 -3.890e+02 -3.900e+02 -3.920e+02 -3.940e+02\n",
      " -3.960e+02  3.960e+02  3.980e+02 -4.000e+02  4.010e+02  4.030e+02\n",
      "  4.060e+02  4.060e+02 -4.080e+02 -4.130e+02 -4.160e+02 -4.190e+02\n",
      "  4.220e+02  4.220e+02  4.260e+02 -4.270e+02 -4.270e+02 -4.280e+02\n",
      " -4.310e+02 -4.340e+02  4.460e+02 -4.490e+02 -4.500e+02  4.570e+02\n",
      " -4.630e+02  4.660e+02 -4.680e+02  4.740e+02 -4.760e+02  4.770e+02\n",
      " -4.790e+02 -4.850e+02 -4.850e+02 -4.860e+02 -4.910e+02  4.960e+02\n",
      "  5.070e+02 -5.110e+02 -5.190e+02 -5.210e+02 -5.230e+02 -5.250e+02\n",
      "  5.260e+02  5.330e+02  5.430e+02 -5.430e+02  5.490e+02  5.520e+02\n",
      "  5.540e+02  5.540e+02  5.600e+02  5.620e+02 -5.640e+02 -5.660e+02\n",
      "  5.700e+02 -5.720e+02 -5.770e+02 -5.790e+02 -5.800e+02 -5.820e+02\n",
      "  5.880e+02 -5.930e+02 -5.940e+02 -5.940e+02 -5.950e+02 -6.000e+02\n",
      " -6.070e+02  6.070e+02 -6.130e+02 -6.150e+02 -6.160e+02  6.180e+02\n",
      " -6.250e+02 -6.280e+02  6.350e+02 -6.360e+02  6.400e+02 -6.410e+02\n",
      " -6.430e+02 -6.440e+02 -6.470e+02 -6.500e+02  6.520e+02  6.610e+02\n",
      " -6.640e+02 -6.660e+02  6.690e+02  6.750e+02 -6.780e+02 -6.780e+02\n",
      " -6.900e+02 -6.920e+02 -6.940e+02  7.120e+02 -7.240e+02 -7.250e+02\n",
      " -7.360e+02 -7.380e+02 -7.390e+02 -7.420e+02 -7.520e+02 -7.540e+02\n",
      "  7.660e+02  7.790e+02  7.820e+02 -7.850e+02  7.920e+02  7.940e+02\n",
      "  8.010e+02  8.050e+02 -8.120e+02 -8.130e+02 -8.190e+02 -8.250e+02\n",
      "  8.320e+02 -8.560e+02  8.690e+02  8.740e+02 -8.810e+02  8.810e+02\n",
      " -8.820e+02 -8.950e+02 -8.990e+02 -9.000e+02 -9.120e+02 -9.280e+02\n",
      "  9.400e+02 -9.400e+02 -9.420e+02 -9.420e+02 -9.460e+02 -9.510e+02\n",
      " -9.720e+02 -9.810e+02 -9.890e+02 -9.910e+02 -9.970e+02 -1.000e+03\n",
      " -1.002e+03 -1.010e+03 -1.016e+03 -1.038e+03 -1.083e+03 -1.090e+03\n",
      " -1.100e+03 -1.106e+03 -1.108e+03 -1.132e+03 -1.138e+03  1.153e+03\n",
      " -1.160e+03 -1.184e+03 -1.190e+03 -1.200e+03 -1.210e+03 -1.223e+03\n",
      "  1.294e+03 -1.297e+03 -1.319e+03 -1.328e+03 -1.389e+03  1.407e+03\n",
      " -1.431e+03 -1.588e+03 -1.645e+03 -1.646e+03  1.747e+03  1.811e+03\n",
      " -1.851e+03 -1.862e+03 -1.918e+03 -1.964e+03 -2.032e+03 -2.171e+03\n",
      "  2.197e+03 -2.267e+03 -2.351e+03 -3.519e+03 -3.540e+03]\n",
      "Concordance Index 0.5113797093995114\n",
      "Integrated Brier Score: 0.49145884279410046\n",
      "y_train breslow final [ 3.000e+00 -7.000e+00 -8.000e+00 -1.400e+01 -1.600e+01 -1.700e+01\n",
      " -2.000e+01 -2.100e+01 -2.100e+01 -2.300e+01  3.000e+01 -3.000e+01\n",
      "  3.100e+01 -3.100e+01 -3.400e+01 -3.500e+01  4.500e+01 -4.600e+01\n",
      "  5.200e+01  6.100e+01 -6.400e+01  6.700e+01 -7.700e+01  8.100e+01\n",
      "  8.100e+01  8.200e+01 -9.200e+01 -9.900e+01  1.030e+02  1.050e+02\n",
      "  1.050e+02  1.060e+02 -1.130e+02  1.220e+02  1.240e+02  1.240e+02\n",
      " -1.310e+02  1.320e+02  1.380e+02  1.410e+02  1.560e+02 -1.640e+02\n",
      "  1.660e+02  1.680e+02 -1.700e+02  1.740e+02 -1.760e+02  1.800e+02\n",
      "  1.800e+02  1.850e+02  1.850e+02  1.880e+02 -1.890e+02  1.910e+02\n",
      " -1.980e+02 -2.000e+02 -2.000e+02  2.000e+02  2.000e+02  2.010e+02\n",
      "  2.120e+02  2.150e+02 -2.240e+02 -2.250e+02  2.290e+02  2.350e+02\n",
      " -2.430e+02  2.430e+02 -2.450e+02  2.450e+02  2.550e+02  2.590e+02\n",
      "  2.620e+02  2.720e+02  2.730e+02  2.740e+02  2.750e+02  2.790e+02\n",
      " -2.800e+02 -2.800e+02  2.820e+02  2.840e+02  2.840e+02 -2.870e+02\n",
      " -2.880e+02  2.890e+02  2.910e+02  2.920e+02  2.940e+02  3.000e+02\n",
      "  3.120e+02 -3.250e+02 -3.350e+02  3.360e+02 -3.370e+02  3.410e+02\n",
      "  3.420e+02 -3.420e+02 -3.440e+02  3.440e+02 -3.460e+02  3.480e+02\n",
      "  3.480e+02  3.540e+02 -3.560e+02  3.560e+02 -3.580e+02  3.580e+02\n",
      "  3.590e+02  3.660e+02 -3.670e+02  3.700e+02 -3.710e+02 -3.740e+02\n",
      " -3.750e+02 -3.750e+02  3.770e+02 -3.770e+02  3.780e+02 -3.780e+02\n",
      " -3.780e+02 -3.790e+02 -3.810e+02 -3.830e+02 -3.830e+02 -3.840e+02\n",
      " -3.850e+02  3.890e+02 -3.890e+02 -3.900e+02 -3.920e+02 -3.940e+02\n",
      " -3.960e+02  3.960e+02  3.980e+02 -4.000e+02  4.010e+02  4.030e+02\n",
      "  4.060e+02  4.060e+02 -4.080e+02 -4.130e+02 -4.160e+02 -4.190e+02\n",
      "  4.220e+02  4.220e+02  4.260e+02 -4.270e+02 -4.270e+02 -4.280e+02\n",
      " -4.310e+02 -4.340e+02  4.460e+02 -4.490e+02 -4.500e+02  4.570e+02\n",
      " -4.630e+02  4.660e+02 -4.680e+02  4.740e+02 -4.760e+02  4.770e+02\n",
      " -4.790e+02 -4.850e+02 -4.850e+02 -4.860e+02 -4.910e+02  4.960e+02\n",
      "  5.070e+02 -5.110e+02 -5.190e+02 -5.210e+02 -5.230e+02 -5.250e+02\n",
      "  5.260e+02  5.330e+02  5.430e+02 -5.430e+02  5.490e+02  5.520e+02\n",
      "  5.540e+02  5.540e+02  5.600e+02  5.620e+02 -5.640e+02 -5.660e+02\n",
      "  5.700e+02 -5.720e+02 -5.770e+02 -5.790e+02 -5.800e+02 -5.820e+02\n",
      "  5.880e+02 -5.930e+02 -5.940e+02 -5.940e+02 -5.950e+02 -6.000e+02\n",
      " -6.070e+02  6.070e+02 -6.130e+02 -6.150e+02 -6.160e+02  6.180e+02\n",
      " -6.250e+02 -6.280e+02  6.350e+02 -6.360e+02  6.400e+02 -6.410e+02\n",
      " -6.430e+02 -6.440e+02 -6.470e+02 -6.500e+02  6.520e+02  6.610e+02\n",
      " -6.640e+02 -6.660e+02  6.690e+02  6.750e+02 -6.780e+02 -6.780e+02\n",
      " -6.900e+02 -6.920e+02 -6.940e+02  7.120e+02 -7.240e+02 -7.250e+02\n",
      " -7.360e+02 -7.380e+02 -7.390e+02 -7.420e+02 -7.520e+02 -7.540e+02\n",
      "  7.660e+02  7.790e+02  7.820e+02 -7.850e+02  7.920e+02  7.940e+02\n",
      "  8.010e+02  8.050e+02 -8.120e+02 -8.130e+02 -8.190e+02 -8.250e+02\n",
      "  8.320e+02 -8.560e+02  8.690e+02  8.740e+02 -8.810e+02  8.810e+02\n",
      " -8.820e+02 -8.950e+02 -8.990e+02 -9.000e+02 -9.120e+02 -9.280e+02\n",
      "  9.400e+02 -9.400e+02 -9.420e+02 -9.420e+02 -9.460e+02 -9.510e+02\n",
      " -9.720e+02 -9.810e+02 -9.890e+02 -9.910e+02 -9.970e+02 -1.000e+03\n",
      " -1.002e+03 -1.010e+03 -1.016e+03 -1.038e+03 -1.083e+03 -1.090e+03\n",
      " -1.100e+03 -1.106e+03 -1.108e+03 -1.132e+03 -1.138e+03  1.153e+03\n",
      " -1.160e+03 -1.184e+03 -1.190e+03 -1.200e+03 -1.210e+03 -1.223e+03\n",
      "  1.294e+03 -1.297e+03 -1.319e+03 -1.328e+03 -1.389e+03  1.407e+03\n",
      " -1.431e+03 -1.588e+03 -1.645e+03 -1.646e+03  1.747e+03  1.811e+03\n",
      " -1.851e+03 -1.862e+03 -1.918e+03 -1.964e+03 -2.032e+03 -2.171e+03\n",
      "  2.197e+03 -2.267e+03 -2.351e+03 -3.519e+03 -3.540e+03]\n",
      "durations 8.0 3720.0\n",
      "Concordance Index 0.524092409240924\n",
      "Integrated Brier Score: 0.4411507675105061\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m70.7648\u001b[0m  0.0018\n",
      "      2       70.7648  0.0015\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      3       70.7648  0.0015\n",
      "      4       70.7648  0.0014\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m263.4180\u001b[0m  0.0018\n",
      "      2      263.4180  0.0016\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      3      263.4180  0.0015\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      4      263.4180  0.0015\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      5       70.7648  0.0077\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m59.4779\u001b[0m  0.0034\n",
      "      5      263.4180  0.0037\n",
      "      6       70.7648  0.0016\n",
      "      2       59.4779  0.0015\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      6      263.4180  0.0016\n",
      "      7       70.7648  0.0015\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      3       59.4779  0.0015\n",
      "      8       70.7648  0.0015\n",
      "      7      263.4180  0.0015\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      4       59.4779  0.0016\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m87.9786\u001b[0m  0.0017\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      9       70.7648  0.0016\n",
      "      8      263.4180  0.0016\n",
      "      5       59.4779  0.0015\n",
      "      2       87.9786  0.0015\n",
      "     10       70.7648  0.0015\n",
      "      9      263.4180  0.0015\n",
      "Restoring best model from epoch 1.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m89.5197\u001b[0m  0.0018\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m90.4784\u001b[0m  0.0017\n",
      "      3       87.9786  0.0014\n",
      "     10      263.4180  0.0015\n",
      "Restoring best model from epoch 1.\n",
      "      6       59.4779  0.0024\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      2       89.5197  0.0016\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      4       87.9786  0.0014\n",
      "      2       90.4784  0.0018\n",
      "      7       59.4779  0.0015\n",
      "      3       89.5197  0.0015\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1           nan  0.0037\n",
      "      3       90.4784  0.0015\n",
      "      8       59.4779  0.0014\n",
      "      5       87.9786  0.0024\n",
      "      4       89.5197  0.0017\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m261.9019\u001b[0m  0.0025\n",
      "      2           nan  0.0017\n",
      "      4       90.4784  0.0015\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      9       59.4779  0.0014\n",
      "      6       87.9786  0.0015\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      5       89.5197  0.0015\n",
      "      2      261.9019  0.0016\n",
      "      3           nan  0.0014\n",
      "      5       90.4784  0.0014\n",
      "     10       59.4779  0.0014\n",
      "Restoring best model from epoch 1.\n",
      "      7       87.9786  0.0015\n",
      "      6       89.5197  0.0014\n",
      "      4           nan  0.0014\n",
      "      3      261.9019  0.0015\n",
      "      6       90.4784  0.0014\n",
      "      8       87.9786  0.0015\n",
      "      7       89.5197  0.0015\n",
      "      5           nan  0.0014\n",
      "      4      261.9019  0.0014\n",
      "      7       90.4784  0.0014\n",
      "      9       87.9786  0.0015\n",
      "      8       89.5197  0.0014\n",
      "      6           nan  0.0014\n",
      "      5      261.9019  0.0014\n",
      "      8       90.4784  0.0014\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m271.5542\u001b[0m  0.0048\n",
      "     10       87.9786  0.0015\n",
      "Restoring best model from epoch 1.\n",
      "      7           nan  0.0014\n",
      "      9       89.5197  0.0014\n",
      "      9       90.4784  0.0017\n",
      "      6      261.9019  0.0020\n",
      "      2      271.5542  0.0016\n",
      "      8           nan  0.0014\n",
      "     10       90.4784  0.0014\n",
      "Restoring best model from epoch 1.\n",
      "     10       89.5197  0.0023\n",
      "Restoring best model from epoch 1.\n",
      "      3      271.5542  0.0015\n",
      "      9           nan  0.0014\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "      4      271.5542  0.0015\n",
      "      5      271.5542  0.0015\n",
      "      6      271.5542  0.0015\n",
      "      7      261.9019  0.0046\n",
      "      8      261.9019  0.0019\n",
      "      7      271.5542  0.0024\n",
      "      9      261.9019  0.0015\n",
      "      8      271.5542  0.0016\n",
      "     10      261.9019  0.0014\n",
      "Restoring best model from epoch 1.\n",
      "      9      271.5542  0.0015\n",
      "     10      271.5542  0.0015\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m395.3599\u001b[0m  0.0049\n",
      "      2      395.3599  0.0028\n",
      "      3      395.3599  0.0016\n",
      "      4      395.3599  0.0015\n",
      "      5      395.3599  0.0015\n",
      "      6      395.3599  0.0014\n",
      "      7      395.3599  0.0014\n",
      "      8      395.3599  0.0020\n",
      "      9      395.3599  0.0015\n",
      "     10      395.3599  0.0015\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m114.0669\u001b[0m  0.0022\n",
      "      2      114.0669  0.0026\n",
      "      3      114.0669  0.0026\n",
      "      4      114.0669  0.0024\n",
      "      5      114.0669  0.0024\n",
      "      6      114.0669  0.0024\n",
      "      7      114.0669  0.0026\n",
      "      8      114.0669  0.0024\n",
      "      9      114.0669  0.0025\n",
      "     10      114.0669  0.0033\n",
      "Restoring best model from epoch 1.\n",
      "y_train breslow final [   -7.     8.    -8.   -14.   -16.   -17.   -20.    22.   -23.    24.\n",
      "   -29.   -30.    31.   -31.   -34.   -46.    52.    57.    61.   -64.\n",
      "    67.    76.   -77.    81.    81.    82.   -92.   103.   106.   113.\n",
      "  -113.   124.   124.  -131.   132.   138.   153.   156.  -156.  -164.\n",
      "   166.  -170.   174.   180.   180.  -183.   185.   188.  -189.   191.\n",
      "   192.  -198.  -198.  -200.  -200.   200.   200.   201.   205.   212.\n",
      "   215.   218.  -224.  -225.  -229.   229.   235.  -243.   243.   243.\n",
      "  -245.   245.   250.   259.   273.  -273.   274.   275.   276.  -280.\n",
      "   281.   282.   284.   284.  -287.   291.   292.   296.   300.   300.\n",
      "   312.  -323.  -335.   336.  -337.   342.  -342.  -344.  -344.   344.\n",
      "  -346.   348.   353.   354.  -356.   356.  -356.  -358.   358.   359.\n",
      "  -365.  -368.   370.  -371.  -374.  -374.  -375.   376.   377.  -377.\n",
      "   378.  -378.  -379.  -381.  -383.  -383.  -383.  -384.  -385.  -388.\n",
      "   389.  -389.  -390.  -392.  -394.   396.   398.  -400.   403.   406.\n",
      "  -408.  -411.  -411.  -413.  -419.   422.   422.   426.  -427.  -427.\n",
      "  -428.   428.  -431.   439.  -440.   446.  -449.  -450.   457.   466.\n",
      "  -468.   474.   476.  -476.  -485.  -485.  -486.   491.   491.  -494.\n",
      "   496.  -500.   507.  -511.   513.  -519.  -521.  -523.  -523.  -525.\n",
      "   526.   533.   543.   549.   552.   554.   554.   558.  -559.   560.\n",
      "   562.  -564.   570.  -572.  -573.  -577.  -580.  -582.  -594.  -595.\n",
      "  -600.  -607.   607.  -613.  -616.  -621.  -628.   633.   640.  -641.\n",
      "  -643.  -644.   661.  -664.  -666.   669.  -675.   675.  -678.  -679.\n",
      "  -690.  -692.  -699.   712.  -724.  -725.  -736.  -738.  -739.  -752.\n",
      "  -754.   762.   766.   779.   782.   792.   794.   801.   805.  -813.\n",
      "  -819.  -825.   832.  -838.  -838.  -862.  -864.   869.  -881.  -882.\n",
      "  -895.  -899.  -912.  -912.  -940.  -942.  -942.  -949.  -972.  -981.\n",
      "  -989.  -991.  -997. -1002. -1016. -1023. -1038.  1043. -1055. -1072.\n",
      " -1083. -1090.  1095. -1100. -1108. -1124. -1133. -1145. -1160. -1200.\n",
      " -1223. -1236.  1294. -1319. -1328. -1367. -1389.  1407. -1431. -1484.\n",
      " -1588. -1645. -1646.  1686. -1690. -1765.  1811. -1851. -1862. -1918.\n",
      " -1935. -1964. -2032.  2100. -2171. -2267. -2351. -3196. -3519. -3540.\n",
      " -3720.]\n",
      "Concordance Index 0.5020824479388015\n",
      "Integrated Brier Score: 0.42427822611005345\n",
      "y_train breslow final [   -7.     8.    -8.   -14.   -16.   -17.   -20.    22.   -23.    24.\n",
      "   -29.   -30.    31.   -31.   -34.   -46.    52.    57.    61.   -64.\n",
      "    67.    76.   -77.    81.    81.    82.   -92.   103.   106.   113.\n",
      "  -113.   124.   124.  -131.   132.   138.   153.   156.  -156.  -164.\n",
      "   166.  -170.   174.   180.   180.  -183.   185.   188.  -189.   191.\n",
      "   192.  -198.  -198.  -200.  -200.   200.   200.   201.   205.   212.\n",
      "   215.   218.  -224.  -225.  -229.   229.   235.  -243.   243.   243.\n",
      "  -245.   245.   250.   259.   273.  -273.   274.   275.   276.  -280.\n",
      "   281.   282.   284.   284.  -287.   291.   292.   296.   300.   300.\n",
      "   312.  -323.  -335.   336.  -337.   342.  -342.  -344.  -344.   344.\n",
      "  -346.   348.   353.   354.  -356.   356.  -356.  -358.   358.   359.\n",
      "  -365.  -368.   370.  -371.  -374.  -374.  -375.   376.   377.  -377.\n",
      "   378.  -378.  -379.  -381.  -383.  -383.  -383.  -384.  -385.  -388.\n",
      "   389.  -389.  -390.  -392.  -394.   396.   398.  -400.   403.   406.\n",
      "  -408.  -411.  -411.  -413.  -419.   422.   422.   426.  -427.  -427.\n",
      "  -428.   428.  -431.   439.  -440.   446.  -449.  -450.   457.   466.\n",
      "  -468.   474.   476.  -476.  -485.  -485.  -486.   491.   491.  -494.\n",
      "   496.  -500.   507.  -511.   513.  -519.  -521.  -523.  -523.  -525.\n",
      "   526.   533.   543.   549.   552.   554.   554.   558.  -559.   560.\n",
      "   562.  -564.   570.  -572.  -573.  -577.  -580.  -582.  -594.  -595.\n",
      "  -600.  -607.   607.  -613.  -616.  -621.  -628.   633.   640.  -641.\n",
      "  -643.  -644.   661.  -664.  -666.   669.  -675.   675.  -678.  -679.\n",
      "  -690.  -692.  -699.   712.  -724.  -725.  -736.  -738.  -739.  -752.\n",
      "  -754.   762.   766.   779.   782.   792.   794.   801.   805.  -813.\n",
      "  -819.  -825.   832.  -838.  -838.  -862.  -864.   869.  -881.  -882.\n",
      "  -895.  -899.  -912.  -912.  -940.  -942.  -942.  -949.  -972.  -981.\n",
      "  -989.  -991.  -997. -1002. -1016. -1023. -1038.  1043. -1055. -1072.\n",
      " -1083. -1090.  1095. -1100. -1108. -1124. -1133. -1145. -1160. -1200.\n",
      " -1223. -1236.  1294. -1319. -1328. -1367. -1389.  1407. -1431. -1484.\n",
      " -1588. -1645. -1646.  1686. -1690. -1765.  1811. -1851. -1862. -1918.\n",
      " -1935. -1964. -2032.  2100. -2171. -2267. -2351. -3196. -3519. -3540.\n",
      " -3720.]\n",
      "durations 3.0 2197.0\n",
      "Concordance Index 0.44214876033057854\n",
      "Integrated Brier Score: 0.34304240093124055\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m80.4718\u001b[0m  0.0018\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m80.0153\u001b[0m  0.0022\n",
      "      2       80.0153  0.0017\n",
      "      2       80.4718  0.0047\n",
      "      3       80.0153  0.0018\n",
      "      4       80.0153  0.0017\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      5       80.0153  0.0016\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      3       80.4718  0.0057\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m335.1528\u001b[0m  0.0017\n",
      "      6       80.0153  0.0034\n",
      "      2      335.1528  0.0016\n",
      "      4       80.4718  0.0038\n",
      "      7       80.0153  0.0015\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      3      335.1528  0.0015\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      5       80.4718  0.0027\n",
      "      4      335.1528  0.0015\n",
      "      8       80.0153  0.0036\n",
      "      6       80.4718  0.0017\n",
      "      5      335.1528  0.0021\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1           nan  0.0018      9       80.0153  0.0023\n",
      "\n",
      "      6      335.1528  0.0017\n",
      "      7       80.4718  0.0020\n",
      "      7      335.1528  0.0016\n",
      "      2           nan  0.0018\n",
      "      8       80.4718  0.0016\n",
      "     10       80.0153  0.0029\n",
      "Restoring best model from epoch 1.\n",
      "      8      335.1528  0.0015\n",
      "      3           nan  0.0016\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      9       80.4718  0.0015\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      9      335.1528  0.0014\n",
      "      4           nan  0.0016\n",
      "     10       80.4718  0.0015\n",
      "Restoring best model from epoch 1.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m243.6594\u001b[0m  0.0017\n",
      "      5           nan  0.0015\n",
      "     10      335.1528  0.0021\n",
      "Restoring best model from epoch 1.\n",
      "      6           nan  0.0015\n",
      "      2      243.6594  0.0015\n",
      "      3      243.6594  0.0015\n",
      "      7           nan  0.0022\n",
      "      4      243.6594  0.0015\n",
      "      8           nan  0.0017\n",
      "      5      243.6594  0.0014\n",
      "      9           nan  0.0015\n",
      "      6      243.6594  0.0020\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "      7      243.6594  0.0016\n",
      "      8      243.6594  0.0014\n",
      "      9      243.6594  0.0014\n",
      "     10      243.6594  0.0014\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m79.1298\u001b[0m  0.0017\n",
      "      2       79.1298  0.0015\n",
      "      3       79.1298  0.0014\n",
      "      4       79.1298  0.0015\n",
      "      5       79.1298  0.0015\n",
      "      6       79.1298  0.0014\n",
      "      7       79.1298  0.0014\n",
      "      8       79.1298  0.0014\n",
      "      9       79.1298  0.0014\n",
      "     10       79.1298  0.0014\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m247.8025\u001b[0m  0.0018\n",
      "      2      247.8025  0.0015\n",
      "      3      247.8025  0.0014\n",
      "      4      247.8025  0.0014\n",
      "      5      247.8025  0.0014\n",
      "      6      247.8025  0.0251\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      7      247.8025  0.0024\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m323.6726\u001b[0m  0.0018\n",
      "      2      323.6726  0.0016\n",
      "      8      247.8025  0.0035\n",
      "      3      323.6726  0.0015\n",
      "      9      247.8025  0.0016\n",
      "      4      323.6726  0.0015\n",
      "     10      247.8025  0.0014\n",
      "Restoring best model from epoch 1.\n",
      "      5      323.6726  0.0014\n",
      "      6      323.6726  0.0014\n",
      "      7      323.6726  0.0014\n",
      "      8      323.6726  0.0015\n",
      "      9      323.6726  0.0015\n",
      "     10      323.6726  0.0035\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m74.7726\u001b[0m  0.0018\n",
      "      2       74.7726  0.0015\n",
      "      3       74.7726  0.0015\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      4       74.7726  0.0015\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      5       74.7726  0.0014\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m88.2669\u001b[0m  0.0017\n",
      "      6       74.7726  0.0014\n",
      "      2       88.2669  0.0015\n",
      "      7       74.7726  0.0015\n",
      "      3       88.2669  0.0015\n",
      "      8       74.7726  0.0019\n",
      "      4       88.2669  0.0015\n",
      "      9       74.7726  0.0016\n",
      "      5       88.2669  0.0014\n",
      "     10       74.7726  0.0015\n",
      "Restoring best model from epoch 1.\n",
      "      6       88.2669  0.0014\n",
      "      7       88.2669  0.0014\n",
      "      8       88.2669  0.0014\n",
      "      9       88.2669  0.0014\n",
      "     10       88.2669  0.0014\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m130.0971\u001b[0m  0.0022\n",
      "      2      130.0971  0.0028\n",
      "      3      130.0971  0.0025\n",
      "      4      130.0971  0.0024\n",
      "      5      130.0971  0.0026\n",
      "      6      130.0971  0.0025\n",
      "      7      130.0971  0.0025\n",
      "      8      130.0971  0.0026\n",
      "      9      130.0971  0.0026\n",
      "     10      130.0971  0.0039\n",
      "Restoring best model from epoch 1.\n",
      "y_train breslow final [ 3.000e+00 -7.000e+00  8.000e+00 -1.400e+01 -1.600e+01 -1.700e+01\n",
      " -2.000e+01 -2.100e+01 -2.100e+01  2.200e+01  2.400e+01 -2.900e+01\n",
      "  3.000e+01 -3.000e+01  3.100e+01 -3.400e+01 -3.500e+01  4.500e+01\n",
      " -4.600e+01  5.700e+01  6.700e+01  7.600e+01 -7.700e+01  8.100e+01\n",
      "  8.200e+01 -9.200e+01 -9.900e+01  1.030e+02  1.050e+02  1.050e+02\n",
      "  1.130e+02  1.220e+02  1.240e+02  1.240e+02 -1.310e+02  1.320e+02\n",
      "  1.380e+02  1.410e+02  1.530e+02  1.560e+02 -1.560e+02 -1.640e+02\n",
      "  1.660e+02  1.680e+02  1.740e+02 -1.760e+02  1.800e+02 -1.830e+02\n",
      "  1.850e+02 -1.890e+02  1.920e+02 -1.980e+02 -1.980e+02 -2.000e+02\n",
      " -2.000e+02  2.000e+02  2.000e+02  2.050e+02  2.150e+02  2.180e+02\n",
      " -2.240e+02 -2.250e+02 -2.290e+02  2.290e+02 -2.430e+02  2.430e+02\n",
      "  2.430e+02 -2.450e+02  2.450e+02  2.500e+02  2.550e+02  2.590e+02\n",
      "  2.620e+02  2.720e+02  2.730e+02 -2.730e+02  2.740e+02  2.750e+02\n",
      "  2.760e+02  2.790e+02 -2.800e+02 -2.800e+02  2.810e+02  2.820e+02\n",
      "  2.840e+02 -2.870e+02 -2.880e+02  2.890e+02  2.920e+02  2.940e+02\n",
      "  2.960e+02  3.000e+02  3.000e+02  3.120e+02 -3.230e+02 -3.250e+02\n",
      " -3.350e+02  3.360e+02 -3.370e+02  3.410e+02  3.420e+02 -3.420e+02\n",
      " -3.440e+02 -3.440e+02  3.440e+02 -3.460e+02  3.480e+02  3.480e+02\n",
      "  3.530e+02 -3.560e+02 -3.560e+02  3.590e+02 -3.650e+02  3.660e+02\n",
      " -3.670e+02 -3.680e+02  3.700e+02 -3.740e+02 -3.740e+02 -3.750e+02\n",
      " -3.750e+02  3.760e+02  3.770e+02 -3.770e+02 -3.780e+02 -3.780e+02\n",
      " -3.810e+02 -3.830e+02 -3.830e+02 -3.830e+02 -3.840e+02 -3.850e+02\n",
      " -3.880e+02 -3.890e+02 -3.890e+02 -3.900e+02 -3.960e+02  3.960e+02\n",
      " -4.000e+02  4.010e+02  4.030e+02  4.060e+02  4.060e+02 -4.110e+02\n",
      " -4.110e+02 -4.160e+02  4.220e+02  4.260e+02 -4.270e+02 -4.270e+02\n",
      "  4.280e+02 -4.310e+02 -4.340e+02  4.390e+02 -4.400e+02  4.460e+02\n",
      " -4.490e+02 -4.500e+02  4.570e+02 -4.630e+02  4.660e+02 -4.680e+02\n",
      "  4.740e+02  4.760e+02 -4.760e+02  4.770e+02 -4.790e+02 -4.850e+02\n",
      " -4.850e+02  4.910e+02  4.910e+02 -4.910e+02 -4.940e+02 -5.000e+02\n",
      " -5.110e+02  5.130e+02 -5.190e+02 -5.210e+02 -5.230e+02 -5.230e+02\n",
      " -5.250e+02  5.260e+02  5.330e+02  5.430e+02 -5.430e+02  5.520e+02\n",
      "  5.540e+02  5.540e+02  5.580e+02 -5.590e+02  5.620e+02 -5.640e+02\n",
      " -5.660e+02 -5.720e+02 -5.730e+02 -5.770e+02 -5.790e+02  5.880e+02\n",
      " -5.930e+02 -5.940e+02 -5.940e+02 -5.950e+02  6.070e+02 -6.150e+02\n",
      " -6.160e+02  6.180e+02 -6.210e+02 -6.250e+02  6.330e+02  6.350e+02\n",
      " -6.360e+02  6.400e+02 -6.410e+02 -6.430e+02 -6.440e+02 -6.470e+02\n",
      " -6.500e+02  6.520e+02  6.610e+02 -6.660e+02 -6.750e+02  6.750e+02\n",
      " -6.780e+02 -6.780e+02 -6.790e+02 -6.900e+02 -6.920e+02 -6.940e+02\n",
      " -6.990e+02  7.120e+02 -7.240e+02 -7.420e+02 -7.540e+02  7.620e+02\n",
      "  7.660e+02  7.790e+02 -7.850e+02  7.940e+02  8.050e+02 -8.120e+02\n",
      " -8.190e+02 -8.250e+02  8.320e+02 -8.380e+02 -8.380e+02 -8.560e+02\n",
      " -8.620e+02 -8.640e+02  8.740e+02  8.810e+02 -8.820e+02 -8.950e+02\n",
      " -8.990e+02 -9.000e+02 -9.120e+02 -9.120e+02 -9.280e+02  9.400e+02\n",
      " -9.420e+02 -9.460e+02 -9.490e+02 -9.510e+02 -9.810e+02 -9.890e+02\n",
      " -9.970e+02 -1.000e+03 -1.002e+03 -1.010e+03 -1.023e+03  1.043e+03\n",
      " -1.055e+03 -1.072e+03 -1.083e+03 -1.090e+03  1.095e+03 -1.100e+03\n",
      " -1.106e+03 -1.124e+03 -1.132e+03 -1.133e+03 -1.138e+03 -1.145e+03\n",
      "  1.153e+03 -1.160e+03 -1.184e+03 -1.190e+03 -1.200e+03 -1.210e+03\n",
      " -1.223e+03 -1.236e+03 -1.297e+03 -1.367e+03 -1.389e+03  1.407e+03\n",
      " -1.484e+03 -1.646e+03  1.686e+03 -1.690e+03  1.747e+03 -1.765e+03\n",
      "  1.811e+03 -1.918e+03 -1.935e+03 -1.964e+03  2.100e+03 -2.171e+03\n",
      "  2.197e+03 -2.267e+03 -2.351e+03 -3.196e+03 -3.540e+03 -3.720e+03]\n",
      "Concordance Index 0.49982908904460777\n",
      "Integrated Brier Score: 0.3537206480491295\n",
      "y_train breslow final [ 3.000e+00 -7.000e+00  8.000e+00 -1.400e+01 -1.600e+01 -1.700e+01\n",
      " -2.000e+01 -2.100e+01 -2.100e+01  2.200e+01  2.400e+01 -2.900e+01\n",
      "  3.000e+01 -3.000e+01  3.100e+01 -3.400e+01 -3.500e+01  4.500e+01\n",
      " -4.600e+01  5.700e+01  6.700e+01  7.600e+01 -7.700e+01  8.100e+01\n",
      "  8.200e+01 -9.200e+01 -9.900e+01  1.030e+02  1.050e+02  1.050e+02\n",
      "  1.130e+02  1.220e+02  1.240e+02  1.240e+02 -1.310e+02  1.320e+02\n",
      "  1.380e+02  1.410e+02  1.530e+02  1.560e+02 -1.560e+02 -1.640e+02\n",
      "  1.660e+02  1.680e+02  1.740e+02 -1.760e+02  1.800e+02 -1.830e+02\n",
      "  1.850e+02 -1.890e+02  1.920e+02 -1.980e+02 -1.980e+02 -2.000e+02\n",
      " -2.000e+02  2.000e+02  2.000e+02  2.050e+02  2.150e+02  2.180e+02\n",
      " -2.240e+02 -2.250e+02 -2.290e+02  2.290e+02 -2.430e+02  2.430e+02\n",
      "  2.430e+02 -2.450e+02  2.450e+02  2.500e+02  2.550e+02  2.590e+02\n",
      "  2.620e+02  2.720e+02  2.730e+02 -2.730e+02  2.740e+02  2.750e+02\n",
      "  2.760e+02  2.790e+02 -2.800e+02 -2.800e+02  2.810e+02  2.820e+02\n",
      "  2.840e+02 -2.870e+02 -2.880e+02  2.890e+02  2.920e+02  2.940e+02\n",
      "  2.960e+02  3.000e+02  3.000e+02  3.120e+02 -3.230e+02 -3.250e+02\n",
      " -3.350e+02  3.360e+02 -3.370e+02  3.410e+02  3.420e+02 -3.420e+02\n",
      " -3.440e+02 -3.440e+02  3.440e+02 -3.460e+02  3.480e+02  3.480e+02\n",
      "  3.530e+02 -3.560e+02 -3.560e+02  3.590e+02 -3.650e+02  3.660e+02\n",
      " -3.670e+02 -3.680e+02  3.700e+02 -3.740e+02 -3.740e+02 -3.750e+02\n",
      " -3.750e+02  3.760e+02  3.770e+02 -3.770e+02 -3.780e+02 -3.780e+02\n",
      " -3.810e+02 -3.830e+02 -3.830e+02 -3.830e+02 -3.840e+02 -3.850e+02\n",
      " -3.880e+02 -3.890e+02 -3.890e+02 -3.900e+02 -3.960e+02  3.960e+02\n",
      " -4.000e+02  4.010e+02  4.030e+02  4.060e+02  4.060e+02 -4.110e+02\n",
      " -4.110e+02 -4.160e+02  4.220e+02  4.260e+02 -4.270e+02 -4.270e+02\n",
      "  4.280e+02 -4.310e+02 -4.340e+02  4.390e+02 -4.400e+02  4.460e+02\n",
      " -4.490e+02 -4.500e+02  4.570e+02 -4.630e+02  4.660e+02 -4.680e+02\n",
      "  4.740e+02  4.760e+02 -4.760e+02  4.770e+02 -4.790e+02 -4.850e+02\n",
      " -4.850e+02  4.910e+02  4.910e+02 -4.910e+02 -4.940e+02 -5.000e+02\n",
      " -5.110e+02  5.130e+02 -5.190e+02 -5.210e+02 -5.230e+02 -5.230e+02\n",
      " -5.250e+02  5.260e+02  5.330e+02  5.430e+02 -5.430e+02  5.520e+02\n",
      "  5.540e+02  5.540e+02  5.580e+02 -5.590e+02  5.620e+02 -5.640e+02\n",
      " -5.660e+02 -5.720e+02 -5.730e+02 -5.770e+02 -5.790e+02  5.880e+02\n",
      " -5.930e+02 -5.940e+02 -5.940e+02 -5.950e+02  6.070e+02 -6.150e+02\n",
      " -6.160e+02  6.180e+02 -6.210e+02 -6.250e+02  6.330e+02  6.350e+02\n",
      " -6.360e+02  6.400e+02 -6.410e+02 -6.430e+02 -6.440e+02 -6.470e+02\n",
      " -6.500e+02  6.520e+02  6.610e+02 -6.660e+02 -6.750e+02  6.750e+02\n",
      " -6.780e+02 -6.780e+02 -6.790e+02 -6.900e+02 -6.920e+02 -6.940e+02\n",
      " -6.990e+02  7.120e+02 -7.240e+02 -7.420e+02 -7.540e+02  7.620e+02\n",
      "  7.660e+02  7.790e+02 -7.850e+02  7.940e+02  8.050e+02 -8.120e+02\n",
      " -8.190e+02 -8.250e+02  8.320e+02 -8.380e+02 -8.380e+02 -8.560e+02\n",
      " -8.620e+02 -8.640e+02  8.740e+02  8.810e+02 -8.820e+02 -8.950e+02\n",
      " -8.990e+02 -9.000e+02 -9.120e+02 -9.120e+02 -9.280e+02  9.400e+02\n",
      " -9.420e+02 -9.460e+02 -9.490e+02 -9.510e+02 -9.810e+02 -9.890e+02\n",
      " -9.970e+02 -1.000e+03 -1.002e+03 -1.010e+03 -1.023e+03  1.043e+03\n",
      " -1.055e+03 -1.072e+03 -1.083e+03 -1.090e+03  1.095e+03 -1.100e+03\n",
      " -1.106e+03 -1.124e+03 -1.132e+03 -1.133e+03 -1.138e+03 -1.145e+03\n",
      "  1.153e+03 -1.160e+03 -1.184e+03 -1.190e+03 -1.200e+03 -1.210e+03\n",
      " -1.223e+03 -1.236e+03 -1.297e+03 -1.367e+03 -1.389e+03  1.407e+03\n",
      " -1.484e+03 -1.646e+03  1.686e+03 -1.690e+03  1.747e+03 -1.765e+03\n",
      "  1.811e+03 -1.918e+03 -1.935e+03 -1.964e+03  2.100e+03 -2.171e+03\n",
      "  2.197e+03 -2.267e+03 -2.351e+03 -3.196e+03 -3.540e+03 -3.720e+03]\n",
      "durations 8.0 3519.0\n",
      "Concordance Index 0.46622516556291393\n",
      "Integrated Brier Score: 0.4984426379373338\n"
     ]
    }
   ],
   "source": [
    "cancer_types = [\n",
    "    'BLCA',\n",
    "    'BRCA',\n",
    "    'HNSC',\n",
    "    'KIRC',\n",
    "    'LGG',\n",
    "    'LIHC',\n",
    "    'LUAD',\n",
    "    'LUSC',\n",
    "    'OV',\n",
    "    'STAD']\n",
    "import skorch.callbacks\n",
    "\n",
    "class InputShapeSetter(skorch.callbacks.Callback):\n",
    "    def on_train_begin(self, net, X, y):\n",
    "        net.set_params(module__input_units=X.shape[-1])\n",
    "\n",
    "for idx, cancer_type in enumerate(cancer_types):\n",
    "    # get name of current dataset\n",
    "    data = load_tcga(path=\"/Users/JUSC/Documents/xgbsurv/xgbsurv/datasets/data/\", cancer_type=cancer_type, as_frame=True)\n",
    "    X  = data.data #.astype(np.float32)\n",
    "    y = data.target #.values #.to_numpy()\n",
    "\n",
    "    X, y = sort_X_y_pandas(X, y)\n",
    "\n",
    "    net = NeuralNet(\n",
    "        SurvivalModel, \n",
    "        module__n_layers = 1,\n",
    "        module__input_units = X.shape[1],\n",
    "        #module__num_nodes = 32,\n",
    "        #module__dropout = 0.1, # these could also be removed\n",
    "        module__out_features = 1,\n",
    "        # for split sizes when result size = 1\n",
    "        iterator_train__drop_last=True,\n",
    "        #iterator_valid__drop_last=True,\n",
    "        criterion=EfronLoss,\n",
    "        optimizer=torch.optim.AdamW,\n",
    "        optimizer__weight_decay = 0.4,\n",
    "        batch_size=32, # separate train and valid->iterator_train__batch_size=128 and iterator_valid__batch_size=128 ?\n",
    "        callbacks=[\n",
    "            (\n",
    "                \"sched\",\n",
    "                LRScheduler(\n",
    "                    torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
    "                    monitor=\"valid_loss\",\n",
    "                    patience=5,\n",
    "                ),\n",
    "            ),\n",
    "            (\n",
    "                \"es\",\n",
    "                EarlyStopping(\n",
    "                    monitor=\"valid_loss\",\n",
    "                    patience=10,\n",
    "                    load_best=True,\n",
    "                ),\n",
    "            ),\n",
    "            (\"seed\", FixSeed(seed=42)),\n",
    "            (\"Input Shape Setter\",InputShapeSetter())\n",
    "        ],#[EarlyStopping(patience=10)],#,InputShapeSetter()],\n",
    "        #TODO: enable stratification, verify\n",
    "        train_split=ValidSplit(0.2), # might cause lower performance in metrics, explain in thesis\n",
    "        #lr=0.001,\n",
    "        #max_epochs=1, #0,#100\n",
    "        #train_split=None,\n",
    "        verbose=1\n",
    "    )\n",
    "    best_model,params, outer_scores, best_preds_train, best_preds_test, X_train, X_test, y_train, y_test = train_eval(X, y, net, n_iter, data.filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xgbsurv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
