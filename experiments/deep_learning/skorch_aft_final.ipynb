{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import savetxt\n",
    "from xgbsurv.datasets import (load_metabric, load_flchain, load_rgbsg, load_support, load_tcga)\n",
    "from xgbsurv.models.utils import sort_X_y_pandas, transform_back, transform\n",
    "from xgbsurv.models.breslow_final import get_cumulative_hazard_function_breslow, breslow_estimator_loop\n",
    "import torch\n",
    "from torch import nn\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold, train_test_split\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, LabelBinarizer, OneHotEncoder\n",
    "from sklearn.compose import make_column_transformer, make_column_selector\n",
    "from sklearn.decomposition import PCA\n",
    "from loss_functions_pytorch import AFTLoss, aft_likelihood_torch\n",
    "from skorch import NeuralNet\n",
    "from skorch.callbacks import EarlyStopping, Callback, LRScheduler\n",
    "from skorch.dataset import ValidSplit\n",
    "from pycox.evaluation import EvalSurv\n",
    "from scipy.stats import uniform as scuniform\n",
    "from scipy.stats import randint as scrandint\n",
    "from scipy.stats import loguniform as scloguniform\n",
    "import random\n",
    "import os\n",
    "#torch.set_default_dtype(torch.float64)\n",
    "#torch.set_default_tensor_type(torch.DoubleTensor)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters, put into function\n",
    "n_outer_splits = 5\n",
    "n_inner_splits = 5\n",
    "rand_state = 42\n",
    "n_iter = 10 # set to 50\n",
    "#n_iter_cind = 200\n",
    "early_stopping_rounds=15\n",
    "base_score = 0.0\n",
    "\n",
    "param_grid_aft = {\n",
    "    'estimator__module__n_layers': [1, 2, 4],\n",
    "    'estimator__module__n_layers': [1, 2, 4],\n",
    "    'estimator__module__num_nodes': [64, 128, 256, 512],\n",
    "    'estimator__module__dropout': scuniform(0.0,0.7),\n",
    "    'estimator__optimizer__weight_decay': [0.4, 0.2, 0.1, 0.05, 0.02, 0.01, 0],\n",
    "    'estimator__batch_size': [64, 128, 256, 512, 1024],\n",
    "    #lr not in paper because of learning rate finder\n",
    "    # note: setting learning rate higher would make exp(partial_hazard) explode\n",
    "    #'estimator__lr': scloguniform(0.001,0.01), # scheduler unten einbauen\n",
    "    # use callback instead\n",
    "    'estimator__lr':[0.01]\n",
    "    #'max_epochs':  scrandint(10,20), # corresponds to num_rounds\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_torch(seed=42):\n",
    "    \"\"\"Sets all seeds within torch and adjacent libraries.\n",
    "\n",
    "    Args:\n",
    "        seed: Random seed to be used by the seeding functions.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    #os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    #torch.backends.cudnn.benchmark = False\n",
    "    #torch.backends.cudnn.deterministic = True\n",
    "    return None\n",
    "\n",
    "\n",
    "class FixSeed(Callback):\n",
    "    def __init__(self, seed):\n",
    "        self.seed = seed\n",
    "\n",
    "    def initialize(self):\n",
    "        seed_torch(self.seed)\n",
    "        return super().initialize()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Scorer\n",
    "def custom_scoring_function(y_true, y_pred):\n",
    "\n",
    "        #y_true = torch.from_numpy(y_true)\n",
    "        if isinstance(y_pred, np.ndarray):\n",
    "            y_pred = torch.from_numpy(y_pred)\n",
    "        if isinstance(y_true, np.ndarray):\n",
    "            y_true = torch.from_numpy(y_true)\n",
    "        if isinstance(y_pred, pd.Series):\n",
    "            y_pred = torch.tensor(y_pred.values)\n",
    "        if isinstance(y_true, pd.Series):\n",
    "            y_true = torch.tensor(y_true.values)\n",
    "        # change order of this later\n",
    "        score = aft_likelihood_torch(y_pred, y_true).to(torch.float32)\n",
    "        return score.numpy()\n",
    "\n",
    "scoring_function = make_scorer(custom_scoring_function, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set up Custom Splitter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Torch Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SurvivalModel(nn.Module):\n",
    "    def __init__(self, n_layers, input_units, num_nodes, dropout, out_features):\n",
    "        super(SurvivalModel, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.in_features = input_units\n",
    "        self.num_nodes = num_nodes\n",
    "        self.dropout = dropout\n",
    "        self.out_features = out_features\n",
    "        model = []\n",
    "        # first layer\n",
    "        model.append(torch.nn.Linear(input_units, num_nodes))\n",
    "        model.append(torch.nn.ReLU())\n",
    "        model.append(torch.nn.Dropout(dropout))\n",
    "        model.append(torch.nn.BatchNorm1d(num_nodes))\n",
    "\n",
    "        for i in range(n_layers-1):\n",
    "            model.append(torch.nn.Linear(num_nodes, num_nodes))\n",
    "            #init.kaiming_normal_(model[-1].weight, nonlinearity='relu')\n",
    "            model.append(torch.nn.ReLU())\n",
    "            model.append(torch.nn.Dropout(dropout))\n",
    "            model.append(torch.nn.BatchNorm1d(num_nodes))\n",
    "\n",
    "        # output layer\n",
    "        model.append(torch.nn.Linear(num_nodes, out_features))\n",
    "    \n",
    "        self.layers = nn.Sequential(*model)\n",
    "\n",
    "        # for layer in self.layers:\n",
    "        #     if isinstance(layer, nn.Linear):\n",
    "        #         #nn.init.uniform_(layer.weight, a=-0.5, b=0.5)\n",
    "        #         nn.init.kaiming_normal_(layer.weight)\n",
    "\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = X.to(torch.float32)\n",
    "        res = self.layers(X)\n",
    "        #print(res)\n",
    "        return res\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomStandardScaler(StandardScaler):\n",
    "    \n",
    "    def __init__(self, copy=True, with_mean=True, with_std=True):\n",
    "        super().__init__(copy=copy, with_mean=with_mean, with_std=with_std)\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return super().fit(X, y)\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X_transformed = super().transform(X, y)\n",
    "        return X_transformed.astype(np.float32)\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        X_transformed = super().fit_transform(X, y)\n",
    "        return X_transformed.astype(np.float32)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define stratified inner k-fold cross-validation\n",
    "class CustomSplit(StratifiedKFold):\n",
    "    def __init__(self, n_splits=5, shuffle=True, random_state=None):\n",
    "        super().__init__(n_splits=n_splits, shuffle=shuffle, random_state=random_state)\n",
    "\n",
    "    def split(self, X, y, groups=None):\n",
    "        print('split', X.dtypes)\n",
    "        try:\n",
    "            if y.shape[1]>1:\n",
    "                y = y[:,0]\n",
    "        except:\n",
    "            pass\n",
    "        bins = np.sign(y)\n",
    "        return super().split(X, bins, groups=groups)\n",
    "\n",
    "    def get_n_splits(self, X=None, y=None, groups=None):\n",
    "        return self.n_splits\n",
    "\n",
    "outer_custom_cv = CustomSplit(n_splits=n_outer_splits, shuffle=True, random_state=rand_state)\n",
    "inner_custom_cv = CustomSplit(n_splits=n_outer_splits, shuffle=True, random_state=rand_state)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Training Procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_eval(X, y, net, n_iter, filename):\n",
    "        model = '_aft_'\n",
    "        dataset_name = filename.split('_')[0]\n",
    "        # add IBS later\n",
    "        outer_scores = {'cindex_train_'+dataset_name:[], 'cindex_test_'+dataset_name:[],\n",
    "                        'ibs_train_'+dataset_name:[], 'ibs_test_'+dataset_name:[]}\n",
    "        best_params = {'best_params_'+dataset_name:[]}\n",
    "        best_model = {'best_model_'+dataset_name:[]}\n",
    "        ct = make_column_transformer(\n",
    "                #(OneHotEncoder(sparse_output=False), make_column_selector(dtype_include=['category', 'object']))\n",
    "                (StandardScaler(), make_column_selector(dtype_include=['float32']))\n",
    "                ,remainder='passthrough')\n",
    "\n",
    "        pipe = Pipeline([('scaler',ct),\n",
    "                        ('estimator', net)])\n",
    "        rs = RandomizedSearchCV(pipe, param_grid_aft, scoring = scoring_function, n_jobs=-1, \n",
    "                                    n_iter=2, refit=True)\n",
    "        for i, (train_index, test_index) in enumerate(outer_custom_cv.split(X, y)):\n",
    "                # Split data into training and testing sets for outer fold\n",
    "                X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "                y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "                X_train, y_train = sort_X_y_pandas(X_train, y_train)\n",
    "                X_test, y_test = sort_X_y_pandas(X_test, y_test)\n",
    "\n",
    "                print(X_train.shape, type(X_train))\n",
    "                print(y_train.shape, type(y_train))\n",
    "                print(X_test.shape, type(X_test))\n",
    "                print(y_test.shape, type(y_test))\n",
    "                # save splits and data\n",
    "                savetxt('splits/train_index_'+str(i)+'_'+filename, train_index, delimiter=',')\n",
    "                savetxt('splits/test_index_'+str(i)+'_'+filename, test_index, delimiter=',')\n",
    "                \n",
    "                savetxt('splits/X_train_'+str(i)+'_'+filename, X_train, delimiter=',')\n",
    "                savetxt('splits/X_test_'+str(i)+'_'+filename, X_test, delimiter=',')\n",
    "\n",
    "                savetxt('splits/y_train_'+str(i)+'_'+filename, y_train, delimiter=',')\n",
    "                savetxt('splits/y_test_'+str(i)+'_'+filename, y_test, delimiter=',')\n",
    "\n",
    "                strat = np.sign(y_train)\n",
    "                valid_split = ValidSplit(cv=0.1, stratified=strat, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                rs.fit(X_train, y_train)\n",
    "                best_preds_train = rs.best_estimator_.predict(X_train)\n",
    "                best_preds_test = rs.best_estimator_.predict(X_test)\n",
    "                # save hyperparameter settings\n",
    "                params = rs.best_estimator_.get_params\n",
    "                best_params['best_params_'+dataset_name] += [rs.best_params_]\n",
    "                best_model['best_model_'+dataset_name] += [params]\n",
    "                try:\n",
    "                    cum_hazard_train = get_cumulative_hazard_function_breslow(\n",
    "                            X_train.values, X_train.values, y_train.values, y_train.values,\n",
    "                            best_preds_train.reshape(-1), best_preds_train.reshape(-1)\n",
    "                            )\n",
    "\n",
    "                    df_survival_train = np.exp(-cum_hazard_train)\n",
    "                    durations_train, events_train = transform_back(y_train.values)\n",
    "                    time_grid_train = np.linspace(durations_train.min(), durations_train.max(), 100)\n",
    "                    ev = EvalSurv(df_survival_train, durations_train, events_train, censor_surv='km')\n",
    "                    print('Concordance Index',ev.concordance_td('antolini'))\n",
    "                    print('Integrated Brier Score:',ev.integrated_brier_score(time_grid_train))\n",
    "                    cindex_score_train = ev.concordance_td('antolini')\n",
    "                    ibs_score_train = ev.integrated_brier_score(time_grid_train)\n",
    "\n",
    "                    outer_scores['cindex_train_'+dataset_name] += [cindex_score_train]\n",
    "                    outer_scores['ibs_train_'+dataset_name] += [ibs_score_train]\n",
    "\n",
    "                except:\n",
    "                    outer_scores['cindex_train_'+dataset_name] += [np.nan]\n",
    "                    outer_scores['ibs_train_'+dataset_name] += [np.nan]\n",
    "                    \n",
    "                try:\n",
    "                    cum_hazard_test = get_cumulative_hazard_function_breslow(\n",
    "                            X_train.values, X_test.values, y_train.values, y_test.values,\n",
    "                            best_preds_train.reshape(-1), best_preds_test.reshape(-1)\n",
    "                            )\n",
    "                    df_survival_test = np.exp(-cum_hazard_test)\n",
    "                    durations_test, events_test = transform_back(y_test.values)\n",
    "                    print('durations',durations_test.min(), durations_test.max())\n",
    "                    time_grid_test = np.linspace(durations_test.min(), durations_test.max(), 100)\n",
    "                    ev = EvalSurv(df_survival_test, durations_test, events_test, censor_surv='km')\n",
    "                    print('Concordance Index',ev.concordance_td('antolini'))\n",
    "                    print('Integrated Brier Score:',ev.integrated_brier_score(time_grid_test))\n",
    "                    cindex_score_test = ev.concordance_td('antolini')\n",
    "                    ibs_score_test = ev.integrated_brier_score(time_grid_test)\n",
    "\n",
    "                    outer_scores['cindex_test_'+dataset_name] += [cindex_score_test]\n",
    "                    outer_scores['ibs_test_'+dataset_name] += [ibs_score_test]\n",
    "                except: \n",
    "                    outer_scores['cindex_test_'+dataset_name] += [np.nan]\n",
    "                    outer_scores['ibs_test_'+dataset_name] += [np.nan]\n",
    "            \n",
    "        df_best_params = pd.DataFrame(best_params)\n",
    "        df_best_model = pd.DataFrame(best_model)\n",
    "        df_outer_scores = pd.DataFrame(outer_scores)\n",
    "        df_metrics = pd.concat([df_best_params,df_best_model,df_outer_scores], axis=1)\n",
    "        df_metrics.to_csv('metrics/metric_summary'+model+str(i)+'_'+filename, index=False)\n",
    "        return best_model, best_params, outer_scores, best_preds_train, best_preds_test, X_train, X_test, y_train, y_test\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_metabric\n",
      "split MKI67                float32\n",
      "EGFR                 float32\n",
      "PGR                  float32\n",
      "ERBB2                float32\n",
      "hormone_treatment    float32\n",
      "radiotherapy         float32\n",
      "chemotherapy         float32\n",
      "ER_positive          float32\n",
      "age                  float32\n",
      "dtype: object\n",
      "(1522, 9) <class 'pandas.core.frame.DataFrame'>\n",
      "(1522,) <class 'pandas.core.series.Series'>\n",
      "(381, 9) <class 'pandas.core.frame.DataFrame'>\n",
      "(381,) <class 'pandas.core.series.Series'>\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.9247\u001b[0m        \u001b[32m2.7730\u001b[0m  0.0170\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.1021\u001b[0m        \u001b[32m3.0375\u001b[0m  0.0179\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.3435\u001b[0m        \u001b[32m2.9866\u001b[0m  0.0213\n",
      "      2        \u001b[36m2.9102\u001b[0m        \u001b[32m2.7554\u001b[0m  0.0151\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.3357\u001b[0m        \u001b[32m2.9716\u001b[0m  0.0201\n",
      "      2        \u001b[36m3.0811\u001b[0m        \u001b[32m3.0219\u001b[0m  0.0225\n",
      "      3        \u001b[36m2.8643\u001b[0m        \u001b[32m2.7486\u001b[0m  0.0181\n",
      "      2        \u001b[36m3.3116\u001b[0m        \u001b[32m2.9766\u001b[0m  0.0215\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.3766\u001b[0m        \u001b[32m3.0212\u001b[0m  0.0185\n",
      "      3        \u001b[36m3.0571\u001b[0m        \u001b[32m3.0095\u001b[0m  0.0166\n",
      "      4        \u001b[36m2.8317\u001b[0m        \u001b[32m2.7465\u001b[0m  0.0177\n",
      "      3        \u001b[36m3.2955\u001b[0m        \u001b[32m2.9693\u001b[0m  0.0190\n",
      "      2        \u001b[36m3.2838\u001b[0m        \u001b[32m2.9650\u001b[0m  0.0276\n",
      "      2        \u001b[36m3.3185\u001b[0m        \u001b[32m3.0194\u001b[0m  0.0196\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.1516\u001b[0m        \u001b[32m3.0812\u001b[0m  0.1002\n",
      "      4        \u001b[36m3.0310\u001b[0m        \u001b[32m2.9989\u001b[0m  0.0195\n",
      "      4        \u001b[36m3.2776\u001b[0m        \u001b[32m2.9662\u001b[0m  0.0191\n",
      "      3        \u001b[36m3.2708\u001b[0m        \u001b[32m2.9620\u001b[0m  0.0186\n",
      "      3        \u001b[36m3.3093\u001b[0m        3.0209  0.0183\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.3483\u001b[0m        \u001b[32m3.0284\u001b[0m  0.1111\n",
      "      5        \u001b[36m2.7633\u001b[0m        \u001b[32m2.7453\u001b[0m  0.0290\n",
      "      5        3.0320        \u001b[32m2.9909\u001b[0m  0.0195\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.4108\u001b[0m        \u001b[32m3.0616\u001b[0m  0.1112\n",
      "      4        \u001b[36m3.2418\u001b[0m        2.9621  0.0170\n",
      "      4        \u001b[36m3.2744\u001b[0m        3.0230  0.0166\n",
      "      5        \u001b[36m3.2622\u001b[0m        \u001b[32m2.9657\u001b[0m  0.0234\n",
      "      6        2.7823        \u001b[32m2.7449\u001b[0m  0.0192\n",
      "      6        \u001b[36m3.0095\u001b[0m        \u001b[32m2.9863\u001b[0m  0.0163\n",
      "  epoch    train_loss    valid_loss     dur      5        \u001b[36m3.2361\u001b[0m        2.9634  0.0166\n",
      "\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.3627\u001b[0m        \u001b[32m2.9687\u001b[0m  0.1257\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.9769\u001b[0m        \u001b[32m2.9333\u001b[0m  0.1492\n",
      "      7        \u001b[36m2.7608\u001b[0m        \u001b[32m2.7443\u001b[0m  0.0161\n",
      "      6        \u001b[36m3.2595\u001b[0m        2.9672  0.0180\n",
      "      5        \u001b[36m3.2528\u001b[0m        3.0252  0.0303\n",
      "      6        \u001b[36m3.2190\u001b[0m        2.9656  0.0175\n",
      "      7        \u001b[36m2.9786\u001b[0m        \u001b[32m2.9832\u001b[0m  0.0238\n",
      "      8        \u001b[36m2.7434\u001b[0m        \u001b[32m2.7434\u001b[0m  0.0226\n",
      "      7        \u001b[36m3.2430\u001b[0m        2.9690  0.0272\n",
      "      7        \u001b[36m3.1968\u001b[0m        2.9679  0.0204\n",
      "      6        3.2556        3.0278  0.0246\n",
      "      9        2.7724        \u001b[32m2.7423\u001b[0m  0.0198\n",
      "      8        2.9863        \u001b[32m2.9820\u001b[0m  0.0347\n",
      "      8        \u001b[36m3.2362\u001b[0m        2.9714  0.0295\n",
      "      8        3.2045        2.9706  0.0264\n",
      "     10        2.7442        \u001b[32m2.7411\u001b[0m  0.0172\n",
      "      7        \u001b[36m3.2186\u001b[0m        3.0304  0.0310\n",
      "      9        2.9947        \u001b[32m2.9817\u001b[0m  0.0185\n",
      "      2        \u001b[36m3.0785\u001b[0m        \u001b[32m3.0236\u001b[0m  0.1267\n",
      "      9        3.2569        2.9742  0.0196\n",
      "      9        3.2178        2.9732  0.0228\n",
      "     10        2.9856        2.9820  0.0188\n",
      "      8        3.2227        3.0326  0.0201\n",
      "Restoring best model from epoch 8.\n",
      "      2        \u001b[36m3.2823\u001b[0m        3.0667  0.1291\n",
      "     10        3.2375        2.9768  0.0166\n",
      "Restoring best model from epoch 5.\n",
      "      2        \u001b[36m3.3392\u001b[0m        3.1220  0.1285\n",
      "     10        3.2082        2.9728  0.0185\n",
      "Restoring best model from epoch 3.\n",
      "      9        3.2346        3.0323  0.0178\n",
      "      2        \u001b[36m3.2570\u001b[0m        \u001b[32m2.9610\u001b[0m  0.1182\n",
      "     10        \u001b[36m3.2175\u001b[0m        3.0319  0.0178\n",
      "Restoring best model from epoch 2.\n",
      "      2        \u001b[36m2.8659\u001b[0m        \u001b[32m2.9268\u001b[0m  0.1243\n",
      "      3        \u001b[36m3.0381\u001b[0m        \u001b[32m3.0111\u001b[0m  0.1022\n",
      "      3        \u001b[36m3.2335\u001b[0m        \u001b[32m2.9636\u001b[0m  0.0979\n",
      "      3        \u001b[36m3.2389\u001b[0m        \u001b[32m3.0440\u001b[0m  0.0950\n",
      "      3        \u001b[36m3.2519\u001b[0m        3.0289  0.0948\n",
      "      3        \u001b[36m2.8118\u001b[0m        \u001b[32m2.7940\u001b[0m  0.0918\n",
      "      4        \u001b[36m3.0264\u001b[0m        \u001b[32m2.9969\u001b[0m  0.0959\n",
      "      4        \u001b[36m3.2046\u001b[0m        3.0050  0.0957\n",
      "      4        \u001b[36m3.2288\u001b[0m        \u001b[32m3.0370\u001b[0m  0.0948\n",
      "      4        \u001b[36m3.1799\u001b[0m        \u001b[32m2.9583\u001b[0m  0.0943\n",
      "      4        \u001b[36m2.7931\u001b[0m        2.8302  0.0935\n",
      "      5        \u001b[36m3.0042\u001b[0m        \u001b[32m2.9956\u001b[0m  0.0926\n",
      "      5        \u001b[36m3.1783\u001b[0m        2.9917  0.0907\n",
      "      5        \u001b[36m3.2102\u001b[0m        \u001b[32m3.0221\u001b[0m  0.0917\n",
      "      5        3.1822        2.9713  0.0934\n",
      "      5        2.7936        2.8009  0.0918\n",
      "      6        \u001b[36m2.9985\u001b[0m        \u001b[32m2.9747\u001b[0m  0.0914\n",
      "      6        \u001b[36m3.1725\u001b[0m        \u001b[32m2.9509\u001b[0m  0.0917\n",
      "      6        \u001b[36m3.1922\u001b[0m        3.0361  0.0923\n",
      "      6        \u001b[36m3.1358\u001b[0m        \u001b[32m2.9561\u001b[0m  0.0918\n",
      "      6        \u001b[36m2.7370\u001b[0m        \u001b[32m2.7761\u001b[0m  0.0908\n",
      "      7        \u001b[36m2.9769\u001b[0m        2.9820  0.0915\n",
      "      7        3.1922        2.9683  0.0916\n",
      "      7        \u001b[36m3.1905\u001b[0m        \u001b[32m3.0146\u001b[0m  0.0916\n",
      "      7        3.1517        2.9762  0.0920\n",
      "      7        2.7587        2.8554  0.0908\n",
      "      8        2.9846        \u001b[32m2.9670\u001b[0m  0.0916\n",
      "      8        \u001b[36m3.1606\u001b[0m        2.9653  0.0911\n",
      "      8        \u001b[36m3.1732\u001b[0m        \u001b[32m3.0076\u001b[0m  0.0925\n",
      "      8        3.1388        \u001b[32m2.9456\u001b[0m  0.0922\n",
      "      8        2.7485        2.7989  0.0901\n",
      "      9        2.9798        \u001b[32m2.9644\u001b[0m  0.0916\n",
      "      9        \u001b[36m3.1522\u001b[0m        2.9827  0.0911\n",
      "      9        3.1886        3.0168  0.0921\n",
      "      9        2.7414        2.7889  0.0912\n",
      "      9        3.1442        2.9559  0.0922\n",
      "     10        2.9818        2.9752  0.0909\n",
      "Restoring best model from epoch 9.\n",
      "     10        \u001b[36m3.1491\u001b[0m        2.9616  0.0911\n",
      "Restoring best model from epoch 6.\n",
      "     10        3.1893        3.0222  0.0919\n",
      "Restoring best model from epoch 8.\n",
      "     10        \u001b[36m2.7119\u001b[0m        2.8180  0.0924\n",
      "Restoring best model from epoch 6.\n",
      "     10        3.1434        2.9484  0.0921\n",
      "Restoring best model from epoch 8.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.1398\u001b[0m        \u001b[32m3.2492\u001b[0m  0.0220\n",
      "      2        \u001b[36m3.0942\u001b[0m        \u001b[32m3.2359\u001b[0m  0.0212\n",
      "      3        \u001b[36m3.0861\u001b[0m        \u001b[32m3.2292\u001b[0m  0.0239\n",
      "      4        \u001b[36m3.0586\u001b[0m        \u001b[32m3.2278\u001b[0m  0.0224\n",
      "      5        3.0668        \u001b[32m3.2270\u001b[0m  0.0227\n",
      "      6        \u001b[36m3.0413\u001b[0m        \u001b[32m3.2263\u001b[0m  0.0223\n",
      "      7        3.0538        \u001b[32m3.2241\u001b[0m  0.0206\n",
      "      8        \u001b[36m3.0310\u001b[0m        \u001b[32m3.2213\u001b[0m  0.0216\n",
      "      9        3.0371        \u001b[32m3.2172\u001b[0m  0.0213\n",
      "     10        \u001b[36m3.0293\u001b[0m        \u001b[32m3.2124\u001b[0m  0.0212\n",
      "y_train breslow final [ 1.0000000e-01 -1.2333333e+00 -1.2666667e+00 ...  3.3573334e+02\n",
      "  3.5100000e+02  3.5520001e+02]\n",
      "Concordance Index 0.65814754757806\n",
      "Integrated Brier Score: 0.16003562559594434\n",
      "y_train breslow final [ 1.0000000e-01 -1.2333333e+00 -1.2666667e+00 ...  3.3573334e+02\n",
      "  3.5100000e+02  3.5520001e+02]\n",
      "durations 0.76666665 337.03333\n",
      "Concordance Index 0.6368001269790882\n",
      "Integrated Brier Score: 0.16607704165904924\n",
      "(1522, 9) <class 'pandas.core.frame.DataFrame'>\n",
      "(1522,) <class 'pandas.core.series.Series'>\n",
      "(381, 9) <class 'pandas.core.frame.DataFrame'>\n",
      "(381,) <class 'pandas.core.series.Series'>\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m2.5993\u001b[0m  0.0037\n",
      "      2        2.5993  0.0043\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.0178\u001b[0m  0.0041\n",
      "      3        2.5993  0.0036\n",
      "      2        3.0178  0.0039\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.1361\u001b[0m        \u001b[32m3.0013\u001b[0m  0.0436\n",
      "      4        2.5993  0.0035\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.1727\u001b[0m        \u001b[32m3.3670\u001b[0m  0.0427\n",
      "      3        3.0178  0.0056\n",
      "      5        2.5993  0.0039\n",
      "      4        3.0178  0.0040\n",
      "      6        2.5993  0.0036\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.9751\u001b[0m        \u001b[32m2.5903\u001b[0m  0.0471\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.1921\u001b[0m        \u001b[32m3.4321\u001b[0m  0.0462\n",
      "      5        3.0178  0.0055\n",
      "      7        2.5993  0.0050\n",
      "      6        3.0178  0.0038\n",
      "      8        2.5993  0.0036\n",
      "      9        2.5993  0.0035\n",
      "      7        3.0178  0.0037\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.4682\u001b[0m  0.0043\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.2231\u001b[0m        \u001b[32m3.4485\u001b[0m  0.0444\n",
      "      8        3.0178  0.0037\n",
      "     10        2.5993  0.0039\n",
      "Restoring best model from epoch 1.\n",
      "      2        3.4682  0.0045\n",
      "      9        3.0178  0.0039\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.4179\u001b[0m  0.0063\n",
      "      3        3.4682  0.0041\n",
      "     10        3.0178  0.0037\n",
      "Restoring best model from epoch 1.\n",
      "      2        3.4179  0.0045\n",
      "      4        3.4682  0.0043\n",
      "      3        3.4179  0.0039\n",
      "      5        3.4682  0.0041\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.4715\u001b[0m  0.0046\n",
      "      4        3.4179  0.0039\n",
      "      6        3.4682  0.0041\n",
      "      2        3.4715  0.0044\n",
      "      5        3.4179  0.0038\n",
      "      7        3.4682  0.0047\n",
      "      6        3.4179  0.0038\n",
      "      3        3.4715  0.0043\n",
      "      8        3.4682  0.0040\n",
      "      2        \u001b[36m3.0615\u001b[0m        \u001b[32m2.9859\u001b[0m  0.0530\n",
      "      2        \u001b[36m3.1349\u001b[0m        \u001b[32m3.3472\u001b[0m  0.0502\n",
      "      7        3.4179  0.0038\n",
      "      4        3.4715  0.0042\n",
      "      2        \u001b[36m3.1177\u001b[0m        \u001b[32m3.4230\u001b[0m  0.0437\n",
      "      2        \u001b[36m2.8901\u001b[0m        \u001b[32m2.5720\u001b[0m  0.0467\n",
      "      9        3.4682  0.0054\n",
      "      8        3.4179  0.0052\n",
      "      5        3.4715  0.0061\n",
      "     10        3.4682  0.0042\n",
      "Restoring best model from epoch 1.\n",
      "      9        3.4179  0.0040\n",
      "      6        3.4715  0.0044\n",
      "     10        3.4179  0.0045\n",
      "Restoring best model from epoch 1.\n",
      "      7        3.4715  0.0042\n",
      "      8        3.4715  0.0041\n",
      "      2        \u001b[36m3.1535\u001b[0m        \u001b[32m3.4475\u001b[0m  0.0500\n",
      "      9        3.4715  0.0040\n",
      "     10        3.4715  0.0045\n",
      "Restoring best model from epoch 1.\n",
      "      3        \u001b[36m3.0123\u001b[0m        3.0040  0.0428\n",
      "      3        \u001b[36m3.0666\u001b[0m        \u001b[32m3.3390\u001b[0m  0.0420\n",
      "      3        \u001b[36m3.0767\u001b[0m        3.4466  0.0429\n",
      "      3        \u001b[36m2.8586\u001b[0m        2.5720  0.0439\n",
      "      3        \u001b[36m3.1026\u001b[0m        3.4632  0.0418\n",
      "      4        \u001b[36m3.0022\u001b[0m        \u001b[32m2.9812\u001b[0m  0.0402\n",
      "      4        \u001b[36m3.0585\u001b[0m        \u001b[32m3.3257\u001b[0m  0.0413\n",
      "      4        \u001b[36m3.0684\u001b[0m        \u001b[32m3.4203\u001b[0m  0.0412\n",
      "      4        \u001b[36m2.8373\u001b[0m        \u001b[32m2.5644\u001b[0m  0.0411\n",
      "      4        \u001b[36m3.0897\u001b[0m        3.4542  0.0420\n",
      "      5        3.0108        3.0127  0.0408\n",
      "      5        \u001b[36m3.0485\u001b[0m        3.3314  0.0400\n",
      "      5        \u001b[36m3.0443\u001b[0m        3.4486  0.0409\n",
      "      5        2.8446        \u001b[32m2.5630\u001b[0m  0.0414\n",
      "      5        \u001b[36m3.0778\u001b[0m        3.4880  0.0414\n",
      "      6        \u001b[36m2.9889\u001b[0m        2.9847  0.0411\n",
      "      6        \u001b[36m3.0301\u001b[0m        3.3505  0.0410\n",
      "      6        3.0485        3.4209  0.0409\n",
      "      6        \u001b[36m2.8305\u001b[0m        2.5782  0.0425\n",
      "      6        3.1043        3.4634  0.0419\n",
      "      7        \u001b[36m2.9832\u001b[0m        \u001b[32m2.9785\u001b[0m  0.0408\n",
      "      7        3.0385        \u001b[32m3.3132\u001b[0m  0.0423\n",
      "      7        \u001b[36m3.0392\u001b[0m        3.4261  0.0417\n",
      "      7        2.8326        \u001b[32m2.5461\u001b[0m  0.0411\n",
      "      7        3.0783        3.4602  0.0419\n",
      "      8        2.9951        2.9785  0.0413\n",
      "      8        \u001b[36m3.0264\u001b[0m        3.3354  0.0410\n",
      "      8        \u001b[36m3.0373\u001b[0m        3.4220  0.0409\n",
      "      8        \u001b[36m2.8220\u001b[0m        2.5517  0.0403\n",
      "      8        \u001b[36m3.0761\u001b[0m        3.4648  0.0415\n",
      "      9        \u001b[36m2.9747\u001b[0m        2.9873  0.0408\n",
      "      9        3.0315        3.3558  0.0413\n",
      "      9        \u001b[36m3.0333\u001b[0m        3.4397  0.0413\n",
      "      9        \u001b[36m2.8161\u001b[0m        2.5472  0.0415\n",
      "      9        \u001b[36m3.0710\u001b[0m        3.4646  0.0420\n",
      "     10        \u001b[36m2.9741\u001b[0m        \u001b[32m2.9766\u001b[0m  0.0395\n",
      "     10        \u001b[36m3.0209\u001b[0m        3.3388  0.0412\n",
      "Restoring best model from epoch 7.\n",
      "     10        \u001b[36m3.0059\u001b[0m        3.4327  0.0413\n",
      "Restoring best model from epoch 4.\n",
      "     10        \u001b[36m2.8079\u001b[0m        2.5507  0.0415\n",
      "Restoring best model from epoch 7.\n",
      "     10        \u001b[36m3.0653\u001b[0m        3.4602  0.0412\n",
      "Restoring best model from epoch 2.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.2178\u001b[0m        \u001b[32m3.1561\u001b[0m  0.0302\n",
      "      2        3.4552        \u001b[32m3.1383\u001b[0m  0.0321\n",
      "      3        3.2755        \u001b[32m3.1303\u001b[0m  0.0285\n",
      "      4        \u001b[36m3.1013\u001b[0m        3.1330  0.0329\n",
      "      5        \u001b[36m3.0521\u001b[0m        3.1460  0.0425\n",
      "      6        3.0574        3.1535  0.0588\n",
      "      7        3.0550        3.1478  0.0276\n",
      "      8        \u001b[36m3.0366\u001b[0m        3.1328  0.0245\n",
      "      9        \u001b[36m3.0172\u001b[0m        \u001b[32m3.1171\u001b[0m  0.0252\n",
      "     10        \u001b[36m3.0063\u001b[0m        \u001b[32m3.1075\u001b[0m  0.0256\n",
      "y_train breslow final [  -0.76666665   -1.2333333    -1.2666667  ... -337.03333     351.\n",
      "  355.2       ]\n",
      "Concordance Index 0.6614524468408661\n",
      "Integrated Brier Score: 0.16233379400789386\n",
      "y_train breslow final [  -0.76666665   -1.2333333    -1.2666667  ... -337.03333     351.\n",
      "  355.2       ]\n",
      "durations 0.1 330.36667\n",
      "Concordance Index 0.6253810301087533\n",
      "Integrated Brier Score: 0.17959729954636766\n",
      "(1522, 9) <class 'pandas.core.frame.DataFrame'>\n",
      "(1522,) <class 'pandas.core.series.Series'>\n",
      "(381, 9) <class 'pandas.core.frame.DataFrame'>\n",
      "(381,) <class 'pandas.core.series.Series'>\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m2.6287\u001b[0m  0.0040\n",
      "      2        2.6287  0.0041\n",
      "      3        2.6287  0.0036\n",
      "      4        2.6287  0.0036\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m2.9733\u001b[0m  0.0040\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.9335\u001b[0m        \u001b[32m2.6523\u001b[0m  0.0421\n",
      "      5        2.6287  0.0035\n",
      "      6        2.6287  0.0035\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.3788\u001b[0m  0.0042\n",
      "      2        2.9733  0.0063\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.0961\u001b[0m        \u001b[32m2.9651\u001b[0m  0.0442\n",
      "      7        2.6287  0.0048\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.3346\u001b[0m  0.0042\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.1686\u001b[0m        \u001b[32m3.3360\u001b[0m  0.0419\n",
      "      3        2.9733  0.0038\n",
      "      2        3.3788  0.0066\n",
      "      8        2.6287  0.0036\n",
      "      2        3.3346  0.0047\n",
      "      4        2.9733  0.0037\n",
      "      3        3.3788  0.0040\n",
      "      9        2.6287  0.0036\n",
      "      5        2.9733  0.0037\n",
      "      3        3.3346  0.0046\n",
      "     10        2.6287  0.0036\n",
      "Restoring best model from epoch 1.\n",
      "      4        3.3788  0.0056\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.1834\u001b[0m        \u001b[32m3.2786\u001b[0m  0.0504\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.2155\u001b[0m        \u001b[32m3.5421\u001b[0m  0.0472\n",
      "      4        3.3346  0.0045\n",
      "      6        2.9733  0.0074\n",
      "      5        3.3788  0.0041\n",
      "      5        3.3346  0.0043\n",
      "      6        3.3788  0.0047\n",
      "      7        2.9733  0.0070\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.5805\u001b[0m  0.0062\n",
      "      6        3.3346  0.0051\n",
      "      8        2.9733  0.0039\n",
      "      7        3.3788  0.0076\n",
      "      2        3.5805  0.0054\n",
      "      7        3.3346  0.0063\n",
      "      9        2.9733  0.0046\n",
      "      3        3.5805  0.0043\n",
      "      8        3.3346  0.0042\n",
      "     10        2.9733  0.0041\n",
      "      8        3.3788  0.0080\n",
      "Restoring best model from epoch 1.\n",
      "      9        3.3346  0.0040\n",
      "      4        3.5805  0.0057\n",
      "      2        \u001b[36m2.8801\u001b[0m        2.6683  0.0510\n",
      "      9        3.3788  0.0047\n",
      "     10        3.3346  0.0041\n",
      "Restoring best model from epoch 1.\n",
      "      5        3.5805  0.0043\n",
      "     10        3.3788  0.0044\n",
      "Restoring best model from epoch 1.\n",
      "      2        \u001b[36m3.0150\u001b[0m        \u001b[32m2.9559\u001b[0m  0.0525\n",
      "      6        3.5805  0.0071\n",
      "      2        \u001b[36m3.1230\u001b[0m        \u001b[32m3.2909\u001b[0m  0.0559\n",
      "      7        3.5805  0.0044\n",
      "      8        3.5805  0.0041\n",
      "      2        \u001b[36m3.1170\u001b[0m        \u001b[32m3.2530\u001b[0m  0.0486\n",
      "      2        \u001b[36m3.1223\u001b[0m        \u001b[32m3.5411\u001b[0m  0.0497\n",
      "      9        3.5805  0.0040\n",
      "     10        3.5805  0.0044\n",
      "Restoring best model from epoch 1.\n",
      "      3        \u001b[36m2.8183\u001b[0m        \u001b[32m2.6246\u001b[0m  0.0422\n",
      "      3        \u001b[36m2.9943\u001b[0m        2.9691  0.0424\n",
      "      3        \u001b[36m3.0669\u001b[0m        3.3158  0.0420\n",
      "      3        \u001b[36m3.0653\u001b[0m        3.2648  0.0420\n",
      "      3        \u001b[36m3.0643\u001b[0m        3.5625  0.0414\n",
      "      4        \u001b[36m2.7962\u001b[0m        2.6751  0.0417\n",
      "      4        \u001b[36m2.9785\u001b[0m        \u001b[32m2.9455\u001b[0m  0.0417\n",
      "      4        \u001b[36m3.0658\u001b[0m        3.3098  0.0418\n",
      "      4        \u001b[36m3.0510\u001b[0m        3.2992  0.0428\n",
      "      4        \u001b[36m3.0431\u001b[0m        3.5795  0.0425\n",
      "      5        2.7967        \u001b[32m2.6233\u001b[0m  0.0414\n",
      "      5        \u001b[36m2.9495\u001b[0m        2.9579  0.0416\n",
      "      5        \u001b[36m3.0551\u001b[0m        3.3330  0.0417\n",
      "      5        \u001b[36m3.0384\u001b[0m        3.3073  0.0416\n",
      "      5        3.0482        3.5631  0.0419\n",
      "      6        2.7977        2.6423  0.0414\n",
      "      6        \u001b[36m2.9450\u001b[0m        2.9570  0.0421\n",
      "      6        \u001b[36m3.0528\u001b[0m        3.3087  0.0418\n",
      "      6        \u001b[36m3.0381\u001b[0m        3.2776  0.0423\n",
      "      6        3.0514        3.5962  0.0424\n",
      "      7        \u001b[36m2.7774\u001b[0m        2.6308  0.0414\n",
      "      7        \u001b[36m2.9425\u001b[0m        \u001b[32m2.9346\u001b[0m  0.0415\n",
      "      7        \u001b[36m3.0385\u001b[0m        3.3063  0.0422\n",
      "      7        \u001b[36m3.0191\u001b[0m        3.2733  0.0419\n",
      "      7        \u001b[36m3.0347\u001b[0m        \u001b[32m3.5406\u001b[0m  0.0413\n",
      "      8        2.7824        2.6237  0.0402\n",
      "      8        2.9647        2.9478  0.0409\n",
      "      8        \u001b[36m3.0379\u001b[0m        3.3095  0.0409\n",
      "      8        3.0300        3.2703  0.0410\n",
      "      8        \u001b[36m3.0346\u001b[0m        3.5587  0.0408\n",
      "      9        2.7793        2.6311  0.0403\n",
      "      9        2.9673        2.9568  0.0412\n",
      "      9        \u001b[36m3.0221\u001b[0m        3.3024  0.0408\n",
      "      9        \u001b[36m3.0124\u001b[0m        3.2664  0.0417\n",
      "      9        \u001b[36m3.0232\u001b[0m        3.5497  0.0413\n",
      "     10        \u001b[36m2.7706\u001b[0m        \u001b[32m2.6215\u001b[0m  0.0409\n",
      "     10        2.9442        2.9703  0.0418\n",
      "Restoring best model from epoch 7.\n",
      "     10        3.0243        3.3010  0.0399\n",
      "Restoring best model from epoch 2.\n",
      "     10        3.0283        3.5420  0.0409\n",
      "Restoring best model from epoch 7.\n",
      "     10        \u001b[36m3.0027\u001b[0m        3.2704  0.0432\n",
      "Restoring best model from epoch 2.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.2369\u001b[0m        \u001b[32m3.0068\u001b[0m  0.0258\n",
      "      2        3.4862        \u001b[32m2.9972\u001b[0m  0.0266\n",
      "      3        3.3266        3.0011  0.0248\n",
      "      4        \u001b[36m3.1475\u001b[0m        3.0124  0.0271\n",
      "      5        \u001b[36m3.0708\u001b[0m        3.0224  0.0261\n",
      "      6        3.0721        3.0219  0.0265\n",
      "      7        3.0862        3.0128  0.0261\n",
      "      8        3.0793        3.0020  0.0267\n",
      "      9        \u001b[36m3.0563\u001b[0m        2.9996  0.0273\n",
      "     10        \u001b[36m3.0545\u001b[0m        \u001b[32m2.9972\u001b[0m  0.0254\n",
      "Restoring best model from epoch 2.\n",
      "y_train breslow final [ 1.0000000e-01 -7.6666665e-01 -1.2333333e+00 ... -3.3703333e+02\n",
      "  3.5100000e+02  3.5520001e+02]\n",
      "Concordance Index 0.6161978695432665\n",
      "Integrated Brier Score: 0.17747844740670043\n",
      "y_train breslow final [ 1.0000000e-01 -7.6666665e-01 -1.2333333e+00 ... -3.3703333e+02\n",
      "  3.5100000e+02  3.5520001e+02]\n",
      "durations 1.4333333 301.23334\n",
      "Concordance Index 0.5722895148980037\n",
      "Integrated Brier Score: 0.19769491318765667\n",
      "(1523, 9) <class 'pandas.core.frame.DataFrame'>\n",
      "(1523,) <class 'pandas.core.series.Series'>\n",
      "(380, 9) <class 'pandas.core.frame.DataFrame'>\n",
      "(380,) <class 'pandas.core.series.Series'>\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m2.7648\u001b[0m  0.0038\n",
      "      2        2.7648  0.0038\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.0183\u001b[0m  0.0042\n",
      "      3        2.7648  0.0035\n",
      "      2        3.0183  0.0042\n",
      "      4        2.7648  0.0035\n",
      "      5        2.7648  0.0035\n",
      "      3        3.0183  0.0039\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.8498\u001b[0m        \u001b[32m2.7647\u001b[0m  0.0418\n",
      "      6        2.7648  0.0035\n",
      "      4        3.0183  0.0039\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.0708\u001b[0m  0.0040\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.0662\u001b[0m        \u001b[32m3.0327\u001b[0m  0.0427\n",
      "      7        2.7648  0.0035\n",
      "      5        3.0183  0.0038\n",
      "      2        3.0708  0.0053\n",
      "      8        2.7648  0.0035\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.2025\u001b[0m        \u001b[32m3.0454\u001b[0m  0.0445\n",
      "      6        3.0183  0.0045\n",
      "      3        3.0708  0.0039\n",
      "      9        2.7648  0.0037\n",
      "     10        2.7648  0.0037\n",
      "Restoring best model from epoch 1.\n",
      "      7        3.0183  0.0052\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.1744\u001b[0m  0.0041\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.2245\u001b[0m        \u001b[32m3.1563\u001b[0m  0.0480\n",
      "      4        3.0708  0.0070\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.3113\u001b[0m        \u001b[32m3.2870\u001b[0m  0.0443\n",
      "      8        3.0183  0.0037\n",
      "      2        3.1744  0.0043\n",
      "      5        3.0708  0.0039\n",
      "      9        3.0183  0.0044\n",
      "      3        3.1744  0.0041\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.2945\u001b[0m  0.0049\n",
      "     10        3.0183  0.0038\n",
      "      6        3.0708  0.0056\n",
      "Restoring best model from epoch 1.\n",
      "      4        3.1744  0.0039\n",
      "      2        3.2945  0.0041\n",
      "      7        3.0708  0.0039\n",
      "      3        3.2945  0.0050\n",
      "      8        3.0708  0.0038\n",
      "      5        3.1744  0.0069\n",
      "      9        3.0708  0.0037\n",
      "      4        3.2945  0.0042\n",
      "      6        3.1744  0.0043\n",
      "     10        3.0708  0.0039\n",
      "Restoring best model from epoch 1.\n",
      "      5        3.2945  0.0040\n",
      "      7        3.1744  0.0039\n",
      "      2        \u001b[36m2.7880\u001b[0m        2.7973  0.0479\n",
      "      6        3.2945  0.0039\n",
      "      2        \u001b[36m3.0097\u001b[0m        \u001b[32m3.0305\u001b[0m  0.0453\n",
      "      8        3.1744  0.0039\n",
      "      7        3.2945  0.0039\n",
      "      9        3.1744  0.0044\n",
      "      2        \u001b[36m3.1354\u001b[0m        \u001b[32m3.0272\u001b[0m  0.0456\n",
      "      8        3.2945  0.0043\n",
      "     10        3.1744  0.0038\n",
      "Restoring best model from epoch 1.\n",
      "      9        3.2945  0.0040\n",
      "      2        \u001b[36m3.2319\u001b[0m        \u001b[32m3.2842\u001b[0m  0.0453\n",
      "      2        \u001b[36m3.1381\u001b[0m        \u001b[32m3.1343\u001b[0m  0.0469\n",
      "     10        3.2945  0.0039\n",
      "Restoring best model from epoch 1.\n",
      "      3        \u001b[36m2.7471\u001b[0m        2.7744  0.0414\n",
      "      3        \u001b[36m2.9624\u001b[0m        3.0579  0.0413\n",
      "      3        \u001b[36m3.0895\u001b[0m        3.0476  0.0421\n",
      "      3        \u001b[36m3.1911\u001b[0m        3.2843  0.0423\n",
      "      3        \u001b[36m3.1126\u001b[0m        \u001b[32m3.1309\u001b[0m  0.0424\n",
      "      4        \u001b[36m2.7359\u001b[0m        \u001b[32m2.7599\u001b[0m  0.0412\n",
      "      4        2.9797        3.0407  0.0414\n",
      "      4        3.0970        3.0417  0.0410\n",
      "      4        \u001b[36m3.1820\u001b[0m        3.2992  0.0414\n",
      "      4        \u001b[36m3.0792\u001b[0m        3.1615  0.0419\n",
      "      5        \u001b[36m2.7261\u001b[0m        2.7702  0.0410\n",
      "      5        \u001b[36m2.9502\u001b[0m        3.0558  0.0400\n",
      "      5        \u001b[36m3.0746\u001b[0m        3.0488  0.0402\n",
      "      5        \u001b[36m3.1608\u001b[0m        3.2892  0.0411\n",
      "      5        \u001b[36m3.0687\u001b[0m        3.1495  0.0417\n",
      "      6        2.7290        2.7602  0.0406\n",
      "      6        \u001b[36m2.9432\u001b[0m        3.0403  0.0415\n",
      "      6        3.0831        3.0460  0.0423\n",
      "      6        \u001b[36m3.1540\u001b[0m        \u001b[32m3.2810\u001b[0m  0.0419\n",
      "      6        \u001b[36m3.0649\u001b[0m        3.1405  0.0423\n",
      "      7        \u001b[36m2.7220\u001b[0m        \u001b[32m2.7443\u001b[0m  0.0412\n",
      "      7        \u001b[36m2.9428\u001b[0m        3.0437  0.0419\n",
      "      7        \u001b[36m3.0711\u001b[0m        3.0478  0.0407\n",
      "      7        3.1580        3.2826  0.0417\n",
      "      7        \u001b[36m3.0646\u001b[0m        \u001b[32m3.1303\u001b[0m  0.0411\n",
      "      8        \u001b[36m2.7129\u001b[0m        2.7607  0.0404\n",
      "      8        \u001b[36m2.9352\u001b[0m        3.0442  0.0411\n",
      "      8        3.0746        \u001b[32m3.0269\u001b[0m  0.0410\n",
      "      8        \u001b[36m3.1432\u001b[0m        \u001b[32m3.2765\u001b[0m  0.0412\n",
      "      8        \u001b[36m3.0492\u001b[0m        3.1308  0.0421\n",
      "      9        \u001b[36m2.7038\u001b[0m        2.7587  0.0408\n",
      "      9        2.9355        \u001b[32m3.0301\u001b[0m  0.0406\n",
      "      9        \u001b[36m3.0623\u001b[0m        3.0270  0.0408\n",
      "      9        \u001b[36m3.1419\u001b[0m        3.2823  0.0420\n",
      "      9        \u001b[36m3.0424\u001b[0m        3.1349  0.0421\n",
      "     10        \u001b[36m2.7030\u001b[0m        2.8009  0.0405\n",
      "Restoring best model from epoch 7.\n",
      "     10        \u001b[36m2.9192\u001b[0m        \u001b[32m3.0264\u001b[0m  0.0410\n",
      "     10        \u001b[36m3.0605\u001b[0m        3.0340  0.0417\n",
      "Restoring best model from epoch 2.\n",
      "     10        \u001b[36m3.1387\u001b[0m        3.2828  0.0411\n",
      "Restoring best model from epoch 8.\n",
      "     10        3.0573        \u001b[32m3.1300\u001b[0m  0.0409\n",
      "Restoring best model from epoch 7.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.1568\u001b[0m        \u001b[32m3.1531\u001b[0m  0.0243\n",
      "      2        3.4106        \u001b[32m3.1321\u001b[0m  0.0271\n",
      "      3        3.2388        \u001b[32m3.1249\u001b[0m  0.0261\n",
      "      4        \u001b[36m3.0682\u001b[0m        3.1390  0.0264\n",
      "      5        \u001b[36m3.0036\u001b[0m        3.1576  0.0264\n",
      "      6        3.0181        3.1637  0.0260\n",
      "      7        3.0250        3.1572  0.0253\n",
      "      8        3.0078        3.1438  0.0248\n",
      "      9        \u001b[36m2.9875\u001b[0m        3.1293  0.0269\n",
      "     10        \u001b[36m2.9803\u001b[0m        3.1271  0.0235\n",
      "Restoring best model from epoch 3.\n",
      "y_train breslow final [ 1.0000000e-01 -7.6666665e-01 -1.2666667e+00 ...  3.3573334e+02\n",
      " -3.3703333e+02  3.5100000e+02]\n",
      "Concordance Index 0.6407836906812377\n",
      "Integrated Brier Score: 0.16843169004017222\n",
      "y_train breslow final [ 1.0000000e-01 -7.6666665e-01 -1.2666667e+00 ...  3.3573334e+02\n",
      " -3.3703333e+02  3.5100000e+02]\n",
      "durations 1.2333333 355.2\n",
      "Concordance Index 0.6574438694513741\n",
      "Integrated Brier Score: 0.15924144349408795\n",
      "(1523, 9) <class 'pandas.core.frame.DataFrame'>\n",
      "(1523,) <class 'pandas.core.series.Series'>\n",
      "(380, 9) <class 'pandas.core.frame.DataFrame'>\n",
      "(380,) <class 'pandas.core.series.Series'>\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m2.8145\u001b[0m  0.0039\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.0927\u001b[0m  0.0040\n",
      "      2        2.8145  0.0040\n",
      "      3        2.8145  0.0036\n",
      "      2        3.0927  0.0068\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.3130\u001b[0m  0.0042\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.8748\u001b[0m        \u001b[32m2.8316\u001b[0m  0.0432\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.0482\u001b[0m        \u001b[32m3.0861\u001b[0m  0.0421\n",
      "      4        2.8145  0.0045\n",
      "      3        3.0927  0.0038\n",
      "      5        2.8145  0.0036\n",
      "      2        3.3130  0.0064\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.1938\u001b[0m        \u001b[32m3.2798\u001b[0m  0.0437\n",
      "      4        3.0927  0.0039\n",
      "      6        2.8145  0.0036\n",
      "      5        3.0927  0.0038\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.1588\u001b[0m  0.0043\n",
      "      3        3.3130  0.0050\n",
      "      7        2.8145  0.0038\n",
      "      8        2.8145  0.0036\n",
      "      4        3.3130  0.0047\n",
      "      6        3.0927  0.0057\n",
      "      2        3.1588  0.0055\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.2645\u001b[0m        \u001b[32m3.1183\u001b[0m  0.0463\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.2744\u001b[0m        \u001b[32m3.2130\u001b[0m  0.0444\n",
      "      7        3.0927  0.0040\n",
      "      9        2.8145  0.0049\n",
      "      5        3.3130  0.0064\n",
      "      3        3.1588  0.0056\n",
      "      8        3.0927  0.0039\n",
      "     10        2.8145  0.0055\n",
      "Restoring best model from epoch 1.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.2367\u001b[0m  0.0067\n",
      "      4        3.1588  0.0048\n",
      "      9        3.0927  0.0037\n",
      "     10        3.0927  0.0037\n",
      "Restoring best model from epoch 1.\n",
      "      2        3.2367  0.0044\n",
      "      6        3.3130  0.0117\n",
      "      3        3.2367  0.0046\n",
      "      5        3.1588  0.0086\n",
      "      7        3.3130  0.0041\n",
      "      4        3.2367  0.0041\n",
      "      6        3.1588  0.0051\n",
      "      8        3.3130  0.0041\n",
      "      2        \u001b[36m3.0107\u001b[0m        3.0906  0.0432\n",
      "      2        \u001b[36m2.8157\u001b[0m        \u001b[32m2.7910\u001b[0m  0.0455\n",
      "      5        3.2367  0.0042\n",
      "      7        3.1588  0.0040\n",
      "      9        3.3130  0.0039\n",
      "      6        3.2367  0.0040\n",
      "      8        3.1588  0.0039\n",
      "     10        3.3130  0.0050\n",
      "Restoring best model from epoch 1.\n",
      "      7        3.2367  0.0039\n",
      "      9        3.1588  0.0043\n",
      "      8        3.2367  0.0040\n",
      "      2        \u001b[36m3.1463\u001b[0m        \u001b[32m3.2633\u001b[0m  0.0517\n",
      "     10        3.1588  0.0039\n",
      "Restoring best model from epoch 1.\n",
      "      9        3.2367  0.0039\n",
      "      2        \u001b[36m3.1960\u001b[0m        \u001b[32m3.1897\u001b[0m  0.0422\n",
      "      2        \u001b[36m3.1992\u001b[0m        \u001b[32m3.0920\u001b[0m  0.0461\n",
      "     10        3.2367  0.0039\n",
      "Restoring best model from epoch 1.\n",
      "      3        \u001b[36m2.9658\u001b[0m        \u001b[32m3.0818\u001b[0m  0.0406\n",
      "      3        \u001b[36m2.7638\u001b[0m        2.8366  0.0410\n",
      "      3        \u001b[36m3.0997\u001b[0m        3.2643  0.0409\n",
      "      3        \u001b[36m3.1620\u001b[0m        3.1938  0.0411\n",
      "      3        \u001b[36m3.1576\u001b[0m        3.0953  0.0408\n",
      "      4        \u001b[36m2.9473\u001b[0m        \u001b[32m3.0745\u001b[0m  0.0410\n",
      "      4        \u001b[36m2.7637\u001b[0m        2.8055  0.0402\n",
      "      4        \u001b[36m3.0885\u001b[0m        3.3033  0.0416\n",
      "      4        \u001b[36m3.1474\u001b[0m        3.1932  0.0412\n",
      "      4        \u001b[36m3.1498\u001b[0m        \u001b[32m3.0799\u001b[0m  0.0410\n",
      "      5        \u001b[36m2.9451\u001b[0m        \u001b[32m3.0637\u001b[0m  0.0405\n",
      "      5        \u001b[36m2.7462\u001b[0m        2.8088  0.0412\n",
      "      5        3.1014        3.2889  0.0414\n",
      "      5        \u001b[36m3.1421\u001b[0m        \u001b[32m3.1853\u001b[0m  0.0413\n",
      "      5        \u001b[36m3.1332\u001b[0m        \u001b[32m3.0796\u001b[0m  0.0415\n",
      "      6        \u001b[36m2.9404\u001b[0m        3.0730  0.0400\n",
      "      6        \u001b[36m2.7348\u001b[0m        2.8103  0.0401\n",
      "      6        \u001b[36m3.0744\u001b[0m        3.2898  0.0407\n",
      "      6        \u001b[36m3.1402\u001b[0m        3.1989  0.0421\n",
      "      6        \u001b[36m3.1278\u001b[0m        \u001b[32m3.0735\u001b[0m  0.0419\n",
      "      7        \u001b[36m2.9194\u001b[0m        \u001b[32m3.0603\u001b[0m  0.0412\n",
      "      7        \u001b[36m2.7281\u001b[0m        2.8079  0.0413\n",
      "      7        \u001b[36m3.0699\u001b[0m        3.2819  0.0412\n",
      "      7        \u001b[36m3.1135\u001b[0m        3.1916  0.0409\n",
      "      7        \u001b[36m3.1078\u001b[0m        3.0952  0.0416\n",
      "      8        \u001b[36m2.7163\u001b[0m        2.8110  0.0403\n",
      "      8        2.9237        \u001b[32m3.0529\u001b[0m  0.0414\n",
      "      8        \u001b[36m3.0526\u001b[0m        3.2771  0.0412\n",
      "      8        3.1339        3.1894  0.0406\n",
      "      8        3.1153        3.0861  0.0413\n",
      "      9        2.7199        2.8146  0.0400\n",
      "      9        \u001b[36m2.9144\u001b[0m        \u001b[32m3.0383\u001b[0m  0.0406\n",
      "      9        \u001b[36m3.0392\u001b[0m        3.2770  0.0411\n",
      "      9        3.1319        3.2017  0.0413\n",
      "      9        3.1134        3.0881  0.0413\n",
      "     10        \u001b[36m2.6982\u001b[0m        2.8193  0.0408\n",
      "Restoring best model from epoch 2.\n",
      "     10        2.9251        3.0439  0.0405\n",
      "Restoring best model from epoch 9.\n",
      "     10        3.0418        3.2762  0.0407\n",
      "Restoring best model from epoch 2.\n",
      "     10        3.1154        3.1909  0.0409\n",
      "Restoring best model from epoch 5.\n",
      "     10        3.1161        3.0863  0.0413\n",
      "Restoring best model from epoch 6.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.1312\u001b[0m        \u001b[32m3.2037\u001b[0m  0.0234\n",
      "      2        3.3336        \u001b[32m3.1920\u001b[0m  0.0261\n",
      "      3        3.1831        3.1922  0.0318\n",
      "      4        \u001b[36m3.0268\u001b[0m        3.1971  0.0291\n",
      "      5        \u001b[36m2.9594\u001b[0m        3.1996  0.0252\n",
      "      6        2.9724        3.2000  0.0237\n",
      "      7        2.9892        3.1948  0.0255\n",
      "      8        2.9785        \u001b[32m3.1866\u001b[0m  0.0238\n",
      "      9        \u001b[36m2.9448\u001b[0m        \u001b[32m3.1793\u001b[0m  0.0251\n",
      "     10        \u001b[36m2.9263\u001b[0m        \u001b[32m3.1754\u001b[0m  0.0234\n",
      "y_train breslow final [ 1.0000000e-01 -7.6666665e-01 -1.2333333e+00 ...  3.3560001e+02\n",
      " -3.3703333e+02  3.5520001e+02]\n",
      "Concordance Index 0.6627292663476874\n",
      "Integrated Brier Score: 0.158956032437661\n",
      "y_train breslow final [ 1.0000000e-01 -7.6666665e-01 -1.2333333e+00 ...  3.3560001e+02\n",
      " -3.3703333e+02  3.5520001e+02]\n",
      "durations 1.2666667 351.0\n",
      "Concordance Index 0.641335014106489\n",
      "Integrated Brier Score: 0.1730156727793618\n",
      "load_flchain\n",
      "split age            float32\n",
      "sex           category\n",
      "sample_yr      float32\n",
      "kappa          float32\n",
      "lambda         float32\n",
      "flc_grp        float32\n",
      "creatinine     float32\n",
      "mgus_0.0         uint8\n",
      "mgus_1.0         uint8\n",
      "dtype: object\n",
      "(6296, 9) <class 'pandas.core.frame.DataFrame'>\n",
      "(6296,) <class 'pandas.core.series.Series'>\n",
      "(1575, 9) <class 'pandas.core.frame.DataFrame'>\n",
      "(1575,) <class 'pandas.core.series.Series'>\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m1.6760\u001b[0m        \u001b[32m1.5239\u001b[0m  0.1041\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.1519\u001b[0m        \u001b[32m2.1929\u001b[0m  0.1126\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.9025\u001b[0m        \u001b[32m2.9041\u001b[0m  0.1446\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.0220\u001b[0m        \u001b[32m2.0909\u001b[0m  0.1868\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.1303\u001b[0m        \u001b[32m3.0862\u001b[0m  0.1477\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m1.5810\u001b[0m        \u001b[32m1.5547\u001b[0m  0.1994\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.7104\u001b[0m        \u001b[32m2.8681\u001b[0m  0.1935\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.1876\u001b[0m        \u001b[32m3.1735\u001b[0m  0.1458\n",
      "      2        \u001b[36m1.5948\u001b[0m        1.5296  0.0963\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.9381\u001b[0m        \u001b[32m3.0199\u001b[0m  0.2159\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.9870\u001b[0m        \u001b[32m3.1253\u001b[0m  0.2388\n",
      "      2        \u001b[36m2.0754\u001b[0m        2.2075  0.1205\n",
      "      3        \u001b[36m1.5512\u001b[0m        1.5396  0.0811\n",
      "      2        \u001b[36m2.8093\u001b[0m        2.9357  0.1267\n",
      "      2        \u001b[36m3.0237\u001b[0m        3.1234  0.1502\n",
      "      2        \u001b[36m3.0676\u001b[0m        3.2170  0.1446\n",
      "      3        \u001b[36m2.0607\u001b[0m        \u001b[32m2.1737\u001b[0m  0.1131\n",
      "      2        \u001b[36m2.0002\u001b[0m        2.0911  0.1903\n",
      "      4        \u001b[36m1.5246\u001b[0m        \u001b[32m1.5185\u001b[0m  0.0951\n",
      "      2        \u001b[36m1.5489\u001b[0m        1.5562  0.2026\n",
      "      2        \u001b[36m2.8987\u001b[0m        \u001b[32m3.0179\u001b[0m  0.1844\n",
      "      3        \u001b[36m2.7831\u001b[0m        2.9143  0.1333\n",
      "      2        \u001b[36m2.6759\u001b[0m        \u001b[32m2.8401\u001b[0m  0.2557\n",
      "      2        \u001b[36m2.9467\u001b[0m        \u001b[32m3.1207\u001b[0m  0.2057\n",
      "      3        \u001b[36m3.0425\u001b[0m        3.1747  0.1359\n",
      "      3        \u001b[36m3.0043\u001b[0m        3.0943  0.1463\n",
      "      5        1.5274        1.5220  0.0936\n",
      "      4        \u001b[36m2.0556\u001b[0m        \u001b[32m2.1357\u001b[0m  0.1204\n",
      "      3        \u001b[36m1.9964\u001b[0m        2.1166  0.1884\n",
      "      4        \u001b[36m2.7673\u001b[0m        \u001b[32m2.8702\u001b[0m  0.1309\n",
      "      6        \u001b[36m1.5184\u001b[0m        1.5328  0.0923\n",
      "      5        \u001b[36m2.0489\u001b[0m        \u001b[32m2.1342\u001b[0m  0.1032\n",
      "      3        \u001b[36m1.5410\u001b[0m        \u001b[32m1.5546\u001b[0m  0.2037\n",
      "      4        \u001b[36m2.9852\u001b[0m        \u001b[32m3.0588\u001b[0m  0.1227\n",
      "      4        \u001b[36m3.0270\u001b[0m        \u001b[32m3.1268\u001b[0m  0.1377\n",
      "      3        \u001b[36m2.8966\u001b[0m        \u001b[32m3.0116\u001b[0m  0.2064\n",
      "      3        \u001b[36m2.6697\u001b[0m        \u001b[32m2.8251\u001b[0m  0.2209\n",
      "      7        \u001b[36m1.5102\u001b[0m        1.5254  0.0973\n",
      "      3        \u001b[36m2.9352\u001b[0m        \u001b[32m3.0989\u001b[0m  0.2230\n",
      "      6        \u001b[36m2.0421\u001b[0m        2.1440  0.1050\n",
      "      5        \u001b[36m2.7670\u001b[0m        \u001b[32m2.8572\u001b[0m  0.1334\n",
      "      5        \u001b[36m2.9850\u001b[0m        \u001b[32m3.0459\u001b[0m  0.1351\n",
      "      4        \u001b[36m1.9927\u001b[0m        2.1172  0.2039\n",
      "      4        \u001b[36m1.5403\u001b[0m        \u001b[32m1.5516\u001b[0m  0.1677\n",
      "      5        \u001b[36m3.0194\u001b[0m        \u001b[32m3.1257\u001b[0m  0.1490\n",
      "      8        \u001b[36m1.5077\u001b[0m        1.5217  0.0978\n",
      "      4        \u001b[36m2.8922\u001b[0m        \u001b[32m3.0102\u001b[0m  0.1803\n",
      "      6        \u001b[36m2.7603\u001b[0m        2.8673  0.1129\n",
      "      7        \u001b[36m2.0405\u001b[0m        2.1403  0.1148\n",
      "      4        \u001b[36m2.9316\u001b[0m        3.1027  0.1875\n",
      "      9        \u001b[36m1.5004\u001b[0m        1.5266  0.0947\n",
      "      6        \u001b[36m2.9772\u001b[0m        3.0503  0.1397\n",
      "      6        \u001b[36m3.0123\u001b[0m        3.1305  0.1231\n",
      "      8        \u001b[36m2.0370\u001b[0m        \u001b[32m2.1230\u001b[0m  0.1009\n",
      "      7        \u001b[36m2.7535\u001b[0m        2.8625  0.1252\n",
      "     10        1.5035        1.5246  0.0927\n",
      "Restoring best model from epoch 4.\n",
      "      5        \u001b[36m1.9924\u001b[0m        \u001b[32m2.0871\u001b[0m  0.2121\n",
      "      5        \u001b[36m1.5371\u001b[0m        \u001b[32m1.5509\u001b[0m  0.2113\n",
      "      4        \u001b[36m2.6616\u001b[0m        2.8304  0.3092\n",
      "      5        \u001b[36m2.8878\u001b[0m        3.0167  0.1841\n",
      "      7        \u001b[36m3.0082\u001b[0m        \u001b[32m3.1243\u001b[0m  0.1315\n",
      "      7        \u001b[36m2.9742\u001b[0m        3.0522  0.1416\n",
      "      9        \u001b[36m2.0349\u001b[0m        \u001b[32m2.1142\u001b[0m  0.1164\n",
      "      8        2.7540        \u001b[32m2.8489\u001b[0m  0.1287\n",
      "      5        \u001b[36m2.9214\u001b[0m        \u001b[32m3.0987\u001b[0m  0.2027\n",
      "      8        \u001b[36m2.9952\u001b[0m        \u001b[32m3.1039\u001b[0m  0.1205\n",
      "     10        \u001b[36m2.0329\u001b[0m        2.1170  0.1145\n",
      "Restoring best model from epoch 9.\n",
      "      8        \u001b[36m2.9680\u001b[0m        \u001b[32m3.0402\u001b[0m  0.1387\n",
      "      6        \u001b[36m1.5355\u001b[0m        1.5549  0.1863\n",
      "      6        2.8951        3.0240  0.1826\n",
      "      6        \u001b[36m1.9839\u001b[0m        2.0932  0.1888\n",
      "      9        \u001b[36m2.7514\u001b[0m        \u001b[32m2.8392\u001b[0m  0.1207\n",
      "      5        \u001b[36m2.6559\u001b[0m        2.8369  0.2062\n",
      "      6        \u001b[36m2.9186\u001b[0m        \u001b[32m3.0973\u001b[0m  0.1802\n",
      "      9        3.0000        \u001b[32m3.0994\u001b[0m  0.1355\n",
      "      9        2.9714        \u001b[32m3.0289\u001b[0m  0.1259\n",
      "     10        \u001b[36m2.7489\u001b[0m        2.8465  0.1182\n",
      "Restoring best model from epoch 9.\n",
      "      7        2.8909        3.0158  0.1715\n",
      "      7        \u001b[36m1.9837\u001b[0m        2.1048  0.1733\n",
      "      7        \u001b[36m1.5325\u001b[0m        1.5602  0.1754\n",
      "      6        2.6560        2.8347  0.1818\n",
      "     10        \u001b[36m2.9932\u001b[0m        3.1058  0.1203\n",
      "Restoring best model from epoch 9.\n",
      "     10        \u001b[36m2.9663\u001b[0m        \u001b[32m3.0285\u001b[0m  0.1181\n",
      "      7        \u001b[36m2.9145\u001b[0m        3.1009  0.1734\n",
      "      8        \u001b[36m1.9822\u001b[0m        2.1011  0.1682\n",
      "      8        \u001b[36m1.5302\u001b[0m        1.5584  0.1681\n",
      "      8        \u001b[36m2.8850\u001b[0m        \u001b[32m3.0068\u001b[0m  0.1711\n",
      "      7        \u001b[36m2.6506\u001b[0m        2.8351  0.1709\n",
      "      8        2.9145        \u001b[32m3.0968\u001b[0m  0.1746\n",
      "      9        1.9856        2.0991  0.1672\n",
      "      9        \u001b[36m1.5295\u001b[0m        1.5536  0.1680\n",
      "      9        2.8881        3.0150  0.1723\n",
      "      8        2.6514        2.8286  0.1692\n",
      "      9        2.9186        3.0974  0.1701\n",
      "     10        \u001b[36m1.5275\u001b[0m        1.5513  0.1657\n",
      "Restoring best model from epoch 5.\n",
      "     10        1.9825        2.1082  0.1682\n",
      "Restoring best model from epoch 5.\n",
      "     10        \u001b[36m2.8836\u001b[0m        3.0116  0.1708\n",
      "Restoring best model from epoch 8.\n",
      "      9        2.6539        2.8259  0.1712\n",
      "     10        2.9180        \u001b[32m3.0963\u001b[0m  0.1725\n",
      "     10        \u001b[36m2.6445\u001b[0m        2.8279  0.1729\n",
      "Restoring best model from epoch 3.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.5916\u001b[0m        \u001b[32m2.6078\u001b[0m  0.0824\n",
      "      2        \u001b[36m2.5067\u001b[0m        2.6093  0.0829\n",
      "      3        \u001b[36m2.4895\u001b[0m        \u001b[32m2.5721\u001b[0m  0.0833\n",
      "      4        \u001b[36m2.4871\u001b[0m        \u001b[32m2.5623\u001b[0m  0.0799\n",
      "      5        \u001b[36m2.4800\u001b[0m        2.5701  0.0837\n",
      "      6        \u001b[36m2.4760\u001b[0m        2.5638  0.0824\n",
      "      7        \u001b[36m2.4728\u001b[0m        \u001b[32m2.5580\u001b[0m  0.0853\n",
      "      8        \u001b[36m2.4678\u001b[0m        \u001b[32m2.5529\u001b[0m  0.0883\n",
      "      9        \u001b[36m2.4642\u001b[0m        2.5563  0.0829\n",
      "     10        2.4651        2.5551  0.0839\n",
      "Restoring best model from epoch 8.\n",
      "y_train breslow final [-1.000e+00  1.000e+00 -1.000e+00 ... -5.171e+03 -5.177e+03 -5.187e+03]\n",
      "Concordance Index 0.7998498044520637\n",
      "Integrated Brier Score: 0.0944203533416943\n",
      "y_train breslow final [-1.000e+00  1.000e+00 -1.000e+00 ... -5.171e+03 -5.177e+03 -5.187e+03]\n",
      "durations 1.0 5215.0\n",
      "Concordance Index 0.8052499343758203\n",
      "Integrated Brier Score: 0.0940596946108105\n",
      "(6297, 9) <class 'pandas.core.frame.DataFrame'>\n",
      "(6297,) <class 'pandas.core.series.Series'>\n",
      "(1574, 9) <class 'pandas.core.frame.DataFrame'>\n",
      "(1574,) <class 'pandas.core.series.Series'>\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.0452\u001b[0m        \u001b[32m2.0968\u001b[0m  0.2537\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m1.5866\u001b[0m        \u001b[32m1.6121\u001b[0m  0.2683\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.9607\u001b[0m        \u001b[32m2.9588\u001b[0m  0.2525\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.0354\u001b[0m        \u001b[32m3.0001\u001b[0m  0.2640\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.6999\u001b[0m        \u001b[32m2.7281\u001b[0m  0.2778\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m1.6192\u001b[0m        \u001b[32m1.6257\u001b[0m  0.3258\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.0675\u001b[0m        \u001b[32m2.2332\u001b[0m  0.3322\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.9873\u001b[0m        \u001b[32m3.0467\u001b[0m  0.3243\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.0792\u001b[0m        \u001b[32m3.0896\u001b[0m  0.3285\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.7337\u001b[0m        \u001b[32m2.8201\u001b[0m  0.3707\n",
      "      2        \u001b[36m2.0227\u001b[0m        \u001b[32m2.0881\u001b[0m  0.2469\n",
      "      2        \u001b[36m1.5596\u001b[0m        \u001b[32m1.5979\u001b[0m  0.2754\n",
      "      2        \u001b[36m2.9338\u001b[0m        \u001b[32m2.9446\u001b[0m  0.2735\n",
      "      2        \u001b[36m2.9982\u001b[0m        \u001b[32m2.9771\u001b[0m  0.2703\n",
      "      2        \u001b[36m2.6665\u001b[0m        \u001b[32m2.7214\u001b[0m  0.2997\n",
      "      2        \u001b[36m1.5509\u001b[0m        \u001b[32m1.6171\u001b[0m  0.3086\n",
      "      2        \u001b[36m2.0158\u001b[0m        \u001b[32m2.1591\u001b[0m  0.3277\n",
      "      2        \u001b[36m2.9345\u001b[0m        \u001b[32m2.9672\u001b[0m  0.3272\n",
      "      2        \u001b[36m2.9872\u001b[0m        \u001b[32m3.0006\u001b[0m  0.3607\n",
      "      2        \u001b[36m2.6537\u001b[0m        \u001b[32m2.7590\u001b[0m  0.3394\n",
      "      3        \u001b[36m2.0171\u001b[0m        2.0896  0.2643\n",
      "      3        \u001b[36m1.5516\u001b[0m        1.5987  0.2496\n",
      "      3        \u001b[36m2.9880\u001b[0m        2.9804  0.2614\n",
      "      3        \u001b[36m2.9191\u001b[0m        \u001b[32m2.9395\u001b[0m  0.2843\n",
      "      3        \u001b[36m2.6652\u001b[0m        2.7504  0.2593\n",
      "      3        \u001b[36m1.5350\u001b[0m        \u001b[32m1.6048\u001b[0m  0.3444\n",
      "      3        \u001b[36m2.0032\u001b[0m        \u001b[32m2.1485\u001b[0m  0.3166\n",
      "      4        2.0178        2.0915  0.2702\n",
      "      3        \u001b[36m2.9165\u001b[0m        \u001b[32m2.9374\u001b[0m  0.3314\n",
      "      4        1.5586        \u001b[32m1.5954\u001b[0m  0.2653\n",
      "      4        \u001b[36m2.9149\u001b[0m        \u001b[32m2.9362\u001b[0m  0.2527\n",
      "      4        \u001b[36m2.9842\u001b[0m        2.9789  0.2707\n",
      "      3        \u001b[36m2.6521\u001b[0m        2.7732  0.3414\n",
      "      4        \u001b[36m2.6641\u001b[0m        \u001b[32m2.7110\u001b[0m  0.2486\n",
      "      3        \u001b[36m2.9683\u001b[0m        \u001b[32m2.9860\u001b[0m  0.3492\n",
      "      5        \u001b[36m2.0062\u001b[0m        2.0929  0.2582\n",
      "      5        2.9199        2.9390  0.2447\n",
      "      4        \u001b[36m1.5263\u001b[0m        \u001b[32m1.5879\u001b[0m  0.3229\n",
      "      4        \u001b[36m1.9991\u001b[0m        \u001b[32m2.1340\u001b[0m  0.3135\n",
      "      5        \u001b[36m1.5492\u001b[0m        1.5999  0.2785\n",
      "      5        \u001b[36m2.9774\u001b[0m        2.9798  0.2662\n",
      "      5        2.6650        2.7134  0.2722\n",
      "      4        \u001b[36m2.9138\u001b[0m        2.9433  0.3490\n",
      "      4        \u001b[36m2.9607\u001b[0m        \u001b[32m2.9849\u001b[0m  0.3170\n",
      "      4        \u001b[36m2.6476\u001b[0m        2.7683  0.3631\n",
      "      6        \u001b[36m2.9094\u001b[0m        \u001b[32m2.9318\u001b[0m  0.2366\n",
      "      6        \u001b[36m1.9949\u001b[0m        2.0939  0.2859\n",
      "      6        \u001b[36m2.6485\u001b[0m        \u001b[32m2.7099\u001b[0m  0.2405\n",
      "      6        1.5493        1.6045  0.2810\n",
      "      6        2.9780        \u001b[32m2.9662\u001b[0m  0.2808\n",
      "      5        \u001b[36m1.5205\u001b[0m        1.5996  0.3093\n",
      "      5        \u001b[36m1.9922\u001b[0m        \u001b[32m2.1299\u001b[0m  0.3392\n",
      "      5        \u001b[36m2.9049\u001b[0m        2.9579  0.3604\n",
      "      5        \u001b[36m2.9570\u001b[0m        2.9924  0.3445\n",
      "      5        2.6601        2.7623  0.3213\n",
      "      7        2.9188        \u001b[32m2.9297\u001b[0m  0.2612\n",
      "      7        \u001b[36m1.9937\u001b[0m        2.0906  0.2907\n",
      "      7        2.6546        2.7102  0.2679\n",
      "      7        \u001b[36m1.5431\u001b[0m        1.6019  0.2712\n",
      "      7        \u001b[36m2.9695\u001b[0m        2.9677  0.2977\n",
      "      6        1.5209        1.5998  0.3378\n",
      "      6        \u001b[36m1.9836\u001b[0m        \u001b[32m2.1242\u001b[0m  0.3200\n",
      "      6        2.6636        \u001b[32m2.7452\u001b[0m  0.3025\n",
      "      8        1.9954        2.0894  0.2520\n",
      "      6        2.9132        2.9600  0.3758\n",
      "      8        \u001b[36m2.6432\u001b[0m        2.7117  0.2521\n",
      "      8        \u001b[36m2.9086\u001b[0m        2.9315  0.3148\n",
      "      6        2.9577        2.9967  0.3829\n",
      "      8        1.5436        1.6004  0.2716\n",
      "      8        2.9722        2.9799  0.2617\n",
      "      7        \u001b[36m1.5154\u001b[0m        1.5947  0.3270\n",
      "      7        1.9838        \u001b[32m2.1175\u001b[0m  0.3430\n",
      "      9        2.0033        \u001b[32m2.0864\u001b[0m  0.2518\n",
      "      9        2.9136        2.9298  0.2507\n",
      "      9        2.6471        2.7140  0.2732\n",
      "      9        1.5460        1.5964  0.2462\n",
      "      7        2.6476        \u001b[32m2.7322\u001b[0m  0.3546\n",
      "      9        2.9753        2.9691  0.2630\n",
      "      7        2.9209        2.9741  0.3460\n",
      "      7        2.9575        2.9966  0.3547\n",
      "     10        \u001b[36m1.9923\u001b[0m        \u001b[32m2.0841\u001b[0m  0.2282\n",
      "      8        1.5177        1.5898  0.3271\n",
      "     10        \u001b[36m2.6423\u001b[0m        2.7223  0.2394\n",
      "Restoring best model from epoch 6.\n",
      "     10        \u001b[36m2.9080\u001b[0m        2.9401  0.2628\n",
      "Restoring best model from epoch 7.\n",
      "      8        1.9862        2.1271  0.3195\n",
      "     10        1.5436        1.6025  0.2967\n",
      "Restoring best model from epoch 4.\n",
      "     10        \u001b[36m2.9639\u001b[0m        \u001b[32m2.9659\u001b[0m  0.2731\n",
      "      8        \u001b[36m2.6386\u001b[0m        2.7369  0.3092\n",
      "      8        2.9167        2.9718  0.3326\n",
      "      8        \u001b[36m2.9565\u001b[0m        2.9885  0.3145\n",
      "      9        \u001b[36m1.5122\u001b[0m        1.5952  0.2808\n",
      "      9        1.9836        2.1186  0.2909\n",
      "      9        2.6389        \u001b[32m2.7257\u001b[0m  0.2899\n",
      "      9        2.9194        2.9533  0.2891\n",
      "      9        \u001b[36m2.9452\u001b[0m        2.9858  0.2916\n",
      "     10        1.5128        1.5959  0.2811\n",
      "Restoring best model from epoch 4.\n",
      "     10        1.9847        2.1218  0.2843\n",
      "Restoring best model from epoch 7.\n",
      "     10        \u001b[36m2.6308\u001b[0m        \u001b[32m2.7248\u001b[0m  0.2875\n",
      "     10        \u001b[36m2.8940\u001b[0m        2.9461  0.2871\n",
      "Restoring best model from epoch 3.\n",
      "     10        2.9470        2.9933  0.2860\n",
      "Restoring best model from epoch 4.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.5190\u001b[0m        \u001b[32m2.3549\u001b[0m  0.3027\n",
      "      2        \u001b[36m2.4929\u001b[0m        \u001b[32m2.3548\u001b[0m  0.2955\n",
      "      3        \u001b[36m2.4812\u001b[0m        2.3670  0.2867\n",
      "      4        \u001b[36m2.4801\u001b[0m        2.3563  0.2968\n",
      "      5        2.4845        \u001b[32m2.3463\u001b[0m  0.2991\n",
      "      6        \u001b[36m2.4709\u001b[0m        2.3536  0.2983\n",
      "      7        2.4721        2.3592  0.2938\n",
      "      8        2.4719        2.3598  0.3020\n",
      "      9        \u001b[36m2.4687\u001b[0m        2.3676  0.2938\n",
      "     10        2.4723        2.3696  0.2979\n",
      "Restoring best model from epoch 5.\n",
      "y_train breslow final [-1.000e+00  1.000e+00 -1.000e+00 ... -5.177e+03 -5.187e+03 -5.215e+03]\n",
      "Concordance Index 0.7982468850116696\n",
      "Integrated Brier Score: 0.1231460970803922\n",
      "y_train breslow final [-1.000e+00  1.000e+00 -1.000e+00 ... -5.177e+03 -5.187e+03 -5.215e+03]\n",
      "durations 1.0 5123.0\n",
      "Concordance Index 0.8042351717927533\n",
      "Integrated Brier Score: 0.124962932887525\n",
      "(6297, 9) <class 'pandas.core.frame.DataFrame'>\n",
      "(6297,) <class 'pandas.core.series.Series'>\n",
      "(1574, 9) <class 'pandas.core.frame.DataFrame'>\n",
      "(1574,) <class 'pandas.core.series.Series'>\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.0857\u001b[0m        \u001b[32m1.9341\u001b[0m  0.2319\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m1.5743\u001b[0m        \u001b[32m1.5938\u001b[0m  0.2554\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.9519\u001b[0m        \u001b[32m2.9274\u001b[0m  0.2476\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.7300\u001b[0m        \u001b[32m2.6309\u001b[0m  0.2507\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.0221\u001b[0m        \u001b[32m3.0045\u001b[0m  0.2707\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m1.5957\u001b[0m        \u001b[32m1.6042\u001b[0m  0.3251\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.1248\u001b[0m        \u001b[32m1.9750\u001b[0m  0.3346\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.7558\u001b[0m        \u001b[32m2.6924\u001b[0m  0.3301\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.0361\u001b[0m        \u001b[32m3.0355\u001b[0m  0.3435\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.9833\u001b[0m        \u001b[32m2.9335\u001b[0m  0.3838\n",
      "      2        \u001b[36m1.5510\u001b[0m        \u001b[32m1.5909\u001b[0m  0.2400\n",
      "      2        \u001b[36m2.9239\u001b[0m        2.9349  0.2497\n",
      "      2        \u001b[36m2.6954\u001b[0m        \u001b[32m2.6297\u001b[0m  0.2547\n",
      "      2        \u001b[36m2.0688\u001b[0m        1.9393  0.2894\n",
      "      2        \u001b[36m2.9838\u001b[0m        3.0057  0.2815\n",
      "      2        \u001b[36m2.0727\u001b[0m        \u001b[32m1.9581\u001b[0m  0.3086\n",
      "      2        \u001b[36m1.5382\u001b[0m        \u001b[32m1.6035\u001b[0m  0.3569\n",
      "      2        \u001b[36m2.9037\u001b[0m        2.9425  0.3140\n",
      "      2        \u001b[36m2.7003\u001b[0m        \u001b[32m2.6662\u001b[0m  0.3748\n",
      "      3        \u001b[36m1.5395\u001b[0m        \u001b[32m1.5891\u001b[0m  0.2602\n",
      "      2        \u001b[36m2.9548\u001b[0m        \u001b[32m3.0136\u001b[0m  0.3454\n",
      "      3        \u001b[36m2.9190\u001b[0m        \u001b[32m2.9216\u001b[0m  0.2553\n",
      "      3        \u001b[36m2.6890\u001b[0m        \u001b[32m2.6227\u001b[0m  0.2529\n",
      "      3        \u001b[36m2.0617\u001b[0m        1.9345  0.2677\n",
      "      3        \u001b[36m2.9749\u001b[0m        \u001b[32m2.9961\u001b[0m  0.2408\n",
      "      4        \u001b[36m2.9069\u001b[0m        2.9328  0.2379\n",
      "      3        \u001b[36m2.0590\u001b[0m        \u001b[32m1.9533\u001b[0m  0.3349\n",
      "      4        1.5474        1.5925  0.2740\n",
      "      3        \u001b[36m1.5338\u001b[0m        \u001b[32m1.5800\u001b[0m  0.3226\n",
      "      4        \u001b[36m2.6818\u001b[0m        2.6394  0.2659\n",
      "      4        \u001b[36m2.9643\u001b[0m        2.9969  0.2572\n",
      "      4        \u001b[36m2.0571\u001b[0m        \u001b[32m1.9245\u001b[0m  0.2860\n",
      "      3        2.9045        2.9540  0.3463\n",
      "      3        \u001b[36m2.6849\u001b[0m        2.6663  0.3418\n",
      "      3        \u001b[36m2.9491\u001b[0m        3.0309  0.3504\n",
      "      5        2.9078        2.9386  0.2788\n",
      "      5        \u001b[36m1.5353\u001b[0m        \u001b[32m1.5879\u001b[0m  0.2602\n",
      "      5        2.6830        2.6409  0.2774\n",
      "      4        \u001b[36m1.5270\u001b[0m        \u001b[32m1.5782\u001b[0m  0.3064\n",
      "      5        2.9698        \u001b[32m2.9921\u001b[0m  0.2727\n",
      "      5        \u001b[36m2.0537\u001b[0m        1.9429  0.2699\n",
      "      4        2.0637        1.9758  0.3319\n",
      "      4        2.9072        \u001b[32m2.9322\u001b[0m  0.3168\n",
      "      4        2.6869        \u001b[32m2.6377\u001b[0m  0.3687\n",
      "      4        2.9498        \u001b[32m3.0129\u001b[0m  0.3600\n",
      "      6        2.9091        2.9241  0.2494\n",
      "      6        1.5354        1.5941  0.2613\n",
      "      6        2.6848        2.6367  0.2576\n",
      "      6        \u001b[36m2.0499\u001b[0m        \u001b[32m1.9244\u001b[0m  0.2434\n",
      "      6        \u001b[36m2.9632\u001b[0m        2.9933  0.2857\n",
      "      5        2.0593        1.9672  0.3335\n",
      "      5        \u001b[36m1.5258\u001b[0m        1.5814  0.3446\n",
      "      5        2.9103        \u001b[32m2.9289\u001b[0m  0.3544\n",
      "      5        \u001b[36m2.6757\u001b[0m        2.6532  0.3068\n",
      "      7        1.5376        1.5941  0.2287\n",
      "      7        \u001b[36m2.9039\u001b[0m        2.9229  0.2443\n",
      "      5        \u001b[36m2.9468\u001b[0m        \u001b[32m3.0065\u001b[0m  0.3458\n",
      "      7        2.0538        1.9282  0.2865\n",
      "      7        \u001b[36m2.9518\u001b[0m        2.9970  0.2491\n",
      "      7        \u001b[36m2.6663\u001b[0m        2.6309  0.3378\n",
      "      6        \u001b[36m1.5191\u001b[0m        \u001b[32m1.5723\u001b[0m  0.3143\n",
      "      6        \u001b[36m2.0557\u001b[0m        1.9596  0.3404\n",
      "      8        \u001b[36m2.8961\u001b[0m        2.9226  0.2415\n",
      "      8        \u001b[36m1.5333\u001b[0m        1.5941  0.2596\n",
      "      6        \u001b[36m2.6746\u001b[0m        2.6695  0.3120\n",
      "      6        \u001b[36m2.9037\u001b[0m        2.9314  0.3276\n",
      "      6        \u001b[36m2.9394\u001b[0m        \u001b[32m3.0015\u001b[0m  0.3271\n",
      "      8        2.9583        3.0045  0.2704\n",
      "      8        2.0535        1.9369  0.2794\n",
      "      8        2.6750        2.6460  0.2566\n",
      "      9        2.9015        2.9269  0.2614\n",
      "      9        \u001b[36m1.5286\u001b[0m        1.5906  0.2517\n",
      "      7        \u001b[36m1.5159\u001b[0m        1.5794  0.3318\n",
      "      7        \u001b[36m2.0534\u001b[0m        1.9609  0.3349\n",
      "      9        \u001b[36m2.0457\u001b[0m        1.9317  0.2411\n",
      "      7        \u001b[36m2.6728\u001b[0m        2.6721  0.3266\n",
      "      9        2.6769        2.6317  0.2571\n",
      "      7        \u001b[36m2.8942\u001b[0m        2.9415  0.3398\n",
      "      9        \u001b[36m2.9494\u001b[0m        \u001b[32m2.9839\u001b[0m  0.2867\n",
      "      7        2.9451        \u001b[32m2.9988\u001b[0m  0.3213\n",
      "     10        1.5295        1.5900  0.2407\n",
      "Restoring best model from epoch 5.\n",
      "     10        2.8991        \u001b[32m2.9171\u001b[0m  0.2821\n",
      "     10        \u001b[36m2.0425\u001b[0m        1.9329  0.2425\n",
      "Restoring best model from epoch 4.\n",
      "      8        \u001b[36m1.5108\u001b[0m        1.5760  0.3298\n",
      "     10        2.9517        2.9899  0.2392\n",
      "Restoring best model from epoch 9.\n",
      "     10        \u001b[36m2.6638\u001b[0m        2.6283  0.2532\n",
      "      8        2.0536        1.9598  0.3271\n",
      "Restoring best model from epoch 3.\n",
      "      8        \u001b[36m2.6693\u001b[0m        2.6444  0.3291\n",
      "      8        2.8990        2.9455  0.3059\n",
      "      8        \u001b[36m2.9374\u001b[0m        \u001b[32m2.9899\u001b[0m  0.3120\n",
      "      9        1.5114        1.5754  0.2753\n",
      "      9        \u001b[36m2.0495\u001b[0m        1.9619  0.2827\n",
      "      9        \u001b[36m2.6597\u001b[0m        2.6464  0.2879\n",
      "      9        \u001b[36m2.8841\u001b[0m        2.9351  0.2857\n",
      "      9        2.9397        2.9985  0.2861\n",
      "     10        1.5120        1.5784  0.2768\n",
      "Restoring best model from epoch 6.\n",
      "     10        \u001b[36m2.0463\u001b[0m        \u001b[32m1.9516\u001b[0m  0.2825\n",
      "     10        2.8871        \u001b[32m2.9278\u001b[0m  0.2859\n",
      "     10        \u001b[36m2.6558\u001b[0m        2.6498  0.2906\n",
      "Restoring best model from epoch 4.\n",
      "     10        2.9384        3.0085  0.2877\n",
      "Restoring best model from epoch 8.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.5128\u001b[0m        \u001b[32m2.4265\u001b[0m  0.2600\n",
      "      2        \u001b[36m2.4651\u001b[0m        2.4272  0.2339\n",
      "      3        \u001b[36m2.4540\u001b[0m        \u001b[32m2.4216\u001b[0m  0.2486\n",
      "      4        2.4560        \u001b[32m2.4131\u001b[0m  0.2547\n",
      "      5        2.4577        \u001b[32m2.4107\u001b[0m  0.2447\n",
      "      6        \u001b[36m2.4514\u001b[0m        \u001b[32m2.4067\u001b[0m  0.2447\n",
      "      7        \u001b[36m2.4467\u001b[0m        \u001b[32m2.4055\u001b[0m  0.2390\n",
      "      8        \u001b[36m2.4405\u001b[0m        2.4083  0.2415\n",
      "      9        2.4447        2.4192  0.2513\n",
      "     10        2.4507        2.4146  0.2447\n",
      "Restoring best model from epoch 7.\n",
      "y_train breslow final [-1.000e+00 -1.000e+00  1.000e+00 ... -5.177e+03 -5.187e+03 -5.215e+03]\n",
      "Concordance Index 0.8036422441207061\n",
      "Integrated Brier Score: 0.10283850946827922\n",
      "y_train breslow final [-1.000e+00 -1.000e+00  1.000e+00 ... -5.177e+03 -5.187e+03 -5.215e+03]\n",
      "durations 1.0 5166.0\n",
      "Concordance Index 0.7904393851534985\n",
      "Integrated Brier Score: 0.10520581074152907\n",
      "(6297, 9) <class 'pandas.core.frame.DataFrame'>\n",
      "(6297,) <class 'pandas.core.series.Series'>\n",
      "(1574, 9) <class 'pandas.core.frame.DataFrame'>\n",
      "(1574,) <class 'pandas.core.series.Series'>\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m1.5193\u001b[0m        \u001b[32m1.7300\u001b[0m  0.2497\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.7063\u001b[0m        \u001b[32m2.6596\u001b[0m  0.2493\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.0236\u001b[0m        \u001b[32m2.1250\u001b[0m  0.2561\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.0114\u001b[0m        \u001b[32m2.9377\u001b[0m  0.2439\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.9441\u001b[0m        \u001b[32m2.8974\u001b[0m  0.2794\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.7339\u001b[0m        \u001b[32m2.6928\u001b[0m  0.3358\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m1.5500\u001b[0m        \u001b[32m1.7667\u001b[0m  0.3474\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.0752\u001b[0m        \u001b[32m2.1809\u001b[0m  0.3451\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.9804\u001b[0m        \u001b[32m2.9229\u001b[0m  0.3504\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.0337\u001b[0m        \u001b[32m3.0117\u001b[0m  0.3430\n",
      "      2        \u001b[36m2.6870\u001b[0m        \u001b[32m2.6530\u001b[0m  0.2474\n",
      "      2        \u001b[36m1.9969\u001b[0m        \u001b[32m2.1217\u001b[0m  0.2440\n",
      "      2        \u001b[36m1.5038\u001b[0m        1.7522  0.2710\n",
      "      2        \u001b[36m2.9814\u001b[0m        2.9437  0.2640\n",
      "      2        \u001b[36m2.9146\u001b[0m        2.9070  0.2969\n",
      "      2        \u001b[36m2.6724\u001b[0m        2.7059  0.3339\n",
      "      2        \u001b[36m1.4855\u001b[0m        \u001b[32m1.7460\u001b[0m  0.3317\n",
      "      2        \u001b[36m2.0205\u001b[0m        \u001b[32m2.1768\u001b[0m  0.3381\n",
      "      2        \u001b[36m2.9626\u001b[0m        \u001b[32m2.9486\u001b[0m  0.3302\n",
      "      2        \u001b[36m2.9030\u001b[0m        \u001b[32m2.9058\u001b[0m  0.3605\n",
      "      3        \u001b[36m1.4861\u001b[0m        \u001b[32m1.7289\u001b[0m  0.2382\n",
      "      3        \u001b[36m1.9837\u001b[0m        2.1218  0.2570\n",
      "      3        \u001b[36m2.6765\u001b[0m        2.6822  0.2708\n",
      "      3        \u001b[36m2.9789\u001b[0m        \u001b[32m2.9336\u001b[0m  0.2532\n",
      "      3        2.9156        2.8999  0.2641\n",
      "      4        1.4919        1.7345  0.2557\n",
      "      3        \u001b[36m2.6670\u001b[0m        \u001b[32m2.6849\u001b[0m  0.3191\n",
      "      4        1.9892        \u001b[32m2.1189\u001b[0m  0.2602\n",
      "      4        \u001b[36m2.9707\u001b[0m        2.9445  0.2496\n",
      "      4        \u001b[36m2.6674\u001b[0m        2.6581  0.2561\n",
      "      3        1.4888        \u001b[32m1.7218\u001b[0m  0.3697\n",
      "      3        \u001b[36m2.9519\u001b[0m        \u001b[32m2.9301\u001b[0m  0.3415\n",
      "      3        \u001b[36m2.0035\u001b[0m        \u001b[32m2.1526\u001b[0m  0.3625\n",
      "      3        \u001b[36m2.8945\u001b[0m        2.9154  0.3483\n",
      "      4        \u001b[36m2.9105\u001b[0m        \u001b[32m2.8894\u001b[0m  0.2527\n",
      "      5        \u001b[36m1.4824\u001b[0m        1.7310  0.2486\n",
      "      5        2.6738        2.6601  0.2442\n",
      "      5        \u001b[36m1.9834\u001b[0m        2.1476  0.2662\n",
      "      5        \u001b[36m2.9647\u001b[0m        \u001b[32m2.9293\u001b[0m  0.2648\n",
      "      4        \u001b[36m2.6609\u001b[0m        \u001b[32m2.6696\u001b[0m  0.3276\n",
      "      5        \u001b[36m2.9097\u001b[0m        2.8945  0.2953\n",
      "      4        \u001b[36m2.9398\u001b[0m        2.9382  0.3261\n",
      "      4        \u001b[36m1.4793\u001b[0m        \u001b[32m1.7177\u001b[0m  0.3330\n",
      "      4        \u001b[36m1.9911\u001b[0m        \u001b[32m2.1486\u001b[0m  0.3395\n",
      "      4        \u001b[36m2.8931\u001b[0m        \u001b[32m2.9043\u001b[0m  0.3523\n",
      "      6        1.9993        2.1384  0.2579\n",
      "      6        \u001b[36m2.6659\u001b[0m        2.6673  0.2706\n",
      "      6        1.4833        1.7305  0.2913\n",
      "      6        \u001b[36m2.9565\u001b[0m        2.9361  0.2729\n",
      "      6        \u001b[36m2.9012\u001b[0m        2.9099  0.2716\n",
      "      5        \u001b[36m2.6552\u001b[0m        2.6888  0.3392\n",
      "      5        \u001b[36m1.4701\u001b[0m        \u001b[32m1.7073\u001b[0m  0.3216\n",
      "      5        \u001b[36m1.9909\u001b[0m        \u001b[32m2.1432\u001b[0m  0.3190\n",
      "      5        2.9435        2.9373  0.3586\n",
      "      5        \u001b[36m2.8863\u001b[0m        \u001b[32m2.8965\u001b[0m  0.3369\n",
      "      7        2.6701        2.6596  0.2527\n",
      "      7        \u001b[36m1.4819\u001b[0m        \u001b[32m1.7259\u001b[0m  0.2540\n",
      "      7        1.9839        2.1354  0.2615\n",
      "      7        2.9685        \u001b[32m2.9236\u001b[0m  0.2522\n",
      "      7        \u001b[36m2.9010\u001b[0m        2.8910  0.2767\n",
      "      6        \u001b[36m2.6539\u001b[0m        2.6804  0.3337\n",
      "      6        \u001b[36m1.4668\u001b[0m        1.7142  0.3110\n",
      "      6        \u001b[36m1.9822\u001b[0m        2.1453  0.3270\n",
      "      8        1.4822        1.7308  0.2597\n",
      "      8        \u001b[36m2.6574\u001b[0m        2.6670  0.2715\n",
      "      8        2.9614        2.9312  0.2608\n",
      "      8        \u001b[36m1.9819\u001b[0m        2.1205  0.2732\n",
      "      6        \u001b[36m2.9349\u001b[0m        2.9312  0.3256\n",
      "      6        \u001b[36m2.8824\u001b[0m        2.9069  0.3838\n",
      "      8        \u001b[36m2.9004\u001b[0m        2.8939  0.2559\n",
      "      9        1.4834        1.7272  0.2467\n",
      "      9        1.9845        2.1246  0.2562\n",
      "      7        \u001b[36m2.6494\u001b[0m        2.6786  0.3149\n",
      "      9        2.6574        2.6650  0.2753\n",
      "      7        \u001b[36m1.4662\u001b[0m        1.7131  0.3056\n",
      "      9        2.9585        2.9329  0.2940\n",
      "      7        \u001b[36m1.9817\u001b[0m        \u001b[32m2.1364\u001b[0m  0.3270\n",
      "      7        \u001b[36m2.9319\u001b[0m        2.9324  0.3507\n",
      "      9        2.9052        2.9017  0.2596\n",
      "      7        2.8837        2.9151  0.3266\n",
      "     10        \u001b[36m1.4780\u001b[0m        1.7291  0.2558\n",
      "Restoring best model from epoch 7.\n",
      "     10        \u001b[36m2.6451\u001b[0m        2.6631  0.2640\n",
      "Restoring best model from epoch 2.\n",
      "     10        \u001b[36m1.9803\u001b[0m        2.1245  0.2761\n",
      "Restoring best model from epoch 4.\n",
      "     10        \u001b[36m2.9453\u001b[0m        2.9298  0.2534\n",
      "Restoring best model from epoch 7.\n",
      "      8        \u001b[36m1.4615\u001b[0m        1.7137  0.3067\n",
      "      8        \u001b[36m2.6480\u001b[0m        \u001b[32m2.6636\u001b[0m  0.3441\n",
      "     10        \u001b[36m2.8908\u001b[0m        2.8957  0.2433\n",
      "Restoring best model from epoch 4.\n",
      "      8        1.9833        2.1540  0.3220\n",
      "      8        2.9355        2.9424  0.3079\n",
      "      8        2.8947        2.9052  0.3227\n",
      "      9        \u001b[36m1.4577\u001b[0m        1.7188  0.2773\n",
      "      9        \u001b[36m2.6423\u001b[0m        2.6759  0.2847\n",
      "      9        1.9822        2.1632  0.2824\n",
      "      9        \u001b[36m2.9237\u001b[0m        2.9338  0.2863\n",
      "      9        2.8831        2.9034  0.2854\n",
      "     10        1.4631        1.7174  0.2764\n",
      "Restoring best model from epoch 5.\n",
      "     10        \u001b[36m2.6346\u001b[0m        2.6801  0.2843\n",
      "Restoring best model from epoch 8.\n",
      "     10        1.9906        2.1431  0.2824\n",
      "Restoring best model from epoch 7.\n",
      "     10        \u001b[36m2.9179\u001b[0m        2.9334  0.2862\n",
      "Restoring best model from epoch 3.\n",
      "     10        \u001b[36m2.8802\u001b[0m        2.9003  0.2842\n",
      "Restoring best model from epoch 5.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.4791\u001b[0m        \u001b[32m2.5365\u001b[0m  0.2516\n",
      "      2        \u001b[36m2.4517\u001b[0m        \u001b[32m2.5277\u001b[0m  0.2522\n",
      "      3        \u001b[36m2.4308\u001b[0m        2.5324  0.2661\n",
      "      4        \u001b[36m2.4238\u001b[0m        \u001b[32m2.5078\u001b[0m  0.2436\n",
      "      5        \u001b[36m2.4188\u001b[0m        \u001b[32m2.5025\u001b[0m  0.2480\n",
      "      6        2.4213        2.5082  0.2554\n",
      "      7        2.4228        2.5064  0.2374\n",
      "      8        2.4226        2.5113  0.2494\n",
      "      9        2.4190        2.5042  0.2520\n",
      "     10        \u001b[36m2.4126\u001b[0m        \u001b[32m2.5012\u001b[0m  0.2410\n",
      "y_train breslow final [-1.000e+00 -1.000e+00  1.000e+00 ... -5.177e+03 -5.187e+03 -5.215e+03]\n",
      "Concordance Index 0.8095003164823741\n",
      "Integrated Brier Score: 0.13125218337929184\n",
      "y_train breslow final [-1.000e+00 -1.000e+00  1.000e+00 ... -5.177e+03 -5.187e+03 -5.215e+03]\n",
      "durations 1.0 5171.0\n",
      "Concordance Index 0.777431343974876\n",
      "Integrated Brier Score: 0.13379714680711852\n",
      "(6297, 9) <class 'pandas.core.frame.DataFrame'>\n",
      "(6297,) <class 'pandas.core.series.Series'>\n",
      "(1574, 9) <class 'pandas.core.frame.DataFrame'>\n",
      "(1574,) <class 'pandas.core.series.Series'>\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.0815\u001b[0m        \u001b[32m2.0528\u001b[0m  0.2356\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m1.5401\u001b[0m        \u001b[32m1.5939\u001b[0m  0.2648\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.7329\u001b[0m        \u001b[32m2.6588\u001b[0m  0.2694\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.0259\u001b[0m        \u001b[32m2.9393\u001b[0m  0.2603\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.9757\u001b[0m        \u001b[32m2.9065\u001b[0m  0.2674\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m1.5838\u001b[0m        \u001b[32m1.6084\u001b[0m  0.2988\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.1100\u001b[0m        \u001b[32m2.1323\u001b[0m  0.3374\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.7777\u001b[0m        \u001b[32m2.7433\u001b[0m  0.3405\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.0555\u001b[0m        \u001b[32m2.9461\u001b[0m  0.3379\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.0156\u001b[0m        \u001b[32m3.0312\u001b[0m  0.3867\n",
      "      2        \u001b[36m2.0441\u001b[0m        \u001b[32m2.0499\u001b[0m  0.2368\n",
      "      2        \u001b[36m1.5205\u001b[0m        1.6036  0.2556\n",
      "      2        \u001b[36m2.9947\u001b[0m        \u001b[32m2.9223\u001b[0m  0.2653\n",
      "      2        \u001b[36m2.7027\u001b[0m        \u001b[32m2.6440\u001b[0m  0.2763\n",
      "      2        \u001b[36m2.9449\u001b[0m        \u001b[32m2.9015\u001b[0m  0.2745\n",
      "      2        \u001b[36m1.5184\u001b[0m        \u001b[32m1.5911\u001b[0m  0.3431\n",
      "      2        \u001b[36m2.7056\u001b[0m        \u001b[32m2.6853\u001b[0m  0.3202\n",
      "      2        \u001b[36m2.0536\u001b[0m        \u001b[32m2.1009\u001b[0m  0.3411\n",
      "      3        \u001b[36m2.0433\u001b[0m        \u001b[32m2.0479\u001b[0m  0.2558\n",
      "      2        \u001b[36m2.9343\u001b[0m        \u001b[32m2.9370\u001b[0m  0.3140\n",
      "      2        \u001b[36m2.9594\u001b[0m        \u001b[32m2.9425\u001b[0m  0.3601\n",
      "      3        \u001b[36m1.5149\u001b[0m        \u001b[32m1.5935\u001b[0m  0.2460\n",
      "      3        \u001b[36m2.9252\u001b[0m        2.9104  0.2643\n",
      "      3        \u001b[36m2.7021\u001b[0m        \u001b[32m2.6367\u001b[0m  0.2720\n",
      "      3        \u001b[36m2.9712\u001b[0m        \u001b[32m2.9101\u001b[0m  0.3080\n",
      "      4        \u001b[36m2.0409\u001b[0m        \u001b[32m2.0392\u001b[0m  0.2580\n",
      "      3        \u001b[36m1.4968\u001b[0m        \u001b[32m1.5881\u001b[0m  0.3324\n",
      "      3        \u001b[36m2.6910\u001b[0m        \u001b[32m2.6750\u001b[0m  0.3152\n",
      "      4        \u001b[36m1.5122\u001b[0m        \u001b[32m1.5898\u001b[0m  0.2579\n",
      "      3        \u001b[36m2.0392\u001b[0m        \u001b[32m2.0915\u001b[0m  0.3307\n",
      "      3        \u001b[36m2.9215\u001b[0m        \u001b[32m2.9184\u001b[0m  0.3282\n",
      "      3        \u001b[36m2.9567\u001b[0m        \u001b[32m2.9245\u001b[0m  0.3241\n",
      "      4        2.9335        \u001b[32m2.8980\u001b[0m  0.2665\n",
      "      4        \u001b[36m2.6817\u001b[0m        \u001b[32m2.6340\u001b[0m  0.2619\n",
      "      4        \u001b[36m2.9678\u001b[0m        \u001b[32m2.9062\u001b[0m  0.2556\n",
      "      5        \u001b[36m2.0250\u001b[0m        2.0589  0.2631\n",
      "      5        \u001b[36m1.5026\u001b[0m        1.5949  0.2571\n",
      "      5        \u001b[36m2.9234\u001b[0m        2.8991  0.2601\n",
      "      4        \u001b[36m1.4964\u001b[0m        1.6083  0.3592\n",
      "      4        \u001b[36m2.6856\u001b[0m        \u001b[32m2.6584\u001b[0m  0.3525\n",
      "      5        2.6893        2.6376  0.2774\n",
      "      4        \u001b[36m2.0389\u001b[0m        \u001b[32m2.0824\u001b[0m  0.3422\n",
      "      4        2.9235        2.9195  0.3209\n",
      "      5        \u001b[36m2.9651\u001b[0m        2.9113  0.2925\n",
      "      4        2.9575        \u001b[32m2.9155\u001b[0m  0.3614\n",
      "      6        2.0310        2.0450  0.2484\n",
      "      6        1.5067        \u001b[32m1.5885\u001b[0m  0.2643\n",
      "      6        \u001b[36m2.9213\u001b[0m        2.8984  0.2629\n",
      "      6        \u001b[36m2.6802\u001b[0m        2.6453  0.2523\n",
      "      5        \u001b[36m1.4895\u001b[0m        \u001b[32m1.5802\u001b[0m  0.3144\n",
      "      5        \u001b[36m2.6817\u001b[0m        \u001b[32m2.6540\u001b[0m  0.3234\n",
      "      6        \u001b[36m2.9629\u001b[0m        \u001b[32m2.9055\u001b[0m  0.2930\n",
      "      5        \u001b[36m2.0311\u001b[0m        \u001b[32m2.0775\u001b[0m  0.3437\n",
      "      5        \u001b[36m2.9157\u001b[0m        2.9233  0.3416\n",
      "      5        \u001b[36m2.9465\u001b[0m        \u001b[32m2.9148\u001b[0m  0.3382\n",
      "      7        2.0345        2.0413  0.2827\n",
      "      7        1.5030        \u001b[32m1.5882\u001b[0m  0.2536\n",
      "      7        \u001b[36m2.6743\u001b[0m        2.6364  0.2381\n",
      "      7        \u001b[36m2.9196\u001b[0m        2.9116  0.2768\n",
      "      7        \u001b[36m2.9576\u001b[0m        \u001b[32m2.9020\u001b[0m  0.2301\n",
      "      6        \u001b[36m2.6815\u001b[0m        \u001b[32m2.6519\u001b[0m  0.3051\n",
      "      6        \u001b[36m1.4828\u001b[0m        1.5991  0.3452\n",
      "      6        \u001b[36m2.0276\u001b[0m        \u001b[32m2.0749\u001b[0m  0.3154\n",
      "      8        2.0282        2.0435  0.2600\n",
      "      6        \u001b[36m2.9141\u001b[0m        2.9223  0.3077\n",
      "      8        1.5036        1.5941  0.2957\n",
      "      6        2.9497        2.9245  0.3317\n",
      "      8        \u001b[36m2.9142\u001b[0m        2.9047  0.2604\n",
      "      8        2.6764        2.6361  0.3307\n",
      "      8        \u001b[36m2.9562\u001b[0m        2.9067  0.2609\n",
      "      9        \u001b[36m2.0245\u001b[0m        2.0614  0.2552\n",
      "      7        1.4877        1.5924  0.3158\n",
      "      7        \u001b[36m2.6738\u001b[0m        \u001b[32m2.6505\u001b[0m  0.3419\n",
      "      9        \u001b[36m1.4994\u001b[0m        1.5910  0.2422\n",
      "      7        2.9201        2.9260  0.3319\n",
      "      7        2.0313        2.0779  0.3387\n",
      "      9        2.9181        2.8995  0.2808\n",
      "      9        2.9591        2.9052  0.2439\n",
      "      7        \u001b[36m2.9423\u001b[0m        \u001b[32m2.9141\u001b[0m  0.3379\n",
      "      9        2.6820        \u001b[32m2.6333\u001b[0m  0.2732\n",
      "     10        2.0343        2.0501  0.2503\n",
      "Restoring best model from epoch 4.\n",
      "     10        \u001b[36m1.4978\u001b[0m        1.5968  0.2708\n",
      "Restoring best model from epoch 7.\n",
      "      8        1.4888        1.5814  0.3211\n",
      "      8        \u001b[36m2.6700\u001b[0m        2.6560  0.3259\n",
      "     10        \u001b[36m2.9550\u001b[0m        \u001b[32m2.8930\u001b[0m  0.2475\n",
      "     10        2.9145        2.9064  0.2581\n",
      "Restoring best model from epoch 4.\n",
      "     10        \u001b[36m2.6699\u001b[0m        2.6384  0.2410\n",
      "Restoring best model from epoch 9.\n",
      "      8        2.0348        2.0914  0.3278\n",
      "      8        2.9185        2.9287  0.3475\n",
      "      8        \u001b[36m2.9418\u001b[0m        2.9389  0.3065\n",
      "      9        1.4847        1.5927  0.2789\n",
      "      9        2.6702        2.6568  0.2903\n",
      "      9        2.0324        2.0844  0.2811\n",
      "      9        2.9172        2.9198  0.2868\n",
      "      9        2.9474        2.9338  0.2890\n",
      "     10        1.4896        1.5866  0.2767\n",
      "Restoring best model from epoch 5.\n",
      "     10        \u001b[36m2.6626\u001b[0m        \u001b[32m2.6490\u001b[0m  0.2851\n",
      "     10        2.0286        2.0867  0.2814\n",
      "Restoring best model from epoch 6.\n",
      "     10        \u001b[36m2.9042\u001b[0m        2.9197  0.2860\n",
      "Restoring best model from epoch 3.\n",
      "     10        2.9459        2.9169  0.2882\n",
      "Restoring best model from epoch 7.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.4616\u001b[0m        \u001b[32m2.5270\u001b[0m  0.3084\n",
      "      2        \u001b[36m2.4490\u001b[0m        \u001b[32m2.5207\u001b[0m  0.2924\n",
      "      3        \u001b[36m2.4310\u001b[0m        \u001b[32m2.5169\u001b[0m  0.2989\n",
      "      4        2.4347        \u001b[32m2.5118\u001b[0m  0.3050\n",
      "      5        \u001b[36m2.4228\u001b[0m        2.5133  0.2918\n",
      "      6        2.4283        \u001b[32m2.5116\u001b[0m  0.2992\n",
      "      7        2.4248        2.5187  0.3381\n",
      "      8        2.4242        2.5132  0.3038\n",
      "      9        2.4247        2.5145  0.2935\n",
      "     10        \u001b[36m2.4161\u001b[0m        2.5150  0.2967\n",
      "Restoring best model from epoch 4.\n",
      "y_train breslow final [-1.000e+00 -1.000e+00 -1.000e+00 ... -5.166e+03 -5.171e+03 -5.215e+03]\n",
      "Concordance Index 0.7951369688458786\n",
      "Integrated Brier Score: 0.14733259865061893\n",
      "y_train breslow final [-1.000e+00 -1.000e+00 -1.000e+00 ... -5.166e+03 -5.171e+03 -5.215e+03]\n",
      "durations 1.0 5187.0\n",
      "Concordance Index 0.7991272664698995\n",
      "Integrated Brier Score: 0.13948703217181788\n",
      "load_rgbsg\n",
      "split horm_treatment      category\n",
      "menopause           category\n",
      "age                  float32\n",
      "n_positive_nodes     float32\n",
      "progesterone         float32\n",
      "estrogene            float32\n",
      "grade_0.0              uint8\n",
      "grade_1.0              uint8\n",
      "grade_2.0              uint8\n",
      "dtype: object\n",
      "(1785, 9) <class 'pandas.core.frame.DataFrame'>\n",
      "(1785,) <class 'pandas.core.series.Series'>\n",
      "(447, 9) <class 'pandas.core.frame.DataFrame'>\n",
      "(447,) <class 'pandas.core.series.Series'>\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.2682\u001b[0m        \u001b[32m2.1088\u001b[0m  0.0680\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.3514\u001b[0m        \u001b[32m2.1442\u001b[0m  0.0659\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.5193\u001b[0m        \u001b[32m2.3430\u001b[0m  0.0728\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.3299\u001b[0m        \u001b[32m2.1015\u001b[0m  0.0827\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.8025\u001b[0m        \u001b[32m2.7405\u001b[0m  0.0720\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.5026\u001b[0m        \u001b[32m2.1032\u001b[0m  0.0948\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.4878\u001b[0m        \u001b[32m2.1297\u001b[0m  0.0892\n",
      "      2        \u001b[36m2.2128\u001b[0m        \u001b[32m2.0742\u001b[0m  0.0666\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.4992\u001b[0m        \u001b[32m2.1359\u001b[0m  0.0974\n",
      "      2        \u001b[36m2.2851\u001b[0m        \u001b[32m2.1319\u001b[0m  0.0734\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.0319\u001b[0m        \u001b[32m2.7341\u001b[0m  0.1010\n",
      "      2        \u001b[36m2.2832\u001b[0m        \u001b[32m2.0683\u001b[0m  0.0734\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.6682\u001b[0m        \u001b[32m2.3158\u001b[0m  0.1151\n",
      "      2        \u001b[36m2.4587\u001b[0m        \u001b[32m2.3038\u001b[0m  0.0844\n",
      "      2        \u001b[36m2.7460\u001b[0m        \u001b[32m2.6784\u001b[0m  0.0930\n",
      "      3        \u001b[36m2.1975\u001b[0m        2.0919  0.0758\n",
      "      2        \u001b[36m2.2667\u001b[0m        \u001b[32m2.0843\u001b[0m  0.0948\n",
      "      2        \u001b[36m2.3461\u001b[0m        \u001b[32m2.1246\u001b[0m  0.0901\n",
      "      3        \u001b[36m2.2607\u001b[0m        \u001b[32m2.0615\u001b[0m  0.0694\n",
      "      3        2.3148        \u001b[32m2.1101\u001b[0m  0.0830\n",
      "      2        \u001b[36m2.3127\u001b[0m        2.1704  0.1044\n",
      "      3        \u001b[36m2.4497\u001b[0m        \u001b[32m2.2851\u001b[0m  0.0678\n",
      "      2        \u001b[36m2.5148\u001b[0m        2.3278  0.0881\n",
      "      3        \u001b[36m2.7212\u001b[0m        2.6957  0.0743\n",
      "      2        \u001b[36m2.7961\u001b[0m        \u001b[32m2.7043\u001b[0m  0.1094\n",
      "      4        \u001b[36m2.1859\u001b[0m        2.0760  0.0828\n",
      "      4        \u001b[36m2.2806\u001b[0m        \u001b[32m2.0760\u001b[0m  0.0659\n",
      "      4        \u001b[36m2.4472\u001b[0m        \u001b[32m2.2811\u001b[0m  0.0630\n",
      "      4        \u001b[36m2.2479\u001b[0m        \u001b[32m2.0553\u001b[0m  0.0751\n",
      "      3        \u001b[36m2.2801\u001b[0m        \u001b[32m2.0883\u001b[0m  0.0901\n",
      "      3        \u001b[36m2.1780\u001b[0m        2.0953  0.0963\n",
      "      4        2.7300        \u001b[32m2.6528\u001b[0m  0.0823\n",
      "      3        \u001b[36m2.2511\u001b[0m        \u001b[32m2.0633\u001b[0m  0.1124\n",
      "      3        \u001b[36m2.7064\u001b[0m        \u001b[32m2.6350\u001b[0m  0.0882\n",
      "      5        2.1964        \u001b[32m2.0686\u001b[0m  0.0703\n",
      "      3        \u001b[36m2.4395\u001b[0m        2.3177  0.1106\n",
      "      5        2.2849        2.0851  0.0704\n",
      "      5        \u001b[36m2.2191\u001b[0m        \u001b[32m2.0486\u001b[0m  0.0735\n",
      "      5        \u001b[36m2.4291\u001b[0m        \u001b[32m2.2807\u001b[0m  0.0806\n",
      "      5        \u001b[36m2.7053\u001b[0m        \u001b[32m2.6528\u001b[0m  0.0690\n",
      "      4        \u001b[36m2.1414\u001b[0m        2.1130  0.0969\n",
      "      4        \u001b[36m2.2490\u001b[0m        2.0905  0.1020\n",
      "      6        \u001b[36m2.1837\u001b[0m        2.0770  0.0667\n",
      "      4        \u001b[36m2.6550\u001b[0m        2.6919  0.0878\n",
      "      4        \u001b[36m2.4176\u001b[0m        \u001b[32m2.2974\u001b[0m  0.0854\n",
      "      6        2.4303        2.2965  0.0711\n",
      "      4        \u001b[36m2.2017\u001b[0m        \u001b[32m2.0578\u001b[0m  0.1090\n",
      "      6        \u001b[36m2.2612\u001b[0m        2.0870  0.0922\n",
      "      6        2.2323        2.0491  0.0868\n",
      "      6        2.7128        \u001b[32m2.6448\u001b[0m  0.0700\n",
      "      7        2.1845        2.0713  0.0674\n",
      "      5        2.1482        \u001b[32m2.0781\u001b[0m  0.0893\n",
      "      7        2.4337        2.2870  0.0642\n",
      "      5        \u001b[36m2.2442\u001b[0m        \u001b[32m2.0843\u001b[0m  0.1148\n",
      "      7        2.2822        2.0850  0.0722\n",
      "      5        2.2163        2.0617  0.0823\n",
      "      5        \u001b[36m2.4003\u001b[0m        \u001b[32m2.2675\u001b[0m  0.0946\n",
      "      5        2.6662        2.6528  0.0998\n",
      "      7        2.2382        \u001b[32m2.0416\u001b[0m  0.0920\n",
      "      7        \u001b[36m2.6866\u001b[0m        \u001b[32m2.6402\u001b[0m  0.0853\n",
      "      8        2.1888        2.0810  0.0835\n",
      "      8        2.4329        2.2873  0.0663\n",
      "      6        \u001b[36m2.1283\u001b[0m        \u001b[32m2.0632\u001b[0m  0.0996\n",
      "      8        2.2824        2.0908  0.0772\n",
      "      6        \u001b[36m2.2273\u001b[0m        2.1127  0.0956\n",
      "      6        \u001b[36m2.6387\u001b[0m        \u001b[32m2.6218\u001b[0m  0.0824\n",
      "      6        \u001b[36m2.1869\u001b[0m        \u001b[32m2.0270\u001b[0m  0.0932\n",
      "      8        \u001b[36m2.2179\u001b[0m        2.0439  0.0748\n",
      "      8        2.7021        2.6419  0.0736\n",
      "      9        \u001b[36m2.1815\u001b[0m        2.0705  0.0713\n",
      "      9        \u001b[36m2.4212\u001b[0m        2.3159  0.0732\n",
      "      6        \u001b[36m2.3951\u001b[0m        \u001b[32m2.2507\u001b[0m  0.1173\n",
      "      9        \u001b[36m2.2600\u001b[0m        2.0927  0.0688\n",
      "      7        2.1338        \u001b[32m2.0584\u001b[0m  0.0871\n",
      "     10        2.1828        2.0709  0.0630\n",
      "Restoring best model from epoch 5.\n",
      "      9        \u001b[36m2.2152\u001b[0m        2.0543  0.0818\n",
      "      9        \u001b[36m2.6841\u001b[0m        2.6419  0.0782\n",
      "      7        2.6468        2.6312  0.0976\n",
      "      7        \u001b[36m2.2269\u001b[0m        2.0877  0.1039\n",
      "      7        \u001b[36m2.1842\u001b[0m        \u001b[32m2.0124\u001b[0m  0.0942\n",
      "     10        \u001b[36m2.2484\u001b[0m        2.0786  0.0647\n",
      "Restoring best model from epoch 4.\n",
      "     10        2.4279        2.2839  0.0898\n",
      "Restoring best model from epoch 5.\n",
      "      7        2.4083        2.2543  0.0936\n",
      "     10        2.2296        2.0535  0.0635\n",
      "Restoring best model from epoch 7.\n",
      "     10        \u001b[36m2.6841\u001b[0m        2.6462  0.0660\n",
      "Restoring best model from epoch 7.\n",
      "      8        \u001b[36m2.1196\u001b[0m        \u001b[32m2.0526\u001b[0m  0.1017\n",
      "      8        \u001b[36m2.2210\u001b[0m        \u001b[32m2.0707\u001b[0m  0.0813\n",
      "      8        \u001b[36m2.1771\u001b[0m        2.0279  0.0820\n",
      "      8        \u001b[36m2.6310\u001b[0m        \u001b[32m2.6068\u001b[0m  0.0906\n",
      "      8        \u001b[36m2.3704\u001b[0m        2.2778  0.0812\n",
      "      9        \u001b[36m2.1130\u001b[0m        2.0561  0.0798\n",
      "      9        \u001b[36m2.1962\u001b[0m        2.1059  0.0806\n",
      "      9        \u001b[36m2.1763\u001b[0m        2.0154  0.0789\n",
      "      9        \u001b[36m2.6037\u001b[0m        2.6344  0.0822\n",
      "      9        \u001b[36m2.3587\u001b[0m        2.2774  0.0816\n",
      "     10        2.1137        2.0820  0.0801\n",
      "Restoring best model from epoch 8.\n",
      "     10        \u001b[36m2.1727\u001b[0m        2.0317  0.0803\n",
      "Restoring best model from epoch 7.\n",
      "     10        2.2127        2.0931  0.0812\n",
      "Restoring best model from epoch 8.\n",
      "     10        2.6272        \u001b[32m2.5938\u001b[0m  0.0835\n",
      "     10        2.3850        2.2871  0.0825\n",
      "Restoring best model from epoch 6.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.4983\u001b[0m        \u001b[32m2.2888\u001b[0m  0.0851\n",
      "      2        \u001b[36m2.4276\u001b[0m        \u001b[32m2.2855\u001b[0m  0.0907\n",
      "      3        2.4300        \u001b[32m2.2699\u001b[0m  0.0938\n",
      "      4        \u001b[36m2.4042\u001b[0m        2.2803  0.1176\n",
      "      5        \u001b[36m2.3950\u001b[0m        \u001b[32m2.2611\u001b[0m  0.0925\n",
      "      6        \u001b[36m2.3875\u001b[0m        2.2698  0.0908\n",
      "      7        2.3959        \u001b[32m2.2576\u001b[0m  0.0891\n",
      "      8        2.4027        2.2599  0.1000\n",
      "      9        \u001b[36m2.3822\u001b[0m        \u001b[32m2.2570\u001b[0m  0.0862\n",
      "     10        \u001b[36m2.3700\u001b[0m        \u001b[32m2.2558\u001b[0m  0.0879\n",
      "y_train breslow final [ -0.26283368  -0.49281314  -0.55852157 ... -84.         -84.20534\n",
      " -85.81519   ]\n",
      "Concordance Index 0.6785289391616011\n",
      "Integrated Brier Score: 0.17763247170880578\n",
      "y_train breslow final [ -0.26283368  -0.49281314  -0.55852157 ... -84.         -84.20534\n",
      " -85.81519   ]\n",
      "durations 0.52566737 87.359344\n",
      "Concordance Index 0.6930122757318224\n",
      "Integrated Brier Score: 0.17166945927125668\n",
      "(1785, 9) <class 'pandas.core.frame.DataFrame'>\n",
      "(1785,) <class 'pandas.core.series.Series'>\n",
      "(447, 9) <class 'pandas.core.frame.DataFrame'>\n",
      "(447,) <class 'pandas.core.series.Series'>\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.2639\u001b[0m        \u001b[32m2.1665\u001b[0m  0.0350\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.2761\u001b[0m        \u001b[32m2.2342\u001b[0m  0.0333\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.4828\u001b[0m        \u001b[32m2.3939\u001b[0m  0.0316\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.2927\u001b[0m        \u001b[32m2.2054\u001b[0m  0.0415\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.7850\u001b[0m        \u001b[32m2.7900\u001b[0m  0.0360\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.2995\u001b[0m        \u001b[32m2.1283\u001b[0m  0.0367\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.3112\u001b[0m        \u001b[32m2.2126\u001b[0m  0.0325\n",
      "      2        \u001b[36m2.2067\u001b[0m        2.2381  0.0330\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.2879\u001b[0m        \u001b[32m2.2745\u001b[0m  0.0429\n",
      "      2        \u001b[36m2.2067\u001b[0m        2.1725  0.0387\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.5061\u001b[0m        \u001b[32m2.4017\u001b[0m  0.0375\n",
      "      2        \u001b[36m2.4041\u001b[0m        \u001b[32m2.3691\u001b[0m  0.0354\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.8152\u001b[0m        \u001b[32m2.8096\u001b[0m  0.0401\n",
      "      2        \u001b[36m2.2292\u001b[0m        \u001b[32m2.1823\u001b[0m  0.0370\n",
      "      2        \u001b[36m2.7083\u001b[0m        \u001b[32m2.7516\u001b[0m  0.0385\n",
      "      2        \u001b[36m2.2220\u001b[0m        2.1714  0.0399\n",
      "      2        \u001b[36m2.2510\u001b[0m        2.2131  0.0398\n",
      "      3        2.2087        \u001b[32m2.1631\u001b[0m  0.0334\n",
      "      2        \u001b[36m2.2188\u001b[0m        \u001b[32m2.2342\u001b[0m  0.0365\n",
      "      3        \u001b[36m2.1909\u001b[0m        \u001b[32m2.2089\u001b[0m  0.0409\n",
      "      2        \u001b[36m2.4089\u001b[0m        \u001b[32m2.3887\u001b[0m  0.0379\n",
      "      3        \u001b[36m2.4026\u001b[0m        2.3804  0.0350\n",
      "      2        \u001b[36m2.7121\u001b[0m        \u001b[32m2.7725\u001b[0m  0.0379\n",
      "      3        \u001b[36m2.6934\u001b[0m        \u001b[32m2.7478\u001b[0m  0.0377\n",
      "      3        \u001b[36m2.2192\u001b[0m        2.1867  0.0468\n",
      "      3        \u001b[36m2.2171\u001b[0m        2.1394  0.0400\n",
      "      4        \u001b[36m2.2005\u001b[0m        \u001b[32m2.1553\u001b[0m  0.0313\n",
      "      3        \u001b[36m2.2351\u001b[0m        \u001b[32m2.1774\u001b[0m  0.0360\n",
      "      4        \u001b[36m2.1746\u001b[0m        2.2216  0.0315\n",
      "      3        \u001b[36m2.1808\u001b[0m        \u001b[32m2.2042\u001b[0m  0.0345\n",
      "      3        \u001b[36m2.3944\u001b[0m        \u001b[32m2.3683\u001b[0m  0.0398\n",
      "      4        \u001b[36m2.1985\u001b[0m        \u001b[32m2.1572\u001b[0m  0.0285\n",
      "      4        \u001b[36m2.3833\u001b[0m        \u001b[32m2.3621\u001b[0m  0.0512\n",
      "      4        \u001b[36m2.6753\u001b[0m        \u001b[32m2.7389\u001b[0m  0.0376\n",
      "      5        \u001b[36m2.1723\u001b[0m        \u001b[32m2.2088\u001b[0m  0.0282\n",
      "      4        \u001b[36m2.1820\u001b[0m        2.1506  0.0348\n",
      "      3        \u001b[36m2.6862\u001b[0m        \u001b[32m2.7207\u001b[0m  0.0510\n",
      "      4        \u001b[36m2.1717\u001b[0m        2.2175  0.0343\n",
      "      4        \u001b[36m2.2194\u001b[0m        \u001b[32m2.1512\u001b[0m  0.0382\n",
      "      5        \u001b[36m2.1872\u001b[0m        2.1697  0.0397\n",
      "      5        \u001b[36m2.1811\u001b[0m        2.1656  0.0283\n",
      "      6        \u001b[36m2.1713\u001b[0m        2.2203  0.0292\n",
      "      4        \u001b[36m2.3717\u001b[0m        2.3769  0.0479\n",
      "      6        \u001b[36m2.1848\u001b[0m        2.1680  0.0282\n",
      "      5        \u001b[36m2.3699\u001b[0m        2.3699  0.0409\n",
      "      5        \u001b[36m2.2105\u001b[0m        2.1585  0.0303\n",
      "      4        \u001b[36m2.6685\u001b[0m        2.7230  0.0352\n",
      "      5        \u001b[36m2.6557\u001b[0m        2.7489  0.0409\n",
      "      5        2.1960        2.1313  0.0373\n",
      "      5        2.1804        \u001b[32m2.1834\u001b[0m  0.0378\n",
      "      6        2.1944        2.1573  0.0362\n",
      "      7        2.1734        \u001b[32m2.1999\u001b[0m  0.0342\n",
      "      7        2.1940        2.1629  0.0277\n",
      "      6        2.3765        \u001b[32m2.3606\u001b[0m  0.0308\n",
      "      5        2.3764        \u001b[32m2.3492\u001b[0m  0.0421\n",
      "      6        \u001b[36m2.2065\u001b[0m        \u001b[32m2.1493\u001b[0m  0.0341\n",
      "      6        2.6640        \u001b[32m2.7307\u001b[0m  0.0354\n",
      "      6        2.1781        2.2460  0.0303\n",
      "      6        2.1829        2.1370  0.0359\n",
      "      8        2.1926        2.1706  0.0279\n",
      "      7        2.1933        \u001b[32m2.1569\u001b[0m  0.0366\n",
      "      8        2.1732        2.2191  0.0312\n",
      "      5        2.6710        \u001b[32m2.7113\u001b[0m  0.0579\n",
      "      7        2.3877        2.3843  0.0374\n",
      "      7        \u001b[36m2.2015\u001b[0m        2.1519  0.0332\n",
      "      7        \u001b[36m2.1732\u001b[0m        2.1348  0.0311\n",
      "      7        2.6666        2.7382  0.0376\n",
      "      7        2.1720        2.1866  0.0432\n",
      "      8        2.1915        \u001b[32m2.1494\u001b[0m  0.0314\n",
      "      6        2.3856        2.3915  0.0532\n",
      "      9        2.1909        \u001b[32m2.1547\u001b[0m  0.0359\n",
      "      6        \u001b[36m2.6571\u001b[0m        \u001b[32m2.7045\u001b[0m  0.0334\n",
      "      8        \u001b[36m2.1890\u001b[0m        2.1641  0.0310\n",
      "      9        \u001b[36m2.1495\u001b[0m        2.2021  0.0420\n",
      "      8        2.6659        \u001b[32m2.7280\u001b[0m  0.0290\n",
      "      8        \u001b[36m2.1681\u001b[0m        2.1314  0.0401\n",
      "      8        2.3813        \u001b[32m2.3528\u001b[0m  0.0432\n",
      "      9        2.1894        2.1552  0.0280\n",
      "      7        \u001b[36m2.3650\u001b[0m        2.3518  0.0335\n",
      "      8        2.1758        2.2166  0.0423\n",
      "     10        2.1880        2.1786  0.0345\n",
      "Restoring best model from epoch 9.\n",
      "      7        2.6635        2.7074  0.0390\n",
      "     10        \u001b[36m2.1485\u001b[0m        2.2272  0.0349\n",
      "Restoring best model from epoch 7.\n",
      "      9        2.6639        2.7321  0.0317\n",
      "      9        2.1681        2.1305  0.0329\n",
      "      9        2.1954        2.1542  0.0473\n",
      "     10        \u001b[36m2.1736\u001b[0m        2.1502  0.0320\n",
      "Restoring best model from epoch 8.\n",
      "      9        \u001b[36m2.3693\u001b[0m        2.3620  0.0405\n",
      "      9        \u001b[36m2.1527\u001b[0m        2.2083  0.0312\n",
      "      8        \u001b[36m2.3643\u001b[0m        2.3622  0.0375\n",
      "     10        \u001b[36m2.6492\u001b[0m        \u001b[32m2.7268\u001b[0m  0.0307\n",
      "      8        \u001b[36m2.6381\u001b[0m        2.7277  0.0358\n",
      "     10        \u001b[36m2.1679\u001b[0m        2.1312  0.0320\n",
      "Restoring best model from epoch 1.\n",
      "     10        2.2018        2.1563  0.0335\n",
      "Restoring best model from epoch 6.\n",
      "     10        \u001b[36m2.3637\u001b[0m        2.3573  0.0297\n",
      "Restoring best model from epoch 8.\n",
      "     10        \u001b[36m2.1507\u001b[0m        2.2018  0.0313\n",
      "Restoring best model from epoch 5.\n",
      "      9        2.3677        2.3700  0.0322\n",
      "      9        2.6401        \u001b[32m2.6979\u001b[0m  0.0333\n",
      "     10        2.3649        2.3581  0.0305\n",
      "Restoring best model from epoch 5.\n",
      "     10        \u001b[36m2.6325\u001b[0m        2.7099  0.0316\n",
      "Restoring best model from epoch 9.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.4653\u001b[0m        \u001b[32m2.3736\u001b[0m  0.0427\n",
      "      2        \u001b[36m2.4004\u001b[0m        \u001b[32m2.3443\u001b[0m  0.0454\n",
      "      3        \u001b[36m2.3812\u001b[0m        2.3476  0.0429\n",
      "      4        \u001b[36m2.3673\u001b[0m        \u001b[32m2.3295\u001b[0m  0.0417\n",
      "      5        \u001b[36m2.3624\u001b[0m        2.3449  0.0434\n",
      "      6        \u001b[36m2.3485\u001b[0m        \u001b[32m2.3277\u001b[0m  0.0425\n",
      "      7        2.3541        2.3408  0.0420\n",
      "      8        \u001b[36m2.3439\u001b[0m        2.3357  0.0433\n",
      "      9        2.3557        2.3310  0.0447\n",
      "     10        \u001b[36m2.3431\u001b[0m        2.3495  0.0433\n",
      "Restoring best model from epoch 6.\n",
      "y_train breslow final [ -0.26283368  -0.49281314  -0.52566737 ... -84.20534    -85.81519\n",
      " -87.359344  ]\n",
      "Concordance Index 0.6885043718669961\n",
      "Integrated Brier Score: 0.1718395671358918\n",
      "y_train breslow final [ -0.26283368  -0.49281314  -0.52566737 ... -84.20534    -85.81519\n",
      " -87.359344  ]\n",
      "durations 1.87269 84.0\n",
      "Concordance Index 0.6697586646821178\n",
      "Integrated Brier Score: 0.1855247253322384\n",
      "(1786, 9) <class 'pandas.core.frame.DataFrame'>\n",
      "(1786,) <class 'pandas.core.series.Series'>\n",
      "(446, 9) <class 'pandas.core.frame.DataFrame'>\n",
      "(446,) <class 'pandas.core.series.Series'>\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.2684\u001b[0m        \u001b[32m2.1152\u001b[0m  0.0312\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.2695\u001b[0m        \u001b[32m2.1513\u001b[0m  0.0337\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.3045\u001b[0m        \u001b[32m2.2075\u001b[0m  0.0310\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.4786\u001b[0m        \u001b[32m2.4158\u001b[0m  0.0307\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.8057\u001b[0m        \u001b[32m2.6928\u001b[0m  0.0331\n",
      "      2        \u001b[36m2.2171\u001b[0m        \u001b[32m2.1108\u001b[0m  0.0354\n",
      "      2        \u001b[36m2.2080\u001b[0m        \u001b[32m2.1499\u001b[0m  0.0343\n",
      "      2        \u001b[36m2.2419\u001b[0m        \u001b[32m2.2057\u001b[0m  0.0371\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.3310\u001b[0m        \u001b[32m2.1125\u001b[0m  0.0500\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.2443\u001b[0m        \u001b[32m2.1858\u001b[0m  0.0441\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.3175\u001b[0m        \u001b[32m2.2375\u001b[0m  0.0433\n",
      "      2        \u001b[36m2.4088\u001b[0m        \u001b[32m2.4130\u001b[0m  0.0464\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.4681\u001b[0m        \u001b[32m2.4168\u001b[0m  0.0415\n",
      "      2        \u001b[36m2.7264\u001b[0m        \u001b[32m2.6745\u001b[0m  0.0408\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.8145\u001b[0m        \u001b[32m2.7105\u001b[0m  0.0421\n",
      "      3        \u001b[36m2.1876\u001b[0m        2.1145  0.0411\n",
      "      3        \u001b[36m2.1998\u001b[0m        2.1543  0.0382\n",
      "      3        \u001b[36m2.2271\u001b[0m        \u001b[32m2.1964\u001b[0m  0.0370\n",
      "      2        \u001b[36m2.2055\u001b[0m        \u001b[32m2.1032\u001b[0m  0.0368\n",
      "      2        \u001b[36m2.2134\u001b[0m        \u001b[32m2.1569\u001b[0m  0.0441\n",
      "      3        \u001b[36m2.3891\u001b[0m        \u001b[32m2.3980\u001b[0m  0.0320\n",
      "      2        \u001b[36m2.2682\u001b[0m        \u001b[32m2.2332\u001b[0m  0.0413\n",
      "      3        \u001b[36m2.7060\u001b[0m        \u001b[32m2.6670\u001b[0m  0.0379\n",
      "      2        \u001b[36m2.4176\u001b[0m        \u001b[32m2.4034\u001b[0m  0.0458\n",
      "      4        \u001b[36m2.1864\u001b[0m        2.1136  0.0357\n",
      "      2        \u001b[36m2.7190\u001b[0m        \u001b[32m2.6886\u001b[0m  0.0454\n",
      "      4        \u001b[36m2.2126\u001b[0m        2.2006  0.0320\n",
      "      3        \u001b[36m2.1936\u001b[0m        2.1062  0.0338\n",
      "      3        \u001b[36m2.1812\u001b[0m        \u001b[32m2.1563\u001b[0m  0.0345\n",
      "      4        \u001b[36m2.3792\u001b[0m        2.3999  0.0372\n",
      "      4        \u001b[36m2.1795\u001b[0m        2.1748  0.0524\n",
      "      4        \u001b[36m2.6962\u001b[0m        \u001b[32m2.6660\u001b[0m  0.0332\n",
      "      5        \u001b[36m2.1682\u001b[0m        2.1159  0.0331\n",
      "      3        \u001b[36m2.2504\u001b[0m        \u001b[32m2.2232\u001b[0m  0.0480\n",
      "      5        2.2131        2.2035  0.0357\n",
      "      3        \u001b[36m2.6850\u001b[0m        \u001b[32m2.6620\u001b[0m  0.0382\n",
      "      3        \u001b[36m2.3778\u001b[0m        \u001b[32m2.3879\u001b[0m  0.0475\n",
      "      4        \u001b[36m2.1842\u001b[0m        2.1169  0.0406\n",
      "      5        \u001b[36m2.1693\u001b[0m        2.1663  0.0291\n",
      "      5        \u001b[36m2.3783\u001b[0m        \u001b[32m2.3980\u001b[0m  0.0359\n",
      "      5        \u001b[36m2.6775\u001b[0m        2.6661  0.0391\n",
      "      6        2.1695        2.1280  0.0390\n",
      "      4        \u001b[36m2.1743\u001b[0m        \u001b[32m2.1474\u001b[0m  0.0525\n",
      "      4        \u001b[36m2.2034\u001b[0m        \u001b[32m2.1983\u001b[0m  0.0381\n",
      "      6        \u001b[36m2.2054\u001b[0m        2.2154  0.0346\n",
      "      4        \u001b[36m2.6827\u001b[0m        \u001b[32m2.6585\u001b[0m  0.0335\n",
      "      4        2.3784        \u001b[32m2.3789\u001b[0m  0.0328\n",
      "      6        2.1792        2.1628  0.0283\n",
      "      6        \u001b[36m2.3776\u001b[0m        2.3997  0.0318\n",
      "      6        2.6805        \u001b[32m2.6659\u001b[0m  0.0301\n",
      "      5        \u001b[36m2.1639\u001b[0m        2.1039  0.0465\n",
      "      7        2.1699        2.1644  0.0283\n",
      "      5        \u001b[36m2.6789\u001b[0m        2.6596  0.0323\n",
      "      7        \u001b[36m2.1889\u001b[0m        2.2157  0.0361\n",
      "      7        2.1747        2.1248  0.0427\n",
      "      7        \u001b[36m2.3621\u001b[0m        2.4060  0.0299\n",
      "      5        \u001b[36m2.1968\u001b[0m        2.2217  0.0453\n",
      "      5        \u001b[36m2.1598\u001b[0m        2.1655  0.0496\n",
      "      5        \u001b[36m2.3600\u001b[0m        2.3849  0.0459\n",
      "      7        \u001b[36m2.6773\u001b[0m        2.6665  0.0359\n",
      "      6        \u001b[36m2.1549\u001b[0m        2.1221  0.0371\n",
      "      8        \u001b[36m2.1621\u001b[0m        2.1790  0.0343\n",
      "      8        2.1727        2.1195  0.0326\n",
      "      8        2.1993        2.2184  0.0355\n",
      "      8        2.3665        2.4127  0.0316\n",
      "      6        2.6841        2.6743  0.0408\n",
      "      6        \u001b[36m2.1556\u001b[0m        2.1607  0.0369\n",
      "      6        \u001b[36m2.1963\u001b[0m        2.2133  0.0428\n",
      "      7        \u001b[36m2.1489\u001b[0m        2.1142  0.0354\n",
      "      9        \u001b[36m2.1610\u001b[0m        2.1209  0.0289\n",
      "      6        2.3614        2.3824  0.0514\n",
      "      9        2.2020        2.2140  0.0286\n",
      "      8        \u001b[36m2.6756\u001b[0m        \u001b[32m2.6636\u001b[0m  0.0465\n",
      "      9        \u001b[36m2.1549\u001b[0m        2.1723  0.0355\n",
      "      9        \u001b[36m2.3509\u001b[0m        \u001b[32m2.3935\u001b[0m  0.0289\n",
      "      7        2.1565        2.1614  0.0354\n",
      "      7        \u001b[36m2.6625\u001b[0m        2.6612  0.0421\n",
      "      8        \u001b[36m2.1469\u001b[0m        2.1206  0.0345\n",
      "     10        2.1674        2.1220  0.0289\n",
      "Restoring best model from epoch 2.\n",
      "      7        \u001b[36m2.1795\u001b[0m        2.2130  0.0441\n",
      "      7        \u001b[36m2.3534\u001b[0m        \u001b[32m2.3771\u001b[0m  0.0339\n",
      "     10        \u001b[36m2.1755\u001b[0m        2.2164  0.0334\n",
      "Restoring best model from epoch 3.\n",
      "     10        \u001b[36m2.1401\u001b[0m        2.1708  0.0340\n",
      "Restoring best model from epoch 2.\n",
      "     10        2.3536        2.3963  0.0376\n",
      "Restoring best model from epoch 9.\n",
      "      8        \u001b[36m2.1453\u001b[0m        2.1615  0.0352\n",
      "      8        2.6631        2.6608  0.0392\n",
      "      9        \u001b[36m2.6703\u001b[0m        2.6771  0.0592\n",
      "      8        \u001b[36m2.1686\u001b[0m        2.2247  0.0339\n",
      "      9        \u001b[36m2.1295\u001b[0m        2.1214  0.0385\n",
      "      8        \u001b[36m2.3483\u001b[0m        2.3916  0.0321\n",
      "      9        \u001b[36m2.1453\u001b[0m        2.1594  0.0321\n",
      "      9        2.6652        2.6714  0.0339\n",
      "     10        \u001b[36m2.6619\u001b[0m        2.6716  0.0324\n",
      "Restoring best model from epoch 8.\n",
      "      9        \u001b[36m2.1667\u001b[0m        2.2088  0.0321\n",
      "     10        2.1382        2.1183  0.0323\n",
      "Restoring best model from epoch 2.\n",
      "      9        \u001b[36m2.3374\u001b[0m        2.3896  0.0332\n",
      "     10        \u001b[36m2.1342\u001b[0m        2.1545  0.0320\n",
      "Restoring best model from epoch 4.\n",
      "     10        \u001b[36m2.6537\u001b[0m        2.6746  0.0337\n",
      "Restoring best model from epoch 4.\n",
      "     10        \u001b[36m2.1630\u001b[0m        2.2253  0.0307\n",
      "Restoring best model from epoch 4.\n",
      "     10        \u001b[36m2.3341\u001b[0m        2.3929  0.0329\n",
      "Restoring best model from epoch 7.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.4357\u001b[0m        \u001b[32m2.3758\u001b[0m  0.0418\n",
      "      2        \u001b[36m2.3931\u001b[0m        \u001b[32m2.3560\u001b[0m  0.0408\n",
      "      3        \u001b[36m2.3760\u001b[0m        \u001b[32m2.3483\u001b[0m  0.0528\n",
      "      4        2.3769        2.3532  0.0430\n",
      "      5        \u001b[36m2.3724\u001b[0m        2.3591  0.0400\n",
      "      6        \u001b[36m2.3596\u001b[0m        2.3555  0.0416\n",
      "      7        2.3611        2.3492  0.0382\n",
      "      8        \u001b[36m2.3566\u001b[0m        2.3485  0.0391\n",
      "      9        2.3569        \u001b[32m2.3454\u001b[0m  0.0386\n",
      "     10        2.3599        \u001b[32m2.3438\u001b[0m  0.0388\n",
      "y_train breslow final [ -0.26283368  -0.52566737  -0.55852157 ... -84.         -84.20534\n",
      " -87.359344  ]\n",
      "Concordance Index 0.6948682664034365\n",
      "Integrated Brier Score: 0.16840123820857006\n",
      "y_train breslow final [ -0.26283368  -0.52566737  -0.55852157 ... -84.         -84.20534\n",
      " -87.359344  ]\n",
      "durations 0.49281314 85.81519\n",
      "Concordance Index 0.6694839441885306\n",
      "Integrated Brier Score: 0.18399487666089137\n",
      "(1786, 9) <class 'pandas.core.frame.DataFrame'>\n",
      "(1786,) <class 'pandas.core.series.Series'>\n",
      "(446, 9) <class 'pandas.core.frame.DataFrame'>\n",
      "(446,) <class 'pandas.core.series.Series'>\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.2444\u001b[0m        \u001b[32m2.1542\u001b[0m  0.0306\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.2716\u001b[0m        \u001b[32m2.1224\u001b[0m  0.0304\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.2949\u001b[0m        \u001b[32m2.1272\u001b[0m  0.0314\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.4834\u001b[0m        \u001b[32m2.3682\u001b[0m  0.0307\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.7471\u001b[0m        \u001b[32m2.7105\u001b[0m  0.0325\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.2571\u001b[0m        \u001b[32m2.1627\u001b[0m  0.0315\n",
      "      2        \u001b[36m2.1892\u001b[0m        \u001b[32m2.1467\u001b[0m  0.0334\n",
      "      2        \u001b[36m2.2004\u001b[0m        \u001b[32m2.1185\u001b[0m  0.0301\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.2799\u001b[0m        \u001b[32m2.1561\u001b[0m  0.0334\n",
      "      2        \u001b[36m2.4202\u001b[0m        \u001b[32m2.3514\u001b[0m  0.0307\n",
      "      2        \u001b[36m2.2407\u001b[0m        \u001b[32m2.1142\u001b[0m  0.0367\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.3414\u001b[0m        \u001b[32m2.1429\u001b[0m  0.0373\n",
      "      2        \u001b[36m2.6808\u001b[0m        \u001b[32m2.6931\u001b[0m  0.0343\n",
      "      3        \u001b[36m2.1842\u001b[0m        2.1536  0.0339\n",
      "      3        \u001b[36m2.1895\u001b[0m        \u001b[32m2.1169\u001b[0m  0.0405\n",
      "      2        \u001b[36m2.2203\u001b[0m        \u001b[32m2.1181\u001b[0m  0.0348\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.7964\u001b[0m        \u001b[32m2.7065\u001b[0m  0.0364\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.5144\u001b[0m        \u001b[32m2.3731\u001b[0m  0.0412\n",
      "      3        \u001b[36m2.2245\u001b[0m        \u001b[32m2.1106\u001b[0m  0.0314\n",
      "      2        \u001b[36m2.2045\u001b[0m        \u001b[32m2.1348\u001b[0m  0.0494\n",
      "      3        \u001b[36m2.4114\u001b[0m        2.3552  0.0435\n",
      "      3        \u001b[36m2.6655\u001b[0m        \u001b[32m2.6818\u001b[0m  0.0384\n",
      "      4        \u001b[36m2.1719\u001b[0m        2.1507  0.0325\n",
      "      2        \u001b[36m2.2314\u001b[0m        \u001b[32m2.1253\u001b[0m  0.0503\n",
      "      4        \u001b[36m2.1719\u001b[0m        2.1208  0.0403\n",
      "      3        \u001b[36m2.1624\u001b[0m        2.1391  0.0330\n",
      "      4        \u001b[36m2.2191\u001b[0m        2.1145  0.0352\n",
      "      3        \u001b[36m2.1818\u001b[0m        \u001b[32m2.1081\u001b[0m  0.0374\n",
      "      2        \u001b[36m2.4546\u001b[0m        \u001b[32m2.3672\u001b[0m  0.0399\n",
      "      2        \u001b[36m2.6823\u001b[0m        \u001b[32m2.6974\u001b[0m  0.0446\n",
      "      4        \u001b[36m2.6526\u001b[0m        2.6928  0.0298\n",
      "      4        \u001b[36m2.4092\u001b[0m        \u001b[32m2.3478\u001b[0m  0.0360\n",
      "      5        2.1743        2.1529  0.0427\n",
      "      3        \u001b[36m2.2183\u001b[0m        2.1289  0.0434\n",
      "      4        \u001b[36m2.1461\u001b[0m        2.1433  0.0328\n",
      "      5        \u001b[36m2.1963\u001b[0m        \u001b[32m2.1097\u001b[0m  0.0331\n",
      "      5        \u001b[36m2.1675\u001b[0m        \u001b[32m2.1122\u001b[0m  0.0399\n",
      "      5        \u001b[36m2.3931\u001b[0m        2.3479  0.0320\n",
      "      5        \u001b[36m2.6447\u001b[0m        2.6834  0.0355\n",
      "      4        \u001b[36m2.1700\u001b[0m        2.1143  0.0483\n",
      "      3        \u001b[36m2.6502\u001b[0m        \u001b[32m2.6716\u001b[0m  0.0417\n",
      "      3        \u001b[36m2.3991\u001b[0m        \u001b[32m2.3495\u001b[0m  0.0511\n",
      "      6        \u001b[36m2.1647\u001b[0m        2.1516  0.0288\n",
      "      4        \u001b[36m2.2009\u001b[0m        2.1309  0.0351\n",
      "      6        2.2031        2.1126  0.0327\n",
      "      6        \u001b[36m2.1590\u001b[0m        \u001b[32m2.1111\u001b[0m  0.0296\n",
      "      6        2.6458        2.6886  0.0291\n",
      "      6        \u001b[36m2.3876\u001b[0m        2.3595  0.0328\n",
      "      5        2.1787        2.1169  0.0354\n",
      "      5        2.1537        2.1537  0.0541\n",
      "      4        \u001b[36m2.3854\u001b[0m        2.3564  0.0317\n",
      "      4        \u001b[36m2.6274\u001b[0m        2.6919  0.0408\n",
      "      7        2.1656        2.1524  0.0385\n",
      "      7        2.2037        \u001b[32m2.1041\u001b[0m  0.0323\n",
      "      7        2.1705        2.1111  0.0314\n",
      "      5        2.2177        2.1366  0.0358\n",
      "      7        2.6481        2.6880  0.0288\n",
      "      7        2.3982        2.3524  0.0362\n",
      "      5        \u001b[36m2.3742\u001b[0m        2.3547  0.0368\n",
      "      8        \u001b[36m2.1517\u001b[0m        2.1494  0.0301\n",
      "      5        \u001b[36m2.6196\u001b[0m        2.6826  0.0338\n",
      "      6        \u001b[36m2.1428\u001b[0m        2.1368  0.0412\n",
      "      6        \u001b[36m2.1677\u001b[0m        2.1221  0.0495\n",
      "      8        \u001b[36m2.1465\u001b[0m        \u001b[32m2.1034\u001b[0m  0.0359\n",
      "      6        2.2115        2.1270  0.0368\n",
      "      8        \u001b[36m2.6386\u001b[0m        2.6930  0.0376\n",
      "      8        \u001b[36m2.1947\u001b[0m        2.1133  0.0451\n",
      "      8        \u001b[36m2.3865\u001b[0m        \u001b[32m2.3472\u001b[0m  0.0412\n",
      "      9        \u001b[36m2.1515\u001b[0m        2.1496  0.0303\n",
      "      6        2.6269        2.6879  0.0384\n",
      "      7        \u001b[36m2.1264\u001b[0m        2.1486  0.0386\n",
      "      9        2.1715        2.1137  0.0289\n",
      "      7        \u001b[36m2.1956\u001b[0m        2.1306  0.0321\n",
      "      6        2.3831        2.3629  0.0487\n",
      "      9        2.1987        2.1201  0.0316\n",
      "      7        \u001b[36m2.1536\u001b[0m        2.1112  0.0444\n",
      "      9        \u001b[36m2.6383\u001b[0m        2.6931  0.0413\n",
      "      9        2.3913        2.3557  0.0336\n",
      "     10        \u001b[36m2.1495\u001b[0m        2.1496  0.0399\n",
      "Restoring best model from epoch 2.\n",
      "      7        \u001b[36m2.6180\u001b[0m        2.6908  0.0342\n",
      "     10        2.1524        2.1104  0.0342\n",
      "Restoring best model from epoch 8.\n",
      "      8        \u001b[36m2.1233\u001b[0m        2.1507  0.0348\n",
      "      8        \u001b[36m2.1889\u001b[0m        \u001b[32m2.1253\u001b[0m  0.0369\n",
      "     10        \u001b[36m2.1914\u001b[0m        2.1231  0.0354\n",
      "Restoring best model from epoch 7.\n",
      "      8        \u001b[36m2.1459\u001b[0m        2.1292  0.0343\n",
      "      7        2.3793        2.3724  0.0433\n",
      "     10        \u001b[36m2.6317\u001b[0m        2.6894  0.0380\n",
      "Restoring best model from epoch 3.\n",
      "     10        2.3875        2.3616  0.0442\n",
      "Restoring best model from epoch 8.\n",
      "      8        \u001b[36m2.6035\u001b[0m        2.6902  0.0353\n",
      "      9        \u001b[36m2.1202\u001b[0m        2.1456  0.0407\n",
      "      9        \u001b[36m2.1709\u001b[0m        2.1257  0.0350\n",
      "      9        2.1492        2.1126  0.0336\n",
      "      8        \u001b[36m2.3646\u001b[0m        2.3568  0.0427\n",
      "      9        2.6100        2.6894  0.0345\n",
      "     10        \u001b[36m2.1166\u001b[0m        2.1407  0.0316\n",
      "Restoring best model from epoch 2.\n",
      "     10        \u001b[36m2.1405\u001b[0m        2.1137  0.0312\n",
      "Restoring best model from epoch 3.\n",
      "     10        \u001b[36m2.1676\u001b[0m        2.1278  0.0354\n",
      "Restoring best model from epoch 2.\n",
      "      9        2.3719        2.3621  0.0324\n",
      "     10        \u001b[36m2.5876\u001b[0m        2.6855  0.0338\n",
      "Restoring best model from epoch 3.\n",
      "     10        \u001b[36m2.3562\u001b[0m        2.3607  0.0321\n",
      "Restoring best model from epoch 3.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.4707\u001b[0m        \u001b[32m2.3469\u001b[0m  0.0472\n",
      "      2        \u001b[36m2.3852\u001b[0m        \u001b[32m2.2987\u001b[0m  0.0442\n",
      "      3        \u001b[36m2.3702\u001b[0m        2.3016  0.0419\n",
      "      4        \u001b[36m2.3586\u001b[0m        \u001b[32m2.2972\u001b[0m  0.0425\n",
      "      5        \u001b[36m2.3527\u001b[0m        \u001b[32m2.2918\u001b[0m  0.0400\n",
      "      6        2.3546        2.3035  0.0442\n",
      "      7        \u001b[36m2.3424\u001b[0m        2.3022  0.0427\n",
      "      8        2.3476        2.2970  0.0413\n",
      "      9        2.3456        2.3042  0.0434\n",
      "     10        \u001b[36m2.3272\u001b[0m        2.3069  0.0414\n",
      "Restoring best model from epoch 5.\n",
      "y_train breslow final [ -0.26283368  -0.49281314  -0.52566737 ... -84.20534    -85.81519\n",
      " -87.359344  ]\n",
      "Concordance Index 0.696600728087464\n",
      "Integrated Brier Score: 0.1710770971915428\n",
      "y_train breslow final [ -0.26283368  -0.49281314  -0.52566737 ... -84.20534    -85.81519\n",
      " -87.359344  ]\n",
      "durations 1.4784395 84.0\n",
      "Concordance Index 0.6534227031683222\n",
      "Integrated Brier Score: 0.17994912425645157\n",
      "(1786, 9) <class 'pandas.core.frame.DataFrame'>\n",
      "(1786,) <class 'pandas.core.series.Series'>\n",
      "(446, 9) <class 'pandas.core.frame.DataFrame'>\n",
      "(446,) <class 'pandas.core.series.Series'>\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.2297\u001b[0m        \u001b[32m2.1158\u001b[0m  0.0300\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.3223\u001b[0m        \u001b[32m2.1217\u001b[0m  0.0315\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.3304\u001b[0m        \u001b[32m2.1456\u001b[0m  0.0308\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.8013\u001b[0m        \u001b[32m2.7487\u001b[0m  0.0299\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.4863\u001b[0m        \u001b[32m2.4289\u001b[0m  0.0343\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.3063\u001b[0m        \u001b[32m2.1168\u001b[0m  0.0319\n",
      "      2        \u001b[36m2.1735\u001b[0m        2.1235  0.0328\n",
      "      2        \u001b[36m2.2406\u001b[0m        \u001b[32m2.1040\u001b[0m  0.0372\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.3147\u001b[0m        \u001b[32m2.1515\u001b[0m  0.0352\n",
      "      2        \u001b[36m2.2742\u001b[0m        \u001b[32m2.1397\u001b[0m  0.0309\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.3382\u001b[0m        \u001b[32m2.1624\u001b[0m  0.0394\n",
      "      2        \u001b[36m2.4106\u001b[0m        2.4326  0.0342\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.8169\u001b[0m        \u001b[32m2.7724\u001b[0m  0.0363\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.4990\u001b[0m        \u001b[32m2.4447\u001b[0m  0.0472\n",
      "      2        \u001b[36m2.7179\u001b[0m        \u001b[32m2.7394\u001b[0m  0.0415\n",
      "      3        \u001b[36m2.1642\u001b[0m        2.1190  0.0353\n",
      "      2        \u001b[36m2.2323\u001b[0m        \u001b[32m2.1038\u001b[0m  0.0383\n",
      "      3        \u001b[36m2.2366\u001b[0m        \u001b[32m2.0809\u001b[0m  0.0350\n",
      "      3        \u001b[36m2.2529\u001b[0m        \u001b[32m2.1268\u001b[0m  0.0365\n",
      "      2        \u001b[36m2.2522\u001b[0m        \u001b[32m2.1138\u001b[0m  0.0413\n",
      "      3        \u001b[36m2.3982\u001b[0m        \u001b[32m2.4263\u001b[0m  0.0297\n",
      "      2        \u001b[36m2.2670\u001b[0m        \u001b[32m2.1476\u001b[0m  0.0410\n",
      "      2        \u001b[36m2.7211\u001b[0m        \u001b[32m2.7226\u001b[0m  0.0359\n",
      "      2        \u001b[36m2.4031\u001b[0m        \u001b[32m2.4240\u001b[0m  0.0351\n",
      "      3        \u001b[36m2.1802\u001b[0m        2.1054  0.0315\n",
      "      4        \u001b[36m2.2325\u001b[0m        2.0861  0.0403\n",
      "      3        \u001b[36m2.6932\u001b[0m        \u001b[32m2.7338\u001b[0m  0.0571\n",
      "      3        \u001b[36m2.2428\u001b[0m        \u001b[32m2.1101\u001b[0m  0.0393\n",
      "      4        \u001b[36m2.2495\u001b[0m        \u001b[32m2.1208\u001b[0m  0.0428\n",
      "      4        \u001b[36m2.1602\u001b[0m        2.1191  0.0633\n",
      "      4        \u001b[36m2.3868\u001b[0m        \u001b[32m2.4182\u001b[0m  0.0452\n",
      "      4        \u001b[36m2.1733\u001b[0m        \u001b[32m2.0990\u001b[0m  0.0549\n",
      "      3        \u001b[36m2.6989\u001b[0m        2.7312  0.0602\n",
      "      3        \u001b[36m2.3953\u001b[0m        2.4264  0.0573\n",
      "      3        \u001b[36m2.2466\u001b[0m        \u001b[32m2.1263\u001b[0m  0.0651\n",
      "      5        \u001b[36m2.2111\u001b[0m        2.0848  0.0455\n",
      "      5        \u001b[36m2.3721\u001b[0m        2.4193  0.0356\n",
      "      4        2.6952        \u001b[32m2.7326\u001b[0m  0.0478\n",
      "      5        \u001b[36m2.1568\u001b[0m        2.1252  0.0405\n",
      "      5        \u001b[36m2.2232\u001b[0m        \u001b[32m2.1127\u001b[0m  0.0456\n",
      "      4        \u001b[36m2.2209\u001b[0m        \u001b[32m2.0773\u001b[0m  0.0498\n",
      "      5        \u001b[36m2.1713\u001b[0m        2.1159  0.0302\n",
      "      4        \u001b[36m2.3684\u001b[0m        2.4261  0.0308\n",
      "      4        \u001b[36m2.2188\u001b[0m        2.1355  0.0325\n",
      "      6        2.2123        2.0881  0.0351\n",
      "      6        \u001b[36m2.1507\u001b[0m        2.1211  0.0306\n",
      "      6        2.3789        2.4210  0.0355\n",
      "      5        \u001b[36m2.6805\u001b[0m        \u001b[32m2.7297\u001b[0m  0.0342\n",
      "      5        \u001b[36m2.2123\u001b[0m        2.0816  0.0340\n",
      "      6        2.2282        \u001b[32m2.1127\u001b[0m  0.0411\n",
      "      4        \u001b[36m2.6815\u001b[0m        \u001b[32m2.7146\u001b[0m  0.0580\n",
      "      6        \u001b[36m2.1689\u001b[0m        2.1068  0.0303\n",
      "      5        2.3923        \u001b[32m2.4233\u001b[0m  0.0331\n",
      "      7        2.2138        \u001b[32m2.0795\u001b[0m  0.0321\n",
      "      5        2.2316        \u001b[32m2.1226\u001b[0m  0.0433\n",
      "      7        2.1521        2.1214  0.0360\n",
      "      6        2.6820        2.7382  0.0355\n",
      "      6        \u001b[36m2.1983\u001b[0m        2.0850  0.0365\n",
      "      7        \u001b[36m2.1569\u001b[0m        2.1096  0.0317\n",
      "      5        2.6864        \u001b[32m2.7098\u001b[0m  0.0348\n",
      "      7        \u001b[36m2.3677\u001b[0m        2.4205  0.0530\n",
      "      6        2.3686        2.4260  0.0371\n",
      "      8        \u001b[36m2.1497\u001b[0m        2.1195  0.0277\n",
      "      7        2.2258        2.1134  0.0543\n",
      "      8        2.2136        \u001b[32m2.0710\u001b[0m  0.0381\n",
      "      7        2.2111        2.0916  0.0306\n",
      "      8        2.1649        2.1098  0.0363\n",
      "      6        \u001b[36m2.2130\u001b[0m        2.1258  0.0479\n",
      "      7        \u001b[36m2.6783\u001b[0m        \u001b[32m2.7268\u001b[0m  0.0496\n",
      "      6        \u001b[36m2.6714\u001b[0m        2.7206  0.0412\n",
      "      9        \u001b[36m2.1415\u001b[0m        2.1186  0.0322\n",
      "      9        2.2146        2.0827  0.0287\n",
      "      7        \u001b[36m2.3524\u001b[0m        2.4316  0.0410\n",
      "      8        2.3809        2.4319  0.0432\n",
      "      8        2.2088        2.1011  0.0316\n",
      "      8        \u001b[36m2.2222\u001b[0m        2.1207  0.0527\n",
      "      7        2.2172        2.1426  0.0368\n",
      "      8        \u001b[36m2.6707\u001b[0m        2.7337  0.0322\n",
      "      7        2.6755        2.7120  0.0345\n",
      "      9        \u001b[36m2.1511\u001b[0m        2.1084  0.0445\n",
      "     10        \u001b[36m2.2027\u001b[0m        2.0869  0.0316\n",
      "      9        2.3759        2.4348  0.0304\n",
      "Restoring best model from epoch 8.\n",
      "     10        2.1419        2.1182  0.0376\n",
      "Restoring best model from epoch 1.\n",
      "      8        \u001b[36m2.3510\u001b[0m        2.4479  0.0369\n",
      "      9        2.1993        2.0897  0.0419\n",
      "      8        2.2151        2.1530  0.0337\n",
      "      9        \u001b[36m2.2186\u001b[0m        2.1219  0.0372\n",
      "      9        2.6809        \u001b[32m2.7259\u001b[0m  0.0366\n",
      "     10        2.3782        2.4227  0.0307\n",
      "Restoring best model from epoch 4.\n",
      "     10        2.1683        2.1052  0.0325\n",
      "Restoring best model from epoch 4.\n",
      "      9        \u001b[36m2.3497\u001b[0m        2.4252  0.0334\n",
      "      8        2.6737        2.7255  0.0486\n",
      "     10        \u001b[36m2.1850\u001b[0m        \u001b[32m2.0759\u001b[0m  0.0311\n",
      "      9        \u001b[36m2.1989\u001b[0m        2.1386  0.0317\n",
      "     10        \u001b[36m2.2170\u001b[0m        2.1221  0.0299\n",
      "Restoring best model from epoch 5.\n",
      "     10        2.6714        2.7298  0.0301\n",
      "Restoring best model from epoch 9.\n",
      "     10        2.3533        2.4372  0.0315\n",
      "Restoring best model from epoch 5.\n",
      "      9        \u001b[36m2.6611\u001b[0m        2.7174  0.0330\n",
      "     10        \u001b[36m2.1945\u001b[0m        2.1448  0.0310\n",
      "Restoring best model from epoch 5.\n",
      "     10        2.6614        2.7119  0.0337\n",
      "Restoring best model from epoch 5.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.4083\u001b[0m        \u001b[32m2.4485\u001b[0m  0.0421\n",
      "      2        \u001b[36m2.3765\u001b[0m        \u001b[32m2.4407\u001b[0m  0.0411\n",
      "      3        \u001b[36m2.3665\u001b[0m        \u001b[32m2.4354\u001b[0m  0.0406\n",
      "      4        \u001b[36m2.3525\u001b[0m        2.4453  0.0392\n",
      "      5        2.3562        \u001b[32m2.4345\u001b[0m  0.0409\n",
      "      6        2.3528        2.4427  0.0390\n",
      "      7        \u001b[36m2.3448\u001b[0m        2.4497  0.0394\n",
      "      8        2.3513        2.4378  0.0393\n",
      "      9        2.3469        2.4465  0.0466\n",
      "     10        \u001b[36m2.3405\u001b[0m        2.4387  0.0425\n",
      "Restoring best model from epoch 5.\n",
      "y_train breslow final [ -0.49281314  -0.52566737  -0.55852157 ... -84.         -85.81519\n",
      " -87.359344  ]\n",
      "Concordance Index 0.6871974917644146\n",
      "Integrated Brier Score: 0.17201626194282923\n",
      "y_train breslow final [ -0.49281314  -0.52566737  -0.55852157 ... -84.         -85.81519\n",
      " -87.359344  ]\n",
      "durations 0.26283368 84.20534\n",
      "Concordance Index 0.6794536277939488\n",
      "Integrated Brier Score: 0.17835665499289102\n",
      "load_support\n",
      "split age                  float32\n",
      "sex                 category\n",
      "race                 float32\n",
      "n_comorbidities      float32\n",
      "diabetes            category\n",
      "dementia            category\n",
      "blood_pressure       float32\n",
      "heart_rate           float32\n",
      "respiration_rate     float32\n",
      "temperature          float32\n",
      "white_blood_cell     float32\n",
      "serum_sodium         float32\n",
      "serum_creatinine     float32\n",
      "cancer_0.0             uint8\n",
      "cancer_1.0             uint8\n",
      "cancer_2.0             uint8\n",
      "dtype: object\n",
      "(7098, 16) <class 'pandas.core.frame.DataFrame'>\n",
      "(7098,) <class 'pandas.core.series.Series'>\n",
      "(1775, 16) <class 'pandas.core.frame.DataFrame'>\n",
      "(1775,) <class 'pandas.core.series.Series'>\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.5903\u001b[0m        \u001b[32m3.5249\u001b[0m  0.1449\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.9021\u001b[0m        \u001b[32m3.9032\u001b[0m  0.1614\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m4.1970\u001b[0m        \u001b[32m4.1514\u001b[0m  0.1535\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.4050\u001b[0m        \u001b[32m3.3249\u001b[0m  0.1737\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m4.0839\u001b[0m        \u001b[32m3.9338\u001b[0m  0.1702\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.4439\u001b[0m        \u001b[32m3.3519\u001b[0m  0.1533\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.9345\u001b[0m        \u001b[32m3.9168\u001b[0m  0.1817\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m4.2310\u001b[0m        \u001b[32m4.1708\u001b[0m  0.1521\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m4.1190\u001b[0m        \u001b[32m3.9547\u001b[0m  0.1601\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.6327\u001b[0m        \u001b[32m3.5410\u001b[0m  0.1987\n",
      "      2        \u001b[36m3.5661\u001b[0m        \u001b[32m3.5188\u001b[0m  0.1199\n",
      "      2        \u001b[36m3.8897\u001b[0m        \u001b[32m3.9023\u001b[0m  0.1484\n",
      "      2        \u001b[36m4.1787\u001b[0m        \u001b[32m4.1477\u001b[0m  0.1432\n",
      "      2        \u001b[36m4.0667\u001b[0m        \u001b[32m3.9317\u001b[0m  0.1501\n",
      "      2        \u001b[36m3.3803\u001b[0m        3.3275  0.1635\n",
      "      2        \u001b[36m3.3917\u001b[0m        3.3592  0.1705\n",
      "      2        \u001b[36m3.9097\u001b[0m        3.9190  0.1654\n",
      "      2        \u001b[36m3.5802\u001b[0m        \u001b[32m3.5382\u001b[0m  0.1542\n",
      "      2        \u001b[36m4.0912\u001b[0m        \u001b[32m3.9488\u001b[0m  0.1713\n",
      "      2        \u001b[36m4.1941\u001b[0m        \u001b[32m4.1632\u001b[0m  0.1813\n",
      "      3        \u001b[36m3.5627\u001b[0m        \u001b[32m3.5154\u001b[0m  0.1498\n",
      "      3        \u001b[36m3.8880\u001b[0m        \u001b[32m3.8983\u001b[0m  0.1536\n",
      "      3        \u001b[36m4.0582\u001b[0m        \u001b[32m3.9306\u001b[0m  0.1321\n",
      "      3        \u001b[36m4.1751\u001b[0m        4.1481  0.1569\n",
      "      3        \u001b[36m3.3769\u001b[0m        \u001b[32m3.3223\u001b[0m  0.1389\n",
      "      3        3.3972        \u001b[32m3.3501\u001b[0m  0.1608\n",
      "      4        \u001b[36m3.5552\u001b[0m        \u001b[32m3.5138\u001b[0m  0.1247\n",
      "      3        \u001b[36m3.5770\u001b[0m        3.5396  0.1579\n",
      "      3        \u001b[36m3.8991\u001b[0m        \u001b[32m3.9087\u001b[0m  0.1889\n",
      "      3        \u001b[36m4.0755\u001b[0m        \u001b[32m3.9457\u001b[0m  0.1637\n",
      "      3        \u001b[36m4.1912\u001b[0m        4.1669  0.1612\n",
      "      4        \u001b[36m4.0548\u001b[0m        3.9318  0.1430\n",
      "      4        \u001b[36m4.1681\u001b[0m        4.1478  0.1447\n",
      "      4        \u001b[36m3.8862\u001b[0m        3.9015  0.1643\n",
      "      4        \u001b[36m3.3691\u001b[0m        \u001b[32m3.3184\u001b[0m  0.1632\n",
      "      5        3.5554        3.5149  0.1498\n",
      "      4        \u001b[36m3.3901\u001b[0m        \u001b[32m3.3385\u001b[0m  0.1571\n",
      "      4        \u001b[36m3.5655\u001b[0m        \u001b[32m3.5209\u001b[0m  0.1729\n",
      "      4        \u001b[36m3.8966\u001b[0m        3.9150  0.1599\n",
      "      5        \u001b[36m4.0525\u001b[0m        3.9336  0.1200\n",
      "      4        \u001b[36m4.1889\u001b[0m        4.1700  0.1682\n",
      "      4        4.0787        3.9470  0.1835\n",
      "      5        \u001b[36m4.1680\u001b[0m        4.1495  0.1402\n",
      "      5        \u001b[36m3.8790\u001b[0m        3.9042  0.1463\n",
      "      5        3.3692        \u001b[32m3.3181\u001b[0m  0.1354\n",
      "      6        \u001b[36m3.5526\u001b[0m        3.5148  0.1439\n",
      "      6        4.0527        \u001b[32m3.9286\u001b[0m  0.1298\n",
      "      5        \u001b[36m3.3810\u001b[0m        \u001b[32m3.3348\u001b[0m  0.1780\n",
      "      5        \u001b[36m3.5603\u001b[0m        3.5225  0.1519\n",
      "      5        3.9000        3.9176  0.1823\n",
      "      6        3.8791        3.9009  0.1456\n",
      "      6        \u001b[36m4.1661\u001b[0m        \u001b[32m4.1470\u001b[0m  0.1545\n",
      "      6        \u001b[36m3.3672\u001b[0m        \u001b[32m3.3154\u001b[0m  0.1347\n",
      "      5        \u001b[36m4.1879\u001b[0m        4.1675  0.1804\n",
      "      5        \u001b[36m4.0726\u001b[0m        3.9458  0.1711\n",
      "      7        \u001b[36m3.5517\u001b[0m        \u001b[32m3.5134\u001b[0m  0.1487\n",
      "      7        \u001b[36m4.0499\u001b[0m        3.9312  0.1315\n",
      "      6        \u001b[36m3.3752\u001b[0m        3.3408  0.1621\n",
      "      6        \u001b[36m3.5589\u001b[0m        3.5298  0.1615\n",
      "      7        \u001b[36m4.1644\u001b[0m        4.1490  0.1331\n",
      "      7        \u001b[36m3.8776\u001b[0m        3.8986  0.1350\n",
      "      7        \u001b[36m3.3658\u001b[0m        3.3178  0.1467\n",
      "      6        \u001b[36m3.8962\u001b[0m        3.9203  0.1663\n",
      "      6        \u001b[36m4.0657\u001b[0m        3.9526  0.1661\n",
      "      6        \u001b[36m4.1825\u001b[0m        4.1641  0.1686\n",
      "      8        \u001b[36m4.0446\u001b[0m        3.9294  0.1335\n",
      "      8        \u001b[36m3.5468\u001b[0m        3.5184  0.1587\n",
      "      7        3.3763        \u001b[32m3.3271\u001b[0m  0.1543\n",
      "      8        \u001b[36m4.1623\u001b[0m        4.1481  0.1441\n",
      "      8        \u001b[36m3.8742\u001b[0m        3.9025  0.1547\n",
      "      8        3.3661        3.3158  0.1396\n",
      "      7        \u001b[36m3.5582\u001b[0m        \u001b[32m3.5205\u001b[0m  0.1654\n",
      "      7        \u001b[36m3.8899\u001b[0m        3.9103  0.1580\n",
      "      7        4.0716        3.9580  0.1765\n",
      "      9        4.0475        \u001b[32m3.9282\u001b[0m  0.1448\n",
      "      9        3.5477        3.5167  0.1330\n",
      "      7        \u001b[36m4.1781\u001b[0m        4.1728  0.1961\n",
      "      9        \u001b[36m3.3657\u001b[0m        \u001b[32m3.3137\u001b[0m  0.1217\n",
      "      9        4.1632        4.1480  0.1431\n",
      "      9        3.8753        3.9029  0.1357\n",
      "      8        \u001b[36m3.3679\u001b[0m        3.3349  0.1682\n",
      "      8        \u001b[36m3.5553\u001b[0m        3.5267  0.1626\n",
      "      8        3.8935        \u001b[32m3.9012\u001b[0m  0.1485\n",
      "     10        3.5471        3.5180  0.1355\n",
      "Restoring best model from epoch 7.\n",
      "     10        4.0472        3.9341  0.1516\n",
      "Restoring best model from epoch 6.\n",
      "      8        \u001b[36m4.0645\u001b[0m        3.9527  0.1839\n",
      "     10        \u001b[36m3.8672\u001b[0m        \u001b[32m3.8974\u001b[0m  0.1231\n",
      "     10        \u001b[36m4.1595\u001b[0m        4.1484  0.1345\n",
      "Restoring best model from epoch 6.\n",
      "     10        \u001b[36m3.3623\u001b[0m        \u001b[32m3.3133\u001b[0m  0.1539\n",
      "      8        4.1788        4.1691  0.2092\n",
      "      9        3.3690        \u001b[32m3.3242\u001b[0m  0.1615\n",
      "      9        \u001b[36m3.5523\u001b[0m        3.5261  0.1588\n",
      "      9        \u001b[36m3.8888\u001b[0m        3.9067  0.1573\n",
      "      9        \u001b[36m4.0615\u001b[0m        3.9570  0.1651\n",
      "      9        \u001b[36m4.1653\u001b[0m        4.1643  0.1562\n",
      "     10        \u001b[36m3.3655\u001b[0m        3.3269  0.1482\n",
      "Restoring best model from epoch 9.\n",
      "     10        3.5524        3.5255  0.1471\n",
      "Restoring best model from epoch 7.\n",
      "     10        \u001b[36m3.8849\u001b[0m        3.9046  0.1454\n",
      "Restoring best model from epoch 8.\n",
      "     10        \u001b[36m4.0514\u001b[0m        3.9510  0.1516\n",
      "Restoring best model from epoch 3.\n",
      "     10        \u001b[36m4.1607\u001b[0m        4.1643  0.1495\n",
      "Restoring best model from epoch 2.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.9309\u001b[0m        \u001b[32m3.9952\u001b[0m  0.1656\n",
      "      2        \u001b[36m3.9009\u001b[0m        \u001b[32m3.9918\u001b[0m  0.1656\n",
      "      3        \u001b[36m3.8964\u001b[0m        \u001b[32m3.9899\u001b[0m  0.1818\n",
      "      4        \u001b[36m3.8925\u001b[0m        3.9919  0.1652\n",
      "      5        \u001b[36m3.8872\u001b[0m        3.9930  0.1650\n",
      "      6        3.8885        3.9935  0.1666\n",
      "      7        3.8904        3.9914  0.1654\n",
      "      8        \u001b[36m3.8765\u001b[0m        \u001b[32m3.9862\u001b[0m  0.1653\n",
      "      9        3.8830        3.9897  0.1656\n",
      "     10        3.8794        3.9903  0.1622\n",
      "Restoring best model from epoch 8.\n",
      "y_train breslow final [    3.     3.     3. ... -2029. -2029. -2029.]\n",
      "Concordance Index 0.623423171310733\n",
      "Integrated Brier Score: 0.22218875735287988\n",
      "y_train breslow final [    3.     3.     3. ... -2029. -2029. -2029.]\n",
      "durations 3.0 2024.0\n",
      "Concordance Index 0.5992737386101705\n",
      "Integrated Brier Score: 0.23314419834693287\n",
      "(7098, 16) <class 'pandas.core.frame.DataFrame'>\n",
      "(7098,) <class 'pandas.core.series.Series'>\n",
      "(1775, 16) <class 'pandas.core.frame.DataFrame'>\n",
      "(1775,) <class 'pandas.core.series.Series'>\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.9329\u001b[0m        \u001b[32m3.9663\u001b[0m  0.1365\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.4205\u001b[0m        \u001b[32m3.3697\u001b[0m  0.1445\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.6365\u001b[0m        \u001b[32m3.5617\u001b[0m  0.1595\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m4.1056\u001b[0m        \u001b[32m4.0267\u001b[0m  0.2056\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m4.2417\u001b[0m        \u001b[32m4.1558\u001b[0m  0.1874\n",
      "      2        \u001b[36m3.9003\u001b[0m        3.9685  0.1572\n",
      "      2        \u001b[36m3.3443\u001b[0m        3.3707  0.1526\n",
      "      2        \u001b[36m3.5498\u001b[0m        \u001b[32m3.5617\u001b[0m  0.1618\n",
      "      2        \u001b[36m4.0773\u001b[0m        \u001b[32m4.0213\u001b[0m  0.1760\n",
      "      2        \u001b[36m4.2042\u001b[0m        \u001b[32m4.1522\u001b[0m  0.2037\n",
      "      3        \u001b[36m3.8948\u001b[0m        \u001b[32m3.9639\u001b[0m  0.1453\n",
      "      3        \u001b[36m3.5342\u001b[0m        3.5630  0.1398\n",
      "      3        \u001b[36m3.3304\u001b[0m        3.3718  0.1608\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.9617\u001b[0m        \u001b[32m3.9908\u001b[0m  0.5373\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.5171\u001b[0m        \u001b[32m3.4202\u001b[0m  0.5493\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.7265\u001b[0m        \u001b[32m3.6829\u001b[0m  0.5562\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m4.1427\u001b[0m        \u001b[32m4.0607\u001b[0m  0.5578\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m4.2905\u001b[0m        \u001b[32m4.1757\u001b[0m  0.5630\n",
      "      3        \u001b[36m4.0562\u001b[0m        4.0237  0.1624\n",
      "      4        \u001b[36m3.8903\u001b[0m        \u001b[32m3.9616\u001b[0m  0.1333\n",
      "      4        \u001b[36m3.5307\u001b[0m        \u001b[32m3.5611\u001b[0m  0.1417\n",
      "      4        \u001b[36m3.3300\u001b[0m        3.3706  0.1470\n",
      "      3        \u001b[36m4.1953\u001b[0m        \u001b[32m4.1513\u001b[0m  0.1862\n",
      "      5        \u001b[36m3.8890\u001b[0m        \u001b[32m3.9585\u001b[0m  0.1456\n",
      "      4        \u001b[36m4.0545\u001b[0m        4.0229  0.1580\n",
      "      5        \u001b[36m3.3298\u001b[0m        \u001b[32m3.3693\u001b[0m  0.1418\n",
      "      5        \u001b[36m3.5297\u001b[0m        \u001b[32m3.5596\u001b[0m  0.1578\n",
      "      4        \u001b[36m4.1916\u001b[0m        \u001b[32m4.1503\u001b[0m  0.1707\n",
      "      6        \u001b[36m3.8864\u001b[0m        3.9589  0.1386\n",
      "      5        \u001b[36m4.0543\u001b[0m        4.0216  0.1843\n",
      "      6        3.3307        \u001b[32m3.3686\u001b[0m  0.1625\n",
      "      6        \u001b[36m3.5278\u001b[0m        \u001b[32m3.5581\u001b[0m  0.1639\n",
      "      5        \u001b[36m4.1887\u001b[0m        4.1507  0.1880\n",
      "      7        \u001b[36m3.8824\u001b[0m        3.9600  0.1515\n",
      "      2        \u001b[36m3.9193\u001b[0m        \u001b[32m3.9878\u001b[0m  0.5484\n",
      "      6        \u001b[36m4.0525\u001b[0m        4.0228  0.1659\n",
      "      2        \u001b[36m3.6601\u001b[0m        \u001b[32m3.6225\u001b[0m  0.5422\n",
      "      7        \u001b[36m3.3249\u001b[0m        3.3697  0.1601\n",
      "      7        \u001b[36m3.5212\u001b[0m        \u001b[32m3.5576\u001b[0m  0.1525\n",
      "      2        \u001b[36m3.4266\u001b[0m        \u001b[32m3.4089\u001b[0m  0.5785\n",
      "      2        \u001b[36m4.2368\u001b[0m        \u001b[32m4.1561\u001b[0m  0.5648\n",
      "      2        \u001b[36m4.1030\u001b[0m        \u001b[32m4.0599\u001b[0m  0.5789\n",
      "      8        3.8843        3.9594  0.1361\n",
      "      6        4.1904        4.1522  0.2436\n",
      "      8        3.3265        \u001b[32m3.3685\u001b[0m  0.1414\n",
      "      8        3.5244        \u001b[32m3.5563\u001b[0m  0.1493\n",
      "      7        4.0544        4.0217  0.1619\n",
      "      9        \u001b[36m3.8824\u001b[0m        \u001b[32m3.9583\u001b[0m  0.1617\n",
      "      9        3.5249        3.5583  0.1368\n",
      "      9        \u001b[36m3.3222\u001b[0m        3.3689  0.1686\n",
      "      8        \u001b[36m4.0478\u001b[0m        4.0220  0.1698\n",
      "      7        \u001b[36m4.1863\u001b[0m        4.1523  0.1862\n",
      "     10        \u001b[36m3.8787\u001b[0m        3.9615  0.1345\n",
      "Restoring best model from epoch 5.\n",
      "     10        3.5253        3.5593  0.1500\n",
      "Restoring best model from epoch 8.\n",
      "     10        3.3237        3.3689  0.1643\n",
      "Restoring best model from epoch 6.\n",
      "      9        4.0516        4.0224  0.1770\n",
      "      3        \u001b[36m3.9023\u001b[0m        \u001b[32m3.9626\u001b[0m  0.5373\n",
      "      8        \u001b[36m4.1851\u001b[0m        4.1513  0.1993\n",
      "      3        \u001b[36m3.3988\u001b[0m        3.4095  0.5382\n",
      "      3        \u001b[36m3.6081\u001b[0m        \u001b[32m3.6077\u001b[0m  0.5807\n",
      "      3        \u001b[36m4.2226\u001b[0m        4.1700  0.5523\n",
      "      3        \u001b[36m4.0705\u001b[0m        \u001b[32m4.0471\u001b[0m  0.5651\n",
      "     10        4.0489        4.0226  0.1600\n",
      "Restoring best model from epoch 2.\n",
      "      9        4.1870        \u001b[32m4.1502\u001b[0m  0.1648\n",
      "     10        \u001b[36m4.1824\u001b[0m        4.1515  0.1555\n",
      "Restoring best model from epoch 4.\n",
      "      4        \u001b[36m3.8881\u001b[0m        3.9687  0.4731\n",
      "      4        \u001b[36m3.3736\u001b[0m        \u001b[32m3.3866\u001b[0m  0.4646\n",
      "      4        \u001b[36m3.5872\u001b[0m        \u001b[32m3.5865\u001b[0m  0.4611\n",
      "      4        \u001b[36m4.2184\u001b[0m        \u001b[32m4.1507\u001b[0m  0.4671\n",
      "      4        \u001b[36m4.0607\u001b[0m        \u001b[32m4.0393\u001b[0m  0.4630\n",
      "      5        \u001b[36m3.8840\u001b[0m        3.9727  0.4576\n",
      "      5        \u001b[36m3.3568\u001b[0m        \u001b[32m3.3855\u001b[0m  0.4576\n",
      "      5        \u001b[36m3.5552\u001b[0m        \u001b[32m3.5798\u001b[0m  0.4568\n",
      "      5        \u001b[36m4.2023\u001b[0m        \u001b[32m4.1495\u001b[0m  0.4640\n",
      "      5        \u001b[36m4.0553\u001b[0m        \u001b[32m4.0285\u001b[0m  0.4608\n",
      "      6        \u001b[36m3.8815\u001b[0m        \u001b[32m3.9566\u001b[0m  0.4667\n",
      "      6        \u001b[36m3.3517\u001b[0m        \u001b[32m3.3796\u001b[0m  0.4670\n",
      "      6        \u001b[36m3.5448\u001b[0m        \u001b[32m3.5740\u001b[0m  0.4680\n",
      "      6        \u001b[36m4.1988\u001b[0m        \u001b[32m4.1493\u001b[0m  0.4750\n",
      "      6        \u001b[36m4.0477\u001b[0m        4.0341  0.4757\n",
      "      7        \u001b[36m3.8688\u001b[0m        3.9637  0.4641\n",
      "      7        \u001b[36m3.3430\u001b[0m        \u001b[32m3.3795\u001b[0m  0.4661\n",
      "      7        \u001b[36m3.5358\u001b[0m        \u001b[32m3.5691\u001b[0m  0.4678\n",
      "      7        \u001b[36m4.1977\u001b[0m        4.1511  0.4714\n",
      "      7        \u001b[36m4.0415\u001b[0m        \u001b[32m4.0283\u001b[0m  0.4718\n",
      "      8        3.8715        3.9567  0.4674\n",
      "      8        \u001b[36m3.3391\u001b[0m        3.3808  0.4667\n",
      "      8        \u001b[36m3.5321\u001b[0m        \u001b[32m3.5690\u001b[0m  0.4654\n",
      "      8        \u001b[36m4.1905\u001b[0m        4.1506  0.4740\n",
      "      8        \u001b[36m4.0403\u001b[0m        \u001b[32m4.0217\u001b[0m  0.4705\n",
      "      9        \u001b[36m3.8663\u001b[0m        3.9585  0.4776\n",
      "      9        \u001b[36m3.3369\u001b[0m        \u001b[32m3.3772\u001b[0m  0.4809\n",
      "      9        \u001b[36m3.5298\u001b[0m        \u001b[32m3.5682\u001b[0m  0.4798\n",
      "      9        \u001b[36m4.1902\u001b[0m        4.1511  0.4898\n",
      "      9        \u001b[36m4.0340\u001b[0m        4.0270  0.4883\n",
      "     10        \u001b[36m3.8647\u001b[0m        3.9576  0.4757\n",
      "Restoring best model from epoch 6.\n",
      "     10        \u001b[36m3.3355\u001b[0m        3.3786  0.4735\n",
      "     10        3.5312        \u001b[32m3.5681\u001b[0m  0.4698\n",
      "Restoring best model from epoch 9.\n",
      "Restoring best model from epoch 9.\n",
      "     10        \u001b[36m4.1898\u001b[0m        \u001b[32m4.1440\u001b[0m  0.4808\n",
      "     10        4.0371        4.0251  0.4760\n",
      "Restoring best model from epoch 8.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.9482\u001b[0m        \u001b[32m3.9462\u001b[0m  0.1170\n",
      "      2        \u001b[36m3.9272\u001b[0m        \u001b[32m3.9425\u001b[0m  0.1214\n",
      "      3        \u001b[36m3.9225\u001b[0m        \u001b[32m3.9424\u001b[0m  0.1138\n",
      "      4        \u001b[36m3.9193\u001b[0m        \u001b[32m3.9419\u001b[0m  0.1293\n",
      "      5        \u001b[36m3.9177\u001b[0m        \u001b[32m3.9415\u001b[0m  0.1126\n",
      "      6        \u001b[36m3.9171\u001b[0m        3.9425  0.1127\n",
      "      7        \u001b[36m3.9156\u001b[0m        3.9417  0.1148\n",
      "      8        3.9159        \u001b[32m3.9400\u001b[0m  0.1146\n",
      "      9        3.9169        3.9403  0.1106\n",
      "     10        \u001b[36m3.9119\u001b[0m        3.9422  0.1126\n",
      "Restoring best model from epoch 8.\n",
      "y_train breslow final [    3.     3.     3. ... -2029. -2029. -2029.]\n",
      "Concordance Index 0.6166509653609096\n",
      "Integrated Brier Score: 0.19871467507995483\n",
      "y_train breslow final [    3.     3.     3. ... -2029. -2029. -2029.]\n",
      "durations 3.0 2026.0\n",
      "Concordance Index 0.6147353407938461\n",
      "Integrated Brier Score: 0.1992408110444531\n",
      "(7098, 16) <class 'pandas.core.frame.DataFrame'>\n",
      "(7098,) <class 'pandas.core.series.Series'>\n",
      "(1775, 16) <class 'pandas.core.frame.DataFrame'>\n",
      "(1775,) <class 'pandas.core.series.Series'>\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.9369\u001b[0m        \u001b[32m3.9371\u001b[0m  0.1996\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.6376\u001b[0m        \u001b[32m3.5426\u001b[0m  0.1962\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m4.0850\u001b[0m        \u001b[32m4.0162\u001b[0m  0.2095\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.4153\u001b[0m        \u001b[32m3.3300\u001b[0m  0.1981\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m4.2358\u001b[0m        \u001b[32m4.1517\u001b[0m  0.2231\n",
      "      2        \u001b[36m3.9056\u001b[0m        \u001b[32m3.9370\u001b[0m  0.1451\n",
      "      2        \u001b[36m3.5522\u001b[0m        3.5440  0.1453\n",
      "      2        \u001b[36m3.3312\u001b[0m        3.3369  0.1576\n",
      "      2        \u001b[36m4.0325\u001b[0m        \u001b[32m4.0159\u001b[0m  0.1924\n",
      "      2        \u001b[36m4.1897\u001b[0m        4.1531  0.1805\n",
      "      3        \u001b[36m3.9037\u001b[0m        3.9386  0.1603\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m4.1168\u001b[0m        \u001b[32m4.0766\u001b[0m  0.5448\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.4866\u001b[0m        \u001b[32m3.4419\u001b[0m  0.5835\n",
      "      3        \u001b[36m3.5502\u001b[0m        \u001b[32m3.5410\u001b[0m  0.1751\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.7182\u001b[0m        \u001b[32m3.6664\u001b[0m  0.6180\n",
      "      3        \u001b[36m3.3220\u001b[0m        3.3333  0.1622\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.9817\u001b[0m        \u001b[32m3.9359\u001b[0m  0.5917\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m4.2928\u001b[0m        \u001b[32m4.2165\u001b[0m  0.6008\n",
      "      3        \u001b[36m4.0319\u001b[0m        4.0160  0.2011\n",
      "      3        \u001b[36m4.1883\u001b[0m        \u001b[32m4.1514\u001b[0m  0.1958\n",
      "      4        \u001b[36m3.8998\u001b[0m        3.9388  0.1886\n",
      "      4        \u001b[36m3.5395\u001b[0m        \u001b[32m3.5388\u001b[0m  0.1610\n",
      "      4        \u001b[36m3.3217\u001b[0m        3.3319  0.1710\n",
      "      5        \u001b[36m3.8975\u001b[0m        3.9377  0.1497\n",
      "      4        \u001b[36m4.0252\u001b[0m        \u001b[32m4.0127\u001b[0m  0.2104\n",
      "      4        \u001b[36m4.1841\u001b[0m        \u001b[32m4.1472\u001b[0m  0.2062\n",
      "      5        \u001b[36m3.5392\u001b[0m        \u001b[32m3.5338\u001b[0m  0.1601\n",
      "      5        \u001b[36m3.3192\u001b[0m        3.3302  0.1630\n",
      "      6        3.8981        \u001b[32m3.9353\u001b[0m  0.1552\n",
      "      6        3.5402        3.5353  0.1539\n",
      "      5        \u001b[36m4.0204\u001b[0m        \u001b[32m4.0101\u001b[0m  0.1937\n",
      "      5        \u001b[36m4.1798\u001b[0m        \u001b[32m4.1464\u001b[0m  0.1932\n",
      "      6        \u001b[36m3.3186\u001b[0m        3.3304  0.1528\n",
      "      2        \u001b[36m4.0802\u001b[0m        \u001b[32m4.0371\u001b[0m  0.5897\n",
      "      2        \u001b[36m3.6725\u001b[0m        \u001b[32m3.5912\u001b[0m  0.5501\n",
      "      2        \u001b[36m3.4160\u001b[0m        \u001b[32m3.3624\u001b[0m  0.6103\n",
      "      7        3.8979        \u001b[32m3.9345\u001b[0m  0.1616\n",
      "      2        \u001b[36m3.9126\u001b[0m        \u001b[32m3.9337\u001b[0m  0.6005\n",
      "      7        \u001b[36m3.5373\u001b[0m        3.5358  0.1763\n",
      "      2        \u001b[36m4.2348\u001b[0m        \u001b[32m4.1739\u001b[0m  0.6304\n",
      "      7        \u001b[36m3.3143\u001b[0m        3.3320  0.1743\n",
      "      6        4.0219        4.0108  0.1925\n",
      "      6        \u001b[36m4.1795\u001b[0m        4.1473  0.2065\n",
      "      8        3.8987        3.9366  0.1492\n",
      "      8        \u001b[36m3.5369\u001b[0m        3.5350  0.1464\n",
      "      8        3.3153        3.3316  0.1570\n",
      "      7        4.0206        4.0114  0.1742\n",
      "      7        \u001b[36m4.1771\u001b[0m        4.1485  0.1707\n",
      "      9        \u001b[36m3.8973\u001b[0m        3.9373  0.1378\n",
      "      9        \u001b[36m3.5327\u001b[0m        3.5338  0.1552\n",
      "      9        \u001b[36m3.3129\u001b[0m        3.3311  0.1524\n",
      "      8        \u001b[36m4.0160\u001b[0m        4.0104  0.1630\n",
      "     10        \u001b[36m3.8924\u001b[0m        3.9360  0.1507\n",
      "Restoring best model from epoch 7.\n",
      "      8        \u001b[36m4.1766\u001b[0m        4.1478  0.1850\n",
      "     10        \u001b[36m3.5307\u001b[0m        \u001b[32m3.5328\u001b[0m  0.1395\n",
      "      3        \u001b[36m4.0497\u001b[0m        \u001b[32m4.0276\u001b[0m  0.5588\n",
      "      3        \u001b[36m3.6191\u001b[0m        3.6170  0.5489\n",
      "      3        \u001b[36m3.3653\u001b[0m        3.3879  0.5154\n",
      "     10        3.3146        3.3309  0.1484\n",
      "Restoring best model from epoch 1.\n",
      "      3        \u001b[36m3.9023\u001b[0m        \u001b[32m3.9258\u001b[0m  0.5597\n",
      "      9        4.0164        \u001b[32m4.0093\u001b[0m  0.1821\n",
      "      9        4.1768        4.1479  0.1758\n",
      "      3        \u001b[36m4.2167\u001b[0m        \u001b[32m4.1545\u001b[0m  0.5927\n",
      "     10        4.0185        \u001b[32m4.0079\u001b[0m  0.1540\n",
      "     10        4.1777        4.1487  0.1545\n",
      "Restoring best model from epoch 5.\n",
      "      4        \u001b[36m3.5804\u001b[0m        \u001b[32m3.5456\u001b[0m  0.4669\n",
      "      4        \u001b[36m3.3536\u001b[0m        \u001b[32m3.3557\u001b[0m  0.4726\n",
      "      4        \u001b[36m4.0407\u001b[0m        \u001b[32m4.0231\u001b[0m  0.4849\n",
      "      4        \u001b[36m3.8901\u001b[0m        3.9374  0.4698\n",
      "      4        \u001b[36m4.1962\u001b[0m        \u001b[32m4.1504\u001b[0m  0.4626\n",
      "      5        \u001b[36m3.5501\u001b[0m        3.5633  0.4540\n",
      "      5        \u001b[36m4.0294\u001b[0m        \u001b[32m4.0204\u001b[0m  0.4452\n",
      "      5        \u001b[36m3.3400\u001b[0m        \u001b[32m3.3545\u001b[0m  0.4553\n",
      "      5        \u001b[36m3.8782\u001b[0m        3.9334  0.4537\n",
      "      5        \u001b[36m4.1909\u001b[0m        4.1539  0.4668\n",
      "      6        \u001b[36m3.5430\u001b[0m        3.5472  0.4691\n",
      "      6        \u001b[36m3.3246\u001b[0m        \u001b[32m3.3489\u001b[0m  0.4684\n",
      "      6        \u001b[36m4.0240\u001b[0m        \u001b[32m4.0180\u001b[0m  0.4733\n",
      "      6        \u001b[36m3.8733\u001b[0m        3.9314  0.4736\n",
      "      6        \u001b[36m4.1891\u001b[0m        \u001b[32m4.1439\u001b[0m  0.4850\n",
      "      7        \u001b[36m3.5320\u001b[0m        3.5481  0.4580\n",
      "      7        \u001b[36m3.3210\u001b[0m        \u001b[32m3.3474\u001b[0m  0.4566\n",
      "      7        \u001b[36m4.0194\u001b[0m        \u001b[32m4.0095\u001b[0m  0.4721\n",
      "      7        \u001b[36m3.8696\u001b[0m        3.9269  0.4547\n",
      "      7        \u001b[36m4.1819\u001b[0m        4.1464  0.4632\n",
      "      8        \u001b[36m3.5282\u001b[0m        3.5493  0.4693\n",
      "      8        \u001b[36m3.3143\u001b[0m        \u001b[32m3.3432\u001b[0m  0.4574\n",
      "      8        \u001b[36m4.0152\u001b[0m        4.0157  0.4637\n",
      "      8        3.8698        \u001b[32m3.9226\u001b[0m  0.4573\n",
      "      8        \u001b[36m4.1804\u001b[0m        4.1465  0.4562\n",
      "      9        3.5296        \u001b[32m3.5453\u001b[0m  0.4529\n",
      "      9        \u001b[36m3.3136\u001b[0m        \u001b[32m3.3404\u001b[0m  0.4610\n",
      "      9        \u001b[36m4.0138\u001b[0m        4.0130  0.4626\n",
      "      9        \u001b[36m3.8646\u001b[0m        3.9281  0.4526\n",
      "      9        4.1805        4.1442  0.4663\n",
      "     10        \u001b[36m3.5239\u001b[0m        \u001b[32m3.5437\u001b[0m  0.4592\n",
      "     10        \u001b[36m3.3096\u001b[0m        \u001b[32m3.3391\u001b[0m  0.4622\n",
      "     10        4.0155        \u001b[32m4.0090\u001b[0m  0.4682\n",
      "     10        3.8686        \u001b[32m3.9197\u001b[0m  0.4621\n",
      "     10        \u001b[36m4.1791\u001b[0m        4.1466  0.4654\n",
      "Restoring best model from epoch 6.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.9334\u001b[0m        \u001b[32m3.9190\u001b[0m  0.1316\n",
      "      2        \u001b[36m3.9070\u001b[0m        \u001b[32m3.9160\u001b[0m  0.1247\n",
      "      3        \u001b[36m3.9023\u001b[0m        3.9166  0.1271\n",
      "      4        3.9041        3.9163  0.1324\n",
      "      5        \u001b[36m3.8967\u001b[0m        \u001b[32m3.9135\u001b[0m  0.1118\n",
      "      6        3.8984        \u001b[32m3.9133\u001b[0m  0.1037\n",
      "      7        3.8974        \u001b[32m3.9117\u001b[0m  0.1113\n",
      "      8        3.8973        \u001b[32m3.9107\u001b[0m  0.1094\n",
      "      9        3.8994        3.9115  0.1118\n",
      "     10        \u001b[36m3.8966\u001b[0m        3.9108  0.1181\n",
      "Restoring best model from epoch 8.\n",
      "y_train breslow final [    3.     3.     3. ... -2029. -2029. -2029.]\n",
      "Concordance Index 0.6150206311914623\n",
      "Integrated Brier Score: 0.20140532108785922\n",
      "y_train breslow final [    3.     3.     3. ... -2029. -2029. -2029.]\n",
      "durations 3.0 2029.0\n",
      "Concordance Index 0.5857909716521019\n",
      "Integrated Brier Score: 0.20421445780943479\n",
      "(7099, 16) <class 'pandas.core.frame.DataFrame'>\n",
      "(7099,) <class 'pandas.core.series.Series'>\n",
      "(1774, 16) <class 'pandas.core.frame.DataFrame'>\n",
      "(1774,) <class 'pandas.core.series.Series'>\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.9706\u001b[0m        \u001b[32m3.9034\u001b[0m  0.1666\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.4398\u001b[0m        \u001b[32m3.3043\u001b[0m  0.1619\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.6574\u001b[0m        \u001b[32m3.4958\u001b[0m  0.1673\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m4.2320\u001b[0m        \u001b[32m4.2109\u001b[0m  0.2212\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m4.1278\u001b[0m        \u001b[32m3.9704\u001b[0m  0.2472\n",
      "      2        \u001b[36m3.9438\u001b[0m        \u001b[32m3.9022\u001b[0m  0.1675\n",
      "      2        \u001b[36m3.5842\u001b[0m        3.4999  0.1765\n",
      "      2        \u001b[36m3.3733\u001b[0m        3.3079  0.1842\n",
      "      2        \u001b[36m4.1898\u001b[0m        \u001b[32m4.2053\u001b[0m  0.1931\n",
      "      3        \u001b[36m3.9314\u001b[0m        3.9033  0.1678\n",
      "      2        \u001b[36m4.0809\u001b[0m        \u001b[32m3.9669\u001b[0m  0.2182\n",
      "      3        \u001b[36m3.3624\u001b[0m        3.3047  0.1880\n",
      "      3        \u001b[36m3.5778\u001b[0m        3.4977  0.2201\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.7222\u001b[0m        \u001b[32m3.6733\u001b[0m  0.6238\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m4.1556\u001b[0m        \u001b[32m4.0110\u001b[0m  0.6177\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.4969\u001b[0m        \u001b[32m3.3768\u001b[0m  0.6303\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.9632\u001b[0m        \u001b[32m3.9446\u001b[0m  0.6592\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m4.2729\u001b[0m        \u001b[32m4.2636\u001b[0m  0.6596\n",
      "      3        \u001b[36m4.1852\u001b[0m        \u001b[32m4.2048\u001b[0m  0.2481\n",
      "      4        \u001b[36m3.9294\u001b[0m        3.9046  0.2060\n",
      "      4        \u001b[36m3.5723\u001b[0m        3.4973  0.1854\n",
      "      3        \u001b[36m4.0755\u001b[0m        \u001b[32m3.9646\u001b[0m  0.2677\n",
      "      4        3.3672        \u001b[32m3.3022\u001b[0m  0.2274\n",
      "      5        \u001b[36m3.9279\u001b[0m        3.9068  0.1988\n",
      "      5        \u001b[36m3.5707\u001b[0m        \u001b[32m3.4944\u001b[0m  0.1858\n",
      "      4        \u001b[36m4.0732\u001b[0m        \u001b[32m3.9629\u001b[0m  0.2040\n",
      "      4        \u001b[36m4.1846\u001b[0m        4.2069  0.2842\n",
      "      5        \u001b[36m3.3608\u001b[0m        \u001b[32m3.3012\u001b[0m  0.2220\n",
      "      6        \u001b[36m3.9234\u001b[0m        3.9066  0.1913\n",
      "      6        \u001b[36m3.5679\u001b[0m        \u001b[32m3.4938\u001b[0m  0.2003\n",
      "      5        \u001b[36m4.0708\u001b[0m        \u001b[32m3.9628\u001b[0m  0.1754\n",
      "      6        \u001b[36m3.3599\u001b[0m        3.3028  0.1683\n",
      "      5        \u001b[36m4.1838\u001b[0m        4.2064  0.2004\n",
      "      7        3.9249        3.9047  0.1808\n",
      "      2        \u001b[36m3.4339\u001b[0m        \u001b[32m3.3603\u001b[0m  0.6733\n",
      "      2        \u001b[36m3.6889\u001b[0m        \u001b[32m3.5596\u001b[0m  0.6857\n",
      "      2        \u001b[36m4.1005\u001b[0m        \u001b[32m4.0011\u001b[0m  0.7006\n",
      "      7        \u001b[36m3.5652\u001b[0m        \u001b[32m3.4922\u001b[0m  0.1700\n",
      "      7        3.3601        3.3043  0.1721\n",
      "      2        \u001b[36m3.9239\u001b[0m        \u001b[32m3.9373\u001b[0m  0.6941\n",
      "      6        \u001b[36m4.0684\u001b[0m        3.9629  0.1942\n",
      "      2        \u001b[36m4.2217\u001b[0m        \u001b[32m4.2426\u001b[0m  0.6929\n",
      "      6        \u001b[36m4.1801\u001b[0m        4.2052  0.2297\n",
      "      8        \u001b[36m3.9221\u001b[0m        3.9064  0.1725\n",
      "      8        3.5659        3.4922  0.1864\n",
      "      8        \u001b[36m3.3581\u001b[0m        3.3029  0.1845\n",
      "      7        4.0724        3.9640  0.1964\n",
      "      9        \u001b[36m3.9219\u001b[0m        3.9068  0.1575\n",
      "      7        \u001b[36m4.1770\u001b[0m        4.2059  0.1963\n",
      "      9        \u001b[36m3.5649\u001b[0m        \u001b[32m3.4913\u001b[0m  0.1661\n",
      "      9        3.3585        \u001b[32m3.3003\u001b[0m  0.1551\n",
      "      8        4.0696        3.9632  0.1781\n",
      "     10        \u001b[36m3.9184\u001b[0m        3.9071  0.1496\n",
      "Restoring best model from epoch 2.\n",
      "     10        \u001b[36m3.5648\u001b[0m        3.4943  0.1440\n",
      "Restoring best model from epoch 9.\n",
      "     10        3.3598        \u001b[32m3.2997\u001b[0m  0.1417\n",
      "      8        \u001b[36m4.1762\u001b[0m        4.2049  0.2117\n",
      "      9        \u001b[36m4.0669\u001b[0m        \u001b[32m3.9627\u001b[0m  0.1815\n",
      "      3        \u001b[36m3.3965\u001b[0m        \u001b[32m3.3225\u001b[0m  0.6176\n",
      "      3        \u001b[36m3.6217\u001b[0m        \u001b[32m3.5368\u001b[0m  0.6197\n",
      "      3        \u001b[36m4.0803\u001b[0m        \u001b[32m3.9954\u001b[0m  0.6266\n",
      "      3        \u001b[36m3.8992\u001b[0m        \u001b[32m3.9280\u001b[0m  0.6271\n",
      "      3        \u001b[36m4.2107\u001b[0m        \u001b[32m4.2322\u001b[0m  0.6495\n",
      "      9        \u001b[36m4.1747\u001b[0m        \u001b[32m4.2040\u001b[0m  0.1738\n",
      "     10        \u001b[36m4.0669\u001b[0m        \u001b[32m3.9620\u001b[0m  0.1605\n",
      "     10        4.1768        4.2041  0.1616\n",
      "Restoring best model from epoch 9.\n",
      "      4        \u001b[36m3.3752\u001b[0m        3.3293  0.4720\n",
      "      4        \u001b[36m3.5838\u001b[0m        \u001b[32m3.5214\u001b[0m  0.4708\n",
      "      4        \u001b[36m4.0704\u001b[0m        \u001b[32m3.9739\u001b[0m  0.4692\n",
      "      4        \u001b[36m3.8852\u001b[0m        \u001b[32m3.9120\u001b[0m  0.4642\n",
      "      4        \u001b[36m4.1915\u001b[0m        \u001b[32m4.2239\u001b[0m  0.4786\n",
      "      5        \u001b[36m3.3624\u001b[0m        \u001b[32m3.3201\u001b[0m  0.4784\n",
      "      5        \u001b[36m3.5703\u001b[0m        \u001b[32m3.5163\u001b[0m  0.4791\n",
      "      5        \u001b[36m4.0627\u001b[0m        \u001b[32m3.9706\u001b[0m  0.4845\n",
      "      5        \u001b[36m3.8777\u001b[0m        \u001b[32m3.9120\u001b[0m  0.4779\n",
      "      5        \u001b[36m4.1880\u001b[0m        \u001b[32m4.2178\u001b[0m  0.4888\n",
      "      6        \u001b[36m3.3509\u001b[0m        \u001b[32m3.3182\u001b[0m  0.4739\n",
      "      6        \u001b[36m3.5562\u001b[0m        \u001b[32m3.5057\u001b[0m  0.4751\n",
      "      6        \u001b[36m4.0558\u001b[0m        \u001b[32m3.9591\u001b[0m  0.4770\n",
      "      6        3.8798        \u001b[32m3.9087\u001b[0m  0.4718\n",
      "      6        \u001b[36m4.1781\u001b[0m        \u001b[32m4.2099\u001b[0m  0.4792\n",
      "      7        \u001b[36m3.3434\u001b[0m        \u001b[32m3.3136\u001b[0m  0.4742\n",
      "      7        \u001b[36m3.5417\u001b[0m        \u001b[32m3.5023\u001b[0m  0.4754\n",
      "      7        \u001b[36m4.0491\u001b[0m        3.9643  0.4822\n",
      "      7        \u001b[36m3.8717\u001b[0m        \u001b[32m3.9064\u001b[0m  0.4703\n",
      "      7        \u001b[36m4.1740\u001b[0m        \u001b[32m4.2076\u001b[0m  0.4872\n",
      "      8        \u001b[36m3.3396\u001b[0m        \u001b[32m3.3120\u001b[0m  0.4750\n",
      "      8        \u001b[36m3.5403\u001b[0m        \u001b[32m3.5017\u001b[0m  0.4726\n",
      "      8        \u001b[36m3.8710\u001b[0m        \u001b[32m3.9041\u001b[0m  0.4690\n",
      "      8        4.0495        3.9635  0.4726\n",
      "      8        \u001b[36m4.1712\u001b[0m        \u001b[32m4.2061\u001b[0m  0.4801\n",
      "      9        \u001b[36m3.5367\u001b[0m        3.5022  0.4767\n",
      "      9        3.3420        3.3143  0.4799\n",
      "      9        \u001b[36m3.8680\u001b[0m        3.9049  0.4741\n",
      "      9        \u001b[36m4.0451\u001b[0m        3.9624  0.4814\n",
      "      9        \u001b[36m4.1708\u001b[0m        4.2062  0.4797\n",
      "     10        \u001b[36m3.5358\u001b[0m        3.5019  0.4722\n",
      "Restoring best model from epoch 8.\n",
      "     10        \u001b[36m3.3384\u001b[0m        \u001b[32m3.3117\u001b[0m  0.4738\n",
      "Restoring best model from epoch 8.\n",
      "     10        3.8704        3.9045  0.4710\n",
      "Restoring best model from epoch 8.\n",
      "     10        4.0452        3.9599  0.4814\n",
      "Restoring best model from epoch 6.\n",
      "     10        4.1726        \u001b[32m4.2052\u001b[0m  0.4756\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.9571\u001b[0m        \u001b[32m3.9028\u001b[0m  0.1085\n",
      "      2        \u001b[36m3.9303\u001b[0m        \u001b[32m3.9013\u001b[0m  0.1033\n",
      "      3        \u001b[36m3.9298\u001b[0m        3.9017  0.1129\n",
      "      4        \u001b[36m3.9233\u001b[0m        3.9025  0.1049\n",
      "      5        \u001b[36m3.9214\u001b[0m        3.9040  0.1029\n",
      "      6        \u001b[36m3.9206\u001b[0m        3.9022  0.1183\n",
      "      7        \u001b[36m3.9204\u001b[0m        \u001b[32m3.9000\u001b[0m  0.1087\n",
      "      8        \u001b[36m3.9177\u001b[0m        3.9031  0.1141\n",
      "      9        3.9182        3.9019  0.1054\n",
      "     10        \u001b[36m3.9161\u001b[0m        3.9025  0.1037\n",
      "Restoring best model from epoch 7.\n",
      "y_train breslow final [    3.     3.     3. ... -2029. -2029. -2029.]\n",
      "Concordance Index 0.6119670955607532\n",
      "Integrated Brier Score: 0.2016873073679917\n",
      "y_train breslow final [    3.     3.     3. ... -2029. -2029. -2029.]\n",
      "durations 3.0 2029.0\n",
      "Concordance Index 0.6246677077453472\n",
      "Integrated Brier Score: 0.19979163339824424\n",
      "(7099, 16) <class 'pandas.core.frame.DataFrame'>\n",
      "(7099,) <class 'pandas.core.series.Series'>\n",
      "(1774, 16) <class 'pandas.core.frame.DataFrame'>\n",
      "(1774,) <class 'pandas.core.series.Series'>\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.9144\u001b[0m        \u001b[32m3.8835\u001b[0m  0.1641\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.6022\u001b[0m        \u001b[32m3.4840\u001b[0m  0.1824\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.3768\u001b[0m        \u001b[32m3.2624\u001b[0m  0.1975\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m4.0857\u001b[0m        \u001b[32m3.9936\u001b[0m  0.2187\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m4.2429\u001b[0m        \u001b[32m4.2118\u001b[0m  0.2325\n",
      "      2        \u001b[36m3.8856\u001b[0m        \u001b[32m3.8829\u001b[0m  0.1603\n",
      "      2        \u001b[36m3.5328\u001b[0m        \u001b[32m3.4795\u001b[0m  0.1964\n",
      "      2        \u001b[36m3.3057\u001b[0m        \u001b[32m3.2588\u001b[0m  0.1885\n",
      "      2        \u001b[36m4.0485\u001b[0m        3.9957  0.1885\n",
      "      3        \u001b[36m3.8776\u001b[0m        \u001b[32m3.8819\u001b[0m  0.1572\n",
      "      2        \u001b[36m4.2019\u001b[0m        4.2122  0.2035\n",
      "      3        \u001b[36m3.5237\u001b[0m        \u001b[32m3.4779\u001b[0m  0.1802\n",
      "      3        \u001b[36m3.2967\u001b[0m        3.2625  0.1629\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m4.1498\u001b[0m        \u001b[32m3.9870\u001b[0m  0.5942\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.7233\u001b[0m        \u001b[32m3.5920\u001b[0m  0.6229\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.4800\u001b[0m        \u001b[32m3.3484\u001b[0m  0.6308\n",
      "      4        \u001b[36m3.8748\u001b[0m        \u001b[32m3.8805\u001b[0m  0.1535\n",
      "      3        \u001b[36m4.0389\u001b[0m        3.9986  0.2075\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.9638\u001b[0m        \u001b[32m3.9162\u001b[0m  0.6545\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m4.2641\u001b[0m        \u001b[32m4.2308\u001b[0m  0.6563\n",
      "      3        \u001b[36m4.1993\u001b[0m        4.2160  0.2150\n",
      "      4        \u001b[36m3.5199\u001b[0m        \u001b[32m3.4773\u001b[0m  0.1655\n",
      "      4        \u001b[36m3.2965\u001b[0m        3.2616  0.1892\n",
      "      5        \u001b[36m3.8736\u001b[0m        \u001b[32m3.8799\u001b[0m  0.1751\n",
      "      4        \u001b[36m4.0375\u001b[0m        3.9985  0.1705\n",
      "      5        \u001b[36m3.5148\u001b[0m        \u001b[32m3.4762\u001b[0m  0.1675\n",
      "      4        \u001b[36m4.1962\u001b[0m        4.2127  0.2105\n",
      "      5        \u001b[36m3.2940\u001b[0m        3.2607  0.1771\n",
      "      6        \u001b[36m3.8692\u001b[0m        3.8806  0.1731\n",
      "      5        \u001b[36m4.0323\u001b[0m        3.9966  0.1912\n",
      "      6        3.5157        \u001b[32m3.4755\u001b[0m  0.1686\n",
      "      5        \u001b[36m4.1933\u001b[0m        \u001b[32m4.2093\u001b[0m  0.1772\n",
      "      6        3.2966        3.2606  0.1821\n",
      "      7        \u001b[36m3.8683\u001b[0m        3.8836  0.1519\n",
      "      2        \u001b[36m4.0752\u001b[0m        4.0132  0.5991\n",
      "      6        \u001b[36m4.0323\u001b[0m        3.9955  0.1878\n",
      "      2        \u001b[36m3.6556\u001b[0m        \u001b[32m3.5331\u001b[0m  0.6081\n",
      "      2        \u001b[36m3.4023\u001b[0m        \u001b[32m3.3122\u001b[0m  0.6192\n",
      "      7        \u001b[36m3.5107\u001b[0m        \u001b[32m3.4722\u001b[0m  0.1694\n",
      "      6        4.1952        \u001b[32m4.2090\u001b[0m  0.1899\n",
      "      2        \u001b[36m3.9162\u001b[0m        \u001b[32m3.8951\u001b[0m  0.6367\n",
      "      8        3.8683        3.8846  0.1490\n",
      "      7        \u001b[36m3.2921\u001b[0m        3.2590  0.1688\n",
      "      2        \u001b[36m4.2265\u001b[0m        \u001b[32m4.2236\u001b[0m  0.6377\n",
      "      7        \u001b[36m4.0305\u001b[0m        3.9947  0.1787\n",
      "      9        \u001b[36m3.8656\u001b[0m        3.8829  0.1409\n",
      "      8        3.5134        3.4733  0.2019\n",
      "      7        \u001b[36m4.1869\u001b[0m        \u001b[32m4.2082\u001b[0m  0.1697\n",
      "      8        \u001b[36m3.2919\u001b[0m        3.2608  0.1731\n",
      "      8        \u001b[36m4.0291\u001b[0m        3.9944  0.1661\n",
      "     10        \u001b[36m3.8647\u001b[0m        3.8821  0.1353\n",
      "Restoring best model from epoch 5.\n",
      "      9        3.5108        3.4748  0.1728\n",
      "      8        4.1920        4.2091  0.1781\n",
      "      9        \u001b[36m3.2914\u001b[0m        3.2605  0.1965\n",
      "      3        \u001b[36m3.5957\u001b[0m        \u001b[32m3.4974\u001b[0m  0.4952\n",
      "      9        4.0293        3.9943  0.1674\n",
      "      3        \u001b[36m3.9018\u001b[0m        \u001b[32m3.8889\u001b[0m  0.5115\n",
      "     10        3.5129        3.4763  0.1739\n",
      "Restoring best model from epoch 7.\n",
      "      9        4.1909        4.2110  0.1768\n",
      "     10        \u001b[36m3.2854\u001b[0m        3.2604  0.1791\n",
      "Restoring best model from epoch 2.\n",
      "      3        \u001b[36m4.0624\u001b[0m        3.9946  0.6756\n",
      "     10        \u001b[36m4.0270\u001b[0m        3.9943  0.1609\n",
      "Restoring best model from epoch 1.\n",
      "      3        \u001b[36m3.3808\u001b[0m        \u001b[32m3.3033\u001b[0m  0.6745\n",
      "      3        \u001b[36m4.2047\u001b[0m        \u001b[32m4.2151\u001b[0m  0.6512\n",
      "     10        \u001b[36m4.1869\u001b[0m        4.2104  0.1572\n",
      "Restoring best model from epoch 7.\n",
      "      4        \u001b[36m3.5900\u001b[0m        3.5039  0.4661\n",
      "      4        \u001b[36m3.8900\u001b[0m        \u001b[32m3.8853\u001b[0m  0.4473\n",
      "      4        \u001b[36m4.0478\u001b[0m        3.9891  0.4655\n",
      "      4        \u001b[36m3.3571\u001b[0m        \u001b[32m3.2812\u001b[0m  0.4621\n",
      "      4        \u001b[36m4.1923\u001b[0m        \u001b[32m4.2054\u001b[0m  0.4651\n",
      "      5        \u001b[36m3.5691\u001b[0m        \u001b[32m3.4933\u001b[0m  0.4551\n",
      "      5        \u001b[36m3.8801\u001b[0m        \u001b[32m3.8844\u001b[0m  0.4511\n",
      "      5        \u001b[36m4.0437\u001b[0m        3.9974  0.4820\n",
      "      5        \u001b[36m3.3406\u001b[0m        \u001b[32m3.2751\u001b[0m  0.4867\n",
      "      5        \u001b[36m4.1835\u001b[0m        4.2100  0.4928\n",
      "      6        \u001b[36m3.5498\u001b[0m        3.4954  0.4831\n",
      "      6        \u001b[36m3.8773\u001b[0m        3.8909  0.4737\n",
      "      6        \u001b[36m4.0323\u001b[0m        3.9886  0.4851\n",
      "      6        \u001b[36m3.3292\u001b[0m        \u001b[32m3.2748\u001b[0m  0.4784\n",
      "      6        \u001b[36m4.1791\u001b[0m        4.2133  0.4860\n",
      "      7        \u001b[36m3.5398\u001b[0m        \u001b[32m3.4834\u001b[0m  0.4662\n",
      "      7        \u001b[36m3.8723\u001b[0m        3.8858  0.4721\n",
      "      7        \u001b[36m4.0305\u001b[0m        \u001b[32m3.9851\u001b[0m  0.4814\n",
      "      7        \u001b[36m3.3224\u001b[0m        \u001b[32m3.2721\u001b[0m  0.4751\n",
      "      7        \u001b[36m4.1732\u001b[0m        4.2086  0.4842\n",
      "      8        \u001b[36m3.5348\u001b[0m        \u001b[32m3.4811\u001b[0m  0.4627\n",
      "      8        3.8726        3.8845  0.4683\n",
      "      8        \u001b[36m4.0246\u001b[0m        \u001b[32m3.9839\u001b[0m  0.4776\n",
      "      8        \u001b[36m3.3156\u001b[0m        \u001b[32m3.2710\u001b[0m  0.4764\n",
      "      8        4.1738        4.2067  0.4876\n",
      "      9        3.5354        3.4842  0.4746\n",
      "      9        \u001b[36m3.8687\u001b[0m        3.8915  0.4723\n",
      "      9        \u001b[36m4.0245\u001b[0m        3.9846  0.4844\n",
      "      9        3.3179        \u001b[32m3.2689\u001b[0m  0.4756\n",
      "      9        \u001b[36m4.1710\u001b[0m        4.2081  0.4775\n",
      "     10        \u001b[36m3.5316\u001b[0m        3.4874  0.4536\n",
      "Restoring best model from epoch 8.\n",
      "     10        3.8693        3.8894  0.4413\n",
      "Restoring best model from epoch 5.\n",
      "     10        4.0252        3.9867  0.4630\n",
      "Restoring best model from epoch 8.\n",
      "     10        3.3157        3.2715  0.4591\n",
      "Restoring best model from epoch 9.\n",
      "     10        \u001b[36m4.1663\u001b[0m        4.2164  0.4691\n",
      "Restoring best model from epoch 4.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.9589\u001b[0m        \u001b[32m3.8620\u001b[0m  0.1509\n",
      "      2        \u001b[36m3.9267\u001b[0m        3.8623  0.1299\n",
      "      3        \u001b[36m3.9230\u001b[0m        \u001b[32m3.8593\u001b[0m  0.1256\n",
      "      4        \u001b[36m3.9216\u001b[0m        3.8597  0.1226\n",
      "      5        \u001b[36m3.9188\u001b[0m        3.8594  0.1207\n",
      "      6        \u001b[36m3.9169\u001b[0m        \u001b[32m3.8565\u001b[0m  0.1187\n",
      "      7        3.9170        3.8576  0.1233\n",
      "      8        \u001b[36m3.9143\u001b[0m        3.8583  0.1088\n",
      "      9        \u001b[36m3.9139\u001b[0m        3.8593  0.1631\n",
      "     10        \u001b[36m3.9133\u001b[0m        3.8578  0.1222\n",
      "Restoring best model from epoch 6.\n",
      "y_train breslow final [    3.     3.     3. ... -2029. -2029. -2029.]\n",
      "Concordance Index 0.614004636700499\n",
      "Integrated Brier Score: 0.20175840718599303\n",
      "y_train breslow final [    3.     3.     3. ... -2029. -2029. -2029.]\n",
      "durations 3.0 2029.0\n",
      "Concordance Index 0.6172598470197125\n",
      "Integrated Brier Score: 0.2005209615559061\n"
     ]
    }
   ],
   "source": [
    "data_set_fns = [load_metabric,  load_flchain, load_rgbsg, load_support] #, load_flchain, load_rgbsg, load_support, load_tcga]\n",
    "data_set_fns_str = ['load_metabric', 'load_flchain', 'load_rgbsg', 'load_support'] \n",
    "one_hot_dict = {'load_flchain': ['mgus'], 'load_support':['cancer'], 'load_rgbsg':['grade']}\n",
    "\n",
    "for idx, dataset in enumerate(data_set_fns):\n",
    "    # get name of current dataset\n",
    "    data = dataset(path=\"/Users/JUSC/Documents/xgbsurv/xgbsurv/datasets/data/\", as_frame=True)\n",
    "    X  = data.data #.astype(np.float32)\n",
    "    y = data.target #.values #.to_numpy()\n",
    "\n",
    "    print(data_set_fns_str[idx])\n",
    "    if data_set_fns_str[idx] in one_hot_dict.keys():\n",
    "        X = pd.get_dummies(X, columns=one_hot_dict[data_set_fns_str[idx]])\n",
    "    X, y = sort_X_y_pandas(X, y)\n",
    "    \n",
    "    net = NeuralNet(\n",
    "        SurvivalModel, \n",
    "        module__n_layers = 1,\n",
    "        module__input_units = X.shape[1],\n",
    "        #module__num_nodes = 32,\n",
    "        #module__dropout = 0.1, # these could also be removed\n",
    "        module__out_features = 1,\n",
    "        # for split sizes when result size = 1\n",
    "        iterator_train__drop_last=True,\n",
    "        #iterator_valid__drop_last=True,\n",
    "        criterion=AFTLoss,\n",
    "        optimizer=torch.optim.AdamW,\n",
    "        optimizer__weight_decay = 0.4,\n",
    "        batch_size=32, # separate train and valid->iterator_train__batch_size=128 and iterator_valid__batch_size=128 ?\n",
    "        callbacks=[\n",
    "            (\n",
    "                \"sched\",\n",
    "                LRScheduler(\n",
    "                    torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
    "                    monitor=\"valid_loss\",\n",
    "                    patience=5,\n",
    "                ),\n",
    "            ),\n",
    "            (\n",
    "                \"es\",\n",
    "                EarlyStopping(\n",
    "                    monitor=\"valid_loss\",\n",
    "                    patience=10,\n",
    "                    load_best=True,\n",
    "                ),\n",
    "            ),\n",
    "            (\"seed\", FixSeed(seed=42)),\n",
    "        ],\n",
    "        \n",
    "        #[EarlyStopping(patience=10)],\n",
    "        # add extensive callback, and random number seed\n",
    "        #TODO: enable stratification, verify\n",
    "        train_split=ValidSplit(0.2), # might cause lower performance in metrics, explain in thesis\n",
    "        #lr=0.001,\n",
    "        #max_epochs=1, #0,#100\n",
    "        #train_split=None,\n",
    "        verbose=1\n",
    "    )\n",
    "    best_model,params, outer_scores, best_preds_train, best_preds_test, X_train, X_test, y_train, y_test = train_eval(X, y, net, n_iter, data.filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log, exp #  erf,\n",
    "from scipy.special import erf\n",
    "\n",
    "PDF_PREFACTOR: float = 0.3989424488876037\n",
    "SQRT_TWO: float = 1.4142135623730951\n",
    "SQRT_EPS: float = 1.4901161193847656e-08\n",
    "EPS: float = 2.220446049250313e-16\n",
    "CDF_ZERO: float = 0.5\n",
    "\n",
    "def gaussian_integrated_kernel(x):\n",
    "    return 0.5 * (1 + erf(x / SQRT_TWO))\n",
    "\n",
    "def gaussian_kernel(x):\n",
    "    return PDF_PREFACTOR * np.exp(-0.5 * (x**2))\n",
    "\n",
    "def aft_baseline_hazard_estimator(\n",
    "    time, # time to integrate over\n",
    "    time_train,\n",
    "    event_train,\n",
    "    predictor_train,\n",
    "):\n",
    "    n_samples: int = time_train.shape[0]\n",
    "    bandwidth = 1.30 * pow(n_samples, -0.2)\n",
    "    inverse_bandwidth: float = 1 / bandwidth\n",
    "    inverse_sample_size: float = 1 / n_samples\n",
    "    log_time: float = np.log(time + EPS)\n",
    "    inverse_bandwidth_sample_size_time: float = (\n",
    "        inverse_sample_size * (1 / (time + EPS)) * inverse_bandwidth\n",
    "    )\n",
    "\n",
    "    R_lp: np.array = np.log(time_train * np.exp(predictor_train))\n",
    "    difference_lp_log_time: np.array = (R_lp - log_time) / bandwidth\n",
    "    numerator: float = 0.0\n",
    "    denominator: float = 0.0\n",
    "    for _ in range(n_samples):\n",
    "        difference_div: float = difference_lp_log_time[_]\n",
    "        denominator += gaussian_integrated_kernel(difference_div)\n",
    "        if event_train[_]:\n",
    "            numerator += gaussian_kernel(difference_div)\n",
    "    numerator = inverse_bandwidth_sample_size_time * numerator\n",
    "    denominator = inverse_sample_size * denominator\n",
    "    return numerator / denominator\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (7099,7099) (10,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m best_model,params, outer_scores, best_preds_train, best_preds_test, X_train, X_test, y_train, y_test\n\u001b[1;32m      3\u001b[0m time_train, event_train \u001b[39m=\u001b[39m transform_back(y_train\u001b[39m.\u001b[39mto_numpy())\n\u001b[0;32m----> 5\u001b[0m baseline_hazard_aft \u001b[39m=\u001b[39m aft_baseline_hazard_estimator(\n\u001b[1;32m      6\u001b[0m     time_train[:\u001b[39m10\u001b[39;49m], \u001b[39m# time to integrate over\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m     time_train,\n\u001b[1;32m      8\u001b[0m     event_train,\n\u001b[1;32m      9\u001b[0m     best_preds_train,\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     11\u001b[0m baseline_hazard_aft\u001b[39m.\u001b[39mshape\n",
      "Cell \u001b[0;32mIn[56], line 32\u001b[0m, in \u001b[0;36maft_baseline_hazard_estimator\u001b[0;34m(time, time_train, event_train, predictor_train)\u001b[0m\n\u001b[1;32m     27\u001b[0m inverse_bandwidth_sample_size_time: \u001b[39mfloat\u001b[39m \u001b[39m=\u001b[39m (\n\u001b[1;32m     28\u001b[0m     inverse_sample_size \u001b[39m*\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m/\u001b[39m (time \u001b[39m+\u001b[39m EPS)) \u001b[39m*\u001b[39m inverse_bandwidth\n\u001b[1;32m     29\u001b[0m )\n\u001b[1;32m     31\u001b[0m R_lp: np\u001b[39m.\u001b[39marray \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mlog(time_train \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39mexp(predictor_train))\n\u001b[0;32m---> 32\u001b[0m difference_lp_log_time: np\u001b[39m.\u001b[39marray \u001b[39m=\u001b[39m (R_lp \u001b[39m-\u001b[39;49m log_time) \u001b[39m/\u001b[39m bandwidth\n\u001b[1;32m     33\u001b[0m numerator: \u001b[39mfloat\u001b[39m \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[1;32m     34\u001b[0m denominator: \u001b[39mfloat\u001b[39m \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (7099,7099) (10,) "
     ]
    }
   ],
   "source": [
    "# AFT baseline hazard\n",
    "best_model,params, outer_scores, best_preds_train, best_preds_test, X_train, X_test, y_train, y_test\n",
    "time_train, event_train = transform_back(y_train.to_numpy())\n",
    "\n",
    "baseline_hazard_aft = aft_baseline_hazard_estimator(\n",
    "    time_train, # time to integrate over\n",
    "    time_train,\n",
    "    event_train,\n",
    "    best_preds_train,\n",
    ")\n",
    "baseline_hazard_aft.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.integrate import quad, cumtrapz\n",
    "from functools import partial\n",
    "\n",
    "def predict_cumulative_hazard_function(X, time, predictor):\n",
    "\n",
    "    theta: np.array = np.exp(predictor)\n",
    "    n_samples: int = X.shape[0]\n",
    "    cumulative_hazard: np.array = np.empty((n_samples, time.shape[0] + 1))\n",
    "    zero_flag: bool = False\n",
    "    if 0 not in time:\n",
    "        zero_flag = True\n",
    "        time = np.concatenate([np.array([0]), time])\n",
    "\n",
    "    for _ in range(n_samples):\n",
    "        cumulative_hazard[_, :] = cumtrapz(\n",
    "            y=aft_baseline_hazard_estimator(time * theta[_]),\n",
    "            x=time,\n",
    "            initial=0,\n",
    "        )\n",
    "        \n",
    "    if zero_flag:\n",
    "        cumulative_hazard = cumulative_hazard[:, 1:]\n",
    "        time = time[1:]\n",
    "    return pd.DataFrame(cumulative_hazard, columns=time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.integrate import quad, cumtrapz\n",
    "from functools import partial\n",
    "\n",
    "def predict_cumulative_hazard_function_old(X_train, time, event_train, predictor):\n",
    "    # change train and test\n",
    "    #theta: np.array = np.exp(predictions)\n",
    "    theta = np.exp(predictor) # eventuell exp\n",
    "    n_samples: int = X_train.shape[0]\n",
    "    cumulative_hazard: np.array = np.empty((n_samples, time.shape[0] + 1))\n",
    "    zero_flag: bool = False\n",
    "    if 0 not in time:\n",
    "        zero_flag = True\n",
    "        time = np.concatenate([np.array([0]), time])\n",
    "    #hazard_function_integrate = partial(aft_baseline_hazard_estimator, time_train=time_train, event_train=event_train, predictor_train=predictor)\n",
    "    integrand = aft_baseline_hazard_estimator(time, time_train=time_train, event_train=event_train, predictor_train=predictor)\n",
    "    for _ in range(n_samples):\n",
    "        cumulative_hazard[_, :] = cumtrapz(\n",
    "            y = integrand,\n",
    "            #y=hazard_function_integrate(time * theta[_]),\n",
    "            x=time * theta[_],\n",
    "            initial=0,\n",
    "        )\n",
    "    if zero_flag:\n",
    "        cumulative_hazard = cumulative_hazard[:, 1:]\n",
    "        time = time[1:]\n",
    "    return pd.DataFrame(cumulative_hazard, columns=time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (7099,7099) (7100,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m predict_cumulative_hazard_function_old(X_train, time_train, event_train, best_preds_train)\n",
      "Cell \u001b[0;32mIn[54], line 17\u001b[0m, in \u001b[0;36mpredict_cumulative_hazard_function_old\u001b[0;34m(X_train, time, event_train, predictor)\u001b[0m\n\u001b[1;32m     15\u001b[0m     time \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate([np\u001b[39m.\u001b[39marray([\u001b[39m0\u001b[39m]), time])\n\u001b[1;32m     16\u001b[0m \u001b[39m#hazard_function_integrate = partial(aft_baseline_hazard_estimator, time_train=time_train, event_train=event_train, predictor_train=predictor)\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m integrand \u001b[39m=\u001b[39m aft_baseline_hazard_estimator(time, time_train\u001b[39m=\u001b[39;49mtime_train, event_train\u001b[39m=\u001b[39;49mevent_train, predictor_train\u001b[39m=\u001b[39;49mpredictor)\n\u001b[1;32m     18\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_samples):\n\u001b[1;32m     19\u001b[0m     cumulative_hazard[_, :] \u001b[39m=\u001b[39m cumtrapz(\n\u001b[1;32m     20\u001b[0m         y \u001b[39m=\u001b[39m integrand,\n\u001b[1;32m     21\u001b[0m         \u001b[39m#y=hazard_function_integrate(time * theta[_]),\u001b[39;00m\n\u001b[1;32m     22\u001b[0m         x\u001b[39m=\u001b[39mtime \u001b[39m*\u001b[39m theta[_],\n\u001b[1;32m     23\u001b[0m         initial\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m,\n\u001b[1;32m     24\u001b[0m     )\n",
      "Cell \u001b[0;32mIn[41], line 32\u001b[0m, in \u001b[0;36maft_baseline_hazard_estimator\u001b[0;34m(time, time_train, event_train, predictor_train)\u001b[0m\n\u001b[1;32m     27\u001b[0m inverse_bandwidth_sample_size_time: \u001b[39mfloat\u001b[39m \u001b[39m=\u001b[39m (\n\u001b[1;32m     28\u001b[0m     inverse_sample_size \u001b[39m*\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m/\u001b[39m (time \u001b[39m+\u001b[39m EPS)) \u001b[39m*\u001b[39m inverse_bandwidth\n\u001b[1;32m     29\u001b[0m )\n\u001b[1;32m     31\u001b[0m R_lp: np\u001b[39m.\u001b[39marray \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mlog(time_train \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39mexp(predictor_train))\n\u001b[0;32m---> 32\u001b[0m difference_lp_log_time: np\u001b[39m.\u001b[39marray \u001b[39m=\u001b[39m (R_lp \u001b[39m-\u001b[39;49m log_time) \u001b[39m/\u001b[39m bandwidth\n\u001b[1;32m     33\u001b[0m numerator: \u001b[39mfloat\u001b[39m \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[1;32m     34\u001b[0m denominator: \u001b[39mfloat\u001b[39m \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (7099,7099) (7100,) "
     ]
    }
   ],
   "source": [
    "predict_cumulative_hazard_function_old(X_train, time_train, event_train, best_preds_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### AFT\n",
    "def predict_cumulative_hazard_function(self, X, time, predictions):\n",
    "\n",
    "    theta: np.array = np.exp(predictions)\n",
    "    n_samples: int = X.shape[0]\n",
    "\n",
    "    zero_flag: bool = False\n",
    "    if 0 not in time:\n",
    "        zero_flag = True\n",
    "        time = np.concatenate([np.array([0]), time])\n",
    "        cumulative_hazard: np.array = np.empty((n_samples, time.shape[0]))\n",
    "    else:\n",
    "        cumulative_hazard: np.array = np.empty((n_samples, time.shape[0]))\n",
    "\n",
    "    def hazard_function_integrate(s):\n",
    "        return self.predict_baseline_hazard_function(s)\n",
    "\n",
    "    for _ in range(n_samples):\n",
    "        cumulative_hazard[_, 0] = 0.0\n",
    "        cumulative_hazard[_, 1] = quad(\n",
    "            hazard_function_integrate, 0, time[1] * theta[_]\n",
    "        )[0]\n",
    "        cumulative_hazard[_, 2:] = cumtrapz(\n",
    "            y=self.predict_baseline_hazard_function(time[1:] * theta[_]),\n",
    "            x=time,\n",
    "            initial=0,\n",
    "        )[1:]\n",
    "    if zero_flag:\n",
    "        cumulative_hazard = cumulative_hazard[:, 1:]\n",
    "        time = time[1:]\n",
    "    return pd.DataFrame(cumulative_hazard, columns=time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create an array with shape (4,)\n",
    "arr = np.array([1, 2, 3, 4])\n",
    "\n",
    "arr.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TCGA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_aft_tcga = {\n",
    "    'estimator__module__n_layers': [1, 2, 4],\n",
    "    'estimator__module__n_layers': [1, 2, 4],\n",
    "    'estimator__module__num_nodes': [64, 128, 256, 512],\n",
    "    'estimator__module__dropout': scuniform(0.0,0.7),\n",
    "    'estimator__optimizer__weight_decay': [0.4, 0.2, 0.1, 0.05, 0.02, 0.01, 0],\n",
    "    'estimator__batch_size': [64, 128, 256, 512, 1024],\n",
    "    #lr not in paper because of learning rate finder\n",
    "    # note: setting learning rate higher would make exp(partial_hazard) explode\n",
    "    #'estimator__lr': scloguniform(0.001,0.01), # scheduler unten einbauen\n",
    "    # use callback instead\n",
    "    'estimator__lr':[0.01],\n",
    "    #'estimator__max_epochs':  scrandint(10,20), # corresponds to num_rounds\n",
    "    'pca__n_components': [8, 10, 12, 14, 16]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_eval(X, y, net, n_iter, filename):\n",
    "        model = '_efron_'\n",
    "        dataset_name = filename.split('_')[0]\n",
    "        # add IBS later\n",
    "        outer_scores = {'cindex_train_'+dataset_name:[], 'cindex_test_'+dataset_name:[],\n",
    "                        'ibs_train_'+dataset_name:[], 'ibs_test_'+dataset_name:[]}\n",
    "        best_params = {'best_params_'+dataset_name:[]}\n",
    "        best_model = {'best_model_'+dataset_name:[]}\n",
    "        ct = make_column_transformer(\n",
    "                #(OneHotEncoder(sparse_output=False), make_column_selector(dtype_include=['category', 'object']))\n",
    "                (StandardScaler(), make_column_selector(dtype_include=['float32']))\n",
    "                ,remainder='drop')\n",
    "        pipe = Pipeline([('scaler',ct),\n",
    "                         ('pca', PCA()),#n_components=10\n",
    "                        ('estimator', net)])\n",
    "        rs = RandomizedSearchCV(pipe, param_grid_aft_tcga, scoring = scoring_function, n_jobs=-1, \n",
    "                                    n_iter=2, refit=True)\n",
    "        for i, (train_index, test_index) in enumerate(outer_custom_cv.split(X, y)):\n",
    "                # Split data into training and testing sets for outer fold\n",
    "                X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "                y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "                X_train, y_train = sort_X_y_pandas(X_train, y_train)\n",
    "                X_test, y_test = sort_X_y_pandas(X_test, y_test)\n",
    "\n",
    "                #print(X_train.shape, type(X_train))\n",
    "                #print(y_train.shape, type(y_train))\n",
    "                #print(X_test.shape, type(X_test))\n",
    "                #print(y_test.shape, type(y_test))\n",
    "                # save splits and data\n",
    "                savetxt('splits/train_index_'+str(i)+'_'+filename, train_index, delimiter=',')\n",
    "                savetxt('splits/test_index_'+str(i)+'_'+filename, test_index, delimiter=',')\n",
    "                \n",
    "                savetxt('splits/X_train_'+str(i)+'_'+filename, X_train, delimiter=',')\n",
    "                savetxt('splits/X_test_'+str(i)+'_'+filename, X_test, delimiter=',')\n",
    "\n",
    "                savetxt('splits/y_train_'+str(i)+'_'+filename, y_train, delimiter=',')\n",
    "                savetxt('splits/y_test_'+str(i)+'_'+filename, y_test, delimiter=',')\n",
    "\n",
    "                strat = np.sign(y_train)\n",
    "                valid_split = ValidSplit(cv=0.1, stratified=strat, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                rs.fit(X_train, y_train)\n",
    "                best_preds_train = rs.best_estimator_.predict(X_train)\n",
    "                best_preds_test = rs.best_estimator_.predict(X_test)\n",
    "                # save hyperparameter settings\n",
    "                params = rs.best_estimator_.get_params\n",
    "                best_params['best_params_'+dataset_name] += [rs.best_params_]\n",
    "                best_model['best_model_'+dataset_name] += [params]\n",
    "                try:\n",
    "                    cum_hazard_train = get_cumulative_hazard_function_breslow(\n",
    "                            X_train.values, X_train.values, y_train.values, y_train.values,\n",
    "                            best_preds_train.reshape(-1), best_preds_train.reshape(-1)\n",
    "                            )\n",
    "\n",
    "                    df_survival_train = np.exp(-cum_hazard_train)\n",
    "                    durations_train, events_train = transform_back(y_train.values)\n",
    "                    time_grid_train = np.linspace(durations_train.min(), durations_train.max(), 100)\n",
    "                    ev = EvalSurv(df_survival_train, durations_train, events_train, censor_surv='km')\n",
    "                    print('Concordance Index',ev.concordance_td('antolini'))\n",
    "                    print('Integrated Brier Score:',ev.integrated_brier_score(time_grid_train))\n",
    "                    cindex_score_train = ev.concordance_td('antolini')\n",
    "                    ibs_score_train = ev.integrated_brier_score(time_grid_train)\n",
    "\n",
    "                    outer_scores['cindex_train_'+dataset_name] += [cindex_score_train]\n",
    "                    outer_scores['ibs_train_'+dataset_name] += [ibs_score_train]\n",
    "\n",
    "                except:\n",
    "                    outer_scores['cindex_train_'+dataset_name] += [np.nan]\n",
    "                    outer_scores['ibs_train_'+dataset_name] += [np.nan]\n",
    "                    \n",
    "                try:\n",
    "                    cum_hazard_test = get_cumulative_hazard_function_breslow(\n",
    "                            X_train.values, X_test.values, y_train.values, y_test.values,\n",
    "                            best_preds_train.reshape(-1), best_preds_test.reshape(-1)\n",
    "                            )\n",
    "                    df_survival_test = np.exp(-cum_hazard_test)\n",
    "                    durations_test, events_test = transform_back(y_test.values)\n",
    "                    print('durations',durations_test.min(), durations_test.max())\n",
    "                    time_grid_test = np.linspace(durations_test.min(), durations_test.max(), 100)\n",
    "                    ev = EvalSurv(df_survival_test, durations_test, events_test, censor_surv='km')\n",
    "                    print('Concordance Index',ev.concordance_td('antolini'))\n",
    "                    print('Integrated Brier Score:',ev.integrated_brier_score(time_grid_test))\n",
    "                    cindex_score_test = ev.concordance_td('antolini')\n",
    "                    ibs_score_test = ev.integrated_brier_score(time_grid_test)\n",
    "\n",
    "                    outer_scores['cindex_test_'+dataset_name] += [cindex_score_test]\n",
    "                    outer_scores['ibs_test_'+dataset_name] += [ibs_score_test]\n",
    "                except: \n",
    "                    outer_scores['cindex_test_'+dataset_name] += [np.nan]\n",
    "                    outer_scores['ibs_test_'+dataset_name] += [np.nan]\n",
    "            \n",
    "                df_best_params = pd.DataFrame(best_params)\n",
    "                df_best_model = pd.DataFrame(best_model)\n",
    "                df_outer_scores = pd.DataFrame(outer_scores)\n",
    "                df_metrics = pd.concat([df_best_params,df_best_model,df_outer_scores], axis=1)\n",
    "                df_metrics.to_csv('metrics/metric_summary'+model+str(i)+'_'+filename, index=False)\n",
    "        return best_model, best_params, outer_scores, best_preds_train, best_preds_test, X_train, X_test, y_train, y_test\n",
    "\n",
    "                \n",
    "#cv=inner_custom_cv,pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split gex_?|100130426      float32\n",
      "gex_?|100133144      float32\n",
      "gex_?|100134869      float32\n",
      "gex_?|10357          float32\n",
      "gex_?|10431          float32\n",
      "                      ...   \n",
      "gex_ZYG11A|440590    float32\n",
      "gex_ZYG11B|79699     float32\n",
      "gex_ZYX|7791         float32\n",
      "gex_ZZEF1|23140      float32\n",
      "gex_ZZZ3|26009       float32\n",
      "Length: 20531, dtype: object\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m111.4011\u001b[0m  0.0027\n",
      "      2      111.4011  0.0016\n",
      "      3      111.4011  0.0015\n",
      "      4      111.4011  0.0015\n",
      "      5      111.4011  0.0015\n",
      "      6      111.4011  0.0015\n",
      "      7      111.4011  0.0015\n",
      "      8      111.4011  0.0015\n",
      "      9      111.4011  0.0015\n",
      "     10      111.4011  0.0022\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m195.5269\u001b[0m  0.0041\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2      195.5269  0.0035\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m176.3496\u001b[0m  0.0039\n",
      "      3      195.5269  0.0019\n",
      "      2      176.3496  0.0017\n",
      "      4      195.5269  0.0016\n",
      "      3      176.3496  0.0016\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "      4      176.3496  0.0016\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m162.8707\u001b[0m  0.0067\n",
      "      5      195.5269  0.0032\n",
      "      5      176.3496  0.0015\n",
      "      2      162.8707  0.0020\n",
      "      6      176.3496  0.0015\n",
      "      3      162.8707  0.0016\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m205.7822\u001b[0m  0.0029\n",
      "      7      176.3496  0.0018\n",
      "      4      162.8707  0.0015\n",
      "      2      205.7822  0.0017\n",
      "      8      176.3496  0.0016\n",
      "      5      162.8707  0.0015\n",
      "      3      205.7822  0.0016\n",
      "      9      176.3496  0.0015\n",
      "      6      162.8707  0.0015\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      6      195.5269  0.0083\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      4      205.7822  0.0015\n",
      "     10      176.3496  0.0016\n",
      "Restoring best model from epoch 1.\n",
      "      7      195.5269  0.0018\n",
      "      5      205.7822  0.0015\n",
      "      7      162.8707  0.0029\n",
      "      8      195.5269  0.0016\n",
      "      8      162.8707  0.0016\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m110.1934\u001b[0m  0.0025\n",
      "      9      195.5269  0.0015\n",
      "      9      162.8707  0.0015\n",
      "      2      110.1934  0.0016\n",
      "     10      195.5269  0.0015\n",
      "Restoring best model from epoch 1.\n",
      "     10      162.8707  0.0015\n",
      "Restoring best model from epoch 1.\n",
      "      3      110.1934  0.0016\n",
      "      4      110.1934  0.0015\n",
      "      6      205.7822  0.0076\n",
      "      7      205.7822  0.0018\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      5      110.1934  0.0036\n",
      "      8      205.7822  0.0019\n",
      "      6      110.1934  0.0015\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      7      110.1934  0.0020\n",
      "      9      205.7822  0.0032\n",
      "     10      205.7822  0.0026\n",
      "Restoring best model from epoch 1.\n",
      "      8      110.1934  0.0035\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m194.8201\u001b[0m  0.0037\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      9      110.1934  0.0032\n",
      "      2      194.8201  0.0025\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      3      194.8201  0.0019\n",
      "     10      110.1934  0.0028\n",
      "Restoring best model from epoch 1.\n",
      "      4      194.8201  0.0022\n",
      "      5      194.8201  0.0015\n",
      "      6      194.8201  0.0015\n",
      "      7      194.8201  0.0025\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m100.1409\u001b[0m  0.0039\n",
      "      8      194.8201  0.0015\n",
      "      2      100.1409  0.0018\n",
      "      9      194.8201  0.0015\n",
      "      3      100.1409  0.0016\n",
      "     10      194.8201  0.0015\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      4      100.1409  0.0015\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      5      100.1409  0.0015\n",
      "      6      100.1409  0.0015\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m106.8826\u001b[0m  0.0024\n",
      "      7      100.1409  0.0015\n",
      "      2      106.8826  0.0016\n",
      "      8      100.1409  0.0015\n",
      "      3      106.8826  0.0015\n",
      "      9      100.1409  0.0015\n",
      "      4      106.8826  0.0015\n",
      "     10      100.1409  0.0015\n",
      "Restoring best model from epoch 1.\n",
      "      5      106.8826  0.0015\n",
      "      6      106.8826  0.0015\n",
      "      7      106.8826  0.0015\n",
      "      8      106.8826  0.0015\n",
      "      9      106.8826  0.0022\n",
      "     10      106.8826  0.0038\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m98.9209\u001b[0m  0.0026\n",
      "      2       98.9209  0.0018\n",
      "      3       98.9209  0.0016\n",
      "      4       98.9209  0.0015\n",
      "      5       98.9209  0.0015\n",
      "      6       98.9209  0.0015\n",
      "      7       98.9209  0.0018\n",
      "      8       98.9209  0.0017\n",
      "      9       98.9209  0.0016\n",
      "     10       98.9209  0.0049\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m102.7934\u001b[0m  0.0052\n",
      "      2      102.7934  0.0028\n",
      "      3      102.7934  0.0052\n",
      "      4      102.7934  0.0045\n",
      "      5      102.7934  0.0031\n",
      "      6      102.7934  0.0035\n",
      "      7      102.7934  0.0033\n",
      "      8      102.7934  0.0032\n",
      "      9      102.7934  0.0029\n",
      "     10      102.7934  0.0032\n",
      "Restoring best model from epoch 1.\n",
      "y_train breslow final [  -13.   -15.    19.   -20.    20.   -35.   -46.   -55.    56.   -59.\n",
      "    62.   -64.    65.   -67.   -68.    68.    69.    76.   -82.   -84.\n",
      "    88.   -89.    90.    92.    93.    98.    99.  -105.  -110.  -117.\n",
      "   118.   122.   128.  -129.   142.   144.   146.   149.   154.   154.\n",
      "  -158.   163.  -163.   168.   168.   173.   182.  -187.   191.   200.\n",
      "   206.   213.   220.  -224.   232.   237.   246.   248.   250.   251.\n",
      "  -251.   254.   254.   258.   259.   261.   262.   272.   272.   273.\n",
      "   274.  -276.   278.  -293.   294.   303.   311.   321.   324.   324.\n",
      "   328.  -330.   332.  -333.  -337.   344.   356.  -359.  -361.   364.\n",
      "  -364.  -365.  -366.  -368.  -368.  -369.   370.  -370.   370.  -372.\n",
      "  -372.  -373.  -376.  -377.  -379.  -382.  -383.  -384.  -384.   385.\n",
      "   386.  -389.   391.   393.  -394.  -398.  -398.   400.   406.   408.\n",
      "   413.  -415.  -415.  -416.   418.  -425.  -428.  -433.   434.   437.\n",
      "   453.   454.  -455.  -460.   460.  -466.  -466.   467.  -467.   467.\n",
      "  -469.   474.  -474.  -475.  -477.   486.   495.  -495.  -495.  -503.\n",
      "  -507.   508.  -508.   510.   510.  -512.   522.  -522.  -536.   536.\n",
      "  -536.  -539.  -540.   544.   544.  -546.   547.   547.   550.  -560.\n",
      "  -562.   565.  -573.   575.   577.  -578.  -578.  -580.  -581.  -582.\n",
      "  -588.   590.  -590.   599.  -603.  -610.   617.  -618.   623.   630.\n",
      "  -636.  -638.  -641.  -642.  -649.   651.  -665.   665.   674.   680.\n",
      "   685.   690.   696.  -700.   706.   712.   719.  -731.   734.   739.\n",
      "  -750.  -758.  -773.   778.  -789.   795.  -798.  -799.  -812.  -813.\n",
      "   819.  -820.   823.  -832.  -832.  -842.   859.   864.  -864.  -887.\n",
      "  -893.  -897.   904.  -906.  -921.   941.  -945.   974.  -997.  1004.\n",
      "  1005.  1008. -1029.  1036. -1048.  1077. -1090. -1094. -1108. -1110.\n",
      " -1115. -1127.  1163. -1174. -1181.  1270. -1326.  1348. -1350. -1370.\n",
      "  1420.  1423. -1429. -1454. -1455. -1529. -1538. -1542.  1556. -1582.\n",
      " -1604. -1621. -1639. -1649.  1670. -1708. -1714.  1718. -1761. -1792.\n",
      "  1804. -1806. -1830. -1845.  1869. -1884. -1912. -1947. -1949. -1952.\n",
      "  1971. -2009.  2020. -2020. -2024. -2027. -2044. -2049. -2139. -2177.\n",
      " -2312. -2330. -2380. -2423. -2625.  2641. -2656. -2703. -2790.  2828.\n",
      " -2868. -2886.  2954. -2964. -3011.  3183. -3314. -3364. -3432. -3817.\n",
      " -3981. -4967. -5041. -5050.]\n",
      "Concordance Index 0.5380360632766695\n",
      "Integrated Brier Score: 0.370768670807669\n",
      "y_train breslow final [  -13.   -15.    19.   -20.    20.   -35.   -46.   -55.    56.   -59.\n",
      "    62.   -64.    65.   -67.   -68.    68.    69.    76.   -82.   -84.\n",
      "    88.   -89.    90.    92.    93.    98.    99.  -105.  -110.  -117.\n",
      "   118.   122.   128.  -129.   142.   144.   146.   149.   154.   154.\n",
      "  -158.   163.  -163.   168.   168.   173.   182.  -187.   191.   200.\n",
      "   206.   213.   220.  -224.   232.   237.   246.   248.   250.   251.\n",
      "  -251.   254.   254.   258.   259.   261.   262.   272.   272.   273.\n",
      "   274.  -276.   278.  -293.   294.   303.   311.   321.   324.   324.\n",
      "   328.  -330.   332.  -333.  -337.   344.   356.  -359.  -361.   364.\n",
      "  -364.  -365.  -366.  -368.  -368.  -369.   370.  -370.   370.  -372.\n",
      "  -372.  -373.  -376.  -377.  -379.  -382.  -383.  -384.  -384.   385.\n",
      "   386.  -389.   391.   393.  -394.  -398.  -398.   400.   406.   408.\n",
      "   413.  -415.  -415.  -416.   418.  -425.  -428.  -433.   434.   437.\n",
      "   453.   454.  -455.  -460.   460.  -466.  -466.   467.  -467.   467.\n",
      "  -469.   474.  -474.  -475.  -477.   486.   495.  -495.  -495.  -503.\n",
      "  -507.   508.  -508.   510.   510.  -512.   522.  -522.  -536.   536.\n",
      "  -536.  -539.  -540.   544.   544.  -546.   547.   547.   550.  -560.\n",
      "  -562.   565.  -573.   575.   577.  -578.  -578.  -580.  -581.  -582.\n",
      "  -588.   590.  -590.   599.  -603.  -610.   617.  -618.   623.   630.\n",
      "  -636.  -638.  -641.  -642.  -649.   651.  -665.   665.   674.   680.\n",
      "   685.   690.   696.  -700.   706.   712.   719.  -731.   734.   739.\n",
      "  -750.  -758.  -773.   778.  -789.   795.  -798.  -799.  -812.  -813.\n",
      "   819.  -820.   823.  -832.  -832.  -842.   859.   864.  -864.  -887.\n",
      "  -893.  -897.   904.  -906.  -921.   941.  -945.   974.  -997.  1004.\n",
      "  1005.  1008. -1029.  1036. -1048.  1077. -1090. -1094. -1108. -1110.\n",
      " -1115. -1127.  1163. -1174. -1181.  1270. -1326.  1348. -1350. -1370.\n",
      "  1420.  1423. -1429. -1454. -1455. -1529. -1538. -1542.  1556. -1582.\n",
      " -1604. -1621. -1639. -1649.  1670. -1708. -1714.  1718. -1761. -1792.\n",
      "  1804. -1806. -1830. -1845.  1869. -1884. -1912. -1947. -1949. -1952.\n",
      "  1971. -2009.  2020. -2020. -2024. -2027. -2044. -2049. -2139. -2177.\n",
      " -2312. -2330. -2380. -2423. -2625.  2641. -2656. -2703. -2790.  2828.\n",
      " -2868. -2886.  2954. -2964. -3011.  3183. -3314. -3364. -3432. -3817.\n",
      " -3981. -4967. -5041. -5050.]\n",
      "durations 17.0 4343.0\n",
      "Concordance Index 0.5524861878453039\n",
      "Integrated Brier Score: 0.33220508277938676\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m259.9547\u001b[0m       \u001b[32m69.7718\u001b[0m  0.0175\n",
      "      2      \u001b[36m258.1624\u001b[0m       70.5339  0.0156\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      3      266.6395       71.9736  0.0149\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m198.0335\u001b[0m      \u001b[32m127.5266\u001b[0m  0.0127\n",
      "      2      \u001b[36m192.7317\u001b[0m       \u001b[32m93.3313\u001b[0m  0.0120\n",
      "      4      \u001b[36m253.7364\u001b[0m       73.5777  0.0229\n",
      "      3      \u001b[36m179.4439\u001b[0m       \u001b[32m86.5549\u001b[0m  0.0110\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      4      \u001b[36m175.4792\u001b[0m       \u001b[32m83.7945\u001b[0m  0.0107\n",
      "      5      259.0532       74.5869  0.0157\n",
      "      5      \u001b[36m171.7268\u001b[0m       \u001b[32m83.7625\u001b[0m  0.0104\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      6      262.0086       74.7023  0.0137\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      6      \u001b[36m167.7424\u001b[0m       87.7789  0.0104\n",
      "      7      257.7091       73.6052  0.0164\n",
      "      7      \u001b[36m164.9112\u001b[0m       89.4994  0.0103\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m205.9205\u001b[0m       \u001b[32m96.6450\u001b[0m  0.0281\n",
      "      8      166.6582       88.8593  0.0155\n",
      "      8      258.2928       72.8502  0.0171\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m235.9228\u001b[0m       \u001b[32m74.5057\u001b[0m  0.0412\n",
      "      9      \u001b[36m161.0076\u001b[0m       87.9405  0.0150\n",
      "      2      \u001b[36m191.7894\u001b[0m      100.1932  0.0274\n",
      "      9      261.2443       72.2289  0.0177\n",
      "     10      \u001b[36m159.6946\u001b[0m       89.0397  0.0149\n",
      "Restoring best model from epoch 5.\n",
      "      2      \u001b[36m227.3885\u001b[0m       75.0613  0.0214\n",
      "     10      \u001b[36m244.6528\u001b[0m       71.9091  0.0158\n",
      "Restoring best model from epoch 1.\n",
      "      3      193.1011      101.0114  0.0206\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      3      230.8223       75.2560  0.0142\n",
      "      4      \u001b[36m188.4686\u001b[0m       99.5902  0.0170\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m223.1638\u001b[0m       \u001b[32m95.9267\u001b[0m  0.0151\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m267.7611\u001b[0m       \u001b[32m93.5881\u001b[0m  0.0117\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.      4      \u001b[36m225.2548\u001b[0m       \u001b[32m74.1215\u001b[0m  0.0128\n",
      "\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2      226.4313       \u001b[32m73.1407\u001b[0m  0.0112\n",
      "      2      \u001b[36m258.0912\u001b[0m       \u001b[32m81.4993\u001b[0m  0.0110\n",
      "      5      234.0999       \u001b[32m73.1724\u001b[0m  0.0154\n",
      "      5      \u001b[36m185.4634\u001b[0m       98.3804  0.0215\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m218.4302\u001b[0m       \u001b[32m92.9375\u001b[0m  0.0155\n",
      "      3      \u001b[36m205.9950\u001b[0m       \u001b[32m72.4172\u001b[0m  0.0099\n",
      "      3      \u001b[36m243.3802\u001b[0m       \u001b[32m77.3408\u001b[0m  0.0142\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m193.2448\u001b[0m       \u001b[32m88.4039\u001b[0m  0.0222\n",
      "      6      \u001b[36m225.2074\u001b[0m       \u001b[32m72.9158\u001b[0m  0.0130\n",
      "      2      \u001b[36m212.6038\u001b[0m       \u001b[32m92.5580\u001b[0m  0.0133\n",
      "      4      217.5901       \u001b[32m70.9599\u001b[0m  0.0108\n",
      "      6      192.6546       \u001b[32m94.7046\u001b[0m  0.0174\n",
      "      4      243.8724       \u001b[32m74.0861\u001b[0m  0.0108\n",
      "      2      \u001b[36m183.8930\u001b[0m       94.1482  0.0136\n",
      "      7      \u001b[36m220.5805\u001b[0m       \u001b[32m72.6709\u001b[0m  0.0134\n",
      "      3      220.7151       94.2132  0.0132\n",
      "      5      \u001b[36m204.6544\u001b[0m       \u001b[32m70.5159\u001b[0m  0.0104\n",
      "      5      \u001b[36m234.2200\u001b[0m       \u001b[32m72.1133\u001b[0m  0.0126\n",
      "      3      189.4635       92.3883  0.0119\n",
      "      8      \u001b[36m217.2740\u001b[0m       \u001b[32m72.6135\u001b[0m  0.0123\n",
      "      6      \u001b[36m204.4673\u001b[0m       72.4652  0.0098\n",
      "      7      \u001b[36m185.4215\u001b[0m       \u001b[32m91.5873\u001b[0m  0.0218\n",
      "      4      224.3950       \u001b[32m91.3287\u001b[0m  0.0120\n",
      "      6      \u001b[36m232.2081\u001b[0m       72.8804  0.0103\n",
      "      7      \u001b[36m199.6557\u001b[0m       74.7855  0.0100\n",
      "      4      \u001b[36m183.4728\u001b[0m       \u001b[32m88.3638\u001b[0m  0.0133\n",
      "      9      219.4993       \u001b[32m72.5729\u001b[0m  0.0130\n",
      "      7      234.2918       74.2472  0.0107\n",
      "      5      \u001b[36m211.1778\u001b[0m       \u001b[32m88.8435\u001b[0m  0.0121\n",
      "      8      188.0296       \u001b[32m89.1955\u001b[0m  0.0134\n",
      "      8      \u001b[36m196.8039\u001b[0m       76.2519  0.0114\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      5      \u001b[36m173.6392\u001b[0m       \u001b[32m84.4091\u001b[0m  0.0124\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      8      \u001b[36m229.4073\u001b[0m       76.0200  0.0114\n",
      "     10      218.0745       72.6551  0.0129\n",
      "Restoring best model from epoch 9.\n",
      "      6      226.6103       \u001b[32m87.0037\u001b[0m  0.0125\n",
      "      9      189.0055       \u001b[32m86.0651\u001b[0m  0.0139\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      9      \u001b[36m193.1768\u001b[0m       76.9571  0.0101\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      9      \u001b[36m226.0729\u001b[0m       77.3399  0.0109\n",
      "      6      181.9592       \u001b[32m81.6362\u001b[0m  0.0134\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m223.1333\u001b[0m      \u001b[32m106.1569\u001b[0m  0.0133\n",
      "     10      \u001b[36m192.0096\u001b[0m       77.5873  0.0101\n",
      "Restoring best model from epoch 5.\n",
      "     10      \u001b[36m184.7851\u001b[0m       \u001b[32m84.6746\u001b[0m  0.0129\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m190.2452\u001b[0m       \u001b[32m88.9124\u001b[0m  0.0114\n",
      "      7      \u001b[36m210.1791\u001b[0m       \u001b[32m84.9000\u001b[0m  0.0176\n",
      "     10      228.2504       77.9806  0.0107\n",
      "Restoring best model from epoch 5.\n",
      "      7      176.5968       \u001b[32m81.2679\u001b[0m  0.0122\n",
      "      2      \u001b[36m207.2733\u001b[0m       \u001b[32m99.0798\u001b[0m  0.0157\n",
      "      8      215.0400       \u001b[32m83.5560\u001b[0m  0.0125\n",
      "      2      \u001b[36m173.6115\u001b[0m       \u001b[32m88.2950\u001b[0m  0.0151\n",
      "      8      \u001b[36m173.5422\u001b[0m       81.4406  0.0124\n",
      "      3      \u001b[36m203.0688\u001b[0m       \u001b[32m96.0242\u001b[0m  0.0103\n",
      "      3      \u001b[36m171.1027\u001b[0m       88.7177  0.0102\n",
      "      9      220.4776       \u001b[32m82.8561\u001b[0m  0.0118\n",
      "      4      \u001b[36m191.7651\u001b[0m       \u001b[32m92.0531\u001b[0m  0.0105\n",
      "      4      \u001b[36m162.5848\u001b[0m       89.1275  0.0101\n",
      "     10      \u001b[36m205.3933\u001b[0m       83.1246  0.0117\n",
      "Restoring best model from epoch 9.\n",
      "      9      \u001b[36m164.4710\u001b[0m       82.5870  0.0215\n",
      "      5      \u001b[36m189.0367\u001b[0m       \u001b[32m90.4123\u001b[0m  0.0099\n",
      "      5      \u001b[36m160.0115\u001b[0m       88.8989  0.0103\n",
      "      6      196.8790       \u001b[32m89.7938\u001b[0m  0.0115\n",
      "     10      178.7919       83.6869  0.0150\n",
      "Restoring best model from epoch 7.\n",
      "      6      \u001b[36m154.5978\u001b[0m       90.5874  0.0094\n",
      "      7      \u001b[36m154.5058\u001b[0m       93.1033  0.0093\n",
      "      7      191.1439       90.9111  0.0113\n",
      "      8      \u001b[36m148.1745\u001b[0m       98.0303  0.0106\n",
      "      8      \u001b[36m187.8833\u001b[0m       90.0651  0.0110\n",
      "      9      150.5821      103.9411  0.0111\n",
      "      9      \u001b[36m184.5344\u001b[0m       \u001b[32m89.6975\u001b[0m  0.0106\n",
      "     10      \u001b[36m144.7250\u001b[0m      106.9980  0.0100\n",
      "Restoring best model from epoch 2.\n",
      "     10      \u001b[36m181.9090\u001b[0m       89.8615  0.0114\n",
      "Restoring best model from epoch 9.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m248.6980\u001b[0m       \u001b[32m74.6826\u001b[0m  0.0253\n",
      "      2      \u001b[36m238.2985\u001b[0m       \u001b[32m69.9292\u001b[0m  0.0244\n",
      "      3      \u001b[36m235.6570\u001b[0m       70.4634  0.0250\n",
      "      4      \u001b[36m229.6257\u001b[0m       70.4203  0.0248\n",
      "      5      234.1897       70.9055  0.0232\n",
      "      6      234.9981       71.7709  0.0235\n",
      "      7      \u001b[36m227.6334\u001b[0m       72.1672  0.0237\n",
      "      8      229.7929       72.9959  0.0225\n",
      "      9      233.5055       73.4986  0.0253\n",
      "     10      228.3308       73.5001  0.0232\n",
      "Restoring best model from epoch 2.\n",
      "y_train breslow final [  -13.   -17.    19.   -20.   -28.   -35.   -37.   -46.   -55.    56.\n",
      "    56.    57.   -59.   -64.   -64.    65.   -67.   -68.    68.    69.\n",
      "    76.    81.   -82.   -84.   -89.    90.    92.    93.   -95.  -105.\n",
      "   106.  -110.  -117.   118.   122.   128.  -129.   131.   149.   154.\n",
      "  -158.   163.  -163.   168.   168.   182.  -187.  -189.   191.   200.\n",
      "   205.   206.   211.   213.   216.   220.   223.  -224.   232.   232.\n",
      "   237.   246.   248.   250.   251.  -251.   253.   254.   254.   259.\n",
      "   261.   272.   272.   272.   273.   274.  -276.   278.   294.   294.\n",
      "   311.   321.   324.   324.  -330.   332.  -337.   340.   344.   344.\n",
      "  -345.  -345.   356.  -359.  -361.   362.  -364.  -366.  -368.  -368.\n",
      "  -369.   370.  -372.  -372.  -373.  -377.  -382.  -383.  -384.   385.\n",
      "   385.   386.   388.   393.  -394.  -398.  -398.  -399.   406.  -410.\n",
      "   413.   413.   413.   415.  -415.  -416.   418.  -423.  -425.  -428.\n",
      "  -428.   453.   455.  -457.   460.  -466.  -466.   467.  -467.   467.\n",
      "   474.  -474.  -475.  -477.  -480.  -481.  -484.  -485.   486.  -491.\n",
      "   492.   495.  -495.  -495.  -503.   508.  -508.   510.  -512.  -524.\n",
      "   530.  -536.   536.  -536.   539.  -540.  -542.   544.   544.  -546.\n",
      "   547.   550.  -560.  -562.   565.   565.  -572.  -572.   577.  -578.\n",
      "   579.  -580.  -581.  -582.   590.   593.   599.   602.  -603.  -610.\n",
      "   612.   615.  -618.   623.   630.  -633.  -636.  -640.  -641.  -642.\n",
      "  -646.  -648.  -649.   651.  -665.   665.   674.   680.   685.   690.\n",
      "  -691.   696.  -699.  -700.   706.   712.   719.  -731.   734.  -750.\n",
      "  -758.  -761.  -773.   778.  -783.  -789.   795.  -799.  -812.  -813.\n",
      "   819.   823.  -832.   835.  -840.  -842.  -851.   864.  -864.  -873.\n",
      "  -887.  -893.  -899.  -906.   941.  -945.   949.   974.  -997. -1003.\n",
      "  1005.  1008. -1029.  1036. -1048.  1064. -1072.  1077. -1090. -1108.\n",
      " -1110. -1115. -1127. -1186. -1219.  1270. -1326.  1348.  1367.  1420.\n",
      "  1423. -1429. -1454. -1460. -1522. -1538. -1542.  1556. -1561. -1582.\n",
      " -1621. -1639. -1649. -1708.  1718. -1792.  1804. -1830. -1845.  1869.\n",
      " -1884. -1912. -2008.  2020. -2020. -2024. -2027. -2044. -2049. -2109.\n",
      " -2139. -2177. -2293. -2330. -2380. -2423. -2625.  2641. -2656. -2703.\n",
      "  2828. -2868.  2954. -2964. -3011.  3183. -3314. -3364. -3420. -3432.\n",
      " -3981. -4343. -4967. -5041. -5050.]\n",
      "Concordance Index 0.63643108885179\n",
      "Integrated Brier Score: 0.19269004901035783\n",
      "y_train breslow final [  -13.   -17.    19.   -20.   -28.   -35.   -37.   -46.   -55.    56.\n",
      "    56.    57.   -59.   -64.   -64.    65.   -67.   -68.    68.    69.\n",
      "    76.    81.   -82.   -84.   -89.    90.    92.    93.   -95.  -105.\n",
      "   106.  -110.  -117.   118.   122.   128.  -129.   131.   149.   154.\n",
      "  -158.   163.  -163.   168.   168.   182.  -187.  -189.   191.   200.\n",
      "   205.   206.   211.   213.   216.   220.   223.  -224.   232.   232.\n",
      "   237.   246.   248.   250.   251.  -251.   253.   254.   254.   259.\n",
      "   261.   272.   272.   272.   273.   274.  -276.   278.   294.   294.\n",
      "   311.   321.   324.   324.  -330.   332.  -337.   340.   344.   344.\n",
      "  -345.  -345.   356.  -359.  -361.   362.  -364.  -366.  -368.  -368.\n",
      "  -369.   370.  -372.  -372.  -373.  -377.  -382.  -383.  -384.   385.\n",
      "   385.   386.   388.   393.  -394.  -398.  -398.  -399.   406.  -410.\n",
      "   413.   413.   413.   415.  -415.  -416.   418.  -423.  -425.  -428.\n",
      "  -428.   453.   455.  -457.   460.  -466.  -466.   467.  -467.   467.\n",
      "   474.  -474.  -475.  -477.  -480.  -481.  -484.  -485.   486.  -491.\n",
      "   492.   495.  -495.  -495.  -503.   508.  -508.   510.  -512.  -524.\n",
      "   530.  -536.   536.  -536.   539.  -540.  -542.   544.   544.  -546.\n",
      "   547.   550.  -560.  -562.   565.   565.  -572.  -572.   577.  -578.\n",
      "   579.  -580.  -581.  -582.   590.   593.   599.   602.  -603.  -610.\n",
      "   612.   615.  -618.   623.   630.  -633.  -636.  -640.  -641.  -642.\n",
      "  -646.  -648.  -649.   651.  -665.   665.   674.   680.   685.   690.\n",
      "  -691.   696.  -699.  -700.   706.   712.   719.  -731.   734.  -750.\n",
      "  -758.  -761.  -773.   778.  -783.  -789.   795.  -799.  -812.  -813.\n",
      "   819.   823.  -832.   835.  -840.  -842.  -851.   864.  -864.  -873.\n",
      "  -887.  -893.  -899.  -906.   941.  -945.   949.   974.  -997. -1003.\n",
      "  1005.  1008. -1029.  1036. -1048.  1064. -1072.  1077. -1090. -1108.\n",
      " -1110. -1115. -1127. -1186. -1219.  1270. -1326.  1348.  1367.  1420.\n",
      "  1423. -1429. -1454. -1460. -1522. -1538. -1542.  1556. -1561. -1582.\n",
      " -1621. -1639. -1649. -1708.  1718. -1792.  1804. -1830. -1845.  1869.\n",
      " -1884. -1912. -2008.  2020. -2020. -2024. -2027. -2044. -2049. -2109.\n",
      " -2139. -2177. -2293. -2330. -2380. -2423. -2625.  2641. -2656. -2703.\n",
      "  2828. -2868.  2954. -2964. -3011.  3183. -3314. -3364. -3420. -3432.\n",
      " -3981. -4343. -4967. -5041. -5050.]\n",
      "durations 15.0 3817.0\n",
      "Concordance Index 0.5745494265428728\n",
      "Integrated Brier Score: 0.28091360229487494\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m242.4409\u001b[0m       \u001b[32m47.3536\u001b[0m  0.0151\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m214.0191\u001b[0m       \u001b[32m64.3211\u001b[0m  0.0115\n",
      "      2      244.6859       \u001b[32m45.8469\u001b[0m  0.0147\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2      \u001b[36m205.6922\u001b[0m       65.8552  0.0114\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m249.3285\u001b[0m       \u001b[32m79.6212\u001b[0m  0.0145\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m251.4267\u001b[0m       \u001b[32m52.4156\u001b[0m  0.0169\n",
      "      3      245.9580       46.6068  0.0142\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m233.4898\u001b[0m       \u001b[32m66.2672\u001b[0m  0.0126\n",
      "      3      \u001b[36m198.7411\u001b[0m       \u001b[32m62.4336\u001b[0m  0.0122\n",
      "      2      \u001b[36m246.7041\u001b[0m       \u001b[32m78.1288\u001b[0m  0.0137\n",
      "      2      256.4211       \u001b[32m50.5383\u001b[0m  0.0135\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2      243.2268       \u001b[32m57.4711\u001b[0m  0.0108\n",
      "      4      \u001b[36m193.8358\u001b[0m       \u001b[32m59.7473\u001b[0m  0.0102\n",
      "      4      \u001b[36m231.0373\u001b[0m       48.0133  0.0197\n",
      "      3      \u001b[36m239.5230\u001b[0m       \u001b[32m76.1259\u001b[0m  0.0137\n",
      "      3      260.6987       50.9903  0.0130\n",
      "      3      \u001b[36m215.8020\u001b[0m       \u001b[32m55.6735\u001b[0m  0.0101\n",
      "      5      200.1050       \u001b[32m59.0422\u001b[0m  0.0097\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m206.3226\u001b[0m       \u001b[32m61.8852\u001b[0m  0.0153\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      5      237.0893       48.8019  0.0162\n",
      "      4      \u001b[36m234.1285\u001b[0m       \u001b[32m73.6566\u001b[0m  0.0128\n",
      "      4      254.5029       50.5867  0.0125\n",
      "      4      \u001b[36m210.8071\u001b[0m       \u001b[32m55.3182\u001b[0m  0.0109\n",
      "      6      \u001b[36m187.0031\u001b[0m       60.1929  0.0152\n",
      "      2      210.8488       63.2138  0.0145\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m250.5922\u001b[0m       \u001b[32m53.3373\u001b[0m  0.0107\n",
      "      5      \u001b[36m209.1614\u001b[0m       55.9705  0.0102\n",
      "      6      234.9049       48.9490  0.0129\n",
      "      5      \u001b[36m223.7644\u001b[0m       \u001b[32m72.3131\u001b[0m  0.0141\n",
      "      5      \u001b[36m241.2218\u001b[0m       50.7047  0.0149\n",
      "      7      187.7895       61.5110  0.0108\n",
      "      3      218.4715       65.2367  0.0126\n",
      "      2      \u001b[36m244.8129\u001b[0m       \u001b[32m50.7309\u001b[0m  0.0128\n",
      "      6      \u001b[36m206.3689\u001b[0m       58.0616  0.0125\n",
      "      6      241.2868       \u001b[32m70.8867\u001b[0m  0.0126\n",
      "      6      \u001b[36m240.2393\u001b[0m       50.9208  0.0128\n",
      "      3      \u001b[36m229.8050\u001b[0m       51.8622  0.0104\n",
      "      4      \u001b[36m202.2908\u001b[0m       65.5333  0.0141\n",
      "      7      244.3304       48.7493  0.0219\n",
      "      7      209.0611       60.3871  0.0105\n",
      "      8      188.3643       62.8266  0.0178\n",
      "      7      243.7243       51.1176  0.0132\n",
      "      7      234.7066       \u001b[32m69.8603\u001b[0m  0.0168\n",
      "      9      \u001b[36m183.3329\u001b[0m       63.9153  0.0100\n",
      "      8      \u001b[36m204.0813\u001b[0m       62.3214  0.0107\n",
      "      5      208.4082       65.3007  0.0143\n",
      "      8      \u001b[36m229.5649\u001b[0m       48.4548  0.0145\n",
      "      4      \u001b[36m224.3015\u001b[0m       51.3759  0.0163\n",
      "      8      \u001b[36m237.0577\u001b[0m       51.5456  0.0131\n",
      "     10      \u001b[36m179.8838\u001b[0m       64.5531  0.0113\n",
      "Restoring best model from epoch 5.\n",
      "      8      228.8939       \u001b[32m69.6134\u001b[0m  0.0125\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      6      \u001b[36m200.6512\u001b[0m       64.9283  0.0119\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      9      232.4364       48.0062  0.0130\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      5      \u001b[36m217.8003\u001b[0m       \u001b[32m50.6111\u001b[0m  0.0133\n",
      "      9      \u001b[36m202.4061\u001b[0m       64.2865  0.0192\n",
      "      9      252.6441       52.0730  0.0128\n",
      "      9      237.9369       \u001b[32m69.1059\u001b[0m  0.0130\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m256.6803\u001b[0m       \u001b[32m55.4000\u001b[0m  0.0114\n",
      "      7      217.2172       65.2517  0.0136\n",
      "     10      239.2448       47.7442  0.0125\n",
      "Restoring best model from epoch 2.\n",
      "     10      \u001b[36m197.6303\u001b[0m       66.1587  0.0102\n",
      "Restoring best model from epoch 4.\n",
      "      6      218.2368       \u001b[32m50.1769\u001b[0m  0.0104\n",
      "     10      239.5706       52.4972  0.0124\n",
      "Restoring best model from epoch 2.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m237.1391\u001b[0m       \u001b[32m56.3169\u001b[0m  0.0183\n",
      "     10      \u001b[36m221.0733\u001b[0m       \u001b[32m69.0373\u001b[0m  0.0128\n",
      "      7      \u001b[36m213.0741\u001b[0m       50.2802  0.0102\n",
      "      8      204.2315       64.5296  0.0129\n",
      "      2      272.6374       55.8426  0.0181\n",
      "      9      209.8720       63.9799  0.0119\n",
      "      2      \u001b[36m229.7429\u001b[0m       56.8242  0.0189\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      3      \u001b[36m244.6725\u001b[0m       57.6137  0.0123\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      8      \u001b[36m208.0324\u001b[0m       51.1347  0.0189\n",
      "     10      \u001b[36m198.3971\u001b[0m       63.1544  0.0116\n",
      "Restoring best model from epoch 1.\n",
      "      4      \u001b[36m237.9191\u001b[0m       57.6550  0.0119\n",
      "      3      235.7282       56.5808  0.0183\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m246.1065\u001b[0m       \u001b[32m91.5305\u001b[0m  0.0126\n",
      "      9      209.8669       52.4031  0.0160\n",
      "      5      \u001b[36m233.9204\u001b[0m       56.8645  0.0130\n",
      "      4      231.3426       56.4027  0.0123\n",
      "     10      \u001b[36m206.6129\u001b[0m       54.3784  0.0101\n",
      "Restoring best model from epoch 6.\n",
      "      2      277.1840       \u001b[32m78.4776\u001b[0m  0.0162\n",
      "      6      \u001b[36m232.6588\u001b[0m       55.8245  0.0116\n",
      "      5      \u001b[36m227.3461\u001b[0m       \u001b[32m56.0622\u001b[0m  0.0133\n",
      "      3      \u001b[36m233.7119\u001b[0m       \u001b[32m73.9592\u001b[0m  0.0118\n",
      "      6      231.3734       56.2864  0.0119\n",
      "      7      233.1293       \u001b[32m54.9464\u001b[0m  0.0212\n",
      "      4      \u001b[36m227.6064\u001b[0m       \u001b[32m72.1618\u001b[0m  0.0114\n",
      "      7      \u001b[36m220.6255\u001b[0m       56.7521  0.0130\n",
      "      8      \u001b[36m231.0734\u001b[0m       \u001b[32m54.4557\u001b[0m  0.0133\n",
      "      5      \u001b[36m226.3271\u001b[0m       \u001b[32m71.1189\u001b[0m  0.0141\n",
      "      8      231.6538       56.7993  0.0133\n",
      "      9      \u001b[36m230.4614\u001b[0m       \u001b[32m54.1693\u001b[0m  0.0105\n",
      "      6      \u001b[36m224.7471\u001b[0m       \u001b[32m70.3827\u001b[0m  0.0102\n",
      "      9      230.7390       56.7720  0.0126\n",
      "     10      \u001b[36m229.4075\u001b[0m       \u001b[32m53.8745\u001b[0m  0.0101\n",
      "      7      \u001b[36m219.3994\u001b[0m       70.6281  0.0099\n",
      "     10      221.6588       56.3824  0.0125\n",
      "Restoring best model from epoch 5.\n",
      "      8      \u001b[36m218.5097\u001b[0m       70.9663  0.0104\n",
      "      9      \u001b[36m215.9244\u001b[0m       71.2890  0.0110\n",
      "     10      \u001b[36m215.4440\u001b[0m       71.5266  0.0102\n",
      "Restoring best model from epoch 6.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m231.3957\u001b[0m       \u001b[32m88.4208\u001b[0m  0.0238\n",
      "      2      \u001b[36m230.7109\u001b[0m       \u001b[32m83.4228\u001b[0m  0.0236\n",
      "      3      \u001b[36m227.5532\u001b[0m       \u001b[32m83.0819\u001b[0m  0.0227\n",
      "      4      \u001b[36m226.5198\u001b[0m       \u001b[32m83.0516\u001b[0m  0.0233\n",
      "      5      \u001b[36m222.8818\u001b[0m       83.5509  0.0231\n",
      "      6      \u001b[36m217.4512\u001b[0m       84.8720  0.0229\n",
      "      7      220.2997       85.7952  0.0235\n",
      "      8      220.9660       85.6060  0.0285\n",
      "      9      227.5964       85.2880  0.0232\n",
      "     10      \u001b[36m215.0699\u001b[0m       84.8577  0.0240\n",
      "Restoring best model from epoch 4.\n",
      "y_train breslow final [  -13.   -15.   -17.    19.   -20.    20.   -28.   -35.   -37.   -46.\n",
      "   -55.    56.    56.    57.    62.   -64.   -64.    65.   -68.    68.\n",
      "    81.   -82.    88.   -89.    90.   -95.    98.    99.  -105.   106.\n",
      "  -110.   118.   128.  -129.   131.   142.   144.   146.   149.   154.\n",
      "  -158.   163.  -163.   168.   173.  -189.   191.   200.   200.   205.\n",
      "   206.   211.   216.   220.   223.  -224.   232.   232.   237.   250.\n",
      "   253.   254.   258.   259.   261.   262.   272.   272.   273.   274.\n",
      "  -293.   294.   294.   303.   321.   324.   328.  -330.   332.  -333.\n",
      "   340.   344.   344.  -345.  -345.   356.  -361.   362.   364.  -365.\n",
      "  -366.  -368.  -369.   370.  -370.   370.  -373.  -376.  -377.  -379.\n",
      "  -382.  -384.   385.   386.   388.  -389.   391.  -398.  -399.   400.\n",
      "   406.   408.  -410.   413.   413.  -415.   415.  -415.  -416.   418.\n",
      "  -423.  -428.  -428.  -433.   434.   437.   454.  -455.   455.  -457.\n",
      "  -460.  -466.   467.  -467.   467.  -469.  -474.  -475.  -480.  -481.\n",
      "  -484.  -485.  -491.   492.   495.  -495.  -503.  -507.   508.   510.\n",
      "  -512.   522.  -522.  -524.   530.   536.  -536.   539.  -539.  -542.\n",
      "   544.  -546.   547.   550.  -560.  -562.   565.   565.  -572.  -572.\n",
      "  -573.   575.   577.  -578.   579.  -581.  -582.  -588.   590.  -590.\n",
      "   593.   599.   602.  -603.  -610.   612.   615.   617.   623.   630.\n",
      "  -633.  -636.  -638.  -640.  -641.  -642.  -646.  -648.  -649.   651.\n",
      "  -665.   665.   680.   685.  -691.   696.  -699.  -700.   706.   719.\n",
      "  -731.   734.   739.  -750.  -758.  -761.   778.  -783.  -789.   795.\n",
      "  -798.  -812.  -813.   819.  -820.   823.  -832.  -832.   835.  -840.\n",
      "  -842.  -851.   859.   864.  -873.  -887.  -893.  -897.  -899.   904.\n",
      "  -906.  -921.   941.   949.   974.  -997. -1003.  1004.  1005.  1008.\n",
      " -1029.  1036. -1048.  1064. -1072. -1094. -1110. -1115. -1127.  1163.\n",
      " -1174. -1181. -1186. -1219.  1270.  1348. -1350.  1367. -1370.  1420.\n",
      " -1429. -1454. -1455. -1460. -1522. -1529. -1538. -1561. -1582. -1604.\n",
      " -1621. -1649.  1670. -1708. -1714.  1718. -1761.  1804. -1806. -1830.\n",
      " -1845.  1869. -1884. -1912. -1947. -1949. -1952.  1971. -2008. -2009.\n",
      " -2020. -2024. -2027. -2044. -2109. -2139. -2177. -2293. -2312. -2330.\n",
      " -2423.  2641. -2790.  2828. -2868. -2886. -2964. -3011.  3183. -3364.\n",
      " -3420. -3432. -3817. -4343. -5041.]\n",
      "Concordance Index 0.6728931364031278\n",
      "Integrated Brier Score: 0.1967562848176492\n",
      "y_train breslow final [  -13.   -15.   -17.    19.   -20.    20.   -28.   -35.   -37.   -46.\n",
      "   -55.    56.    56.    57.    62.   -64.   -64.    65.   -68.    68.\n",
      "    81.   -82.    88.   -89.    90.   -95.    98.    99.  -105.   106.\n",
      "  -110.   118.   128.  -129.   131.   142.   144.   146.   149.   154.\n",
      "  -158.   163.  -163.   168.   173.  -189.   191.   200.   200.   205.\n",
      "   206.   211.   216.   220.   223.  -224.   232.   232.   237.   250.\n",
      "   253.   254.   258.   259.   261.   262.   272.   272.   273.   274.\n",
      "  -293.   294.   294.   303.   321.   324.   328.  -330.   332.  -333.\n",
      "   340.   344.   344.  -345.  -345.   356.  -361.   362.   364.  -365.\n",
      "  -366.  -368.  -369.   370.  -370.   370.  -373.  -376.  -377.  -379.\n",
      "  -382.  -384.   385.   386.   388.  -389.   391.  -398.  -399.   400.\n",
      "   406.   408.  -410.   413.   413.  -415.   415.  -415.  -416.   418.\n",
      "  -423.  -428.  -428.  -433.   434.   437.   454.  -455.   455.  -457.\n",
      "  -460.  -466.   467.  -467.   467.  -469.  -474.  -475.  -480.  -481.\n",
      "  -484.  -485.  -491.   492.   495.  -495.  -503.  -507.   508.   510.\n",
      "  -512.   522.  -522.  -524.   530.   536.  -536.   539.  -539.  -542.\n",
      "   544.  -546.   547.   550.  -560.  -562.   565.   565.  -572.  -572.\n",
      "  -573.   575.   577.  -578.   579.  -581.  -582.  -588.   590.  -590.\n",
      "   593.   599.   602.  -603.  -610.   612.   615.   617.   623.   630.\n",
      "  -633.  -636.  -638.  -640.  -641.  -642.  -646.  -648.  -649.   651.\n",
      "  -665.   665.   680.   685.  -691.   696.  -699.  -700.   706.   719.\n",
      "  -731.   734.   739.  -750.  -758.  -761.   778.  -783.  -789.   795.\n",
      "  -798.  -812.  -813.   819.  -820.   823.  -832.  -832.   835.  -840.\n",
      "  -842.  -851.   859.   864.  -873.  -887.  -893.  -897.  -899.   904.\n",
      "  -906.  -921.   941.   949.   974.  -997. -1003.  1004.  1005.  1008.\n",
      " -1029.  1036. -1048.  1064. -1072. -1094. -1110. -1115. -1127.  1163.\n",
      " -1174. -1181. -1186. -1219.  1270.  1348. -1350.  1367. -1370.  1420.\n",
      " -1429. -1454. -1455. -1460. -1522. -1529. -1538. -1561. -1582. -1604.\n",
      " -1621. -1649.  1670. -1708. -1714.  1718. -1761.  1804. -1806. -1830.\n",
      " -1845.  1869. -1884. -1912. -1947. -1949. -1952.  1971. -2008. -2009.\n",
      " -2020. -2024. -2027. -2044. -2109. -2139. -2177. -2293. -2312. -2330.\n",
      " -2423.  2641. -2790.  2828. -2868. -2886. -2964. -3011.  3183. -3364.\n",
      " -3420. -3432. -3817. -4343. -5041.]\n",
      "durations 59.0 5050.0\n",
      "Concordance Index 0.6063321385902031\n",
      "Integrated Brier Score: 0.210025935913591\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m227.9112\u001b[0m       \u001b[32m84.1196\u001b[0m  0.0200\n",
      "      2      \u001b[36m206.5973\u001b[0m       \u001b[32m81.8561\u001b[0m  0.0151\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      3      211.7348       \u001b[32m75.5329\u001b[0m  0.0155\n",
      "      4      209.7269       \u001b[32m71.3692\u001b[0m  0.0133\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m216.3599\u001b[0m       \u001b[32m57.5747\u001b[0m  0.0211\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      5      206.6799       \u001b[32m69.3727\u001b[0m  0.0153\n",
      "      2      \u001b[36m202.4060\u001b[0m       \u001b[32m57.1640\u001b[0m  0.0151\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m250.4218\u001b[0m       \u001b[32m67.3414\u001b[0m  0.0156\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      6      \u001b[36m205.4895\u001b[0m       69.4976  0.0173\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m197.9052\u001b[0m       \u001b[32m58.9142\u001b[0m  0.0119\n",
      "      3      \u001b[36m197.3689\u001b[0m       58.1364  0.0164\n",
      "      2      \u001b[36m245.6107\u001b[0m       \u001b[32m65.8832\u001b[0m  0.0161\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      7      213.5042       70.0556  0.0141\n",
      "      2      203.4284       \u001b[32m55.3602\u001b[0m  0.0111\n",
      "      4      \u001b[36m196.5423\u001b[0m       59.2021  0.0126\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m213.7289\u001b[0m       \u001b[32m95.8256\u001b[0m  0.0142\n",
      "      3      249.5343       66.3288  0.0162\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      3      \u001b[36m187.0331\u001b[0m       60.0857  0.0107\n",
      "      5      199.8125       62.3716  0.0134\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m224.4179\u001b[0m       \u001b[32m87.2645\u001b[0m  0.0130\n",
      "      2      224.0809       \u001b[32m69.0172\u001b[0m  0.0122\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m248.9391\u001b[0m       \u001b[32m99.3746\u001b[0m  0.0225\n",
      "      4      \u001b[36m241.9364\u001b[0m       66.2237  0.0139\n",
      "      8      \u001b[36m202.3495\u001b[0m       70.7831  0.0218\n",
      "      4      \u001b[36m177.6867\u001b[0m       65.7271  0.0105\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m247.0682\u001b[0m       \u001b[32m68.4330\u001b[0m  0.0121\n",
      "      6      197.7521       68.2592  0.0150\n",
      "      2      252.7611       \u001b[32m85.5461\u001b[0m  0.0131\n",
      "      3      \u001b[36m193.2431\u001b[0m       71.0438  0.0115\n",
      "      5      179.3515       68.3597  0.0151\n",
      "      9      202.7358       70.9417  0.0152\n",
      "      2      \u001b[36m229.8320\u001b[0m       \u001b[32m98.3091\u001b[0m  0.0168\n",
      "      2      \u001b[36m242.7215\u001b[0m       \u001b[32m68.3797\u001b[0m  0.0145\n",
      "      5      \u001b[36m229.9370\u001b[0m       67.4254  0.0174\n",
      "      7      \u001b[36m194.4658\u001b[0m       73.5175  0.0137\n",
      "      3      \u001b[36m219.2460\u001b[0m       \u001b[32m78.4432\u001b[0m  0.0158\n",
      "      4      193.7763       70.9333  0.0124\n",
      "      3      \u001b[36m235.7831\u001b[0m       71.5755  0.0112\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "     10      205.5730       71.5191  0.0165\n",
      "Restoring best model from epoch 5.\n",
      "      3      234.9272       \u001b[32m96.0928\u001b[0m  0.0167\n",
      "      6      243.4158       68.1423  0.0176\n",
      "      5      \u001b[36m193.0391\u001b[0m       \u001b[32m68.8027\u001b[0m  0.0108\n",
      "      4      \u001b[36m211.0249\u001b[0m       \u001b[32m77.2068\u001b[0m  0.0128\n",
      "      6      \u001b[36m171.4828\u001b[0m       73.0021  0.0205\n",
      "      8      198.1216       73.5002  0.0183\n",
      "      4      \u001b[36m229.0329\u001b[0m       69.7479  0.0181\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m236.7507\u001b[0m       \u001b[32m77.8926\u001b[0m  0.0150\n",
      "      7      244.3611       67.6997  0.0147\n",
      "      7      174.6618       70.2935  0.0106\n",
      "      5      212.3822       77.5405  0.0159\n",
      "      4      \u001b[36m226.9162\u001b[0m       \u001b[32m92.8703\u001b[0m  0.0225\n",
      "      5      \u001b[36m223.8360\u001b[0m       \u001b[32m67.0283\u001b[0m  0.0103\n",
      "      6      \u001b[36m187.8913\u001b[0m       \u001b[32m67.7149\u001b[0m  0.0239\n",
      "      8      233.9975       67.2933  0.0130\n",
      "      8      172.8209       68.0056  0.0129\n",
      "      2      \u001b[36m227.3809\u001b[0m       \u001b[32m76.9487\u001b[0m  0.0201\n",
      "      6      224.5491       \u001b[32m65.3097\u001b[0m  0.0101\n",
      "      9      \u001b[36m192.9922\u001b[0m       72.4187  0.0283\n",
      "      6      212.8299       \u001b[32m76.9104\u001b[0m  0.0163\n",
      "      9      242.1611       66.6872  0.0123\n",
      "      9      \u001b[36m170.6002\u001b[0m       66.9965  0.0114\n",
      "      7      \u001b[36m184.8322\u001b[0m       68.8035  0.0178\n",
      "      7      \u001b[36m217.5621\u001b[0m       \u001b[32m64.7292\u001b[0m  0.0110\n",
      "      5      \u001b[36m226.2614\u001b[0m       \u001b[32m90.3246\u001b[0m  0.0227\n",
      "     10      198.7172       68.4868  0.0176\n",
      "Restoring best model from epoch 2.\n",
      "     10      234.9226       \u001b[32m65.8679\u001b[0m  0.0124\n",
      "      7      \u001b[36m204.9035\u001b[0m       \u001b[32m76.1003\u001b[0m  0.0161\n",
      "      3      227.5768       \u001b[32m76.9455\u001b[0m  0.0211\n",
      "      8      187.1283       70.2242  0.0103\n",
      "      8      223.1623       \u001b[32m64.6835\u001b[0m  0.0106\n",
      "     10      \u001b[36m168.7148\u001b[0m       68.6932  0.0146\n",
      "Restoring best model from epoch 2.\n",
      "      6      250.5844       \u001b[32m89.4402\u001b[0m  0.0173\n",
      "      9      \u001b[36m182.5682\u001b[0m       70.6296  0.0103\n",
      "      9      225.4435       64.9583  0.0107\n",
      "      4      \u001b[36m218.3194\u001b[0m       78.6284  0.0169\n",
      "      8      \u001b[36m200.1750\u001b[0m       \u001b[32m75.1420\u001b[0m  0.0160\n",
      "     10      \u001b[36m211.2401\u001b[0m       65.5741  0.0109\n",
      "Restoring best model from epoch 8.\n",
      "     10      184.2409       70.9467  0.0137\n",
      "Restoring best model from epoch 6.\n",
      "      9      \u001b[36m198.4683\u001b[0m       \u001b[32m74.7076\u001b[0m  0.0143\n",
      "      7      233.7008       \u001b[32m89.0855\u001b[0m  0.0237\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      5      \u001b[36m214.4402\u001b[0m       79.2490  0.0171\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "     10      201.7158       74.8735  0.0105\n",
      "Restoring best model from epoch 9.\n",
      "      6      228.5632       78.6957  0.0133\n",
      "      8      227.5576       \u001b[32m88.2561\u001b[0m  0.0214\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m238.3533\u001b[0m       \u001b[32m97.3584\u001b[0m  0.0228\n",
      "      7      \u001b[36m213.9608\u001b[0m       77.9058  0.0135\n",
      "      9      231.4581       \u001b[32m87.5633\u001b[0m  0.0139\n",
      "      2      244.2713       \u001b[32m87.1931\u001b[0m  0.0106\n",
      "      8      214.1538       77.4144  0.0123\n",
      "     10      226.8880       \u001b[32m86.5425\u001b[0m  0.0123\n",
      "      3      \u001b[36m218.9645\u001b[0m       \u001b[32m86.8222\u001b[0m  0.0102\n",
      "      9      \u001b[36m210.0713\u001b[0m       \u001b[32m76.6450\u001b[0m  0.0165\n",
      "      4      \u001b[36m215.9592\u001b[0m       87.3633  0.0121\n",
      "     10      217.4133       \u001b[32m75.9298\u001b[0m  0.0128\n",
      "      5      216.3653       87.6683  0.0106\n",
      "      6      \u001b[36m215.1937\u001b[0m       87.7268  0.0102\n",
      "      7      \u001b[36m213.9291\u001b[0m       88.6938  0.0104\n",
      "      8      \u001b[36m211.8776\u001b[0m       89.4676  0.0097\n",
      "      9      \u001b[36m210.2926\u001b[0m       89.9704  0.0096\n",
      "     10      \u001b[36m205.2562\u001b[0m       90.5413  0.0096\n",
      "Restoring best model from epoch 3.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m223.0069\u001b[0m       \u001b[32m99.0710\u001b[0m  0.0236\n",
      "      2      \u001b[36m221.3530\u001b[0m       \u001b[32m92.9089\u001b[0m  0.0237\n",
      "      3      \u001b[36m217.9957\u001b[0m       95.1760  0.0221\n",
      "      4      \u001b[36m217.4899\u001b[0m       93.2876  0.0218\n",
      "      5      \u001b[36m217.3242\u001b[0m       93.3903  0.0222\n",
      "      6      \u001b[36m209.8369\u001b[0m       93.9458  0.0224\n",
      "      7      216.4594       93.7781  0.0219\n",
      "      8      212.7904       \u001b[32m92.6093\u001b[0m  0.0221\n",
      "      9      213.2373       \u001b[32m90.7343\u001b[0m  0.0227\n",
      "     10      \u001b[36m209.5357\u001b[0m       \u001b[32m90.1884\u001b[0m  0.0224\n",
      "y_train breslow final [  -13.   -15.   -17.    19.   -20.    20.   -28.   -35.   -37.   -46.\n",
      "    56.    56.    57.   -59.    62.   -64.   -64.   -67.    69.    76.\n",
      "    81.   -82.   -84.    88.    92.    93.   -95.    98.    99.  -105.\n",
      "   106.  -110.  -117.   118.   122.   131.   142.   144.   146.   154.\n",
      "   154.  -158.   168.   168.   173.   182.  -187.  -189.   200.   200.\n",
      "   205.   211.   213.   216.   223.   232.   246.   248.   251.  -251.\n",
      "   253.   254.   254.   258.   259.   262.   272.   272.   272.   273.\n",
      "  -276.   278.  -293.   294.   294.   303.   311.   324.   328.  -330.\n",
      "  -333.  -337.   340.   344.  -345.  -345.  -359.   362.   364.  -364.\n",
      "  -365.  -368.   370.  -370.   370.  -372.  -372.  -376.  -379.  -383.\n",
      "  -384.  -384.   385.   385.   386.   388.  -389.   391.   393.  -394.\n",
      "  -398.  -398.  -399.   400.   408.  -410.   413.   413.   413.  -415.\n",
      "   415.  -415.  -416.  -423.  -425.  -428.  -433.   434.   437.   453.\n",
      "   454.  -455.   455.  -457.  -460.   460.  -466.  -467.  -469.   474.\n",
      "  -475.  -477.  -480.  -481.  -484.  -485.   486.  -491.   492.  -495.\n",
      "  -495.  -507.   508.  -508.   510.   510.   522.  -522.  -524.   530.\n",
      "  -536.   539.  -539.  -540.  -542.   544.  -546.   547.   547.   565.\n",
      "   565.  -572.  -572.  -573.   575.   577.  -578.  -578.   579.  -580.\n",
      "  -588.   590.  -590.   593.   599.   602.   612.   615.   617.  -618.\n",
      "   623.   630.  -633.  -638.  -640.  -641.  -642.  -646.  -648.  -649.\n",
      "  -665.   665.   674.   680.   685.   690.  -691.   696.  -699.   706.\n",
      "   712.  -731.   739.  -750.  -761.  -773.  -783.   795.  -798.  -799.\n",
      "  -812.   819.  -820.   823.  -832.   835.  -840.  -851.   859.  -864.\n",
      "  -873.  -897.  -899.   904.  -906.  -921.   941.  -945.   949. -1003.\n",
      "  1004.  1008.  1036. -1048.  1064. -1072.  1077. -1090. -1094. -1108.\n",
      " -1110. -1127.  1163. -1174. -1181. -1186. -1219.  1270. -1326.  1348.\n",
      " -1350.  1367. -1370.  1423. -1429. -1454. -1455. -1460. -1522. -1529.\n",
      " -1538. -1542.  1556. -1561. -1604. -1621. -1639. -1649.  1670. -1708.\n",
      " -1714. -1761. -1792.  1804. -1806. -1845.  1869. -1884. -1947. -1949.\n",
      " -1952.  1971. -2008. -2009.  2020. -2024. -2027. -2044. -2049. -2109.\n",
      " -2177. -2293. -2312. -2380. -2625.  2641. -2656. -2703. -2790.  2828.\n",
      " -2868. -2886.  2954. -2964. -3011.  3183. -3314. -3364. -3420. -3432.\n",
      " -3817. -3981. -4343. -4967. -5050.]\n",
      "Concordance Index 0.6399180717603599\n",
      "Integrated Brier Score: 0.2193498108738935\n",
      "y_train breslow final [  -13.   -15.   -17.    19.   -20.    20.   -28.   -35.   -37.   -46.\n",
      "    56.    56.    57.   -59.    62.   -64.   -64.   -67.    69.    76.\n",
      "    81.   -82.   -84.    88.    92.    93.   -95.    98.    99.  -105.\n",
      "   106.  -110.  -117.   118.   122.   131.   142.   144.   146.   154.\n",
      "   154.  -158.   168.   168.   173.   182.  -187.  -189.   200.   200.\n",
      "   205.   211.   213.   216.   223.   232.   246.   248.   251.  -251.\n",
      "   253.   254.   254.   258.   259.   262.   272.   272.   272.   273.\n",
      "  -276.   278.  -293.   294.   294.   303.   311.   324.   328.  -330.\n",
      "  -333.  -337.   340.   344.  -345.  -345.  -359.   362.   364.  -364.\n",
      "  -365.  -368.   370.  -370.   370.  -372.  -372.  -376.  -379.  -383.\n",
      "  -384.  -384.   385.   385.   386.   388.  -389.   391.   393.  -394.\n",
      "  -398.  -398.  -399.   400.   408.  -410.   413.   413.   413.  -415.\n",
      "   415.  -415.  -416.  -423.  -425.  -428.  -433.   434.   437.   453.\n",
      "   454.  -455.   455.  -457.  -460.   460.  -466.  -467.  -469.   474.\n",
      "  -475.  -477.  -480.  -481.  -484.  -485.   486.  -491.   492.  -495.\n",
      "  -495.  -507.   508.  -508.   510.   510.   522.  -522.  -524.   530.\n",
      "  -536.   539.  -539.  -540.  -542.   544.  -546.   547.   547.   565.\n",
      "   565.  -572.  -572.  -573.   575.   577.  -578.  -578.   579.  -580.\n",
      "  -588.   590.  -590.   593.   599.   602.   612.   615.   617.  -618.\n",
      "   623.   630.  -633.  -638.  -640.  -641.  -642.  -646.  -648.  -649.\n",
      "  -665.   665.   674.   680.   685.   690.  -691.   696.  -699.   706.\n",
      "   712.  -731.   739.  -750.  -761.  -773.  -783.   795.  -798.  -799.\n",
      "  -812.   819.  -820.   823.  -832.   835.  -840.  -851.   859.  -864.\n",
      "  -873.  -897.  -899.   904.  -906.  -921.   941.  -945.   949. -1003.\n",
      "  1004.  1008.  1036. -1048.  1064. -1072.  1077. -1090. -1094. -1108.\n",
      " -1110. -1127.  1163. -1174. -1181. -1186. -1219.  1270. -1326.  1348.\n",
      " -1350.  1367. -1370.  1423. -1429. -1454. -1455. -1460. -1522. -1529.\n",
      " -1538. -1542.  1556. -1561. -1604. -1621. -1639. -1649.  1670. -1708.\n",
      " -1714. -1761. -1792.  1804. -1806. -1845.  1869. -1884. -1947. -1949.\n",
      " -1952.  1971. -2008. -2009.  2020. -2024. -2027. -2044. -2049. -2109.\n",
      " -2177. -2293. -2312. -2380. -2625.  2641. -2656. -2703. -2790.  2828.\n",
      " -2868. -2886.  2954. -2964. -3011.  3183. -3314. -3364. -3420. -3432.\n",
      " -3817. -3981. -4343. -4967. -5050.]\n",
      "durations 55.0 5041.0\n",
      "Concordance Index 0.5933562428407789\n",
      "Integrated Brier Score: 0.23337334933817805\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m227.2924\u001b[0m       \u001b[32m64.0785\u001b[0m  0.0145\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m234.2950\u001b[0m       \u001b[32m65.2424\u001b[0m  0.0144\n",
      "      2      \u001b[36m209.5124\u001b[0m       67.9114  0.0230\n",
      "      2      235.6110       \u001b[32m64.0626\u001b[0m  0.0202\n",
      "      3      215.6873       65.9818  0.0158\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      3      \u001b[36m227.3258\u001b[0m       \u001b[32m63.5238\u001b[0m  0.0136\n",
      "      4      \u001b[36m206.0458\u001b[0m       65.9197  0.0136\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      4      \u001b[36m227.0305\u001b[0m       63.6958  0.0127\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m254.1511\u001b[0m       \u001b[32m69.5032\u001b[0m  0.0136\n",
      "      5      \u001b[36m196.9558\u001b[0m       65.5453  0.0127\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m283.7505\u001b[0m       \u001b[32m78.8192\u001b[0m  0.0136\n",
      "      5      \u001b[36m226.0998\u001b[0m       64.8230  0.0123\n",
      "      2      255.5649       \u001b[32m69.2695\u001b[0m  0.0140\n",
      "      6      207.7786       \u001b[32m63.0717\u001b[0m  0.0124\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m198.8544\u001b[0m       \u001b[32m52.5589\u001b[0m  0.0118\n",
      "      6      \u001b[36m213.2150\u001b[0m       66.0391  0.0121\n",
      "      2      \u001b[36m272.0072\u001b[0m       \u001b[32m78.1999\u001b[0m  0.0135\n",
      "      3      \u001b[36m240.8121\u001b[0m       69.8153  0.0140\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2      \u001b[36m187.7224\u001b[0m       \u001b[32m50.3872\u001b[0m  0.0110\n",
      "      7      204.2811       \u001b[32m62.1763\u001b[0m  0.0180\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      3      279.3638       78.9254  0.0146\n",
      "      7      220.0541       67.3247  0.0169\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m252.2069\u001b[0m       \u001b[32m75.5371\u001b[0m  0.0105\n",
      "      3      \u001b[36m177.7287\u001b[0m       50.5232  0.0103\n",
      "      4      243.3461       71.7139  0.0175\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      8      201.5279       \u001b[32m61.9080\u001b[0m  0.0121\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m218.4444\u001b[0m       \u001b[32m94.5992\u001b[0m  0.0103\n",
      "      8      225.0174       67.9591  0.0122\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      4      \u001b[36m167.0780\u001b[0m       50.5127  0.0095\n",
      "      2      \u001b[36m249.8436\u001b[0m       \u001b[32m73.9458\u001b[0m  0.0106\n",
      "      4      273.9712       78.7188  0.0193\n",
      "      5      244.7980       73.1343  0.0124\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m234.3487\u001b[0m       \u001b[32m72.1660\u001b[0m  0.0109\n",
      "      2      224.0846       \u001b[32m78.7541\u001b[0m  0.0103\n",
      "      9      198.6216       62.3501  0.0119\n",
      "      5      173.3800       52.0483  0.0095\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m275.9956\u001b[0m       \u001b[32m96.5897\u001b[0m  0.0109\n",
      "      9      225.0343       68.7742  0.0155\n",
      "      3      \u001b[36m230.9093\u001b[0m       75.7828  0.0160\n",
      "      5      273.0980       78.7486  0.0136\n",
      "      6      167.7352       53.6352  0.0095\n",
      "     10      \u001b[36m196.6019\u001b[0m       62.9455  0.0142\n",
      "      2      239.0318       \u001b[32m66.5373\u001b[0m  0.0148\n",
      "Restoring best model from epoch 8.\n",
      "      3      \u001b[36m197.7453\u001b[0m       \u001b[32m68.5206\u001b[0m  0.0143\n",
      "      2      296.2927       \u001b[32m84.1430\u001b[0m  0.0110\n",
      "      6      \u001b[36m233.1224\u001b[0m       73.8994  0.0190\n",
      "      4      231.6712       76.8363  0.0108\n",
      "     10      223.7858       69.3683  0.0162\n",
      "      3      \u001b[36m214.6740\u001b[0m       69.9754  0.0101\n",
      "Restoring best model from epoch 3.\n",
      "      7      \u001b[36m162.9219\u001b[0m       53.6596  0.0142\n",
      "      3      \u001b[36m264.8821\u001b[0m       \u001b[32m81.2812\u001b[0m  0.0104\n",
      "      4      \u001b[36m189.6774\u001b[0m       \u001b[32m66.0612\u001b[0m  0.0147\n",
      "      6      273.1656       78.8959  0.0197\n",
      "      7      246.9430       74.1446  0.0151\n",
      "      5      \u001b[36m225.0080\u001b[0m       76.2593  0.0120\n",
      "      8      168.4986       52.9175  0.0107\n",
      "      4      219.2338       71.8435  0.0161\n",
      "      4      \u001b[36m259.3618\u001b[0m       81.6696  0.0133\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      5      197.1380       \u001b[32m65.7276\u001b[0m  0.0120\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      7      272.0175       78.9044  0.0132\n",
      "      6      \u001b[36m217.9722\u001b[0m       76.2873  0.0115\n",
      "      8      237.3436       74.1451  0.0139\n",
      "      5      \u001b[36m209.4981\u001b[0m       68.9196  0.0123\n",
      "      9      167.3729       53.3139  0.0147\n",
      "      7      \u001b[36m217.4576\u001b[0m       76.8267  0.0109\n",
      "      8      \u001b[36m271.1576\u001b[0m       79.2142  0.0134\n",
      "      6      190.0882       67.7410  0.0186\n",
      "      5      259.6994       \u001b[32m80.3824\u001b[0m  0.0202\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m202.7811\u001b[0m       \u001b[32m51.4564\u001b[0m  0.0148\n",
      "      9      245.6639       73.8156  0.0194\n",
      "     10      \u001b[36m161.1616\u001b[0m       54.2839  0.0136\n",
      "Restoring best model from epoch 2.\n",
      "      8      \u001b[36m213.9455\u001b[0m       78.2302  0.0122\n",
      "      6      \u001b[36m202.9979\u001b[0m       \u001b[32m65.9161\u001b[0m  0.0171\n",
      "      7      \u001b[36m184.5701\u001b[0m       70.4247  0.0128\n",
      "      9      278.3213       78.7935  0.0181\n",
      "      9      215.7189       81.9425  0.0102\n",
      "      6      \u001b[36m256.2349\u001b[0m       \u001b[32m79.1988\u001b[0m  0.0186\n",
      "      2      \u001b[36m193.9277\u001b[0m       \u001b[32m50.0637\u001b[0m  0.0189\n",
      "      7      \u001b[36m200.7440\u001b[0m       \u001b[32m64.6477\u001b[0m  0.0119\n",
      "     10      245.9681       73.7836  0.0173\n",
      "Restoring best model from epoch 2.\n",
      "      8      \u001b[36m180.2247\u001b[0m       72.4341  0.0121\n",
      "     10      \u001b[36m209.9799\u001b[0m       84.4194  0.0102\n",
      "Restoring best model from epoch 2.\n",
      "     10      273.2505       78.4552  0.0161\n",
      "Restoring best model from epoch 2.\n",
      "      7      258.9499       79.4064  0.0116\n",
      "      3      195.1466       \u001b[32m49.5401\u001b[0m  0.0122\n",
      "      8      202.1563       65.4366  0.0186\n",
      "      9      182.4120       73.1615  0.0152\n",
      "      8      \u001b[36m254.7922\u001b[0m       79.4530  0.0146\n",
      "      4      \u001b[36m187.6027\u001b[0m       50.1132  0.0130\n",
      "      9      \u001b[36m197.4368\u001b[0m       67.5465  0.0101\n",
      "     10      \u001b[36m176.2288\u001b[0m       72.8067  0.0102\n",
      "Restoring best model from epoch 5.\n",
      "      9      256.0214       79.5696  0.0113\n",
      "      5      198.7408       50.9699  0.0133\n",
      "     10      \u001b[36m195.9086\u001b[0m       70.5135  0.0099\n",
      "Restoring best model from epoch 7.\n",
      "     10      \u001b[36m246.9175\u001b[0m       79.7904  0.0115\n",
      "Restoring best model from epoch 6.\n",
      "      6      \u001b[36m185.7065\u001b[0m       52.0578  0.0128\n",
      "      7      193.7479       52.8628  0.0140\n",
      "      8      187.2467       53.4657  0.0146\n",
      "      9      191.1895       53.7663  0.0193\n",
      "     10      186.1516       53.3571  0.0165\n",
      "Restoring best model from epoch 3.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m222.3093\u001b[0m      \u001b[32m104.1144\u001b[0m  0.0230\n",
      "      2      \u001b[36m214.3600\u001b[0m      \u001b[32m103.4435\u001b[0m  0.0222\n",
      "      3      220.0378      \u001b[32m102.5309\u001b[0m  0.0224\n",
      "      4      \u001b[36m214.2429\u001b[0m      \u001b[32m101.2903\u001b[0m  0.0222\n",
      "      5      223.0340      \u001b[32m100.3796\u001b[0m  0.0246\n",
      "      6      214.4726       \u001b[32m99.5709\u001b[0m  0.0260\n",
      "      7      214.5019       \u001b[32m98.5907\u001b[0m  0.0230\n",
      "      8      \u001b[36m211.3374\u001b[0m       \u001b[32m98.2990\u001b[0m  0.0224\n",
      "      9      214.5025       \u001b[32m98.2366\u001b[0m  0.0219\n",
      "     10      \u001b[36m208.5550\u001b[0m       \u001b[32m97.9202\u001b[0m  0.0227\n",
      "y_train breslow final [  -15.   -17.    20.   -28.   -37.   -55.    56.    57.   -59.    62.\n",
      "   -64.    65.   -67.   -68.    68.    69.    76.    81.   -84.    88.\n",
      "   -89.    90.    92.    93.   -95.    98.    99.   106.  -117.   122.\n",
      "   128.  -129.   131.   142.   144.   146.   149.   154.   154.   163.\n",
      "  -163.   168.   173.   182.  -187.  -189.   191.   200.   200.   205.\n",
      "   206.   211.   213.   216.   220.   223.  -224.   232.   232.   237.\n",
      "   246.   248.   250.   251.  -251.   253.   254.   258.   261.   262.\n",
      "   272.   272.   274.  -276.   278.  -293.   294.   303.   311.   321.\n",
      "   324.   324.   328.   332.  -333.  -337.   340.   344.   344.  -345.\n",
      "  -345.   356.  -359.  -361.   362.   364.  -364.  -365.  -366.  -368.\n",
      "  -368.  -369.  -370.   370.  -372.  -372.  -373.  -376.  -377.  -379.\n",
      "  -382.  -383.  -384.  -384.   385.   385.   388.  -389.   391.   393.\n",
      "  -394.  -398.  -399.   400.   406.   408.  -410.   413.   413.   413.\n",
      "  -415.   415.   418.  -423.  -425.  -428.  -428.  -433.   434.   437.\n",
      "   453.   454.  -455.   455.  -457.  -460.   460.  -466.  -466.   467.\n",
      "   467.  -469.   474.  -474.  -477.  -480.  -481.  -484.  -485.   486.\n",
      "  -491.   492.   495.  -495.  -503.  -507.  -508.   510.   510.  -512.\n",
      "   522.  -522.  -524.   530.  -536.   536.  -536.   539.  -539.  -540.\n",
      "  -542.   544.   544.   547.   547.   550.  -560.  -562.   565.  -572.\n",
      "  -572.  -573.   575.  -578.  -578.   579.  -580.  -581.  -582.  -588.\n",
      "  -590.   593.   602.  -603.  -610.   612.   615.   617.  -618.  -633.\n",
      "  -636.  -638.  -640.  -646.  -648.   651.   674.   690.  -691.  -699.\n",
      "  -700.   712.   719.   734.   739.  -758.  -761.  -773.   778.  -783.\n",
      "  -789.  -798.  -799.  -813.  -820.  -832.  -832.   835.  -840.  -842.\n",
      "  -851.   859.   864.  -864.  -873.  -887.  -893.  -897.  -899.   904.\n",
      "  -921.  -945.   949.   974.  -997. -1003.  1004.  1005. -1029.  1064.\n",
      " -1072.  1077. -1090. -1094. -1108. -1115.  1163. -1174. -1181. -1186.\n",
      " -1219. -1326. -1350.  1367. -1370.  1420.  1423. -1455. -1460. -1522.\n",
      " -1529. -1542.  1556. -1561. -1582. -1604. -1639.  1670. -1714.  1718.\n",
      " -1761. -1792. -1806. -1830. -1912. -1947. -1949. -1952.  1971. -2008.\n",
      " -2009.  2020. -2020. -2049. -2109. -2139. -2293. -2312. -2330. -2380.\n",
      " -2423. -2625. -2656. -2703. -2790. -2886.  2954. -3314. -3420. -3817.\n",
      " -3981. -4343. -4967. -5041. -5050.]\n",
      "Concordance Index 0.6532093285422391\n",
      "Integrated Brier Score: 0.19490068600296948\n",
      "y_train breslow final [  -15.   -17.    20.   -28.   -37.   -55.    56.    57.   -59.    62.\n",
      "   -64.    65.   -67.   -68.    68.    69.    76.    81.   -84.    88.\n",
      "   -89.    90.    92.    93.   -95.    98.    99.   106.  -117.   122.\n",
      "   128.  -129.   131.   142.   144.   146.   149.   154.   154.   163.\n",
      "  -163.   168.   173.   182.  -187.  -189.   191.   200.   200.   205.\n",
      "   206.   211.   213.   216.   220.   223.  -224.   232.   232.   237.\n",
      "   246.   248.   250.   251.  -251.   253.   254.   258.   261.   262.\n",
      "   272.   272.   274.  -276.   278.  -293.   294.   303.   311.   321.\n",
      "   324.   324.   328.   332.  -333.  -337.   340.   344.   344.  -345.\n",
      "  -345.   356.  -359.  -361.   362.   364.  -364.  -365.  -366.  -368.\n",
      "  -368.  -369.  -370.   370.  -372.  -372.  -373.  -376.  -377.  -379.\n",
      "  -382.  -383.  -384.  -384.   385.   385.   388.  -389.   391.   393.\n",
      "  -394.  -398.  -399.   400.   406.   408.  -410.   413.   413.   413.\n",
      "  -415.   415.   418.  -423.  -425.  -428.  -428.  -433.   434.   437.\n",
      "   453.   454.  -455.   455.  -457.  -460.   460.  -466.  -466.   467.\n",
      "   467.  -469.   474.  -474.  -477.  -480.  -481.  -484.  -485.   486.\n",
      "  -491.   492.   495.  -495.  -503.  -507.  -508.   510.   510.  -512.\n",
      "   522.  -522.  -524.   530.  -536.   536.  -536.   539.  -539.  -540.\n",
      "  -542.   544.   544.   547.   547.   550.  -560.  -562.   565.  -572.\n",
      "  -572.  -573.   575.  -578.  -578.   579.  -580.  -581.  -582.  -588.\n",
      "  -590.   593.   602.  -603.  -610.   612.   615.   617.  -618.  -633.\n",
      "  -636.  -638.  -640.  -646.  -648.   651.   674.   690.  -691.  -699.\n",
      "  -700.   712.   719.   734.   739.  -758.  -761.  -773.   778.  -783.\n",
      "  -789.  -798.  -799.  -813.  -820.  -832.  -832.   835.  -840.  -842.\n",
      "  -851.   859.   864.  -864.  -873.  -887.  -893.  -897.  -899.   904.\n",
      "  -921.  -945.   949.   974.  -997. -1003.  1004.  1005. -1029.  1064.\n",
      " -1072.  1077. -1090. -1094. -1108. -1115.  1163. -1174. -1181. -1186.\n",
      " -1219. -1326. -1350.  1367. -1370.  1420.  1423. -1455. -1460. -1522.\n",
      " -1529. -1542.  1556. -1561. -1582. -1604. -1639.  1670. -1714.  1718.\n",
      " -1761. -1792. -1806. -1830. -1912. -1947. -1949. -1952.  1971. -2008.\n",
      " -2009.  2020. -2020. -2049. -2109. -2139. -2293. -2312. -2330. -2380.\n",
      " -2423. -2625. -2656. -2703. -2790. -2886.  2954. -3314. -3420. -3817.\n",
      " -3981. -4343. -4967. -5041. -5050.]\n",
      "durations 13.0 3432.0\n",
      "Concordance Index 0.6095669036845508\n",
      "Integrated Brier Score: 0.22184698686879128\n",
      "split gex_?|100130426      float32\n",
      "gex_?|100133144      float32\n",
      "gex_?|100134869      float32\n",
      "gex_?|10357          float32\n",
      "gex_?|10431          float32\n",
      "                      ...   \n",
      "gex_ZYG11A|440590    float32\n",
      "gex_ZYG11B|79699     float32\n",
      "gex_ZYX|7791         float32\n",
      "gex_ZZEF1|23140      float32\n",
      "gex_ZZZ3|26009       float32\n",
      "Length: 20531, dtype: object\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m63.0155\u001b[0m       \u001b[32m53.6488\u001b[0m  0.0450\n",
      "      2       \u001b[36m61.9621\u001b[0m       54.9567  0.0464\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      3       \u001b[36m59.8262\u001b[0m       60.1849  0.0402\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m67.6275\u001b[0m       \u001b[32m70.0543\u001b[0m  0.0329\n",
      "      4       61.3063       62.9753  0.0396\n",
      "      2       \u001b[36m56.7903\u001b[0m       \u001b[32m58.2581\u001b[0m  0.0299\n",
      "      3       57.1453       58.3088  0.0318\n",
      "      5       61.2701       62.1428  0.0397\n",
      "      4       \u001b[36m55.2877\u001b[0m       63.0588  0.0283\n",
      "      6       60.2064       58.0671  0.0438\n",
      "      5       55.8821       65.0081  0.0299\n",
      "      6       \u001b[36m52.9933\u001b[0m       63.7804  0.0295\n",
      "      7       \u001b[36m59.3529\u001b[0m       56.2883  0.0427\n",
      "      7       \u001b[36m51.4908\u001b[0m       64.2878  0.0290\n",
      "      8       61.8828       56.7359  0.0403\n",
      "      8       \u001b[36m50.6016\u001b[0m       65.7077  0.0286\n",
      "      9       \u001b[36m58.3665\u001b[0m       57.5371  0.0391\n",
      "      9       \u001b[36m49.2440\u001b[0m       70.1172  0.0297\n",
      "     10       49.4555       71.3078  0.0284\n",
      "Restoring best model from epoch 2.\n",
      "     10       60.3766       57.8328  0.0394\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m68.4527\u001b[0m       \u001b[32m69.3845\u001b[0m  0.0519\n",
      "      2       \u001b[36m67.8798\u001b[0m       \u001b[32m65.1105\u001b[0m  0.0484\n",
      "      3       68.5889       \u001b[32m63.5930\u001b[0m  0.0503\n",
      "      4       \u001b[36m67.1854\u001b[0m       \u001b[32m62.8195\u001b[0m  0.0482\n",
      "      5       \u001b[36m66.3549\u001b[0m       63.0109  0.0480\n",
      "      6       \u001b[36m66.2851\u001b[0m       64.9717  0.0484\n",
      "      7       \u001b[36m65.1370\u001b[0m       65.0848  0.0468\n",
      "      8       65.1989       65.4174  0.0463\n",
      "      9       \u001b[36m64.3750\u001b[0m       64.9655  0.0446\n",
      "     10       64.4051       64.9642  0.0491\n",
      "Restoring best model from epoch 4.\n",
      "y_train breslow final [-1.000e+00 -1.000e+00 -5.000e+00 -5.000e+00 -7.000e+00 -9.000e+00\n",
      " -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01\n",
      " -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01\n",
      " -1.100e+01 -1.600e+01 -1.900e+01 -2.100e+01 -2.400e+01 -2.600e+01\n",
      " -3.000e+01 -3.000e+01 -3.000e+01 -3.000e+01 -3.100e+01 -3.100e+01\n",
      " -3.100e+01 -3.100e+01 -3.400e+01 -3.400e+01 -4.900e+01 -5.100e+01\n",
      " -5.200e+01 -5.900e+01 -6.400e+01 -7.000e+01 -7.200e+01 -7.600e+01\n",
      " -7.800e+01 -7.800e+01 -7.900e+01 -8.000e+01 -9.000e+01 -9.200e+01\n",
      " -9.800e+01 -1.180e+02 -1.340e+02 -1.490e+02  1.580e+02 -1.600e+02\n",
      "  1.600e+02 -1.620e+02 -1.630e+02  1.720e+02 -1.720e+02  1.740e+02\n",
      " -1.780e+02 -1.870e+02 -1.960e+02  1.970e+02 -2.000e+02 -2.100e+02\n",
      " -2.130e+02 -2.150e+02 -2.170e+02 -2.180e+02 -2.220e+02 -2.240e+02\n",
      "  2.240e+02 -2.250e+02  2.270e+02 -2.270e+02 -2.310e+02 -2.420e+02\n",
      " -2.430e+02 -2.500e+02  2.550e+02 -2.580e+02 -2.590e+02 -2.660e+02\n",
      "  2.660e+02 -2.710e+02 -2.730e+02 -2.750e+02 -2.850e+02 -2.870e+02\n",
      " -2.880e+02 -2.930e+02  2.950e+02 -2.970e+02 -3.000e+02 -3.020e+02\n",
      "  3.020e+02 -3.030e+02 -3.030e+02 -3.040e+02 -3.040e+02 -3.040e+02\n",
      " -3.040e+02 -3.070e+02 -3.130e+02 -3.170e+02 -3.170e+02  3.200e+02\n",
      " -3.200e+02  3.220e+02 -3.220e+02 -3.230e+02 -3.260e+02 -3.280e+02\n",
      " -3.320e+02 -3.340e+02  3.360e+02 -3.360e+02 -3.370e+02 -3.390e+02\n",
      " -3.430e+02 -3.430e+02 -3.450e+02 -3.450e+02  3.480e+02 -3.480e+02\n",
      " -3.500e+02 -3.580e+02 -3.580e+02 -3.600e+02  3.620e+02 -3.630e+02\n",
      " -3.640e+02 -3.650e+02 -3.650e+02 -3.650e+02  3.650e+02 -3.650e+02\n",
      " -3.660e+02 -3.680e+02 -3.700e+02 -3.710e+02 -3.710e+02 -3.730e+02\n",
      " -3.730e+02 -3.750e+02 -3.750e+02 -3.750e+02 -3.750e+02 -3.760e+02\n",
      " -3.760e+02 -3.790e+02 -3.800e+02 -3.800e+02 -3.820e+02 -3.830e+02\n",
      " -3.830e+02  3.850e+02 -3.850e+02  3.850e+02 -3.850e+02 -3.850e+02\n",
      " -3.930e+02 -3.930e+02 -3.940e+02 -3.940e+02 -3.950e+02 -3.960e+02\n",
      " -3.960e+02 -3.960e+02 -3.970e+02 -3.980e+02 -4.030e+02 -4.030e+02\n",
      " -4.040e+02 -4.050e+02 -4.080e+02 -4.080e+02 -4.110e+02 -4.120e+02\n",
      " -4.130e+02 -4.140e+02 -4.160e+02 -4.180e+02 -4.210e+02 -4.240e+02\n",
      " -4.250e+02 -4.260e+02  4.260e+02 -4.260e+02 -4.280e+02 -4.280e+02\n",
      " -4.310e+02 -4.310e+02 -4.330e+02 -4.340e+02 -4.350e+02 -4.370e+02\n",
      " -4.390e+02 -4.390e+02 -4.390e+02 -4.400e+02 -4.410e+02 -4.410e+02\n",
      " -4.410e+02 -4.460e+02  4.460e+02 -4.470e+02 -4.470e+02 -4.470e+02\n",
      " -4.480e+02 -4.480e+02 -4.500e+02 -4.510e+02 -4.540e+02 -4.550e+02\n",
      " -4.560e+02 -4.570e+02 -4.580e+02 -4.610e+02 -4.630e+02 -4.660e+02\n",
      " -4.670e+02 -4.700e+02 -4.710e+02 -4.720e+02 -4.750e+02 -4.760e+02\n",
      " -4.770e+02 -4.770e+02 -4.770e+02 -4.800e+02 -4.820e+02 -4.880e+02\n",
      " -4.880e+02 -4.890e+02 -4.920e+02 -4.950e+02 -4.960e+02 -4.960e+02\n",
      " -4.990e+02 -4.990e+02 -5.010e+02 -5.010e+02 -5.020e+02 -5.040e+02\n",
      " -5.060e+02 -5.080e+02 -5.090e+02 -5.100e+02 -5.130e+02 -5.160e+02\n",
      " -5.180e+02 -5.180e+02 -5.180e+02 -5.190e+02 -5.190e+02 -5.210e+02\n",
      " -5.220e+02 -5.230e+02  5.240e+02 -5.260e+02 -5.280e+02 -5.280e+02\n",
      " -5.290e+02 -5.300e+02 -5.320e+02 -5.320e+02 -5.330e+02 -5.380e+02\n",
      " -5.380e+02  5.380e+02 -5.470e+02  5.480e+02 -5.490e+02 -5.510e+02\n",
      " -5.520e+02 -5.520e+02 -5.540e+02 -5.540e+02  5.580e+02 -5.620e+02\n",
      " -5.630e+02  5.630e+02 -5.650e+02 -5.670e+02 -5.680e+02  5.710e+02\n",
      " -5.720e+02  5.730e+02 -5.740e+02 -5.750e+02 -5.750e+02 -5.760e+02\n",
      " -5.770e+02 -5.790e+02 -5.790e+02 -5.840e+02  5.840e+02 -5.840e+02\n",
      " -5.850e+02 -5.860e+02 -5.880e+02 -5.880e+02 -5.900e+02 -5.910e+02\n",
      " -5.940e+02 -5.950e+02 -5.950e+02 -5.960e+02 -5.980e+02 -6.020e+02\n",
      " -6.060e+02 -6.070e+02 -6.080e+02 -6.110e+02 -6.110e+02 -6.120e+02\n",
      "  6.120e+02 -6.120e+02 -6.120e+02  6.140e+02  6.160e+02 -6.160e+02\n",
      " -6.180e+02 -6.200e+02 -6.200e+02 -6.200e+02 -6.220e+02 -6.220e+02\n",
      " -6.240e+02 -6.260e+02 -6.260e+02 -6.290e+02 -6.300e+02 -6.310e+02\n",
      " -6.350e+02 -6.350e+02 -6.390e+02  6.390e+02 -6.400e+02 -6.430e+02\n",
      " -6.430e+02 -6.440e+02 -6.480e+02 -6.520e+02 -6.580e+02 -6.590e+02\n",
      " -6.590e+02 -6.600e+02 -6.610e+02 -6.620e+02 -6.640e+02 -6.720e+02\n",
      " -6.750e+02 -6.770e+02  6.780e+02 -6.790e+02 -6.800e+02 -6.810e+02\n",
      " -6.940e+02 -6.980e+02 -7.010e+02 -7.020e+02 -7.060e+02 -7.070e+02\n",
      " -7.070e+02 -7.090e+02 -7.100e+02 -7.140e+02 -7.140e+02 -7.150e+02\n",
      " -7.150e+02  7.230e+02 -7.240e+02 -7.250e+02 -7.260e+02 -7.270e+02\n",
      " -7.270e+02 -7.270e+02 -7.280e+02 -7.380e+02 -7.420e+02 -7.450e+02\n",
      " -7.470e+02 -7.470e+02 -7.520e+02 -7.520e+02  7.540e+02 -7.540e+02\n",
      " -7.540e+02 -7.550e+02 -7.590e+02 -7.590e+02 -7.600e+02 -7.600e+02\n",
      " -7.610e+02 -7.620e+02 -7.620e+02 -7.620e+02 -7.650e+02 -7.670e+02\n",
      " -7.690e+02 -7.770e+02 -7.830e+02 -7.850e+02  7.850e+02  7.860e+02\n",
      " -7.890e+02 -7.910e+02  7.920e+02  7.920e+02 -7.920e+02 -8.030e+02\n",
      " -8.060e+02  8.110e+02 -8.120e+02 -8.190e+02  8.210e+02 -8.220e+02\n",
      "  8.250e+02 -8.290e+02 -8.370e+02 -8.390e+02 -8.470e+02 -8.490e+02\n",
      " -8.520e+02 -8.580e+02  8.600e+02 -8.650e+02 -8.670e+02 -8.750e+02\n",
      " -8.780e+02 -8.810e+02  8.830e+02 -8.830e+02 -8.890e+02 -8.900e+02\n",
      "  9.040e+02 -9.060e+02 -9.070e+02 -9.110e+02  9.120e+02 -9.120e+02\n",
      " -9.120e+02 -9.150e+02  9.210e+02 -9.230e+02 -9.260e+02 -9.310e+02\n",
      " -9.310e+02 -9.420e+02 -9.430e+02  9.430e+02 -9.430e+02 -9.520e+02\n",
      " -9.540e+02 -9.650e+02 -9.650e+02 -9.680e+02 -9.720e+02 -9.730e+02\n",
      " -9.740e+02 -9.740e+02 -9.750e+02 -9.750e+02  9.760e+02 -9.890e+02\n",
      "  9.910e+02  9.910e+02 -9.960e+02 -9.960e+02 -1.000e+03 -1.001e+03\n",
      " -1.001e+03 -1.001e+03  1.004e+03 -1.004e+03 -1.004e+03 -1.005e+03\n",
      " -1.006e+03 -1.007e+03  1.009e+03 -1.009e+03 -1.015e+03 -1.025e+03\n",
      " -1.026e+03 -1.026e+03 -1.027e+03  1.032e+03  1.034e+03  1.034e+03\n",
      " -1.035e+03 -1.039e+03 -1.043e+03 -1.043e+03 -1.047e+03  1.048e+03\n",
      " -1.051e+03 -1.051e+03 -1.059e+03 -1.062e+03 -1.063e+03 -1.066e+03\n",
      "  1.072e+03 -1.074e+03 -1.079e+03 -1.085e+03 -1.088e+03 -1.093e+03\n",
      "  1.093e+03 -1.101e+03  1.104e+03 -1.106e+03 -1.120e+03 -1.121e+03\n",
      " -1.124e+03  1.127e+03 -1.132e+03 -1.133e+03 -1.138e+03 -1.139e+03\n",
      " -1.140e+03 -1.141e+03  1.142e+03 -1.148e+03 -1.150e+03 -1.156e+03\n",
      " -1.156e+03 -1.157e+03 -1.161e+03 -1.162e+03 -1.163e+03 -1.165e+03\n",
      " -1.167e+03 -1.174e+03 -1.179e+03 -1.185e+03 -1.186e+03 -1.189e+03\n",
      " -1.191e+03 -1.196e+03 -1.198e+03 -1.203e+03 -1.206e+03 -1.208e+03\n",
      " -1.210e+03 -1.217e+03 -1.220e+03 -1.220e+03 -1.229e+03 -1.229e+03\n",
      " -1.232e+03 -1.239e+03 -1.246e+03 -1.247e+03 -1.248e+03 -1.251e+03\n",
      " -1.269e+03  1.275e+03 -1.277e+03 -1.285e+03  1.286e+03 -1.288e+03\n",
      " -1.291e+03 -1.301e+03 -1.308e+03 -1.308e+03 -1.308e+03 -1.309e+03\n",
      " -1.309e+03 -1.318e+03 -1.321e+03  1.324e+03 -1.326e+03 -1.330e+03\n",
      " -1.347e+03 -1.351e+03 -1.359e+03 -1.363e+03  1.365e+03 -1.371e+03\n",
      " -1.371e+03  1.388e+03 -1.417e+03 -1.417e+03 -1.419e+03 -1.434e+03\n",
      " -1.437e+03  1.439e+03 -1.448e+03 -1.461e+03 -1.463e+03 -1.471e+03\n",
      " -1.474e+03 -1.474e+03 -1.476e+03 -1.477e+03 -1.492e+03  1.508e+03\n",
      " -1.519e+03 -1.522e+03 -1.523e+03 -1.528e+03 -1.532e+03 -1.534e+03\n",
      " -1.542e+03 -1.545e+03 -1.545e+03 -1.546e+03 -1.547e+03 -1.548e+03\n",
      " -1.550e+03  1.556e+03 -1.561e+03 -1.563e+03 -1.569e+03 -1.572e+03\n",
      " -1.587e+03 -1.596e+03 -1.604e+03 -1.604e+03 -1.611e+03 -1.611e+03\n",
      " -1.611e+03 -1.612e+03 -1.613e+03 -1.616e+03 -1.620e+03 -1.631e+03\n",
      " -1.639e+03 -1.640e+03  1.642e+03 -1.642e+03 -1.644e+03 -1.648e+03\n",
      "  1.649e+03 -1.662e+03 -1.673e+03 -1.673e+03  1.673e+03 -1.682e+03\n",
      " -1.683e+03 -1.686e+03 -1.688e+03  1.692e+03 -1.692e+03  1.699e+03\n",
      " -1.712e+03 -1.728e+03 -1.728e+03 -1.732e+03 -1.742e+03  1.759e+03\n",
      " -1.780e+03  1.781e+03 -1.783e+03 -1.800e+03  1.812e+03 -1.820e+03\n",
      " -1.836e+03 -1.842e+03 -1.847e+03 -1.853e+03 -1.855e+03 -1.864e+03\n",
      " -1.866e+03 -1.871e+03 -1.873e+03 -1.876e+03 -1.882e+03 -1.887e+03\n",
      "  1.900e+03 -1.914e+03 -1.919e+03 -1.925e+03 -1.928e+03 -1.935e+03\n",
      " -1.953e+03 -1.980e+03  1.993e+03 -1.996e+03  2.009e+03 -2.012e+03\n",
      " -2.019e+03 -2.031e+03 -2.033e+03 -2.041e+03 -2.064e+03 -2.072e+03\n",
      " -2.082e+03 -2.091e+03 -2.108e+03 -2.109e+03 -2.124e+03  2.127e+03\n",
      " -2.128e+03 -2.136e+03 -2.140e+03 -2.142e+03 -2.155e+03 -2.156e+03\n",
      " -2.161e+03 -2.184e+03 -2.190e+03 -2.190e+03 -2.191e+03  2.192e+03\n",
      " -2.193e+03 -2.197e+03  2.207e+03 -2.222e+03 -2.231e+03 -2.240e+03\n",
      " -2.246e+03 -2.248e+03 -2.255e+03 -2.263e+03 -2.278e+03 -2.288e+03\n",
      " -2.289e+03  2.296e+03 -2.306e+03 -2.311e+03 -2.329e+03 -2.330e+03\n",
      " -2.335e+03  2.348e+03  2.361e+03 -2.362e+03 -2.365e+03 -2.371e+03\n",
      " -2.372e+03 -2.385e+03 -2.403e+03 -2.426e+03 -2.442e+03  2.469e+03\n",
      " -2.477e+03  2.483e+03 -2.513e+03 -2.515e+03  2.520e+03  2.534e+03\n",
      " -2.535e+03 -2.541e+03  2.551e+03 -2.558e+03 -2.559e+03  2.573e+03\n",
      " -2.596e+03 -2.596e+03 -2.605e+03 -2.612e+03 -2.618e+03 -2.629e+03\n",
      " -2.632e+03 -2.632e+03  2.636e+03 -2.645e+03 -2.645e+03 -2.650e+03\n",
      " -2.653e+03 -2.654e+03 -2.695e+03 -2.709e+03  2.712e+03 -2.721e+03\n",
      " -2.759e+03  2.763e+03 -2.767e+03 -2.770e+03  2.798e+03 -2.813e+03\n",
      " -2.838e+03  2.854e+03  2.866e+03 -2.868e+03  2.911e+03 -2.920e+03\n",
      "  2.965e+03  2.965e+03 -2.971e+03 -2.976e+03 -2.991e+03 -3.001e+03\n",
      " -3.004e+03 -3.009e+03 -3.011e+03 -3.015e+03 -3.021e+03 -3.022e+03\n",
      " -3.030e+03 -3.035e+03  3.063e+03 -3.072e+03 -3.091e+03 -3.094e+03\n",
      " -3.102e+03 -3.112e+03 -3.128e+03 -3.152e+03 -3.159e+03 -3.172e+03\n",
      " -3.202e+03 -3.203e+03 -3.204e+03 -3.226e+03 -3.247e+03 -3.248e+03\n",
      " -3.256e+03 -3.261e+03  3.262e+03 -3.276e+03 -3.283e+03 -3.283e+03\n",
      " -3.286e+03 -3.287e+03 -3.296e+03 -3.307e+03 -3.342e+03 -3.361e+03\n",
      " -3.364e+03  3.409e+03 -3.430e+03 -3.456e+03  3.461e+03  3.462e+03\n",
      "  3.472e+03  3.492e+03 -3.519e+03 -3.603e+03 -3.660e+03  3.669e+03\n",
      " -3.709e+03  3.736e+03 -3.807e+03  3.873e+03  3.926e+03  3.945e+03\n",
      " -3.957e+03  3.959e+03 -4.005e+03 -4.047e+03 -4.052e+03 -4.080e+03\n",
      " -4.088e+03 -4.159e+03 -4.275e+03 -4.285e+03 -4.361e+03  4.456e+03\n",
      " -4.929e+03 -5.042e+03 -5.062e+03 -5.156e+03 -5.739e+03  6.456e+03\n",
      "  6.593e+03 -7.106e+03  7.455e+03 -8.008e+03 -8.391e+03 -8.556e+03]\n",
      "Concordance Index 0.6414717291642177\n",
      "Integrated Brier Score: 0.19099640177029975\n",
      "y_train breslow final [-1.000e+00 -1.000e+00 -5.000e+00 -5.000e+00 -7.000e+00 -9.000e+00\n",
      " -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01\n",
      " -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01\n",
      " -1.100e+01 -1.600e+01 -1.900e+01 -2.100e+01 -2.400e+01 -2.600e+01\n",
      " -3.000e+01 -3.000e+01 -3.000e+01 -3.000e+01 -3.100e+01 -3.100e+01\n",
      " -3.100e+01 -3.100e+01 -3.400e+01 -3.400e+01 -4.900e+01 -5.100e+01\n",
      " -5.200e+01 -5.900e+01 -6.400e+01 -7.000e+01 -7.200e+01 -7.600e+01\n",
      " -7.800e+01 -7.800e+01 -7.900e+01 -8.000e+01 -9.000e+01 -9.200e+01\n",
      " -9.800e+01 -1.180e+02 -1.340e+02 -1.490e+02  1.580e+02 -1.600e+02\n",
      "  1.600e+02 -1.620e+02 -1.630e+02  1.720e+02 -1.720e+02  1.740e+02\n",
      " -1.780e+02 -1.870e+02 -1.960e+02  1.970e+02 -2.000e+02 -2.100e+02\n",
      " -2.130e+02 -2.150e+02 -2.170e+02 -2.180e+02 -2.220e+02 -2.240e+02\n",
      "  2.240e+02 -2.250e+02  2.270e+02 -2.270e+02 -2.310e+02 -2.420e+02\n",
      " -2.430e+02 -2.500e+02  2.550e+02 -2.580e+02 -2.590e+02 -2.660e+02\n",
      "  2.660e+02 -2.710e+02 -2.730e+02 -2.750e+02 -2.850e+02 -2.870e+02\n",
      " -2.880e+02 -2.930e+02  2.950e+02 -2.970e+02 -3.000e+02 -3.020e+02\n",
      "  3.020e+02 -3.030e+02 -3.030e+02 -3.040e+02 -3.040e+02 -3.040e+02\n",
      " -3.040e+02 -3.070e+02 -3.130e+02 -3.170e+02 -3.170e+02  3.200e+02\n",
      " -3.200e+02  3.220e+02 -3.220e+02 -3.230e+02 -3.260e+02 -3.280e+02\n",
      " -3.320e+02 -3.340e+02  3.360e+02 -3.360e+02 -3.370e+02 -3.390e+02\n",
      " -3.430e+02 -3.430e+02 -3.450e+02 -3.450e+02  3.480e+02 -3.480e+02\n",
      " -3.500e+02 -3.580e+02 -3.580e+02 -3.600e+02  3.620e+02 -3.630e+02\n",
      " -3.640e+02 -3.650e+02 -3.650e+02 -3.650e+02  3.650e+02 -3.650e+02\n",
      " -3.660e+02 -3.680e+02 -3.700e+02 -3.710e+02 -3.710e+02 -3.730e+02\n",
      " -3.730e+02 -3.750e+02 -3.750e+02 -3.750e+02 -3.750e+02 -3.760e+02\n",
      " -3.760e+02 -3.790e+02 -3.800e+02 -3.800e+02 -3.820e+02 -3.830e+02\n",
      " -3.830e+02  3.850e+02 -3.850e+02  3.850e+02 -3.850e+02 -3.850e+02\n",
      " -3.930e+02 -3.930e+02 -3.940e+02 -3.940e+02 -3.950e+02 -3.960e+02\n",
      " -3.960e+02 -3.960e+02 -3.970e+02 -3.980e+02 -4.030e+02 -4.030e+02\n",
      " -4.040e+02 -4.050e+02 -4.080e+02 -4.080e+02 -4.110e+02 -4.120e+02\n",
      " -4.130e+02 -4.140e+02 -4.160e+02 -4.180e+02 -4.210e+02 -4.240e+02\n",
      " -4.250e+02 -4.260e+02  4.260e+02 -4.260e+02 -4.280e+02 -4.280e+02\n",
      " -4.310e+02 -4.310e+02 -4.330e+02 -4.340e+02 -4.350e+02 -4.370e+02\n",
      " -4.390e+02 -4.390e+02 -4.390e+02 -4.400e+02 -4.410e+02 -4.410e+02\n",
      " -4.410e+02 -4.460e+02  4.460e+02 -4.470e+02 -4.470e+02 -4.470e+02\n",
      " -4.480e+02 -4.480e+02 -4.500e+02 -4.510e+02 -4.540e+02 -4.550e+02\n",
      " -4.560e+02 -4.570e+02 -4.580e+02 -4.610e+02 -4.630e+02 -4.660e+02\n",
      " -4.670e+02 -4.700e+02 -4.710e+02 -4.720e+02 -4.750e+02 -4.760e+02\n",
      " -4.770e+02 -4.770e+02 -4.770e+02 -4.800e+02 -4.820e+02 -4.880e+02\n",
      " -4.880e+02 -4.890e+02 -4.920e+02 -4.950e+02 -4.960e+02 -4.960e+02\n",
      " -4.990e+02 -4.990e+02 -5.010e+02 -5.010e+02 -5.020e+02 -5.040e+02\n",
      " -5.060e+02 -5.080e+02 -5.090e+02 -5.100e+02 -5.130e+02 -5.160e+02\n",
      " -5.180e+02 -5.180e+02 -5.180e+02 -5.190e+02 -5.190e+02 -5.210e+02\n",
      " -5.220e+02 -5.230e+02  5.240e+02 -5.260e+02 -5.280e+02 -5.280e+02\n",
      " -5.290e+02 -5.300e+02 -5.320e+02 -5.320e+02 -5.330e+02 -5.380e+02\n",
      " -5.380e+02  5.380e+02 -5.470e+02  5.480e+02 -5.490e+02 -5.510e+02\n",
      " -5.520e+02 -5.520e+02 -5.540e+02 -5.540e+02  5.580e+02 -5.620e+02\n",
      " -5.630e+02  5.630e+02 -5.650e+02 -5.670e+02 -5.680e+02  5.710e+02\n",
      " -5.720e+02  5.730e+02 -5.740e+02 -5.750e+02 -5.750e+02 -5.760e+02\n",
      " -5.770e+02 -5.790e+02 -5.790e+02 -5.840e+02  5.840e+02 -5.840e+02\n",
      " -5.850e+02 -5.860e+02 -5.880e+02 -5.880e+02 -5.900e+02 -5.910e+02\n",
      " -5.940e+02 -5.950e+02 -5.950e+02 -5.960e+02 -5.980e+02 -6.020e+02\n",
      " -6.060e+02 -6.070e+02 -6.080e+02 -6.110e+02 -6.110e+02 -6.120e+02\n",
      "  6.120e+02 -6.120e+02 -6.120e+02  6.140e+02  6.160e+02 -6.160e+02\n",
      " -6.180e+02 -6.200e+02 -6.200e+02 -6.200e+02 -6.220e+02 -6.220e+02\n",
      " -6.240e+02 -6.260e+02 -6.260e+02 -6.290e+02 -6.300e+02 -6.310e+02\n",
      " -6.350e+02 -6.350e+02 -6.390e+02  6.390e+02 -6.400e+02 -6.430e+02\n",
      " -6.430e+02 -6.440e+02 -6.480e+02 -6.520e+02 -6.580e+02 -6.590e+02\n",
      " -6.590e+02 -6.600e+02 -6.610e+02 -6.620e+02 -6.640e+02 -6.720e+02\n",
      " -6.750e+02 -6.770e+02  6.780e+02 -6.790e+02 -6.800e+02 -6.810e+02\n",
      " -6.940e+02 -6.980e+02 -7.010e+02 -7.020e+02 -7.060e+02 -7.070e+02\n",
      " -7.070e+02 -7.090e+02 -7.100e+02 -7.140e+02 -7.140e+02 -7.150e+02\n",
      " -7.150e+02  7.230e+02 -7.240e+02 -7.250e+02 -7.260e+02 -7.270e+02\n",
      " -7.270e+02 -7.270e+02 -7.280e+02 -7.380e+02 -7.420e+02 -7.450e+02\n",
      " -7.470e+02 -7.470e+02 -7.520e+02 -7.520e+02  7.540e+02 -7.540e+02\n",
      " -7.540e+02 -7.550e+02 -7.590e+02 -7.590e+02 -7.600e+02 -7.600e+02\n",
      " -7.610e+02 -7.620e+02 -7.620e+02 -7.620e+02 -7.650e+02 -7.670e+02\n",
      " -7.690e+02 -7.770e+02 -7.830e+02 -7.850e+02  7.850e+02  7.860e+02\n",
      " -7.890e+02 -7.910e+02  7.920e+02  7.920e+02 -7.920e+02 -8.030e+02\n",
      " -8.060e+02  8.110e+02 -8.120e+02 -8.190e+02  8.210e+02 -8.220e+02\n",
      "  8.250e+02 -8.290e+02 -8.370e+02 -8.390e+02 -8.470e+02 -8.490e+02\n",
      " -8.520e+02 -8.580e+02  8.600e+02 -8.650e+02 -8.670e+02 -8.750e+02\n",
      " -8.780e+02 -8.810e+02  8.830e+02 -8.830e+02 -8.890e+02 -8.900e+02\n",
      "  9.040e+02 -9.060e+02 -9.070e+02 -9.110e+02  9.120e+02 -9.120e+02\n",
      " -9.120e+02 -9.150e+02  9.210e+02 -9.230e+02 -9.260e+02 -9.310e+02\n",
      " -9.310e+02 -9.420e+02 -9.430e+02  9.430e+02 -9.430e+02 -9.520e+02\n",
      " -9.540e+02 -9.650e+02 -9.650e+02 -9.680e+02 -9.720e+02 -9.730e+02\n",
      " -9.740e+02 -9.740e+02 -9.750e+02 -9.750e+02  9.760e+02 -9.890e+02\n",
      "  9.910e+02  9.910e+02 -9.960e+02 -9.960e+02 -1.000e+03 -1.001e+03\n",
      " -1.001e+03 -1.001e+03  1.004e+03 -1.004e+03 -1.004e+03 -1.005e+03\n",
      " -1.006e+03 -1.007e+03  1.009e+03 -1.009e+03 -1.015e+03 -1.025e+03\n",
      " -1.026e+03 -1.026e+03 -1.027e+03  1.032e+03  1.034e+03  1.034e+03\n",
      " -1.035e+03 -1.039e+03 -1.043e+03 -1.043e+03 -1.047e+03  1.048e+03\n",
      " -1.051e+03 -1.051e+03 -1.059e+03 -1.062e+03 -1.063e+03 -1.066e+03\n",
      "  1.072e+03 -1.074e+03 -1.079e+03 -1.085e+03 -1.088e+03 -1.093e+03\n",
      "  1.093e+03 -1.101e+03  1.104e+03 -1.106e+03 -1.120e+03 -1.121e+03\n",
      " -1.124e+03  1.127e+03 -1.132e+03 -1.133e+03 -1.138e+03 -1.139e+03\n",
      " -1.140e+03 -1.141e+03  1.142e+03 -1.148e+03 -1.150e+03 -1.156e+03\n",
      " -1.156e+03 -1.157e+03 -1.161e+03 -1.162e+03 -1.163e+03 -1.165e+03\n",
      " -1.167e+03 -1.174e+03 -1.179e+03 -1.185e+03 -1.186e+03 -1.189e+03\n",
      " -1.191e+03 -1.196e+03 -1.198e+03 -1.203e+03 -1.206e+03 -1.208e+03\n",
      " -1.210e+03 -1.217e+03 -1.220e+03 -1.220e+03 -1.229e+03 -1.229e+03\n",
      " -1.232e+03 -1.239e+03 -1.246e+03 -1.247e+03 -1.248e+03 -1.251e+03\n",
      " -1.269e+03  1.275e+03 -1.277e+03 -1.285e+03  1.286e+03 -1.288e+03\n",
      " -1.291e+03 -1.301e+03 -1.308e+03 -1.308e+03 -1.308e+03 -1.309e+03\n",
      " -1.309e+03 -1.318e+03 -1.321e+03  1.324e+03 -1.326e+03 -1.330e+03\n",
      " -1.347e+03 -1.351e+03 -1.359e+03 -1.363e+03  1.365e+03 -1.371e+03\n",
      " -1.371e+03  1.388e+03 -1.417e+03 -1.417e+03 -1.419e+03 -1.434e+03\n",
      " -1.437e+03  1.439e+03 -1.448e+03 -1.461e+03 -1.463e+03 -1.471e+03\n",
      " -1.474e+03 -1.474e+03 -1.476e+03 -1.477e+03 -1.492e+03  1.508e+03\n",
      " -1.519e+03 -1.522e+03 -1.523e+03 -1.528e+03 -1.532e+03 -1.534e+03\n",
      " -1.542e+03 -1.545e+03 -1.545e+03 -1.546e+03 -1.547e+03 -1.548e+03\n",
      " -1.550e+03  1.556e+03 -1.561e+03 -1.563e+03 -1.569e+03 -1.572e+03\n",
      " -1.587e+03 -1.596e+03 -1.604e+03 -1.604e+03 -1.611e+03 -1.611e+03\n",
      " -1.611e+03 -1.612e+03 -1.613e+03 -1.616e+03 -1.620e+03 -1.631e+03\n",
      " -1.639e+03 -1.640e+03  1.642e+03 -1.642e+03 -1.644e+03 -1.648e+03\n",
      "  1.649e+03 -1.662e+03 -1.673e+03 -1.673e+03  1.673e+03 -1.682e+03\n",
      " -1.683e+03 -1.686e+03 -1.688e+03  1.692e+03 -1.692e+03  1.699e+03\n",
      " -1.712e+03 -1.728e+03 -1.728e+03 -1.732e+03 -1.742e+03  1.759e+03\n",
      " -1.780e+03  1.781e+03 -1.783e+03 -1.800e+03  1.812e+03 -1.820e+03\n",
      " -1.836e+03 -1.842e+03 -1.847e+03 -1.853e+03 -1.855e+03 -1.864e+03\n",
      " -1.866e+03 -1.871e+03 -1.873e+03 -1.876e+03 -1.882e+03 -1.887e+03\n",
      "  1.900e+03 -1.914e+03 -1.919e+03 -1.925e+03 -1.928e+03 -1.935e+03\n",
      " -1.953e+03 -1.980e+03  1.993e+03 -1.996e+03  2.009e+03 -2.012e+03\n",
      " -2.019e+03 -2.031e+03 -2.033e+03 -2.041e+03 -2.064e+03 -2.072e+03\n",
      " -2.082e+03 -2.091e+03 -2.108e+03 -2.109e+03 -2.124e+03  2.127e+03\n",
      " -2.128e+03 -2.136e+03 -2.140e+03 -2.142e+03 -2.155e+03 -2.156e+03\n",
      " -2.161e+03 -2.184e+03 -2.190e+03 -2.190e+03 -2.191e+03  2.192e+03\n",
      " -2.193e+03 -2.197e+03  2.207e+03 -2.222e+03 -2.231e+03 -2.240e+03\n",
      " -2.246e+03 -2.248e+03 -2.255e+03 -2.263e+03 -2.278e+03 -2.288e+03\n",
      " -2.289e+03  2.296e+03 -2.306e+03 -2.311e+03 -2.329e+03 -2.330e+03\n",
      " -2.335e+03  2.348e+03  2.361e+03 -2.362e+03 -2.365e+03 -2.371e+03\n",
      " -2.372e+03 -2.385e+03 -2.403e+03 -2.426e+03 -2.442e+03  2.469e+03\n",
      " -2.477e+03  2.483e+03 -2.513e+03 -2.515e+03  2.520e+03  2.534e+03\n",
      " -2.535e+03 -2.541e+03  2.551e+03 -2.558e+03 -2.559e+03  2.573e+03\n",
      " -2.596e+03 -2.596e+03 -2.605e+03 -2.612e+03 -2.618e+03 -2.629e+03\n",
      " -2.632e+03 -2.632e+03  2.636e+03 -2.645e+03 -2.645e+03 -2.650e+03\n",
      " -2.653e+03 -2.654e+03 -2.695e+03 -2.709e+03  2.712e+03 -2.721e+03\n",
      " -2.759e+03  2.763e+03 -2.767e+03 -2.770e+03  2.798e+03 -2.813e+03\n",
      " -2.838e+03  2.854e+03  2.866e+03 -2.868e+03  2.911e+03 -2.920e+03\n",
      "  2.965e+03  2.965e+03 -2.971e+03 -2.976e+03 -2.991e+03 -3.001e+03\n",
      " -3.004e+03 -3.009e+03 -3.011e+03 -3.015e+03 -3.021e+03 -3.022e+03\n",
      " -3.030e+03 -3.035e+03  3.063e+03 -3.072e+03 -3.091e+03 -3.094e+03\n",
      " -3.102e+03 -3.112e+03 -3.128e+03 -3.152e+03 -3.159e+03 -3.172e+03\n",
      " -3.202e+03 -3.203e+03 -3.204e+03 -3.226e+03 -3.247e+03 -3.248e+03\n",
      " -3.256e+03 -3.261e+03  3.262e+03 -3.276e+03 -3.283e+03 -3.283e+03\n",
      " -3.286e+03 -3.287e+03 -3.296e+03 -3.307e+03 -3.342e+03 -3.361e+03\n",
      " -3.364e+03  3.409e+03 -3.430e+03 -3.456e+03  3.461e+03  3.462e+03\n",
      "  3.472e+03  3.492e+03 -3.519e+03 -3.603e+03 -3.660e+03  3.669e+03\n",
      " -3.709e+03  3.736e+03 -3.807e+03  3.873e+03  3.926e+03  3.945e+03\n",
      " -3.957e+03  3.959e+03 -4.005e+03 -4.047e+03 -4.052e+03 -4.080e+03\n",
      " -4.088e+03 -4.159e+03 -4.275e+03 -4.285e+03 -4.361e+03  4.456e+03\n",
      " -4.929e+03 -5.042e+03 -5.062e+03 -5.156e+03 -5.739e+03  6.456e+03\n",
      "  6.593e+03 -7.106e+03  7.455e+03 -8.008e+03 -8.391e+03 -8.556e+03]\n",
      "durations 5.0 8605.0\n",
      "Concordance Index 0.5381944444444444\n",
      "Integrated Brier Score: 0.24450130850101098\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m166.2278\u001b[0m      \u001b[32m112.1142\u001b[0m  0.0773\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m79.1207\u001b[0m       \u001b[32m81.3508\u001b[0m  0.0872\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2      \u001b[36m159.9155\u001b[0m      113.2174  0.0372\n",
      "      2       \u001b[36m71.9729\u001b[0m       \u001b[32m75.9502\u001b[0m  0.0584\n",
      "      3      162.0939      \u001b[32m107.7626\u001b[0m  0.0395\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m179.1424\u001b[0m      \u001b[32m129.0722\u001b[0m  0.0504\n",
      "      4      160.3162      \u001b[32m103.1379\u001b[0m  0.0356\n",
      "      3       \u001b[36m67.8452\u001b[0m       \u001b[32m73.6712\u001b[0m  0.0527\n",
      "      2      \u001b[36m171.6364\u001b[0m      \u001b[32m126.9213\u001b[0m  0.0489\n",
      "      5      \u001b[36m154.9371\u001b[0m       \u001b[32m99.9599\u001b[0m  0.0438\n",
      "      4       68.4423       \u001b[32m72.3132\u001b[0m  0.0492\n",
      "      3      \u001b[36m169.4402\u001b[0m      \u001b[32m122.1958\u001b[0m  0.0408\n",
      "      6      155.7627       \u001b[32m98.2962\u001b[0m  0.0323\n",
      "      4      171.0787      \u001b[32m117.8654\u001b[0m  0.0373\n",
      "      7      \u001b[36m150.7393\u001b[0m       \u001b[32m97.6670\u001b[0m  0.0364\n",
      "      5       69.0046       \u001b[32m72.1036\u001b[0m  0.0539\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      5      \u001b[36m167.1888\u001b[0m      \u001b[32m114.6766\u001b[0m  0.0396\n",
      "      8      154.1427       97.8575  0.0399\n",
      "      6       \u001b[36m66.2828\u001b[0m       72.9151  0.0502\n",
      "      6      \u001b[36m165.8787\u001b[0m      \u001b[32m111.9367\u001b[0m  0.0363\n",
      "      9      153.2242       98.9654  0.0305\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m67.7950\u001b[0m       \u001b[32m57.2882\u001b[0m  0.0709\n",
      "      7      \u001b[36m162.3228\u001b[0m      \u001b[32m110.6165\u001b[0m  0.0322\n",
      "      7       \u001b[36m64.7694\u001b[0m       73.0621  0.0536\n",
      "     10      \u001b[36m149.1265\u001b[0m      100.1800  0.0395\n",
      "Restoring best model from epoch 7.\n",
      "      8      166.0399      \u001b[32m110.3480\u001b[0m  0.0394\n",
      "      2       \u001b[36m63.8204\u001b[0m       \u001b[32m45.9575\u001b[0m  0.0658\n",
      "      8       65.7937       72.7463  0.0544\n",
      "      9      164.7577      111.1332  0.0404\n",
      "      9       67.2876       72.1344  0.0544\n",
      "     10      162.8028      112.1985  0.0409\n",
      "      3       \u001b[36m62.7674\u001b[0m       46.0854  0.0604\n",
      "Restoring best model from epoch 8.\n",
      "      4       \u001b[36m58.0639\u001b[0m       49.8529  0.0486\n",
      "     10       66.0369       \u001b[32m70.8439\u001b[0m  0.0540\n",
      "      5       58.3918       50.7743  0.0529\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      6       \u001b[36m57.8872\u001b[0m       48.8366  0.0591\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m165.8280\u001b[0m       \u001b[32m61.0325\u001b[0m  0.0595\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m163.2511\u001b[0m       \u001b[32m91.3505\u001b[0m  0.0621\n",
      "      7       \u001b[36m57.5548\u001b[0m       48.9237  0.0470\n",
      "      2      \u001b[36m158.5236\u001b[0m       65.1023  0.0346\n",
      "      2      \u001b[36m157.2028\u001b[0m       \u001b[32m89.0668\u001b[0m  0.0416\n",
      "      8       \u001b[36m56.0194\u001b[0m       48.2207  0.0455\n",
      "      3      \u001b[36m153.1652\u001b[0m       65.1427  0.0362\n",
      "      3      \u001b[36m155.2906\u001b[0m       \u001b[32m86.7015\u001b[0m  0.0326\n",
      "      9       58.2344       48.5485  0.0439\n",
      "      4      154.7488       64.1349  0.0355\n",
      "      4      158.5843       \u001b[32m84.6722\u001b[0m  0.0315\n",
      "      5      \u001b[36m153.1617\u001b[0m       62.7253  0.0299\n",
      "      5      156.2647       \u001b[32m83.0854\u001b[0m  0.0284\n",
      "     10       58.2537       48.9127  0.0466\n",
      "Restoring best model from epoch 2.\n",
      "      6      154.0168       62.1217  0.0295\n",
      "      6      \u001b[36m153.0561\u001b[0m       \u001b[32m81.8096\u001b[0m  0.0308\n",
      "      7      \u001b[36m150.7494\u001b[0m       61.7021  0.0287\n",
      "      7      \u001b[36m148.8259\u001b[0m       \u001b[32m81.1296\u001b[0m  0.0278\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      8      \u001b[36m149.5242\u001b[0m       61.5807  0.0282\n",
      "      8      153.0715       \u001b[32m80.9987\u001b[0m  0.0288\n",
      "      9      149.7054       61.8222  0.0294\n",
      "      9      149.5078       81.2409  0.0346\n",
      "     10      \u001b[36m148.3274\u001b[0m       62.0584  0.0308\n",
      "Restoring best model from epoch 1.\n",
      "     10      148.9745       81.5762  0.0348\n",
      "Restoring best model from epoch 8.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m77.5540\u001b[0m       \u001b[32m62.0259\u001b[0m  0.0848\n",
      "      2       \u001b[36m70.5218\u001b[0m       \u001b[32m54.8487\u001b[0m  0.0469\n",
      "      3       \u001b[36m67.0456\u001b[0m       \u001b[32m53.8735\u001b[0m  0.0512\n",
      "      4       \u001b[36m65.3507\u001b[0m       55.3074  0.0430\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      5       \u001b[36m65.0836\u001b[0m       \u001b[32m53.5821\u001b[0m  0.0395\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m145.7547\u001b[0m       \u001b[32m52.1231\u001b[0m  0.0364\n",
      "      6       \u001b[36m65.0011\u001b[0m       \u001b[32m51.0404\u001b[0m  0.0392\n",
      "      2      \u001b[36m141.9648\u001b[0m       54.3190  0.0406\n",
      "      7       66.0519       52.1204  0.0424\n",
      "      3      \u001b[36m136.1848\u001b[0m       54.5368  0.0323\n",
      "      4      137.8549       54.2425  0.0264\n",
      "      8       \u001b[36m63.2854\u001b[0m       54.8222  0.0434\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      5      137.8234       53.9181  0.0258\n",
      "      9       \u001b[36m62.1354\u001b[0m       56.1404  0.0402\n",
      "      6      137.0543       53.5773  0.0252\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m86.2324\u001b[0m      \u001b[32m131.1211\u001b[0m  0.0479\n",
      "      7      \u001b[36m133.4981\u001b[0m       53.3419  0.0262\n",
      "     10       64.0321       55.7969  0.0392\n",
      "Restoring best model from epoch 6.\n",
      "      8      135.2643       53.5097  0.0276\n",
      "      2       \u001b[36m75.6874\u001b[0m       \u001b[32m96.3149\u001b[0m  0.0463\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m78.4035\u001b[0m       \u001b[32m88.3904\u001b[0m  0.0585\n",
      "      9      \u001b[36m131.4898\u001b[0m       53.9821  0.0279\n",
      "     10      \u001b[36m130.8632\u001b[0m       54.3758  0.0262\n",
      "      3       \u001b[36m75.5386\u001b[0m       96.3603  0.0482\n",
      "Restoring best model from epoch 1.\n",
      "      2       \u001b[36m72.2828\u001b[0m       \u001b[32m78.2153\u001b[0m  0.0450\n",
      "      4       \u001b[36m75.0631\u001b[0m       97.9147  0.0418\n",
      "      3       \u001b[36m69.0418\u001b[0m       78.2791  0.0406\n",
      "      5       \u001b[36m74.8078\u001b[0m       \u001b[32m94.4157\u001b[0m  0.0400\n",
      "      4       \u001b[36m68.2712\u001b[0m       \u001b[32m77.4933\u001b[0m  0.0382\n",
      "      6       \u001b[36m73.8284\u001b[0m       \u001b[32m93.3686\u001b[0m  0.0397\n",
      "      5       \u001b[36m68.0719\u001b[0m       78.2555  0.0428\n",
      "      6       \u001b[36m67.6305\u001b[0m       \u001b[32m77.4463\u001b[0m  0.0431\n",
      "      7       \u001b[36m71.9784\u001b[0m       94.3950  0.0523\n",
      "      7       \u001b[36m65.9477\u001b[0m       \u001b[32m76.8039\u001b[0m  0.0411\n",
      "      8       \u001b[36m71.1791\u001b[0m       96.4524  0.0402\n",
      "      8       \u001b[36m65.1268\u001b[0m       \u001b[32m76.2632\u001b[0m  0.0397\n",
      "      9       73.4206       96.6516  0.0384\n",
      "      9       \u001b[36m64.0251\u001b[0m       77.3130  0.0383\n",
      "     10       72.5630       94.2817  0.0382\n",
      "Restoring best model from epoch 6.\n",
      "     10       64.6992       77.2986  0.0390\n",
      "Restoring best model from epoch 8.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m159.7136\u001b[0m      \u001b[32m120.0238\u001b[0m  0.0327\n",
      "      2      \u001b[36m152.4079\u001b[0m      \u001b[32m119.9326\u001b[0m  0.0288\n",
      "      3      \u001b[36m149.8489\u001b[0m      \u001b[32m114.8568\u001b[0m  0.0286\n",
      "      4      \u001b[36m148.4414\u001b[0m      \u001b[32m111.1282\u001b[0m  0.0282\n",
      "      5      \u001b[36m146.3156\u001b[0m      \u001b[32m109.3119\u001b[0m  0.0292\n",
      "      6      \u001b[36m145.0510\u001b[0m      \u001b[32m108.8285\u001b[0m  0.0295\n",
      "      7      145.1893      \u001b[32m108.7564\u001b[0m  0.0286\n",
      "      8      146.3532      109.4021  0.0281\n",
      "      9      147.7318      110.2765  0.0287\n",
      "     10      \u001b[36m143.2900\u001b[0m      111.1389  0.0321\n",
      "Restoring best model from epoch 7.\n",
      "y_train breslow final [-1.000e+00 -1.000e+00 -5.000e+00 -5.000e+00 -8.000e+00 -9.000e+00\n",
      " -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01\n",
      " -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01 -1.100e+01\n",
      " -1.600e+01 -1.900e+01 -2.100e+01 -2.400e+01 -2.600e+01 -3.000e+01\n",
      " -3.000e+01 -3.100e+01 -3.100e+01 -3.400e+01 -4.000e+01 -5.100e+01\n",
      " -5.200e+01 -5.400e+01 -5.900e+01 -7.200e+01 -7.600e+01 -7.800e+01\n",
      " -7.800e+01 -7.800e+01 -7.900e+01 -8.000e+01 -8.400e+01 -9.800e+01\n",
      "  1.160e+02 -1.180e+02 -1.340e+02 -1.490e+02  1.580e+02 -1.600e+02\n",
      "  1.600e+02 -1.620e+02 -1.630e+02 -1.700e+02 -1.700e+02  1.720e+02\n",
      " -1.720e+02  1.740e+02 -1.780e+02 -1.860e+02 -1.870e+02 -1.960e+02\n",
      "  1.970e+02 -2.000e+02 -2.130e+02 -2.140e+02 -2.160e+02 -2.170e+02\n",
      " -2.180e+02 -2.220e+02 -2.240e+02  2.240e+02 -2.250e+02  2.270e+02\n",
      "  2.390e+02 -2.420e+02 -2.420e+02 -2.430e+02 -2.520e+02  2.550e+02\n",
      " -2.580e+02 -2.590e+02  2.660e+02 -2.710e+02 -2.730e+02 -2.730e+02\n",
      " -2.740e+02 -2.750e+02 -2.850e+02 -2.880e+02 -2.930e+02 -2.930e+02\n",
      " -2.970e+02 -3.020e+02  3.020e+02 -3.030e+02 -3.030e+02 -3.040e+02\n",
      " -3.040e+02  3.040e+02 -3.070e+02 -3.130e+02 -3.130e+02 -3.170e+02\n",
      "  3.200e+02 -3.200e+02 -3.220e+02 -3.220e+02 -3.230e+02 -3.320e+02\n",
      "  3.360e+02 -3.360e+02 -3.370e+02 -3.390e+02 -3.430e+02 -3.430e+02\n",
      " -3.450e+02 -3.470e+02  3.480e+02 -3.480e+02 -3.500e+02 -3.520e+02\n",
      " -3.580e+02 -3.580e+02 -3.600e+02  3.620e+02 -3.630e+02 -3.640e+02\n",
      " -3.650e+02 -3.650e+02 -3.650e+02 -3.650e+02 -3.660e+02 -3.660e+02\n",
      " -3.680e+02 -3.700e+02 -3.710e+02 -3.710e+02 -3.730e+02 -3.750e+02\n",
      " -3.750e+02 -3.750e+02 -3.750e+02 -3.750e+02 -3.760e+02  3.770e+02\n",
      " -3.790e+02 -3.800e+02 -3.810e+02 -3.810e+02 -3.820e+02 -3.830e+02\n",
      " -3.830e+02 -3.850e+02 -3.850e+02 -3.850e+02  3.850e+02 -3.850e+02\n",
      " -3.850e+02 -3.920e+02 -3.930e+02 -3.940e+02 -3.940e+02 -3.960e+02\n",
      " -3.960e+02 -3.960e+02 -3.960e+02 -3.970e+02 -3.980e+02 -4.020e+02\n",
      " -4.040e+02 -4.050e+02 -4.080e+02 -4.080e+02 -4.090e+02 -4.100e+02\n",
      " -4.100e+02 -4.110e+02 -4.130e+02 -4.140e+02 -4.160e+02 -4.170e+02\n",
      " -4.180e+02 -4.230e+02 -4.240e+02 -4.240e+02 -4.260e+02 -4.260e+02\n",
      " -4.280e+02 -4.300e+02 -4.310e+02 -4.330e+02 -4.340e+02 -4.350e+02\n",
      " -4.390e+02 -4.400e+02 -4.410e+02 -4.410e+02 -4.460e+02  4.460e+02\n",
      " -4.470e+02 -4.470e+02 -4.480e+02 -4.500e+02 -4.500e+02 -4.550e+02\n",
      " -4.570e+02 -4.610e+02 -4.610e+02 -4.630e+02 -4.630e+02 -4.660e+02\n",
      " -4.660e+02 -4.700e+02 -4.750e+02 -4.760e+02 -4.770e+02 -4.770e+02\n",
      " -4.800e+02 -4.880e+02 -4.880e+02 -4.920e+02 -4.950e+02 -4.960e+02\n",
      " -4.960e+02 -4.990e+02 -4.990e+02 -5.010e+02 -5.030e+02 -5.040e+02\n",
      " -5.060e+02 -5.080e+02 -5.090e+02 -5.100e+02 -5.110e+02 -5.130e+02\n",
      " -5.160e+02 -5.180e+02 -5.180e+02 -5.180e+02 -5.190e+02 -5.210e+02\n",
      " -5.220e+02  5.240e+02 -5.250e+02 -5.260e+02 -5.280e+02 -5.290e+02\n",
      " -5.300e+02 -5.300e+02 -5.310e+02 -5.320e+02 -5.320e+02 -5.320e+02\n",
      " -5.330e+02  5.380e+02 -5.410e+02 -5.410e+02 -5.450e+02 -5.460e+02\n",
      " -5.470e+02  5.480e+02 -5.510e+02 -5.520e+02 -5.520e+02 -5.520e+02\n",
      " -5.540e+02 -5.540e+02 -5.540e+02 -5.540e+02 -5.540e+02  5.580e+02\n",
      " -5.620e+02 -5.620e+02  5.630e+02 -5.650e+02 -5.660e+02  5.710e+02\n",
      " -5.720e+02 -5.740e+02 -5.740e+02 -5.750e+02 -5.750e+02 -5.750e+02\n",
      " -5.760e+02 -5.770e+02 -5.770e+02 -5.770e+02 -5.790e+02 -5.790e+02\n",
      " -5.840e+02 -5.840e+02  5.840e+02 -5.840e+02 -5.850e+02 -5.860e+02\n",
      " -5.880e+02 -5.880e+02 -5.900e+02 -5.910e+02 -5.950e+02 -5.950e+02\n",
      " -5.960e+02 -5.980e+02 -6.000e+02 -6.020e+02 -6.060e+02 -6.070e+02\n",
      " -6.070e+02 -6.080e+02  6.120e+02 -6.120e+02 -6.120e+02 -6.140e+02\n",
      "  6.140e+02  6.160e+02 -6.160e+02 -6.180e+02 -6.200e+02 -6.200e+02\n",
      " -6.220e+02 -6.220e+02 -6.240e+02 -6.260e+02 -6.260e+02 -6.270e+02\n",
      " -6.290e+02 -6.300e+02 -6.310e+02 -6.350e+02 -6.350e+02 -6.390e+02\n",
      " -6.400e+02 -6.400e+02 -6.410e+02 -6.430e+02 -6.430e+02 -6.440e+02\n",
      " -6.460e+02 -6.470e+02 -6.480e+02 -6.510e+02 -6.520e+02 -6.550e+02\n",
      " -6.590e+02 -6.590e+02 -6.600e+02 -6.610e+02 -6.620e+02 -6.640e+02\n",
      " -6.660e+02 -6.660e+02 -6.720e+02 -6.750e+02 -6.750e+02 -6.770e+02\n",
      "  6.780e+02 -6.790e+02 -6.790e+02 -6.800e+02 -6.810e+02 -6.830e+02\n",
      " -6.940e+02 -6.940e+02 -6.980e+02 -7.010e+02 -7.020e+02 -7.030e+02\n",
      " -7.060e+02 -7.070e+02 -7.070e+02 -7.090e+02 -7.100e+02 -7.140e+02\n",
      " -7.150e+02 -7.180e+02 -7.220e+02 -7.240e+02 -7.260e+02 -7.270e+02\n",
      " -7.270e+02 -7.270e+02 -7.340e+02 -7.380e+02 -7.420e+02 -7.470e+02\n",
      " -7.470e+02 -7.480e+02  7.490e+02 -7.540e+02 -7.540e+02 -7.550e+02\n",
      " -7.590e+02 -7.600e+02 -7.600e+02 -7.610e+02 -7.620e+02 -7.620e+02\n",
      "  7.630e+02 -7.670e+02 -7.670e+02 -7.690e+02 -7.770e+02 -7.830e+02\n",
      "  7.850e+02  7.860e+02 -7.880e+02 -7.890e+02 -7.910e+02  7.920e+02\n",
      " -7.920e+02 -7.950e+02 -7.980e+02 -8.030e+02 -8.060e+02  8.110e+02\n",
      " -8.120e+02 -8.200e+02 -8.220e+02  8.250e+02 -8.290e+02 -8.470e+02\n",
      " -8.490e+02 -8.560e+02 -8.580e+02 -8.600e+02 -8.650e+02 -8.650e+02\n",
      " -8.780e+02 -8.780e+02  8.790e+02 -8.810e+02  8.830e+02 -8.830e+02\n",
      " -8.890e+02 -8.900e+02 -8.990e+02  9.040e+02 -9.070e+02 -9.080e+02\n",
      " -9.110e+02 -9.120e+02 -9.150e+02 -9.180e+02  9.210e+02 -9.230e+02\n",
      " -9.310e+02 -9.420e+02  9.430e+02 -9.430e+02  9.590e+02 -9.650e+02\n",
      " -9.650e+02  9.670e+02 -9.680e+02 -9.730e+02 -9.740e+02 -9.750e+02\n",
      "  9.760e+02 -9.840e+02 -9.870e+02 -9.890e+02 -9.900e+02  9.910e+02\n",
      "  9.910e+02 -9.960e+02 -9.970e+02 -1.000e+03 -1.001e+03 -1.001e+03\n",
      "  1.004e+03 -1.004e+03 -1.004e+03 -1.004e+03 -1.005e+03 -1.006e+03\n",
      " -1.007e+03  1.009e+03 -1.009e+03 -1.010e+03 -1.013e+03 -1.025e+03\n",
      " -1.026e+03 -1.026e+03 -1.027e+03  1.034e+03  1.034e+03 -1.034e+03\n",
      " -1.039e+03 -1.042e+03 -1.043e+03 -1.047e+03 -1.051e+03 -1.051e+03\n",
      " -1.059e+03 -1.062e+03 -1.063e+03 -1.066e+03  1.072e+03 -1.074e+03\n",
      " -1.079e+03 -1.085e+03 -1.088e+03 -1.093e+03  1.093e+03 -1.099e+03\n",
      " -1.101e+03 -1.102e+03 -1.106e+03 -1.112e+03 -1.119e+03 -1.120e+03\n",
      " -1.120e+03 -1.121e+03 -1.124e+03 -1.132e+03 -1.133e+03 -1.138e+03\n",
      " -1.140e+03  1.142e+03 -1.148e+03  1.148e+03 -1.148e+03 -1.150e+03\n",
      "  1.152e+03 -1.156e+03 -1.158e+03 -1.161e+03 -1.162e+03 -1.163e+03\n",
      " -1.165e+03 -1.167e+03 -1.174e+03  1.174e+03 -1.185e+03 -1.186e+03\n",
      " -1.189e+03 -1.191e+03 -1.196e+03 -1.198e+03 -1.203e+03 -1.203e+03\n",
      " -1.206e+03 -1.208e+03 -1.208e+03 -1.210e+03 -1.217e+03 -1.219e+03\n",
      " -1.220e+03 -1.224e+03 -1.229e+03 -1.229e+03 -1.232e+03 -1.233e+03\n",
      " -1.234e+03 -1.234e+03 -1.239e+03 -1.246e+03 -1.248e+03 -1.251e+03\n",
      " -1.266e+03 -1.269e+03 -1.270e+03  1.272e+03  1.275e+03 -1.277e+03\n",
      "  1.286e+03 -1.291e+03 -1.301e+03 -1.308e+03 -1.308e+03 -1.308e+03\n",
      " -1.309e+03 -1.321e+03  1.324e+03 -1.325e+03 -1.326e+03 -1.330e+03\n",
      " -1.347e+03 -1.351e+03 -1.359e+03 -1.363e+03 -1.363e+03  1.365e+03\n",
      " -1.369e+03 -1.371e+03 -1.371e+03  1.388e+03  1.411e+03 -1.417e+03\n",
      " -1.417e+03  1.430e+03 -1.434e+03  1.439e+03 -1.449e+03 -1.461e+03\n",
      " -1.463e+03 -1.467e+03 -1.471e+03 -1.474e+03 -1.492e+03 -1.505e+03\n",
      "  1.508e+03 -1.516e+03 -1.522e+03 -1.528e+03 -1.532e+03 -1.534e+03\n",
      " -1.535e+03  1.542e+03 -1.542e+03 -1.545e+03 -1.545e+03 -1.546e+03\n",
      " -1.547e+03 -1.550e+03  1.556e+03 -1.561e+03  1.563e+03 -1.563e+03\n",
      " -1.569e+03 -1.572e+03 -1.587e+03 -1.596e+03 -1.604e+03 -1.604e+03\n",
      " -1.611e+03 -1.611e+03 -1.612e+03 -1.613e+03 -1.614e+03 -1.616e+03\n",
      " -1.631e+03 -1.639e+03 -1.640e+03 -1.642e+03 -1.644e+03 -1.648e+03\n",
      "  1.649e+03 -1.673e+03 -1.673e+03  1.673e+03 -1.683e+03 -1.686e+03\n",
      "  1.688e+03 -1.688e+03  1.692e+03 -1.692e+03  1.694e+03 -1.728e+03\n",
      " -1.732e+03 -1.780e+03  1.781e+03 -1.783e+03  1.793e+03  1.812e+03\n",
      " -1.820e+03 -1.836e+03 -1.842e+03 -1.853e+03 -1.855e+03 -1.864e+03\n",
      " -1.866e+03 -1.871e+03 -1.873e+03 -1.876e+03 -1.882e+03  1.884e+03\n",
      " -1.887e+03 -1.914e+03 -1.919e+03  1.920e+03 -1.925e+03 -1.926e+03\n",
      "  1.927e+03 -1.935e+03 -1.935e+03 -1.953e+03 -1.972e+03 -1.980e+03\n",
      " -1.988e+03  1.993e+03 -1.996e+03  2.009e+03 -2.012e+03 -2.019e+03\n",
      " -2.031e+03 -2.033e+03 -2.041e+03 -2.048e+03 -2.064e+03 -2.082e+03\n",
      " -2.091e+03  2.097e+03 -2.108e+03 -2.109e+03 -2.124e+03  2.127e+03\n",
      " -2.128e+03 -2.140e+03 -2.142e+03 -2.155e+03 -2.156e+03 -2.164e+03\n",
      " -2.184e+03 -2.190e+03 -2.190e+03 -2.191e+03  2.192e+03 -2.193e+03\n",
      " -2.197e+03  2.207e+03 -2.222e+03 -2.231e+03 -2.236e+03 -2.240e+03\n",
      " -2.246e+03 -2.248e+03 -2.255e+03 -2.255e+03 -2.255e+03 -2.263e+03\n",
      "  2.273e+03 -2.281e+03 -2.282e+03 -2.288e+03 -2.306e+03 -2.311e+03\n",
      " -2.329e+03 -2.335e+03  2.361e+03 -2.362e+03 -2.365e+03 -2.371e+03\n",
      " -2.372e+03 -2.372e+03  2.373e+03 -2.381e+03 -2.385e+03 -2.403e+03\n",
      " -2.406e+03  2.417e+03 -2.426e+03 -2.442e+03 -2.442e+03  2.469e+03\n",
      " -2.477e+03  2.483e+03 -2.483e+03 -2.486e+03 -2.489e+03 -2.513e+03\n",
      " -2.515e+03  2.520e+03 -2.541e+03  2.551e+03 -2.559e+03 -2.590e+03\n",
      " -2.596e+03 -2.605e+03 -2.612e+03 -2.629e+03 -2.632e+03  2.636e+03\n",
      " -2.645e+03 -2.645e+03 -2.650e+03 -2.653e+03 -2.707e+03 -2.709e+03\n",
      "  2.712e+03 -2.721e+03 -2.759e+03  2.763e+03 -2.767e+03 -2.770e+03\n",
      "  2.798e+03 -2.813e+03 -2.838e+03  2.854e+03 -2.856e+03  2.866e+03\n",
      " -2.868e+03  2.911e+03 -2.920e+03 -2.953e+03  2.965e+03  2.965e+03\n",
      " -2.971e+03 -2.976e+03 -2.989e+03 -2.991e+03 -3.001e+03 -3.004e+03\n",
      " -3.009e+03 -3.011e+03 -3.017e+03 -3.022e+03 -3.035e+03  3.063e+03\n",
      " -3.088e+03 -3.094e+03 -3.102e+03 -3.121e+03  3.126e+03 -3.128e+03\n",
      " -3.152e+03 -3.159e+03 -3.172e+03 -3.202e+03 -3.203e+03 -3.204e+03\n",
      " -3.226e+03 -3.248e+03 -3.256e+03 -3.261e+03 -3.276e+03 -3.283e+03\n",
      " -3.286e+03 -3.287e+03 -3.296e+03 -3.307e+03 -3.316e+03 -3.342e+03\n",
      " -3.361e+03 -3.364e+03  3.418e+03 -3.430e+03  3.461e+03  3.462e+03\n",
      "  3.492e+03 -3.506e+03 -3.603e+03 -3.607e+03  3.669e+03  3.736e+03\n",
      " -3.807e+03  3.873e+03  3.926e+03  3.941e+03  3.945e+03 -3.957e+03\n",
      "  3.959e+03 -4.005e+03 -4.047e+03 -4.080e+03 -4.159e+03 -4.233e+03\n",
      "  4.267e+03 -4.275e+03 -4.285e+03 -4.354e+03 -4.361e+03 -4.894e+03\n",
      " -4.929e+03 -5.156e+03 -5.176e+03 -5.739e+03 -6.292e+03  6.456e+03\n",
      "  6.593e+03 -7.106e+03 -7.126e+03 -7.777e+03 -8.391e+03 -8.556e+03\n",
      " -8.605e+03]\n",
      "Concordance Index 0.5658072626360855\n",
      "Integrated Brier Score: 0.19798898517850336\n",
      "y_train breslow final [-1.000e+00 -1.000e+00 -5.000e+00 -5.000e+00 -8.000e+00 -9.000e+00\n",
      " -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01\n",
      " -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01 -1.100e+01\n",
      " -1.600e+01 -1.900e+01 -2.100e+01 -2.400e+01 -2.600e+01 -3.000e+01\n",
      " -3.000e+01 -3.100e+01 -3.100e+01 -3.400e+01 -4.000e+01 -5.100e+01\n",
      " -5.200e+01 -5.400e+01 -5.900e+01 -7.200e+01 -7.600e+01 -7.800e+01\n",
      " -7.800e+01 -7.800e+01 -7.900e+01 -8.000e+01 -8.400e+01 -9.800e+01\n",
      "  1.160e+02 -1.180e+02 -1.340e+02 -1.490e+02  1.580e+02 -1.600e+02\n",
      "  1.600e+02 -1.620e+02 -1.630e+02 -1.700e+02 -1.700e+02  1.720e+02\n",
      " -1.720e+02  1.740e+02 -1.780e+02 -1.860e+02 -1.870e+02 -1.960e+02\n",
      "  1.970e+02 -2.000e+02 -2.130e+02 -2.140e+02 -2.160e+02 -2.170e+02\n",
      " -2.180e+02 -2.220e+02 -2.240e+02  2.240e+02 -2.250e+02  2.270e+02\n",
      "  2.390e+02 -2.420e+02 -2.420e+02 -2.430e+02 -2.520e+02  2.550e+02\n",
      " -2.580e+02 -2.590e+02  2.660e+02 -2.710e+02 -2.730e+02 -2.730e+02\n",
      " -2.740e+02 -2.750e+02 -2.850e+02 -2.880e+02 -2.930e+02 -2.930e+02\n",
      " -2.970e+02 -3.020e+02  3.020e+02 -3.030e+02 -3.030e+02 -3.040e+02\n",
      " -3.040e+02  3.040e+02 -3.070e+02 -3.130e+02 -3.130e+02 -3.170e+02\n",
      "  3.200e+02 -3.200e+02 -3.220e+02 -3.220e+02 -3.230e+02 -3.320e+02\n",
      "  3.360e+02 -3.360e+02 -3.370e+02 -3.390e+02 -3.430e+02 -3.430e+02\n",
      " -3.450e+02 -3.470e+02  3.480e+02 -3.480e+02 -3.500e+02 -3.520e+02\n",
      " -3.580e+02 -3.580e+02 -3.600e+02  3.620e+02 -3.630e+02 -3.640e+02\n",
      " -3.650e+02 -3.650e+02 -3.650e+02 -3.650e+02 -3.660e+02 -3.660e+02\n",
      " -3.680e+02 -3.700e+02 -3.710e+02 -3.710e+02 -3.730e+02 -3.750e+02\n",
      " -3.750e+02 -3.750e+02 -3.750e+02 -3.750e+02 -3.760e+02  3.770e+02\n",
      " -3.790e+02 -3.800e+02 -3.810e+02 -3.810e+02 -3.820e+02 -3.830e+02\n",
      " -3.830e+02 -3.850e+02 -3.850e+02 -3.850e+02  3.850e+02 -3.850e+02\n",
      " -3.850e+02 -3.920e+02 -3.930e+02 -3.940e+02 -3.940e+02 -3.960e+02\n",
      " -3.960e+02 -3.960e+02 -3.960e+02 -3.970e+02 -3.980e+02 -4.020e+02\n",
      " -4.040e+02 -4.050e+02 -4.080e+02 -4.080e+02 -4.090e+02 -4.100e+02\n",
      " -4.100e+02 -4.110e+02 -4.130e+02 -4.140e+02 -4.160e+02 -4.170e+02\n",
      " -4.180e+02 -4.230e+02 -4.240e+02 -4.240e+02 -4.260e+02 -4.260e+02\n",
      " -4.280e+02 -4.300e+02 -4.310e+02 -4.330e+02 -4.340e+02 -4.350e+02\n",
      " -4.390e+02 -4.400e+02 -4.410e+02 -4.410e+02 -4.460e+02  4.460e+02\n",
      " -4.470e+02 -4.470e+02 -4.480e+02 -4.500e+02 -4.500e+02 -4.550e+02\n",
      " -4.570e+02 -4.610e+02 -4.610e+02 -4.630e+02 -4.630e+02 -4.660e+02\n",
      " -4.660e+02 -4.700e+02 -4.750e+02 -4.760e+02 -4.770e+02 -4.770e+02\n",
      " -4.800e+02 -4.880e+02 -4.880e+02 -4.920e+02 -4.950e+02 -4.960e+02\n",
      " -4.960e+02 -4.990e+02 -4.990e+02 -5.010e+02 -5.030e+02 -5.040e+02\n",
      " -5.060e+02 -5.080e+02 -5.090e+02 -5.100e+02 -5.110e+02 -5.130e+02\n",
      " -5.160e+02 -5.180e+02 -5.180e+02 -5.180e+02 -5.190e+02 -5.210e+02\n",
      " -5.220e+02  5.240e+02 -5.250e+02 -5.260e+02 -5.280e+02 -5.290e+02\n",
      " -5.300e+02 -5.300e+02 -5.310e+02 -5.320e+02 -5.320e+02 -5.320e+02\n",
      " -5.330e+02  5.380e+02 -5.410e+02 -5.410e+02 -5.450e+02 -5.460e+02\n",
      " -5.470e+02  5.480e+02 -5.510e+02 -5.520e+02 -5.520e+02 -5.520e+02\n",
      " -5.540e+02 -5.540e+02 -5.540e+02 -5.540e+02 -5.540e+02  5.580e+02\n",
      " -5.620e+02 -5.620e+02  5.630e+02 -5.650e+02 -5.660e+02  5.710e+02\n",
      " -5.720e+02 -5.740e+02 -5.740e+02 -5.750e+02 -5.750e+02 -5.750e+02\n",
      " -5.760e+02 -5.770e+02 -5.770e+02 -5.770e+02 -5.790e+02 -5.790e+02\n",
      " -5.840e+02 -5.840e+02  5.840e+02 -5.840e+02 -5.850e+02 -5.860e+02\n",
      " -5.880e+02 -5.880e+02 -5.900e+02 -5.910e+02 -5.950e+02 -5.950e+02\n",
      " -5.960e+02 -5.980e+02 -6.000e+02 -6.020e+02 -6.060e+02 -6.070e+02\n",
      " -6.070e+02 -6.080e+02  6.120e+02 -6.120e+02 -6.120e+02 -6.140e+02\n",
      "  6.140e+02  6.160e+02 -6.160e+02 -6.180e+02 -6.200e+02 -6.200e+02\n",
      " -6.220e+02 -6.220e+02 -6.240e+02 -6.260e+02 -6.260e+02 -6.270e+02\n",
      " -6.290e+02 -6.300e+02 -6.310e+02 -6.350e+02 -6.350e+02 -6.390e+02\n",
      " -6.400e+02 -6.400e+02 -6.410e+02 -6.430e+02 -6.430e+02 -6.440e+02\n",
      " -6.460e+02 -6.470e+02 -6.480e+02 -6.510e+02 -6.520e+02 -6.550e+02\n",
      " -6.590e+02 -6.590e+02 -6.600e+02 -6.610e+02 -6.620e+02 -6.640e+02\n",
      " -6.660e+02 -6.660e+02 -6.720e+02 -6.750e+02 -6.750e+02 -6.770e+02\n",
      "  6.780e+02 -6.790e+02 -6.790e+02 -6.800e+02 -6.810e+02 -6.830e+02\n",
      " -6.940e+02 -6.940e+02 -6.980e+02 -7.010e+02 -7.020e+02 -7.030e+02\n",
      " -7.060e+02 -7.070e+02 -7.070e+02 -7.090e+02 -7.100e+02 -7.140e+02\n",
      " -7.150e+02 -7.180e+02 -7.220e+02 -7.240e+02 -7.260e+02 -7.270e+02\n",
      " -7.270e+02 -7.270e+02 -7.340e+02 -7.380e+02 -7.420e+02 -7.470e+02\n",
      " -7.470e+02 -7.480e+02  7.490e+02 -7.540e+02 -7.540e+02 -7.550e+02\n",
      " -7.590e+02 -7.600e+02 -7.600e+02 -7.610e+02 -7.620e+02 -7.620e+02\n",
      "  7.630e+02 -7.670e+02 -7.670e+02 -7.690e+02 -7.770e+02 -7.830e+02\n",
      "  7.850e+02  7.860e+02 -7.880e+02 -7.890e+02 -7.910e+02  7.920e+02\n",
      " -7.920e+02 -7.950e+02 -7.980e+02 -8.030e+02 -8.060e+02  8.110e+02\n",
      " -8.120e+02 -8.200e+02 -8.220e+02  8.250e+02 -8.290e+02 -8.470e+02\n",
      " -8.490e+02 -8.560e+02 -8.580e+02 -8.600e+02 -8.650e+02 -8.650e+02\n",
      " -8.780e+02 -8.780e+02  8.790e+02 -8.810e+02  8.830e+02 -8.830e+02\n",
      " -8.890e+02 -8.900e+02 -8.990e+02  9.040e+02 -9.070e+02 -9.080e+02\n",
      " -9.110e+02 -9.120e+02 -9.150e+02 -9.180e+02  9.210e+02 -9.230e+02\n",
      " -9.310e+02 -9.420e+02  9.430e+02 -9.430e+02  9.590e+02 -9.650e+02\n",
      " -9.650e+02  9.670e+02 -9.680e+02 -9.730e+02 -9.740e+02 -9.750e+02\n",
      "  9.760e+02 -9.840e+02 -9.870e+02 -9.890e+02 -9.900e+02  9.910e+02\n",
      "  9.910e+02 -9.960e+02 -9.970e+02 -1.000e+03 -1.001e+03 -1.001e+03\n",
      "  1.004e+03 -1.004e+03 -1.004e+03 -1.004e+03 -1.005e+03 -1.006e+03\n",
      " -1.007e+03  1.009e+03 -1.009e+03 -1.010e+03 -1.013e+03 -1.025e+03\n",
      " -1.026e+03 -1.026e+03 -1.027e+03  1.034e+03  1.034e+03 -1.034e+03\n",
      " -1.039e+03 -1.042e+03 -1.043e+03 -1.047e+03 -1.051e+03 -1.051e+03\n",
      " -1.059e+03 -1.062e+03 -1.063e+03 -1.066e+03  1.072e+03 -1.074e+03\n",
      " -1.079e+03 -1.085e+03 -1.088e+03 -1.093e+03  1.093e+03 -1.099e+03\n",
      " -1.101e+03 -1.102e+03 -1.106e+03 -1.112e+03 -1.119e+03 -1.120e+03\n",
      " -1.120e+03 -1.121e+03 -1.124e+03 -1.132e+03 -1.133e+03 -1.138e+03\n",
      " -1.140e+03  1.142e+03 -1.148e+03  1.148e+03 -1.148e+03 -1.150e+03\n",
      "  1.152e+03 -1.156e+03 -1.158e+03 -1.161e+03 -1.162e+03 -1.163e+03\n",
      " -1.165e+03 -1.167e+03 -1.174e+03  1.174e+03 -1.185e+03 -1.186e+03\n",
      " -1.189e+03 -1.191e+03 -1.196e+03 -1.198e+03 -1.203e+03 -1.203e+03\n",
      " -1.206e+03 -1.208e+03 -1.208e+03 -1.210e+03 -1.217e+03 -1.219e+03\n",
      " -1.220e+03 -1.224e+03 -1.229e+03 -1.229e+03 -1.232e+03 -1.233e+03\n",
      " -1.234e+03 -1.234e+03 -1.239e+03 -1.246e+03 -1.248e+03 -1.251e+03\n",
      " -1.266e+03 -1.269e+03 -1.270e+03  1.272e+03  1.275e+03 -1.277e+03\n",
      "  1.286e+03 -1.291e+03 -1.301e+03 -1.308e+03 -1.308e+03 -1.308e+03\n",
      " -1.309e+03 -1.321e+03  1.324e+03 -1.325e+03 -1.326e+03 -1.330e+03\n",
      " -1.347e+03 -1.351e+03 -1.359e+03 -1.363e+03 -1.363e+03  1.365e+03\n",
      " -1.369e+03 -1.371e+03 -1.371e+03  1.388e+03  1.411e+03 -1.417e+03\n",
      " -1.417e+03  1.430e+03 -1.434e+03  1.439e+03 -1.449e+03 -1.461e+03\n",
      " -1.463e+03 -1.467e+03 -1.471e+03 -1.474e+03 -1.492e+03 -1.505e+03\n",
      "  1.508e+03 -1.516e+03 -1.522e+03 -1.528e+03 -1.532e+03 -1.534e+03\n",
      " -1.535e+03  1.542e+03 -1.542e+03 -1.545e+03 -1.545e+03 -1.546e+03\n",
      " -1.547e+03 -1.550e+03  1.556e+03 -1.561e+03  1.563e+03 -1.563e+03\n",
      " -1.569e+03 -1.572e+03 -1.587e+03 -1.596e+03 -1.604e+03 -1.604e+03\n",
      " -1.611e+03 -1.611e+03 -1.612e+03 -1.613e+03 -1.614e+03 -1.616e+03\n",
      " -1.631e+03 -1.639e+03 -1.640e+03 -1.642e+03 -1.644e+03 -1.648e+03\n",
      "  1.649e+03 -1.673e+03 -1.673e+03  1.673e+03 -1.683e+03 -1.686e+03\n",
      "  1.688e+03 -1.688e+03  1.692e+03 -1.692e+03  1.694e+03 -1.728e+03\n",
      " -1.732e+03 -1.780e+03  1.781e+03 -1.783e+03  1.793e+03  1.812e+03\n",
      " -1.820e+03 -1.836e+03 -1.842e+03 -1.853e+03 -1.855e+03 -1.864e+03\n",
      " -1.866e+03 -1.871e+03 -1.873e+03 -1.876e+03 -1.882e+03  1.884e+03\n",
      " -1.887e+03 -1.914e+03 -1.919e+03  1.920e+03 -1.925e+03 -1.926e+03\n",
      "  1.927e+03 -1.935e+03 -1.935e+03 -1.953e+03 -1.972e+03 -1.980e+03\n",
      " -1.988e+03  1.993e+03 -1.996e+03  2.009e+03 -2.012e+03 -2.019e+03\n",
      " -2.031e+03 -2.033e+03 -2.041e+03 -2.048e+03 -2.064e+03 -2.082e+03\n",
      " -2.091e+03  2.097e+03 -2.108e+03 -2.109e+03 -2.124e+03  2.127e+03\n",
      " -2.128e+03 -2.140e+03 -2.142e+03 -2.155e+03 -2.156e+03 -2.164e+03\n",
      " -2.184e+03 -2.190e+03 -2.190e+03 -2.191e+03  2.192e+03 -2.193e+03\n",
      " -2.197e+03  2.207e+03 -2.222e+03 -2.231e+03 -2.236e+03 -2.240e+03\n",
      " -2.246e+03 -2.248e+03 -2.255e+03 -2.255e+03 -2.255e+03 -2.263e+03\n",
      "  2.273e+03 -2.281e+03 -2.282e+03 -2.288e+03 -2.306e+03 -2.311e+03\n",
      " -2.329e+03 -2.335e+03  2.361e+03 -2.362e+03 -2.365e+03 -2.371e+03\n",
      " -2.372e+03 -2.372e+03  2.373e+03 -2.381e+03 -2.385e+03 -2.403e+03\n",
      " -2.406e+03  2.417e+03 -2.426e+03 -2.442e+03 -2.442e+03  2.469e+03\n",
      " -2.477e+03  2.483e+03 -2.483e+03 -2.486e+03 -2.489e+03 -2.513e+03\n",
      " -2.515e+03  2.520e+03 -2.541e+03  2.551e+03 -2.559e+03 -2.590e+03\n",
      " -2.596e+03 -2.605e+03 -2.612e+03 -2.629e+03 -2.632e+03  2.636e+03\n",
      " -2.645e+03 -2.645e+03 -2.650e+03 -2.653e+03 -2.707e+03 -2.709e+03\n",
      "  2.712e+03 -2.721e+03 -2.759e+03  2.763e+03 -2.767e+03 -2.770e+03\n",
      "  2.798e+03 -2.813e+03 -2.838e+03  2.854e+03 -2.856e+03  2.866e+03\n",
      " -2.868e+03  2.911e+03 -2.920e+03 -2.953e+03  2.965e+03  2.965e+03\n",
      " -2.971e+03 -2.976e+03 -2.989e+03 -2.991e+03 -3.001e+03 -3.004e+03\n",
      " -3.009e+03 -3.011e+03 -3.017e+03 -3.022e+03 -3.035e+03  3.063e+03\n",
      " -3.088e+03 -3.094e+03 -3.102e+03 -3.121e+03  3.126e+03 -3.128e+03\n",
      " -3.152e+03 -3.159e+03 -3.172e+03 -3.202e+03 -3.203e+03 -3.204e+03\n",
      " -3.226e+03 -3.248e+03 -3.256e+03 -3.261e+03 -3.276e+03 -3.283e+03\n",
      " -3.286e+03 -3.287e+03 -3.296e+03 -3.307e+03 -3.316e+03 -3.342e+03\n",
      " -3.361e+03 -3.364e+03  3.418e+03 -3.430e+03  3.461e+03  3.462e+03\n",
      "  3.492e+03 -3.506e+03 -3.603e+03 -3.607e+03  3.669e+03  3.736e+03\n",
      " -3.807e+03  3.873e+03  3.926e+03  3.941e+03  3.945e+03 -3.957e+03\n",
      "  3.959e+03 -4.005e+03 -4.047e+03 -4.080e+03 -4.159e+03 -4.233e+03\n",
      "  4.267e+03 -4.275e+03 -4.285e+03 -4.354e+03 -4.361e+03 -4.894e+03\n",
      " -4.929e+03 -5.156e+03 -5.176e+03 -5.739e+03 -6.292e+03  6.456e+03\n",
      "  6.593e+03 -7.106e+03 -7.126e+03 -7.777e+03 -8.391e+03 -8.556e+03\n",
      " -8.605e+03]\n",
      "durations 5.0 8008.0\n",
      "Concordance Index 0.5140644109253975\n",
      "Integrated Brier Score: 0.20394792409957263\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m77.7088\u001b[0m       \u001b[32m80.9302\u001b[0m  0.0798\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m178.6639\u001b[0m       \u001b[32m99.7606\u001b[0m  0.0388\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m165.8774\u001b[0m       \u001b[32m82.5237\u001b[0m  0.0385\n",
      "      2       \u001b[36m72.6548\u001b[0m       \u001b[32m59.4217\u001b[0m  0.0676\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m197.0850\u001b[0m      \u001b[32m112.0678\u001b[0m  0.0370\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m88.4737\u001b[0m      \u001b[32m121.1075\u001b[0m  0.0604\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m73.0249\u001b[0m       \u001b[32m81.4008\u001b[0m  0.0568\n",
      "      2      \u001b[36m158.1637\u001b[0m       \u001b[32m76.1021\u001b[0m  0.0340\n",
      "      2      \u001b[36m170.7580\u001b[0m      100.5347  0.0354\n",
      "      2      \u001b[36m186.8407\u001b[0m      \u001b[32m107.6522\u001b[0m  0.0368\n",
      "      3       \u001b[36m69.9213\u001b[0m       \u001b[32m58.3148\u001b[0m  0.0515\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      3      \u001b[36m152.9066\u001b[0m       \u001b[32m73.5108\u001b[0m  0.0335\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m59.6790\u001b[0m       \u001b[32m67.8409\u001b[0m  0.0745\n",
      "      3      \u001b[36m167.4543\u001b[0m       \u001b[32m99.4253\u001b[0m  0.0390\n",
      "      2       \u001b[36m83.3473\u001b[0m       \u001b[32m89.1032\u001b[0m  0.0623\n",
      "      3      \u001b[36m180.7175\u001b[0m      \u001b[32m106.6761\u001b[0m  0.0303\n",
      "      2       \u001b[36m70.7274\u001b[0m       \u001b[32m57.5298\u001b[0m  0.0603\n",
      "      4      \u001b[36m152.6905\u001b[0m       \u001b[32m73.0355\u001b[0m  0.0282\n",
      "      4      \u001b[36m164.0857\u001b[0m       99.9955  0.0380\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m143.8033\u001b[0m       \u001b[32m73.0523\u001b[0m  0.0390\n",
      "      4      \u001b[36m179.9778\u001b[0m      107.1365  0.0372\n",
      "      4       \u001b[36m69.7085\u001b[0m       62.2940  0.0644\n",
      "      3       \u001b[36m80.0715\u001b[0m       \u001b[32m89.0246\u001b[0m  0.0447\n",
      "      2       \u001b[36m59.1579\u001b[0m       \u001b[32m62.9863\u001b[0m  0.0609\n",
      "      5      \u001b[36m151.3020\u001b[0m       73.5573  0.0393\n",
      "      2      \u001b[36m136.8071\u001b[0m       73.6668  0.0316\n",
      "      3       71.6966       60.4516  0.0540\n",
      "      5      \u001b[36m162.5877\u001b[0m      101.6007  0.0366\n",
      "      5      \u001b[36m178.0132\u001b[0m      108.4147  0.0419\n",
      "      6      \u001b[36m150.8553\u001b[0m       73.8828  0.0320\n",
      "      4       \u001b[36m79.2475\u001b[0m       92.8446  0.0524\n",
      "      5       \u001b[36m68.4058\u001b[0m       61.3987  0.0593\n",
      "      3      \u001b[36m133.0676\u001b[0m       73.6084  0.0421\n",
      "      6      \u001b[36m162.2136\u001b[0m      102.0918  0.0388\n",
      "      3       \u001b[36m55.0774\u001b[0m       \u001b[32m62.7100\u001b[0m  0.0550\n",
      "      6      179.3282      109.4895  0.0340\n",
      "      4       \u001b[36m68.9672\u001b[0m       62.9172  0.0562\n",
      "      7      \u001b[36m149.0763\u001b[0m       74.0017  0.0327\n",
      "      4      \u001b[36m129.2568\u001b[0m       73.8201  0.0322\n",
      "      7      \u001b[36m161.8524\u001b[0m      101.8535  0.0351\n",
      "      6       \u001b[36m66.1708\u001b[0m       61.8600  0.0465\n",
      "      5       \u001b[36m77.8690\u001b[0m       93.3541  0.0529\n",
      "      7      \u001b[36m176.7136\u001b[0m      109.2828  0.0366\n",
      "      8      151.8931       74.6189  0.0321\n",
      "      4       55.9341       \u001b[32m62.2486\u001b[0m  0.0494\n",
      "      5      \u001b[36m128.0438\u001b[0m       74.4055  0.0332\n",
      "      8      \u001b[36m160.7626\u001b[0m      101.9052  0.0357\n",
      "      8      \u001b[36m176.2027\u001b[0m      108.9843  0.0340\n",
      "      5       \u001b[36m67.1991\u001b[0m       59.7207  0.0670\n",
      "      9      \u001b[36m147.0641\u001b[0m       75.2065  0.0350\n",
      "      7       67.0232       63.2560  0.0563\n",
      "      6      \u001b[36m126.5785\u001b[0m       74.9845  0.0323\n",
      "      6       \u001b[36m76.3320\u001b[0m       92.4138  0.0611\n",
      "      9      \u001b[36m156.1440\u001b[0m      102.4687  0.0357\n",
      "      5       56.5040       \u001b[32m59.5229\u001b[0m  0.0626\n",
      "     10      148.7509       76.0068  0.0383\n",
      "Restoring best model from epoch 4.\n",
      "      7      \u001b[36m126.0912\u001b[0m       74.5788  0.0308\n",
      "      9      \u001b[36m173.7271\u001b[0m      109.2214  0.0475\n",
      "      6       \u001b[36m64.3300\u001b[0m       61.0551  0.0562\n",
      "     10      159.2342      103.3649  0.0349\n",
      "Restoring best model from epoch 3.\n",
      "      8       \u001b[36m65.2536\u001b[0m       59.7354  0.0560\n",
      "     10      176.3874      109.6166  0.0293\n",
      "Restoring best model from epoch 3.\n",
      "      7       77.0768       92.0671  0.0538\n",
      "      8      \u001b[36m125.3433\u001b[0m       74.4884  0.0317\n",
      "      6       \u001b[36m52.9590\u001b[0m       \u001b[32m57.7950\u001b[0m  0.0547\n",
      "      9      126.6147       74.5485  0.0276\n",
      "      7       64.7315       61.3313  0.0542\n",
      "      9       67.0520       59.1986  0.0471\n",
      "     10      \u001b[36m124.9203\u001b[0m       75.1318  0.0264\n",
      "Restoring best model from epoch 1.\n",
      "      7       53.7003       58.3382  0.0471\n",
      "      8       \u001b[36m75.5344\u001b[0m       90.7016  0.0637\n",
      "     10       65.3160       59.7630  0.0440\n",
      "      8       \u001b[36m63.3600\u001b[0m       60.8428  0.0522\n",
      "Restoring best model from epoch 3.\n",
      "      8       \u001b[36m51.8673\u001b[0m       59.9283  0.0482\n",
      "      9       \u001b[36m74.7935\u001b[0m       90.9975  0.0531\n",
      "      9       \u001b[36m62.8220\u001b[0m       59.2391  0.0554\n",
      "      9       \u001b[36m50.8012\u001b[0m       62.2529  0.0413\n",
      "     10       \u001b[36m72.6406\u001b[0m       91.7895  0.0453\n",
      "Restoring best model from epoch 3.\n",
      "     10       \u001b[36m61.7438\u001b[0m       59.4608  0.0431\n",
      "Restoring best model from epoch 2.\n",
      "     10       53.3359       60.5556  0.0445\n",
      "Restoring best model from epoch 6.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m173.4735\u001b[0m       \u001b[32m72.1027\u001b[0m  0.0317\n",
      "      2      \u001b[36m164.6952\u001b[0m       \u001b[32m70.9500\u001b[0m  0.0282\n",
      "      3      \u001b[36m159.5668\u001b[0m       73.4220  0.0280\n",
      "      4      \u001b[36m158.6242\u001b[0m       74.8179  0.0271\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      5      \u001b[36m154.5895\u001b[0m       75.1273  0.0259\n",
      "      6      157.6579       74.6397  0.0268\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m81.1869\u001b[0m       \u001b[32m99.0482\u001b[0m  0.0454\n",
      "      7      154.7432       73.3819  0.0271\n",
      "      2       \u001b[36m71.9950\u001b[0m       \u001b[32m90.8804\u001b[0m  0.0377\n",
      "      8      158.0337       73.5164  0.0265\n",
      "      9      \u001b[36m152.5069\u001b[0m       73.7067  0.0263\n",
      "      3       \u001b[36m69.0084\u001b[0m       91.7381  0.0386\n",
      "     10      \u001b[36m152.4570\u001b[0m       74.3969  0.0253\n",
      "Restoring best model from epoch 2.\n",
      "      4       69.9140       94.3057  0.0377\n",
      "      5       71.2868       \u001b[32m89.4275\u001b[0m  0.0376\n",
      "      6       \u001b[36m67.1792\u001b[0m       \u001b[32m86.2747\u001b[0m  0.0382\n",
      "      7       69.4328       86.9136  0.0379\n",
      "      8       \u001b[36m66.8696\u001b[0m       \u001b[32m86.1730\u001b[0m  0.0370\n",
      "      9       68.3048       \u001b[32m85.0417\u001b[0m  0.0369\n",
      "     10       66.9958       86.3872  0.0368\n",
      "Restoring best model from epoch 9.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m161.7629\u001b[0m      \u001b[32m113.3477\u001b[0m  0.0297\n",
      "      2      \u001b[36m152.3196\u001b[0m      \u001b[32m112.6906\u001b[0m  0.0292\n",
      "      3      153.1300      113.6892  0.0293\n",
      "      4      \u001b[36m147.1254\u001b[0m      115.0590  0.0303\n",
      "      5      \u001b[36m144.8454\u001b[0m      116.0886  0.0280\n",
      "      6      146.1888      116.0292  0.0283\n",
      "      7      145.1855      115.4895  0.0280\n",
      "      8      146.4310      114.9683  0.0305\n",
      "      9      \u001b[36m142.9887\u001b[0m      114.7336  0.0312\n",
      "     10      143.2015      114.8688  0.0314\n",
      "Restoring best model from epoch 2.\n",
      "y_train breslow final [-1.000e+00 -1.000e+00 -5.000e+00 -5.000e+00 -7.000e+00 -8.000e+00\n",
      " -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01\n",
      " -1.000e+01 -1.000e+01 -1.000e+01 -1.100e+01 -1.600e+01 -2.100e+01\n",
      " -2.400e+01 -2.600e+01 -3.000e+01 -3.000e+01 -3.000e+01 -3.100e+01\n",
      " -3.100e+01 -3.100e+01 -3.400e+01 -3.400e+01 -4.000e+01 -4.900e+01\n",
      " -5.100e+01 -5.200e+01 -5.400e+01 -5.900e+01 -6.400e+01 -7.000e+01\n",
      " -7.200e+01 -7.800e+01 -7.800e+01 -7.900e+01 -8.000e+01 -8.400e+01\n",
      " -9.000e+01 -9.200e+01 -9.800e+01  1.160e+02 -1.180e+02 -1.340e+02\n",
      "  1.580e+02  1.600e+02 -1.620e+02 -1.630e+02 -1.700e+02 -1.700e+02\n",
      "  1.720e+02 -1.720e+02  1.740e+02 -1.860e+02 -1.870e+02 -1.960e+02\n",
      "  1.970e+02 -2.000e+02 -2.100e+02 -2.130e+02 -2.140e+02 -2.150e+02\n",
      " -2.160e+02 -2.170e+02 -2.180e+02 -2.240e+02 -2.250e+02  2.270e+02\n",
      " -2.270e+02 -2.310e+02  2.390e+02 -2.420e+02 -2.420e+02 -2.430e+02\n",
      " -2.500e+02 -2.520e+02 -2.580e+02 -2.590e+02 -2.660e+02  2.660e+02\n",
      " -2.730e+02 -2.730e+02 -2.740e+02 -2.870e+02 -2.880e+02 -2.930e+02\n",
      " -2.930e+02  2.950e+02 -2.970e+02 -3.000e+02 -3.020e+02 -3.030e+02\n",
      " -3.030e+02 -3.040e+02 -3.040e+02  3.040e+02 -3.070e+02 -3.130e+02\n",
      " -3.130e+02 -3.170e+02  3.200e+02 -3.200e+02  3.220e+02 -3.220e+02\n",
      " -3.220e+02 -3.230e+02 -3.260e+02 -3.280e+02 -3.340e+02  3.360e+02\n",
      " -3.360e+02 -3.370e+02 -3.390e+02 -3.430e+02 -3.430e+02 -3.450e+02\n",
      " -3.450e+02 -3.470e+02  3.480e+02 -3.500e+02 -3.520e+02 -3.580e+02\n",
      " -3.580e+02 -3.600e+02  3.620e+02 -3.630e+02 -3.650e+02 -3.650e+02\n",
      "  3.650e+02 -3.650e+02 -3.650e+02 -3.660e+02 -3.660e+02 -3.680e+02\n",
      " -3.700e+02 -3.710e+02 -3.730e+02 -3.750e+02 -3.750e+02 -3.750e+02\n",
      " -3.760e+02 -3.760e+02  3.770e+02 -3.790e+02 -3.800e+02 -3.800e+02\n",
      " -3.810e+02 -3.810e+02 -3.820e+02  3.850e+02 -3.850e+02 -3.850e+02\n",
      " -3.850e+02  3.850e+02 -3.850e+02 -3.850e+02 -3.920e+02 -3.930e+02\n",
      " -3.940e+02 -3.940e+02 -3.950e+02 -3.960e+02 -3.960e+02 -3.960e+02\n",
      " -4.020e+02 -4.030e+02 -4.030e+02 -4.040e+02 -4.050e+02 -4.080e+02\n",
      " -4.090e+02 -4.100e+02 -4.100e+02 -4.120e+02 -4.130e+02 -4.160e+02\n",
      " -4.170e+02 -4.180e+02 -4.210e+02 -4.230e+02 -4.240e+02 -4.240e+02\n",
      " -4.250e+02 -4.260e+02  4.260e+02 -4.260e+02 -4.280e+02 -4.300e+02\n",
      " -4.310e+02 -4.310e+02 -4.330e+02 -4.340e+02 -4.350e+02 -4.370e+02\n",
      " -4.390e+02 -4.390e+02 -4.390e+02 -4.400e+02 -4.410e+02 -4.410e+02\n",
      "  4.460e+02 -4.470e+02 -4.470e+02 -4.470e+02 -4.480e+02 -4.480e+02\n",
      " -4.500e+02 -4.510e+02 -4.540e+02 -4.560e+02 -4.570e+02 -4.580e+02\n",
      " -4.610e+02 -4.630e+02 -4.630e+02 -4.660e+02 -4.660e+02 -4.670e+02\n",
      " -4.710e+02 -4.720e+02 -4.750e+02 -4.760e+02 -4.770e+02 -4.770e+02\n",
      " -4.770e+02 -4.820e+02 -4.880e+02 -4.890e+02 -4.960e+02 -5.010e+02\n",
      " -5.010e+02 -5.020e+02 -5.030e+02 -5.040e+02 -5.040e+02 -5.110e+02\n",
      " -5.130e+02 -5.160e+02 -5.180e+02 -5.190e+02 -5.190e+02 -5.210e+02\n",
      " -5.220e+02 -5.230e+02 -5.250e+02 -5.280e+02 -5.290e+02 -5.300e+02\n",
      " -5.300e+02 -5.310e+02 -5.320e+02 -5.320e+02 -5.320e+02 -5.330e+02\n",
      " -5.330e+02 -5.380e+02 -5.380e+02 -5.410e+02 -5.410e+02 -5.450e+02\n",
      " -5.460e+02 -5.470e+02  5.480e+02 -5.490e+02 -5.520e+02 -5.520e+02\n",
      " -5.540e+02 -5.540e+02 -5.540e+02 -5.540e+02 -5.540e+02  5.580e+02\n",
      " -5.620e+02 -5.620e+02 -5.630e+02 -5.660e+02 -5.670e+02 -5.680e+02\n",
      " -5.720e+02  5.730e+02 -5.740e+02 -5.740e+02 -5.750e+02 -5.750e+02\n",
      " -5.750e+02 -5.760e+02 -5.770e+02 -5.770e+02 -5.770e+02 -5.790e+02\n",
      " -5.790e+02 -5.840e+02  5.840e+02 -5.860e+02 -5.880e+02 -5.880e+02\n",
      " -5.900e+02 -5.910e+02 -5.940e+02 -5.950e+02 -5.950e+02 -5.980e+02\n",
      " -6.000e+02 -6.020e+02 -6.070e+02 -6.070e+02 -6.080e+02 -6.080e+02\n",
      " -6.110e+02 -6.110e+02 -6.120e+02 -6.120e+02 -6.140e+02 -6.160e+02\n",
      " -6.160e+02 -6.180e+02 -6.200e+02 -6.200e+02 -6.220e+02 -6.260e+02\n",
      " -6.260e+02 -6.270e+02 -6.290e+02 -6.350e+02 -6.350e+02 -6.350e+02\n",
      " -6.390e+02  6.390e+02 -6.400e+02 -6.410e+02 -6.460e+02 -6.470e+02\n",
      " -6.480e+02 -6.510e+02 -6.550e+02 -6.580e+02 -6.590e+02 -6.590e+02\n",
      " -6.600e+02 -6.610e+02 -6.640e+02 -6.660e+02 -6.660e+02 -6.750e+02\n",
      " -6.770e+02 -6.770e+02  6.780e+02 -6.790e+02 -6.790e+02 -6.800e+02\n",
      " -6.810e+02 -6.830e+02 -6.940e+02 -6.980e+02 -7.010e+02 -7.020e+02\n",
      " -7.030e+02 -7.060e+02 -7.070e+02 -7.070e+02 -7.090e+02 -7.140e+02\n",
      " -7.150e+02 -7.150e+02 -7.180e+02 -7.220e+02  7.230e+02 -7.250e+02\n",
      " -7.260e+02 -7.270e+02 -7.280e+02 -7.340e+02 -7.380e+02 -7.450e+02\n",
      " -7.470e+02 -7.470e+02 -7.480e+02  7.490e+02 -7.520e+02 -7.520e+02\n",
      "  7.540e+02 -7.540e+02 -7.540e+02 -7.550e+02 -7.590e+02 -7.590e+02\n",
      " -7.600e+02 -7.620e+02 -7.620e+02  7.630e+02 -7.650e+02 -7.670e+02\n",
      " -7.770e+02 -7.830e+02 -7.850e+02  7.850e+02  7.860e+02 -7.880e+02\n",
      " -7.890e+02  7.920e+02  7.920e+02 -7.920e+02 -7.950e+02 -7.980e+02\n",
      " -8.060e+02 -8.120e+02 -8.190e+02 -8.200e+02  8.210e+02  8.250e+02\n",
      " -8.290e+02 -8.370e+02 -8.390e+02 -8.470e+02 -8.490e+02 -8.520e+02\n",
      " -8.560e+02 -8.580e+02 -8.600e+02  8.600e+02 -8.650e+02 -8.650e+02\n",
      " -8.670e+02 -8.750e+02 -8.780e+02 -8.780e+02  8.790e+02 -8.810e+02\n",
      "  8.830e+02 -8.830e+02 -8.990e+02  9.040e+02 -9.060e+02 -9.070e+02\n",
      " -9.080e+02  9.120e+02 -9.120e+02 -9.120e+02 -9.150e+02 -9.180e+02\n",
      "  9.210e+02 -9.260e+02 -9.310e+02 -9.310e+02 -9.420e+02 -9.430e+02\n",
      "  9.430e+02 -9.430e+02 -9.430e+02 -9.520e+02 -9.540e+02  9.590e+02\n",
      " -9.650e+02 -9.650e+02  9.670e+02 -9.680e+02 -9.720e+02 -9.740e+02\n",
      " -9.740e+02 -9.750e+02 -9.750e+02 -9.840e+02 -9.870e+02 -9.890e+02\n",
      " -9.900e+02  9.910e+02  9.910e+02 -9.960e+02 -9.960e+02 -9.970e+02\n",
      " -1.001e+03 -1.001e+03 -1.004e+03 -1.004e+03 -1.004e+03 -1.005e+03\n",
      "  1.009e+03 -1.009e+03 -1.010e+03 -1.013e+03 -1.015e+03 -1.025e+03\n",
      " -1.026e+03 -1.026e+03 -1.027e+03  1.032e+03  1.034e+03 -1.034e+03\n",
      " -1.035e+03 -1.039e+03 -1.042e+03 -1.043e+03 -1.043e+03 -1.043e+03\n",
      " -1.047e+03  1.048e+03 -1.051e+03 -1.051e+03 -1.051e+03 -1.059e+03\n",
      " -1.062e+03 -1.062e+03 -1.063e+03 -1.066e+03  1.072e+03 -1.074e+03\n",
      " -1.079e+03 -1.085e+03 -1.093e+03 -1.099e+03 -1.101e+03 -1.102e+03\n",
      "  1.104e+03 -1.106e+03 -1.112e+03 -1.119e+03 -1.120e+03 -1.121e+03\n",
      " -1.124e+03  1.127e+03 -1.133e+03 -1.139e+03 -1.140e+03 -1.141e+03\n",
      "  1.142e+03 -1.148e+03  1.148e+03 -1.148e+03  1.152e+03 -1.156e+03\n",
      " -1.156e+03 -1.157e+03 -1.158e+03 -1.161e+03 -1.165e+03 -1.167e+03\n",
      " -1.174e+03  1.174e+03 -1.179e+03 -1.185e+03 -1.186e+03 -1.191e+03\n",
      " -1.196e+03 -1.198e+03 -1.203e+03 -1.203e+03 -1.206e+03 -1.208e+03\n",
      " -1.208e+03 -1.210e+03 -1.217e+03 -1.219e+03 -1.220e+03 -1.220e+03\n",
      " -1.224e+03 -1.232e+03 -1.233e+03 -1.234e+03 -1.234e+03 -1.246e+03\n",
      " -1.247e+03 -1.248e+03 -1.251e+03 -1.266e+03 -1.269e+03 -1.270e+03\n",
      "  1.272e+03 -1.285e+03 -1.288e+03 -1.291e+03 -1.301e+03 -1.308e+03\n",
      " -1.308e+03 -1.308e+03 -1.309e+03 -1.309e+03 -1.318e+03 -1.325e+03\n",
      " -1.326e+03 -1.330e+03 -1.347e+03 -1.359e+03 -1.363e+03 -1.369e+03\n",
      " -1.371e+03 -1.371e+03  1.388e+03  1.411e+03 -1.417e+03 -1.417e+03\n",
      " -1.419e+03  1.430e+03 -1.437e+03  1.439e+03 -1.448e+03 -1.449e+03\n",
      " -1.461e+03 -1.463e+03 -1.467e+03 -1.471e+03 -1.474e+03 -1.474e+03\n",
      " -1.476e+03 -1.477e+03 -1.505e+03 -1.516e+03 -1.519e+03 -1.523e+03\n",
      " -1.532e+03 -1.534e+03 -1.535e+03  1.542e+03 -1.542e+03 -1.545e+03\n",
      " -1.546e+03 -1.547e+03 -1.548e+03 -1.550e+03  1.556e+03 -1.561e+03\n",
      "  1.563e+03 -1.563e+03 -1.569e+03 -1.572e+03 -1.587e+03 -1.596e+03\n",
      " -1.604e+03 -1.611e+03 -1.611e+03 -1.614e+03 -1.616e+03 -1.620e+03\n",
      " -1.631e+03 -1.639e+03 -1.640e+03  1.642e+03 -1.644e+03 -1.648e+03\n",
      "  1.649e+03 -1.662e+03 -1.673e+03 -1.673e+03  1.673e+03 -1.682e+03\n",
      " -1.683e+03  1.688e+03 -1.688e+03 -1.692e+03  1.694e+03  1.699e+03\n",
      " -1.712e+03 -1.728e+03 -1.728e+03 -1.732e+03 -1.742e+03  1.759e+03\n",
      "  1.793e+03 -1.800e+03 -1.820e+03 -1.836e+03 -1.847e+03 -1.855e+03\n",
      " -1.864e+03 -1.866e+03 -1.871e+03 -1.873e+03 -1.876e+03 -1.882e+03\n",
      "  1.884e+03 -1.887e+03  1.900e+03 -1.919e+03  1.920e+03 -1.925e+03\n",
      " -1.926e+03  1.927e+03 -1.928e+03 -1.935e+03 -1.935e+03 -1.972e+03\n",
      " -1.988e+03  1.993e+03 -1.996e+03 -2.012e+03 -2.019e+03 -2.031e+03\n",
      " -2.033e+03 -2.041e+03 -2.048e+03 -2.064e+03 -2.072e+03 -2.082e+03\n",
      " -2.091e+03  2.097e+03 -2.108e+03 -2.128e+03 -2.136e+03 -2.155e+03\n",
      " -2.156e+03 -2.161e+03 -2.164e+03 -2.184e+03 -2.190e+03 -2.190e+03\n",
      " -2.191e+03  2.192e+03 -2.193e+03  2.207e+03 -2.222e+03 -2.231e+03\n",
      " -2.236e+03 -2.240e+03 -2.246e+03 -2.255e+03 -2.255e+03 -2.263e+03\n",
      "  2.273e+03 -2.278e+03 -2.281e+03 -2.282e+03 -2.288e+03 -2.289e+03\n",
      "  2.296e+03 -2.311e+03 -2.330e+03  2.348e+03  2.361e+03 -2.362e+03\n",
      " -2.371e+03 -2.372e+03  2.373e+03 -2.381e+03 -2.403e+03 -2.406e+03\n",
      "  2.417e+03 -2.426e+03 -2.442e+03  2.469e+03 -2.477e+03  2.483e+03\n",
      " -2.483e+03 -2.486e+03 -2.489e+03  2.520e+03  2.534e+03 -2.535e+03\n",
      " -2.541e+03  2.551e+03 -2.558e+03 -2.559e+03  2.573e+03 -2.590e+03\n",
      " -2.596e+03 -2.596e+03 -2.605e+03 -2.612e+03 -2.618e+03 -2.629e+03\n",
      " -2.632e+03 -2.632e+03  2.636e+03 -2.650e+03 -2.653e+03 -2.654e+03\n",
      " -2.695e+03 -2.707e+03  2.712e+03 -2.721e+03  2.763e+03 -2.767e+03\n",
      "  2.798e+03 -2.813e+03 -2.838e+03  2.854e+03 -2.856e+03  2.866e+03\n",
      " -2.868e+03 -2.953e+03  2.965e+03  2.965e+03 -2.971e+03 -2.976e+03\n",
      " -2.989e+03 -2.991e+03 -2.991e+03 -3.001e+03 -3.015e+03 -3.017e+03\n",
      " -3.021e+03 -3.022e+03 -3.030e+03 -3.035e+03 -3.072e+03 -3.088e+03\n",
      " -3.091e+03 -3.094e+03 -3.112e+03 -3.121e+03  3.126e+03 -3.128e+03\n",
      " -3.152e+03 -3.159e+03 -3.172e+03 -3.203e+03 -3.226e+03 -3.247e+03\n",
      " -3.248e+03 -3.256e+03 -3.261e+03  3.262e+03 -3.276e+03 -3.283e+03\n",
      " -3.283e+03 -3.286e+03 -3.296e+03 -3.316e+03 -3.361e+03 -3.364e+03\n",
      "  3.409e+03  3.418e+03 -3.430e+03 -3.456e+03  3.461e+03  3.462e+03\n",
      "  3.472e+03  3.492e+03 -3.506e+03 -3.519e+03 -3.607e+03 -3.660e+03\n",
      "  3.669e+03 -3.709e+03  3.926e+03  3.941e+03  3.945e+03 -3.957e+03\n",
      " -4.052e+03 -4.080e+03 -4.088e+03 -4.159e+03 -4.233e+03  4.267e+03\n",
      " -4.275e+03 -4.285e+03 -4.354e+03  4.456e+03 -4.894e+03 -5.042e+03\n",
      " -5.062e+03 -5.176e+03 -5.739e+03 -6.292e+03  6.456e+03  6.593e+03\n",
      " -7.106e+03 -7.126e+03  7.455e+03 -7.777e+03 -8.008e+03 -8.391e+03\n",
      " -8.605e+03]\n",
      "Concordance Index 0.651966135409844\n",
      "Integrated Brier Score: 0.21040946171849834\n",
      "y_train breslow final [-1.000e+00 -1.000e+00 -5.000e+00 -5.000e+00 -7.000e+00 -8.000e+00\n",
      " -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01\n",
      " -1.000e+01 -1.000e+01 -1.000e+01 -1.100e+01 -1.600e+01 -2.100e+01\n",
      " -2.400e+01 -2.600e+01 -3.000e+01 -3.000e+01 -3.000e+01 -3.100e+01\n",
      " -3.100e+01 -3.100e+01 -3.400e+01 -3.400e+01 -4.000e+01 -4.900e+01\n",
      " -5.100e+01 -5.200e+01 -5.400e+01 -5.900e+01 -6.400e+01 -7.000e+01\n",
      " -7.200e+01 -7.800e+01 -7.800e+01 -7.900e+01 -8.000e+01 -8.400e+01\n",
      " -9.000e+01 -9.200e+01 -9.800e+01  1.160e+02 -1.180e+02 -1.340e+02\n",
      "  1.580e+02  1.600e+02 -1.620e+02 -1.630e+02 -1.700e+02 -1.700e+02\n",
      "  1.720e+02 -1.720e+02  1.740e+02 -1.860e+02 -1.870e+02 -1.960e+02\n",
      "  1.970e+02 -2.000e+02 -2.100e+02 -2.130e+02 -2.140e+02 -2.150e+02\n",
      " -2.160e+02 -2.170e+02 -2.180e+02 -2.240e+02 -2.250e+02  2.270e+02\n",
      " -2.270e+02 -2.310e+02  2.390e+02 -2.420e+02 -2.420e+02 -2.430e+02\n",
      " -2.500e+02 -2.520e+02 -2.580e+02 -2.590e+02 -2.660e+02  2.660e+02\n",
      " -2.730e+02 -2.730e+02 -2.740e+02 -2.870e+02 -2.880e+02 -2.930e+02\n",
      " -2.930e+02  2.950e+02 -2.970e+02 -3.000e+02 -3.020e+02 -3.030e+02\n",
      " -3.030e+02 -3.040e+02 -3.040e+02  3.040e+02 -3.070e+02 -3.130e+02\n",
      " -3.130e+02 -3.170e+02  3.200e+02 -3.200e+02  3.220e+02 -3.220e+02\n",
      " -3.220e+02 -3.230e+02 -3.260e+02 -3.280e+02 -3.340e+02  3.360e+02\n",
      " -3.360e+02 -3.370e+02 -3.390e+02 -3.430e+02 -3.430e+02 -3.450e+02\n",
      " -3.450e+02 -3.470e+02  3.480e+02 -3.500e+02 -3.520e+02 -3.580e+02\n",
      " -3.580e+02 -3.600e+02  3.620e+02 -3.630e+02 -3.650e+02 -3.650e+02\n",
      "  3.650e+02 -3.650e+02 -3.650e+02 -3.660e+02 -3.660e+02 -3.680e+02\n",
      " -3.700e+02 -3.710e+02 -3.730e+02 -3.750e+02 -3.750e+02 -3.750e+02\n",
      " -3.760e+02 -3.760e+02  3.770e+02 -3.790e+02 -3.800e+02 -3.800e+02\n",
      " -3.810e+02 -3.810e+02 -3.820e+02  3.850e+02 -3.850e+02 -3.850e+02\n",
      " -3.850e+02  3.850e+02 -3.850e+02 -3.850e+02 -3.920e+02 -3.930e+02\n",
      " -3.940e+02 -3.940e+02 -3.950e+02 -3.960e+02 -3.960e+02 -3.960e+02\n",
      " -4.020e+02 -4.030e+02 -4.030e+02 -4.040e+02 -4.050e+02 -4.080e+02\n",
      " -4.090e+02 -4.100e+02 -4.100e+02 -4.120e+02 -4.130e+02 -4.160e+02\n",
      " -4.170e+02 -4.180e+02 -4.210e+02 -4.230e+02 -4.240e+02 -4.240e+02\n",
      " -4.250e+02 -4.260e+02  4.260e+02 -4.260e+02 -4.280e+02 -4.300e+02\n",
      " -4.310e+02 -4.310e+02 -4.330e+02 -4.340e+02 -4.350e+02 -4.370e+02\n",
      " -4.390e+02 -4.390e+02 -4.390e+02 -4.400e+02 -4.410e+02 -4.410e+02\n",
      "  4.460e+02 -4.470e+02 -4.470e+02 -4.470e+02 -4.480e+02 -4.480e+02\n",
      " -4.500e+02 -4.510e+02 -4.540e+02 -4.560e+02 -4.570e+02 -4.580e+02\n",
      " -4.610e+02 -4.630e+02 -4.630e+02 -4.660e+02 -4.660e+02 -4.670e+02\n",
      " -4.710e+02 -4.720e+02 -4.750e+02 -4.760e+02 -4.770e+02 -4.770e+02\n",
      " -4.770e+02 -4.820e+02 -4.880e+02 -4.890e+02 -4.960e+02 -5.010e+02\n",
      " -5.010e+02 -5.020e+02 -5.030e+02 -5.040e+02 -5.040e+02 -5.110e+02\n",
      " -5.130e+02 -5.160e+02 -5.180e+02 -5.190e+02 -5.190e+02 -5.210e+02\n",
      " -5.220e+02 -5.230e+02 -5.250e+02 -5.280e+02 -5.290e+02 -5.300e+02\n",
      " -5.300e+02 -5.310e+02 -5.320e+02 -5.320e+02 -5.320e+02 -5.330e+02\n",
      " -5.330e+02 -5.380e+02 -5.380e+02 -5.410e+02 -5.410e+02 -5.450e+02\n",
      " -5.460e+02 -5.470e+02  5.480e+02 -5.490e+02 -5.520e+02 -5.520e+02\n",
      " -5.540e+02 -5.540e+02 -5.540e+02 -5.540e+02 -5.540e+02  5.580e+02\n",
      " -5.620e+02 -5.620e+02 -5.630e+02 -5.660e+02 -5.670e+02 -5.680e+02\n",
      " -5.720e+02  5.730e+02 -5.740e+02 -5.740e+02 -5.750e+02 -5.750e+02\n",
      " -5.750e+02 -5.760e+02 -5.770e+02 -5.770e+02 -5.770e+02 -5.790e+02\n",
      " -5.790e+02 -5.840e+02  5.840e+02 -5.860e+02 -5.880e+02 -5.880e+02\n",
      " -5.900e+02 -5.910e+02 -5.940e+02 -5.950e+02 -5.950e+02 -5.980e+02\n",
      " -6.000e+02 -6.020e+02 -6.070e+02 -6.070e+02 -6.080e+02 -6.080e+02\n",
      " -6.110e+02 -6.110e+02 -6.120e+02 -6.120e+02 -6.140e+02 -6.160e+02\n",
      " -6.160e+02 -6.180e+02 -6.200e+02 -6.200e+02 -6.220e+02 -6.260e+02\n",
      " -6.260e+02 -6.270e+02 -6.290e+02 -6.350e+02 -6.350e+02 -6.350e+02\n",
      " -6.390e+02  6.390e+02 -6.400e+02 -6.410e+02 -6.460e+02 -6.470e+02\n",
      " -6.480e+02 -6.510e+02 -6.550e+02 -6.580e+02 -6.590e+02 -6.590e+02\n",
      " -6.600e+02 -6.610e+02 -6.640e+02 -6.660e+02 -6.660e+02 -6.750e+02\n",
      " -6.770e+02 -6.770e+02  6.780e+02 -6.790e+02 -6.790e+02 -6.800e+02\n",
      " -6.810e+02 -6.830e+02 -6.940e+02 -6.980e+02 -7.010e+02 -7.020e+02\n",
      " -7.030e+02 -7.060e+02 -7.070e+02 -7.070e+02 -7.090e+02 -7.140e+02\n",
      " -7.150e+02 -7.150e+02 -7.180e+02 -7.220e+02  7.230e+02 -7.250e+02\n",
      " -7.260e+02 -7.270e+02 -7.280e+02 -7.340e+02 -7.380e+02 -7.450e+02\n",
      " -7.470e+02 -7.470e+02 -7.480e+02  7.490e+02 -7.520e+02 -7.520e+02\n",
      "  7.540e+02 -7.540e+02 -7.540e+02 -7.550e+02 -7.590e+02 -7.590e+02\n",
      " -7.600e+02 -7.620e+02 -7.620e+02  7.630e+02 -7.650e+02 -7.670e+02\n",
      " -7.770e+02 -7.830e+02 -7.850e+02  7.850e+02  7.860e+02 -7.880e+02\n",
      " -7.890e+02  7.920e+02  7.920e+02 -7.920e+02 -7.950e+02 -7.980e+02\n",
      " -8.060e+02 -8.120e+02 -8.190e+02 -8.200e+02  8.210e+02  8.250e+02\n",
      " -8.290e+02 -8.370e+02 -8.390e+02 -8.470e+02 -8.490e+02 -8.520e+02\n",
      " -8.560e+02 -8.580e+02 -8.600e+02  8.600e+02 -8.650e+02 -8.650e+02\n",
      " -8.670e+02 -8.750e+02 -8.780e+02 -8.780e+02  8.790e+02 -8.810e+02\n",
      "  8.830e+02 -8.830e+02 -8.990e+02  9.040e+02 -9.060e+02 -9.070e+02\n",
      " -9.080e+02  9.120e+02 -9.120e+02 -9.120e+02 -9.150e+02 -9.180e+02\n",
      "  9.210e+02 -9.260e+02 -9.310e+02 -9.310e+02 -9.420e+02 -9.430e+02\n",
      "  9.430e+02 -9.430e+02 -9.430e+02 -9.520e+02 -9.540e+02  9.590e+02\n",
      " -9.650e+02 -9.650e+02  9.670e+02 -9.680e+02 -9.720e+02 -9.740e+02\n",
      " -9.740e+02 -9.750e+02 -9.750e+02 -9.840e+02 -9.870e+02 -9.890e+02\n",
      " -9.900e+02  9.910e+02  9.910e+02 -9.960e+02 -9.960e+02 -9.970e+02\n",
      " -1.001e+03 -1.001e+03 -1.004e+03 -1.004e+03 -1.004e+03 -1.005e+03\n",
      "  1.009e+03 -1.009e+03 -1.010e+03 -1.013e+03 -1.015e+03 -1.025e+03\n",
      " -1.026e+03 -1.026e+03 -1.027e+03  1.032e+03  1.034e+03 -1.034e+03\n",
      " -1.035e+03 -1.039e+03 -1.042e+03 -1.043e+03 -1.043e+03 -1.043e+03\n",
      " -1.047e+03  1.048e+03 -1.051e+03 -1.051e+03 -1.051e+03 -1.059e+03\n",
      " -1.062e+03 -1.062e+03 -1.063e+03 -1.066e+03  1.072e+03 -1.074e+03\n",
      " -1.079e+03 -1.085e+03 -1.093e+03 -1.099e+03 -1.101e+03 -1.102e+03\n",
      "  1.104e+03 -1.106e+03 -1.112e+03 -1.119e+03 -1.120e+03 -1.121e+03\n",
      " -1.124e+03  1.127e+03 -1.133e+03 -1.139e+03 -1.140e+03 -1.141e+03\n",
      "  1.142e+03 -1.148e+03  1.148e+03 -1.148e+03  1.152e+03 -1.156e+03\n",
      " -1.156e+03 -1.157e+03 -1.158e+03 -1.161e+03 -1.165e+03 -1.167e+03\n",
      " -1.174e+03  1.174e+03 -1.179e+03 -1.185e+03 -1.186e+03 -1.191e+03\n",
      " -1.196e+03 -1.198e+03 -1.203e+03 -1.203e+03 -1.206e+03 -1.208e+03\n",
      " -1.208e+03 -1.210e+03 -1.217e+03 -1.219e+03 -1.220e+03 -1.220e+03\n",
      " -1.224e+03 -1.232e+03 -1.233e+03 -1.234e+03 -1.234e+03 -1.246e+03\n",
      " -1.247e+03 -1.248e+03 -1.251e+03 -1.266e+03 -1.269e+03 -1.270e+03\n",
      "  1.272e+03 -1.285e+03 -1.288e+03 -1.291e+03 -1.301e+03 -1.308e+03\n",
      " -1.308e+03 -1.308e+03 -1.309e+03 -1.309e+03 -1.318e+03 -1.325e+03\n",
      " -1.326e+03 -1.330e+03 -1.347e+03 -1.359e+03 -1.363e+03 -1.369e+03\n",
      " -1.371e+03 -1.371e+03  1.388e+03  1.411e+03 -1.417e+03 -1.417e+03\n",
      " -1.419e+03  1.430e+03 -1.437e+03  1.439e+03 -1.448e+03 -1.449e+03\n",
      " -1.461e+03 -1.463e+03 -1.467e+03 -1.471e+03 -1.474e+03 -1.474e+03\n",
      " -1.476e+03 -1.477e+03 -1.505e+03 -1.516e+03 -1.519e+03 -1.523e+03\n",
      " -1.532e+03 -1.534e+03 -1.535e+03  1.542e+03 -1.542e+03 -1.545e+03\n",
      " -1.546e+03 -1.547e+03 -1.548e+03 -1.550e+03  1.556e+03 -1.561e+03\n",
      "  1.563e+03 -1.563e+03 -1.569e+03 -1.572e+03 -1.587e+03 -1.596e+03\n",
      " -1.604e+03 -1.611e+03 -1.611e+03 -1.614e+03 -1.616e+03 -1.620e+03\n",
      " -1.631e+03 -1.639e+03 -1.640e+03  1.642e+03 -1.644e+03 -1.648e+03\n",
      "  1.649e+03 -1.662e+03 -1.673e+03 -1.673e+03  1.673e+03 -1.682e+03\n",
      " -1.683e+03  1.688e+03 -1.688e+03 -1.692e+03  1.694e+03  1.699e+03\n",
      " -1.712e+03 -1.728e+03 -1.728e+03 -1.732e+03 -1.742e+03  1.759e+03\n",
      "  1.793e+03 -1.800e+03 -1.820e+03 -1.836e+03 -1.847e+03 -1.855e+03\n",
      " -1.864e+03 -1.866e+03 -1.871e+03 -1.873e+03 -1.876e+03 -1.882e+03\n",
      "  1.884e+03 -1.887e+03  1.900e+03 -1.919e+03  1.920e+03 -1.925e+03\n",
      " -1.926e+03  1.927e+03 -1.928e+03 -1.935e+03 -1.935e+03 -1.972e+03\n",
      " -1.988e+03  1.993e+03 -1.996e+03 -2.012e+03 -2.019e+03 -2.031e+03\n",
      " -2.033e+03 -2.041e+03 -2.048e+03 -2.064e+03 -2.072e+03 -2.082e+03\n",
      " -2.091e+03  2.097e+03 -2.108e+03 -2.128e+03 -2.136e+03 -2.155e+03\n",
      " -2.156e+03 -2.161e+03 -2.164e+03 -2.184e+03 -2.190e+03 -2.190e+03\n",
      " -2.191e+03  2.192e+03 -2.193e+03  2.207e+03 -2.222e+03 -2.231e+03\n",
      " -2.236e+03 -2.240e+03 -2.246e+03 -2.255e+03 -2.255e+03 -2.263e+03\n",
      "  2.273e+03 -2.278e+03 -2.281e+03 -2.282e+03 -2.288e+03 -2.289e+03\n",
      "  2.296e+03 -2.311e+03 -2.330e+03  2.348e+03  2.361e+03 -2.362e+03\n",
      " -2.371e+03 -2.372e+03  2.373e+03 -2.381e+03 -2.403e+03 -2.406e+03\n",
      "  2.417e+03 -2.426e+03 -2.442e+03  2.469e+03 -2.477e+03  2.483e+03\n",
      " -2.483e+03 -2.486e+03 -2.489e+03  2.520e+03  2.534e+03 -2.535e+03\n",
      " -2.541e+03  2.551e+03 -2.558e+03 -2.559e+03  2.573e+03 -2.590e+03\n",
      " -2.596e+03 -2.596e+03 -2.605e+03 -2.612e+03 -2.618e+03 -2.629e+03\n",
      " -2.632e+03 -2.632e+03  2.636e+03 -2.650e+03 -2.653e+03 -2.654e+03\n",
      " -2.695e+03 -2.707e+03  2.712e+03 -2.721e+03  2.763e+03 -2.767e+03\n",
      "  2.798e+03 -2.813e+03 -2.838e+03  2.854e+03 -2.856e+03  2.866e+03\n",
      " -2.868e+03 -2.953e+03  2.965e+03  2.965e+03 -2.971e+03 -2.976e+03\n",
      " -2.989e+03 -2.991e+03 -2.991e+03 -3.001e+03 -3.015e+03 -3.017e+03\n",
      " -3.021e+03 -3.022e+03 -3.030e+03 -3.035e+03 -3.072e+03 -3.088e+03\n",
      " -3.091e+03 -3.094e+03 -3.112e+03 -3.121e+03  3.126e+03 -3.128e+03\n",
      " -3.152e+03 -3.159e+03 -3.172e+03 -3.203e+03 -3.226e+03 -3.247e+03\n",
      " -3.248e+03 -3.256e+03 -3.261e+03  3.262e+03 -3.276e+03 -3.283e+03\n",
      " -3.283e+03 -3.286e+03 -3.296e+03 -3.316e+03 -3.361e+03 -3.364e+03\n",
      "  3.409e+03  3.418e+03 -3.430e+03 -3.456e+03  3.461e+03  3.462e+03\n",
      "  3.472e+03  3.492e+03 -3.506e+03 -3.519e+03 -3.607e+03 -3.660e+03\n",
      "  3.669e+03 -3.709e+03  3.926e+03  3.941e+03  3.945e+03 -3.957e+03\n",
      " -4.052e+03 -4.080e+03 -4.088e+03 -4.159e+03 -4.233e+03  4.267e+03\n",
      " -4.275e+03 -4.285e+03 -4.354e+03  4.456e+03 -4.894e+03 -5.042e+03\n",
      " -5.062e+03 -5.176e+03 -5.739e+03 -6.292e+03  6.456e+03  6.593e+03\n",
      " -7.106e+03 -7.126e+03  7.455e+03 -7.777e+03 -8.008e+03 -8.391e+03\n",
      " -8.605e+03]\n",
      "durations 5.0 8556.0\n",
      "Concordance Index 0.5100746268656716\n",
      "Integrated Brier Score: 0.17943674452267058\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m177.6334\u001b[0m      \u001b[32m100.0803\u001b[0m  0.0721\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m170.6819\u001b[0m      \u001b[32m127.1943\u001b[0m  0.0741\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m160.5002\u001b[0m      \u001b[32m112.0579\u001b[0m  0.0575\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m77.2032\u001b[0m      \u001b[32m108.3421\u001b[0m  0.0794\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m145.1245\u001b[0m       \u001b[32m97.5344\u001b[0m  0.0448\n",
      "      2      \u001b[36m172.0447\u001b[0m       \u001b[32m95.7562\u001b[0m  0.0468\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m147.8532\u001b[0m       \u001b[32m71.9767\u001b[0m  0.0594\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m80.8127\u001b[0m       \u001b[32m83.9614\u001b[0m  0.0964\n",
      "      2      \u001b[36m163.5615\u001b[0m      \u001b[32m119.3377\u001b[0m  0.0584\n",
      "      2      \u001b[36m152.0753\u001b[0m      \u001b[32m110.1395\u001b[0m  0.0477\n",
      "      3      \u001b[36m167.7578\u001b[0m       \u001b[32m93.6896\u001b[0m  0.0339\n",
      "      2      \u001b[36m139.1876\u001b[0m       \u001b[32m92.2853\u001b[0m  0.0364\n",
      "      2      \u001b[36m140.3666\u001b[0m       73.9295  0.0327\n",
      "      2       \u001b[36m73.1166\u001b[0m       \u001b[32m97.3599\u001b[0m  0.0621\n",
      "      3      \u001b[36m160.1768\u001b[0m      \u001b[32m114.5004\u001b[0m  0.0320\n",
      "      3      \u001b[36m151.6088\u001b[0m      \u001b[32m104.0990\u001b[0m  0.0313\n",
      "      4      \u001b[36m166.8022\u001b[0m       \u001b[32m93.3777\u001b[0m  0.0307\n",
      "      3      \u001b[36m137.9567\u001b[0m       \u001b[32m86.6261\u001b[0m  0.0334\n",
      "      2       \u001b[36m74.9979\u001b[0m       \u001b[32m80.8440\u001b[0m  0.0513\n",
      "      3      \u001b[36m139.5246\u001b[0m       73.7055  0.0301\n",
      "      4      \u001b[36m146.7247\u001b[0m       \u001b[32m99.7490\u001b[0m  0.0297\n",
      "      4      \u001b[36m159.4638\u001b[0m      \u001b[32m111.4997\u001b[0m  0.0332\n",
      "      5      168.1124       \u001b[32m93.3626\u001b[0m  0.0299\n",
      "      4      \u001b[36m136.8622\u001b[0m       \u001b[32m83.7113\u001b[0m  0.0288\n",
      "      3       \u001b[36m70.0216\u001b[0m      102.0259  0.0476\n",
      "      4      \u001b[36m137.6699\u001b[0m       73.1612  0.0278\n",
      "      3       \u001b[36m72.9784\u001b[0m       83.5712  0.0456\n",
      "      5      160.2184      \u001b[32m109.9261\u001b[0m  0.0316\n",
      "      5      \u001b[36m145.5180\u001b[0m       \u001b[32m97.5599\u001b[0m  0.0432\n",
      "      5      \u001b[36m133.5685\u001b[0m       \u001b[32m82.5826\u001b[0m  0.0298\n",
      "      6      167.1067       \u001b[32m93.0147\u001b[0m  0.0327\n",
      "      5      \u001b[36m137.0364\u001b[0m       72.6755  0.0262\n",
      "      6      \u001b[36m155.9023\u001b[0m      \u001b[32m108.9962\u001b[0m  0.0304\n",
      "      6      \u001b[36m143.6198\u001b[0m       \u001b[32m96.8029\u001b[0m  0.0295\n",
      "      4       \u001b[36m68.9233\u001b[0m      102.0036  0.0597\n",
      "      6      \u001b[36m133.2539\u001b[0m       \u001b[32m82.4457\u001b[0m  0.0296\n",
      "      7      \u001b[36m166.1594\u001b[0m       \u001b[32m92.5966\u001b[0m  0.0295\n",
      "      4       \u001b[36m72.2706\u001b[0m       84.0396  0.0438\n",
      "      6      \u001b[36m133.4967\u001b[0m       72.1659  0.0334\n",
      "      7      159.1674      \u001b[32m108.4152\u001b[0m  0.0353\n",
      "      7      144.5852       96.9660  0.0309\n",
      "      8      \u001b[36m163.2414\u001b[0m       \u001b[32m92.2438\u001b[0m  0.0323\n",
      "      7      134.1576       83.0207  0.0369\n",
      "      7      \u001b[36m132.4746\u001b[0m       \u001b[32m71.9148\u001b[0m  0.0308\n",
      "      5       69.0729       98.0907  0.0468\n",
      "      5       \u001b[36m70.9100\u001b[0m       \u001b[32m80.7950\u001b[0m  0.0523\n",
      "      8      \u001b[36m143.1176\u001b[0m       97.7962  0.0308\n",
      "      8      156.4367      \u001b[32m107.9395\u001b[0m  0.0349\n",
      "      9      164.3094       \u001b[32m92.1413\u001b[0m  0.0325\n",
      "      8      \u001b[36m131.7903\u001b[0m       71.9842  0.0301\n",
      "      8      133.5473       83.8743  0.0328\n",
      "      9      \u001b[36m143.0631\u001b[0m       98.7794  0.0285\n",
      "      6       \u001b[36m68.5764\u001b[0m       \u001b[32m95.2677\u001b[0m  0.0447\n",
      "      9      \u001b[36m155.7375\u001b[0m      \u001b[32m107.4323\u001b[0m  0.0315\n",
      "     10      165.1654       92.1866  0.0298\n",
      "Restoring best model from epoch 9.\n",
      "      9      \u001b[36m130.6237\u001b[0m       84.4414  0.0272\n",
      "      9      133.8515       72.0444  0.0311\n",
      "      6       72.7297       \u001b[32m78.5706\u001b[0m  0.0482\n",
      "     10      144.8245       99.6155  0.0284\n",
      "Restoring best model from epoch 6.\n",
      "     10      157.8517      \u001b[32m107.1697\u001b[0m  0.0302\n",
      "     10      \u001b[36m129.8558\u001b[0m       72.0233  0.0273\n",
      "Restoring best model from epoch 7.\n",
      "     10      \u001b[36m129.4280\u001b[0m       84.8674  0.0326\n",
      "Restoring best model from epoch 6.\n",
      "      7       \u001b[36m66.9448\u001b[0m       \u001b[32m94.1650\u001b[0m  0.0453\n",
      "      7       \u001b[36m69.7295\u001b[0m       78.7459  0.0450\n",
      "      8       68.6551       94.4410  0.0451\n",
      "      8       71.3702       78.8229  0.0491\n",
      "      9       67.7226       96.4416  0.0437\n",
      "      9       \u001b[36m68.7645\u001b[0m       80.8343  0.0443\n",
      "     10       67.5651       95.6400  0.0433\n",
      "Restoring best model from epoch 7.\n",
      "     10       69.9983       80.5529  0.0441\n",
      "Restoring best model from epoch 6.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m159.7007\u001b[0m      \u001b[32m101.3594\u001b[0m  0.0308\n",
      "      2      \u001b[36m154.7812\u001b[0m       \u001b[32m98.0485\u001b[0m  0.0279\n",
      "      3      \u001b[36m154.2551\u001b[0m       \u001b[32m96.0766\u001b[0m  0.0286\n",
      "      4      \u001b[36m151.6728\u001b[0m       \u001b[32m94.8343\u001b[0m  0.0286\n",
      "      5      \u001b[36m147.2723\u001b[0m       94.8527  0.0310\n",
      "      6      148.2233       95.2988  0.0301\n",
      "      7      \u001b[36m144.0907\u001b[0m       95.5178  0.0286\n",
      "      8      148.4729       95.3650  0.0286\n",
      "      9      145.1087       95.2388  0.0319\n",
      "     10      147.8977       94.9675  0.0395\n",
      "Restoring best model from epoch 4.\n",
      "y_train breslow final [-1.000e+00 -5.000e+00 -5.000e+00 -5.000e+00 -7.000e+00 -8.000e+00\n",
      " -9.000e+00 -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01\n",
      " -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01\n",
      " -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01 -1.100e+01 -1.900e+01\n",
      " -2.100e+01 -2.600e+01 -3.000e+01 -3.000e+01 -3.000e+01 -3.100e+01\n",
      " -3.100e+01 -3.100e+01 -3.400e+01 -3.400e+01 -4.000e+01 -4.900e+01\n",
      " -5.200e+01 -5.400e+01 -6.400e+01 -7.000e+01 -7.200e+01 -7.600e+01\n",
      " -7.800e+01 -7.800e+01 -7.900e+01 -8.400e+01 -9.000e+01 -9.200e+01\n",
      " -9.800e+01  1.160e+02 -1.340e+02 -1.490e+02 -1.600e+02 -1.700e+02\n",
      " -1.700e+02  1.720e+02 -1.720e+02  1.740e+02 -1.780e+02 -1.860e+02\n",
      " -2.000e+02 -2.100e+02 -2.140e+02 -2.150e+02 -2.160e+02 -2.220e+02\n",
      " -2.240e+02  2.240e+02 -2.250e+02 -2.270e+02 -2.310e+02  2.390e+02\n",
      " -2.420e+02 -2.430e+02 -2.500e+02 -2.520e+02  2.550e+02 -2.580e+02\n",
      " -2.590e+02 -2.660e+02  2.660e+02 -2.710e+02 -2.730e+02 -2.740e+02\n",
      " -2.750e+02 -2.850e+02 -2.870e+02 -2.930e+02  2.950e+02 -3.000e+02\n",
      "  3.020e+02 -3.030e+02 -3.030e+02 -3.040e+02 -3.040e+02 -3.040e+02\n",
      " -3.040e+02  3.040e+02 -3.130e+02 -3.170e+02 -3.170e+02  3.200e+02\n",
      "  3.220e+02 -3.220e+02 -3.220e+02 -3.230e+02 -3.260e+02 -3.280e+02\n",
      " -3.320e+02 -3.340e+02 -3.360e+02 -3.450e+02 -3.470e+02 -3.480e+02\n",
      " -3.500e+02 -3.520e+02 -3.580e+02 -3.580e+02 -3.580e+02 -3.600e+02\n",
      " -3.640e+02 -3.650e+02 -3.650e+02  3.650e+02 -3.650e+02 -3.650e+02\n",
      " -3.660e+02 -3.660e+02 -3.710e+02 -3.710e+02 -3.730e+02 -3.730e+02\n",
      " -3.750e+02 -3.750e+02 -3.750e+02 -3.750e+02 -3.760e+02 -3.760e+02\n",
      "  3.770e+02 -3.790e+02 -3.800e+02 -3.800e+02 -3.810e+02 -3.810e+02\n",
      " -3.830e+02 -3.830e+02  3.850e+02 -3.850e+02 -3.850e+02 -3.850e+02\n",
      " -3.920e+02 -3.930e+02 -3.930e+02 -3.940e+02 -3.940e+02 -3.940e+02\n",
      " -3.950e+02 -3.960e+02 -3.960e+02 -3.960e+02 -3.960e+02 -3.960e+02\n",
      " -3.970e+02 -3.980e+02 -4.020e+02 -4.030e+02 -4.030e+02 -4.050e+02\n",
      " -4.080e+02 -4.080e+02 -4.090e+02 -4.100e+02 -4.100e+02 -4.110e+02\n",
      " -4.120e+02 -4.130e+02 -4.140e+02 -4.170e+02 -4.180e+02 -4.210e+02\n",
      " -4.230e+02 -4.240e+02 -4.250e+02  4.260e+02 -4.280e+02 -4.280e+02\n",
      " -4.300e+02 -4.310e+02 -4.330e+02 -4.340e+02 -4.350e+02 -4.370e+02\n",
      " -4.390e+02 -4.390e+02 -4.390e+02 -4.410e+02 -4.410e+02 -4.410e+02\n",
      " -4.460e+02  4.460e+02 -4.470e+02 -4.470e+02 -4.470e+02 -4.480e+02\n",
      " -4.480e+02 -4.500e+02 -4.500e+02 -4.510e+02 -4.540e+02 -4.550e+02\n",
      " -4.560e+02 -4.580e+02 -4.610e+02 -4.610e+02 -4.630e+02 -4.660e+02\n",
      " -4.660e+02 -4.670e+02 -4.700e+02 -4.710e+02 -4.720e+02 -4.750e+02\n",
      " -4.760e+02 -4.770e+02 -4.800e+02 -4.820e+02 -4.880e+02 -4.880e+02\n",
      " -4.890e+02 -4.920e+02 -4.950e+02 -4.960e+02 -4.960e+02 -4.990e+02\n",
      " -4.990e+02 -5.010e+02 -5.020e+02 -5.030e+02 -5.040e+02 -5.040e+02\n",
      " -5.060e+02 -5.080e+02 -5.090e+02 -5.100e+02 -5.110e+02 -5.180e+02\n",
      " -5.180e+02 -5.180e+02 -5.190e+02 -5.210e+02 -5.220e+02 -5.230e+02\n",
      "  5.240e+02 -5.250e+02 -5.260e+02 -5.280e+02 -5.280e+02 -5.290e+02\n",
      " -5.300e+02 -5.300e+02 -5.310e+02 -5.320e+02 -5.320e+02 -5.330e+02\n",
      " -5.330e+02 -5.380e+02 -5.380e+02  5.380e+02 -5.410e+02 -5.410e+02\n",
      " -5.450e+02 -5.460e+02 -5.470e+02  5.480e+02 -5.490e+02 -5.510e+02\n",
      " -5.520e+02 -5.520e+02 -5.540e+02 -5.540e+02 -5.540e+02 -5.620e+02\n",
      " -5.630e+02  5.630e+02 -5.650e+02 -5.660e+02 -5.670e+02 -5.680e+02\n",
      "  5.710e+02 -5.720e+02  5.730e+02 -5.740e+02 -5.750e+02 -5.750e+02\n",
      " -5.750e+02 -5.750e+02 -5.770e+02 -5.770e+02 -5.770e+02 -5.790e+02\n",
      " -5.790e+02 -5.840e+02 -5.840e+02  5.840e+02 -5.840e+02 -5.850e+02\n",
      " -5.880e+02 -5.880e+02 -5.880e+02 -5.900e+02 -5.940e+02 -5.950e+02\n",
      " -5.960e+02 -5.980e+02 -6.000e+02 -6.060e+02 -6.070e+02 -6.080e+02\n",
      " -6.080e+02 -6.110e+02 -6.110e+02 -6.120e+02  6.120e+02 -6.120e+02\n",
      " -6.140e+02  6.140e+02  6.160e+02 -6.160e+02 -6.160e+02 -6.200e+02\n",
      " -6.200e+02 -6.220e+02 -6.220e+02 -6.240e+02 -6.260e+02 -6.260e+02\n",
      " -6.270e+02 -6.300e+02 -6.310e+02 -6.350e+02 -6.350e+02 -6.390e+02\n",
      "  6.390e+02 -6.400e+02 -6.400e+02 -6.410e+02 -6.430e+02 -6.430e+02\n",
      " -6.440e+02 -6.460e+02 -6.470e+02 -6.510e+02 -6.520e+02 -6.550e+02\n",
      " -6.580e+02 -6.590e+02 -6.590e+02 -6.600e+02 -6.610e+02 -6.620e+02\n",
      " -6.640e+02 -6.660e+02 -6.660e+02 -6.720e+02 -6.750e+02 -6.750e+02\n",
      " -6.770e+02 -6.770e+02  6.780e+02 -6.790e+02 -6.790e+02 -6.810e+02\n",
      " -6.830e+02 -6.940e+02 -6.940e+02 -7.010e+02 -7.030e+02 -7.060e+02\n",
      " -7.100e+02 -7.140e+02 -7.140e+02 -7.150e+02 -7.180e+02 -7.220e+02\n",
      "  7.230e+02 -7.240e+02 -7.250e+02 -7.260e+02 -7.270e+02 -7.270e+02\n",
      " -7.280e+02 -7.340e+02 -7.380e+02 -7.420e+02 -7.450e+02 -7.470e+02\n",
      " -7.470e+02 -7.470e+02 -7.480e+02  7.490e+02 -7.520e+02 -7.520e+02\n",
      "  7.540e+02 -7.540e+02 -7.550e+02 -7.590e+02 -7.600e+02 -7.600e+02\n",
      " -7.610e+02 -7.620e+02 -7.620e+02  7.630e+02 -7.650e+02 -7.670e+02\n",
      " -7.670e+02 -7.690e+02 -7.830e+02 -7.850e+02  7.850e+02 -7.880e+02\n",
      " -7.890e+02 -7.910e+02  7.920e+02 -7.920e+02 -7.950e+02 -7.980e+02\n",
      " -8.030e+02  8.110e+02 -8.190e+02 -8.200e+02  8.210e+02 -8.220e+02\n",
      "  8.250e+02 -8.370e+02 -8.390e+02 -8.520e+02 -8.560e+02 -8.580e+02\n",
      " -8.600e+02  8.600e+02 -8.650e+02 -8.650e+02 -8.670e+02 -8.750e+02\n",
      " -8.780e+02 -8.780e+02  8.790e+02 -8.810e+02 -8.830e+02 -8.890e+02\n",
      " -8.900e+02 -8.990e+02  9.040e+02 -9.060e+02 -9.080e+02 -9.110e+02\n",
      "  9.120e+02 -9.120e+02 -9.180e+02  9.210e+02 -9.230e+02 -9.260e+02\n",
      " -9.310e+02 -9.420e+02 -9.430e+02 -9.430e+02 -9.430e+02 -9.520e+02\n",
      " -9.540e+02  9.590e+02 -9.650e+02 -9.650e+02  9.670e+02 -9.680e+02\n",
      " -9.720e+02 -9.730e+02 -9.740e+02 -9.750e+02 -9.750e+02  9.760e+02\n",
      " -9.840e+02 -9.870e+02 -9.900e+02 -9.960e+02 -9.960e+02 -9.970e+02\n",
      " -1.000e+03 -1.001e+03 -1.001e+03  1.004e+03 -1.004e+03 -1.004e+03\n",
      " -1.005e+03 -1.006e+03 -1.007e+03  1.009e+03 -1.010e+03 -1.013e+03\n",
      " -1.015e+03 -1.025e+03 -1.027e+03  1.032e+03  1.034e+03 -1.034e+03\n",
      " -1.035e+03 -1.042e+03 -1.043e+03 -1.043e+03 -1.043e+03  1.048e+03\n",
      " -1.051e+03 -1.051e+03 -1.051e+03 -1.062e+03 -1.062e+03 -1.063e+03\n",
      "  1.072e+03 -1.085e+03 -1.088e+03 -1.093e+03  1.093e+03 -1.099e+03\n",
      " -1.101e+03 -1.102e+03  1.104e+03 -1.112e+03 -1.119e+03 -1.120e+03\n",
      " -1.120e+03 -1.121e+03 -1.124e+03  1.127e+03 -1.132e+03 -1.138e+03\n",
      " -1.139e+03 -1.141e+03 -1.148e+03  1.148e+03 -1.148e+03 -1.150e+03\n",
      "  1.152e+03 -1.156e+03 -1.156e+03 -1.157e+03 -1.158e+03 -1.161e+03\n",
      " -1.162e+03 -1.163e+03 -1.165e+03 -1.167e+03  1.174e+03 -1.179e+03\n",
      " -1.185e+03 -1.186e+03 -1.189e+03 -1.191e+03 -1.196e+03 -1.203e+03\n",
      " -1.203e+03 -1.208e+03 -1.217e+03 -1.219e+03 -1.220e+03 -1.224e+03\n",
      " -1.229e+03 -1.229e+03 -1.233e+03 -1.234e+03 -1.234e+03 -1.239e+03\n",
      " -1.246e+03 -1.247e+03 -1.248e+03 -1.266e+03 -1.269e+03 -1.270e+03\n",
      "  1.272e+03  1.275e+03 -1.277e+03 -1.285e+03  1.286e+03 -1.288e+03\n",
      " -1.291e+03 -1.309e+03 -1.318e+03 -1.321e+03  1.324e+03 -1.325e+03\n",
      " -1.347e+03 -1.351e+03 -1.359e+03 -1.363e+03 -1.363e+03  1.365e+03\n",
      " -1.369e+03 -1.371e+03  1.388e+03  1.411e+03 -1.419e+03  1.430e+03\n",
      " -1.434e+03 -1.437e+03 -1.448e+03 -1.449e+03 -1.461e+03 -1.467e+03\n",
      " -1.471e+03 -1.474e+03 -1.476e+03 -1.477e+03 -1.492e+03 -1.505e+03\n",
      "  1.508e+03 -1.516e+03 -1.519e+03 -1.522e+03 -1.523e+03 -1.528e+03\n",
      " -1.534e+03 -1.535e+03  1.542e+03 -1.545e+03 -1.545e+03 -1.547e+03\n",
      " -1.548e+03 -1.561e+03  1.563e+03 -1.569e+03 -1.587e+03 -1.604e+03\n",
      " -1.604e+03 -1.611e+03 -1.611e+03 -1.611e+03 -1.612e+03 -1.613e+03\n",
      " -1.614e+03 -1.620e+03 -1.631e+03  1.642e+03 -1.642e+03 -1.644e+03\n",
      " -1.648e+03  1.649e+03 -1.662e+03 -1.673e+03  1.673e+03 -1.682e+03\n",
      " -1.686e+03  1.688e+03 -1.688e+03  1.692e+03  1.694e+03  1.699e+03\n",
      " -1.712e+03 -1.728e+03 -1.742e+03  1.759e+03 -1.780e+03  1.781e+03\n",
      " -1.783e+03  1.793e+03 -1.800e+03  1.812e+03 -1.820e+03 -1.836e+03\n",
      " -1.842e+03 -1.847e+03 -1.853e+03 -1.864e+03 -1.866e+03 -1.871e+03\n",
      " -1.876e+03  1.884e+03 -1.887e+03  1.900e+03 -1.914e+03 -1.919e+03\n",
      "  1.920e+03 -1.926e+03  1.927e+03 -1.928e+03 -1.935e+03 -1.953e+03\n",
      " -1.972e+03 -1.980e+03 -1.988e+03  2.009e+03 -2.012e+03 -2.019e+03\n",
      " -2.031e+03 -2.033e+03 -2.048e+03 -2.072e+03  2.097e+03 -2.109e+03\n",
      " -2.124e+03  2.127e+03 -2.128e+03 -2.136e+03 -2.140e+03 -2.142e+03\n",
      " -2.155e+03 -2.156e+03 -2.161e+03 -2.164e+03 -2.184e+03 -2.190e+03\n",
      " -2.197e+03  2.207e+03 -2.236e+03 -2.248e+03 -2.255e+03 -2.255e+03\n",
      " -2.255e+03 -2.263e+03  2.273e+03 -2.278e+03 -2.281e+03 -2.282e+03\n",
      " -2.288e+03 -2.289e+03  2.296e+03 -2.306e+03 -2.311e+03 -2.329e+03\n",
      " -2.330e+03 -2.335e+03  2.348e+03  2.361e+03 -2.365e+03 -2.372e+03\n",
      " -2.372e+03  2.373e+03 -2.381e+03 -2.385e+03 -2.406e+03  2.417e+03\n",
      " -2.426e+03 -2.442e+03 -2.442e+03  2.469e+03 -2.477e+03  2.483e+03\n",
      " -2.483e+03 -2.486e+03 -2.489e+03 -2.513e+03 -2.515e+03  2.520e+03\n",
      "  2.534e+03 -2.535e+03 -2.541e+03  2.551e+03 -2.558e+03 -2.559e+03\n",
      "  2.573e+03 -2.590e+03 -2.596e+03 -2.596e+03 -2.618e+03 -2.632e+03\n",
      " -2.645e+03 -2.645e+03 -2.650e+03 -2.653e+03 -2.654e+03 -2.695e+03\n",
      " -2.707e+03 -2.709e+03 -2.759e+03  2.763e+03 -2.770e+03 -2.813e+03\n",
      " -2.838e+03 -2.856e+03  2.866e+03  2.911e+03 -2.920e+03 -2.953e+03\n",
      "  2.965e+03  2.965e+03 -2.971e+03 -2.989e+03 -2.991e+03 -2.991e+03\n",
      " -3.001e+03 -3.004e+03 -3.009e+03 -3.011e+03 -3.015e+03 -3.017e+03\n",
      " -3.021e+03 -3.030e+03 -3.035e+03  3.063e+03 -3.072e+03 -3.088e+03\n",
      " -3.091e+03 -3.102e+03 -3.112e+03 -3.121e+03  3.126e+03 -3.128e+03\n",
      " -3.152e+03 -3.172e+03 -3.202e+03 -3.203e+03 -3.204e+03 -3.226e+03\n",
      " -3.247e+03 -3.256e+03  3.262e+03 -3.283e+03 -3.283e+03 -3.287e+03\n",
      " -3.307e+03 -3.316e+03 -3.342e+03 -3.364e+03  3.409e+03  3.418e+03\n",
      " -3.430e+03 -3.456e+03  3.461e+03  3.462e+03  3.472e+03 -3.506e+03\n",
      " -3.519e+03 -3.603e+03 -3.607e+03 -3.660e+03  3.669e+03 -3.709e+03\n",
      "  3.736e+03 -3.807e+03  3.873e+03  3.941e+03 -3.957e+03  3.959e+03\n",
      " -4.005e+03 -4.047e+03 -4.052e+03 -4.080e+03 -4.088e+03 -4.233e+03\n",
      "  4.267e+03 -4.285e+03 -4.354e+03 -4.361e+03  4.456e+03 -4.894e+03\n",
      " -4.929e+03 -5.042e+03 -5.062e+03 -5.156e+03 -5.176e+03 -6.292e+03\n",
      " -7.126e+03  7.455e+03 -7.777e+03 -8.008e+03 -8.391e+03 -8.556e+03\n",
      " -8.605e+03]\n",
      "Concordance Index 0.5850117676204633\n",
      "Integrated Brier Score: 0.20752416166511614\n",
      "y_train breslow final [-1.000e+00 -5.000e+00 -5.000e+00 -5.000e+00 -7.000e+00 -8.000e+00\n",
      " -9.000e+00 -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01\n",
      " -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01\n",
      " -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01 -1.100e+01 -1.900e+01\n",
      " -2.100e+01 -2.600e+01 -3.000e+01 -3.000e+01 -3.000e+01 -3.100e+01\n",
      " -3.100e+01 -3.100e+01 -3.400e+01 -3.400e+01 -4.000e+01 -4.900e+01\n",
      " -5.200e+01 -5.400e+01 -6.400e+01 -7.000e+01 -7.200e+01 -7.600e+01\n",
      " -7.800e+01 -7.800e+01 -7.900e+01 -8.400e+01 -9.000e+01 -9.200e+01\n",
      " -9.800e+01  1.160e+02 -1.340e+02 -1.490e+02 -1.600e+02 -1.700e+02\n",
      " -1.700e+02  1.720e+02 -1.720e+02  1.740e+02 -1.780e+02 -1.860e+02\n",
      " -2.000e+02 -2.100e+02 -2.140e+02 -2.150e+02 -2.160e+02 -2.220e+02\n",
      " -2.240e+02  2.240e+02 -2.250e+02 -2.270e+02 -2.310e+02  2.390e+02\n",
      " -2.420e+02 -2.430e+02 -2.500e+02 -2.520e+02  2.550e+02 -2.580e+02\n",
      " -2.590e+02 -2.660e+02  2.660e+02 -2.710e+02 -2.730e+02 -2.740e+02\n",
      " -2.750e+02 -2.850e+02 -2.870e+02 -2.930e+02  2.950e+02 -3.000e+02\n",
      "  3.020e+02 -3.030e+02 -3.030e+02 -3.040e+02 -3.040e+02 -3.040e+02\n",
      " -3.040e+02  3.040e+02 -3.130e+02 -3.170e+02 -3.170e+02  3.200e+02\n",
      "  3.220e+02 -3.220e+02 -3.220e+02 -3.230e+02 -3.260e+02 -3.280e+02\n",
      " -3.320e+02 -3.340e+02 -3.360e+02 -3.450e+02 -3.470e+02 -3.480e+02\n",
      " -3.500e+02 -3.520e+02 -3.580e+02 -3.580e+02 -3.580e+02 -3.600e+02\n",
      " -3.640e+02 -3.650e+02 -3.650e+02  3.650e+02 -3.650e+02 -3.650e+02\n",
      " -3.660e+02 -3.660e+02 -3.710e+02 -3.710e+02 -3.730e+02 -3.730e+02\n",
      " -3.750e+02 -3.750e+02 -3.750e+02 -3.750e+02 -3.760e+02 -3.760e+02\n",
      "  3.770e+02 -3.790e+02 -3.800e+02 -3.800e+02 -3.810e+02 -3.810e+02\n",
      " -3.830e+02 -3.830e+02  3.850e+02 -3.850e+02 -3.850e+02 -3.850e+02\n",
      " -3.920e+02 -3.930e+02 -3.930e+02 -3.940e+02 -3.940e+02 -3.940e+02\n",
      " -3.950e+02 -3.960e+02 -3.960e+02 -3.960e+02 -3.960e+02 -3.960e+02\n",
      " -3.970e+02 -3.980e+02 -4.020e+02 -4.030e+02 -4.030e+02 -4.050e+02\n",
      " -4.080e+02 -4.080e+02 -4.090e+02 -4.100e+02 -4.100e+02 -4.110e+02\n",
      " -4.120e+02 -4.130e+02 -4.140e+02 -4.170e+02 -4.180e+02 -4.210e+02\n",
      " -4.230e+02 -4.240e+02 -4.250e+02  4.260e+02 -4.280e+02 -4.280e+02\n",
      " -4.300e+02 -4.310e+02 -4.330e+02 -4.340e+02 -4.350e+02 -4.370e+02\n",
      " -4.390e+02 -4.390e+02 -4.390e+02 -4.410e+02 -4.410e+02 -4.410e+02\n",
      " -4.460e+02  4.460e+02 -4.470e+02 -4.470e+02 -4.470e+02 -4.480e+02\n",
      " -4.480e+02 -4.500e+02 -4.500e+02 -4.510e+02 -4.540e+02 -4.550e+02\n",
      " -4.560e+02 -4.580e+02 -4.610e+02 -4.610e+02 -4.630e+02 -4.660e+02\n",
      " -4.660e+02 -4.670e+02 -4.700e+02 -4.710e+02 -4.720e+02 -4.750e+02\n",
      " -4.760e+02 -4.770e+02 -4.800e+02 -4.820e+02 -4.880e+02 -4.880e+02\n",
      " -4.890e+02 -4.920e+02 -4.950e+02 -4.960e+02 -4.960e+02 -4.990e+02\n",
      " -4.990e+02 -5.010e+02 -5.020e+02 -5.030e+02 -5.040e+02 -5.040e+02\n",
      " -5.060e+02 -5.080e+02 -5.090e+02 -5.100e+02 -5.110e+02 -5.180e+02\n",
      " -5.180e+02 -5.180e+02 -5.190e+02 -5.210e+02 -5.220e+02 -5.230e+02\n",
      "  5.240e+02 -5.250e+02 -5.260e+02 -5.280e+02 -5.280e+02 -5.290e+02\n",
      " -5.300e+02 -5.300e+02 -5.310e+02 -5.320e+02 -5.320e+02 -5.330e+02\n",
      " -5.330e+02 -5.380e+02 -5.380e+02  5.380e+02 -5.410e+02 -5.410e+02\n",
      " -5.450e+02 -5.460e+02 -5.470e+02  5.480e+02 -5.490e+02 -5.510e+02\n",
      " -5.520e+02 -5.520e+02 -5.540e+02 -5.540e+02 -5.540e+02 -5.620e+02\n",
      " -5.630e+02  5.630e+02 -5.650e+02 -5.660e+02 -5.670e+02 -5.680e+02\n",
      "  5.710e+02 -5.720e+02  5.730e+02 -5.740e+02 -5.750e+02 -5.750e+02\n",
      " -5.750e+02 -5.750e+02 -5.770e+02 -5.770e+02 -5.770e+02 -5.790e+02\n",
      " -5.790e+02 -5.840e+02 -5.840e+02  5.840e+02 -5.840e+02 -5.850e+02\n",
      " -5.880e+02 -5.880e+02 -5.880e+02 -5.900e+02 -5.940e+02 -5.950e+02\n",
      " -5.960e+02 -5.980e+02 -6.000e+02 -6.060e+02 -6.070e+02 -6.080e+02\n",
      " -6.080e+02 -6.110e+02 -6.110e+02 -6.120e+02  6.120e+02 -6.120e+02\n",
      " -6.140e+02  6.140e+02  6.160e+02 -6.160e+02 -6.160e+02 -6.200e+02\n",
      " -6.200e+02 -6.220e+02 -6.220e+02 -6.240e+02 -6.260e+02 -6.260e+02\n",
      " -6.270e+02 -6.300e+02 -6.310e+02 -6.350e+02 -6.350e+02 -6.390e+02\n",
      "  6.390e+02 -6.400e+02 -6.400e+02 -6.410e+02 -6.430e+02 -6.430e+02\n",
      " -6.440e+02 -6.460e+02 -6.470e+02 -6.510e+02 -6.520e+02 -6.550e+02\n",
      " -6.580e+02 -6.590e+02 -6.590e+02 -6.600e+02 -6.610e+02 -6.620e+02\n",
      " -6.640e+02 -6.660e+02 -6.660e+02 -6.720e+02 -6.750e+02 -6.750e+02\n",
      " -6.770e+02 -6.770e+02  6.780e+02 -6.790e+02 -6.790e+02 -6.810e+02\n",
      " -6.830e+02 -6.940e+02 -6.940e+02 -7.010e+02 -7.030e+02 -7.060e+02\n",
      " -7.100e+02 -7.140e+02 -7.140e+02 -7.150e+02 -7.180e+02 -7.220e+02\n",
      "  7.230e+02 -7.240e+02 -7.250e+02 -7.260e+02 -7.270e+02 -7.270e+02\n",
      " -7.280e+02 -7.340e+02 -7.380e+02 -7.420e+02 -7.450e+02 -7.470e+02\n",
      " -7.470e+02 -7.470e+02 -7.480e+02  7.490e+02 -7.520e+02 -7.520e+02\n",
      "  7.540e+02 -7.540e+02 -7.550e+02 -7.590e+02 -7.600e+02 -7.600e+02\n",
      " -7.610e+02 -7.620e+02 -7.620e+02  7.630e+02 -7.650e+02 -7.670e+02\n",
      " -7.670e+02 -7.690e+02 -7.830e+02 -7.850e+02  7.850e+02 -7.880e+02\n",
      " -7.890e+02 -7.910e+02  7.920e+02 -7.920e+02 -7.950e+02 -7.980e+02\n",
      " -8.030e+02  8.110e+02 -8.190e+02 -8.200e+02  8.210e+02 -8.220e+02\n",
      "  8.250e+02 -8.370e+02 -8.390e+02 -8.520e+02 -8.560e+02 -8.580e+02\n",
      " -8.600e+02  8.600e+02 -8.650e+02 -8.650e+02 -8.670e+02 -8.750e+02\n",
      " -8.780e+02 -8.780e+02  8.790e+02 -8.810e+02 -8.830e+02 -8.890e+02\n",
      " -8.900e+02 -8.990e+02  9.040e+02 -9.060e+02 -9.080e+02 -9.110e+02\n",
      "  9.120e+02 -9.120e+02 -9.180e+02  9.210e+02 -9.230e+02 -9.260e+02\n",
      " -9.310e+02 -9.420e+02 -9.430e+02 -9.430e+02 -9.430e+02 -9.520e+02\n",
      " -9.540e+02  9.590e+02 -9.650e+02 -9.650e+02  9.670e+02 -9.680e+02\n",
      " -9.720e+02 -9.730e+02 -9.740e+02 -9.750e+02 -9.750e+02  9.760e+02\n",
      " -9.840e+02 -9.870e+02 -9.900e+02 -9.960e+02 -9.960e+02 -9.970e+02\n",
      " -1.000e+03 -1.001e+03 -1.001e+03  1.004e+03 -1.004e+03 -1.004e+03\n",
      " -1.005e+03 -1.006e+03 -1.007e+03  1.009e+03 -1.010e+03 -1.013e+03\n",
      " -1.015e+03 -1.025e+03 -1.027e+03  1.032e+03  1.034e+03 -1.034e+03\n",
      " -1.035e+03 -1.042e+03 -1.043e+03 -1.043e+03 -1.043e+03  1.048e+03\n",
      " -1.051e+03 -1.051e+03 -1.051e+03 -1.062e+03 -1.062e+03 -1.063e+03\n",
      "  1.072e+03 -1.085e+03 -1.088e+03 -1.093e+03  1.093e+03 -1.099e+03\n",
      " -1.101e+03 -1.102e+03  1.104e+03 -1.112e+03 -1.119e+03 -1.120e+03\n",
      " -1.120e+03 -1.121e+03 -1.124e+03  1.127e+03 -1.132e+03 -1.138e+03\n",
      " -1.139e+03 -1.141e+03 -1.148e+03  1.148e+03 -1.148e+03 -1.150e+03\n",
      "  1.152e+03 -1.156e+03 -1.156e+03 -1.157e+03 -1.158e+03 -1.161e+03\n",
      " -1.162e+03 -1.163e+03 -1.165e+03 -1.167e+03  1.174e+03 -1.179e+03\n",
      " -1.185e+03 -1.186e+03 -1.189e+03 -1.191e+03 -1.196e+03 -1.203e+03\n",
      " -1.203e+03 -1.208e+03 -1.217e+03 -1.219e+03 -1.220e+03 -1.224e+03\n",
      " -1.229e+03 -1.229e+03 -1.233e+03 -1.234e+03 -1.234e+03 -1.239e+03\n",
      " -1.246e+03 -1.247e+03 -1.248e+03 -1.266e+03 -1.269e+03 -1.270e+03\n",
      "  1.272e+03  1.275e+03 -1.277e+03 -1.285e+03  1.286e+03 -1.288e+03\n",
      " -1.291e+03 -1.309e+03 -1.318e+03 -1.321e+03  1.324e+03 -1.325e+03\n",
      " -1.347e+03 -1.351e+03 -1.359e+03 -1.363e+03 -1.363e+03  1.365e+03\n",
      " -1.369e+03 -1.371e+03  1.388e+03  1.411e+03 -1.419e+03  1.430e+03\n",
      " -1.434e+03 -1.437e+03 -1.448e+03 -1.449e+03 -1.461e+03 -1.467e+03\n",
      " -1.471e+03 -1.474e+03 -1.476e+03 -1.477e+03 -1.492e+03 -1.505e+03\n",
      "  1.508e+03 -1.516e+03 -1.519e+03 -1.522e+03 -1.523e+03 -1.528e+03\n",
      " -1.534e+03 -1.535e+03  1.542e+03 -1.545e+03 -1.545e+03 -1.547e+03\n",
      " -1.548e+03 -1.561e+03  1.563e+03 -1.569e+03 -1.587e+03 -1.604e+03\n",
      " -1.604e+03 -1.611e+03 -1.611e+03 -1.611e+03 -1.612e+03 -1.613e+03\n",
      " -1.614e+03 -1.620e+03 -1.631e+03  1.642e+03 -1.642e+03 -1.644e+03\n",
      " -1.648e+03  1.649e+03 -1.662e+03 -1.673e+03  1.673e+03 -1.682e+03\n",
      " -1.686e+03  1.688e+03 -1.688e+03  1.692e+03  1.694e+03  1.699e+03\n",
      " -1.712e+03 -1.728e+03 -1.742e+03  1.759e+03 -1.780e+03  1.781e+03\n",
      " -1.783e+03  1.793e+03 -1.800e+03  1.812e+03 -1.820e+03 -1.836e+03\n",
      " -1.842e+03 -1.847e+03 -1.853e+03 -1.864e+03 -1.866e+03 -1.871e+03\n",
      " -1.876e+03  1.884e+03 -1.887e+03  1.900e+03 -1.914e+03 -1.919e+03\n",
      "  1.920e+03 -1.926e+03  1.927e+03 -1.928e+03 -1.935e+03 -1.953e+03\n",
      " -1.972e+03 -1.980e+03 -1.988e+03  2.009e+03 -2.012e+03 -2.019e+03\n",
      " -2.031e+03 -2.033e+03 -2.048e+03 -2.072e+03  2.097e+03 -2.109e+03\n",
      " -2.124e+03  2.127e+03 -2.128e+03 -2.136e+03 -2.140e+03 -2.142e+03\n",
      " -2.155e+03 -2.156e+03 -2.161e+03 -2.164e+03 -2.184e+03 -2.190e+03\n",
      " -2.197e+03  2.207e+03 -2.236e+03 -2.248e+03 -2.255e+03 -2.255e+03\n",
      " -2.255e+03 -2.263e+03  2.273e+03 -2.278e+03 -2.281e+03 -2.282e+03\n",
      " -2.288e+03 -2.289e+03  2.296e+03 -2.306e+03 -2.311e+03 -2.329e+03\n",
      " -2.330e+03 -2.335e+03  2.348e+03  2.361e+03 -2.365e+03 -2.372e+03\n",
      " -2.372e+03  2.373e+03 -2.381e+03 -2.385e+03 -2.406e+03  2.417e+03\n",
      " -2.426e+03 -2.442e+03 -2.442e+03  2.469e+03 -2.477e+03  2.483e+03\n",
      " -2.483e+03 -2.486e+03 -2.489e+03 -2.513e+03 -2.515e+03  2.520e+03\n",
      "  2.534e+03 -2.535e+03 -2.541e+03  2.551e+03 -2.558e+03 -2.559e+03\n",
      "  2.573e+03 -2.590e+03 -2.596e+03 -2.596e+03 -2.618e+03 -2.632e+03\n",
      " -2.645e+03 -2.645e+03 -2.650e+03 -2.653e+03 -2.654e+03 -2.695e+03\n",
      " -2.707e+03 -2.709e+03 -2.759e+03  2.763e+03 -2.770e+03 -2.813e+03\n",
      " -2.838e+03 -2.856e+03  2.866e+03  2.911e+03 -2.920e+03 -2.953e+03\n",
      "  2.965e+03  2.965e+03 -2.971e+03 -2.989e+03 -2.991e+03 -2.991e+03\n",
      " -3.001e+03 -3.004e+03 -3.009e+03 -3.011e+03 -3.015e+03 -3.017e+03\n",
      " -3.021e+03 -3.030e+03 -3.035e+03  3.063e+03 -3.072e+03 -3.088e+03\n",
      " -3.091e+03 -3.102e+03 -3.112e+03 -3.121e+03  3.126e+03 -3.128e+03\n",
      " -3.152e+03 -3.172e+03 -3.202e+03 -3.203e+03 -3.204e+03 -3.226e+03\n",
      " -3.247e+03 -3.256e+03  3.262e+03 -3.283e+03 -3.283e+03 -3.287e+03\n",
      " -3.307e+03 -3.316e+03 -3.342e+03 -3.364e+03  3.409e+03  3.418e+03\n",
      " -3.430e+03 -3.456e+03  3.461e+03  3.462e+03  3.472e+03 -3.506e+03\n",
      " -3.519e+03 -3.603e+03 -3.607e+03 -3.660e+03  3.669e+03 -3.709e+03\n",
      "  3.736e+03 -3.807e+03  3.873e+03  3.941e+03 -3.957e+03  3.959e+03\n",
      " -4.005e+03 -4.047e+03 -4.052e+03 -4.080e+03 -4.088e+03 -4.233e+03\n",
      "  4.267e+03 -4.285e+03 -4.354e+03 -4.361e+03  4.456e+03 -4.894e+03\n",
      " -4.929e+03 -5.042e+03 -5.062e+03 -5.156e+03 -5.176e+03 -6.292e+03\n",
      " -7.126e+03  7.455e+03 -7.777e+03 -8.008e+03 -8.391e+03 -8.556e+03\n",
      " -8.605e+03]\n",
      "durations 1.0 7106.0\n",
      "Concordance Index 0.5799930289299408\n",
      "Integrated Brier Score: 0.1804174047425288\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m66.5370\u001b[0m       \u001b[32m64.4246\u001b[0m  0.0644\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m76.2652\u001b[0m       \u001b[32m49.0435\u001b[0m  0.0678\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m155.5762\u001b[0m       \u001b[32m58.8139\u001b[0m  0.0507\n",
      "      2       \u001b[36m57.5256\u001b[0m       \u001b[32m60.1811\u001b[0m  0.0663\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m83.7746\u001b[0m       \u001b[32m89.3985\u001b[0m  0.0769\n",
      "      2      \u001b[36m149.5479\u001b[0m       \u001b[32m55.9437\u001b[0m  0.0359\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m71.3087\u001b[0m       \u001b[32m79.9014\u001b[0m  0.0704\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m161.1121\u001b[0m       \u001b[32m82.3043\u001b[0m  0.0505\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2       \u001b[36m67.2001\u001b[0m       \u001b[32m47.4849\u001b[0m  0.0553\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      3       \u001b[36m57.5228\u001b[0m       60.6062  0.0511\n",
      "      2       \u001b[36m74.1134\u001b[0m       \u001b[32m85.7196\u001b[0m  0.0595\n",
      "      3      \u001b[36m149.2958\u001b[0m       \u001b[32m55.6555\u001b[0m  0.0359\n",
      "      2      \u001b[36m152.8608\u001b[0m       \u001b[32m78.7366\u001b[0m  0.0426\n",
      "      2       \u001b[36m67.7481\u001b[0m       \u001b[32m72.2315\u001b[0m  0.0503\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m141.2457\u001b[0m       \u001b[32m80.3316\u001b[0m  0.0460\n",
      "      4      \u001b[36m146.7251\u001b[0m       56.1410  0.0343\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      3      154.3373       \u001b[32m78.5910\u001b[0m  0.0351\n",
      "      4       57.5422       60.5947  0.0619\n",
      "      3       77.4324       \u001b[32m83.9406\u001b[0m  0.0512\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "      3       \u001b[36m65.9169\u001b[0m       51.2085  0.0732\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m79.0797\u001b[0m       \u001b[32m89.1004\u001b[0m  0.0823\n",
      "      2      \u001b[36m135.4940\u001b[0m       \u001b[32m74.5370\u001b[0m  0.0324\n",
      "      5      \u001b[36m146.4232\u001b[0m       56.2406  0.0360\n",
      "      3       \u001b[36m65.1511\u001b[0m       \u001b[32m68.0209\u001b[0m  0.0614\n",
      "      4      \u001b[36m152.7585\u001b[0m       79.5540  0.0343\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m172.3228\u001b[0m      \u001b[32m115.7775\u001b[0m  0.0421\n",
      "      4       \u001b[36m72.8483\u001b[0m       86.2602  0.0493\n",
      "      3      \u001b[36m133.0452\u001b[0m       \u001b[32m73.2506\u001b[0m  0.0356\n",
      "      5       \u001b[36m56.0163\u001b[0m       62.9717  0.0548\n",
      "      6      148.1006       55.8613  0.0318\n",
      "      2       \u001b[36m69.2111\u001b[0m       \u001b[32m86.2854\u001b[0m  0.0507\n",
      "      4       \u001b[36m61.8120\u001b[0m       49.0117  0.0591\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m183.2904\u001b[0m      \u001b[32m111.7265\u001b[0m  0.0549\n",
      "      5      \u001b[36m150.9881\u001b[0m       80.6220  0.0357\n",
      "      2      \u001b[36m162.0589\u001b[0m      124.2266  0.0318\n",
      "      4       \u001b[36m64.4049\u001b[0m       68.3053  0.0440\n",
      "      4      133.6363       \u001b[32m72.3715\u001b[0m  0.0326\n",
      "      7      \u001b[36m145.8790\u001b[0m       \u001b[32m55.4943\u001b[0m  0.0312\n",
      "      6      \u001b[36m147.9922\u001b[0m       81.0098  0.0288\n",
      "      2      \u001b[36m176.3520\u001b[0m      \u001b[32m105.3485\u001b[0m  0.0363\n",
      "      3      163.3808      116.4760  0.0291\n",
      "      5       \u001b[36m71.5950\u001b[0m       90.8152  0.0513\n",
      "      6       \u001b[36m55.7775\u001b[0m       64.2837  0.0544\n",
      "      3       69.5912       \u001b[32m84.4109\u001b[0m  0.0480\n",
      "      8      \u001b[36m144.6054\u001b[0m       \u001b[32m55.3550\u001b[0m  0.0301\n",
      "      5       \u001b[36m63.5430\u001b[0m       71.1057  0.0451\n",
      "      5       63.3203       47.9710  0.0557\n",
      "      5      134.2793       \u001b[32m71.6301\u001b[0m  0.0413\n",
      "      7      148.9351       80.8413  0.0349\n",
      "      4      \u001b[36m161.5557\u001b[0m      \u001b[32m107.4818\u001b[0m  0.0302\n",
      "      3      177.8194      \u001b[32m104.4177\u001b[0m  0.0317\n",
      "      7       56.4200       62.1037  0.0432\n",
      "      6       \u001b[36m71.3562\u001b[0m       90.2481  0.0509\n",
      "      9      \u001b[36m143.5876\u001b[0m       \u001b[32m55.3518\u001b[0m  0.0358\n",
      "      6      \u001b[36m132.0520\u001b[0m       \u001b[32m71.2231\u001b[0m  0.0280\n",
      "      4       \u001b[36m68.2611\u001b[0m       85.4577  0.0485\n",
      "      4      \u001b[36m175.0053\u001b[0m      106.4291  0.0298\n",
      "      5      \u001b[36m157.9968\u001b[0m      \u001b[32m104.0650\u001b[0m  0.0320\n",
      "      8      148.8590       80.0856  0.0345\n",
      "      6       61.8482       \u001b[32m47.4669\u001b[0m  0.0474\n",
      "      6       65.3413       72.7607  0.0514\n",
      "     10      \u001b[36m142.7066\u001b[0m       55.4072  0.0312\n",
      "Restoring best model from epoch 8.\n",
      "      7      \u001b[36m130.3509\u001b[0m       \u001b[32m70.8437\u001b[0m  0.0355\n",
      "      5      \u001b[36m170.8857\u001b[0m      108.3725  0.0386\n",
      "      7       71.9188       86.9333  0.0530\n",
      "      6      \u001b[36m153.7933\u001b[0m      \u001b[32m102.1270\u001b[0m  0.0428\n",
      "      9      \u001b[36m146.4806\u001b[0m       79.2789  0.0428\n",
      "      8       55.8830       60.6134  0.0574\n",
      "      7       \u001b[36m62.9537\u001b[0m       72.2022  0.0450\n",
      "      5       \u001b[36m66.8504\u001b[0m       88.3184  0.0586\n",
      "      8      \u001b[36m129.9802\u001b[0m       \u001b[32m70.0015\u001b[0m  0.0348\n",
      "      7       63.2715       \u001b[32m47.0944\u001b[0m  0.0589\n",
      "      6      \u001b[36m168.8894\u001b[0m      107.8535  0.0390\n",
      "     10      148.6491       78.9717  0.0339\n",
      "Restoring best model from epoch 3.\n",
      "      7      157.5754      \u001b[32m100.9488\u001b[0m  0.0430\n",
      "      8       \u001b[36m67.3506\u001b[0m       85.9918  0.0495\n",
      "      9       \u001b[36m55.6770\u001b[0m       62.1277  0.0539\n",
      "      8       \u001b[36m62.8311\u001b[0m       71.4913  0.0479\n",
      "      6       \u001b[36m65.7217\u001b[0m       86.7520  0.0468\n",
      "      9      130.4984       \u001b[32m69.6519\u001b[0m  0.0392\n",
      "      7      170.1438      106.2990  0.0324\n",
      "      8      157.0296      101.0246  0.0299\n",
      "      8       \u001b[36m61.2757\u001b[0m       \u001b[32m46.9795\u001b[0m  0.0594\n",
      "      8      170.4300      \u001b[32m104.1863\u001b[0m  0.0277\n",
      "     10       \u001b[36m53.7904\u001b[0m       64.2409  0.0394\n",
      "Restoring best model from epoch 2.\n",
      "      9       71.1056       88.0743  0.0517\n",
      "     10      \u001b[36m128.4361\u001b[0m       \u001b[32m69.4727\u001b[0m  0.0391\n",
      "      7       \u001b[36m64.1182\u001b[0m       \u001b[32m83.9885\u001b[0m  0.0459\n",
      "      9      \u001b[36m151.8359\u001b[0m      102.9608  0.0405\n",
      "      9       \u001b[36m60.5320\u001b[0m       70.9204  0.0604\n",
      "      9       61.5042       \u001b[32m46.9790\u001b[0m  0.0412\n",
      "      9      \u001b[36m167.1893\u001b[0m      \u001b[32m103.4228\u001b[0m  0.0441\n",
      "     10       70.0514       91.0566  0.0404\n",
      "Restoring best model from epoch 3.\n",
      "     10      153.7516      105.4013  0.0309\n",
      "Restoring best model from epoch 7.\n",
      "      8       \u001b[36m61.6460\u001b[0m       \u001b[32m82.5281\u001b[0m  0.0402\n",
      "     10       61.6736       72.8763  0.0482\n",
      "Restoring best model from epoch 3.\n",
      "     10      167.4786      104.0366  0.0280\n",
      "Restoring best model from epoch 9.\n",
      "     10       62.4457       46.9969  0.0455\n",
      "Restoring best model from epoch 8.\n",
      "      9       62.4625       \u001b[32m82.3150\u001b[0m  0.0475\n",
      "     10       63.4393       84.4486  0.0516\n",
      "Restoring best model from epoch 9.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m171.7508\u001b[0m      \u001b[32m102.0168\u001b[0m  0.0309\n",
      "      2      \u001b[36m165.2764\u001b[0m       \u001b[32m97.8663\u001b[0m  0.0285\n",
      "      3      \u001b[36m159.9738\u001b[0m       \u001b[32m95.5876\u001b[0m  0.0287\n",
      "      4      164.5777       \u001b[32m92.4925\u001b[0m  0.0277\n",
      "      5      \u001b[36m158.8994\u001b[0m       \u001b[32m91.2114\u001b[0m  0.0282\n",
      "      6      \u001b[36m156.7537\u001b[0m       \u001b[32m90.9437\u001b[0m  0.0288\n",
      "      7      156.9638       \u001b[32m90.5337\u001b[0m  0.0278\n",
      "      8      \u001b[36m155.3533\u001b[0m       \u001b[32m89.4760\u001b[0m  0.0273\n",
      "      9      \u001b[36m152.8020\u001b[0m       \u001b[32m87.9256\u001b[0m  0.0311\n",
      "     10      156.7779       \u001b[32m87.0144\u001b[0m  0.0304\n",
      "y_train breslow final [-1.000e+00 -5.000e+00 -5.000e+00 -5.000e+00 -7.000e+00 -8.000e+00\n",
      " -9.000e+00 -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01\n",
      " -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01\n",
      " -1.000e+01 -1.000e+01 -1.600e+01 -1.900e+01 -2.400e+01 -3.000e+01\n",
      " -3.000e+01 -3.000e+01 -3.000e+01 -3.100e+01 -3.100e+01 -3.100e+01\n",
      " -3.100e+01 -3.400e+01 -4.000e+01 -4.900e+01 -5.100e+01 -5.400e+01\n",
      " -5.900e+01 -6.400e+01 -7.000e+01 -7.600e+01 -7.800e+01 -7.800e+01\n",
      " -7.800e+01 -8.000e+01 -8.400e+01 -9.000e+01 -9.200e+01  1.160e+02\n",
      " -1.180e+02 -1.490e+02  1.580e+02 -1.600e+02  1.600e+02 -1.620e+02\n",
      " -1.630e+02 -1.700e+02 -1.700e+02 -1.780e+02 -1.860e+02 -1.870e+02\n",
      " -1.960e+02  1.970e+02 -2.100e+02 -2.130e+02 -2.140e+02 -2.150e+02\n",
      " -2.160e+02 -2.170e+02 -2.180e+02 -2.220e+02  2.240e+02  2.270e+02\n",
      " -2.270e+02 -2.310e+02  2.390e+02 -2.420e+02 -2.420e+02 -2.500e+02\n",
      " -2.520e+02  2.550e+02 -2.660e+02 -2.710e+02 -2.730e+02 -2.730e+02\n",
      " -2.740e+02 -2.750e+02 -2.850e+02 -2.870e+02 -2.880e+02 -2.930e+02\n",
      " -2.930e+02  2.950e+02 -2.970e+02 -3.000e+02 -3.020e+02  3.020e+02\n",
      " -3.040e+02 -3.040e+02 -3.040e+02 -3.040e+02  3.040e+02 -3.070e+02\n",
      " -3.130e+02 -3.130e+02 -3.170e+02 -3.170e+02 -3.200e+02  3.220e+02\n",
      " -3.220e+02 -3.260e+02 -3.280e+02 -3.320e+02 -3.340e+02  3.360e+02\n",
      " -3.370e+02 -3.390e+02 -3.430e+02 -3.430e+02 -3.450e+02 -3.450e+02\n",
      " -3.470e+02  3.480e+02 -3.480e+02 -3.520e+02 -3.580e+02 -3.580e+02\n",
      " -3.580e+02  3.620e+02 -3.630e+02 -3.640e+02 -3.650e+02 -3.650e+02\n",
      " -3.650e+02  3.650e+02 -3.650e+02 -3.660e+02 -3.680e+02 -3.700e+02\n",
      " -3.710e+02 -3.730e+02 -3.730e+02 -3.750e+02 -3.750e+02 -3.750e+02\n",
      " -3.750e+02 -3.760e+02  3.770e+02 -3.800e+02 -3.810e+02 -3.810e+02\n",
      " -3.820e+02 -3.830e+02 -3.830e+02  3.850e+02 -3.850e+02 -3.850e+02\n",
      " -3.850e+02  3.850e+02 -3.850e+02 -3.920e+02 -3.930e+02 -3.930e+02\n",
      " -3.940e+02 -3.940e+02 -3.940e+02 -3.950e+02 -3.960e+02 -3.960e+02\n",
      " -3.960e+02 -3.960e+02 -3.960e+02 -3.970e+02 -3.980e+02 -4.020e+02\n",
      " -4.030e+02 -4.030e+02 -4.040e+02 -4.080e+02 -4.090e+02 -4.100e+02\n",
      " -4.100e+02 -4.110e+02 -4.120e+02 -4.140e+02 -4.160e+02 -4.170e+02\n",
      " -4.210e+02 -4.230e+02 -4.240e+02 -4.240e+02 -4.250e+02 -4.260e+02\n",
      "  4.260e+02 -4.260e+02 -4.280e+02 -4.280e+02 -4.300e+02 -4.310e+02\n",
      " -4.310e+02 -4.370e+02 -4.390e+02 -4.390e+02 -4.400e+02 -4.410e+02\n",
      " -4.410e+02 -4.460e+02 -4.470e+02 -4.480e+02 -4.500e+02 -4.500e+02\n",
      " -4.510e+02 -4.540e+02 -4.550e+02 -4.560e+02 -4.570e+02 -4.580e+02\n",
      " -4.610e+02 -4.610e+02 -4.630e+02 -4.630e+02 -4.660e+02 -4.670e+02\n",
      " -4.700e+02 -4.710e+02 -4.720e+02 -4.770e+02 -4.770e+02 -4.770e+02\n",
      " -4.800e+02 -4.820e+02 -4.880e+02 -4.890e+02 -4.920e+02 -4.950e+02\n",
      " -4.960e+02 -4.990e+02 -4.990e+02 -5.010e+02 -5.010e+02 -5.020e+02\n",
      " -5.030e+02 -5.040e+02 -5.040e+02 -5.060e+02 -5.080e+02 -5.090e+02\n",
      " -5.100e+02 -5.110e+02 -5.130e+02 -5.160e+02 -5.180e+02 -5.180e+02\n",
      " -5.190e+02 -5.190e+02 -5.230e+02  5.240e+02 -5.250e+02 -5.260e+02\n",
      " -5.280e+02 -5.280e+02 -5.300e+02 -5.310e+02 -5.320e+02 -5.320e+02\n",
      " -5.330e+02 -5.330e+02 -5.380e+02 -5.380e+02  5.380e+02 -5.410e+02\n",
      " -5.410e+02 -5.450e+02 -5.460e+02 -5.490e+02 -5.510e+02 -5.520e+02\n",
      " -5.520e+02 -5.520e+02 -5.540e+02 -5.540e+02 -5.540e+02 -5.540e+02\n",
      " -5.540e+02  5.580e+02 -5.620e+02 -5.620e+02 -5.630e+02  5.630e+02\n",
      " -5.650e+02 -5.660e+02 -5.670e+02 -5.680e+02  5.710e+02  5.730e+02\n",
      " -5.740e+02 -5.740e+02 -5.750e+02 -5.750e+02 -5.750e+02 -5.750e+02\n",
      " -5.760e+02 -5.770e+02 -5.770e+02 -5.840e+02 -5.840e+02 -5.840e+02\n",
      " -5.850e+02 -5.860e+02 -5.880e+02 -5.880e+02 -5.880e+02 -5.910e+02\n",
      " -5.940e+02 -5.950e+02 -5.960e+02 -6.000e+02 -6.020e+02 -6.060e+02\n",
      " -6.070e+02 -6.070e+02 -6.080e+02 -6.080e+02 -6.110e+02 -6.110e+02\n",
      " -6.120e+02  6.120e+02 -6.120e+02 -6.120e+02 -6.140e+02  6.140e+02\n",
      "  6.160e+02 -6.160e+02 -6.160e+02 -6.180e+02 -6.200e+02 -6.200e+02\n",
      " -6.200e+02 -6.220e+02 -6.240e+02 -6.270e+02 -6.290e+02 -6.300e+02\n",
      " -6.310e+02 -6.350e+02 -6.350e+02 -6.350e+02  6.390e+02 -6.400e+02\n",
      " -6.400e+02 -6.410e+02 -6.430e+02 -6.430e+02 -6.440e+02 -6.460e+02\n",
      " -6.470e+02 -6.480e+02 -6.510e+02 -6.520e+02 -6.550e+02 -6.580e+02\n",
      " -6.620e+02 -6.660e+02 -6.660e+02 -6.720e+02 -6.750e+02 -6.750e+02\n",
      " -6.770e+02 -6.770e+02 -6.790e+02 -6.800e+02 -6.830e+02 -6.940e+02\n",
      " -6.940e+02 -6.980e+02 -7.020e+02 -7.030e+02 -7.070e+02 -7.070e+02\n",
      " -7.090e+02 -7.100e+02 -7.140e+02 -7.140e+02 -7.150e+02 -7.150e+02\n",
      " -7.180e+02 -7.220e+02  7.230e+02 -7.240e+02 -7.250e+02 -7.270e+02\n",
      " -7.270e+02 -7.270e+02 -7.280e+02 -7.340e+02 -7.420e+02 -7.450e+02\n",
      " -7.470e+02 -7.470e+02 -7.470e+02 -7.480e+02  7.490e+02 -7.520e+02\n",
      " -7.520e+02  7.540e+02 -7.540e+02 -7.590e+02 -7.590e+02 -7.600e+02\n",
      " -7.610e+02 -7.620e+02 -7.620e+02 -7.620e+02  7.630e+02 -7.650e+02\n",
      " -7.670e+02 -7.670e+02 -7.690e+02 -7.770e+02 -7.850e+02  7.860e+02\n",
      " -7.880e+02 -7.910e+02  7.920e+02  7.920e+02 -7.950e+02 -7.980e+02\n",
      " -8.030e+02 -8.060e+02  8.110e+02 -8.120e+02 -8.190e+02 -8.200e+02\n",
      "  8.210e+02 -8.220e+02 -8.290e+02 -8.370e+02 -8.390e+02 -8.470e+02\n",
      " -8.490e+02 -8.520e+02 -8.560e+02 -8.600e+02  8.600e+02 -8.650e+02\n",
      " -8.670e+02 -8.750e+02 -8.780e+02  8.790e+02  8.830e+02 -8.890e+02\n",
      " -8.900e+02 -8.990e+02 -9.060e+02 -9.070e+02 -9.080e+02 -9.110e+02\n",
      "  9.120e+02 -9.120e+02 -9.120e+02 -9.150e+02 -9.180e+02 -9.230e+02\n",
      " -9.260e+02 -9.310e+02 -9.310e+02 -9.430e+02  9.430e+02 -9.430e+02\n",
      " -9.430e+02 -9.520e+02 -9.540e+02  9.590e+02  9.670e+02 -9.720e+02\n",
      " -9.730e+02 -9.740e+02 -9.740e+02 -9.750e+02  9.760e+02 -9.840e+02\n",
      " -9.870e+02 -9.890e+02 -9.900e+02  9.910e+02  9.910e+02 -9.960e+02\n",
      " -9.970e+02 -1.000e+03 -1.001e+03 -1.001e+03 -1.001e+03  1.004e+03\n",
      " -1.004e+03 -1.004e+03 -1.006e+03 -1.007e+03 -1.009e+03 -1.010e+03\n",
      " -1.013e+03 -1.015e+03 -1.026e+03 -1.026e+03  1.032e+03  1.034e+03\n",
      "  1.034e+03 -1.034e+03 -1.035e+03 -1.039e+03 -1.042e+03 -1.043e+03\n",
      " -1.043e+03 -1.043e+03 -1.047e+03  1.048e+03 -1.051e+03 -1.051e+03\n",
      " -1.059e+03 -1.062e+03 -1.062e+03 -1.066e+03 -1.074e+03 -1.079e+03\n",
      " -1.088e+03  1.093e+03 -1.099e+03 -1.102e+03  1.104e+03 -1.106e+03\n",
      " -1.112e+03 -1.119e+03 -1.120e+03 -1.120e+03  1.127e+03 -1.132e+03\n",
      " -1.133e+03 -1.138e+03 -1.139e+03 -1.140e+03 -1.141e+03  1.142e+03\n",
      " -1.148e+03  1.148e+03 -1.150e+03  1.152e+03 -1.156e+03 -1.157e+03\n",
      " -1.158e+03 -1.162e+03 -1.163e+03 -1.174e+03  1.174e+03 -1.179e+03\n",
      " -1.189e+03 -1.198e+03 -1.203e+03 -1.206e+03 -1.208e+03 -1.208e+03\n",
      " -1.210e+03 -1.219e+03 -1.220e+03 -1.220e+03 -1.224e+03 -1.229e+03\n",
      " -1.229e+03 -1.232e+03 -1.233e+03 -1.234e+03 -1.234e+03 -1.239e+03\n",
      " -1.247e+03 -1.251e+03 -1.266e+03 -1.270e+03  1.272e+03  1.275e+03\n",
      " -1.277e+03 -1.285e+03  1.286e+03 -1.288e+03 -1.301e+03 -1.308e+03\n",
      " -1.308e+03 -1.308e+03 -1.309e+03 -1.309e+03 -1.318e+03 -1.321e+03\n",
      "  1.324e+03 -1.325e+03 -1.326e+03 -1.330e+03 -1.351e+03 -1.363e+03\n",
      " -1.363e+03  1.365e+03 -1.369e+03 -1.371e+03  1.411e+03 -1.417e+03\n",
      " -1.417e+03 -1.419e+03  1.430e+03 -1.434e+03 -1.437e+03  1.439e+03\n",
      " -1.448e+03 -1.449e+03 -1.463e+03 -1.467e+03 -1.474e+03 -1.474e+03\n",
      " -1.476e+03 -1.477e+03 -1.492e+03 -1.505e+03  1.508e+03 -1.516e+03\n",
      " -1.519e+03 -1.522e+03 -1.523e+03 -1.528e+03 -1.532e+03 -1.535e+03\n",
      "  1.542e+03 -1.542e+03 -1.545e+03 -1.546e+03 -1.548e+03 -1.550e+03\n",
      "  1.556e+03  1.563e+03 -1.563e+03 -1.572e+03 -1.596e+03 -1.604e+03\n",
      " -1.611e+03 -1.611e+03 -1.612e+03 -1.613e+03 -1.614e+03 -1.616e+03\n",
      " -1.620e+03 -1.639e+03 -1.640e+03  1.642e+03 -1.642e+03 -1.662e+03\n",
      " -1.673e+03 -1.682e+03 -1.683e+03 -1.686e+03  1.688e+03  1.692e+03\n",
      " -1.692e+03  1.694e+03  1.699e+03 -1.712e+03 -1.728e+03 -1.728e+03\n",
      " -1.732e+03 -1.742e+03  1.759e+03 -1.780e+03  1.781e+03 -1.783e+03\n",
      "  1.793e+03 -1.800e+03  1.812e+03 -1.842e+03 -1.847e+03 -1.853e+03\n",
      " -1.855e+03 -1.873e+03 -1.882e+03  1.884e+03  1.900e+03 -1.914e+03\n",
      "  1.920e+03 -1.925e+03 -1.926e+03  1.927e+03 -1.928e+03 -1.935e+03\n",
      " -1.935e+03 -1.953e+03 -1.972e+03 -1.980e+03 -1.988e+03  1.993e+03\n",
      " -1.996e+03  2.009e+03 -2.041e+03 -2.048e+03 -2.064e+03 -2.072e+03\n",
      " -2.082e+03 -2.091e+03  2.097e+03 -2.108e+03 -2.109e+03 -2.124e+03\n",
      "  2.127e+03 -2.136e+03 -2.140e+03 -2.142e+03 -2.161e+03 -2.164e+03\n",
      " -2.190e+03 -2.191e+03  2.192e+03 -2.193e+03 -2.197e+03 -2.222e+03\n",
      " -2.231e+03 -2.236e+03 -2.240e+03 -2.246e+03 -2.248e+03 -2.255e+03\n",
      " -2.255e+03 -2.255e+03  2.273e+03 -2.278e+03 -2.281e+03 -2.282e+03\n",
      " -2.289e+03  2.296e+03 -2.306e+03 -2.329e+03 -2.330e+03 -2.335e+03\n",
      "  2.348e+03 -2.362e+03 -2.365e+03 -2.371e+03 -2.372e+03 -2.372e+03\n",
      "  2.373e+03 -2.381e+03 -2.385e+03 -2.403e+03 -2.406e+03  2.417e+03\n",
      " -2.442e+03 -2.442e+03 -2.483e+03 -2.486e+03 -2.489e+03 -2.513e+03\n",
      " -2.515e+03  2.534e+03 -2.535e+03 -2.558e+03  2.573e+03 -2.590e+03\n",
      " -2.596e+03 -2.605e+03 -2.612e+03 -2.618e+03 -2.629e+03 -2.632e+03\n",
      " -2.632e+03  2.636e+03 -2.645e+03 -2.645e+03 -2.654e+03 -2.695e+03\n",
      " -2.707e+03 -2.709e+03  2.712e+03 -2.721e+03 -2.759e+03 -2.767e+03\n",
      " -2.770e+03  2.798e+03  2.854e+03 -2.856e+03 -2.868e+03  2.911e+03\n",
      " -2.920e+03 -2.953e+03 -2.976e+03 -2.989e+03 -2.991e+03 -2.991e+03\n",
      " -3.004e+03 -3.009e+03 -3.011e+03 -3.015e+03 -3.017e+03 -3.021e+03\n",
      " -3.022e+03 -3.030e+03  3.063e+03 -3.072e+03 -3.088e+03 -3.091e+03\n",
      " -3.094e+03 -3.102e+03 -3.112e+03 -3.121e+03  3.126e+03 -3.159e+03\n",
      " -3.202e+03 -3.204e+03 -3.247e+03 -3.248e+03 -3.261e+03  3.262e+03\n",
      " -3.276e+03 -3.283e+03 -3.286e+03 -3.287e+03 -3.296e+03 -3.307e+03\n",
      " -3.316e+03 -3.342e+03 -3.361e+03  3.409e+03  3.418e+03 -3.456e+03\n",
      "  3.472e+03  3.492e+03 -3.506e+03 -3.519e+03 -3.603e+03 -3.607e+03\n",
      " -3.660e+03 -3.709e+03  3.736e+03 -3.807e+03  3.873e+03  3.926e+03\n",
      "  3.941e+03  3.945e+03  3.959e+03 -4.005e+03 -4.047e+03 -4.052e+03\n",
      " -4.088e+03 -4.159e+03 -4.233e+03  4.267e+03 -4.275e+03 -4.354e+03\n",
      " -4.361e+03  4.456e+03 -4.894e+03 -4.929e+03 -5.042e+03 -5.062e+03\n",
      " -5.156e+03 -5.176e+03 -5.739e+03 -6.292e+03  6.456e+03  6.593e+03\n",
      " -7.106e+03 -7.126e+03  7.455e+03 -7.777e+03 -8.008e+03 -8.556e+03\n",
      " -8.605e+03]\n",
      "Concordance Index 0.6724017614360999\n",
      "Integrated Brier Score: 0.1929104959749627\n",
      "y_train breslow final [-1.000e+00 -5.000e+00 -5.000e+00 -5.000e+00 -7.000e+00 -8.000e+00\n",
      " -9.000e+00 -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01\n",
      " -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01 -1.000e+01\n",
      " -1.000e+01 -1.000e+01 -1.600e+01 -1.900e+01 -2.400e+01 -3.000e+01\n",
      " -3.000e+01 -3.000e+01 -3.000e+01 -3.100e+01 -3.100e+01 -3.100e+01\n",
      " -3.100e+01 -3.400e+01 -4.000e+01 -4.900e+01 -5.100e+01 -5.400e+01\n",
      " -5.900e+01 -6.400e+01 -7.000e+01 -7.600e+01 -7.800e+01 -7.800e+01\n",
      " -7.800e+01 -8.000e+01 -8.400e+01 -9.000e+01 -9.200e+01  1.160e+02\n",
      " -1.180e+02 -1.490e+02  1.580e+02 -1.600e+02  1.600e+02 -1.620e+02\n",
      " -1.630e+02 -1.700e+02 -1.700e+02 -1.780e+02 -1.860e+02 -1.870e+02\n",
      " -1.960e+02  1.970e+02 -2.100e+02 -2.130e+02 -2.140e+02 -2.150e+02\n",
      " -2.160e+02 -2.170e+02 -2.180e+02 -2.220e+02  2.240e+02  2.270e+02\n",
      " -2.270e+02 -2.310e+02  2.390e+02 -2.420e+02 -2.420e+02 -2.500e+02\n",
      " -2.520e+02  2.550e+02 -2.660e+02 -2.710e+02 -2.730e+02 -2.730e+02\n",
      " -2.740e+02 -2.750e+02 -2.850e+02 -2.870e+02 -2.880e+02 -2.930e+02\n",
      " -2.930e+02  2.950e+02 -2.970e+02 -3.000e+02 -3.020e+02  3.020e+02\n",
      " -3.040e+02 -3.040e+02 -3.040e+02 -3.040e+02  3.040e+02 -3.070e+02\n",
      " -3.130e+02 -3.130e+02 -3.170e+02 -3.170e+02 -3.200e+02  3.220e+02\n",
      " -3.220e+02 -3.260e+02 -3.280e+02 -3.320e+02 -3.340e+02  3.360e+02\n",
      " -3.370e+02 -3.390e+02 -3.430e+02 -3.430e+02 -3.450e+02 -3.450e+02\n",
      " -3.470e+02  3.480e+02 -3.480e+02 -3.520e+02 -3.580e+02 -3.580e+02\n",
      " -3.580e+02  3.620e+02 -3.630e+02 -3.640e+02 -3.650e+02 -3.650e+02\n",
      " -3.650e+02  3.650e+02 -3.650e+02 -3.660e+02 -3.680e+02 -3.700e+02\n",
      " -3.710e+02 -3.730e+02 -3.730e+02 -3.750e+02 -3.750e+02 -3.750e+02\n",
      " -3.750e+02 -3.760e+02  3.770e+02 -3.800e+02 -3.810e+02 -3.810e+02\n",
      " -3.820e+02 -3.830e+02 -3.830e+02  3.850e+02 -3.850e+02 -3.850e+02\n",
      " -3.850e+02  3.850e+02 -3.850e+02 -3.920e+02 -3.930e+02 -3.930e+02\n",
      " -3.940e+02 -3.940e+02 -3.940e+02 -3.950e+02 -3.960e+02 -3.960e+02\n",
      " -3.960e+02 -3.960e+02 -3.960e+02 -3.970e+02 -3.980e+02 -4.020e+02\n",
      " -4.030e+02 -4.030e+02 -4.040e+02 -4.080e+02 -4.090e+02 -4.100e+02\n",
      " -4.100e+02 -4.110e+02 -4.120e+02 -4.140e+02 -4.160e+02 -4.170e+02\n",
      " -4.210e+02 -4.230e+02 -4.240e+02 -4.240e+02 -4.250e+02 -4.260e+02\n",
      "  4.260e+02 -4.260e+02 -4.280e+02 -4.280e+02 -4.300e+02 -4.310e+02\n",
      " -4.310e+02 -4.370e+02 -4.390e+02 -4.390e+02 -4.400e+02 -4.410e+02\n",
      " -4.410e+02 -4.460e+02 -4.470e+02 -4.480e+02 -4.500e+02 -4.500e+02\n",
      " -4.510e+02 -4.540e+02 -4.550e+02 -4.560e+02 -4.570e+02 -4.580e+02\n",
      " -4.610e+02 -4.610e+02 -4.630e+02 -4.630e+02 -4.660e+02 -4.670e+02\n",
      " -4.700e+02 -4.710e+02 -4.720e+02 -4.770e+02 -4.770e+02 -4.770e+02\n",
      " -4.800e+02 -4.820e+02 -4.880e+02 -4.890e+02 -4.920e+02 -4.950e+02\n",
      " -4.960e+02 -4.990e+02 -4.990e+02 -5.010e+02 -5.010e+02 -5.020e+02\n",
      " -5.030e+02 -5.040e+02 -5.040e+02 -5.060e+02 -5.080e+02 -5.090e+02\n",
      " -5.100e+02 -5.110e+02 -5.130e+02 -5.160e+02 -5.180e+02 -5.180e+02\n",
      " -5.190e+02 -5.190e+02 -5.230e+02  5.240e+02 -5.250e+02 -5.260e+02\n",
      " -5.280e+02 -5.280e+02 -5.300e+02 -5.310e+02 -5.320e+02 -5.320e+02\n",
      " -5.330e+02 -5.330e+02 -5.380e+02 -5.380e+02  5.380e+02 -5.410e+02\n",
      " -5.410e+02 -5.450e+02 -5.460e+02 -5.490e+02 -5.510e+02 -5.520e+02\n",
      " -5.520e+02 -5.520e+02 -5.540e+02 -5.540e+02 -5.540e+02 -5.540e+02\n",
      " -5.540e+02  5.580e+02 -5.620e+02 -5.620e+02 -5.630e+02  5.630e+02\n",
      " -5.650e+02 -5.660e+02 -5.670e+02 -5.680e+02  5.710e+02  5.730e+02\n",
      " -5.740e+02 -5.740e+02 -5.750e+02 -5.750e+02 -5.750e+02 -5.750e+02\n",
      " -5.760e+02 -5.770e+02 -5.770e+02 -5.840e+02 -5.840e+02 -5.840e+02\n",
      " -5.850e+02 -5.860e+02 -5.880e+02 -5.880e+02 -5.880e+02 -5.910e+02\n",
      " -5.940e+02 -5.950e+02 -5.960e+02 -6.000e+02 -6.020e+02 -6.060e+02\n",
      " -6.070e+02 -6.070e+02 -6.080e+02 -6.080e+02 -6.110e+02 -6.110e+02\n",
      " -6.120e+02  6.120e+02 -6.120e+02 -6.120e+02 -6.140e+02  6.140e+02\n",
      "  6.160e+02 -6.160e+02 -6.160e+02 -6.180e+02 -6.200e+02 -6.200e+02\n",
      " -6.200e+02 -6.220e+02 -6.240e+02 -6.270e+02 -6.290e+02 -6.300e+02\n",
      " -6.310e+02 -6.350e+02 -6.350e+02 -6.350e+02  6.390e+02 -6.400e+02\n",
      " -6.400e+02 -6.410e+02 -6.430e+02 -6.430e+02 -6.440e+02 -6.460e+02\n",
      " -6.470e+02 -6.480e+02 -6.510e+02 -6.520e+02 -6.550e+02 -6.580e+02\n",
      " -6.620e+02 -6.660e+02 -6.660e+02 -6.720e+02 -6.750e+02 -6.750e+02\n",
      " -6.770e+02 -6.770e+02 -6.790e+02 -6.800e+02 -6.830e+02 -6.940e+02\n",
      " -6.940e+02 -6.980e+02 -7.020e+02 -7.030e+02 -7.070e+02 -7.070e+02\n",
      " -7.090e+02 -7.100e+02 -7.140e+02 -7.140e+02 -7.150e+02 -7.150e+02\n",
      " -7.180e+02 -7.220e+02  7.230e+02 -7.240e+02 -7.250e+02 -7.270e+02\n",
      " -7.270e+02 -7.270e+02 -7.280e+02 -7.340e+02 -7.420e+02 -7.450e+02\n",
      " -7.470e+02 -7.470e+02 -7.470e+02 -7.480e+02  7.490e+02 -7.520e+02\n",
      " -7.520e+02  7.540e+02 -7.540e+02 -7.590e+02 -7.590e+02 -7.600e+02\n",
      " -7.610e+02 -7.620e+02 -7.620e+02 -7.620e+02  7.630e+02 -7.650e+02\n",
      " -7.670e+02 -7.670e+02 -7.690e+02 -7.770e+02 -7.850e+02  7.860e+02\n",
      " -7.880e+02 -7.910e+02  7.920e+02  7.920e+02 -7.950e+02 -7.980e+02\n",
      " -8.030e+02 -8.060e+02  8.110e+02 -8.120e+02 -8.190e+02 -8.200e+02\n",
      "  8.210e+02 -8.220e+02 -8.290e+02 -8.370e+02 -8.390e+02 -8.470e+02\n",
      " -8.490e+02 -8.520e+02 -8.560e+02 -8.600e+02  8.600e+02 -8.650e+02\n",
      " -8.670e+02 -8.750e+02 -8.780e+02  8.790e+02  8.830e+02 -8.890e+02\n",
      " -8.900e+02 -8.990e+02 -9.060e+02 -9.070e+02 -9.080e+02 -9.110e+02\n",
      "  9.120e+02 -9.120e+02 -9.120e+02 -9.150e+02 -9.180e+02 -9.230e+02\n",
      " -9.260e+02 -9.310e+02 -9.310e+02 -9.430e+02  9.430e+02 -9.430e+02\n",
      " -9.430e+02 -9.520e+02 -9.540e+02  9.590e+02  9.670e+02 -9.720e+02\n",
      " -9.730e+02 -9.740e+02 -9.740e+02 -9.750e+02  9.760e+02 -9.840e+02\n",
      " -9.870e+02 -9.890e+02 -9.900e+02  9.910e+02  9.910e+02 -9.960e+02\n",
      " -9.970e+02 -1.000e+03 -1.001e+03 -1.001e+03 -1.001e+03  1.004e+03\n",
      " -1.004e+03 -1.004e+03 -1.006e+03 -1.007e+03 -1.009e+03 -1.010e+03\n",
      " -1.013e+03 -1.015e+03 -1.026e+03 -1.026e+03  1.032e+03  1.034e+03\n",
      "  1.034e+03 -1.034e+03 -1.035e+03 -1.039e+03 -1.042e+03 -1.043e+03\n",
      " -1.043e+03 -1.043e+03 -1.047e+03  1.048e+03 -1.051e+03 -1.051e+03\n",
      " -1.059e+03 -1.062e+03 -1.062e+03 -1.066e+03 -1.074e+03 -1.079e+03\n",
      " -1.088e+03  1.093e+03 -1.099e+03 -1.102e+03  1.104e+03 -1.106e+03\n",
      " -1.112e+03 -1.119e+03 -1.120e+03 -1.120e+03  1.127e+03 -1.132e+03\n",
      " -1.133e+03 -1.138e+03 -1.139e+03 -1.140e+03 -1.141e+03  1.142e+03\n",
      " -1.148e+03  1.148e+03 -1.150e+03  1.152e+03 -1.156e+03 -1.157e+03\n",
      " -1.158e+03 -1.162e+03 -1.163e+03 -1.174e+03  1.174e+03 -1.179e+03\n",
      " -1.189e+03 -1.198e+03 -1.203e+03 -1.206e+03 -1.208e+03 -1.208e+03\n",
      " -1.210e+03 -1.219e+03 -1.220e+03 -1.220e+03 -1.224e+03 -1.229e+03\n",
      " -1.229e+03 -1.232e+03 -1.233e+03 -1.234e+03 -1.234e+03 -1.239e+03\n",
      " -1.247e+03 -1.251e+03 -1.266e+03 -1.270e+03  1.272e+03  1.275e+03\n",
      " -1.277e+03 -1.285e+03  1.286e+03 -1.288e+03 -1.301e+03 -1.308e+03\n",
      " -1.308e+03 -1.308e+03 -1.309e+03 -1.309e+03 -1.318e+03 -1.321e+03\n",
      "  1.324e+03 -1.325e+03 -1.326e+03 -1.330e+03 -1.351e+03 -1.363e+03\n",
      " -1.363e+03  1.365e+03 -1.369e+03 -1.371e+03  1.411e+03 -1.417e+03\n",
      " -1.417e+03 -1.419e+03  1.430e+03 -1.434e+03 -1.437e+03  1.439e+03\n",
      " -1.448e+03 -1.449e+03 -1.463e+03 -1.467e+03 -1.474e+03 -1.474e+03\n",
      " -1.476e+03 -1.477e+03 -1.492e+03 -1.505e+03  1.508e+03 -1.516e+03\n",
      " -1.519e+03 -1.522e+03 -1.523e+03 -1.528e+03 -1.532e+03 -1.535e+03\n",
      "  1.542e+03 -1.542e+03 -1.545e+03 -1.546e+03 -1.548e+03 -1.550e+03\n",
      "  1.556e+03  1.563e+03 -1.563e+03 -1.572e+03 -1.596e+03 -1.604e+03\n",
      " -1.611e+03 -1.611e+03 -1.612e+03 -1.613e+03 -1.614e+03 -1.616e+03\n",
      " -1.620e+03 -1.639e+03 -1.640e+03  1.642e+03 -1.642e+03 -1.662e+03\n",
      " -1.673e+03 -1.682e+03 -1.683e+03 -1.686e+03  1.688e+03  1.692e+03\n",
      " -1.692e+03  1.694e+03  1.699e+03 -1.712e+03 -1.728e+03 -1.728e+03\n",
      " -1.732e+03 -1.742e+03  1.759e+03 -1.780e+03  1.781e+03 -1.783e+03\n",
      "  1.793e+03 -1.800e+03  1.812e+03 -1.842e+03 -1.847e+03 -1.853e+03\n",
      " -1.855e+03 -1.873e+03 -1.882e+03  1.884e+03  1.900e+03 -1.914e+03\n",
      "  1.920e+03 -1.925e+03 -1.926e+03  1.927e+03 -1.928e+03 -1.935e+03\n",
      " -1.935e+03 -1.953e+03 -1.972e+03 -1.980e+03 -1.988e+03  1.993e+03\n",
      " -1.996e+03  2.009e+03 -2.041e+03 -2.048e+03 -2.064e+03 -2.072e+03\n",
      " -2.082e+03 -2.091e+03  2.097e+03 -2.108e+03 -2.109e+03 -2.124e+03\n",
      "  2.127e+03 -2.136e+03 -2.140e+03 -2.142e+03 -2.161e+03 -2.164e+03\n",
      " -2.190e+03 -2.191e+03  2.192e+03 -2.193e+03 -2.197e+03 -2.222e+03\n",
      " -2.231e+03 -2.236e+03 -2.240e+03 -2.246e+03 -2.248e+03 -2.255e+03\n",
      " -2.255e+03 -2.255e+03  2.273e+03 -2.278e+03 -2.281e+03 -2.282e+03\n",
      " -2.289e+03  2.296e+03 -2.306e+03 -2.329e+03 -2.330e+03 -2.335e+03\n",
      "  2.348e+03 -2.362e+03 -2.365e+03 -2.371e+03 -2.372e+03 -2.372e+03\n",
      "  2.373e+03 -2.381e+03 -2.385e+03 -2.403e+03 -2.406e+03  2.417e+03\n",
      " -2.442e+03 -2.442e+03 -2.483e+03 -2.486e+03 -2.489e+03 -2.513e+03\n",
      " -2.515e+03  2.534e+03 -2.535e+03 -2.558e+03  2.573e+03 -2.590e+03\n",
      " -2.596e+03 -2.605e+03 -2.612e+03 -2.618e+03 -2.629e+03 -2.632e+03\n",
      " -2.632e+03  2.636e+03 -2.645e+03 -2.645e+03 -2.654e+03 -2.695e+03\n",
      " -2.707e+03 -2.709e+03  2.712e+03 -2.721e+03 -2.759e+03 -2.767e+03\n",
      " -2.770e+03  2.798e+03  2.854e+03 -2.856e+03 -2.868e+03  2.911e+03\n",
      " -2.920e+03 -2.953e+03 -2.976e+03 -2.989e+03 -2.991e+03 -2.991e+03\n",
      " -3.004e+03 -3.009e+03 -3.011e+03 -3.015e+03 -3.017e+03 -3.021e+03\n",
      " -3.022e+03 -3.030e+03  3.063e+03 -3.072e+03 -3.088e+03 -3.091e+03\n",
      " -3.094e+03 -3.102e+03 -3.112e+03 -3.121e+03  3.126e+03 -3.159e+03\n",
      " -3.202e+03 -3.204e+03 -3.247e+03 -3.248e+03 -3.261e+03  3.262e+03\n",
      " -3.276e+03 -3.283e+03 -3.286e+03 -3.287e+03 -3.296e+03 -3.307e+03\n",
      " -3.316e+03 -3.342e+03 -3.361e+03  3.409e+03  3.418e+03 -3.456e+03\n",
      "  3.472e+03  3.492e+03 -3.506e+03 -3.519e+03 -3.603e+03 -3.607e+03\n",
      " -3.660e+03 -3.709e+03  3.736e+03 -3.807e+03  3.873e+03  3.926e+03\n",
      "  3.941e+03  3.945e+03  3.959e+03 -4.005e+03 -4.047e+03 -4.052e+03\n",
      " -4.088e+03 -4.159e+03 -4.233e+03  4.267e+03 -4.275e+03 -4.354e+03\n",
      " -4.361e+03  4.456e+03 -4.894e+03 -4.929e+03 -5.042e+03 -5.062e+03\n",
      " -5.156e+03 -5.176e+03 -5.739e+03 -6.292e+03  6.456e+03  6.593e+03\n",
      " -7.106e+03 -7.126e+03  7.455e+03 -7.777e+03 -8.008e+03 -8.556e+03\n",
      " -8.605e+03]\n",
      "durations 1.0 8391.0\n",
      "Concordance Index 0.5435717076983196\n",
      "Integrated Brier Score: 0.21926749974658102\n",
      "split gex_?|100130426      float32\n",
      "gex_?|100133144      float32\n",
      "gex_?|100134869      float32\n",
      "gex_?|10357          float32\n",
      "gex_?|10431          float32\n",
      "                      ...   \n",
      "gex_ZYG11A|440590    float32\n",
      "gex_ZYG11B|79699     float32\n",
      "gex_ZYX|7791         float32\n",
      "gex_ZZEF1|23140      float32\n",
      "gex_ZZZ3|26009       float32\n",
      "Length: 20531, dtype: object\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m497.8682\u001b[0m      \u001b[32m101.3600\u001b[0m  0.0219\n",
      "      2      \u001b[36m462.6114\u001b[0m       \u001b[32m97.7380\u001b[0m  0.0252\n",
      "      3      \u001b[36m459.8615\u001b[0m       \u001b[32m96.5778\u001b[0m  0.0196\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      4      462.7873       \u001b[32m95.7333\u001b[0m  0.0222\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      5      \u001b[36m459.2295\u001b[0m       \u001b[32m95.6427\u001b[0m  0.0279\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m442.8130\u001b[0m       \u001b[32m71.9221\u001b[0m  0.0553\n",
      "      6      \u001b[36m455.0723\u001b[0m       96.0330  0.0299\n",
      "      2      \u001b[36m412.0978\u001b[0m       \u001b[32m65.2997\u001b[0m  0.0341\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      7      \u001b[36m452.2943\u001b[0m       96.5026  0.0327\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m226.0866\u001b[0m      \u001b[32m134.0328\u001b[0m  0.0707\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m198.0711\u001b[0m       \u001b[32m67.3614\u001b[0m  0.0548\n",
      "      3      \u001b[36m407.5418\u001b[0m       \u001b[32m63.3251\u001b[0m  0.0214\n",
      "      8      \u001b[36m451.2580\u001b[0m       96.6615  0.0217\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m533.9033\u001b[0m      \u001b[32m105.3906\u001b[0m  0.0227\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2      \u001b[36m222.0104\u001b[0m      \u001b[32m132.2170\u001b[0m  0.0293\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m231.8859\u001b[0m      \u001b[32m116.7906\u001b[0m  0.0306\n",
      "      2      \u001b[36m182.2072\u001b[0m       \u001b[32m62.6832\u001b[0m  0.0295\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m531.2939\u001b[0m      \u001b[32m134.6496\u001b[0m  0.0359\n",
      "      9      \u001b[36m447.3824\u001b[0m       96.6687  0.0212\n",
      "      2      \u001b[36m510.4750\u001b[0m      \u001b[32m102.1416\u001b[0m  0.0223\n",
      "      4      408.8406       \u001b[32m62.1118\u001b[0m  0.0343\n",
      "     10      \u001b[36m444.9051\u001b[0m       96.5466  0.0216\n",
      "Restoring best model from epoch 5.\n",
      "      3      \u001b[36m212.2194\u001b[0m      \u001b[32m122.0518\u001b[0m  0.0306\n",
      "      2      \u001b[36m504.8201\u001b[0m      \u001b[32m127.9244\u001b[0m  0.0260\n",
      "      3      \u001b[36m510.2074\u001b[0m      \u001b[32m101.6095\u001b[0m  0.0188\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m213.8140\u001b[0m      \u001b[32m137.2234\u001b[0m  0.0393\n",
      "      5      410.8080       \u001b[32m61.0961\u001b[0m  0.0181\n",
      "      2      \u001b[36m224.7951\u001b[0m       \u001b[32m96.0471\u001b[0m  0.0367\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      3      \u001b[36m178.4151\u001b[0m       64.8679  0.0384\n",
      "      4      \u001b[36m509.3165\u001b[0m      101.6143  0.0204\n",
      "      3      \u001b[36m500.8489\u001b[0m      \u001b[32m124.0202\u001b[0m  0.0255\n",
      "      6      \u001b[36m406.8146\u001b[0m       \u001b[32m60.1168\u001b[0m  0.0265\n",
      "      4      \u001b[36m203.4628\u001b[0m      \u001b[32m121.1503\u001b[0m  0.0309\n",
      "      4      178.6533       68.6614  0.0318\n",
      "      2      \u001b[36m207.6417\u001b[0m      144.4078  0.0411\n",
      "      3      \u001b[36m213.6201\u001b[0m       99.1005  0.0413\n",
      "      5      \u001b[36m502.1527\u001b[0m      \u001b[32m101.1910\u001b[0m  0.0239\n",
      "      4      \u001b[36m496.8527\u001b[0m      \u001b[32m121.5811\u001b[0m  0.0205\n",
      "      7      \u001b[36m401.6454\u001b[0m       \u001b[32m59.6080\u001b[0m  0.0200\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m509.2001\u001b[0m      \u001b[32m143.6270\u001b[0m  0.0501\n",
      "      6      505.0303      \u001b[32m100.5767\u001b[0m  0.0198\n",
      "      5      501.8273      \u001b[32m120.7135\u001b[0m  0.0183\n",
      "      8      406.8080       \u001b[32m59.4395\u001b[0m  0.0166\n",
      "      3      \u001b[36m206.0737\u001b[0m      \u001b[32m132.9658\u001b[0m  0.0280\n",
      "      5      \u001b[36m202.6918\u001b[0m      122.6080  0.0413\n",
      "      5      \u001b[36m175.3439\u001b[0m       66.8165  0.0330\n",
      "      4      215.3910      105.6772  0.0339\n",
      "      2      \u001b[36m486.2190\u001b[0m      \u001b[32m133.7514\u001b[0m  0.0180\n",
      "      6      498.3208      121.1156  0.0179\n",
      "      7      \u001b[36m497.4183\u001b[0m      \u001b[32m100.4103\u001b[0m  0.0185\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      9      \u001b[36m401.1287\u001b[0m       59.5837  0.0169\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      6      \u001b[36m171.1338\u001b[0m       64.2028  0.0243\n",
      "      3      \u001b[36m471.7692\u001b[0m      \u001b[32m131.2845\u001b[0m  0.0182\n",
      "      4      \u001b[36m198.0036\u001b[0m      \u001b[32m122.8856\u001b[0m  0.0319\n",
      "      6      204.8711      122.7016  0.0326\n",
      "      7      \u001b[36m488.9061\u001b[0m      122.0500  0.0230\n",
      "     10      \u001b[36m397.5550\u001b[0m       59.9846  0.0276\n",
      "Restoring best model from epoch 8.\n",
      "      8      \u001b[36m494.3842\u001b[0m      100.5882  0.0306\n",
      "      5      \u001b[36m208.9889\u001b[0m      105.7089  0.0431\n",
      "      4      476.9062      \u001b[32m130.5826\u001b[0m  0.0260\n",
      "      8      493.2916      123.2644  0.0198\n",
      "      7      \u001b[36m170.8121\u001b[0m       63.6663  0.0387\n",
      "      7      203.3077      124.7068  0.0348\n",
      "      9      \u001b[36m492.6059\u001b[0m      100.8969  0.0243\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m216.1612\u001b[0m      \u001b[32m102.1057\u001b[0m  0.0471\n",
      "      5      \u001b[36m197.5510\u001b[0m      \u001b[32m121.3960\u001b[0m  0.0359\n",
      "      9      490.3599      123.6905  0.0181\n",
      "      5      478.3589      \u001b[32m130.1159\u001b[0m  0.0260\n",
      "      6      \u001b[36m208.3991\u001b[0m      104.6349  0.0409\n",
      "     10      \u001b[36m489.4688\u001b[0m      101.1565  0.0225\n",
      "Restoring best model from epoch 7.\n",
      "      8      174.1376       63.8366  0.0294\n",
      "      8      \u001b[36m200.6214\u001b[0m      128.1824  0.0285\n",
      "     10      \u001b[36m487.9856\u001b[0m      123.8952  0.0245\n",
      "Restoring best model from epoch 5.\n",
      "      6      199.3078      \u001b[32m121.1420\u001b[0m  0.0319\n",
      "      2      \u001b[36m200.9138\u001b[0m      \u001b[32m101.0142\u001b[0m  0.0346\n",
      "      6      479.6433      130.4324  0.0380\n",
      "      7      \u001b[36m208.3025\u001b[0m      101.6360  0.0352\n",
      "      9      171.9943       62.9264  0.0376\n",
      "      9      \u001b[36m199.8467\u001b[0m      130.6635  0.0410\n",
      "      7      198.8763      122.3121  0.0350\n",
      "      7      \u001b[36m467.5086\u001b[0m      132.0374  0.0236\n",
      "      3      \u001b[36m196.0404\u001b[0m       \u001b[32m99.5792\u001b[0m  0.0392\n",
      "     10      \u001b[36m170.7588\u001b[0m       62.8290  0.0248\n",
      "Restoring best model from epoch 2.\n",
      "      8      210.1817       98.1134  0.0353\n",
      "     10      204.1177      130.3609  0.0284\n",
      "      8      \u001b[36m465.8910\u001b[0m      134.2120  0.0230\n",
      "Restoring best model from epoch 4.\n",
      "      8      \u001b[36m192.7692\u001b[0m      124.1812  0.0314\n",
      "      4      \u001b[36m193.9402\u001b[0m       \u001b[32m99.1160\u001b[0m  0.0275\n",
      "      9      468.5213      135.4193  0.0193\n",
      "      9      208.3534       96.8229  0.0304\n",
      "      9      \u001b[36m189.3812\u001b[0m      127.7003  0.0262\n",
      "      5      \u001b[36m190.8777\u001b[0m       \u001b[32m97.6695\u001b[0m  0.0295\n",
      "     10      467.4301      134.6756  0.0207\n",
      "Restoring best model from epoch 5.\n",
      "     10      \u001b[36m205.7164\u001b[0m       97.5393  0.0312\n",
      "Restoring best model from epoch 2.\n",
      "     10      196.3048      130.4417  0.0307\n",
      "Restoring best model from epoch 6.\n",
      "      6      191.6030       98.7909  0.0299\n",
      "      7      \u001b[36m186.6669\u001b[0m       97.7454  0.0247\n",
      "      8      190.2662       \u001b[32m95.6439\u001b[0m  0.0259\n",
      "      9      \u001b[36m186.4964\u001b[0m       \u001b[32m94.9921\u001b[0m  0.0285\n",
      "     10      \u001b[36m186.3009\u001b[0m       95.3008  0.0252\n",
      "Restoring best model from epoch 9.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m222.3372\u001b[0m      \u001b[32m147.7221\u001b[0m  0.0269\n",
      "      2      \u001b[36m214.1509\u001b[0m      \u001b[32m140.1844\u001b[0m  0.0244\n",
      "      3      \u001b[36m208.6607\u001b[0m      \u001b[32m137.8964\u001b[0m  0.0251\n",
      "      4      \u001b[36m204.6370\u001b[0m      137.9657  0.0233\n",
      "      5      207.9834      138.8338  0.0244\n",
      "      6      \u001b[36m200.0747\u001b[0m      139.6890  0.0245\n",
      "      7      201.5901      139.2073  0.0234\n",
      "      8      \u001b[36m198.7371\u001b[0m      137.9938  0.0243\n",
      "      9      \u001b[36m194.8595\u001b[0m      \u001b[32m137.7119\u001b[0m  0.0226\n",
      "     10      195.8382      138.2020  0.0229\n",
      "Restoring best model from epoch 9.\n",
      "y_train breslow final [ 2.000e+00  2.000e+00 -1.100e+01  1.100e+01 -1.400e+01  1.400e+01\n",
      "  3.200e+01  5.600e+01 -5.600e+01  6.200e+01  6.400e+01  6.500e+01\n",
      "  6.900e+01 -7.700e+01  8.200e+01  8.900e+01  9.000e+01 -9.200e+01\n",
      "  9.400e+01 -1.020e+02 -1.050e+02  1.070e+02  1.080e+02 -1.110e+02\n",
      "  1.120e+02  1.260e+02  1.290e+02  1.300e+02 -1.340e+02  1.420e+02\n",
      "  1.440e+02  1.510e+02 -1.560e+02  1.600e+02  1.660e+02  1.710e+02\n",
      "  1.730e+02 -1.810e+02  1.820e+02  1.830e+02  1.850e+02 -1.870e+02\n",
      "  1.910e+02  1.940e+02  1.940e+02  1.970e+02  1.980e+02  2.050e+02\n",
      "  2.090e+02  2.150e+02  2.150e+02 -2.160e+02  2.170e+02  2.170e+02\n",
      "  2.180e+02  2.220e+02 -2.250e+02 -2.320e+02 -2.340e+02 -2.420e+02\n",
      "  2.430e+02 -2.530e+02  2.530e+02  2.610e+02  2.680e+02  2.760e+02\n",
      " -2.780e+02  2.790e+02  2.810e+02  2.820e+02  2.840e+02  2.920e+02\n",
      "  2.940e+02  2.950e+02 -3.110e+02  3.170e+02  3.170e+02 -3.180e+02\n",
      "  3.210e+02  3.270e+02  3.270e+02  3.420e+02 -3.450e+02  3.480e+02\n",
      " -3.510e+02  3.510e+02  3.520e+02  3.530e+02 -3.540e+02  3.570e+02\n",
      " -3.580e+02  3.590e+02 -3.600e+02  3.610e+02  3.660e+02 -3.670e+02\n",
      " -3.680e+02  3.710e+02 -3.760e+02  3.770e+02 -3.780e+02  3.790e+02\n",
      " -3.790e+02  3.800e+02 -3.840e+02  3.850e+02 -3.860e+02  3.870e+02\n",
      " -3.870e+02 -3.890e+02 -3.920e+02 -3.920e+02  3.930e+02 -3.930e+02\n",
      "  3.950e+02  3.950e+02  3.970e+02 -4.000e+02 -4.030e+02  4.030e+02\n",
      "  4.030e+02  4.060e+02  4.070e+02 -4.120e+02 -4.140e+02 -4.170e+02\n",
      "  4.210e+02 -4.210e+02 -4.240e+02  4.240e+02  4.360e+02 -4.400e+02\n",
      " -4.410e+02 -4.430e+02  4.460e+02  4.530e+02  4.560e+02 -4.580e+02\n",
      "  4.590e+02 -4.610e+02 -4.660e+02 -4.690e+02 -4.710e+02  4.720e+02\n",
      "  4.790e+02 -4.790e+02  4.800e+02  4.840e+02 -4.850e+02  4.870e+02\n",
      " -4.910e+02  4.930e+02  4.950e+02 -4.990e+02  5.060e+02 -5.090e+02\n",
      " -5.160e+02 -5.180e+02 -5.200e+02  5.210e+02  5.210e+02  5.220e+02\n",
      "  5.260e+02  5.300e+02 -5.390e+02 -5.420e+02 -5.430e+02  5.430e+02\n",
      "  5.450e+02  5.460e+02  5.460e+02 -5.470e+02 -5.490e+02 -5.520e+02\n",
      " -5.590e+02  5.600e+02 -5.680e+02  5.720e+02 -5.730e+02 -5.760e+02\n",
      "  5.770e+02  5.770e+02 -5.780e+02  5.840e+02 -5.860e+02 -5.940e+02\n",
      "  6.010e+02  6.020e+02  6.060e+02 -6.060e+02 -6.160e+02 -6.190e+02\n",
      " -6.220e+02 -6.220e+02 -6.230e+02 -6.230e+02 -6.240e+02  6.250e+02\n",
      " -6.250e+02  6.310e+02 -6.350e+02 -6.390e+02 -6.410e+02 -6.460e+02\n",
      "  6.540e+02 -6.580e+02  6.630e+02  6.660e+02 -6.670e+02 -6.700e+02\n",
      "  6.750e+02 -6.790e+02  6.800e+02 -6.820e+02 -6.850e+02 -6.930e+02\n",
      " -6.950e+02  6.950e+02 -7.010e+02 -7.040e+02 -7.070e+02 -7.100e+02\n",
      " -7.130e+02 -7.140e+02 -7.170e+02 -7.250e+02  7.270e+02  7.390e+02\n",
      " -7.420e+02 -7.500e+02 -7.540e+02 -7.590e+02 -7.590e+02 -7.590e+02\n",
      " -7.590e+02 -7.700e+02 -7.730e+02 -7.820e+02 -7.830e+02  7.890e+02\n",
      " -7.960e+02 -7.970e+02 -7.990e+02  8.040e+02  8.060e+02 -8.170e+02\n",
      "  8.230e+02 -8.270e+02  8.360e+02  8.390e+02 -8.510e+02 -8.520e+02\n",
      " -8.660e+02 -8.750e+02  8.820e+02 -8.930e+02 -8.960e+02 -9.040e+02\n",
      " -9.060e+02 -9.100e+02 -9.110e+02 -9.130e+02 -9.140e+02  9.150e+02\n",
      " -9.180e+02  9.270e+02 -9.300e+02 -9.470e+02 -9.500e+02 -9.540e+02\n",
      " -9.640e+02  9.800e+02  9.850e+02 -9.870e+02  9.880e+02  9.930e+02\n",
      " -9.970e+02  9.980e+02 -1.000e+03 -1.021e+03 -1.022e+03 -1.025e+03\n",
      " -1.027e+03 -1.030e+03 -1.036e+03  1.037e+03 -1.038e+03 -1.045e+03\n",
      " -1.057e+03 -1.065e+03 -1.066e+03 -1.077e+03  1.079e+03  1.081e+03\n",
      "  1.090e+03  1.093e+03 -1.117e+03 -1.120e+03  1.133e+03  1.134e+03\n",
      " -1.138e+03 -1.152e+03 -1.157e+03 -1.168e+03 -1.172e+03 -1.179e+03\n",
      " -1.183e+03 -1.191e+03  1.202e+03 -1.224e+03 -1.245e+03 -1.273e+03\n",
      " -1.278e+03  1.289e+03 -1.309e+03 -1.315e+03 -1.346e+03 -1.353e+03\n",
      " -1.358e+03 -1.368e+03  1.394e+03  1.398e+03 -1.399e+03 -1.409e+03\n",
      " -1.409e+03 -1.413e+03 -1.425e+03 -1.428e+03 -1.430e+03 -1.435e+03\n",
      " -1.440e+03  1.459e+03 -1.461e+03  1.466e+03 -1.472e+03 -1.478e+03\n",
      " -1.483e+03  1.504e+03 -1.506e+03 -1.508e+03 -1.512e+03 -1.521e+03\n",
      " -1.523e+03 -1.527e+03 -1.555e+03 -1.559e+03 -1.560e+03 -1.584e+03\n",
      " -1.584e+03 -1.586e+03  1.591e+03 -1.593e+03 -1.628e+03  1.641e+03\n",
      " -1.660e+03 -1.663e+03 -1.663e+03 -1.665e+03  1.671e+03 -1.686e+03\n",
      " -1.690e+03 -1.699e+03 -1.721e+03 -1.722e+03 -1.724e+03  1.732e+03\n",
      "  1.748e+03 -1.777e+03 -1.778e+03  1.838e+03 -1.840e+03 -1.897e+03\n",
      " -1.899e+03 -1.971e+03  1.972e+03  2.002e+03 -2.016e+03  2.064e+03\n",
      "  2.083e+03  2.120e+03 -2.133e+03 -2.143e+03  2.166e+03 -2.182e+03\n",
      " -2.239e+03 -2.265e+03 -2.298e+03  2.319e+03 -2.327e+03 -2.347e+03\n",
      " -2.359e+03 -2.437e+03  2.570e+03 -2.641e+03  2.703e+03  2.717e+03\n",
      " -2.727e+03  2.741e+03 -2.784e+03 -2.886e+03  3.059e+03 -3.221e+03\n",
      " -3.270e+03  3.314e+03 -3.381e+03 -3.930e+03 -3.981e+03 -4.241e+03\n",
      " -4.282e+03  4.680e+03  4.856e+03  5.152e+03 -5.252e+03 -5.480e+03\n",
      "  6.417e+03]\n",
      "Concordance Index 0.6221321314519964\n",
      "Integrated Brier Score: 0.186779989348572\n",
      "y_train breslow final [ 2.000e+00  2.000e+00 -1.100e+01  1.100e+01 -1.400e+01  1.400e+01\n",
      "  3.200e+01  5.600e+01 -5.600e+01  6.200e+01  6.400e+01  6.500e+01\n",
      "  6.900e+01 -7.700e+01  8.200e+01  8.900e+01  9.000e+01 -9.200e+01\n",
      "  9.400e+01 -1.020e+02 -1.050e+02  1.070e+02  1.080e+02 -1.110e+02\n",
      "  1.120e+02  1.260e+02  1.290e+02  1.300e+02 -1.340e+02  1.420e+02\n",
      "  1.440e+02  1.510e+02 -1.560e+02  1.600e+02  1.660e+02  1.710e+02\n",
      "  1.730e+02 -1.810e+02  1.820e+02  1.830e+02  1.850e+02 -1.870e+02\n",
      "  1.910e+02  1.940e+02  1.940e+02  1.970e+02  1.980e+02  2.050e+02\n",
      "  2.090e+02  2.150e+02  2.150e+02 -2.160e+02  2.170e+02  2.170e+02\n",
      "  2.180e+02  2.220e+02 -2.250e+02 -2.320e+02 -2.340e+02 -2.420e+02\n",
      "  2.430e+02 -2.530e+02  2.530e+02  2.610e+02  2.680e+02  2.760e+02\n",
      " -2.780e+02  2.790e+02  2.810e+02  2.820e+02  2.840e+02  2.920e+02\n",
      "  2.940e+02  2.950e+02 -3.110e+02  3.170e+02  3.170e+02 -3.180e+02\n",
      "  3.210e+02  3.270e+02  3.270e+02  3.420e+02 -3.450e+02  3.480e+02\n",
      " -3.510e+02  3.510e+02  3.520e+02  3.530e+02 -3.540e+02  3.570e+02\n",
      " -3.580e+02  3.590e+02 -3.600e+02  3.610e+02  3.660e+02 -3.670e+02\n",
      " -3.680e+02  3.710e+02 -3.760e+02  3.770e+02 -3.780e+02  3.790e+02\n",
      " -3.790e+02  3.800e+02 -3.840e+02  3.850e+02 -3.860e+02  3.870e+02\n",
      " -3.870e+02 -3.890e+02 -3.920e+02 -3.920e+02  3.930e+02 -3.930e+02\n",
      "  3.950e+02  3.950e+02  3.970e+02 -4.000e+02 -4.030e+02  4.030e+02\n",
      "  4.030e+02  4.060e+02  4.070e+02 -4.120e+02 -4.140e+02 -4.170e+02\n",
      "  4.210e+02 -4.210e+02 -4.240e+02  4.240e+02  4.360e+02 -4.400e+02\n",
      " -4.410e+02 -4.430e+02  4.460e+02  4.530e+02  4.560e+02 -4.580e+02\n",
      "  4.590e+02 -4.610e+02 -4.660e+02 -4.690e+02 -4.710e+02  4.720e+02\n",
      "  4.790e+02 -4.790e+02  4.800e+02  4.840e+02 -4.850e+02  4.870e+02\n",
      " -4.910e+02  4.930e+02  4.950e+02 -4.990e+02  5.060e+02 -5.090e+02\n",
      " -5.160e+02 -5.180e+02 -5.200e+02  5.210e+02  5.210e+02  5.220e+02\n",
      "  5.260e+02  5.300e+02 -5.390e+02 -5.420e+02 -5.430e+02  5.430e+02\n",
      "  5.450e+02  5.460e+02  5.460e+02 -5.470e+02 -5.490e+02 -5.520e+02\n",
      " -5.590e+02  5.600e+02 -5.680e+02  5.720e+02 -5.730e+02 -5.760e+02\n",
      "  5.770e+02  5.770e+02 -5.780e+02  5.840e+02 -5.860e+02 -5.940e+02\n",
      "  6.010e+02  6.020e+02  6.060e+02 -6.060e+02 -6.160e+02 -6.190e+02\n",
      " -6.220e+02 -6.220e+02 -6.230e+02 -6.230e+02 -6.240e+02  6.250e+02\n",
      " -6.250e+02  6.310e+02 -6.350e+02 -6.390e+02 -6.410e+02 -6.460e+02\n",
      "  6.540e+02 -6.580e+02  6.630e+02  6.660e+02 -6.670e+02 -6.700e+02\n",
      "  6.750e+02 -6.790e+02  6.800e+02 -6.820e+02 -6.850e+02 -6.930e+02\n",
      " -6.950e+02  6.950e+02 -7.010e+02 -7.040e+02 -7.070e+02 -7.100e+02\n",
      " -7.130e+02 -7.140e+02 -7.170e+02 -7.250e+02  7.270e+02  7.390e+02\n",
      " -7.420e+02 -7.500e+02 -7.540e+02 -7.590e+02 -7.590e+02 -7.590e+02\n",
      " -7.590e+02 -7.700e+02 -7.730e+02 -7.820e+02 -7.830e+02  7.890e+02\n",
      " -7.960e+02 -7.970e+02 -7.990e+02  8.040e+02  8.060e+02 -8.170e+02\n",
      "  8.230e+02 -8.270e+02  8.360e+02  8.390e+02 -8.510e+02 -8.520e+02\n",
      " -8.660e+02 -8.750e+02  8.820e+02 -8.930e+02 -8.960e+02 -9.040e+02\n",
      " -9.060e+02 -9.100e+02 -9.110e+02 -9.130e+02 -9.140e+02  9.150e+02\n",
      " -9.180e+02  9.270e+02 -9.300e+02 -9.470e+02 -9.500e+02 -9.540e+02\n",
      " -9.640e+02  9.800e+02  9.850e+02 -9.870e+02  9.880e+02  9.930e+02\n",
      " -9.970e+02  9.980e+02 -1.000e+03 -1.021e+03 -1.022e+03 -1.025e+03\n",
      " -1.027e+03 -1.030e+03 -1.036e+03  1.037e+03 -1.038e+03 -1.045e+03\n",
      " -1.057e+03 -1.065e+03 -1.066e+03 -1.077e+03  1.079e+03  1.081e+03\n",
      "  1.090e+03  1.093e+03 -1.117e+03 -1.120e+03  1.133e+03  1.134e+03\n",
      " -1.138e+03 -1.152e+03 -1.157e+03 -1.168e+03 -1.172e+03 -1.179e+03\n",
      " -1.183e+03 -1.191e+03  1.202e+03 -1.224e+03 -1.245e+03 -1.273e+03\n",
      " -1.278e+03  1.289e+03 -1.309e+03 -1.315e+03 -1.346e+03 -1.353e+03\n",
      " -1.358e+03 -1.368e+03  1.394e+03  1.398e+03 -1.399e+03 -1.409e+03\n",
      " -1.409e+03 -1.413e+03 -1.425e+03 -1.428e+03 -1.430e+03 -1.435e+03\n",
      " -1.440e+03  1.459e+03 -1.461e+03  1.466e+03 -1.472e+03 -1.478e+03\n",
      " -1.483e+03  1.504e+03 -1.506e+03 -1.508e+03 -1.512e+03 -1.521e+03\n",
      " -1.523e+03 -1.527e+03 -1.555e+03 -1.559e+03 -1.560e+03 -1.584e+03\n",
      " -1.584e+03 -1.586e+03  1.591e+03 -1.593e+03 -1.628e+03  1.641e+03\n",
      " -1.660e+03 -1.663e+03 -1.663e+03 -1.665e+03  1.671e+03 -1.686e+03\n",
      " -1.690e+03 -1.699e+03 -1.721e+03 -1.722e+03 -1.724e+03  1.732e+03\n",
      "  1.748e+03 -1.777e+03 -1.778e+03  1.838e+03 -1.840e+03 -1.897e+03\n",
      " -1.899e+03 -1.971e+03  1.972e+03  2.002e+03 -2.016e+03  2.064e+03\n",
      "  2.083e+03  2.120e+03 -2.133e+03 -2.143e+03  2.166e+03 -2.182e+03\n",
      " -2.239e+03 -2.265e+03 -2.298e+03  2.319e+03 -2.327e+03 -2.347e+03\n",
      " -2.359e+03 -2.437e+03  2.570e+03 -2.641e+03  2.703e+03  2.717e+03\n",
      " -2.727e+03  2.741e+03 -2.784e+03 -2.886e+03  3.059e+03 -3.221e+03\n",
      " -3.270e+03  3.314e+03 -3.381e+03 -3.930e+03 -3.981e+03 -4.241e+03\n",
      " -4.282e+03  4.680e+03  4.856e+03  5.152e+03 -5.252e+03 -5.480e+03\n",
      "  6.417e+03]\n",
      "durations 23.0 4760.0\n",
      "Concordance Index 0.5740676193795747\n",
      "Integrated Brier Score: 0.2329299819518419\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m119.4586\u001b[0m  0.0122\n",
      "      2      119.4586  0.0032\n",
      "      3      119.4586  0.0026\n",
      "      4      119.4586  0.0044\n",
      "      5      119.4586  0.0079\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      6      119.4586  0.0024\n",
      "      7      119.4586  0.0022\n",
      "      8      119.4586  0.0021\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m141.5646\u001b[0m  0.0049\n",
      "      9      119.4586  0.0045\n",
      "      2      141.5646  0.0046\n",
      "     10      119.4586  0.0043\n",
      "      3      141.5646  0.0023\n",
      "      4      141.5646  0.0019\n",
      "Restoring best model from epoch 1.\n",
      "      5      141.5646  0.0021\n",
      "      6      141.5646  0.0041\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      7      141.5646  0.0022\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      8      141.5646  0.0033\n",
      "      9      141.5646  0.0021\n",
      "     10      141.5646  0.0019\n",
      "Restoring best model from epoch 1.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m107.6794\u001b[0m  0.0055\n",
      "      2      107.6794  0.0038\n",
      "      3      107.6794  0.0029\n",
      "      4      107.6794  0.0039\n",
      "      5      107.6794  0.0034\n",
      "      6      107.6794  0.0027\n",
      "      7      107.6794  0.0026\n",
      "      8      107.6794  0.0026\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      9      107.6794  0.0022\n",
      "     10      107.6794  0.0028\n",
      "Restoring best model from epoch 1.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m152.4867\u001b[0m  0.0034\n",
      "      2      152.4867  0.0035\n",
      "      3      152.4867  0.0023\n",
      "      4      152.4867  0.0020\n",
      "      5      152.4867  0.0052\n",
      "      6      152.4867  0.0027\n",
      "      7      152.4867  0.0019\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      8      152.4867  0.0019\n",
      "      9      152.4867  0.0018\n",
      "     10      152.4867  0.0019\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m262.3289\u001b[0m  0.0089\n",
      "      2      262.3289  0.0037\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      3      262.3289  0.0020\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m107.7587\u001b[0m  0.0043\n",
      "      4      262.3289  0.0027\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m192.4468\u001b[0m  0.0031\n",
      "      2      107.7587  0.0024\n",
      "      5      262.3289  0.0020\n",
      "      2      192.4468  0.0023\n",
      "      3      107.7587  0.0021\n",
      "      6      262.3289  0.0020\n",
      "      7      262.3289  0.0019\n",
      "      3      192.4468  0.0021\n",
      "      4      107.7587  0.0022\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      4      192.4468  0.0021\n",
      "      8      262.3289  0.0023\n",
      "      5      107.7587  0.0025\n",
      "      9      262.3289  0.0020\n",
      "      5      192.4468  0.0021\n",
      "      6      107.7587  0.0022\n",
      "      6      192.4468  0.0020\n",
      "      7      107.7587  0.0021\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m299.7368\u001b[0m  0.0055\n",
      "     10      262.3289  0.0040\n",
      "      7      192.4468  0.0023\n",
      "Restoring best model from epoch 1.\n",
      "      8      107.7587  0.0021\n",
      "      2      299.7368  0.0024\n",
      "      8      192.4468  0.0023\n",
      "      9      107.7587  0.0022\n",
      "      3      299.7368  0.0020\n",
      "      9      192.4468  0.0019\n",
      "     10      107.7587  0.0020\n",
      "Restoring best model from epoch 1.\n",
      "      4      299.7368  0.0020\n",
      "      5      299.7368  0.0019\n",
      "     10      192.4468  0.0041\n",
      "Restoring best model from epoch 1.\n",
      "      6      299.7368  0.0034\n",
      "      7      299.7368  0.0023\n",
      "      8      299.7368  0.0020\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      9      299.7368  0.0026\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "     10      299.7368  0.0035\n",
      "Restoring best model from epoch 1.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m87.8568\u001b[0m  0.0079\n",
      "      2       87.8568  0.0033\n",
      "      3       87.8568  0.0036\n",
      "      4       87.8568  0.0032\n",
      "      5       87.8568  0.0036\n",
      "      6       87.8568  0.0043\n",
      "      7       87.8568  0.0030\n",
      "      8       87.8568  0.0022\n",
      "      9       87.8568  0.0020\n",
      "     10       87.8568  0.0026\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m66.0426\u001b[0m  0.0033\n",
      "      2       66.0426  0.0023\n",
      "      3       66.0426  0.0020\n",
      "      4       66.0426  0.0019\n",
      "      5       66.0426  0.0021\n",
      "      6       66.0426  0.0019\n",
      "      7       66.0426  0.0019\n",
      "      8       66.0426  0.0020\n",
      "      9       66.0426  0.0019\n",
      "     10       66.0426  0.0020\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m153.3701\u001b[0m  0.0036\n",
      "      2      153.3701  0.0033\n",
      "      3      153.3701  0.0032\n",
      "      4      153.3701  0.0031\n",
      "      5      153.3701  0.0035\n",
      "      6      153.3701  0.0034\n",
      "      7      153.3701  0.0033\n",
      "      8      153.3701  0.0032\n",
      "      9      153.3701  0.0036\n",
      "     10      153.3701  0.0035\n",
      "Restoring best model from epoch 1.\n",
      "y_train breslow final [ 2.000e+00  2.000e+00 -1.100e+01  1.100e+01  1.400e+01  2.300e+01\n",
      " -3.000e+01  3.200e+01  5.600e+01  6.200e+01  6.500e+01  6.900e+01\n",
      "  7.600e+01 -7.700e+01  8.200e+01  8.600e+01  9.000e+01 -9.200e+01\n",
      "  9.400e+01  9.500e+01 -1.020e+02 -1.050e+02  1.070e+02 -1.100e+02\n",
      " -1.110e+02  1.120e+02 -1.220e+02  1.260e+02  1.290e+02  1.290e+02\n",
      "  1.300e+02 -1.340e+02  1.420e+02  1.440e+02  1.440e+02 -1.460e+02\n",
      " -1.560e+02  1.600e+02  1.660e+02  1.730e+02 -1.810e+02  1.820e+02\n",
      "  1.850e+02 -1.870e+02  1.940e+02  1.940e+02  1.970e+02  2.050e+02\n",
      "  2.090e+02 -2.100e+02  2.150e+02 -2.160e+02 -2.160e+02  2.170e+02\n",
      "  2.170e+02  2.180e+02  2.220e+02 -2.250e+02 -2.340e+02 -2.420e+02\n",
      "  2.430e+02 -2.530e+02  2.530e+02  2.560e+02  2.590e+02  2.610e+02\n",
      "  2.680e+02  2.740e+02  2.760e+02 -2.780e+02  2.790e+02  2.830e+02\n",
      "  2.840e+02 -2.920e+02  2.940e+02  2.950e+02 -3.110e+02  3.170e+02\n",
      " -3.180e+02 -3.210e+02  3.210e+02  3.270e+02  3.270e+02  3.300e+02\n",
      "  3.340e+02  3.360e+02  3.370e+02  3.410e+02 -3.450e+02  3.480e+02\n",
      "  3.510e+02  3.530e+02 -3.540e+02  3.570e+02  3.600e+02 -3.600e+02\n",
      "  3.610e+02 -3.620e+02 -3.670e+02 -3.680e+02  3.710e+02 -3.760e+02\n",
      "  3.770e+02  3.770e+02 -3.780e+02  3.790e+02 -3.790e+02  3.800e+02\n",
      "  3.840e+02 -3.840e+02  3.850e+02 -3.860e+02 -3.890e+02 -3.920e+02\n",
      " -3.920e+02  3.930e+02 -3.930e+02  3.950e+02  3.950e+02 -4.000e+02\n",
      "  4.030e+02  4.030e+02  4.060e+02  4.060e+02 -4.070e+02 -4.140e+02\n",
      "  4.150e+02  4.210e+02 -4.210e+02 -4.240e+02  4.240e+02  4.270e+02\n",
      "  4.300e+02  4.360e+02 -4.370e+02 -4.410e+02 -4.430e+02  4.460e+02\n",
      "  4.490e+02  4.510e+02  4.530e+02  4.560e+02 -4.580e+02  4.590e+02\n",
      " -4.610e+02  4.620e+02  4.640e+02 -4.660e+02 -4.690e+02 -4.710e+02\n",
      "  4.720e+02  4.790e+02  4.800e+02  4.840e+02  4.870e+02  4.890e+02\n",
      " -4.910e+02  4.950e+02 -4.990e+02  5.060e+02 -5.140e+02 -5.160e+02\n",
      " -5.180e+02  5.210e+02  5.210e+02  5.220e+02  5.260e+02 -5.390e+02\n",
      " -5.430e+02  5.430e+02  5.460e+02 -5.470e+02  5.480e+02  5.600e+02\n",
      "  5.640e+02 -5.680e+02  5.720e+02  5.770e+02 -5.780e+02 -5.790e+02\n",
      "  5.800e+02  5.840e+02 -5.940e+02 -6.000e+02  6.020e+02 -6.030e+02\n",
      "  6.060e+02 -6.060e+02 -6.160e+02 -6.190e+02 -6.230e+02 -6.230e+02\n",
      "  6.240e+02  6.250e+02 -6.250e+02  6.310e+02 -6.350e+02 -6.370e+02\n",
      " -6.390e+02 -6.410e+02 -6.410e+02  6.410e+02 -6.440e+02 -6.450e+02\n",
      " -6.450e+02 -6.460e+02 -6.530e+02  6.540e+02  6.660e+02 -6.700e+02\n",
      "  6.750e+02 -6.790e+02  6.800e+02 -6.820e+02 -6.870e+02 -6.930e+02\n",
      "  6.950e+02 -7.010e+02 -7.010e+02 -7.040e+02 -7.070e+02 -7.130e+02\n",
      " -7.170e+02 -7.220e+02 -7.420e+02 -7.500e+02 -7.540e+02 -7.590e+02\n",
      " -7.590e+02 -7.590e+02  7.630e+02 -7.700e+02  7.730e+02 -7.730e+02\n",
      "  7.890e+02 -7.970e+02 -7.990e+02  8.040e+02 -8.170e+02  8.230e+02\n",
      " -8.330e+02  8.360e+02 -8.360e+02  8.390e+02 -8.500e+02 -8.510e+02\n",
      " -8.520e+02  8.530e+02 -8.550e+02  8.620e+02 -8.660e+02 -8.750e+02\n",
      " -8.890e+02 -8.930e+02 -8.950e+02 -8.960e+02 -8.970e+02 -9.040e+02\n",
      " -9.060e+02 -9.100e+02 -9.110e+02 -9.130e+02 -9.140e+02  9.150e+02\n",
      " -9.180e+02  9.270e+02 -9.330e+02  9.410e+02 -9.500e+02 -9.540e+02\n",
      " -9.640e+02 -9.720e+02  9.800e+02 -9.870e+02  9.880e+02  9.930e+02\n",
      " -9.930e+02 -9.950e+02 -9.970e+02  9.980e+02 -1.021e+03 -1.022e+03\n",
      " -1.025e+03 -1.027e+03 -1.036e+03  1.037e+03 -1.038e+03 -1.045e+03\n",
      " -1.050e+03 -1.057e+03 -1.065e+03 -1.066e+03 -1.075e+03  1.090e+03\n",
      " -1.095e+03 -1.117e+03 -1.120e+03 -1.125e+03 -1.131e+03  1.133e+03\n",
      "  1.134e+03 -1.138e+03 -1.143e+03 -1.147e+03 -1.152e+03 -1.157e+03\n",
      " -1.172e+03 -1.179e+03 -1.190e+03 -1.191e+03  1.202e+03 -1.218e+03\n",
      " -1.224e+03 -1.245e+03 -1.253e+03 -1.265e+03 -1.273e+03 -1.274e+03\n",
      " -1.288e+03 -1.309e+03 -1.311e+03 -1.346e+03 -1.358e+03 -1.368e+03\n",
      "  1.398e+03 -1.399e+03 -1.409e+03 -1.409e+03 -1.413e+03 -1.425e+03\n",
      " -1.428e+03  1.430e+03 -1.430e+03 -1.435e+03 -1.440e+03 -1.443e+03\n",
      " -1.460e+03 -1.461e+03 -1.466e+03  1.466e+03 -1.478e+03 -1.483e+03\n",
      "  1.504e+03 -1.506e+03 -1.508e+03 -1.512e+03 -1.521e+03 -1.523e+03\n",
      " -1.527e+03 -1.560e+03 -1.584e+03 -1.586e+03  1.591e+03 -1.593e+03\n",
      " -1.628e+03  1.641e+03 -1.660e+03 -1.663e+03 -1.663e+03 -1.665e+03\n",
      "  1.671e+03  1.718e+03 -1.722e+03  1.732e+03 -1.737e+03  1.748e+03\n",
      "  1.762e+03 -1.777e+03 -1.778e+03 -1.840e+03 -1.897e+03 -1.899e+03\n",
      " -1.971e+03  1.972e+03 -1.995e+03  2.002e+03 -2.016e+03 -2.016e+03\n",
      "  2.064e+03  2.083e+03  2.120e+03 -2.133e+03 -2.143e+03 -2.161e+03\n",
      "  2.166e+03 -2.169e+03 -2.239e+03 -2.265e+03 -2.298e+03 -2.347e+03\n",
      " -2.359e+03 -2.437e+03 -2.562e+03  2.570e+03 -2.641e+03  2.717e+03\n",
      " -2.727e+03  2.741e+03 -2.784e+03 -2.886e+03  2.900e+03  3.059e+03\n",
      " -3.221e+03 -3.270e+03 -3.381e+03 -3.930e+03 -3.981e+03 -4.241e+03\n",
      "  4.680e+03  4.760e+03  4.856e+03  5.152e+03 -5.252e+03 -5.480e+03\n",
      "  6.417e+03]\n",
      "Concordance Index 0.5510327253218884\n",
      "Integrated Brier Score: 0.19225146058057827\n",
      "y_train breslow final [ 2.000e+00  2.000e+00 -1.100e+01  1.100e+01  1.400e+01  2.300e+01\n",
      " -3.000e+01  3.200e+01  5.600e+01  6.200e+01  6.500e+01  6.900e+01\n",
      "  7.600e+01 -7.700e+01  8.200e+01  8.600e+01  9.000e+01 -9.200e+01\n",
      "  9.400e+01  9.500e+01 -1.020e+02 -1.050e+02  1.070e+02 -1.100e+02\n",
      " -1.110e+02  1.120e+02 -1.220e+02  1.260e+02  1.290e+02  1.290e+02\n",
      "  1.300e+02 -1.340e+02  1.420e+02  1.440e+02  1.440e+02 -1.460e+02\n",
      " -1.560e+02  1.600e+02  1.660e+02  1.730e+02 -1.810e+02  1.820e+02\n",
      "  1.850e+02 -1.870e+02  1.940e+02  1.940e+02  1.970e+02  2.050e+02\n",
      "  2.090e+02 -2.100e+02  2.150e+02 -2.160e+02 -2.160e+02  2.170e+02\n",
      "  2.170e+02  2.180e+02  2.220e+02 -2.250e+02 -2.340e+02 -2.420e+02\n",
      "  2.430e+02 -2.530e+02  2.530e+02  2.560e+02  2.590e+02  2.610e+02\n",
      "  2.680e+02  2.740e+02  2.760e+02 -2.780e+02  2.790e+02  2.830e+02\n",
      "  2.840e+02 -2.920e+02  2.940e+02  2.950e+02 -3.110e+02  3.170e+02\n",
      " -3.180e+02 -3.210e+02  3.210e+02  3.270e+02  3.270e+02  3.300e+02\n",
      "  3.340e+02  3.360e+02  3.370e+02  3.410e+02 -3.450e+02  3.480e+02\n",
      "  3.510e+02  3.530e+02 -3.540e+02  3.570e+02  3.600e+02 -3.600e+02\n",
      "  3.610e+02 -3.620e+02 -3.670e+02 -3.680e+02  3.710e+02 -3.760e+02\n",
      "  3.770e+02  3.770e+02 -3.780e+02  3.790e+02 -3.790e+02  3.800e+02\n",
      "  3.840e+02 -3.840e+02  3.850e+02 -3.860e+02 -3.890e+02 -3.920e+02\n",
      " -3.920e+02  3.930e+02 -3.930e+02  3.950e+02  3.950e+02 -4.000e+02\n",
      "  4.030e+02  4.030e+02  4.060e+02  4.060e+02 -4.070e+02 -4.140e+02\n",
      "  4.150e+02  4.210e+02 -4.210e+02 -4.240e+02  4.240e+02  4.270e+02\n",
      "  4.300e+02  4.360e+02 -4.370e+02 -4.410e+02 -4.430e+02  4.460e+02\n",
      "  4.490e+02  4.510e+02  4.530e+02  4.560e+02 -4.580e+02  4.590e+02\n",
      " -4.610e+02  4.620e+02  4.640e+02 -4.660e+02 -4.690e+02 -4.710e+02\n",
      "  4.720e+02  4.790e+02  4.800e+02  4.840e+02  4.870e+02  4.890e+02\n",
      " -4.910e+02  4.950e+02 -4.990e+02  5.060e+02 -5.140e+02 -5.160e+02\n",
      " -5.180e+02  5.210e+02  5.210e+02  5.220e+02  5.260e+02 -5.390e+02\n",
      " -5.430e+02  5.430e+02  5.460e+02 -5.470e+02  5.480e+02  5.600e+02\n",
      "  5.640e+02 -5.680e+02  5.720e+02  5.770e+02 -5.780e+02 -5.790e+02\n",
      "  5.800e+02  5.840e+02 -5.940e+02 -6.000e+02  6.020e+02 -6.030e+02\n",
      "  6.060e+02 -6.060e+02 -6.160e+02 -6.190e+02 -6.230e+02 -6.230e+02\n",
      "  6.240e+02  6.250e+02 -6.250e+02  6.310e+02 -6.350e+02 -6.370e+02\n",
      " -6.390e+02 -6.410e+02 -6.410e+02  6.410e+02 -6.440e+02 -6.450e+02\n",
      " -6.450e+02 -6.460e+02 -6.530e+02  6.540e+02  6.660e+02 -6.700e+02\n",
      "  6.750e+02 -6.790e+02  6.800e+02 -6.820e+02 -6.870e+02 -6.930e+02\n",
      "  6.950e+02 -7.010e+02 -7.010e+02 -7.040e+02 -7.070e+02 -7.130e+02\n",
      " -7.170e+02 -7.220e+02 -7.420e+02 -7.500e+02 -7.540e+02 -7.590e+02\n",
      " -7.590e+02 -7.590e+02  7.630e+02 -7.700e+02  7.730e+02 -7.730e+02\n",
      "  7.890e+02 -7.970e+02 -7.990e+02  8.040e+02 -8.170e+02  8.230e+02\n",
      " -8.330e+02  8.360e+02 -8.360e+02  8.390e+02 -8.500e+02 -8.510e+02\n",
      " -8.520e+02  8.530e+02 -8.550e+02  8.620e+02 -8.660e+02 -8.750e+02\n",
      " -8.890e+02 -8.930e+02 -8.950e+02 -8.960e+02 -8.970e+02 -9.040e+02\n",
      " -9.060e+02 -9.100e+02 -9.110e+02 -9.130e+02 -9.140e+02  9.150e+02\n",
      " -9.180e+02  9.270e+02 -9.330e+02  9.410e+02 -9.500e+02 -9.540e+02\n",
      " -9.640e+02 -9.720e+02  9.800e+02 -9.870e+02  9.880e+02  9.930e+02\n",
      " -9.930e+02 -9.950e+02 -9.970e+02  9.980e+02 -1.021e+03 -1.022e+03\n",
      " -1.025e+03 -1.027e+03 -1.036e+03  1.037e+03 -1.038e+03 -1.045e+03\n",
      " -1.050e+03 -1.057e+03 -1.065e+03 -1.066e+03 -1.075e+03  1.090e+03\n",
      " -1.095e+03 -1.117e+03 -1.120e+03 -1.125e+03 -1.131e+03  1.133e+03\n",
      "  1.134e+03 -1.138e+03 -1.143e+03 -1.147e+03 -1.152e+03 -1.157e+03\n",
      " -1.172e+03 -1.179e+03 -1.190e+03 -1.191e+03  1.202e+03 -1.218e+03\n",
      " -1.224e+03 -1.245e+03 -1.253e+03 -1.265e+03 -1.273e+03 -1.274e+03\n",
      " -1.288e+03 -1.309e+03 -1.311e+03 -1.346e+03 -1.358e+03 -1.368e+03\n",
      "  1.398e+03 -1.399e+03 -1.409e+03 -1.409e+03 -1.413e+03 -1.425e+03\n",
      " -1.428e+03  1.430e+03 -1.430e+03 -1.435e+03 -1.440e+03 -1.443e+03\n",
      " -1.460e+03 -1.461e+03 -1.466e+03  1.466e+03 -1.478e+03 -1.483e+03\n",
      "  1.504e+03 -1.506e+03 -1.508e+03 -1.512e+03 -1.521e+03 -1.523e+03\n",
      " -1.527e+03 -1.560e+03 -1.584e+03 -1.586e+03  1.591e+03 -1.593e+03\n",
      " -1.628e+03  1.641e+03 -1.660e+03 -1.663e+03 -1.663e+03 -1.665e+03\n",
      "  1.671e+03  1.718e+03 -1.722e+03  1.732e+03 -1.737e+03  1.748e+03\n",
      "  1.762e+03 -1.777e+03 -1.778e+03 -1.840e+03 -1.897e+03 -1.899e+03\n",
      " -1.971e+03  1.972e+03 -1.995e+03  2.002e+03 -2.016e+03 -2.016e+03\n",
      "  2.064e+03  2.083e+03  2.120e+03 -2.133e+03 -2.143e+03 -2.161e+03\n",
      "  2.166e+03 -2.169e+03 -2.239e+03 -2.265e+03 -2.298e+03 -2.347e+03\n",
      " -2.359e+03 -2.437e+03 -2.562e+03  2.570e+03 -2.641e+03  2.717e+03\n",
      " -2.727e+03  2.741e+03 -2.784e+03 -2.886e+03  2.900e+03  3.059e+03\n",
      " -3.221e+03 -3.270e+03 -3.381e+03 -3.930e+03 -3.981e+03 -4.241e+03\n",
      "  4.680e+03  4.760e+03  4.856e+03  5.152e+03 -5.252e+03 -5.480e+03\n",
      "  6.417e+03]\n",
      "durations 14.0 4282.0\n",
      "Concordance Index 0.5189169139465876\n",
      "Integrated Brier Score: 0.19769345197364777\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m87.7420\u001b[0m  0.0028\n",
      "      2       87.7420  0.0037\n",
      "      3       87.7420  0.0027\n",
      "      4       87.7420  0.0020\n",
      "      5       87.7420  0.0019\n",
      "      6       87.7420  0.0022\n",
      "      7       87.7420  0.0021\n",
      "      8       87.7420  0.0022\n",
      "      9       87.7420  0.0022\n",
      "     10       87.7420  0.0020\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m113.3181\u001b[0m  0.0035\n",
      "      2      113.3181  0.0028\n",
      "      3      113.3181  0.0032\n",
      "      4      113.3181  0.0033\n",
      "      5      113.3181  0.0027\n",
      "      6      113.3181  0.0023\n",
      "      7      113.3181  0.0031\n",
      "      8      113.3181  0.0026\n",
      "      9      113.3181  0.0030\n",
      "     10      113.3181  0.0026\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m172.2200\u001b[0m  0.0026\n",
      "      2      172.2200  0.0021\n",
      "      3      172.2200  0.0029\n",
      "      4      172.2200  0.0039\n",
      "      5      172.2200  0.0027\n",
      "      6      172.2200  0.0021\n",
      "      7      172.2200  0.0036\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      8      172.2200  0.0019\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      9      172.2200  0.0019\n",
      "     10      172.2200  0.0024\n",
      "Restoring best model from epoch 1.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m176.1534\u001b[0m  0.0048\n",
      "      2      176.1534  0.0049\n",
      "      3      176.1534  0.0030\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      4      176.1534  0.0037\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      5      176.1534  0.0025\n",
      "      6      176.1534  0.0028\n",
      "      7      176.1534  0.0039\n",
      "      8      176.1534  0.0042\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m95.6578\u001b[0m  0.0080\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      9      176.1534  0.0064\n",
      "      2       95.6578  0.0046\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "     10      176.1534  0.0034\n",
      "Restoring best model from epoch 1.\n",
      "      3       95.6578  0.0035\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m117.6578\u001b[0m  0.0070\n",
      "      4       95.6578  0.0033\n",
      "      5       95.6578  0.0024\n",
      "      2      117.6578  0.0030\n",
      "      6       95.6578  0.0025\n",
      "      3      117.6578  0.0020\n",
      "      4      117.6578  0.0029\n",
      "      7       95.6578  0.0042\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m78.0266\u001b[0m  0.0089\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      5      117.6578  0.0038\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      8       95.6578  0.0032\n",
      "      2       78.0266  0.0032\n",
      "      6      117.6578  0.0038\n",
      "      9       95.6578  0.0038\n",
      "      3       78.0266  0.0036\n",
      "     10       95.6578  0.0023\n",
      "Restoring best model from epoch 1.\n",
      "      4       78.0266  0.0022\n",
      "      7      117.6578  0.0053\n",
      "      5       78.0266  0.0032\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m196.5370\u001b[0m  0.0068\n",
      "      8      117.6578  0.0034\n",
      "      6       78.0266  0.0029\n",
      "      9      117.6578  0.0023\n",
      "      7       78.0266  0.0030\n",
      "      2      196.5370  0.0049\n",
      "     10      117.6578  0.0033\n",
      "      8       78.0266  0.0031\n",
      "      3      196.5370  0.0036\n",
      "Restoring best model from epoch 1.\n",
      "      9       78.0266  0.0037\n",
      "      4      196.5370  0.0083\n",
      "     10       78.0266  0.0055\n",
      "Restoring best model from epoch 1.\n",
      "      5      196.5370  0.0025\n",
      "      6      196.5370  0.0023\n",
      "      7      196.5370  0.0023\n",
      "      8      196.5370  0.0022\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      9      196.5370  0.0034\n",
      "     10      196.5370  0.0023\n",
      "Restoring best model from epoch 1.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m95.5950\u001b[0m  0.0043\n",
      "      2       95.5950  0.0029\n",
      "      3       95.5950  0.0037\n",
      "      4       95.5950  0.0031\n",
      "      5       95.5950  0.0022\n",
      "      6       95.5950  0.0020\n",
      "      7       95.5950  0.0020\n",
      "      8       95.5950  0.0020\n",
      "      9       95.5950  0.0024\n",
      "     10       95.5950  0.0020\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m252.5598\u001b[0m  0.0031\n",
      "      2      252.5598  0.0024\n",
      "      3      252.5598  0.0029\n",
      "      4      252.5598  0.0027\n",
      "      5      252.5598  0.0021\n",
      "      6      252.5598  0.0023\n",
      "      7      252.5598  0.0019\n",
      "      8      252.5598  0.0019\n",
      "      9      252.5598  0.0019\n",
      "     10      252.5598  0.0020\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m130.8282\u001b[0m  0.0044\n",
      "      2      130.8282  0.0037\n",
      "      3      130.8282  0.0037\n",
      "      4      130.8282  0.0035\n",
      "      5      130.8282  0.0040\n",
      "      6      130.8282  0.0034\n",
      "      7      130.8282  0.0037\n",
      "      8      130.8282  0.0057\n",
      "      9      130.8282  0.0055\n",
      "     10      130.8282  0.0090\n",
      "Restoring best model from epoch 1.\n",
      "y_train breslow final [ 2.000e+00  2.000e+00 -1.400e+01  1.400e+01  2.300e+01 -3.000e+01\n",
      "  5.600e+01 -5.600e+01  6.200e+01  6.400e+01  6.500e+01  7.600e+01\n",
      "  8.200e+01  8.600e+01  8.900e+01  9.000e+01 -9.200e+01  9.400e+01\n",
      "  9.500e+01 -1.020e+02 -1.050e+02  1.070e+02  1.080e+02 -1.100e+02\n",
      " -1.110e+02  1.120e+02 -1.220e+02  1.290e+02  1.290e+02 -1.340e+02\n",
      "  1.420e+02  1.440e+02 -1.460e+02  1.510e+02 -1.560e+02  1.600e+02\n",
      "  1.710e+02 -1.810e+02  1.820e+02  1.830e+02  1.910e+02  1.940e+02\n",
      "  1.940e+02  1.980e+02  2.050e+02 -2.100e+02  2.150e+02 -2.160e+02\n",
      "  2.170e+02  2.180e+02 -2.250e+02 -2.320e+02  2.430e+02  2.530e+02\n",
      "  2.560e+02  2.590e+02  2.610e+02  2.680e+02  2.740e+02  2.760e+02\n",
      "  2.760e+02 -2.780e+02  2.790e+02  2.810e+02  2.820e+02  2.830e+02\n",
      "  2.840e+02 -2.920e+02  2.920e+02  2.940e+02  2.950e+02  3.170e+02\n",
      "  3.170e+02 -3.180e+02 -3.210e+02  3.270e+02  3.270e+02  3.300e+02\n",
      "  3.340e+02  3.360e+02  3.370e+02  3.410e+02  3.420e+02 -3.510e+02\n",
      "  3.510e+02  3.520e+02 -3.540e+02  3.570e+02 -3.580e+02  3.590e+02\n",
      "  3.600e+02 -3.600e+02  3.610e+02 -3.620e+02  3.660e+02 -3.670e+02\n",
      " -3.680e+02  3.710e+02  3.770e+02  3.770e+02 -3.780e+02  3.790e+02\n",
      " -3.790e+02  3.800e+02  3.840e+02 -3.840e+02  3.850e+02  3.850e+02\n",
      " -3.860e+02  3.870e+02 -3.870e+02 -3.890e+02 -3.920e+02 -3.920e+02\n",
      " -3.920e+02  3.930e+02  3.970e+02 -4.000e+02 -4.030e+02  4.030e+02\n",
      "  4.060e+02  4.070e+02 -4.070e+02 -4.120e+02 -4.140e+02  4.150e+02\n",
      " -4.170e+02  4.210e+02 -4.210e+02  4.270e+02  4.300e+02  4.360e+02\n",
      " -4.370e+02 -4.400e+02 -4.410e+02 -4.430e+02  4.460e+02  4.490e+02\n",
      "  4.510e+02  4.530e+02  4.560e+02  4.590e+02  4.620e+02  4.640e+02\n",
      " -4.660e+02 -4.690e+02  4.790e+02 -4.790e+02  4.840e+02 -4.850e+02\n",
      "  4.890e+02 -4.910e+02  4.930e+02  4.950e+02 -4.990e+02  5.060e+02\n",
      " -5.090e+02 -5.140e+02 -5.160e+02 -5.180e+02 -5.200e+02  5.210e+02\n",
      "  5.220e+02  5.260e+02  5.300e+02 -5.420e+02  5.430e+02  5.450e+02\n",
      "  5.460e+02  5.460e+02 -5.470e+02  5.480e+02 -5.490e+02 -5.520e+02\n",
      " -5.590e+02  5.640e+02 -5.680e+02  5.720e+02 -5.730e+02 -5.760e+02\n",
      "  5.770e+02  5.770e+02 -5.780e+02 -5.790e+02  5.800e+02  5.840e+02\n",
      " -5.860e+02 -6.000e+02  6.010e+02 -6.030e+02  6.060e+02 -6.060e+02\n",
      " -6.160e+02 -6.160e+02 -6.190e+02 -6.220e+02 -6.220e+02 -6.240e+02\n",
      "  6.240e+02  6.250e+02  6.310e+02 -6.370e+02 -6.390e+02 -6.410e+02\n",
      "  6.410e+02 -6.440e+02 -6.450e+02 -6.450e+02 -6.460e+02 -6.530e+02\n",
      " -6.580e+02  6.630e+02  6.660e+02 -6.670e+02 -6.700e+02  6.750e+02\n",
      " -6.790e+02  6.800e+02 -6.820e+02 -6.850e+02 -6.870e+02 -6.930e+02\n",
      " -6.950e+02  6.950e+02 -7.010e+02 -7.040e+02 -7.070e+02 -7.100e+02\n",
      " -7.140e+02 -7.220e+02 -7.250e+02  7.270e+02  7.390e+02 -7.420e+02\n",
      " -7.590e+02 -7.590e+02 -7.590e+02  7.630e+02  7.730e+02 -7.820e+02\n",
      " -7.830e+02 -7.960e+02 -7.970e+02 -7.990e+02  8.060e+02 -8.170e+02\n",
      "  8.230e+02 -8.270e+02 -8.330e+02  8.360e+02 -8.360e+02 -8.500e+02\n",
      " -8.510e+02 -8.520e+02  8.530e+02 -8.550e+02  8.620e+02 -8.660e+02\n",
      " -8.750e+02  8.820e+02 -8.890e+02 -8.930e+02 -8.950e+02 -8.960e+02\n",
      " -8.970e+02 -9.100e+02  9.150e+02 -9.180e+02 -9.300e+02 -9.330e+02\n",
      "  9.410e+02 -9.470e+02 -9.500e+02 -9.540e+02 -9.720e+02  9.800e+02\n",
      "  9.850e+02 -9.870e+02  9.880e+02  9.930e+02 -9.930e+02 -9.950e+02\n",
      " -9.970e+02  9.980e+02 -1.000e+03 -1.021e+03 -1.022e+03 -1.025e+03\n",
      " -1.027e+03 -1.030e+03 -1.036e+03  1.037e+03 -1.038e+03 -1.045e+03\n",
      " -1.050e+03 -1.057e+03 -1.065e+03 -1.066e+03 -1.075e+03 -1.077e+03\n",
      "  1.079e+03  1.081e+03  1.093e+03 -1.095e+03 -1.117e+03 -1.125e+03\n",
      " -1.131e+03  1.133e+03  1.134e+03 -1.138e+03 -1.143e+03 -1.147e+03\n",
      " -1.152e+03 -1.157e+03 -1.168e+03 -1.172e+03 -1.179e+03 -1.183e+03\n",
      " -1.190e+03 -1.191e+03  1.202e+03 -1.218e+03 -1.224e+03 -1.245e+03\n",
      " -1.253e+03 -1.265e+03 -1.273e+03 -1.274e+03 -1.278e+03 -1.288e+03\n",
      "  1.289e+03 -1.311e+03 -1.315e+03 -1.346e+03 -1.353e+03 -1.358e+03\n",
      " -1.368e+03  1.394e+03 -1.409e+03 -1.409e+03 -1.425e+03  1.430e+03\n",
      " -1.435e+03 -1.440e+03 -1.443e+03  1.459e+03 -1.460e+03 -1.466e+03\n",
      "  1.466e+03 -1.472e+03 -1.478e+03 -1.483e+03  1.504e+03 -1.508e+03\n",
      " -1.512e+03 -1.523e+03 -1.527e+03 -1.555e+03 -1.559e+03 -1.560e+03\n",
      " -1.584e+03  1.591e+03 -1.593e+03 -1.628e+03  1.641e+03 -1.660e+03\n",
      " -1.663e+03 -1.663e+03 -1.665e+03 -1.686e+03 -1.690e+03 -1.699e+03\n",
      "  1.718e+03 -1.721e+03 -1.724e+03  1.732e+03 -1.737e+03  1.748e+03\n",
      "  1.762e+03  1.838e+03 -1.840e+03 -1.897e+03 -1.899e+03 -1.971e+03\n",
      "  1.972e+03 -1.995e+03 -2.016e+03  2.064e+03 -2.143e+03 -2.161e+03\n",
      " -2.169e+03 -2.182e+03 -2.239e+03 -2.265e+03 -2.298e+03  2.319e+03\n",
      " -2.327e+03 -2.359e+03 -2.562e+03  2.570e+03  2.703e+03 -2.727e+03\n",
      "  2.741e+03 -2.784e+03  2.900e+03  3.059e+03 -3.221e+03  3.314e+03\n",
      " -3.381e+03 -3.930e+03 -4.241e+03 -4.282e+03  4.760e+03  5.152e+03\n",
      " -5.480e+03]\n",
      "Concordance Index 0.552373122389121\n",
      "Integrated Brier Score: 0.18525625605790097\n",
      "y_train breslow final [ 2.000e+00  2.000e+00 -1.400e+01  1.400e+01  2.300e+01 -3.000e+01\n",
      "  5.600e+01 -5.600e+01  6.200e+01  6.400e+01  6.500e+01  7.600e+01\n",
      "  8.200e+01  8.600e+01  8.900e+01  9.000e+01 -9.200e+01  9.400e+01\n",
      "  9.500e+01 -1.020e+02 -1.050e+02  1.070e+02  1.080e+02 -1.100e+02\n",
      " -1.110e+02  1.120e+02 -1.220e+02  1.290e+02  1.290e+02 -1.340e+02\n",
      "  1.420e+02  1.440e+02 -1.460e+02  1.510e+02 -1.560e+02  1.600e+02\n",
      "  1.710e+02 -1.810e+02  1.820e+02  1.830e+02  1.910e+02  1.940e+02\n",
      "  1.940e+02  1.980e+02  2.050e+02 -2.100e+02  2.150e+02 -2.160e+02\n",
      "  2.170e+02  2.180e+02 -2.250e+02 -2.320e+02  2.430e+02  2.530e+02\n",
      "  2.560e+02  2.590e+02  2.610e+02  2.680e+02  2.740e+02  2.760e+02\n",
      "  2.760e+02 -2.780e+02  2.790e+02  2.810e+02  2.820e+02  2.830e+02\n",
      "  2.840e+02 -2.920e+02  2.920e+02  2.940e+02  2.950e+02  3.170e+02\n",
      "  3.170e+02 -3.180e+02 -3.210e+02  3.270e+02  3.270e+02  3.300e+02\n",
      "  3.340e+02  3.360e+02  3.370e+02  3.410e+02  3.420e+02 -3.510e+02\n",
      "  3.510e+02  3.520e+02 -3.540e+02  3.570e+02 -3.580e+02  3.590e+02\n",
      "  3.600e+02 -3.600e+02  3.610e+02 -3.620e+02  3.660e+02 -3.670e+02\n",
      " -3.680e+02  3.710e+02  3.770e+02  3.770e+02 -3.780e+02  3.790e+02\n",
      " -3.790e+02  3.800e+02  3.840e+02 -3.840e+02  3.850e+02  3.850e+02\n",
      " -3.860e+02  3.870e+02 -3.870e+02 -3.890e+02 -3.920e+02 -3.920e+02\n",
      " -3.920e+02  3.930e+02  3.970e+02 -4.000e+02 -4.030e+02  4.030e+02\n",
      "  4.060e+02  4.070e+02 -4.070e+02 -4.120e+02 -4.140e+02  4.150e+02\n",
      " -4.170e+02  4.210e+02 -4.210e+02  4.270e+02  4.300e+02  4.360e+02\n",
      " -4.370e+02 -4.400e+02 -4.410e+02 -4.430e+02  4.460e+02  4.490e+02\n",
      "  4.510e+02  4.530e+02  4.560e+02  4.590e+02  4.620e+02  4.640e+02\n",
      " -4.660e+02 -4.690e+02  4.790e+02 -4.790e+02  4.840e+02 -4.850e+02\n",
      "  4.890e+02 -4.910e+02  4.930e+02  4.950e+02 -4.990e+02  5.060e+02\n",
      " -5.090e+02 -5.140e+02 -5.160e+02 -5.180e+02 -5.200e+02  5.210e+02\n",
      "  5.220e+02  5.260e+02  5.300e+02 -5.420e+02  5.430e+02  5.450e+02\n",
      "  5.460e+02  5.460e+02 -5.470e+02  5.480e+02 -5.490e+02 -5.520e+02\n",
      " -5.590e+02  5.640e+02 -5.680e+02  5.720e+02 -5.730e+02 -5.760e+02\n",
      "  5.770e+02  5.770e+02 -5.780e+02 -5.790e+02  5.800e+02  5.840e+02\n",
      " -5.860e+02 -6.000e+02  6.010e+02 -6.030e+02  6.060e+02 -6.060e+02\n",
      " -6.160e+02 -6.160e+02 -6.190e+02 -6.220e+02 -6.220e+02 -6.240e+02\n",
      "  6.240e+02  6.250e+02  6.310e+02 -6.370e+02 -6.390e+02 -6.410e+02\n",
      "  6.410e+02 -6.440e+02 -6.450e+02 -6.450e+02 -6.460e+02 -6.530e+02\n",
      " -6.580e+02  6.630e+02  6.660e+02 -6.670e+02 -6.700e+02  6.750e+02\n",
      " -6.790e+02  6.800e+02 -6.820e+02 -6.850e+02 -6.870e+02 -6.930e+02\n",
      " -6.950e+02  6.950e+02 -7.010e+02 -7.040e+02 -7.070e+02 -7.100e+02\n",
      " -7.140e+02 -7.220e+02 -7.250e+02  7.270e+02  7.390e+02 -7.420e+02\n",
      " -7.590e+02 -7.590e+02 -7.590e+02  7.630e+02  7.730e+02 -7.820e+02\n",
      " -7.830e+02 -7.960e+02 -7.970e+02 -7.990e+02  8.060e+02 -8.170e+02\n",
      "  8.230e+02 -8.270e+02 -8.330e+02  8.360e+02 -8.360e+02 -8.500e+02\n",
      " -8.510e+02 -8.520e+02  8.530e+02 -8.550e+02  8.620e+02 -8.660e+02\n",
      " -8.750e+02  8.820e+02 -8.890e+02 -8.930e+02 -8.950e+02 -8.960e+02\n",
      " -8.970e+02 -9.100e+02  9.150e+02 -9.180e+02 -9.300e+02 -9.330e+02\n",
      "  9.410e+02 -9.470e+02 -9.500e+02 -9.540e+02 -9.720e+02  9.800e+02\n",
      "  9.850e+02 -9.870e+02  9.880e+02  9.930e+02 -9.930e+02 -9.950e+02\n",
      " -9.970e+02  9.980e+02 -1.000e+03 -1.021e+03 -1.022e+03 -1.025e+03\n",
      " -1.027e+03 -1.030e+03 -1.036e+03  1.037e+03 -1.038e+03 -1.045e+03\n",
      " -1.050e+03 -1.057e+03 -1.065e+03 -1.066e+03 -1.075e+03 -1.077e+03\n",
      "  1.079e+03  1.081e+03  1.093e+03 -1.095e+03 -1.117e+03 -1.125e+03\n",
      " -1.131e+03  1.133e+03  1.134e+03 -1.138e+03 -1.143e+03 -1.147e+03\n",
      " -1.152e+03 -1.157e+03 -1.168e+03 -1.172e+03 -1.179e+03 -1.183e+03\n",
      " -1.190e+03 -1.191e+03  1.202e+03 -1.218e+03 -1.224e+03 -1.245e+03\n",
      " -1.253e+03 -1.265e+03 -1.273e+03 -1.274e+03 -1.278e+03 -1.288e+03\n",
      "  1.289e+03 -1.311e+03 -1.315e+03 -1.346e+03 -1.353e+03 -1.358e+03\n",
      " -1.368e+03  1.394e+03 -1.409e+03 -1.409e+03 -1.425e+03  1.430e+03\n",
      " -1.435e+03 -1.440e+03 -1.443e+03  1.459e+03 -1.460e+03 -1.466e+03\n",
      "  1.466e+03 -1.472e+03 -1.478e+03 -1.483e+03  1.504e+03 -1.508e+03\n",
      " -1.512e+03 -1.523e+03 -1.527e+03 -1.555e+03 -1.559e+03 -1.560e+03\n",
      " -1.584e+03  1.591e+03 -1.593e+03 -1.628e+03  1.641e+03 -1.660e+03\n",
      " -1.663e+03 -1.663e+03 -1.665e+03 -1.686e+03 -1.690e+03 -1.699e+03\n",
      "  1.718e+03 -1.721e+03 -1.724e+03  1.732e+03 -1.737e+03  1.748e+03\n",
      "  1.762e+03  1.838e+03 -1.840e+03 -1.897e+03 -1.899e+03 -1.971e+03\n",
      "  1.972e+03 -1.995e+03 -2.016e+03  2.064e+03 -2.143e+03 -2.161e+03\n",
      " -2.169e+03 -2.182e+03 -2.239e+03 -2.265e+03 -2.298e+03  2.319e+03\n",
      " -2.327e+03 -2.359e+03 -2.562e+03  2.570e+03  2.703e+03 -2.727e+03\n",
      "  2.741e+03 -2.784e+03  2.900e+03  3.059e+03 -3.221e+03  3.314e+03\n",
      " -3.381e+03 -3.930e+03 -4.241e+03 -4.282e+03  4.760e+03  5.152e+03\n",
      " -5.480e+03]\n",
      "durations 11.0 6417.0\n",
      "Concordance Index 0.5112781954887218\n",
      "Integrated Brier Score: 0.21270593121875642\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m87.8816\u001b[0m  0.0071\n",
      "      2       87.8816  0.0042\n",
      "      3       87.8816  0.0029\n",
      "      4       87.8816  0.0021\n",
      "      5       87.8816  0.0022\n",
      "      6       87.8816  0.0021\n",
      "      7       87.8816  0.0020\n",
      "      8       87.8816  0.0020\n",
      "      9       87.8816  0.0020\n",
      "     10       87.8816  0.0034\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m102.4996\u001b[0m  0.0041\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m76.1810\u001b[0m  0.0033\n",
      "      2      102.4996  0.0027\n",
      "      2       76.1810  0.0024\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m99.1225\u001b[0m  0.0037\n",
      "      3      102.4996  0.0024\n",
      "      3       76.1810  0.0020\n",
      "      4       76.1810  0.0020\n",
      "      4      102.4996  0.0023\n",
      "      2       99.1225  0.0025\n",
      "      5       76.1810  0.0019\n",
      "      5      102.4996  0.0020\n",
      "      3       99.1225  0.0021\n",
      "      6       76.1810  0.0021\n",
      "      6      102.4996  0.0022\n",
      "      4       99.1225  0.0023\n",
      "      7       76.1810  0.0020\n",
      "      7      102.4996  0.0020\n",
      "      5       99.1225  0.0021\n",
      "      8       76.1810  0.0019\n",
      "      8      102.4996  0.0020\n",
      "      6       99.1225  0.0019\n",
      "      9       76.1810  0.0019\n",
      "      9      102.4996  0.0020\n",
      "      7       99.1225  0.0019\n",
      "     10      102.4996  0.0020\n",
      "     10       76.1810  0.0024\n",
      "Restoring best model from epoch 1.\n",
      "Restoring best model from epoch 1.\n",
      "      8       99.1225  0.0039\n",
      "      9       99.1225  0.0044\n",
      "     10       99.1225  0.0043\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m223.6412\u001b[0m  0.0102\n",
      "      2      223.6412  0.0046\n",
      "      3      223.6412  0.0066\n",
      "      4      223.6412  0.0039\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      5      223.6412  0.0039\n",
      "      6      223.6412  0.0030\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing optimizer.\n",
      "      7      223.6412  0.0020\n",
      "      8      223.6412  0.0020\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m100.4782\u001b[0m  0.0035\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m163.0410\u001b[0m  0.0032\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m115.4507\u001b[0m  0.0038\n",
      "      2      163.0410  0.0021\n",
      "      9      223.6412  0.0042\n",
      "      2      100.4782  0.0029\n",
      "      2      115.4507  0.0024\n",
      "      3      163.0410  0.0020\n",
      "      3      115.4507  0.0023\n",
      "     10      223.6412  0.0035\n",
      "      3      100.4782  0.0026\n",
      "Restoring best model from epoch 1.\n",
      "      4      163.0410  0.0022\n",
      "      4      100.4782  0.0021\n",
      "      4      115.4507  0.0027\n",
      "      5      163.0410  0.0030\n",
      "      5      100.4782  0.0020\n",
      "      5      115.4507  0.0027\n",
      "      6      163.0410  0.0024\n",
      "      6      100.4782  0.0022\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      7      163.0410  0.0021\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      6      115.4507  0.0030\n",
      "      7      100.4782  0.0036\n",
      "      8      163.0410  0.0022\n",
      "      7      115.4507  0.0021\n",
      "      9      163.0410  0.0021\n",
      "      8      100.4782  0.0026\n",
      "      8      115.4507  0.0028\n",
      "     10      163.0410  0.0026\n",
      "Restoring best model from epoch 1.\n",
      "      9      115.4507  0.0028\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m257.2838\u001b[0m  0.0055\n",
      "      9      100.4782  0.0059\n",
      "     10      115.4507  0.0035\n",
      "Restoring best model from epoch 1.\n",
      "      2      257.2838  0.0032\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "     10      100.4782  0.0027\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Restoring best model from epoch 1.\n",
      "      3      257.2838  0.0043\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m243.2712\u001b[0m  0.0037\n",
      "      4      257.2838  0.0022\n",
      "      2      243.2712  0.0027\n",
      "      5      257.2838  0.0024\n",
      "      6      257.2838  0.0026\n",
      "      3      243.2712  0.0031\n",
      "      7      257.2838  0.0022\n",
      "      4      243.2712  0.0023\n",
      "      8      257.2838  0.0020\n",
      "      5      243.2712  0.0024\n",
      "      9      257.2838  0.0019\n",
      "      6      243.2712  0.0023\n",
      "      7      243.2712  0.0021\n",
      "     10      257.2838  0.0044\n",
      "Restoring best model from epoch 1.\n",
      "      8      243.2712  0.0034\n",
      "      9      243.2712  0.0039\n",
      "     10      243.2712  0.0024\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m140.2177\u001b[0m  0.0041\n",
      "      2      140.2177  0.0037\n",
      "      3      140.2177  0.0063\n",
      "      4      140.2177  0.0038\n",
      "      5      140.2177  0.0042\n",
      "      6      140.2177  0.0034\n",
      "      7      140.2177  0.0040\n",
      "      8      140.2177  0.0063\n",
      "      9      140.2177  0.0077\n",
      "     10      140.2177  0.0065\n",
      "Restoring best model from epoch 1.\n",
      "y_train breslow final [  -11.    11.   -14.    14.    23.   -30.    32.   -56.    62.    64.\n",
      "    69.    76.   -77.    82.    86.    89.    90.    95.  -105.   107.\n",
      "   108.  -110.   112.  -122.   126.   129.   129.   130.   142.   144.\n",
      "   144.  -146.   151.  -156.   166.   171.   173.  -181.   182.   183.\n",
      "   185.  -187.   191.   197.   198.   205.   209.  -210.   215.   215.\n",
      "  -216.  -216.   217.   217.   222.  -225.  -232.  -234.  -242.  -253.\n",
      "   253.   256.   259.   261.   274.   276.   276.   279.   281.   282.\n",
      "   283.  -292.   292.   295.  -311.   317.  -321.   321.   327.   330.\n",
      "   334.   336.   337.   341.   342.  -345.   348.  -351.   352.   353.\n",
      "  -354.   357.  -358.   359.   360.  -362.   366.  -367.   371.  -376.\n",
      "   377.  -379.   380.   384.   385.   385.  -386.   387.  -387.  -389.\n",
      "  -392.  -392.  -392.  -393.   395.   395.   397.  -400.  -403.   403.\n",
      "   406.   406.   407.  -407.  -412.  -414.   415.  -417.  -424.   424.\n",
      "   427.   430.   436.  -437.  -440.  -441.   446.   449.   451.   456.\n",
      "  -458.   459.  -461.   462.   464.  -471.   472.  -479.   480.  -485.\n",
      "   487.   489.   493.  -499.   506.  -509.  -514.  -520.   521.   521.\n",
      "   522.   530.  -539.  -542.  -543.   545.   546.   548.  -549.  -552.\n",
      "  -559.   560.   564.  -573.  -576.   577.  -578.  -579.   580.   584.\n",
      "  -586.  -594.  -600.   601.   602.  -603.  -606.  -616.  -616.  -622.\n",
      "  -622.  -623.  -623.  -624.   624.  -625.  -635.  -637.  -639.  -641.\n",
      "  -641.   641.  -644.  -645.  -645.  -646.  -653.   654.  -658.   663.\n",
      "  -667.   675.  -685.  -687.  -695.   695.  -701.  -701.  -704.  -710.\n",
      "  -713.  -714.  -717.  -722.  -725.   727.   739.  -750.  -754.  -759.\n",
      "  -759.  -759.   763.  -770.   773.  -773.  -782.  -783.   789.  -796.\n",
      "   804.   806.   823.  -827.  -833.   836.  -836.   839.  -850.  -852.\n",
      "   853.  -855.   862.  -866.  -875.   882.  -889.  -895.  -896.  -897.\n",
      "  -904.  -906.  -911.  -913.  -914.   915.  -918.   927.  -930.  -933.\n",
      "   941.  -947.  -954.  -964.  -972.   985.  -987.   988.  -993.  -995.\n",
      "   998. -1000. -1022. -1025. -1027. -1030.  1037. -1045. -1050. -1057.\n",
      " -1065. -1066. -1075. -1077.  1079.  1081.  1090.  1093. -1095. -1117.\n",
      " -1120. -1125. -1131.  1134. -1143. -1147. -1152. -1157. -1168. -1183.\n",
      " -1190. -1191. -1218. -1253. -1265. -1274. -1278. -1288.  1289. -1309.\n",
      " -1311. -1315. -1353. -1358.  1394.  1398. -1399. -1413. -1428.  1430.\n",
      " -1430. -1435. -1440. -1443.  1459. -1460. -1461. -1466. -1472. -1478.\n",
      "  1504. -1506. -1512. -1521. -1523. -1555. -1559. -1560. -1584. -1584.\n",
      " -1586.  1591. -1593.  1641. -1660. -1663. -1665.  1671. -1686. -1690.\n",
      " -1699.  1718. -1721. -1722. -1724. -1737.  1748.  1762. -1777. -1778.\n",
      "  1838. -1897. -1899. -1995.  2002. -2016. -2016.  2083.  2120. -2133.\n",
      " -2143. -2161.  2166. -2169. -2182. -2239. -2265. -2298.  2319. -2327.\n",
      " -2347. -2359. -2437. -2562.  2570. -2641.  2703.  2717.  2741. -2784.\n",
      " -2886.  2900.  3059. -3221. -3270.  3314. -3381. -3981. -4282.  4680.\n",
      "  4760.  4856.  5152. -5252.  6417.]\n",
      "Concordance Index 0.5290154264972777\n",
      "Integrated Brier Score: 0.1748608282633937\n",
      "y_train breslow final [  -11.    11.   -14.    14.    23.   -30.    32.   -56.    62.    64.\n",
      "    69.    76.   -77.    82.    86.    89.    90.    95.  -105.   107.\n",
      "   108.  -110.   112.  -122.   126.   129.   129.   130.   142.   144.\n",
      "   144.  -146.   151.  -156.   166.   171.   173.  -181.   182.   183.\n",
      "   185.  -187.   191.   197.   198.   205.   209.  -210.   215.   215.\n",
      "  -216.  -216.   217.   217.   222.  -225.  -232.  -234.  -242.  -253.\n",
      "   253.   256.   259.   261.   274.   276.   276.   279.   281.   282.\n",
      "   283.  -292.   292.   295.  -311.   317.  -321.   321.   327.   330.\n",
      "   334.   336.   337.   341.   342.  -345.   348.  -351.   352.   353.\n",
      "  -354.   357.  -358.   359.   360.  -362.   366.  -367.   371.  -376.\n",
      "   377.  -379.   380.   384.   385.   385.  -386.   387.  -387.  -389.\n",
      "  -392.  -392.  -392.  -393.   395.   395.   397.  -400.  -403.   403.\n",
      "   406.   406.   407.  -407.  -412.  -414.   415.  -417.  -424.   424.\n",
      "   427.   430.   436.  -437.  -440.  -441.   446.   449.   451.   456.\n",
      "  -458.   459.  -461.   462.   464.  -471.   472.  -479.   480.  -485.\n",
      "   487.   489.   493.  -499.   506.  -509.  -514.  -520.   521.   521.\n",
      "   522.   530.  -539.  -542.  -543.   545.   546.   548.  -549.  -552.\n",
      "  -559.   560.   564.  -573.  -576.   577.  -578.  -579.   580.   584.\n",
      "  -586.  -594.  -600.   601.   602.  -603.  -606.  -616.  -616.  -622.\n",
      "  -622.  -623.  -623.  -624.   624.  -625.  -635.  -637.  -639.  -641.\n",
      "  -641.   641.  -644.  -645.  -645.  -646.  -653.   654.  -658.   663.\n",
      "  -667.   675.  -685.  -687.  -695.   695.  -701.  -701.  -704.  -710.\n",
      "  -713.  -714.  -717.  -722.  -725.   727.   739.  -750.  -754.  -759.\n",
      "  -759.  -759.   763.  -770.   773.  -773.  -782.  -783.   789.  -796.\n",
      "   804.   806.   823.  -827.  -833.   836.  -836.   839.  -850.  -852.\n",
      "   853.  -855.   862.  -866.  -875.   882.  -889.  -895.  -896.  -897.\n",
      "  -904.  -906.  -911.  -913.  -914.   915.  -918.   927.  -930.  -933.\n",
      "   941.  -947.  -954.  -964.  -972.   985.  -987.   988.  -993.  -995.\n",
      "   998. -1000. -1022. -1025. -1027. -1030.  1037. -1045. -1050. -1057.\n",
      " -1065. -1066. -1075. -1077.  1079.  1081.  1090.  1093. -1095. -1117.\n",
      " -1120. -1125. -1131.  1134. -1143. -1147. -1152. -1157. -1168. -1183.\n",
      " -1190. -1191. -1218. -1253. -1265. -1274. -1278. -1288.  1289. -1309.\n",
      " -1311. -1315. -1353. -1358.  1394.  1398. -1399. -1413. -1428.  1430.\n",
      " -1430. -1435. -1440. -1443.  1459. -1460. -1461. -1466. -1472. -1478.\n",
      "  1504. -1506. -1512. -1521. -1523. -1555. -1559. -1560. -1584. -1584.\n",
      " -1586.  1591. -1593.  1641. -1660. -1663. -1665.  1671. -1686. -1690.\n",
      " -1699.  1718. -1721. -1722. -1724. -1737.  1748.  1762. -1777. -1778.\n",
      "  1838. -1897. -1899. -1995.  2002. -2016. -2016.  2083.  2120. -2133.\n",
      " -2143. -2161.  2166. -2169. -2182. -2239. -2265. -2298.  2319. -2327.\n",
      " -2347. -2359. -2437. -2562.  2570. -2641.  2703.  2717.  2741. -2784.\n",
      " -2886.  2900.  3059. -3221. -3270.  3314. -3381. -3981. -4282.  4680.\n",
      "  4760.  4856.  5152. -5252.  6417.]\n",
      "durations 2.0 5480.0\n",
      "Concordance Index 0.4785788923719958\n",
      "Integrated Brier Score: 0.2200171825229733\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m70.8828\u001b[0m  0.0032\n",
      "      2       70.8828  0.0031\n",
      "      3       70.8828  0.0022\n",
      "      4       70.8828  0.0020\n",
      "      5       70.8828  0.0045\n",
      "      6       70.8828  0.0037\n",
      "      7       70.8828  0.0019\n",
      "      8       70.8828  0.0019\n",
      "      9       70.8828  0.0059\n",
      "     10       70.8828  0.0038\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m163.6536\u001b[0m  0.0045\n",
      "      2      163.6536  0.0028\n",
      "      3      163.6536  0.0020\n",
      "      4      163.6536  0.0048\n",
      "      5      163.6536  0.0046\n",
      "      6      163.6536  0.0041\n",
      "      7      163.6536  0.0034\n",
      "      8      163.6536  0.0027\n",
      "      9      163.6536  0.0026\n",
      "     10      163.6536  0.0022\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m105.0456\u001b[0m  0.0041\n",
      "      2      105.0456  0.0029\n",
      "      3      105.0456  0.0023\n",
      "      4      105.0456  0.0036\n",
      "      5      105.0456  0.0056\n",
      "      6      105.0456  0.0031\n",
      "      7      105.0456  0.0044\n",
      "      8      105.0456  0.0036\n",
      "      9      105.0456  0.0069\n",
      "     10      105.0456  0.0057\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m103.2281\u001b[0m  0.0043\n",
      "      2      103.2281  0.0025\n",
      "      3      103.2281  0.0031\n",
      "      4      103.2281  0.0026\n",
      "      5      103.2281  0.0022\n",
      "      6      103.2281  0.0026\n",
      "      7      103.2281  0.0023\n",
      "      8      103.2281  0.0022\n",
      "      9      103.2281  0.0065\n",
      "     10      103.2281  0.0026\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m227.8382\u001b[0m  0.0030\n",
      "      2      227.8382  0.0021\n",
      "      3      227.8382  0.0020\n",
      "      4      227.8382  0.0019\n",
      "      5      227.8382  0.0019\n",
      "      6      227.8382  0.0019\n",
      "      7      227.8382  0.0026\n",
      "      8      227.8382  0.0035\n",
      "      9      227.8382  0.0021\n",
      "     10      227.8382  0.0020\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m166.6253\u001b[0m  0.0030\n",
      "      2      166.6253  0.0021\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      3      166.6253  0.0021\n",
      "      4      166.6253  0.0023\n",
      "      5      166.6253  0.0019\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m235.4548\u001b[0m  0.0034\n",
      "      6      166.6253  0.0019\n",
      "      2      235.4548  0.0024\n",
      "      7      166.6253  0.0019\n",
      "      3      235.4548  0.0023\n",
      "      8      166.6253  0.0020\n",
      "      4      235.4548  0.0023\n",
      "      9      166.6253  0.0021\n",
      "      5      235.4548  0.0022\n",
      "     10      166.6253  0.0025\n",
      "Restoring best model from epoch 1.\n",
      "      6      235.4548  0.0021\n",
      "      7      235.4548  0.0020\n",
      "      8      235.4548  0.0019\n",
      "      9      235.4548  0.0019\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "     10      235.4548  0.0019\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m107.5689\u001b[0m  0.0094\n",
      "      2      107.5689  0.0036\n",
      "      3      107.5689  0.0029\n",
      "      4      107.5689  0.0022\n",
      "      5      107.5689  0.0056\n",
      "      6      107.5689  0.0037\n",
      "      7      107.5689  0.0023\n",
      "      8      107.5689  0.0021\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      9      107.5689  0.0035\n",
      "     10      107.5689  0.0023\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Restoring best model from epoch 1.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m199.0015\u001b[0m  0.0035\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2      199.0015  0.0030\n",
      "      3      199.0015  0.0028\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m79.9892\u001b[0m  0.0038\n",
      "      4      199.0015  0.0020\n",
      "      2       79.9892  0.0029\n",
      "      5      199.0015  0.0024\n",
      "      3       79.9892  0.0021\n",
      "      6      199.0015  0.0023\n",
      "      4       79.9892  0.0022\n",
      "      7      199.0015  0.0021\n",
      "      5       79.9892  0.0023\n",
      "      8      199.0015  0.0019\n",
      "      9      199.0015  0.0019\n",
      "      6       79.9892  0.0026\n",
      "     10      199.0015  0.0024\n",
      "Restoring best model from epoch 1.\n",
      "      7       79.9892  0.0023\n",
      "      8       79.9892  0.0022\n",
      "      9       79.9892  0.0020\n",
      "     10       79.9892  0.0019\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m103.1230\u001b[0m  0.0047\n",
      "      2      103.1230  0.0042\n",
      "      3      103.1230  0.0039\n",
      "      4      103.1230  0.0036\n",
      "      5      103.1230  0.0034\n",
      "      6      103.1230  0.0056\n",
      "      7      103.1230  0.0080\n",
      "      8      103.1230  0.0070\n",
      "      9      103.1230  0.0055\n",
      "     10      103.1230  0.0047\n",
      "Restoring best model from epoch 1.\n",
      "y_train breslow final [ 2.000e+00  2.000e+00 -1.100e+01  1.100e+01 -1.400e+01  2.300e+01\n",
      " -3.000e+01  3.200e+01  5.600e+01 -5.600e+01  6.400e+01  6.500e+01\n",
      "  6.900e+01  7.600e+01 -7.700e+01  8.600e+01  8.900e+01 -9.200e+01\n",
      "  9.400e+01  9.500e+01 -1.020e+02  1.080e+02 -1.100e+02 -1.110e+02\n",
      " -1.220e+02  1.260e+02  1.290e+02  1.300e+02 -1.340e+02  1.440e+02\n",
      "  1.440e+02 -1.460e+02  1.510e+02  1.600e+02  1.660e+02  1.710e+02\n",
      "  1.730e+02  1.830e+02  1.850e+02 -1.870e+02  1.910e+02  1.940e+02\n",
      "  1.940e+02  1.970e+02  1.980e+02  2.090e+02 -2.100e+02  2.150e+02\n",
      "  2.150e+02 -2.160e+02 -2.160e+02  2.170e+02  2.180e+02  2.220e+02\n",
      " -2.320e+02 -2.340e+02 -2.420e+02  2.430e+02 -2.530e+02  2.560e+02\n",
      "  2.590e+02  2.680e+02  2.740e+02  2.760e+02  2.760e+02 -2.780e+02\n",
      "  2.810e+02  2.820e+02  2.830e+02  2.840e+02 -2.920e+02  2.920e+02\n",
      "  2.940e+02 -3.110e+02  3.170e+02  3.170e+02 -3.180e+02 -3.210e+02\n",
      "  3.210e+02  3.270e+02  3.300e+02  3.340e+02  3.360e+02  3.370e+02\n",
      "  3.410e+02  3.420e+02 -3.450e+02  3.480e+02 -3.510e+02  3.510e+02\n",
      "  3.520e+02  3.530e+02 -3.580e+02  3.590e+02  3.600e+02 -3.600e+02\n",
      "  3.610e+02 -3.620e+02  3.660e+02 -3.680e+02 -3.760e+02  3.770e+02\n",
      "  3.770e+02 -3.780e+02  3.790e+02  3.840e+02 -3.840e+02  3.850e+02\n",
      "  3.850e+02  3.870e+02 -3.870e+02 -3.920e+02 -3.920e+02  3.930e+02\n",
      " -3.930e+02  3.950e+02  3.950e+02  3.970e+02 -4.030e+02  4.030e+02\n",
      "  4.030e+02  4.060e+02  4.060e+02  4.070e+02 -4.070e+02 -4.120e+02\n",
      "  4.150e+02 -4.170e+02  4.210e+02 -4.210e+02 -4.240e+02  4.240e+02\n",
      "  4.270e+02  4.300e+02 -4.370e+02 -4.400e+02 -4.430e+02  4.490e+02\n",
      "  4.510e+02  4.530e+02 -4.580e+02 -4.610e+02  4.620e+02  4.640e+02\n",
      " -4.660e+02 -4.690e+02 -4.710e+02  4.720e+02  4.790e+02 -4.790e+02\n",
      "  4.800e+02  4.840e+02 -4.850e+02  4.870e+02  4.890e+02 -4.910e+02\n",
      "  4.930e+02  4.950e+02 -5.090e+02 -5.140e+02 -5.160e+02 -5.180e+02\n",
      " -5.200e+02  5.210e+02  5.260e+02  5.300e+02 -5.390e+02 -5.420e+02\n",
      " -5.430e+02  5.430e+02  5.450e+02  5.460e+02  5.460e+02 -5.470e+02\n",
      "  5.480e+02 -5.490e+02 -5.520e+02 -5.590e+02  5.600e+02  5.640e+02\n",
      " -5.680e+02  5.720e+02 -5.730e+02 -5.760e+02  5.770e+02  5.770e+02\n",
      " -5.790e+02  5.800e+02 -5.860e+02 -5.940e+02 -6.000e+02  6.010e+02\n",
      "  6.020e+02 -6.030e+02  6.060e+02 -6.160e+02 -6.160e+02 -6.190e+02\n",
      " -6.220e+02 -6.220e+02 -6.230e+02 -6.230e+02 -6.240e+02  6.240e+02\n",
      "  6.250e+02 -6.250e+02  6.310e+02 -6.350e+02 -6.370e+02 -6.410e+02\n",
      " -6.410e+02  6.410e+02 -6.440e+02 -6.450e+02 -6.450e+02 -6.530e+02\n",
      "  6.540e+02 -6.580e+02  6.630e+02  6.660e+02 -6.670e+02 -6.700e+02\n",
      " -6.790e+02  6.800e+02 -6.820e+02 -6.850e+02 -6.870e+02 -6.930e+02\n",
      " -6.950e+02 -7.010e+02 -7.010e+02 -7.070e+02 -7.100e+02 -7.130e+02\n",
      " -7.140e+02 -7.170e+02 -7.220e+02 -7.250e+02  7.270e+02  7.390e+02\n",
      " -7.420e+02 -7.500e+02 -7.540e+02 -7.590e+02 -7.590e+02 -7.590e+02\n",
      "  7.630e+02 -7.700e+02  7.730e+02 -7.730e+02 -7.820e+02 -7.830e+02\n",
      "  7.890e+02 -7.960e+02 -7.970e+02 -7.990e+02  8.040e+02  8.060e+02\n",
      " -8.170e+02 -8.270e+02 -8.330e+02 -8.360e+02  8.390e+02 -8.500e+02\n",
      " -8.510e+02  8.530e+02 -8.550e+02  8.620e+02  8.820e+02 -8.890e+02\n",
      " -8.930e+02 -8.950e+02 -8.970e+02 -9.040e+02 -9.060e+02 -9.100e+02\n",
      " -9.110e+02 -9.130e+02 -9.140e+02  9.270e+02 -9.300e+02 -9.330e+02\n",
      "  9.410e+02 -9.470e+02 -9.500e+02 -9.640e+02 -9.720e+02  9.800e+02\n",
      "  9.850e+02  9.930e+02 -9.930e+02 -9.950e+02 -9.970e+02 -1.000e+03\n",
      " -1.021e+03 -1.030e+03 -1.036e+03 -1.038e+03 -1.050e+03 -1.075e+03\n",
      " -1.077e+03  1.079e+03  1.081e+03  1.090e+03  1.093e+03 -1.095e+03\n",
      " -1.120e+03 -1.125e+03 -1.131e+03  1.133e+03 -1.138e+03 -1.143e+03\n",
      " -1.147e+03 -1.168e+03 -1.172e+03 -1.179e+03 -1.183e+03 -1.190e+03\n",
      "  1.202e+03 -1.218e+03 -1.224e+03 -1.245e+03 -1.253e+03 -1.265e+03\n",
      " -1.273e+03 -1.274e+03 -1.278e+03 -1.288e+03  1.289e+03 -1.309e+03\n",
      " -1.311e+03 -1.315e+03 -1.346e+03 -1.353e+03 -1.368e+03  1.394e+03\n",
      "  1.398e+03 -1.399e+03 -1.409e+03 -1.409e+03 -1.413e+03 -1.425e+03\n",
      " -1.428e+03  1.430e+03 -1.430e+03 -1.443e+03  1.459e+03 -1.460e+03\n",
      " -1.461e+03 -1.466e+03  1.466e+03 -1.472e+03 -1.483e+03 -1.506e+03\n",
      " -1.508e+03 -1.521e+03 -1.527e+03 -1.555e+03 -1.559e+03 -1.584e+03\n",
      " -1.584e+03 -1.586e+03 -1.628e+03 -1.663e+03  1.671e+03 -1.686e+03\n",
      " -1.690e+03 -1.699e+03  1.718e+03 -1.721e+03 -1.722e+03 -1.724e+03\n",
      "  1.732e+03 -1.737e+03  1.762e+03 -1.777e+03 -1.778e+03  1.838e+03\n",
      " -1.840e+03 -1.971e+03  1.972e+03 -1.995e+03  2.002e+03 -2.016e+03\n",
      " -2.016e+03  2.064e+03  2.083e+03  2.120e+03 -2.133e+03 -2.161e+03\n",
      "  2.166e+03 -2.169e+03 -2.182e+03  2.319e+03 -2.327e+03 -2.347e+03\n",
      " -2.437e+03 -2.562e+03 -2.641e+03  2.703e+03  2.717e+03 -2.727e+03\n",
      " -2.886e+03  2.900e+03 -3.270e+03  3.314e+03 -3.930e+03 -3.981e+03\n",
      " -4.241e+03 -4.282e+03  4.680e+03  4.760e+03  4.856e+03 -5.252e+03\n",
      " -5.480e+03  6.417e+03]\n",
      "Concordance Index 0.5136607122845552\n",
      "Integrated Brier Score: 0.1869276258463748\n",
      "y_train breslow final [ 2.000e+00  2.000e+00 -1.100e+01  1.100e+01 -1.400e+01  2.300e+01\n",
      " -3.000e+01  3.200e+01  5.600e+01 -5.600e+01  6.400e+01  6.500e+01\n",
      "  6.900e+01  7.600e+01 -7.700e+01  8.600e+01  8.900e+01 -9.200e+01\n",
      "  9.400e+01  9.500e+01 -1.020e+02  1.080e+02 -1.100e+02 -1.110e+02\n",
      " -1.220e+02  1.260e+02  1.290e+02  1.300e+02 -1.340e+02  1.440e+02\n",
      "  1.440e+02 -1.460e+02  1.510e+02  1.600e+02  1.660e+02  1.710e+02\n",
      "  1.730e+02  1.830e+02  1.850e+02 -1.870e+02  1.910e+02  1.940e+02\n",
      "  1.940e+02  1.970e+02  1.980e+02  2.090e+02 -2.100e+02  2.150e+02\n",
      "  2.150e+02 -2.160e+02 -2.160e+02  2.170e+02  2.180e+02  2.220e+02\n",
      " -2.320e+02 -2.340e+02 -2.420e+02  2.430e+02 -2.530e+02  2.560e+02\n",
      "  2.590e+02  2.680e+02  2.740e+02  2.760e+02  2.760e+02 -2.780e+02\n",
      "  2.810e+02  2.820e+02  2.830e+02  2.840e+02 -2.920e+02  2.920e+02\n",
      "  2.940e+02 -3.110e+02  3.170e+02  3.170e+02 -3.180e+02 -3.210e+02\n",
      "  3.210e+02  3.270e+02  3.300e+02  3.340e+02  3.360e+02  3.370e+02\n",
      "  3.410e+02  3.420e+02 -3.450e+02  3.480e+02 -3.510e+02  3.510e+02\n",
      "  3.520e+02  3.530e+02 -3.580e+02  3.590e+02  3.600e+02 -3.600e+02\n",
      "  3.610e+02 -3.620e+02  3.660e+02 -3.680e+02 -3.760e+02  3.770e+02\n",
      "  3.770e+02 -3.780e+02  3.790e+02  3.840e+02 -3.840e+02  3.850e+02\n",
      "  3.850e+02  3.870e+02 -3.870e+02 -3.920e+02 -3.920e+02  3.930e+02\n",
      " -3.930e+02  3.950e+02  3.950e+02  3.970e+02 -4.030e+02  4.030e+02\n",
      "  4.030e+02  4.060e+02  4.060e+02  4.070e+02 -4.070e+02 -4.120e+02\n",
      "  4.150e+02 -4.170e+02  4.210e+02 -4.210e+02 -4.240e+02  4.240e+02\n",
      "  4.270e+02  4.300e+02 -4.370e+02 -4.400e+02 -4.430e+02  4.490e+02\n",
      "  4.510e+02  4.530e+02 -4.580e+02 -4.610e+02  4.620e+02  4.640e+02\n",
      " -4.660e+02 -4.690e+02 -4.710e+02  4.720e+02  4.790e+02 -4.790e+02\n",
      "  4.800e+02  4.840e+02 -4.850e+02  4.870e+02  4.890e+02 -4.910e+02\n",
      "  4.930e+02  4.950e+02 -5.090e+02 -5.140e+02 -5.160e+02 -5.180e+02\n",
      " -5.200e+02  5.210e+02  5.260e+02  5.300e+02 -5.390e+02 -5.420e+02\n",
      " -5.430e+02  5.430e+02  5.450e+02  5.460e+02  5.460e+02 -5.470e+02\n",
      "  5.480e+02 -5.490e+02 -5.520e+02 -5.590e+02  5.600e+02  5.640e+02\n",
      " -5.680e+02  5.720e+02 -5.730e+02 -5.760e+02  5.770e+02  5.770e+02\n",
      " -5.790e+02  5.800e+02 -5.860e+02 -5.940e+02 -6.000e+02  6.010e+02\n",
      "  6.020e+02 -6.030e+02  6.060e+02 -6.160e+02 -6.160e+02 -6.190e+02\n",
      " -6.220e+02 -6.220e+02 -6.230e+02 -6.230e+02 -6.240e+02  6.240e+02\n",
      "  6.250e+02 -6.250e+02  6.310e+02 -6.350e+02 -6.370e+02 -6.410e+02\n",
      " -6.410e+02  6.410e+02 -6.440e+02 -6.450e+02 -6.450e+02 -6.530e+02\n",
      "  6.540e+02 -6.580e+02  6.630e+02  6.660e+02 -6.670e+02 -6.700e+02\n",
      " -6.790e+02  6.800e+02 -6.820e+02 -6.850e+02 -6.870e+02 -6.930e+02\n",
      " -6.950e+02 -7.010e+02 -7.010e+02 -7.070e+02 -7.100e+02 -7.130e+02\n",
      " -7.140e+02 -7.170e+02 -7.220e+02 -7.250e+02  7.270e+02  7.390e+02\n",
      " -7.420e+02 -7.500e+02 -7.540e+02 -7.590e+02 -7.590e+02 -7.590e+02\n",
      "  7.630e+02 -7.700e+02  7.730e+02 -7.730e+02 -7.820e+02 -7.830e+02\n",
      "  7.890e+02 -7.960e+02 -7.970e+02 -7.990e+02  8.040e+02  8.060e+02\n",
      " -8.170e+02 -8.270e+02 -8.330e+02 -8.360e+02  8.390e+02 -8.500e+02\n",
      " -8.510e+02  8.530e+02 -8.550e+02  8.620e+02  8.820e+02 -8.890e+02\n",
      " -8.930e+02 -8.950e+02 -8.970e+02 -9.040e+02 -9.060e+02 -9.100e+02\n",
      " -9.110e+02 -9.130e+02 -9.140e+02  9.270e+02 -9.300e+02 -9.330e+02\n",
      "  9.410e+02 -9.470e+02 -9.500e+02 -9.640e+02 -9.720e+02  9.800e+02\n",
      "  9.850e+02  9.930e+02 -9.930e+02 -9.950e+02 -9.970e+02 -1.000e+03\n",
      " -1.021e+03 -1.030e+03 -1.036e+03 -1.038e+03 -1.050e+03 -1.075e+03\n",
      " -1.077e+03  1.079e+03  1.081e+03  1.090e+03  1.093e+03 -1.095e+03\n",
      " -1.120e+03 -1.125e+03 -1.131e+03  1.133e+03 -1.138e+03 -1.143e+03\n",
      " -1.147e+03 -1.168e+03 -1.172e+03 -1.179e+03 -1.183e+03 -1.190e+03\n",
      "  1.202e+03 -1.218e+03 -1.224e+03 -1.245e+03 -1.253e+03 -1.265e+03\n",
      " -1.273e+03 -1.274e+03 -1.278e+03 -1.288e+03  1.289e+03 -1.309e+03\n",
      " -1.311e+03 -1.315e+03 -1.346e+03 -1.353e+03 -1.368e+03  1.394e+03\n",
      "  1.398e+03 -1.399e+03 -1.409e+03 -1.409e+03 -1.413e+03 -1.425e+03\n",
      " -1.428e+03  1.430e+03 -1.430e+03 -1.443e+03  1.459e+03 -1.460e+03\n",
      " -1.461e+03 -1.466e+03  1.466e+03 -1.472e+03 -1.483e+03 -1.506e+03\n",
      " -1.508e+03 -1.521e+03 -1.527e+03 -1.555e+03 -1.559e+03 -1.584e+03\n",
      " -1.584e+03 -1.586e+03 -1.628e+03 -1.663e+03  1.671e+03 -1.686e+03\n",
      " -1.690e+03 -1.699e+03  1.718e+03 -1.721e+03 -1.722e+03 -1.724e+03\n",
      "  1.732e+03 -1.737e+03  1.762e+03 -1.777e+03 -1.778e+03  1.838e+03\n",
      " -1.840e+03 -1.971e+03  1.972e+03 -1.995e+03  2.002e+03 -2.016e+03\n",
      " -2.016e+03  2.064e+03  2.083e+03  2.120e+03 -2.133e+03 -2.161e+03\n",
      "  2.166e+03 -2.169e+03 -2.182e+03  2.319e+03 -2.327e+03 -2.347e+03\n",
      " -2.437e+03 -2.562e+03 -2.641e+03  2.703e+03  2.717e+03 -2.727e+03\n",
      " -2.886e+03  2.900e+03 -3.270e+03  3.314e+03 -3.930e+03 -3.981e+03\n",
      " -4.241e+03 -4.282e+03  4.680e+03  4.760e+03  4.856e+03 -5.252e+03\n",
      " -5.480e+03  6.417e+03]\n",
      "durations 14.0 5152.0\n",
      "Concordance Index 0.5849673202614379\n",
      "Integrated Brier Score: 0.19062248911329635\n",
      "split gex_?|100130426      float32\n",
      "gex_?|100133144      float32\n",
      "gex_?|100134869      float32\n",
      "gex_?|10357          float32\n",
      "gex_?|10431          float32\n",
      "                      ...   \n",
      "gex_ZYG11A|440590    float32\n",
      "gex_ZYG11B|79699     float32\n",
      "gex_ZYX|7791         float32\n",
      "gex_ZZEF1|23140      float32\n",
      "gex_ZZZ3|26009       float32\n",
      "Length: 20531, dtype: object\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m72.0179\u001b[0m  0.0044\n",
      "      2       72.0179  0.0116\n",
      "      3       72.0179  0.0054\n",
      "      4       72.0179  0.0033\n",
      "      5       72.0179  0.0026\n",
      "      6       72.0179  0.0021\n",
      "      7       72.0179  0.0020\n",
      "      8       72.0179  0.0035\n",
      "      9       72.0179  0.0036\n",
      "     10       72.0179  0.0038\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m96.3845\u001b[0m  0.0063\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m84.5898\u001b[0m  0.0035\n",
      "      2       96.3845  0.0023\n",
      "      2       84.5898  0.0025\n",
      "      3       96.3845  0.0021\n",
      "      3       84.5898  0.0022\n",
      "      4       96.3845  0.0020\n",
      "      4       84.5898  0.0021\n",
      "      5       96.3845  0.0020\n",
      "      5       84.5898  0.0020\n",
      "      6       96.3845  0.0030\n",
      "      6       84.5898  0.0045\n",
      "      7       96.3845  0.0024\n",
      "      7       84.5898  0.0022\n",
      "      8       96.3845  0.0021\n",
      "      8       84.5898  0.0021\n",
      "      9       96.3845  0.0024\n",
      "      9       84.5898  0.0025\n",
      "     10       96.3845  0.0027\n",
      "Restoring best model from epoch 1.\n",
      "     10       84.5898  0.0028\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m114.6297\u001b[0m       \u001b[32m87.7460\u001b[0m  0.1440\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m60.9829\u001b[0m  0.0043\n",
      "      2       60.9829  0.0035\n",
      "      3       60.9829  0.0022\n",
      "      4       60.9829  0.0020\n",
      "      5       60.9829  0.0026\n",
      "      6       60.9829  0.0055\n",
      "      7       60.9829  0.0027\n",
      "      8       60.9829  0.0032\n",
      "      9       60.9829  0.0027\n",
      "     10       60.9829  0.0021\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2       \u001b[36m82.1859\u001b[0m      103.8665  0.0879\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      3       \u001b[36m66.7662\u001b[0m       \u001b[32m85.6008\u001b[0m  0.0864\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m95.4518\u001b[0m  0.0031\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m157.3189\u001b[0m           nan  0.1061\n",
      "      2       95.4518  0.0026\n",
      "      3       95.4518  0.0028\n",
      "      4       95.4518  0.0026\n",
      "      5       95.4518  0.0024\n",
      "      6       95.4518  0.0022\n",
      "      7       95.4518  0.0022\n",
      "      8       95.4518  0.0021\n",
      "      9       95.4518  0.0021\n",
      "     10       95.4518  0.0021\n",
      "Restoring best model from epoch 1.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m145.5859\u001b[0m      \u001b[32m310.5548\u001b[0m  0.1155\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m132.0601\u001b[0m      \u001b[32m255.8503\u001b[0m  0.1098\n",
      "      4       \u001b[36m63.6246\u001b[0m       \u001b[32m54.6164\u001b[0m  0.0840\n",
      "      2      \u001b[36m105.6317\u001b[0m      \u001b[32m253.2969\u001b[0m  0.0831\n",
      "      2       \u001b[36m92.8435\u001b[0m      383.3716  0.0761\n",
      "      2       \u001b[36m91.0272\u001b[0m      \u001b[32m130.4572\u001b[0m  0.0980\n",
      "      5       \u001b[36m62.8966\u001b[0m       \u001b[32m42.6407\u001b[0m  0.0759\n",
      "      3       \u001b[36m80.4085\u001b[0m      \u001b[32m138.9944\u001b[0m  0.0779\n",
      "      3       \u001b[36m73.3167\u001b[0m      \u001b[32m228.7507\u001b[0m  0.0760\n",
      "      3       \u001b[36m68.4971\u001b[0m      132.9191  0.0822\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      6       \u001b[36m59.0770\u001b[0m       \u001b[32m33.8821\u001b[0m  0.0756\n",
      "      4       \u001b[36m75.8006\u001b[0m       \u001b[32m90.3700\u001b[0m  0.0783\n",
      "      4       75.4162       \u001b[32m93.1698\u001b[0m  0.0815\n",
      "      4       \u001b[36m67.7122\u001b[0m       \u001b[32m85.8134\u001b[0m  0.0798\n",
      "      7       \u001b[36m58.4075\u001b[0m       35.6035  0.0841\n",
      "      5       \u001b[36m75.6273\u001b[0m       \u001b[32m74.5683\u001b[0m  0.0852\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m127.1663\u001b[0m      \u001b[32m227.4519\u001b[0m  0.1144\n",
      "      5       \u001b[36m67.0094\u001b[0m      101.9129  0.0827\n",
      "      5       \u001b[36m66.0471\u001b[0m       \u001b[32m73.6335\u001b[0m  0.0777\n",
      "      8       \u001b[36m55.5074\u001b[0m       34.9808  0.0730\n",
      "      6       76.0516       \u001b[32m70.4288\u001b[0m  0.0759\n",
      "      2       \u001b[36m76.5604\u001b[0m      \u001b[32m158.3798\u001b[0m  0.0789\n",
      "      6       \u001b[36m66.7723\u001b[0m       \u001b[32m59.8107\u001b[0m  0.0747\n",
      "      6       \u001b[36m62.5463\u001b[0m       \u001b[32m63.5250\u001b[0m  0.0767\n",
      "      9       \u001b[36m55.2728\u001b[0m       \u001b[32m33.2142\u001b[0m  0.0742\n",
      "      7       \u001b[36m72.1944\u001b[0m       70.9923  0.0759\n",
      "      7       \u001b[36m65.0929\u001b[0m       \u001b[32m59.3417\u001b[0m  0.0769\n",
      "      3       \u001b[36m66.0472\u001b[0m       \u001b[32m90.5928\u001b[0m  0.0765\n",
      "      7       63.1848       \u001b[32m61.1270\u001b[0m  0.0739\n",
      "     10       \u001b[36m53.1006\u001b[0m       35.1819  0.0779\n",
      "Restoring best model from epoch 9.\n",
      "      8       72.5163       72.7662  0.0752\n",
      "      8       \u001b[36m64.8088\u001b[0m       \u001b[32m52.6301\u001b[0m  0.0773\n",
      "      4       \u001b[36m60.6524\u001b[0m       \u001b[32m53.5976\u001b[0m  0.0782\n",
      "      8       \u001b[36m61.3867\u001b[0m       61.5238  0.0801\n",
      "      9       \u001b[36m71.2878\u001b[0m       71.5730  0.0728\n",
      "      9       \u001b[36m64.2949\u001b[0m       \u001b[32m51.9941\u001b[0m  0.0757\n",
      "      5       \u001b[36m58.7852\u001b[0m       55.5172  0.0769\n",
      "      9       61.6561       62.4344  0.0719\n",
      "     10       \u001b[36m68.6195\u001b[0m       \u001b[32m69.5090\u001b[0m  0.0733\n",
      "      6       \u001b[36m56.3450\u001b[0m       56.5262  0.0730\n",
      "     10       \u001b[36m64.2234\u001b[0m       53.6338  0.0799\n",
      "Restoring best model from epoch 9.\n",
      "     10       \u001b[36m60.9089\u001b[0m       63.8625  0.0715\n",
      "Restoring best model from epoch 7.\n",
      "      7       \u001b[36m53.5094\u001b[0m       54.4469  0.0713\n",
      "      8       53.6608       54.3271  0.0734\n",
      "      9       \u001b[36m52.7043\u001b[0m       \u001b[32m50.7040\u001b[0m  0.0700\n",
      "     10       \u001b[36m50.6530\u001b[0m       \u001b[32m46.3963\u001b[0m  0.0701\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m100.6625\u001b[0m      \u001b[32m184.1879\u001b[0m  0.0753\n",
      "      2       \u001b[36m80.9653\u001b[0m      184.6409  0.0629\n",
      "      3       \u001b[36m73.2549\u001b[0m      \u001b[32m116.8289\u001b[0m  0.0724\n",
      "      4       \u001b[36m69.7231\u001b[0m       \u001b[32m70.1107\u001b[0m  0.0672\n",
      "      5       \u001b[36m63.7527\u001b[0m       \u001b[32m62.1612\u001b[0m  0.0743\n",
      "      6       \u001b[36m60.1246\u001b[0m       \u001b[32m57.8802\u001b[0m  0.0740\n",
      "      7       \u001b[36m58.3559\u001b[0m       58.5285  0.0632\n",
      "      8       \u001b[36m57.2234\u001b[0m       59.4974  0.0590\n",
      "      9       \u001b[36m55.4995\u001b[0m       59.9787  0.0627\n",
      "     10       \u001b[36m55.2760\u001b[0m       59.2745  0.0713\n",
      "Restoring best model from epoch 6.\n",
      "y_train breslow final [ 2.000e+00 -3.000e+00 -7.000e+00 -1.100e+01 -1.300e+01 -1.600e+01\n",
      " -1.600e+01 -1.800e+01  1.800e+01 -2.600e+01 -2.900e+01 -3.800e+01\n",
      "  4.100e+01  4.200e+01  4.300e+01 -4.300e+01  5.100e+01 -5.300e+01\n",
      "  5.900e+01 -6.100e+01  6.500e+01  6.800e+01  6.900e+01  7.300e+01\n",
      "  7.700e+01  9.300e+01  1.060e+02  1.090e+02 -1.270e+02  1.370e+02\n",
      "  1.390e+02  1.390e+02 -1.410e+02 -1.500e+02 -1.550e+02  1.620e+02\n",
      " -1.660e+02  1.660e+02  1.680e+02 -1.750e+02 -1.770e+02  1.820e+02\n",
      " -1.930e+02 -1.940e+02  2.040e+02 -2.050e+02  2.240e+02  2.380e+02\n",
      "  2.420e+02  2.450e+02 -2.560e+02 -2.930e+02  3.070e+02  3.110e+02\n",
      "  3.130e+02  3.130e+02 -3.190e+02  3.200e+02 -3.230e+02  3.290e+02\n",
      "  3.330e+02  3.340e+02  3.360e+02  3.420e+02  3.440e+02 -3.540e+02\n",
      " -3.560e+02 -3.620e+02 -3.650e+02 -3.650e+02 -3.690e+02 -3.700e+02\n",
      " -3.720e+02 -3.730e+02 -3.740e+02  3.750e+02 -3.850e+02 -3.860e+02\n",
      " -4.000e+02 -4.060e+02 -4.080e+02 -4.110e+02  4.310e+02 -4.310e+02\n",
      " -4.330e+02  4.450e+02  4.460e+02  4.540e+02 -4.540e+02  4.590e+02\n",
      " -4.690e+02  4.780e+02  4.800e+02  4.800e+02  4.850e+02 -4.950e+02\n",
      " -4.980e+02 -5.010e+02 -5.050e+02 -5.070e+02 -5.110e+02 -5.190e+02\n",
      " -5.280e+02 -5.510e+02  5.520e+02  5.610e+02  5.630e+02 -5.630e+02\n",
      " -5.670e+02 -5.740e+02  5.740e+02  5.780e+02  5.870e+02 -5.910e+02\n",
      " -6.030e+02 -6.070e+02 -6.170e+02 -6.300e+02  6.370e+02  6.450e+02\n",
      " -6.480e+02 -6.560e+02 -6.620e+02 -6.650e+02  6.790e+02  6.830e+02\n",
      " -6.850e+02 -6.890e+02 -6.930e+02  7.010e+02  7.090e+02 -7.140e+02\n",
      "  7.220e+02 -7.220e+02  7.350e+02 -7.350e+02 -7.370e+02 -7.490e+02\n",
      " -7.550e+02 -7.620e+02  7.680e+02 -7.850e+02  7.930e+02  8.190e+02\n",
      "  8.220e+02  8.280e+02 -8.300e+02  8.340e+02 -8.400e+02  8.410e+02\n",
      "  8.450e+02 -8.610e+02  8.660e+02 -8.730e+02 -8.740e+02  8.780e+02\n",
      "  8.830e+02  8.850e+02  8.850e+02 -9.090e+02 -9.100e+02 -9.190e+02\n",
      "  9.270e+02  9.320e+02 -9.320e+02 -9.450e+02  9.460e+02 -9.510e+02\n",
      "  9.520e+02 -9.520e+02 -9.670e+02 -9.920e+02  9.920e+02  1.003e+03\n",
      " -1.014e+03 -1.018e+03 -1.018e+03  1.019e+03  1.034e+03  1.045e+03\n",
      " -1.046e+03 -1.063e+03 -1.071e+03  1.075e+03  1.091e+03 -1.092e+03\n",
      "  1.092e+03  1.097e+03 -1.107e+03  1.111e+03 -1.120e+03 -1.124e+03\n",
      " -1.124e+03 -1.126e+03 -1.130e+03 -1.133e+03  1.133e+03 -1.133e+03\n",
      " -1.137e+03 -1.140e+03 -1.143e+03 -1.168e+03 -1.169e+03  1.170e+03\n",
      " -1.175e+03 -1.186e+03  1.191e+03  1.200e+03  1.200e+03 -1.217e+03\n",
      " -1.218e+03 -1.238e+03  1.238e+03 -1.266e+03 -1.274e+03 -1.290e+03\n",
      " -1.314e+03  1.317e+03 -1.329e+03  1.337e+03  1.343e+03 -1.367e+03\n",
      " -1.371e+03  1.371e+03  1.378e+03 -1.380e+03 -1.385e+03 -1.398e+03\n",
      "  1.404e+03 -1.411e+03 -1.413e+03 -1.413e+03 -1.416e+03  1.417e+03\n",
      " -1.423e+03  1.432e+03 -1.433e+03 -1.435e+03 -1.436e+03 -1.450e+03\n",
      " -1.462e+03  1.463e+03 -1.471e+03 -1.476e+03 -1.478e+03 -1.485e+03\n",
      " -1.487e+03 -1.493e+03 -1.496e+03 -1.498e+03 -1.499e+03 -1.502e+03\n",
      " -1.508e+03 -1.516e+03 -1.520e+03 -1.520e+03 -1.521e+03 -1.525e+03\n",
      " -1.528e+03 -1.531e+03 -1.552e+03 -1.560e+03  1.584e+03  1.588e+03\n",
      "  1.589e+03  1.598e+03 -1.608e+03  1.610e+03  1.620e+03 -1.621e+03\n",
      "  1.625e+03  1.625e+03  1.626e+03  1.639e+03  1.657e+03 -1.657e+03\n",
      "  1.661e+03 -1.683e+03  1.696e+03  1.714e+03  1.724e+03 -1.729e+03\n",
      " -1.733e+03 -1.746e+03 -1.755e+03 -1.778e+03 -1.782e+03 -1.785e+03\n",
      " -1.790e+03 -1.794e+03 -1.843e+03 -1.862e+03 -1.871e+03 -1.876e+03\n",
      " -1.879e+03 -1.883e+03 -1.885e+03 -1.886e+03 -1.888e+03 -1.889e+03\n",
      " -1.893e+03 -1.905e+03 -1.906e+03  1.912e+03  1.913e+03 -1.924e+03\n",
      " -1.928e+03 -1.946e+03 -1.955e+03  1.964e+03  1.980e+03  1.986e+03\n",
      " -1.993e+03 -1.997e+03 -2.004e+03 -2.009e+03 -2.014e+03 -2.016e+03\n",
      " -2.017e+03 -2.038e+03 -2.067e+03 -2.080e+03 -2.087e+03  2.090e+03\n",
      " -2.128e+03 -2.133e+03  2.145e+03 -2.150e+03 -2.172e+03 -2.172e+03\n",
      " -2.184e+03 -2.186e+03  2.190e+03 -2.208e+03 -2.217e+03 -2.226e+03\n",
      "  2.227e+03 -2.231e+03  2.241e+03 -2.246e+03  2.256e+03 -2.257e+03\n",
      " -2.263e+03 -2.271e+03 -2.274e+03 -2.283e+03  2.299e+03  2.343e+03\n",
      " -2.372e+03 -2.378e+03  2.386e+03 -2.392e+03 -2.412e+03  2.419e+03\n",
      " -2.422e+03 -2.423e+03 -2.430e+03 -2.439e+03  2.454e+03 -2.470e+03\n",
      " -2.489e+03 -2.504e+03 -2.508e+03 -2.531e+03 -2.552e+03 -2.554e+03\n",
      "  2.564e+03 -2.609e+03 -2.609e+03 -2.630e+03 -2.660e+03 -2.688e+03\n",
      " -2.718e+03 -2.722e+03 -2.746e+03  2.752e+03 -2.754e+03  2.764e+03\n",
      " -2.782e+03 -2.789e+03 -2.799e+03 -2.839e+03 -2.859e+03 -2.865e+03\n",
      " -2.868e+03 -2.873e+03 -2.881e+03 -2.964e+03 -3.074e+03 -3.146e+03\n",
      " -3.205e+03 -3.205e+03 -3.229e+03 -3.267e+03 -3.302e+03 -3.328e+03\n",
      " -3.331e+03 -3.341e+03 -3.343e+03 -3.377e+03 -3.392e+03 -3.409e+03\n",
      " -3.451e+03 -3.480e+03 -3.498e+03 -3.519e+03  3.554e+03 -3.583e+03\n",
      "  3.615e+03 -3.631e+03 -3.639e+03 -3.728e+03 -3.736e+03 -3.744e+03\n",
      " -3.834e+03 -3.841e+03 -3.936e+03 -3.944e+03 -3.974e+03 -3.987e+03\n",
      " -3.989e+03 -4.067e+03 -4.074e+03 -4.537e+03]\n",
      "Concordance Index 0.721002531088368\n",
      "Integrated Brier Score: 0.15771944821115164\n",
      "y_train breslow final [ 2.000e+00 -3.000e+00 -7.000e+00 -1.100e+01 -1.300e+01 -1.600e+01\n",
      " -1.600e+01 -1.800e+01  1.800e+01 -2.600e+01 -2.900e+01 -3.800e+01\n",
      "  4.100e+01  4.200e+01  4.300e+01 -4.300e+01  5.100e+01 -5.300e+01\n",
      "  5.900e+01 -6.100e+01  6.500e+01  6.800e+01  6.900e+01  7.300e+01\n",
      "  7.700e+01  9.300e+01  1.060e+02  1.090e+02 -1.270e+02  1.370e+02\n",
      "  1.390e+02  1.390e+02 -1.410e+02 -1.500e+02 -1.550e+02  1.620e+02\n",
      " -1.660e+02  1.660e+02  1.680e+02 -1.750e+02 -1.770e+02  1.820e+02\n",
      " -1.930e+02 -1.940e+02  2.040e+02 -2.050e+02  2.240e+02  2.380e+02\n",
      "  2.420e+02  2.450e+02 -2.560e+02 -2.930e+02  3.070e+02  3.110e+02\n",
      "  3.130e+02  3.130e+02 -3.190e+02  3.200e+02 -3.230e+02  3.290e+02\n",
      "  3.330e+02  3.340e+02  3.360e+02  3.420e+02  3.440e+02 -3.540e+02\n",
      " -3.560e+02 -3.620e+02 -3.650e+02 -3.650e+02 -3.690e+02 -3.700e+02\n",
      " -3.720e+02 -3.730e+02 -3.740e+02  3.750e+02 -3.850e+02 -3.860e+02\n",
      " -4.000e+02 -4.060e+02 -4.080e+02 -4.110e+02  4.310e+02 -4.310e+02\n",
      " -4.330e+02  4.450e+02  4.460e+02  4.540e+02 -4.540e+02  4.590e+02\n",
      " -4.690e+02  4.780e+02  4.800e+02  4.800e+02  4.850e+02 -4.950e+02\n",
      " -4.980e+02 -5.010e+02 -5.050e+02 -5.070e+02 -5.110e+02 -5.190e+02\n",
      " -5.280e+02 -5.510e+02  5.520e+02  5.610e+02  5.630e+02 -5.630e+02\n",
      " -5.670e+02 -5.740e+02  5.740e+02  5.780e+02  5.870e+02 -5.910e+02\n",
      " -6.030e+02 -6.070e+02 -6.170e+02 -6.300e+02  6.370e+02  6.450e+02\n",
      " -6.480e+02 -6.560e+02 -6.620e+02 -6.650e+02  6.790e+02  6.830e+02\n",
      " -6.850e+02 -6.890e+02 -6.930e+02  7.010e+02  7.090e+02 -7.140e+02\n",
      "  7.220e+02 -7.220e+02  7.350e+02 -7.350e+02 -7.370e+02 -7.490e+02\n",
      " -7.550e+02 -7.620e+02  7.680e+02 -7.850e+02  7.930e+02  8.190e+02\n",
      "  8.220e+02  8.280e+02 -8.300e+02  8.340e+02 -8.400e+02  8.410e+02\n",
      "  8.450e+02 -8.610e+02  8.660e+02 -8.730e+02 -8.740e+02  8.780e+02\n",
      "  8.830e+02  8.850e+02  8.850e+02 -9.090e+02 -9.100e+02 -9.190e+02\n",
      "  9.270e+02  9.320e+02 -9.320e+02 -9.450e+02  9.460e+02 -9.510e+02\n",
      "  9.520e+02 -9.520e+02 -9.670e+02 -9.920e+02  9.920e+02  1.003e+03\n",
      " -1.014e+03 -1.018e+03 -1.018e+03  1.019e+03  1.034e+03  1.045e+03\n",
      " -1.046e+03 -1.063e+03 -1.071e+03  1.075e+03  1.091e+03 -1.092e+03\n",
      "  1.092e+03  1.097e+03 -1.107e+03  1.111e+03 -1.120e+03 -1.124e+03\n",
      " -1.124e+03 -1.126e+03 -1.130e+03 -1.133e+03  1.133e+03 -1.133e+03\n",
      " -1.137e+03 -1.140e+03 -1.143e+03 -1.168e+03 -1.169e+03  1.170e+03\n",
      " -1.175e+03 -1.186e+03  1.191e+03  1.200e+03  1.200e+03 -1.217e+03\n",
      " -1.218e+03 -1.238e+03  1.238e+03 -1.266e+03 -1.274e+03 -1.290e+03\n",
      " -1.314e+03  1.317e+03 -1.329e+03  1.337e+03  1.343e+03 -1.367e+03\n",
      " -1.371e+03  1.371e+03  1.378e+03 -1.380e+03 -1.385e+03 -1.398e+03\n",
      "  1.404e+03 -1.411e+03 -1.413e+03 -1.413e+03 -1.416e+03  1.417e+03\n",
      " -1.423e+03  1.432e+03 -1.433e+03 -1.435e+03 -1.436e+03 -1.450e+03\n",
      " -1.462e+03  1.463e+03 -1.471e+03 -1.476e+03 -1.478e+03 -1.485e+03\n",
      " -1.487e+03 -1.493e+03 -1.496e+03 -1.498e+03 -1.499e+03 -1.502e+03\n",
      " -1.508e+03 -1.516e+03 -1.520e+03 -1.520e+03 -1.521e+03 -1.525e+03\n",
      " -1.528e+03 -1.531e+03 -1.552e+03 -1.560e+03  1.584e+03  1.588e+03\n",
      "  1.589e+03  1.598e+03 -1.608e+03  1.610e+03  1.620e+03 -1.621e+03\n",
      "  1.625e+03  1.625e+03  1.626e+03  1.639e+03  1.657e+03 -1.657e+03\n",
      "  1.661e+03 -1.683e+03  1.696e+03  1.714e+03  1.724e+03 -1.729e+03\n",
      " -1.733e+03 -1.746e+03 -1.755e+03 -1.778e+03 -1.782e+03 -1.785e+03\n",
      " -1.790e+03 -1.794e+03 -1.843e+03 -1.862e+03 -1.871e+03 -1.876e+03\n",
      " -1.879e+03 -1.883e+03 -1.885e+03 -1.886e+03 -1.888e+03 -1.889e+03\n",
      " -1.893e+03 -1.905e+03 -1.906e+03  1.912e+03  1.913e+03 -1.924e+03\n",
      " -1.928e+03 -1.946e+03 -1.955e+03  1.964e+03  1.980e+03  1.986e+03\n",
      " -1.993e+03 -1.997e+03 -2.004e+03 -2.009e+03 -2.014e+03 -2.016e+03\n",
      " -2.017e+03 -2.038e+03 -2.067e+03 -2.080e+03 -2.087e+03  2.090e+03\n",
      " -2.128e+03 -2.133e+03  2.145e+03 -2.150e+03 -2.172e+03 -2.172e+03\n",
      " -2.184e+03 -2.186e+03  2.190e+03 -2.208e+03 -2.217e+03 -2.226e+03\n",
      "  2.227e+03 -2.231e+03  2.241e+03 -2.246e+03  2.256e+03 -2.257e+03\n",
      " -2.263e+03 -2.271e+03 -2.274e+03 -2.283e+03  2.299e+03  2.343e+03\n",
      " -2.372e+03 -2.378e+03  2.386e+03 -2.392e+03 -2.412e+03  2.419e+03\n",
      " -2.422e+03 -2.423e+03 -2.430e+03 -2.439e+03  2.454e+03 -2.470e+03\n",
      " -2.489e+03 -2.504e+03 -2.508e+03 -2.531e+03 -2.552e+03 -2.554e+03\n",
      "  2.564e+03 -2.609e+03 -2.609e+03 -2.630e+03 -2.660e+03 -2.688e+03\n",
      " -2.718e+03 -2.722e+03 -2.746e+03  2.752e+03 -2.754e+03  2.764e+03\n",
      " -2.782e+03 -2.789e+03 -2.799e+03 -2.839e+03 -2.859e+03 -2.865e+03\n",
      " -2.868e+03 -2.873e+03 -2.881e+03 -2.964e+03 -3.074e+03 -3.146e+03\n",
      " -3.205e+03 -3.205e+03 -3.229e+03 -3.267e+03 -3.302e+03 -3.328e+03\n",
      " -3.331e+03 -3.341e+03 -3.343e+03 -3.377e+03 -3.392e+03 -3.409e+03\n",
      " -3.451e+03 -3.480e+03 -3.498e+03 -3.519e+03  3.554e+03 -3.583e+03\n",
      "  3.615e+03 -3.631e+03 -3.639e+03 -3.728e+03 -3.736e+03 -3.744e+03\n",
      " -3.834e+03 -3.841e+03 -3.936e+03 -3.944e+03 -3.974e+03 -3.987e+03\n",
      " -3.989e+03 -4.067e+03 -4.074e+03 -4.537e+03]\n",
      "durations 3.0 3431.0\n",
      "Concordance Index 0.6732334047109207\n",
      "Integrated Brier Score: 0.17355095157170536\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m160.6686\u001b[0m       \u001b[32m43.0601\u001b[0m  0.0280\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m212.2574\u001b[0m  0.0054\n",
      "      2      212.2574  0.0023\n",
      "      3      212.2574  0.0019\n",
      "      4      212.2574  0.0018\n",
      "      2      167.3148       \u001b[32m42.3773\u001b[0m  0.0255\n",
      "      5      212.2574  0.0020\n",
      "      6      212.2574  0.0018\n",
      "      7      212.2574  0.0018\n",
      "      8      212.2574  0.0017\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      9      212.2574  0.0017\n",
      "     10      212.2574  0.0017\n",
      "Restoring best model from epoch 1.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m384.1610\u001b[0m  0.0031\n",
      "      2      384.1610  0.0025\n",
      "      3      384.1610  0.0020\n",
      "      4      384.1610  0.0020\n",
      "      5      384.1610  0.0020\n",
      "      6      384.1610  0.0025\n",
      "      7      384.1610  0.0018\n",
      "      8      384.1610  0.0018\n",
      "      3      \u001b[36m160.3455\u001b[0m       \u001b[32m41.3900\u001b[0m  0.0280\n",
      "      9      384.1610  0.0018\n",
      "     10      384.1610  0.0019\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      4      \u001b[36m159.2169\u001b[0m       \u001b[32m40.5992\u001b[0m  0.0274\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "Re-initializing optimizer.\n",
      "      1      \u001b[36m320.2158\u001b[0m  0.0034\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m220.0913\u001b[0m  0.0038\n",
      "      2      320.2158  0.0025\n",
      "      2      220.0913  0.0021\n",
      "      3      220.0913  0.0018\n",
      "      4      220.0913  0.0017\n",
      "      5      220.0913  0.0017\n",
      "      5      162.6241       40.8157  0.0257\n",
      "      3      320.2158  0.0093\n",
      "      6      220.0913  0.0028\n",
      "      4      320.2158  0.0023\n",
      "      7      220.0913  0.0019\n",
      "      5      320.2158  0.0018\n",
      "      8      220.0913  0.0017\n",
      "      9      220.0913  0.0017\n",
      "      6      320.2158  0.0022\n",
      "     10      220.0913  0.0026\n",
      "      7      320.2158  0.0021\n",
      "Restoring best model from epoch 1.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m181.3175\u001b[0m       \u001b[32m78.8210\u001b[0m  0.0482\n",
      "      8      320.2158  0.0019\n",
      "      9      320.2158  0.0018\n",
      "     10      320.2158  0.0037\n",
      "Restoring best model from epoch 1.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m178.9634\u001b[0m       \u001b[32m92.0731\u001b[0m  0.0266\n",
      "      6      \u001b[36m153.4346\u001b[0m       41.1602  0.0297\n",
      "      2      \u001b[36m178.9935\u001b[0m       \u001b[32m77.8472\u001b[0m  0.0264\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      7      161.5742       41.2431  0.0254\n",
      "      2      180.6262       \u001b[32m90.5160\u001b[0m  0.0353\n",
      "      3      \u001b[36m178.8505\u001b[0m       \u001b[32m77.5558\u001b[0m  0.0230\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m152.9319\u001b[0m  0.0068\n",
      "      2      152.9319  0.0021\n",
      "      8      155.3641       41.6728  0.0239\n",
      "      3      152.9319  0.0017\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m160.5830\u001b[0m       \u001b[32m38.1696\u001b[0m  0.0259\n",
      "      3      182.5688       \u001b[32m88.6683\u001b[0m  0.0244\n",
      "      4      152.9319  0.0017\n",
      "      5      152.9319  0.0017\n",
      "      6      152.9319  0.0024\n",
      "      7      152.9319  0.0027\n",
      "      4      185.1888       77.5936  0.0297\n",
      "      8      152.9319  0.0128\n",
      "      9      154.2137       42.1421  0.0236\n",
      "      4      184.7612       \u001b[32m87.7043\u001b[0m  0.0289\n",
      "      2      \u001b[36m155.5927\u001b[0m       \u001b[32m38.0034\u001b[0m  0.0314\n",
      "      9      152.9319  0.0112\n",
      "     10      152.9319  0.0049\n",
      "      5      182.3396       \u001b[32m77.1433\u001b[0m  0.0257\n",
      "Restoring best model from epoch 1.\n",
      "     10      153.5482       42.3481  0.0270\n",
      "Restoring best model from epoch 4.\n",
      "      5      180.6363       \u001b[32m87.5540\u001b[0m  0.0322\n",
      "      6      181.8020       \u001b[32m77.1411\u001b[0m  0.0305\n",
      "      3      157.2029       \u001b[32m37.8380\u001b[0m  0.0360\n",
      "      6      184.5859       87.6120  0.0284\n",
      "      4      \u001b[36m152.9551\u001b[0m       \u001b[32m37.7864\u001b[0m  0.0256\n",
      "      7      179.8960       77.2266  0.0302\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      7      \u001b[36m173.9529\u001b[0m       87.6720  0.0244\n",
      "      5      153.6299       37.8970  0.0253\n",
      "      8      \u001b[36m177.4531\u001b[0m       77.3968  0.0301\n",
      "      8      177.4308       88.1397  0.0237\n",
      "      6      158.8606       37.9277  0.0233\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m183.0434\u001b[0m       \u001b[32m62.3447\u001b[0m  0.0308\n",
      "      9      179.3122       77.4263  0.0238\n",
      "      9      177.9333       88.3041  0.0234\n",
      "      7      157.3870       37.9102  0.0239\n",
      "      2      183.1980       \u001b[32m62.2944\u001b[0m  0.0308\n",
      "     10      180.4053       \u001b[32m76.9636\u001b[0m  0.0256\n",
      "     10      179.0593       87.9206  0.0241\n",
      "Restoring best model from epoch 5.\n",
      "      8      159.1377       37.9439  0.0237\n",
      "      3      \u001b[36m181.3510\u001b[0m       \u001b[32m61.3295\u001b[0m  0.0242\n",
      "      9      154.1648       37.9386  0.0225\n",
      "      4      192.9418       \u001b[32m60.5278\u001b[0m  0.0265\n",
      "     10      154.4365       38.0563  0.0231\n",
      "Restoring best model from epoch 4.\n",
      "      5      \u001b[36m177.7128\u001b[0m       \u001b[32m59.8669\u001b[0m  0.0257\n",
      "      6      180.7405       \u001b[32m59.7657\u001b[0m  0.0210\n",
      "      7      \u001b[36m174.9145\u001b[0m       \u001b[32m59.7456\u001b[0m  0.0208\n",
      "      8      175.0228       59.8753  0.0209\n",
      "      9      176.0550       59.7817  0.0207\n",
      "     10      176.7206       \u001b[32m59.5095\u001b[0m  0.0204\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m163.5238\u001b[0m      \u001b[32m108.9401\u001b[0m  0.0287\n",
      "      2      163.7306      \u001b[32m108.6543\u001b[0m  0.0247\n",
      "      3      164.1769      \u001b[32m107.8734\u001b[0m  0.0252\n",
      "      4      168.6006      \u001b[32m106.5635\u001b[0m  0.0268\n",
      "      5      \u001b[36m162.4625\u001b[0m      \u001b[32m106.2905\u001b[0m  0.0303\n",
      "      6      163.8085      106.3926  0.0224\n",
      "      7      \u001b[36m162.3896\u001b[0m      106.5593  0.0256\n",
      "      8      \u001b[36m159.4818\u001b[0m      106.5804  0.0225\n",
      "      9      160.3196      106.7265  0.0227\n",
      "     10      \u001b[36m158.2660\u001b[0m      106.5842  0.0231\n",
      "Restoring best model from epoch 5.\n",
      "y_train breslow final [ 2.000e+00 -3.000e+00 -1.300e+01 -1.600e+01 -1.600e+01 -1.600e+01\n",
      "  1.800e+01 -2.600e+01 -2.900e+01 -3.600e+01  4.100e+01  4.200e+01\n",
      "  4.300e+01 -4.300e+01  5.100e+01 -5.300e+01  6.200e+01  6.500e+01\n",
      "  6.900e+01  9.900e+01  1.010e+02  1.060e+02  1.100e+02 -1.180e+02\n",
      " -1.190e+02 -1.270e+02  1.370e+02  1.390e+02  1.390e+02 -1.410e+02\n",
      " -1.500e+02 -1.550e+02  1.620e+02  1.640e+02 -1.820e+02  1.820e+02\n",
      "  1.830e+02 -1.930e+02  2.020e+02  2.040e+02 -2.040e+02 -2.050e+02\n",
      "  2.060e+02 -2.060e+02  2.110e+02 -2.110e+02  2.220e+02  2.380e+02\n",
      "  2.420e+02  2.450e+02 -2.560e+02 -2.930e+02  3.070e+02  3.130e+02\n",
      "  3.130e+02 -3.230e+02  3.290e+02  3.300e+02  3.330e+02  3.340e+02\n",
      "  3.360e+02 -3.400e+02  3.420e+02  3.440e+02 -3.540e+02 -3.550e+02\n",
      "  3.620e+02 -3.620e+02 -3.650e+02 -3.650e+02 -3.690e+02 -3.700e+02\n",
      " -3.730e+02 -3.740e+02  3.750e+02 -3.860e+02 -4.000e+02 -4.060e+02\n",
      " -4.110e+02  4.310e+02 -4.330e+02 -4.350e+02  4.450e+02 -4.540e+02\n",
      "  4.590e+02 -4.680e+02  4.750e+02  4.780e+02  4.800e+02  4.850e+02\n",
      " -4.950e+02 -5.010e+02 -5.050e+02 -5.070e+02  5.100e+02 -5.190e+02\n",
      " -5.230e+02 -5.510e+02  5.520e+02  5.610e+02  5.610e+02  5.620e+02\n",
      "  5.630e+02 -5.630e+02 -5.670e+02  5.710e+02  5.720e+02 -5.740e+02\n",
      "  5.740e+02  5.780e+02  5.870e+02  6.000e+02 -6.030e+02 -6.070e+02\n",
      " -6.200e+02 -6.300e+02  6.370e+02  6.450e+02  6.460e+02 -6.480e+02\n",
      " -6.620e+02  6.790e+02  6.830e+02 -6.850e+02 -6.890e+02 -6.930e+02\n",
      " -6.930e+02 -7.000e+02 -7.140e+02  7.220e+02 -7.220e+02  7.270e+02\n",
      " -7.350e+02 -7.490e+02 -7.500e+02 -7.550e+02 -7.620e+02  7.680e+02\n",
      "  7.700e+02 -7.740e+02 -7.770e+02  7.820e+02 -7.850e+02 -7.880e+02\n",
      "  7.930e+02  8.190e+02  8.220e+02 -8.220e+02  8.280e+02 -8.300e+02\n",
      "  8.410e+02  8.450e+02  8.660e+02  8.780e+02  8.830e+02  8.850e+02\n",
      " -9.090e+02 -9.100e+02 -9.190e+02  9.270e+02  9.320e+02 -9.320e+02\n",
      " -9.320e+02 -9.450e+02  9.520e+02 -9.520e+02  9.530e+02 -9.630e+02\n",
      " -9.670e+02 -9.700e+02 -9.720e+02 -9.920e+02  9.920e+02  1.003e+03\n",
      " -1.011e+03 -1.018e+03 -1.018e+03  1.019e+03  1.045e+03 -1.046e+03\n",
      " -1.063e+03 -1.071e+03  1.075e+03  1.091e+03  1.092e+03 -1.106e+03\n",
      "  1.111e+03  1.111e+03 -1.120e+03  1.121e+03 -1.124e+03 -1.124e+03\n",
      " -1.126e+03 -1.130e+03 -1.130e+03 -1.132e+03 -1.133e+03  1.133e+03\n",
      " -1.133e+03 -1.137e+03 -1.140e+03 -1.165e+03 -1.168e+03 -1.169e+03\n",
      " -1.175e+03 -1.177e+03 -1.186e+03  1.200e+03  1.200e+03  1.230e+03\n",
      " -1.238e+03 -1.266e+03  1.270e+03 -1.274e+03 -1.290e+03 -1.291e+03\n",
      " -1.299e+03 -1.307e+03 -1.308e+03 -1.314e+03 -1.329e+03  1.337e+03\n",
      "  1.343e+03 -1.355e+03 -1.367e+03 -1.371e+03  1.371e+03 -1.373e+03\n",
      "  1.378e+03 -1.380e+03 -1.384e+03 -1.385e+03 -1.385e+03 -1.398e+03\n",
      "  1.404e+03 -1.411e+03 -1.413e+03 -1.413e+03 -1.416e+03  1.417e+03\n",
      " -1.423e+03  1.432e+03 -1.435e+03 -1.450e+03 -1.459e+03  1.463e+03\n",
      " -1.471e+03 -1.476e+03 -1.478e+03 -1.485e+03 -1.485e+03 -1.487e+03\n",
      " -1.489e+03 -1.491e+03 -1.493e+03  1.493e+03 -1.495e+03 -1.496e+03\n",
      " -1.498e+03 -1.499e+03 -1.502e+03 -1.508e+03 -1.516e+03 -1.520e+03\n",
      " -1.521e+03 -1.525e+03 -1.528e+03 -1.531e+03 -1.552e+03 -1.559e+03\n",
      "  1.567e+03  1.588e+03  1.589e+03  1.590e+03  1.598e+03 -1.604e+03\n",
      " -1.608e+03  1.610e+03  1.620e+03 -1.621e+03 -1.624e+03  1.625e+03\n",
      "  1.625e+03 -1.632e+03  1.657e+03 -1.657e+03 -1.666e+03 -1.670e+03\n",
      " -1.683e+03  1.696e+03 -1.729e+03 -1.731e+03 -1.733e+03 -1.746e+03\n",
      " -1.759e+03 -1.778e+03 -1.779e+03 -1.782e+03 -1.785e+03 -1.787e+03\n",
      " -1.789e+03 -1.790e+03 -1.793e+03 -1.834e+03 -1.843e+03 -1.854e+03\n",
      " -1.871e+03 -1.879e+03 -1.883e+03 -1.885e+03 -1.886e+03 -1.888e+03\n",
      " -1.889e+03 -1.893e+03  1.912e+03  1.912e+03 -1.924e+03 -1.928e+03\n",
      " -1.929e+03 -1.935e+03 -1.952e+03 -1.955e+03 -1.955e+03  1.964e+03\n",
      "  1.980e+03 -1.993e+03 -1.997e+03 -2.004e+03 -2.009e+03 -2.014e+03\n",
      " -2.016e+03 -2.017e+03 -2.038e+03 -2.067e+03 -2.080e+03 -2.087e+03\n",
      "  2.090e+03  2.105e+03 -2.133e+03 -2.172e+03 -2.172e+03 -2.186e+03\n",
      "  2.190e+03 -2.208e+03 -2.226e+03  2.227e+03 -2.231e+03  2.241e+03\n",
      " -2.246e+03  2.256e+03 -2.257e+03 -2.259e+03 -2.263e+03 -2.270e+03\n",
      " -2.274e+03 -2.283e+03  2.299e+03  2.343e+03 -2.361e+03 -2.378e+03\n",
      "  2.386e+03 -2.392e+03 -2.412e+03  2.419e+03 -2.423e+03 -2.430e+03\n",
      "  2.454e+03 -2.461e+03 -2.489e+03 -2.504e+03 -2.508e+03 -2.531e+03\n",
      " -2.552e+03 -2.554e+03  2.564e+03  2.601e+03 -2.609e+03 -2.630e+03\n",
      " -2.688e+03 -2.718e+03 -2.722e+03  2.752e+03 -2.754e+03  2.764e+03\n",
      " -2.782e+03 -2.789e+03  2.830e+03 -2.839e+03 -2.859e+03 -2.865e+03\n",
      " -2.868e+03 -2.873e+03 -2.881e+03 -2.964e+03 -3.037e+03 -3.074e+03\n",
      " -3.205e+03 -3.205e+03 -3.222e+03 -3.229e+03 -3.271e+03 -3.302e+03\n",
      " -3.328e+03 -3.341e+03 -3.377e+03 -3.392e+03 -3.409e+03 -3.431e+03\n",
      " -3.451e+03 -3.498e+03 -3.519e+03  3.554e+03  3.615e+03 -3.631e+03\n",
      " -3.639e+03 -3.728e+03 -3.736e+03 -3.744e+03 -3.834e+03 -3.841e+03\n",
      " -3.936e+03 -3.944e+03 -3.987e+03 -3.989e+03 -4.067e+03]\n",
      "Concordance Index 0.7193244860219732\n",
      "Integrated Brier Score: 0.17619746920053658\n",
      "y_train breslow final [ 2.000e+00 -3.000e+00 -1.300e+01 -1.600e+01 -1.600e+01 -1.600e+01\n",
      "  1.800e+01 -2.600e+01 -2.900e+01 -3.600e+01  4.100e+01  4.200e+01\n",
      "  4.300e+01 -4.300e+01  5.100e+01 -5.300e+01  6.200e+01  6.500e+01\n",
      "  6.900e+01  9.900e+01  1.010e+02  1.060e+02  1.100e+02 -1.180e+02\n",
      " -1.190e+02 -1.270e+02  1.370e+02  1.390e+02  1.390e+02 -1.410e+02\n",
      " -1.500e+02 -1.550e+02  1.620e+02  1.640e+02 -1.820e+02  1.820e+02\n",
      "  1.830e+02 -1.930e+02  2.020e+02  2.040e+02 -2.040e+02 -2.050e+02\n",
      "  2.060e+02 -2.060e+02  2.110e+02 -2.110e+02  2.220e+02  2.380e+02\n",
      "  2.420e+02  2.450e+02 -2.560e+02 -2.930e+02  3.070e+02  3.130e+02\n",
      "  3.130e+02 -3.230e+02  3.290e+02  3.300e+02  3.330e+02  3.340e+02\n",
      "  3.360e+02 -3.400e+02  3.420e+02  3.440e+02 -3.540e+02 -3.550e+02\n",
      "  3.620e+02 -3.620e+02 -3.650e+02 -3.650e+02 -3.690e+02 -3.700e+02\n",
      " -3.730e+02 -3.740e+02  3.750e+02 -3.860e+02 -4.000e+02 -4.060e+02\n",
      " -4.110e+02  4.310e+02 -4.330e+02 -4.350e+02  4.450e+02 -4.540e+02\n",
      "  4.590e+02 -4.680e+02  4.750e+02  4.780e+02  4.800e+02  4.850e+02\n",
      " -4.950e+02 -5.010e+02 -5.050e+02 -5.070e+02  5.100e+02 -5.190e+02\n",
      " -5.230e+02 -5.510e+02  5.520e+02  5.610e+02  5.610e+02  5.620e+02\n",
      "  5.630e+02 -5.630e+02 -5.670e+02  5.710e+02  5.720e+02 -5.740e+02\n",
      "  5.740e+02  5.780e+02  5.870e+02  6.000e+02 -6.030e+02 -6.070e+02\n",
      " -6.200e+02 -6.300e+02  6.370e+02  6.450e+02  6.460e+02 -6.480e+02\n",
      " -6.620e+02  6.790e+02  6.830e+02 -6.850e+02 -6.890e+02 -6.930e+02\n",
      " -6.930e+02 -7.000e+02 -7.140e+02  7.220e+02 -7.220e+02  7.270e+02\n",
      " -7.350e+02 -7.490e+02 -7.500e+02 -7.550e+02 -7.620e+02  7.680e+02\n",
      "  7.700e+02 -7.740e+02 -7.770e+02  7.820e+02 -7.850e+02 -7.880e+02\n",
      "  7.930e+02  8.190e+02  8.220e+02 -8.220e+02  8.280e+02 -8.300e+02\n",
      "  8.410e+02  8.450e+02  8.660e+02  8.780e+02  8.830e+02  8.850e+02\n",
      " -9.090e+02 -9.100e+02 -9.190e+02  9.270e+02  9.320e+02 -9.320e+02\n",
      " -9.320e+02 -9.450e+02  9.520e+02 -9.520e+02  9.530e+02 -9.630e+02\n",
      " -9.670e+02 -9.700e+02 -9.720e+02 -9.920e+02  9.920e+02  1.003e+03\n",
      " -1.011e+03 -1.018e+03 -1.018e+03  1.019e+03  1.045e+03 -1.046e+03\n",
      " -1.063e+03 -1.071e+03  1.075e+03  1.091e+03  1.092e+03 -1.106e+03\n",
      "  1.111e+03  1.111e+03 -1.120e+03  1.121e+03 -1.124e+03 -1.124e+03\n",
      " -1.126e+03 -1.130e+03 -1.130e+03 -1.132e+03 -1.133e+03  1.133e+03\n",
      " -1.133e+03 -1.137e+03 -1.140e+03 -1.165e+03 -1.168e+03 -1.169e+03\n",
      " -1.175e+03 -1.177e+03 -1.186e+03  1.200e+03  1.200e+03  1.230e+03\n",
      " -1.238e+03 -1.266e+03  1.270e+03 -1.274e+03 -1.290e+03 -1.291e+03\n",
      " -1.299e+03 -1.307e+03 -1.308e+03 -1.314e+03 -1.329e+03  1.337e+03\n",
      "  1.343e+03 -1.355e+03 -1.367e+03 -1.371e+03  1.371e+03 -1.373e+03\n",
      "  1.378e+03 -1.380e+03 -1.384e+03 -1.385e+03 -1.385e+03 -1.398e+03\n",
      "  1.404e+03 -1.411e+03 -1.413e+03 -1.413e+03 -1.416e+03  1.417e+03\n",
      " -1.423e+03  1.432e+03 -1.435e+03 -1.450e+03 -1.459e+03  1.463e+03\n",
      " -1.471e+03 -1.476e+03 -1.478e+03 -1.485e+03 -1.485e+03 -1.487e+03\n",
      " -1.489e+03 -1.491e+03 -1.493e+03  1.493e+03 -1.495e+03 -1.496e+03\n",
      " -1.498e+03 -1.499e+03 -1.502e+03 -1.508e+03 -1.516e+03 -1.520e+03\n",
      " -1.521e+03 -1.525e+03 -1.528e+03 -1.531e+03 -1.552e+03 -1.559e+03\n",
      "  1.567e+03  1.588e+03  1.589e+03  1.590e+03  1.598e+03 -1.604e+03\n",
      " -1.608e+03  1.610e+03  1.620e+03 -1.621e+03 -1.624e+03  1.625e+03\n",
      "  1.625e+03 -1.632e+03  1.657e+03 -1.657e+03 -1.666e+03 -1.670e+03\n",
      " -1.683e+03  1.696e+03 -1.729e+03 -1.731e+03 -1.733e+03 -1.746e+03\n",
      " -1.759e+03 -1.778e+03 -1.779e+03 -1.782e+03 -1.785e+03 -1.787e+03\n",
      " -1.789e+03 -1.790e+03 -1.793e+03 -1.834e+03 -1.843e+03 -1.854e+03\n",
      " -1.871e+03 -1.879e+03 -1.883e+03 -1.885e+03 -1.886e+03 -1.888e+03\n",
      " -1.889e+03 -1.893e+03  1.912e+03  1.912e+03 -1.924e+03 -1.928e+03\n",
      " -1.929e+03 -1.935e+03 -1.952e+03 -1.955e+03 -1.955e+03  1.964e+03\n",
      "  1.980e+03 -1.993e+03 -1.997e+03 -2.004e+03 -2.009e+03 -2.014e+03\n",
      " -2.016e+03 -2.017e+03 -2.038e+03 -2.067e+03 -2.080e+03 -2.087e+03\n",
      "  2.090e+03  2.105e+03 -2.133e+03 -2.172e+03 -2.172e+03 -2.186e+03\n",
      "  2.190e+03 -2.208e+03 -2.226e+03  2.227e+03 -2.231e+03  2.241e+03\n",
      " -2.246e+03  2.256e+03 -2.257e+03 -2.259e+03 -2.263e+03 -2.270e+03\n",
      " -2.274e+03 -2.283e+03  2.299e+03  2.343e+03 -2.361e+03 -2.378e+03\n",
      "  2.386e+03 -2.392e+03 -2.412e+03  2.419e+03 -2.423e+03 -2.430e+03\n",
      "  2.454e+03 -2.461e+03 -2.489e+03 -2.504e+03 -2.508e+03 -2.531e+03\n",
      " -2.552e+03 -2.554e+03  2.564e+03  2.601e+03 -2.609e+03 -2.630e+03\n",
      " -2.688e+03 -2.718e+03 -2.722e+03  2.752e+03 -2.754e+03  2.764e+03\n",
      " -2.782e+03 -2.789e+03  2.830e+03 -2.839e+03 -2.859e+03 -2.865e+03\n",
      " -2.868e+03 -2.873e+03 -2.881e+03 -2.964e+03 -3.037e+03 -3.074e+03\n",
      " -3.205e+03 -3.205e+03 -3.222e+03 -3.229e+03 -3.271e+03 -3.302e+03\n",
      " -3.328e+03 -3.341e+03 -3.377e+03 -3.392e+03 -3.409e+03 -3.431e+03\n",
      " -3.451e+03 -3.498e+03 -3.519e+03  3.554e+03  3.615e+03 -3.631e+03\n",
      " -3.639e+03 -3.728e+03 -3.736e+03 -3.744e+03 -3.834e+03 -3.841e+03\n",
      " -3.936e+03 -3.944e+03 -3.987e+03 -3.989e+03 -4.067e+03]\n",
      "durations 3.0 4537.0\n",
      "Concordance Index 0.6560364464692483\n",
      "Integrated Brier Score: 0.1855597006132907\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m65.8425\u001b[0m  0.0064\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m85.2064\u001b[0m  0.0048\n",
      "      2       65.8425  0.0037\n",
      "      2       85.2064  0.0031\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m76.5408\u001b[0m  0.0050\n",
      "      3       85.2064  0.0029\n",
      "      3       65.8425  0.0031\n",
      "      4       85.2064  0.0032\n",
      "      4       65.8425  0.0036\n",
      "      2       76.5408  0.0054\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m79.5612\u001b[0m       \u001b[32m55.5711\u001b[0m  0.1072\n",
      "      5       65.8425  0.0039\n",
      "      5       85.2064  0.0053\n",
      "      3       76.5408  0.0046\n",
      "      6       65.8425  0.0043\n",
      "      6       85.2064  0.0048\n",
      "      4       76.5408  0.0047\n",
      "      7       65.8425  0.0045\n",
      "      7       85.2064  0.0047\n",
      "      5       76.5408  0.0045\n",
      "      8       65.8425  0.0031\n",
      "      6       76.5408  0.0030\n",
      "      9       65.8425  0.0031\n",
      "      7       76.5408  0.0036\n",
      "      8       85.2064  0.0077\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m80.2196\u001b[0m       \u001b[32m46.7074\u001b[0m  0.1115\n",
      "     10       65.8425  0.0035\n",
      "Restoring best model from epoch 1.\n",
      "      9       85.2064  0.0031\n",
      "     10       85.2064  0.0030\n",
      "      8       76.5408  0.0066\n",
      "Restoring best model from epoch 1.\n",
      "      9       76.5408  0.0031\n",
      "     10       76.5408  0.0029\n",
      "Restoring best model from epoch 1.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m84.8132\u001b[0m       \u001b[32m64.1648\u001b[0m  0.1255\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m73.5501\u001b[0m       \u001b[32m48.7258\u001b[0m  0.1154\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m150.9027\u001b[0m  0.0098\n",
      "      2      150.9027  0.0075\n",
      "      3      150.9027  0.0030\n",
      "      4      150.9027  0.0029\n",
      "      5      150.9027  0.0030\n",
      "      6      150.9027  0.0038\n",
      "      2       83.4882       61.9063  0.1107\n",
      "      7      150.9027  0.0098\n",
      "      8      150.9027  0.0029\n",
      "      9      150.9027  0.0029\n",
      "     10      150.9027  0.0033\n",
      "Restoring best model from epoch 1.\n",
      "      2       84.7094       52.5474  0.1063\n",
      "      2       96.1464      191.5819  0.0970\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2       84.0664       86.8647  0.1039\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m76.6803\u001b[0m  0.0128\n",
      "      2       76.6803  0.0065\n",
      "      3       76.6803  0.0060\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      4       76.6803  0.0088\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      5       76.6803  0.0066\n",
      "      6       76.6803  0.0035\n",
      "      7       76.6803  0.0031\n",
      "      8       76.6803  0.0029\n",
      "      9       76.6803  0.0032\n",
      "     10       76.6803  0.0029\n",
      "Restoring best model from epoch 1.\n",
      "      3       81.3557       \u001b[32m53.8698\u001b[0m  0.0984\n",
      "      3       86.7233       72.1175  0.0980\n",
      "      3       83.1539       \u001b[32m44.0390\u001b[0m  0.1094\n",
      "      3       76.2884      121.9135  0.0991\n",
      "      4       79.5720       60.6067  0.0845\n",
      "      4       92.6107       73.3789  0.0895\n",
      "      4       84.2275       60.8360  0.0879\n",
      "      4       77.2824       70.1706  0.1146\n",
      "      5       90.5829       56.2641  0.0904\n",
      "      5       \u001b[36m84.7845\u001b[0m       77.7879  0.0857\n",
      "      5       83.7716       87.4464  0.0826\n",
      "      5       \u001b[36m72.2201\u001b[0m       \u001b[32m47.9631\u001b[0m  0.0765\n",
      "      6       \u001b[36m78.1747\u001b[0m       \u001b[32m49.3870\u001b[0m  0.0735\n",
      "      6       \u001b[36m82.9028\u001b[0m       75.4887  0.0742\n",
      "      6       \u001b[36m78.2349\u001b[0m       55.1802  0.0732\n",
      "      6       \u001b[36m71.5409\u001b[0m       63.8232  0.0731\n",
      "      7       78.4976       52.3331  0.0741\n",
      "      7       88.9691       68.2451  0.0745\n",
      "      7       78.3830       44.3553  0.0747\n",
      "      7       75.9101       61.0656  0.0724\n",
      "      8       83.1403       51.7279  0.0726\n",
      "      8       \u001b[36m80.7918\u001b[0m       64.7274  0.0732\n",
      "      8       80.1322       \u001b[32m43.9420\u001b[0m  0.0729\n",
      "      8       72.2255       48.9447  0.0722\n",
      "      9       78.4219       52.6729  0.0737\n",
      "      9       86.3537       65.2471  0.0730\n",
      "      9       83.1493       \u001b[32m43.6037\u001b[0m  0.0739\n",
      "      9       \u001b[36m71.3625\u001b[0m       \u001b[32m47.1652\u001b[0m  0.0724\n",
      "     10       \u001b[36m77.8699\u001b[0m       52.5647  0.0734\n",
      "Restoring best model from epoch 6.\n",
      "     10       87.9875       \u001b[32m63.7022\u001b[0m  0.0730\n",
      "     10       \u001b[36m77.5020\u001b[0m       \u001b[32m41.9525\u001b[0m  0.0732\n",
      "     10       72.8072       47.4501  0.0734\n",
      "Restoring best model from epoch 9.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m141.1963\u001b[0m  0.0045\n",
      "      2      141.1963  0.0038\n",
      "      3      141.1963  0.0038\n",
      "      4      141.1963  0.0037\n",
      "      5      141.1963  0.0036\n",
      "      6      141.1963  0.0059\n",
      "      7      141.1963  0.0054\n",
      "      8      141.1963  0.0048\n",
      "      9      141.1963  0.0041\n",
      "     10      141.1963  0.0040\n",
      "Restoring best model from epoch 1.\n",
      "y_train breslow final [ 2.000e+00 -3.000e+00 -3.000e+00 -7.000e+00 -1.100e+01 -1.300e+01\n",
      " -1.600e+01 -1.600e+01 -1.800e+01  1.800e+01 -2.600e+01 -3.600e+01\n",
      " -3.800e+01  4.200e+01  4.300e+01 -4.300e+01  5.100e+01 -5.300e+01\n",
      "  5.900e+01 -6.100e+01  6.200e+01  6.500e+01  6.800e+01  6.900e+01\n",
      "  7.300e+01  7.700e+01  9.300e+01  9.900e+01  1.010e+02  1.060e+02\n",
      "  1.090e+02  1.100e+02 -1.180e+02 -1.190e+02 -1.270e+02  1.370e+02\n",
      "  1.390e+02  1.390e+02 -1.410e+02 -1.500e+02  1.620e+02  1.640e+02\n",
      " -1.660e+02  1.660e+02  1.680e+02 -1.750e+02 -1.770e+02 -1.820e+02\n",
      "  1.820e+02  1.830e+02 -1.930e+02 -1.940e+02  2.020e+02 -2.040e+02\n",
      "  2.060e+02 -2.060e+02  2.110e+02 -2.110e+02  2.220e+02  2.240e+02\n",
      "  2.380e+02  2.420e+02  2.450e+02  3.070e+02  3.110e+02  3.130e+02\n",
      " -3.190e+02  3.200e+02  3.290e+02  3.300e+02  3.340e+02  3.360e+02\n",
      " -3.400e+02  3.440e+02 -3.540e+02 -3.550e+02 -3.560e+02  3.620e+02\n",
      " -3.620e+02 -3.650e+02 -3.700e+02 -3.720e+02 -3.740e+02  3.750e+02\n",
      " -3.850e+02 -3.860e+02 -4.060e+02 -4.080e+02 -4.110e+02 -4.310e+02\n",
      " -4.350e+02  4.450e+02  4.460e+02  4.540e+02 -4.540e+02 -4.680e+02\n",
      " -4.690e+02  4.750e+02  4.780e+02  4.800e+02  4.850e+02 -4.950e+02\n",
      " -4.980e+02 -5.010e+02 -5.050e+02 -5.070e+02  5.100e+02 -5.110e+02\n",
      " -5.190e+02 -5.230e+02 -5.280e+02 -5.510e+02  5.610e+02  5.620e+02\n",
      " -5.670e+02  5.710e+02  5.720e+02 -5.740e+02  5.780e+02  5.870e+02\n",
      " -5.910e+02  6.000e+02 -6.030e+02 -6.070e+02 -6.170e+02 -6.200e+02\n",
      "  6.370e+02  6.450e+02  6.460e+02 -6.560e+02 -6.620e+02 -6.650e+02\n",
      "  6.790e+02  6.830e+02 -6.890e+02 -6.930e+02 -7.000e+02  7.010e+02\n",
      "  7.090e+02 -7.140e+02  7.270e+02  7.350e+02 -7.370e+02 -7.490e+02\n",
      " -7.500e+02 -7.550e+02 -7.620e+02  7.680e+02  7.700e+02 -7.740e+02\n",
      " -7.770e+02  7.820e+02 -7.850e+02 -7.880e+02  7.930e+02 -8.220e+02\n",
      "  8.340e+02 -8.400e+02 -8.610e+02  8.660e+02 -8.730e+02 -8.740e+02\n",
      "  8.780e+02  8.830e+02  8.850e+02 -9.090e+02 -9.100e+02  9.270e+02\n",
      "  9.320e+02 -9.320e+02 -9.450e+02  9.460e+02 -9.510e+02  9.530e+02\n",
      " -9.630e+02 -9.670e+02 -9.700e+02 -9.720e+02  9.920e+02  1.003e+03\n",
      " -1.011e+03 -1.014e+03 -1.018e+03  1.019e+03  1.034e+03  1.045e+03\n",
      " -1.046e+03 -1.063e+03 -1.071e+03  1.075e+03  1.091e+03 -1.092e+03\n",
      "  1.092e+03  1.097e+03 -1.106e+03 -1.107e+03  1.111e+03  1.111e+03\n",
      "  1.121e+03 -1.124e+03 -1.126e+03 -1.130e+03 -1.132e+03  1.133e+03\n",
      " -1.133e+03 -1.137e+03 -1.140e+03 -1.143e+03 -1.165e+03 -1.168e+03\n",
      " -1.169e+03  1.170e+03 -1.177e+03 -1.186e+03  1.191e+03 -1.217e+03\n",
      " -1.218e+03  1.230e+03 -1.238e+03  1.238e+03 -1.266e+03  1.270e+03\n",
      " -1.274e+03 -1.290e+03 -1.291e+03 -1.299e+03 -1.307e+03 -1.308e+03\n",
      " -1.314e+03  1.317e+03 -1.329e+03  1.343e+03 -1.355e+03 -1.367e+03\n",
      " -1.371e+03  1.371e+03 -1.373e+03 -1.380e+03 -1.384e+03 -1.385e+03\n",
      " -1.385e+03 -1.398e+03 -1.413e+03 -1.416e+03  1.417e+03  1.432e+03\n",
      " -1.433e+03 -1.435e+03 -1.436e+03 -1.459e+03 -1.462e+03 -1.478e+03\n",
      " -1.485e+03 -1.485e+03 -1.487e+03 -1.489e+03 -1.491e+03 -1.493e+03\n",
      "  1.493e+03 -1.495e+03 -1.496e+03 -1.498e+03 -1.502e+03 -1.516e+03\n",
      " -1.520e+03 -1.525e+03 -1.525e+03 -1.528e+03 -1.531e+03 -1.552e+03\n",
      " -1.559e+03 -1.560e+03  1.567e+03  1.584e+03  1.588e+03  1.589e+03\n",
      "  1.590e+03 -1.604e+03 -1.608e+03  1.610e+03  1.620e+03 -1.621e+03\n",
      " -1.624e+03  1.625e+03  1.625e+03  1.626e+03 -1.632e+03  1.639e+03\n",
      "  1.657e+03 -1.657e+03  1.661e+03 -1.666e+03 -1.670e+03 -1.683e+03\n",
      "  1.696e+03  1.714e+03  1.724e+03 -1.729e+03 -1.731e+03 -1.755e+03\n",
      " -1.759e+03 -1.778e+03 -1.779e+03 -1.782e+03 -1.785e+03 -1.787e+03\n",
      " -1.789e+03 -1.793e+03 -1.794e+03 -1.834e+03 -1.854e+03 -1.862e+03\n",
      " -1.876e+03 -1.879e+03 -1.886e+03 -1.888e+03 -1.905e+03 -1.906e+03\n",
      "  1.912e+03  1.913e+03 -1.924e+03 -1.928e+03 -1.929e+03 -1.935e+03\n",
      " -1.946e+03 -1.952e+03 -1.955e+03 -1.955e+03  1.964e+03  1.980e+03\n",
      "  1.986e+03 -1.993e+03 -2.009e+03 -2.038e+03 -2.067e+03 -2.080e+03\n",
      " -2.087e+03  2.090e+03  2.105e+03 -2.128e+03 -2.133e+03  2.145e+03\n",
      " -2.150e+03 -2.172e+03 -2.184e+03 -2.186e+03  2.190e+03 -2.208e+03\n",
      " -2.217e+03  2.227e+03 -2.231e+03  2.241e+03 -2.246e+03 -2.257e+03\n",
      " -2.259e+03 -2.263e+03 -2.270e+03 -2.271e+03 -2.274e+03 -2.283e+03\n",
      "  2.343e+03 -2.361e+03 -2.372e+03 -2.378e+03 -2.392e+03 -2.412e+03\n",
      " -2.422e+03 -2.430e+03 -2.439e+03  2.454e+03 -2.461e+03 -2.470e+03\n",
      " -2.504e+03 -2.531e+03 -2.554e+03  2.564e+03  2.601e+03 -2.609e+03\n",
      " -2.609e+03 -2.660e+03 -2.718e+03 -2.722e+03 -2.746e+03 -2.754e+03\n",
      "  2.764e+03 -2.782e+03 -2.789e+03 -2.799e+03  2.830e+03 -2.865e+03\n",
      " -2.868e+03 -2.873e+03 -2.881e+03 -2.964e+03 -3.037e+03 -3.074e+03\n",
      " -3.146e+03 -3.205e+03 -3.205e+03 -3.222e+03 -3.229e+03 -3.267e+03\n",
      " -3.271e+03 -3.302e+03 -3.331e+03 -3.341e+03 -3.343e+03 -3.377e+03\n",
      " -3.392e+03 -3.409e+03 -3.431e+03 -3.451e+03 -3.480e+03 -3.519e+03\n",
      " -3.583e+03 -3.639e+03 -3.744e+03 -3.834e+03 -3.841e+03 -3.944e+03\n",
      " -3.974e+03 -3.989e+03 -4.067e+03 -4.074e+03 -4.537e+03]\n",
      "Concordance Index 0.6203573742172317\n",
      "Integrated Brier Score: 0.3188012051582324\n",
      "y_train breslow final [ 2.000e+00 -3.000e+00 -3.000e+00 -7.000e+00 -1.100e+01 -1.300e+01\n",
      " -1.600e+01 -1.600e+01 -1.800e+01  1.800e+01 -2.600e+01 -3.600e+01\n",
      " -3.800e+01  4.200e+01  4.300e+01 -4.300e+01  5.100e+01 -5.300e+01\n",
      "  5.900e+01 -6.100e+01  6.200e+01  6.500e+01  6.800e+01  6.900e+01\n",
      "  7.300e+01  7.700e+01  9.300e+01  9.900e+01  1.010e+02  1.060e+02\n",
      "  1.090e+02  1.100e+02 -1.180e+02 -1.190e+02 -1.270e+02  1.370e+02\n",
      "  1.390e+02  1.390e+02 -1.410e+02 -1.500e+02  1.620e+02  1.640e+02\n",
      " -1.660e+02  1.660e+02  1.680e+02 -1.750e+02 -1.770e+02 -1.820e+02\n",
      "  1.820e+02  1.830e+02 -1.930e+02 -1.940e+02  2.020e+02 -2.040e+02\n",
      "  2.060e+02 -2.060e+02  2.110e+02 -2.110e+02  2.220e+02  2.240e+02\n",
      "  2.380e+02  2.420e+02  2.450e+02  3.070e+02  3.110e+02  3.130e+02\n",
      " -3.190e+02  3.200e+02  3.290e+02  3.300e+02  3.340e+02  3.360e+02\n",
      " -3.400e+02  3.440e+02 -3.540e+02 -3.550e+02 -3.560e+02  3.620e+02\n",
      " -3.620e+02 -3.650e+02 -3.700e+02 -3.720e+02 -3.740e+02  3.750e+02\n",
      " -3.850e+02 -3.860e+02 -4.060e+02 -4.080e+02 -4.110e+02 -4.310e+02\n",
      " -4.350e+02  4.450e+02  4.460e+02  4.540e+02 -4.540e+02 -4.680e+02\n",
      " -4.690e+02  4.750e+02  4.780e+02  4.800e+02  4.850e+02 -4.950e+02\n",
      " -4.980e+02 -5.010e+02 -5.050e+02 -5.070e+02  5.100e+02 -5.110e+02\n",
      " -5.190e+02 -5.230e+02 -5.280e+02 -5.510e+02  5.610e+02  5.620e+02\n",
      " -5.670e+02  5.710e+02  5.720e+02 -5.740e+02  5.780e+02  5.870e+02\n",
      " -5.910e+02  6.000e+02 -6.030e+02 -6.070e+02 -6.170e+02 -6.200e+02\n",
      "  6.370e+02  6.450e+02  6.460e+02 -6.560e+02 -6.620e+02 -6.650e+02\n",
      "  6.790e+02  6.830e+02 -6.890e+02 -6.930e+02 -7.000e+02  7.010e+02\n",
      "  7.090e+02 -7.140e+02  7.270e+02  7.350e+02 -7.370e+02 -7.490e+02\n",
      " -7.500e+02 -7.550e+02 -7.620e+02  7.680e+02  7.700e+02 -7.740e+02\n",
      " -7.770e+02  7.820e+02 -7.850e+02 -7.880e+02  7.930e+02 -8.220e+02\n",
      "  8.340e+02 -8.400e+02 -8.610e+02  8.660e+02 -8.730e+02 -8.740e+02\n",
      "  8.780e+02  8.830e+02  8.850e+02 -9.090e+02 -9.100e+02  9.270e+02\n",
      "  9.320e+02 -9.320e+02 -9.450e+02  9.460e+02 -9.510e+02  9.530e+02\n",
      " -9.630e+02 -9.670e+02 -9.700e+02 -9.720e+02  9.920e+02  1.003e+03\n",
      " -1.011e+03 -1.014e+03 -1.018e+03  1.019e+03  1.034e+03  1.045e+03\n",
      " -1.046e+03 -1.063e+03 -1.071e+03  1.075e+03  1.091e+03 -1.092e+03\n",
      "  1.092e+03  1.097e+03 -1.106e+03 -1.107e+03  1.111e+03  1.111e+03\n",
      "  1.121e+03 -1.124e+03 -1.126e+03 -1.130e+03 -1.132e+03  1.133e+03\n",
      " -1.133e+03 -1.137e+03 -1.140e+03 -1.143e+03 -1.165e+03 -1.168e+03\n",
      " -1.169e+03  1.170e+03 -1.177e+03 -1.186e+03  1.191e+03 -1.217e+03\n",
      " -1.218e+03  1.230e+03 -1.238e+03  1.238e+03 -1.266e+03  1.270e+03\n",
      " -1.274e+03 -1.290e+03 -1.291e+03 -1.299e+03 -1.307e+03 -1.308e+03\n",
      " -1.314e+03  1.317e+03 -1.329e+03  1.343e+03 -1.355e+03 -1.367e+03\n",
      " -1.371e+03  1.371e+03 -1.373e+03 -1.380e+03 -1.384e+03 -1.385e+03\n",
      " -1.385e+03 -1.398e+03 -1.413e+03 -1.416e+03  1.417e+03  1.432e+03\n",
      " -1.433e+03 -1.435e+03 -1.436e+03 -1.459e+03 -1.462e+03 -1.478e+03\n",
      " -1.485e+03 -1.485e+03 -1.487e+03 -1.489e+03 -1.491e+03 -1.493e+03\n",
      "  1.493e+03 -1.495e+03 -1.496e+03 -1.498e+03 -1.502e+03 -1.516e+03\n",
      " -1.520e+03 -1.525e+03 -1.525e+03 -1.528e+03 -1.531e+03 -1.552e+03\n",
      " -1.559e+03 -1.560e+03  1.567e+03  1.584e+03  1.588e+03  1.589e+03\n",
      "  1.590e+03 -1.604e+03 -1.608e+03  1.610e+03  1.620e+03 -1.621e+03\n",
      " -1.624e+03  1.625e+03  1.625e+03  1.626e+03 -1.632e+03  1.639e+03\n",
      "  1.657e+03 -1.657e+03  1.661e+03 -1.666e+03 -1.670e+03 -1.683e+03\n",
      "  1.696e+03  1.714e+03  1.724e+03 -1.729e+03 -1.731e+03 -1.755e+03\n",
      " -1.759e+03 -1.778e+03 -1.779e+03 -1.782e+03 -1.785e+03 -1.787e+03\n",
      " -1.789e+03 -1.793e+03 -1.794e+03 -1.834e+03 -1.854e+03 -1.862e+03\n",
      " -1.876e+03 -1.879e+03 -1.886e+03 -1.888e+03 -1.905e+03 -1.906e+03\n",
      "  1.912e+03  1.913e+03 -1.924e+03 -1.928e+03 -1.929e+03 -1.935e+03\n",
      " -1.946e+03 -1.952e+03 -1.955e+03 -1.955e+03  1.964e+03  1.980e+03\n",
      "  1.986e+03 -1.993e+03 -2.009e+03 -2.038e+03 -2.067e+03 -2.080e+03\n",
      " -2.087e+03  2.090e+03  2.105e+03 -2.128e+03 -2.133e+03  2.145e+03\n",
      " -2.150e+03 -2.172e+03 -2.184e+03 -2.186e+03  2.190e+03 -2.208e+03\n",
      " -2.217e+03  2.227e+03 -2.231e+03  2.241e+03 -2.246e+03 -2.257e+03\n",
      " -2.259e+03 -2.263e+03 -2.270e+03 -2.271e+03 -2.274e+03 -2.283e+03\n",
      "  2.343e+03 -2.361e+03 -2.372e+03 -2.378e+03 -2.392e+03 -2.412e+03\n",
      " -2.422e+03 -2.430e+03 -2.439e+03  2.454e+03 -2.461e+03 -2.470e+03\n",
      " -2.504e+03 -2.531e+03 -2.554e+03  2.564e+03  2.601e+03 -2.609e+03\n",
      " -2.609e+03 -2.660e+03 -2.718e+03 -2.722e+03 -2.746e+03 -2.754e+03\n",
      "  2.764e+03 -2.782e+03 -2.789e+03 -2.799e+03  2.830e+03 -2.865e+03\n",
      " -2.868e+03 -2.873e+03 -2.881e+03 -2.964e+03 -3.037e+03 -3.074e+03\n",
      " -3.146e+03 -3.205e+03 -3.205e+03 -3.222e+03 -3.229e+03 -3.267e+03\n",
      " -3.271e+03 -3.302e+03 -3.331e+03 -3.341e+03 -3.343e+03 -3.377e+03\n",
      " -3.392e+03 -3.409e+03 -3.431e+03 -3.451e+03 -3.480e+03 -3.519e+03\n",
      " -3.583e+03 -3.639e+03 -3.744e+03 -3.834e+03 -3.841e+03 -3.944e+03\n",
      " -3.974e+03 -3.989e+03 -4.067e+03 -4.074e+03 -4.537e+03]\n",
      "durations 16.0 3987.0\n",
      "Concordance Index 0.5162505887894488\n",
      "Integrated Brier Score: 0.30233277166406375\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m78.4893\u001b[0m  0.0044\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2       78.4893  0.0104\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      3       78.4893  0.0030\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      4       78.4893  0.0048\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      5       78.4893  0.0063\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m86.6475\u001b[0m  0.0070\n",
      "      6       78.4893  0.0075\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m93.5130\u001b[0m  0.0055\n",
      "      2       86.6475  0.0046\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      7       78.4893  0.0034\n",
      "      3       86.6475  0.0031\n",
      "      2       93.5130  0.0045\n",
      "      8       78.4893  0.0029\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      4       86.6475  0.0030\n",
      "      3       93.5130  0.0031\n",
      "      9       78.4893  0.0028\n",
      "      5       86.6475  0.0029\n",
      "      4       93.5130  0.0029\n",
      "     10       78.4893  0.0032\n",
      "Restoring best model from epoch 1.\n",
      "      6       86.6475  0.0029\n",
      "      5       93.5130  0.0032\n",
      "      7       86.6475  0.0038\n",
      "      8       86.6475  0.0029\n",
      "      6       93.5130  0.0055\n",
      "      9       86.6475  0.0028\n",
      "      7       93.5130  0.0028\n",
      "     10       86.6475  0.0030\n",
      "Restoring best model from epoch 1.\n",
      "      8       93.5130  0.0039\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      9       93.5130  0.0030\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "     10       93.5130  0.0029\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m122.2841\u001b[0m  0.0064\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m79.6784\u001b[0m      \u001b[32m108.7907\u001b[0m  0.1114\n",
      "      2      122.2841  0.0058\n",
      "      3      122.2841  0.0032\n",
      "      4      122.2841  0.0048\n",
      "      5      122.2841  0.0038\n",
      "      6      122.2841  0.0035\n",
      "      7      122.2841  0.0038\n",
      "      8      122.2841  0.0033\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      9      122.2841  0.0031\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "     10      122.2841  0.0031\n",
      "Restoring best model from epoch 1.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m104.4218\u001b[0m  0.0071\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m85.5775\u001b[0m       \u001b[32m84.1507\u001b[0m  0.1043\n",
      "      2      104.4218  0.0036\n",
      "      3      104.4218  0.0033\n",
      "      4      104.4218  0.0033\n",
      "      5      104.4218  0.0030\n",
      "      6      104.4218  0.0030\n",
      "      7      104.4218  0.0030\n",
      "      8      104.4218  0.0029\n",
      "      9      104.4218  0.0029\n",
      "     10      104.4218  0.0031\n",
      "Restoring best model from epoch 1.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m70.3133\u001b[0m       \u001b[32m44.0431\u001b[0m  0.1060\n",
      "      2       84.0480       \u001b[32m81.1675\u001b[0m  0.0951\n",
      "      2       88.0685       \u001b[32m83.5685\u001b[0m  0.0825\n",
      "      2       85.0145      162.2955  0.0887\n",
      "      3       82.5159       \u001b[32m69.3057\u001b[0m  0.0840\n",
      "      3       \u001b[36m78.6844\u001b[0m       \u001b[32m82.5902\u001b[0m  0.0949\n",
      "      3       72.5651       57.1576  0.0792\n",
      "      4       84.7784       73.0973  0.0818\n",
      "      4       87.4282       \u001b[32m79.2636\u001b[0m  0.0766\n",
      "      4       74.4638       87.1649  0.0737\n",
      "      5       \u001b[36m79.3814\u001b[0m       \u001b[32m67.7229\u001b[0m  0.0784\n",
      "      5       85.4937       \u001b[32m77.3107\u001b[0m  0.0751\n",
      "      5       70.5783       97.4339  0.0711\n",
      "      6       79.5917       69.9932  0.0740\n",
      "      6       82.1294       79.5047  0.0745\n",
      "      6       \u001b[36m65.7428\u001b[0m       44.7464  0.0710\n",
      "      7       84.6243       68.9365  0.0730\n",
      "      7       \u001b[36m78.0361\u001b[0m       82.3548  0.0745\n",
      "      7       68.8745       \u001b[32m39.6159\u001b[0m  0.0724\n",
      "      8       \u001b[36m78.0288\u001b[0m       \u001b[32m63.4301\u001b[0m  0.0799\n",
      "      8       \u001b[36m65.3845\u001b[0m       39.8695  0.0777\n",
      "      8       \u001b[36m76.8269\u001b[0m       82.1470  0.0874\n",
      "      9       78.8956       \u001b[32m59.7011\u001b[0m  0.0757\n",
      "      9       70.5915       \u001b[32m38.0727\u001b[0m  0.0712\n",
      "      9       78.0214       82.4777  0.0736\n",
      "     10       79.2461       61.3113  0.0745\n",
      "Restoring best model from epoch 9.\n",
      "     10       67.3687       43.7836  0.0742\n",
      "Restoring best model from epoch 9.\n",
      "     10       80.3202       83.8173  0.0738\n",
      "Restoring best model from epoch 5.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m125.6441\u001b[0m  0.0042\n",
      "      2      125.6441  0.0046\n",
      "      3      125.6441  0.0035\n",
      "      4      125.6441  0.0038\n",
      "      5      125.6441  0.0036\n",
      "      6      125.6441  0.0047\n",
      "      7      125.6441  0.0049\n",
      "      8      125.6441  0.0053\n",
      "      9      125.6441  0.0058\n",
      "     10      125.6441  0.0045\n",
      "Restoring best model from epoch 1.\n",
      "y_train breslow final [ 2.000e+00 -3.000e+00 -3.000e+00 -7.000e+00 -1.100e+01 -1.600e+01\n",
      " -1.600e+01 -1.800e+01  1.800e+01 -2.600e+01 -2.900e+01 -3.600e+01\n",
      " -3.800e+01  4.100e+01  4.300e+01  5.900e+01 -6.100e+01  6.200e+01\n",
      "  6.800e+01  7.300e+01  7.700e+01  9.300e+01  9.900e+01  1.010e+02\n",
      "  1.090e+02  1.100e+02 -1.180e+02 -1.190e+02  1.370e+02  1.390e+02\n",
      " -1.550e+02  1.640e+02 -1.660e+02  1.660e+02  1.680e+02 -1.750e+02\n",
      " -1.770e+02 -1.820e+02  1.830e+02 -1.940e+02  2.020e+02  2.040e+02\n",
      " -2.040e+02 -2.050e+02  2.060e+02 -2.060e+02  2.110e+02 -2.110e+02\n",
      "  2.220e+02  2.240e+02  2.450e+02 -2.560e+02 -2.930e+02  3.070e+02\n",
      "  3.110e+02  3.130e+02 -3.190e+02  3.200e+02 -3.230e+02  3.290e+02\n",
      "  3.300e+02  3.330e+02  3.340e+02  3.360e+02 -3.400e+02  3.420e+02\n",
      " -3.550e+02 -3.560e+02  3.620e+02 -3.620e+02 -3.650e+02 -3.690e+02\n",
      " -3.700e+02 -3.720e+02 -3.730e+02 -3.740e+02  3.750e+02 -3.850e+02\n",
      " -3.860e+02 -4.000e+02 -4.080e+02 -4.110e+02  4.310e+02 -4.310e+02\n",
      " -4.330e+02 -4.350e+02  4.460e+02  4.540e+02  4.590e+02 -4.680e+02\n",
      " -4.690e+02  4.750e+02  4.780e+02  4.800e+02  4.800e+02  4.850e+02\n",
      " -4.980e+02 -5.010e+02 -5.050e+02 -5.070e+02  5.100e+02 -5.110e+02\n",
      " -5.190e+02 -5.230e+02 -5.280e+02  5.520e+02  5.610e+02  5.610e+02\n",
      "  5.620e+02  5.630e+02 -5.630e+02 -5.670e+02  5.710e+02  5.720e+02\n",
      " -5.740e+02  5.740e+02  5.870e+02 -5.910e+02  6.000e+02 -6.170e+02\n",
      " -6.200e+02 -6.300e+02  6.370e+02  6.460e+02 -6.480e+02 -6.560e+02\n",
      " -6.620e+02 -6.650e+02 -6.850e+02 -6.930e+02 -6.930e+02 -7.000e+02\n",
      "  7.010e+02  7.090e+02  7.220e+02 -7.220e+02  7.270e+02  7.350e+02\n",
      " -7.350e+02 -7.370e+02 -7.500e+02  7.700e+02 -7.740e+02 -7.770e+02\n",
      "  7.820e+02 -7.880e+02  7.930e+02  8.190e+02  8.220e+02 -8.220e+02\n",
      "  8.280e+02 -8.300e+02  8.340e+02 -8.400e+02  8.410e+02  8.450e+02\n",
      " -8.610e+02 -8.730e+02 -8.740e+02  8.780e+02  8.830e+02  8.850e+02\n",
      "  8.850e+02 -9.190e+02  9.320e+02 -9.320e+02 -9.320e+02  9.460e+02\n",
      " -9.510e+02  9.520e+02 -9.520e+02  9.530e+02 -9.630e+02 -9.670e+02\n",
      " -9.700e+02 -9.720e+02 -9.920e+02 -1.011e+03 -1.014e+03 -1.018e+03\n",
      " -1.018e+03  1.034e+03  1.045e+03 -1.092e+03  1.097e+03 -1.106e+03\n",
      " -1.107e+03  1.111e+03  1.111e+03 -1.120e+03  1.121e+03 -1.124e+03\n",
      " -1.124e+03 -1.126e+03 -1.130e+03 -1.130e+03 -1.132e+03 -1.133e+03\n",
      "  1.133e+03 -1.133e+03 -1.137e+03 -1.140e+03 -1.143e+03 -1.165e+03\n",
      "  1.170e+03 -1.175e+03 -1.177e+03 -1.186e+03  1.191e+03  1.200e+03\n",
      "  1.200e+03 -1.217e+03 -1.218e+03  1.230e+03  1.238e+03  1.270e+03\n",
      " -1.291e+03 -1.299e+03 -1.307e+03 -1.308e+03  1.317e+03  1.337e+03\n",
      "  1.343e+03 -1.355e+03 -1.371e+03 -1.373e+03  1.378e+03 -1.384e+03\n",
      " -1.385e+03 -1.385e+03 -1.398e+03  1.404e+03 -1.411e+03 -1.413e+03\n",
      " -1.416e+03  1.417e+03 -1.423e+03  1.432e+03 -1.433e+03 -1.435e+03\n",
      " -1.436e+03 -1.450e+03 -1.459e+03 -1.462e+03  1.463e+03 -1.471e+03\n",
      " -1.476e+03 -1.485e+03 -1.485e+03 -1.489e+03 -1.491e+03 -1.493e+03\n",
      "  1.493e+03 -1.495e+03 -1.496e+03 -1.499e+03 -1.502e+03 -1.508e+03\n",
      " -1.516e+03 -1.520e+03 -1.520e+03 -1.521e+03 -1.525e+03 -1.525e+03\n",
      " -1.531e+03 -1.559e+03 -1.560e+03  1.567e+03  1.584e+03  1.588e+03\n",
      "  1.590e+03  1.598e+03 -1.604e+03  1.610e+03 -1.621e+03 -1.624e+03\n",
      "  1.625e+03  1.625e+03  1.626e+03 -1.632e+03  1.639e+03 -1.657e+03\n",
      "  1.661e+03 -1.666e+03 -1.670e+03  1.696e+03  1.714e+03  1.724e+03\n",
      " -1.729e+03 -1.731e+03 -1.733e+03 -1.746e+03 -1.755e+03 -1.759e+03\n",
      " -1.778e+03 -1.779e+03 -1.785e+03 -1.787e+03 -1.789e+03 -1.790e+03\n",
      " -1.793e+03 -1.794e+03 -1.834e+03 -1.843e+03 -1.854e+03 -1.862e+03\n",
      " -1.871e+03 -1.876e+03 -1.879e+03 -1.883e+03 -1.885e+03 -1.889e+03\n",
      " -1.893e+03 -1.905e+03 -1.906e+03  1.912e+03  1.912e+03  1.913e+03\n",
      " -1.924e+03 -1.928e+03 -1.929e+03 -1.935e+03 -1.946e+03 -1.952e+03\n",
      " -1.955e+03 -1.955e+03  1.964e+03  1.986e+03 -1.997e+03 -2.004e+03\n",
      " -2.014e+03 -2.016e+03 -2.017e+03  2.090e+03  2.105e+03 -2.128e+03\n",
      " -2.133e+03  2.145e+03 -2.150e+03 -2.172e+03 -2.172e+03 -2.184e+03\n",
      " -2.186e+03  2.190e+03 -2.208e+03 -2.217e+03 -2.226e+03 -2.231e+03\n",
      "  2.241e+03 -2.246e+03  2.256e+03 -2.257e+03 -2.259e+03 -2.263e+03\n",
      " -2.270e+03 -2.271e+03 -2.283e+03  2.299e+03 -2.361e+03 -2.372e+03\n",
      " -2.378e+03  2.386e+03 -2.392e+03  2.419e+03 -2.422e+03 -2.423e+03\n",
      " -2.439e+03 -2.461e+03 -2.470e+03 -2.489e+03 -2.504e+03 -2.508e+03\n",
      " -2.552e+03 -2.554e+03  2.564e+03  2.601e+03 -2.609e+03 -2.609e+03\n",
      " -2.630e+03 -2.660e+03 -2.688e+03 -2.722e+03 -2.746e+03  2.752e+03\n",
      " -2.782e+03 -2.799e+03  2.830e+03 -2.839e+03 -2.859e+03 -2.868e+03\n",
      " -2.873e+03 -2.964e+03 -3.037e+03 -3.074e+03 -3.146e+03 -3.205e+03\n",
      " -3.222e+03 -3.267e+03 -3.271e+03 -3.302e+03 -3.328e+03 -3.331e+03\n",
      " -3.341e+03 -3.343e+03 -3.377e+03 -3.392e+03 -3.431e+03 -3.451e+03\n",
      " -3.480e+03 -3.498e+03 -3.519e+03  3.554e+03 -3.583e+03  3.615e+03\n",
      " -3.631e+03 -3.639e+03 -3.728e+03 -3.736e+03 -3.834e+03 -3.936e+03\n",
      " -3.944e+03 -3.974e+03 -3.987e+03 -4.074e+03 -4.537e+03]\n",
      "Concordance Index 0.5868099336659175\n",
      "Integrated Brier Score: 0.38573934371533186\n",
      "y_train breslow final [ 2.000e+00 -3.000e+00 -3.000e+00 -7.000e+00 -1.100e+01 -1.600e+01\n",
      " -1.600e+01 -1.800e+01  1.800e+01 -2.600e+01 -2.900e+01 -3.600e+01\n",
      " -3.800e+01  4.100e+01  4.300e+01  5.900e+01 -6.100e+01  6.200e+01\n",
      "  6.800e+01  7.300e+01  7.700e+01  9.300e+01  9.900e+01  1.010e+02\n",
      "  1.090e+02  1.100e+02 -1.180e+02 -1.190e+02  1.370e+02  1.390e+02\n",
      " -1.550e+02  1.640e+02 -1.660e+02  1.660e+02  1.680e+02 -1.750e+02\n",
      " -1.770e+02 -1.820e+02  1.830e+02 -1.940e+02  2.020e+02  2.040e+02\n",
      " -2.040e+02 -2.050e+02  2.060e+02 -2.060e+02  2.110e+02 -2.110e+02\n",
      "  2.220e+02  2.240e+02  2.450e+02 -2.560e+02 -2.930e+02  3.070e+02\n",
      "  3.110e+02  3.130e+02 -3.190e+02  3.200e+02 -3.230e+02  3.290e+02\n",
      "  3.300e+02  3.330e+02  3.340e+02  3.360e+02 -3.400e+02  3.420e+02\n",
      " -3.550e+02 -3.560e+02  3.620e+02 -3.620e+02 -3.650e+02 -3.690e+02\n",
      " -3.700e+02 -3.720e+02 -3.730e+02 -3.740e+02  3.750e+02 -3.850e+02\n",
      " -3.860e+02 -4.000e+02 -4.080e+02 -4.110e+02  4.310e+02 -4.310e+02\n",
      " -4.330e+02 -4.350e+02  4.460e+02  4.540e+02  4.590e+02 -4.680e+02\n",
      " -4.690e+02  4.750e+02  4.780e+02  4.800e+02  4.800e+02  4.850e+02\n",
      " -4.980e+02 -5.010e+02 -5.050e+02 -5.070e+02  5.100e+02 -5.110e+02\n",
      " -5.190e+02 -5.230e+02 -5.280e+02  5.520e+02  5.610e+02  5.610e+02\n",
      "  5.620e+02  5.630e+02 -5.630e+02 -5.670e+02  5.710e+02  5.720e+02\n",
      " -5.740e+02  5.740e+02  5.870e+02 -5.910e+02  6.000e+02 -6.170e+02\n",
      " -6.200e+02 -6.300e+02  6.370e+02  6.460e+02 -6.480e+02 -6.560e+02\n",
      " -6.620e+02 -6.650e+02 -6.850e+02 -6.930e+02 -6.930e+02 -7.000e+02\n",
      "  7.010e+02  7.090e+02  7.220e+02 -7.220e+02  7.270e+02  7.350e+02\n",
      " -7.350e+02 -7.370e+02 -7.500e+02  7.700e+02 -7.740e+02 -7.770e+02\n",
      "  7.820e+02 -7.880e+02  7.930e+02  8.190e+02  8.220e+02 -8.220e+02\n",
      "  8.280e+02 -8.300e+02  8.340e+02 -8.400e+02  8.410e+02  8.450e+02\n",
      " -8.610e+02 -8.730e+02 -8.740e+02  8.780e+02  8.830e+02  8.850e+02\n",
      "  8.850e+02 -9.190e+02  9.320e+02 -9.320e+02 -9.320e+02  9.460e+02\n",
      " -9.510e+02  9.520e+02 -9.520e+02  9.530e+02 -9.630e+02 -9.670e+02\n",
      " -9.700e+02 -9.720e+02 -9.920e+02 -1.011e+03 -1.014e+03 -1.018e+03\n",
      " -1.018e+03  1.034e+03  1.045e+03 -1.092e+03  1.097e+03 -1.106e+03\n",
      " -1.107e+03  1.111e+03  1.111e+03 -1.120e+03  1.121e+03 -1.124e+03\n",
      " -1.124e+03 -1.126e+03 -1.130e+03 -1.130e+03 -1.132e+03 -1.133e+03\n",
      "  1.133e+03 -1.133e+03 -1.137e+03 -1.140e+03 -1.143e+03 -1.165e+03\n",
      "  1.170e+03 -1.175e+03 -1.177e+03 -1.186e+03  1.191e+03  1.200e+03\n",
      "  1.200e+03 -1.217e+03 -1.218e+03  1.230e+03  1.238e+03  1.270e+03\n",
      " -1.291e+03 -1.299e+03 -1.307e+03 -1.308e+03  1.317e+03  1.337e+03\n",
      "  1.343e+03 -1.355e+03 -1.371e+03 -1.373e+03  1.378e+03 -1.384e+03\n",
      " -1.385e+03 -1.385e+03 -1.398e+03  1.404e+03 -1.411e+03 -1.413e+03\n",
      " -1.416e+03  1.417e+03 -1.423e+03  1.432e+03 -1.433e+03 -1.435e+03\n",
      " -1.436e+03 -1.450e+03 -1.459e+03 -1.462e+03  1.463e+03 -1.471e+03\n",
      " -1.476e+03 -1.485e+03 -1.485e+03 -1.489e+03 -1.491e+03 -1.493e+03\n",
      "  1.493e+03 -1.495e+03 -1.496e+03 -1.499e+03 -1.502e+03 -1.508e+03\n",
      " -1.516e+03 -1.520e+03 -1.520e+03 -1.521e+03 -1.525e+03 -1.525e+03\n",
      " -1.531e+03 -1.559e+03 -1.560e+03  1.567e+03  1.584e+03  1.588e+03\n",
      "  1.590e+03  1.598e+03 -1.604e+03  1.610e+03 -1.621e+03 -1.624e+03\n",
      "  1.625e+03  1.625e+03  1.626e+03 -1.632e+03  1.639e+03 -1.657e+03\n",
      "  1.661e+03 -1.666e+03 -1.670e+03  1.696e+03  1.714e+03  1.724e+03\n",
      " -1.729e+03 -1.731e+03 -1.733e+03 -1.746e+03 -1.755e+03 -1.759e+03\n",
      " -1.778e+03 -1.779e+03 -1.785e+03 -1.787e+03 -1.789e+03 -1.790e+03\n",
      " -1.793e+03 -1.794e+03 -1.834e+03 -1.843e+03 -1.854e+03 -1.862e+03\n",
      " -1.871e+03 -1.876e+03 -1.879e+03 -1.883e+03 -1.885e+03 -1.889e+03\n",
      " -1.893e+03 -1.905e+03 -1.906e+03  1.912e+03  1.912e+03  1.913e+03\n",
      " -1.924e+03 -1.928e+03 -1.929e+03 -1.935e+03 -1.946e+03 -1.952e+03\n",
      " -1.955e+03 -1.955e+03  1.964e+03  1.986e+03 -1.997e+03 -2.004e+03\n",
      " -2.014e+03 -2.016e+03 -2.017e+03  2.090e+03  2.105e+03 -2.128e+03\n",
      " -2.133e+03  2.145e+03 -2.150e+03 -2.172e+03 -2.172e+03 -2.184e+03\n",
      " -2.186e+03  2.190e+03 -2.208e+03 -2.217e+03 -2.226e+03 -2.231e+03\n",
      "  2.241e+03 -2.246e+03  2.256e+03 -2.257e+03 -2.259e+03 -2.263e+03\n",
      " -2.270e+03 -2.271e+03 -2.283e+03  2.299e+03 -2.361e+03 -2.372e+03\n",
      " -2.378e+03  2.386e+03 -2.392e+03  2.419e+03 -2.422e+03 -2.423e+03\n",
      " -2.439e+03 -2.461e+03 -2.470e+03 -2.489e+03 -2.504e+03 -2.508e+03\n",
      " -2.552e+03 -2.554e+03  2.564e+03  2.601e+03 -2.609e+03 -2.609e+03\n",
      " -2.630e+03 -2.660e+03 -2.688e+03 -2.722e+03 -2.746e+03  2.752e+03\n",
      " -2.782e+03 -2.799e+03  2.830e+03 -2.839e+03 -2.859e+03 -2.868e+03\n",
      " -2.873e+03 -2.964e+03 -3.037e+03 -3.074e+03 -3.146e+03 -3.205e+03\n",
      " -3.222e+03 -3.267e+03 -3.271e+03 -3.302e+03 -3.328e+03 -3.331e+03\n",
      " -3.341e+03 -3.343e+03 -3.377e+03 -3.392e+03 -3.431e+03 -3.451e+03\n",
      " -3.480e+03 -3.498e+03 -3.519e+03  3.554e+03 -3.583e+03  3.615e+03\n",
      " -3.631e+03 -3.639e+03 -3.728e+03 -3.736e+03 -3.834e+03 -3.936e+03\n",
      " -3.944e+03 -3.974e+03 -3.987e+03 -4.074e+03 -4.537e+03]\n",
      "durations 13.0 4067.0\n",
      "Concordance Index 0.6036235086168803\n",
      "Integrated Brier Score: 0.3607977922614494\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m94.9736\u001b[0m  0.0037\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2       94.9736  0.0031\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      3       94.9736  0.0030\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m106.0289\u001b[0m  0.0038\n",
      "      4       94.9736  0.0030\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m120.2609\u001b[0m  0.0036\n",
      "      2      106.0289  0.0042\n",
      "      5       94.9736  0.0030\n",
      "      2      120.2609  0.0031\n",
      "      6       94.9736  0.0032\n",
      "      3      120.2609  0.0031\n",
      "      7       94.9736  0.0031\n",
      "      4      120.2609  0.0033\n",
      "      8       94.9736  0.0030\n",
      "      5      120.2609  0.0033\n",
      "      9       94.9736  0.0030\n",
      "      3      106.0289  0.0080\n",
      "      6      120.2609  0.0030\n",
      "     10       94.9736  0.0030\n",
      "Restoring best model from epoch 1.\n",
      "      4      106.0289  0.0033\n",
      "      7      120.2609  0.0030\n",
      "      5      106.0289  0.0032\n",
      "      8      120.2609  0.0031\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      9      120.2609  0.0031\n",
      "      6      106.0289  0.0036\n",
      "      7      106.0289  0.0032\n",
      "     10      120.2609  0.0036\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Restoring best model from epoch 1.\n",
      "      8      106.0289  0.0030\n",
      "      9      106.0289  0.0043\n",
      "     10      106.0289  0.0031\n",
      "Restoring best model from epoch 1.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m80.2278\u001b[0m      \u001b[32m108.4730\u001b[0m  0.1081\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m70.4430\u001b[0m      \u001b[32m139.7341\u001b[0m  0.0958\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m71.4134\u001b[0m  0.0082\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m70.1981\u001b[0m       \u001b[32m56.6886\u001b[0m  0.0868\n",
      "      2       71.4134  0.0040\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m109.5450\u001b[0m  0.0051\n",
      "      3       71.4134  0.0031\n",
      "      2      109.5450  0.0033\n",
      "      3      109.5450  0.0039\n",
      "      4       71.4134  0.0060\n",
      "      4      109.5450  0.0061\n",
      "      5       71.4134  0.0092\n",
      "      5      109.5450  0.0031\n",
      "      6      109.5450  0.0038\n",
      "      6       71.4134  0.0052\n",
      "      7      109.5450  0.0032\n",
      "      8      109.5450  0.0031\n",
      "      7       71.4134  0.0053\n",
      "      9      109.5450  0.0030\n",
      "      8       71.4134  0.0083\n",
      "      9       71.4134  0.0035\n",
      "     10       71.4134  0.0038\n",
      "     10      109.5450  0.0126\n",
      "Restoring best model from epoch 1.\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m64.2133\u001b[0m       \u001b[32m77.2340\u001b[0m  0.1170\n",
      "      2       83.9025      139.1991  0.1193\n",
      "      2       76.3015       \u001b[32m99.3761\u001b[0m  0.1283\n",
      "      2       \u001b[36m67.7895\u001b[0m       \u001b[32m54.3912\u001b[0m  0.1243\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m77.7453\u001b[0m      \u001b[32m178.4938\u001b[0m  0.0971\n",
      "      2       65.5793       \u001b[32m70.1083\u001b[0m  0.0852\n",
      "      3       81.2905      189.5563  0.0867\n",
      "      3       74.6094       \u001b[32m89.0574\u001b[0m  0.0927\n",
      "      3       \u001b[36m64.7104\u001b[0m       57.5390  0.0851\n",
      "      2       \u001b[36m77.0361\u001b[0m      289.4438  0.0786\n",
      "      3       \u001b[36m60.0451\u001b[0m       \u001b[32m66.3296\u001b[0m  0.0771\n",
      "      4       \u001b[36m78.9678\u001b[0m      112.1383  0.0786\n",
      "      4       \u001b[36m70.4352\u001b[0m       \u001b[32m87.7110\u001b[0m  0.0754\n",
      "      4       67.9533       55.9127  0.0750\n",
      "      4       67.3524       66.7449  0.0777\n",
      "      3       86.6854      \u001b[32m113.9324\u001b[0m  0.0819\n",
      "      5       80.9244      \u001b[32m105.1021\u001b[0m  0.0774\n",
      "      5       \u001b[36m67.9413\u001b[0m       88.6261  0.0779\n",
      "      5       \u001b[36m61.8791\u001b[0m       \u001b[32m48.3070\u001b[0m  0.0731\n",
      "      5       66.5442       67.0576  0.0767\n",
      "      4       \u001b[36m76.6880\u001b[0m       \u001b[32m94.9980\u001b[0m  0.0818\n",
      "      6       82.1687       \u001b[32m90.2178\u001b[0m  0.0767\n",
      "      6       74.8558       \u001b[32m85.0514\u001b[0m  0.0780\n",
      "      6       65.8271       50.6011  0.0822\n",
      "      6       62.1687       \u001b[32m57.6172\u001b[0m  0.0747\n",
      "      5       \u001b[36m73.8337\u001b[0m      106.7490  0.0760\n",
      "      7       80.0588       97.4476  0.0743\n",
      "      7       72.1499       \u001b[32m82.1589\u001b[0m  0.0742\n",
      "      7       \u001b[36m59.1101\u001b[0m       51.8488  0.0752\n",
      "      7       62.3303       59.4169  0.0759\n",
      "      8       \u001b[36m75.5786\u001b[0m       95.6068  0.0752\n",
      "      6       \u001b[36m72.6146\u001b[0m      101.3821  0.0764\n",
      "      8       68.4374       87.9955  0.0729\n",
      "      8       61.2830       50.1910  0.0770\n",
      "      8       61.7544       59.5818  0.0747\n",
      "      9       \u001b[36m70.8886\u001b[0m       90.2357  0.0740\n",
      "      7       76.8836       \u001b[32m91.4547\u001b[0m  0.0756\n",
      "      9       \u001b[36m65.5428\u001b[0m       \u001b[32m81.3392\u001b[0m  0.0724\n",
      "      9       \u001b[36m58.5933\u001b[0m       49.5785  0.0751\n",
      "      9       \u001b[36m59.2759\u001b[0m       61.2280  0.0751\n",
      "     10       76.3020       \u001b[32m89.1592\u001b[0m  0.0745\n",
      "      8       81.2469       \u001b[32m90.9843\u001b[0m  0.0783\n",
      "     10       68.8710       \u001b[32m76.3683\u001b[0m  0.0731\n",
      "     10       58.6288       49.4648  0.0745\n",
      "Restoring best model from epoch 5.\n",
      "     10       \u001b[36m57.1586\u001b[0m       63.2028  0.0742\n",
      "Restoring best model from epoch 6.\n",
      "      9       72.8255       \u001b[32m90.1336\u001b[0m  0.0775\n",
      "     10       \u001b[36m69.5877\u001b[0m       \u001b[32m89.9615\u001b[0m  0.0740\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m75.3788\u001b[0m       \u001b[32m60.4321\u001b[0m  0.0738\n",
      "      2       \u001b[36m71.1927\u001b[0m       \u001b[32m53.4200\u001b[0m  0.0654\n",
      "      3       71.5996       65.2122  0.0705\n",
      "      4       71.2991       59.5630  0.0609\n",
      "      5       \u001b[36m66.4741\u001b[0m       59.2798  0.0644\n",
      "      6       68.6928       55.0232  0.0726\n",
      "      7       72.2082       56.4379  0.0748\n",
      "      8       72.3104       55.2937  0.0704\n",
      "      9       66.7817       55.5139  0.0641\n",
      "     10       69.4086       56.7835  0.0671\n",
      "Restoring best model from epoch 2.\n",
      "y_train breslow final [-3.000e+00 -3.000e+00 -7.000e+00 -1.100e+01 -1.300e+01 -1.600e+01\n",
      " -1.600e+01 -1.600e+01 -1.800e+01 -2.900e+01 -3.600e+01 -3.800e+01\n",
      "  4.100e+01  4.200e+01 -4.300e+01  5.100e+01 -5.300e+01  5.900e+01\n",
      " -6.100e+01  6.200e+01  6.500e+01  6.800e+01  6.900e+01  7.300e+01\n",
      "  7.700e+01  9.300e+01  9.900e+01  1.010e+02  1.060e+02  1.090e+02\n",
      "  1.100e+02 -1.180e+02 -1.190e+02 -1.270e+02  1.390e+02 -1.410e+02\n",
      " -1.500e+02 -1.550e+02  1.620e+02  1.640e+02 -1.660e+02  1.660e+02\n",
      "  1.680e+02 -1.750e+02 -1.770e+02 -1.820e+02  1.820e+02  1.830e+02\n",
      " -1.930e+02 -1.940e+02  2.020e+02  2.040e+02 -2.040e+02 -2.050e+02\n",
      "  2.060e+02 -2.060e+02  2.110e+02 -2.110e+02  2.220e+02  2.240e+02\n",
      "  2.380e+02  2.420e+02 -2.560e+02 -2.930e+02  3.110e+02  3.130e+02\n",
      "  3.130e+02 -3.190e+02  3.200e+02 -3.230e+02  3.300e+02  3.330e+02\n",
      " -3.400e+02  3.420e+02  3.440e+02 -3.540e+02 -3.550e+02 -3.560e+02\n",
      "  3.620e+02 -3.650e+02 -3.650e+02 -3.690e+02 -3.720e+02 -3.730e+02\n",
      " -3.850e+02 -4.000e+02 -4.060e+02 -4.080e+02  4.310e+02 -4.310e+02\n",
      " -4.330e+02 -4.350e+02  4.450e+02  4.460e+02  4.540e+02 -4.540e+02\n",
      "  4.590e+02 -4.680e+02 -4.690e+02  4.750e+02  4.800e+02  4.800e+02\n",
      " -4.950e+02 -4.980e+02  5.100e+02 -5.110e+02 -5.230e+02 -5.280e+02\n",
      " -5.510e+02  5.520e+02  5.610e+02  5.610e+02  5.620e+02  5.630e+02\n",
      " -5.630e+02  5.710e+02  5.720e+02  5.740e+02  5.780e+02 -5.910e+02\n",
      "  6.000e+02 -6.030e+02 -6.070e+02 -6.170e+02 -6.200e+02 -6.300e+02\n",
      "  6.450e+02  6.460e+02 -6.480e+02 -6.560e+02 -6.650e+02  6.790e+02\n",
      "  6.830e+02 -6.850e+02 -6.890e+02 -6.930e+02 -6.930e+02 -7.000e+02\n",
      "  7.010e+02  7.090e+02 -7.140e+02  7.220e+02 -7.220e+02  7.270e+02\n",
      "  7.350e+02 -7.350e+02 -7.370e+02 -7.490e+02 -7.500e+02 -7.550e+02\n",
      " -7.620e+02  7.680e+02  7.700e+02 -7.740e+02 -7.770e+02  7.820e+02\n",
      " -7.850e+02 -7.880e+02  8.190e+02  8.220e+02 -8.220e+02  8.280e+02\n",
      " -8.300e+02  8.340e+02 -8.400e+02  8.410e+02  8.450e+02 -8.610e+02\n",
      "  8.660e+02 -8.730e+02 -8.740e+02  8.850e+02  8.850e+02 -9.090e+02\n",
      " -9.100e+02 -9.190e+02  9.270e+02 -9.320e+02 -9.320e+02 -9.450e+02\n",
      "  9.460e+02 -9.510e+02  9.520e+02 -9.520e+02  9.530e+02 -9.630e+02\n",
      " -9.700e+02 -9.720e+02 -9.920e+02  9.920e+02  1.003e+03 -1.011e+03\n",
      " -1.014e+03 -1.018e+03  1.019e+03  1.034e+03 -1.046e+03 -1.063e+03\n",
      " -1.071e+03  1.075e+03  1.091e+03 -1.092e+03  1.092e+03  1.097e+03\n",
      " -1.106e+03 -1.107e+03  1.111e+03 -1.120e+03  1.121e+03 -1.124e+03\n",
      " -1.130e+03 -1.130e+03 -1.132e+03 -1.133e+03 -1.143e+03 -1.165e+03\n",
      " -1.168e+03 -1.169e+03  1.170e+03 -1.175e+03 -1.177e+03  1.191e+03\n",
      "  1.200e+03  1.200e+03 -1.217e+03 -1.218e+03  1.230e+03 -1.238e+03\n",
      "  1.238e+03 -1.266e+03  1.270e+03 -1.274e+03 -1.290e+03 -1.291e+03\n",
      " -1.299e+03 -1.307e+03 -1.308e+03 -1.314e+03  1.317e+03 -1.329e+03\n",
      "  1.337e+03 -1.355e+03 -1.367e+03  1.371e+03 -1.373e+03  1.378e+03\n",
      " -1.380e+03 -1.384e+03 -1.385e+03  1.404e+03 -1.411e+03 -1.413e+03\n",
      " -1.413e+03 -1.423e+03 -1.433e+03 -1.436e+03 -1.450e+03 -1.459e+03\n",
      " -1.462e+03  1.463e+03 -1.471e+03 -1.476e+03 -1.478e+03 -1.485e+03\n",
      " -1.487e+03 -1.489e+03 -1.491e+03  1.493e+03 -1.495e+03 -1.498e+03\n",
      " -1.499e+03 -1.508e+03 -1.520e+03 -1.520e+03 -1.521e+03 -1.525e+03\n",
      " -1.525e+03 -1.528e+03 -1.552e+03 -1.559e+03 -1.560e+03  1.567e+03\n",
      "  1.584e+03  1.589e+03  1.590e+03  1.598e+03 -1.604e+03 -1.608e+03\n",
      "  1.620e+03 -1.624e+03  1.626e+03 -1.632e+03  1.639e+03  1.657e+03\n",
      "  1.661e+03 -1.666e+03 -1.670e+03 -1.683e+03  1.714e+03  1.724e+03\n",
      " -1.731e+03 -1.733e+03 -1.746e+03 -1.755e+03 -1.759e+03 -1.779e+03\n",
      " -1.782e+03 -1.787e+03 -1.789e+03 -1.790e+03 -1.793e+03 -1.794e+03\n",
      " -1.834e+03 -1.843e+03 -1.854e+03 -1.862e+03 -1.871e+03 -1.876e+03\n",
      " -1.883e+03 -1.885e+03 -1.886e+03 -1.888e+03 -1.889e+03 -1.893e+03\n",
      " -1.905e+03 -1.906e+03  1.912e+03  1.912e+03  1.913e+03 -1.929e+03\n",
      " -1.935e+03 -1.946e+03 -1.952e+03 -1.955e+03  1.980e+03  1.986e+03\n",
      " -1.993e+03 -1.997e+03 -2.004e+03 -2.009e+03 -2.014e+03 -2.016e+03\n",
      " -2.017e+03 -2.038e+03 -2.067e+03 -2.080e+03 -2.087e+03  2.105e+03\n",
      " -2.128e+03  2.145e+03 -2.150e+03 -2.172e+03 -2.184e+03 -2.217e+03\n",
      " -2.226e+03  2.227e+03  2.256e+03 -2.259e+03 -2.270e+03 -2.271e+03\n",
      " -2.274e+03  2.299e+03  2.343e+03 -2.361e+03 -2.372e+03  2.386e+03\n",
      " -2.412e+03  2.419e+03 -2.422e+03 -2.423e+03 -2.430e+03 -2.439e+03\n",
      "  2.454e+03 -2.461e+03 -2.470e+03 -2.489e+03 -2.508e+03 -2.531e+03\n",
      " -2.552e+03  2.601e+03 -2.609e+03 -2.630e+03 -2.660e+03 -2.688e+03\n",
      " -2.718e+03 -2.746e+03  2.752e+03 -2.754e+03  2.764e+03 -2.789e+03\n",
      " -2.799e+03  2.830e+03 -2.839e+03 -2.859e+03 -2.865e+03 -2.881e+03\n",
      " -3.037e+03 -3.146e+03 -3.205e+03 -3.222e+03 -3.229e+03 -3.267e+03\n",
      " -3.271e+03 -3.328e+03 -3.331e+03 -3.343e+03 -3.409e+03 -3.431e+03\n",
      " -3.480e+03 -3.498e+03  3.554e+03 -3.583e+03  3.615e+03 -3.631e+03\n",
      " -3.728e+03 -3.736e+03 -3.744e+03 -3.841e+03 -3.936e+03 -3.974e+03\n",
      " -3.987e+03 -3.989e+03 -4.067e+03 -4.074e+03 -4.537e+03]\n",
      "Concordance Index 0.6910205109550326\n",
      "Integrated Brier Score: 0.2156189454760904\n",
      "y_train breslow final [-3.000e+00 -3.000e+00 -7.000e+00 -1.100e+01 -1.300e+01 -1.600e+01\n",
      " -1.600e+01 -1.600e+01 -1.800e+01 -2.900e+01 -3.600e+01 -3.800e+01\n",
      "  4.100e+01  4.200e+01 -4.300e+01  5.100e+01 -5.300e+01  5.900e+01\n",
      " -6.100e+01  6.200e+01  6.500e+01  6.800e+01  6.900e+01  7.300e+01\n",
      "  7.700e+01  9.300e+01  9.900e+01  1.010e+02  1.060e+02  1.090e+02\n",
      "  1.100e+02 -1.180e+02 -1.190e+02 -1.270e+02  1.390e+02 -1.410e+02\n",
      " -1.500e+02 -1.550e+02  1.620e+02  1.640e+02 -1.660e+02  1.660e+02\n",
      "  1.680e+02 -1.750e+02 -1.770e+02 -1.820e+02  1.820e+02  1.830e+02\n",
      " -1.930e+02 -1.940e+02  2.020e+02  2.040e+02 -2.040e+02 -2.050e+02\n",
      "  2.060e+02 -2.060e+02  2.110e+02 -2.110e+02  2.220e+02  2.240e+02\n",
      "  2.380e+02  2.420e+02 -2.560e+02 -2.930e+02  3.110e+02  3.130e+02\n",
      "  3.130e+02 -3.190e+02  3.200e+02 -3.230e+02  3.300e+02  3.330e+02\n",
      " -3.400e+02  3.420e+02  3.440e+02 -3.540e+02 -3.550e+02 -3.560e+02\n",
      "  3.620e+02 -3.650e+02 -3.650e+02 -3.690e+02 -3.720e+02 -3.730e+02\n",
      " -3.850e+02 -4.000e+02 -4.060e+02 -4.080e+02  4.310e+02 -4.310e+02\n",
      " -4.330e+02 -4.350e+02  4.450e+02  4.460e+02  4.540e+02 -4.540e+02\n",
      "  4.590e+02 -4.680e+02 -4.690e+02  4.750e+02  4.800e+02  4.800e+02\n",
      " -4.950e+02 -4.980e+02  5.100e+02 -5.110e+02 -5.230e+02 -5.280e+02\n",
      " -5.510e+02  5.520e+02  5.610e+02  5.610e+02  5.620e+02  5.630e+02\n",
      " -5.630e+02  5.710e+02  5.720e+02  5.740e+02  5.780e+02 -5.910e+02\n",
      "  6.000e+02 -6.030e+02 -6.070e+02 -6.170e+02 -6.200e+02 -6.300e+02\n",
      "  6.450e+02  6.460e+02 -6.480e+02 -6.560e+02 -6.650e+02  6.790e+02\n",
      "  6.830e+02 -6.850e+02 -6.890e+02 -6.930e+02 -6.930e+02 -7.000e+02\n",
      "  7.010e+02  7.090e+02 -7.140e+02  7.220e+02 -7.220e+02  7.270e+02\n",
      "  7.350e+02 -7.350e+02 -7.370e+02 -7.490e+02 -7.500e+02 -7.550e+02\n",
      " -7.620e+02  7.680e+02  7.700e+02 -7.740e+02 -7.770e+02  7.820e+02\n",
      " -7.850e+02 -7.880e+02  8.190e+02  8.220e+02 -8.220e+02  8.280e+02\n",
      " -8.300e+02  8.340e+02 -8.400e+02  8.410e+02  8.450e+02 -8.610e+02\n",
      "  8.660e+02 -8.730e+02 -8.740e+02  8.850e+02  8.850e+02 -9.090e+02\n",
      " -9.100e+02 -9.190e+02  9.270e+02 -9.320e+02 -9.320e+02 -9.450e+02\n",
      "  9.460e+02 -9.510e+02  9.520e+02 -9.520e+02  9.530e+02 -9.630e+02\n",
      " -9.700e+02 -9.720e+02 -9.920e+02  9.920e+02  1.003e+03 -1.011e+03\n",
      " -1.014e+03 -1.018e+03  1.019e+03  1.034e+03 -1.046e+03 -1.063e+03\n",
      " -1.071e+03  1.075e+03  1.091e+03 -1.092e+03  1.092e+03  1.097e+03\n",
      " -1.106e+03 -1.107e+03  1.111e+03 -1.120e+03  1.121e+03 -1.124e+03\n",
      " -1.130e+03 -1.130e+03 -1.132e+03 -1.133e+03 -1.143e+03 -1.165e+03\n",
      " -1.168e+03 -1.169e+03  1.170e+03 -1.175e+03 -1.177e+03  1.191e+03\n",
      "  1.200e+03  1.200e+03 -1.217e+03 -1.218e+03  1.230e+03 -1.238e+03\n",
      "  1.238e+03 -1.266e+03  1.270e+03 -1.274e+03 -1.290e+03 -1.291e+03\n",
      " -1.299e+03 -1.307e+03 -1.308e+03 -1.314e+03  1.317e+03 -1.329e+03\n",
      "  1.337e+03 -1.355e+03 -1.367e+03  1.371e+03 -1.373e+03  1.378e+03\n",
      " -1.380e+03 -1.384e+03 -1.385e+03  1.404e+03 -1.411e+03 -1.413e+03\n",
      " -1.413e+03 -1.423e+03 -1.433e+03 -1.436e+03 -1.450e+03 -1.459e+03\n",
      " -1.462e+03  1.463e+03 -1.471e+03 -1.476e+03 -1.478e+03 -1.485e+03\n",
      " -1.487e+03 -1.489e+03 -1.491e+03  1.493e+03 -1.495e+03 -1.498e+03\n",
      " -1.499e+03 -1.508e+03 -1.520e+03 -1.520e+03 -1.521e+03 -1.525e+03\n",
      " -1.525e+03 -1.528e+03 -1.552e+03 -1.559e+03 -1.560e+03  1.567e+03\n",
      "  1.584e+03  1.589e+03  1.590e+03  1.598e+03 -1.604e+03 -1.608e+03\n",
      "  1.620e+03 -1.624e+03  1.626e+03 -1.632e+03  1.639e+03  1.657e+03\n",
      "  1.661e+03 -1.666e+03 -1.670e+03 -1.683e+03  1.714e+03  1.724e+03\n",
      " -1.731e+03 -1.733e+03 -1.746e+03 -1.755e+03 -1.759e+03 -1.779e+03\n",
      " -1.782e+03 -1.787e+03 -1.789e+03 -1.790e+03 -1.793e+03 -1.794e+03\n",
      " -1.834e+03 -1.843e+03 -1.854e+03 -1.862e+03 -1.871e+03 -1.876e+03\n",
      " -1.883e+03 -1.885e+03 -1.886e+03 -1.888e+03 -1.889e+03 -1.893e+03\n",
      " -1.905e+03 -1.906e+03  1.912e+03  1.912e+03  1.913e+03 -1.929e+03\n",
      " -1.935e+03 -1.946e+03 -1.952e+03 -1.955e+03  1.980e+03  1.986e+03\n",
      " -1.993e+03 -1.997e+03 -2.004e+03 -2.009e+03 -2.014e+03 -2.016e+03\n",
      " -2.017e+03 -2.038e+03 -2.067e+03 -2.080e+03 -2.087e+03  2.105e+03\n",
      " -2.128e+03  2.145e+03 -2.150e+03 -2.172e+03 -2.184e+03 -2.217e+03\n",
      " -2.226e+03  2.227e+03  2.256e+03 -2.259e+03 -2.270e+03 -2.271e+03\n",
      " -2.274e+03  2.299e+03  2.343e+03 -2.361e+03 -2.372e+03  2.386e+03\n",
      " -2.412e+03  2.419e+03 -2.422e+03 -2.423e+03 -2.430e+03 -2.439e+03\n",
      "  2.454e+03 -2.461e+03 -2.470e+03 -2.489e+03 -2.508e+03 -2.531e+03\n",
      " -2.552e+03  2.601e+03 -2.609e+03 -2.630e+03 -2.660e+03 -2.688e+03\n",
      " -2.718e+03 -2.746e+03  2.752e+03 -2.754e+03  2.764e+03 -2.789e+03\n",
      " -2.799e+03  2.830e+03 -2.839e+03 -2.859e+03 -2.865e+03 -2.881e+03\n",
      " -3.037e+03 -3.146e+03 -3.205e+03 -3.222e+03 -3.229e+03 -3.267e+03\n",
      " -3.271e+03 -3.328e+03 -3.331e+03 -3.343e+03 -3.409e+03 -3.431e+03\n",
      " -3.480e+03 -3.498e+03  3.554e+03 -3.583e+03  3.615e+03 -3.631e+03\n",
      " -3.728e+03 -3.736e+03 -3.744e+03 -3.841e+03 -3.936e+03 -3.974e+03\n",
      " -3.987e+03 -3.989e+03 -4.067e+03 -4.074e+03 -4.537e+03]\n",
      "durations 2.0 3944.0\n",
      "Concordance Index 0.6515580736543909\n",
      "Integrated Brier Score: 0.18513830856968377\n",
      "split gex_?|100130426      float32\n",
      "gex_?|100133144      float32\n",
      "gex_?|100134869      float32\n",
      "gex_?|10357          float32\n",
      "gex_?|10431          float32\n",
      "                      ...   \n",
      "gex_ZYG11A|440590    float32\n",
      "gex_ZYG11B|79699     float32\n",
      "gex_ZYX|7791         float32\n",
      "gex_ZZEF1|23140      float32\n",
      "gex_ZZZ3|26009       float32\n",
      "Length: 20531, dtype: object\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m79.4129\u001b[0m  0.0046\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      2       79.4129  0.0031\n",
      "      3       79.4129  0.0029\n",
      "      4       79.4129  0.0028\n",
      "      5       79.4129  0.0032\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      6       79.4129  0.0031\n",
      "      7       79.4129  0.0148\n",
      "      8       79.4129  0.0031\n",
      "      9       79.4129  0.0029\n",
      "     10       79.4129  0.0043\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m41.8809\u001b[0m  0.0042\n",
      "      2       41.8809  0.0029\n",
      "      3       41.8809  0.0027\n",
      "      4       41.8809  0.0028\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      5       41.8809  0.0029\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      6       41.8809  0.0029\n",
      "      7       41.8809  0.0027\n",
      "      8       41.8809  0.0027\n",
      "      9       41.8809  0.0035\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "     10       41.8809  0.0042\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m70.0404\u001b[0m  0.0045\n",
      "      2       70.0404  0.0032\n",
      "      3       70.0404  0.0031\n",
      "      4       70.0404  0.0037\n",
      "      5       70.0404  0.0032\n",
      "      6       70.0404  0.0029\n",
      "      7       70.0404  0.0035\n",
      "      8       70.0404  0.0029\n",
      "      9       70.0404  0.0029\n",
      "     10       70.0404  0.0031\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m41.3467\u001b[0m  0.0044\n",
      "      2       41.3467  0.0029\n",
      "      3       41.3467  0.0028\n",
      "      4       41.3467  0.0030\n",
      "      5       41.3467  0.0028\n",
      "      6       41.3467  0.0027\n",
      "      7       41.3467  0.0027\n",
      "      8       41.3467  0.0027\n",
      "      9       41.3467  0.0029\n",
      "     10       41.3467  0.0029\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m81.6915\u001b[0m  0.0045\n",
      "      2       81.6915  0.0032\n",
      "      3       81.6915  0.0035\n",
      "      4       81.6915  0.0030\n",
      "      5       81.6915  0.0029\n",
      "      6       81.6915  0.0029\n",
      "      7       81.6915  0.0029\n",
      "      8       81.6915  0.0029\n",
      "      9       81.6915  0.0028\n",
      "     10       81.6915  0.0029\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m35.8106\u001b[0m  0.0058\n",
      "      2       35.8106  0.0044\n",
      "      3       35.8106  0.0036\n",
      "      4       35.8106  0.0039\n",
      "      5       35.8106  0.0038\n",
      "      6       35.8106  0.0034\n",
      "      7       35.8106  0.0033\n",
      "      8       35.8106  0.0048\n",
      "      9       35.8106  0.0031\n",
      "     10       35.8106  0.0050\n",
      "Restoring best model from epoch 1.\n",
      "y_train breslow final [-1.000e+00 -2.000e+00 -3.000e+00 -3.000e+00 -3.000e+00 -3.000e+00\n",
      " -3.000e+00 -3.000e+00 -3.000e+00 -4.000e+00 -4.000e+00 -5.000e+00\n",
      " -6.000e+00 -7.000e+00 -7.000e+00 -7.000e+00 -7.000e+00  7.000e+00\n",
      " -8.000e+00 -1.400e+01 -1.700e+01  2.300e+01 -2.300e+01  3.700e+01\n",
      " -3.900e+01 -5.500e+01 -6.200e+01 -6.300e+01 -7.000e+01 -7.200e+01\n",
      " -7.300e+01 -7.400e+01 -8.200e+01 -8.400e+01 -9.000e+01  9.600e+01\n",
      " -1.050e+02  1.110e+02 -1.120e+02  1.130e+02 -1.390e+02 -1.530e+02\n",
      " -1.620e+02 -1.660e+02 -1.740e+02  1.780e+02 -1.790e+02 -1.820e+02\n",
      " -1.840e+02 -1.900e+02 -1.940e+02 -1.940e+02  1.940e+02  1.990e+02\n",
      " -1.990e+02 -2.030e+02  2.050e+02 -2.100e+02  2.140e+02  2.280e+02\n",
      " -2.300e+02 -2.310e+02  2.340e+02  2.400e+02  2.410e+02 -2.420e+02\n",
      " -2.430e+02  2.450e+02  2.690e+02 -2.740e+02 -2.790e+02 -2.860e+02\n",
      " -2.920e+02 -3.010e+02 -3.130e+02  3.160e+02 -3.170e+02 -3.200e+02\n",
      " -3.260e+02 -3.280e+02 -3.330e+02 -3.360e+02 -3.370e+02 -3.420e+02\n",
      " -3.430e+02  3.470e+02  3.490e+02 -3.540e+02 -3.720e+02  3.720e+02\n",
      " -3.720e+02  3.780e+02  3.980e+02 -4.030e+02 -4.080e+02 -4.110e+02\n",
      " -4.140e+02 -4.140e+02 -4.160e+02 -4.170e+02 -4.180e+02 -4.190e+02\n",
      " -4.280e+02 -4.310e+02 -4.330e+02 -4.340e+02  4.350e+02  4.380e+02\n",
      " -4.380e+02 -4.380e+02 -4.420e+02 -4.430e+02 -4.490e+02 -4.510e+02\n",
      " -4.540e+02 -4.540e+02 -4.550e+02 -4.550e+02  4.560e+02 -4.570e+02\n",
      " -4.580e+02 -4.610e+02 -4.610e+02 -4.670e+02 -4.670e+02 -4.710e+02\n",
      " -4.780e+02 -4.820e+02 -4.860e+02 -4.870e+02 -4.910e+02  4.920e+02\n",
      " -4.920e+02 -4.940e+02 -4.940e+02 -5.000e+02 -5.020e+02 -5.080e+02\n",
      " -5.090e+02  5.120e+02 -5.150e+02 -5.160e+02 -5.220e+02 -5.230e+02\n",
      " -5.230e+02 -5.260e+02 -5.300e+02  5.310e+02 -5.310e+02 -5.320e+02\n",
      " -5.330e+02 -5.330e+02  5.370e+02  5.380e+02 -5.420e+02 -5.440e+02\n",
      " -5.440e+02  5.470e+02  5.590e+02  5.610e+02 -5.630e+02 -5.640e+02\n",
      " -5.660e+02 -5.660e+02 -5.660e+02 -5.670e+02 -5.710e+02 -5.710e+02\n",
      "  5.760e+02 -5.760e+02  5.780e+02 -5.820e+02 -5.840e+02 -5.850e+02\n",
      " -5.880e+02 -5.960e+02 -5.990e+02 -6.040e+02  6.050e+02  6.070e+02\n",
      " -6.080e+02 -6.090e+02 -6.090e+02 -6.110e+02 -6.110e+02 -6.150e+02\n",
      " -6.220e+02 -6.230e+02 -6.270e+02 -6.290e+02 -6.290e+02 -6.300e+02\n",
      " -6.330e+02  6.390e+02  6.480e+02  6.480e+02 -6.510e+02 -6.510e+02\n",
      " -6.550e+02 -6.560e+02 -6.710e+02 -6.770e+02 -6.780e+02  6.820e+02\n",
      " -6.850e+02 -6.860e+02 -7.050e+02 -7.060e+02 -7.060e+02 -7.140e+02\n",
      " -7.150e+02 -7.180e+02 -7.210e+02  7.220e+02 -7.240e+02  7.270e+02\n",
      " -7.350e+02 -7.360e+02 -7.380e+02  7.420e+02 -7.480e+02 -7.540e+02\n",
      "  7.580e+02 -7.580e+02 -7.600e+02  7.750e+02  7.750e+02 -7.770e+02\n",
      " -7.850e+02 -7.870e+02  7.880e+02 -7.920e+02 -7.950e+02 -7.960e+02\n",
      " -8.000e+02  8.140e+02  8.140e+02  8.190e+02  8.210e+02 -8.260e+02\n",
      " -8.350e+02 -8.370e+02 -8.460e+02 -8.460e+02 -8.540e+02 -8.540e+02\n",
      " -8.600e+02 -8.620e+02 -8.680e+02  8.860e+02 -8.890e+02 -9.000e+02\n",
      " -9.080e+02 -9.080e+02 -9.140e+02  9.190e+02 -9.260e+02  9.330e+02\n",
      " -9.330e+02 -9.350e+02 -9.390e+02 -9.460e+02 -9.490e+02  9.540e+02\n",
      " -9.560e+02  9.610e+02 -9.620e+02  9.620e+02 -9.640e+02 -9.640e+02\n",
      "  9.840e+02  9.870e+02 -9.920e+02 -9.920e+02 -9.930e+02 -1.004e+03\n",
      "  1.011e+03 -1.021e+03 -1.026e+03 -1.032e+03  1.033e+03 -1.040e+03\n",
      " -1.058e+03 -1.069e+03 -1.076e+03 -1.078e+03 -1.079e+03  1.106e+03\n",
      " -1.116e+03 -1.120e+03  1.120e+03  1.120e+03 -1.127e+03  1.137e+03\n",
      " -1.137e+03 -1.139e+03 -1.147e+03  1.152e+03 -1.164e+03 -1.173e+03\n",
      "  1.183e+03 -1.189e+03 -1.191e+03 -1.201e+03  1.209e+03 -1.210e+03\n",
      " -1.222e+03 -1.229e+03 -1.236e+03 -1.245e+03 -1.250e+03 -1.257e+03\n",
      " -1.277e+03 -1.279e+03 -1.294e+03 -1.294e+03 -1.300e+03 -1.301e+03\n",
      " -1.314e+03 -1.320e+03  1.335e+03 -1.337e+03  1.339e+03 -1.341e+03\n",
      "  1.351e+03 -1.354e+03 -1.359e+03 -1.387e+03 -1.393e+03 -1.397e+03\n",
      " -1.399e+03  1.401e+03 -1.401e+03 -1.426e+03 -1.428e+03 -1.458e+03\n",
      " -1.469e+03  1.481e+03 -1.494e+03 -1.500e+03 -1.540e+03  1.547e+03\n",
      " -1.553e+03 -1.567e+03 -1.568e+03 -1.588e+03 -1.631e+03 -1.650e+03\n",
      "  1.666e+03 -1.706e+03 -1.721e+03 -1.752e+03  1.762e+03 -1.796e+03\n",
      " -1.834e+03 -1.868e+03  1.886e+03  1.891e+03  1.915e+03  1.933e+03\n",
      " -1.943e+03 -1.989e+03  2.000e+03 -2.078e+03 -2.219e+03  2.235e+03\n",
      "  2.282e+03 -2.287e+03 -2.289e+03  2.379e+03 -2.381e+03  2.433e+03\n",
      " -2.493e+03 -2.565e+03 -2.585e+03 -2.602e+03 -2.650e+03 -2.702e+03\n",
      " -2.761e+03 -2.772e+03  2.835e+03 -2.860e+03 -2.869e+03  2.875e+03\n",
      " -2.893e+03  2.907e+03 -2.918e+03 -2.918e+03  2.988e+03 -3.000e+03\n",
      " -3.013e+03  3.200e+03 -3.253e+03  3.571e+03 -3.574e+03 -3.725e+03\n",
      " -3.761e+03  3.978e+03  4.068e+03  4.084e+03  4.229e+03  4.412e+03\n",
      "  4.445e+03 -4.752e+03  5.166e+03 -5.255e+03 -5.546e+03 -6.423e+03]\n",
      "Concordance Index 0.6317709484855185\n",
      "Integrated Brier Score: 0.2920227544007943\n",
      "y_train breslow final [-1.000e+00 -2.000e+00 -3.000e+00 -3.000e+00 -3.000e+00 -3.000e+00\n",
      " -3.000e+00 -3.000e+00 -3.000e+00 -4.000e+00 -4.000e+00 -5.000e+00\n",
      " -6.000e+00 -7.000e+00 -7.000e+00 -7.000e+00 -7.000e+00  7.000e+00\n",
      " -8.000e+00 -1.400e+01 -1.700e+01  2.300e+01 -2.300e+01  3.700e+01\n",
      " -3.900e+01 -5.500e+01 -6.200e+01 -6.300e+01 -7.000e+01 -7.200e+01\n",
      " -7.300e+01 -7.400e+01 -8.200e+01 -8.400e+01 -9.000e+01  9.600e+01\n",
      " -1.050e+02  1.110e+02 -1.120e+02  1.130e+02 -1.390e+02 -1.530e+02\n",
      " -1.620e+02 -1.660e+02 -1.740e+02  1.780e+02 -1.790e+02 -1.820e+02\n",
      " -1.840e+02 -1.900e+02 -1.940e+02 -1.940e+02  1.940e+02  1.990e+02\n",
      " -1.990e+02 -2.030e+02  2.050e+02 -2.100e+02  2.140e+02  2.280e+02\n",
      " -2.300e+02 -2.310e+02  2.340e+02  2.400e+02  2.410e+02 -2.420e+02\n",
      " -2.430e+02  2.450e+02  2.690e+02 -2.740e+02 -2.790e+02 -2.860e+02\n",
      " -2.920e+02 -3.010e+02 -3.130e+02  3.160e+02 -3.170e+02 -3.200e+02\n",
      " -3.260e+02 -3.280e+02 -3.330e+02 -3.360e+02 -3.370e+02 -3.420e+02\n",
      " -3.430e+02  3.470e+02  3.490e+02 -3.540e+02 -3.720e+02  3.720e+02\n",
      " -3.720e+02  3.780e+02  3.980e+02 -4.030e+02 -4.080e+02 -4.110e+02\n",
      " -4.140e+02 -4.140e+02 -4.160e+02 -4.170e+02 -4.180e+02 -4.190e+02\n",
      " -4.280e+02 -4.310e+02 -4.330e+02 -4.340e+02  4.350e+02  4.380e+02\n",
      " -4.380e+02 -4.380e+02 -4.420e+02 -4.430e+02 -4.490e+02 -4.510e+02\n",
      " -4.540e+02 -4.540e+02 -4.550e+02 -4.550e+02  4.560e+02 -4.570e+02\n",
      " -4.580e+02 -4.610e+02 -4.610e+02 -4.670e+02 -4.670e+02 -4.710e+02\n",
      " -4.780e+02 -4.820e+02 -4.860e+02 -4.870e+02 -4.910e+02  4.920e+02\n",
      " -4.920e+02 -4.940e+02 -4.940e+02 -5.000e+02 -5.020e+02 -5.080e+02\n",
      " -5.090e+02  5.120e+02 -5.150e+02 -5.160e+02 -5.220e+02 -5.230e+02\n",
      " -5.230e+02 -5.260e+02 -5.300e+02  5.310e+02 -5.310e+02 -5.320e+02\n",
      " -5.330e+02 -5.330e+02  5.370e+02  5.380e+02 -5.420e+02 -5.440e+02\n",
      " -5.440e+02  5.470e+02  5.590e+02  5.610e+02 -5.630e+02 -5.640e+02\n",
      " -5.660e+02 -5.660e+02 -5.660e+02 -5.670e+02 -5.710e+02 -5.710e+02\n",
      "  5.760e+02 -5.760e+02  5.780e+02 -5.820e+02 -5.840e+02 -5.850e+02\n",
      " -5.880e+02 -5.960e+02 -5.990e+02 -6.040e+02  6.050e+02  6.070e+02\n",
      " -6.080e+02 -6.090e+02 -6.090e+02 -6.110e+02 -6.110e+02 -6.150e+02\n",
      " -6.220e+02 -6.230e+02 -6.270e+02 -6.290e+02 -6.290e+02 -6.300e+02\n",
      " -6.330e+02  6.390e+02  6.480e+02  6.480e+02 -6.510e+02 -6.510e+02\n",
      " -6.550e+02 -6.560e+02 -6.710e+02 -6.770e+02 -6.780e+02  6.820e+02\n",
      " -6.850e+02 -6.860e+02 -7.050e+02 -7.060e+02 -7.060e+02 -7.140e+02\n",
      " -7.150e+02 -7.180e+02 -7.210e+02  7.220e+02 -7.240e+02  7.270e+02\n",
      " -7.350e+02 -7.360e+02 -7.380e+02  7.420e+02 -7.480e+02 -7.540e+02\n",
      "  7.580e+02 -7.580e+02 -7.600e+02  7.750e+02  7.750e+02 -7.770e+02\n",
      " -7.850e+02 -7.870e+02  7.880e+02 -7.920e+02 -7.950e+02 -7.960e+02\n",
      " -8.000e+02  8.140e+02  8.140e+02  8.190e+02  8.210e+02 -8.260e+02\n",
      " -8.350e+02 -8.370e+02 -8.460e+02 -8.460e+02 -8.540e+02 -8.540e+02\n",
      " -8.600e+02 -8.620e+02 -8.680e+02  8.860e+02 -8.890e+02 -9.000e+02\n",
      " -9.080e+02 -9.080e+02 -9.140e+02  9.190e+02 -9.260e+02  9.330e+02\n",
      " -9.330e+02 -9.350e+02 -9.390e+02 -9.460e+02 -9.490e+02  9.540e+02\n",
      " -9.560e+02  9.610e+02 -9.620e+02  9.620e+02 -9.640e+02 -9.640e+02\n",
      "  9.840e+02  9.870e+02 -9.920e+02 -9.920e+02 -9.930e+02 -1.004e+03\n",
      "  1.011e+03 -1.021e+03 -1.026e+03 -1.032e+03  1.033e+03 -1.040e+03\n",
      " -1.058e+03 -1.069e+03 -1.076e+03 -1.078e+03 -1.079e+03  1.106e+03\n",
      " -1.116e+03 -1.120e+03  1.120e+03  1.120e+03 -1.127e+03  1.137e+03\n",
      " -1.137e+03 -1.139e+03 -1.147e+03  1.152e+03 -1.164e+03 -1.173e+03\n",
      "  1.183e+03 -1.189e+03 -1.191e+03 -1.201e+03  1.209e+03 -1.210e+03\n",
      " -1.222e+03 -1.229e+03 -1.236e+03 -1.245e+03 -1.250e+03 -1.257e+03\n",
      " -1.277e+03 -1.279e+03 -1.294e+03 -1.294e+03 -1.300e+03 -1.301e+03\n",
      " -1.314e+03 -1.320e+03  1.335e+03 -1.337e+03  1.339e+03 -1.341e+03\n",
      "  1.351e+03 -1.354e+03 -1.359e+03 -1.387e+03 -1.393e+03 -1.397e+03\n",
      " -1.399e+03  1.401e+03 -1.401e+03 -1.426e+03 -1.428e+03 -1.458e+03\n",
      " -1.469e+03  1.481e+03 -1.494e+03 -1.500e+03 -1.540e+03  1.547e+03\n",
      " -1.553e+03 -1.567e+03 -1.568e+03 -1.588e+03 -1.631e+03 -1.650e+03\n",
      "  1.666e+03 -1.706e+03 -1.721e+03 -1.752e+03  1.762e+03 -1.796e+03\n",
      " -1.834e+03 -1.868e+03  1.886e+03  1.891e+03  1.915e+03  1.933e+03\n",
      " -1.943e+03 -1.989e+03  2.000e+03 -2.078e+03 -2.219e+03  2.235e+03\n",
      "  2.282e+03 -2.287e+03 -2.289e+03  2.379e+03 -2.381e+03  2.433e+03\n",
      " -2.493e+03 -2.565e+03 -2.585e+03 -2.602e+03 -2.650e+03 -2.702e+03\n",
      " -2.761e+03 -2.772e+03  2.835e+03 -2.860e+03 -2.869e+03  2.875e+03\n",
      " -2.893e+03  2.907e+03 -2.918e+03 -2.918e+03  2.988e+03 -3.000e+03\n",
      " -3.013e+03  3.200e+03 -3.253e+03  3.571e+03 -3.574e+03 -3.725e+03\n",
      " -3.761e+03  3.978e+03  4.068e+03  4.084e+03  4.229e+03  4.412e+03\n",
      "  4.445e+03 -4.752e+03  5.166e+03 -5.255e+03 -5.546e+03 -6.423e+03]\n",
      "durations 3.0 4695.0\n",
      "Concordance Index 0.5705824284304047\n",
      "Integrated Brier Score: 0.27941860633833004\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m304.7208\u001b[0m       \u001b[32m52.5646\u001b[0m  0.0414\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m295.0052\u001b[0m       \u001b[32m91.1413\u001b[0m  0.0184\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m255.7170\u001b[0m       \u001b[32m60.5453\u001b[0m  0.0177\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2      \u001b[36m287.2098\u001b[0m       \u001b[32m81.0462\u001b[0m  0.0174\n",
      "      2      \u001b[36m234.0378\u001b[0m       61.9533  0.0182\n",
      "      2      305.8200       \u001b[32m51.7635\u001b[0m  0.0335\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m294.1860\u001b[0m       \u001b[32m81.2273\u001b[0m  0.0227\n",
      "      3      292.2438       \u001b[32m72.7930\u001b[0m  0.0228\n",
      "      3      234.3233       62.4849  0.0194\n",
      "      2      \u001b[36m271.9681\u001b[0m       \u001b[32m79.9505\u001b[0m  0.0186\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m308.5206\u001b[0m       \u001b[32m55.7313\u001b[0m  0.0456\n",
      "      4      \u001b[36m233.7416\u001b[0m       \u001b[32m59.1908\u001b[0m  0.0152\n",
      "      4      \u001b[36m285.2092\u001b[0m       \u001b[32m68.0310\u001b[0m  0.0188\n",
      "      3      312.3458       52.4522  0.0333\n",
      "      3      274.0356       \u001b[32m78.5930\u001b[0m  0.0158\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m283.7347\u001b[0m       \u001b[32m62.4660\u001b[0m  0.0608\n",
      "      5      233.8646       \u001b[32m55.8995\u001b[0m  0.0148\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m310.9779\u001b[0m       \u001b[32m54.8556\u001b[0m  0.0208\n",
      "      5      285.8224       \u001b[32m65.3564\u001b[0m  0.0172\n",
      "      4      \u001b[36m269.1288\u001b[0m       \u001b[32m76.0607\u001b[0m  0.0163\n",
      "      6      \u001b[36m221.0741\u001b[0m       \u001b[32m53.3862\u001b[0m  0.0160\n",
      "      2      \u001b[36m296.0523\u001b[0m       57.2639  0.0202\n",
      "      2      323.1052       \u001b[32m54.4511\u001b[0m  0.0345\n",
      "      6      \u001b[36m279.6259\u001b[0m       \u001b[32m64.2669\u001b[0m  0.0163\n",
      "      5      \u001b[36m267.8598\u001b[0m       \u001b[32m72.1856\u001b[0m  0.0159\n",
      "      4      \u001b[36m291.5710\u001b[0m       54.4555  0.0406\n",
      "      2      291.4144       \u001b[32m61.3193\u001b[0m  0.0341\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m240.3015\u001b[0m       \u001b[32m50.8829\u001b[0m  0.0438\n",
      "      7      \u001b[36m221.0649\u001b[0m       \u001b[32m51.7582\u001b[0m  0.0157\n",
      "      3      \u001b[36m293.6528\u001b[0m       59.0995  0.0157\n",
      "      7      \u001b[36m279.0596\u001b[0m       \u001b[32m64.0733\u001b[0m  0.0157\n",
      "      6      \u001b[36m260.7962\u001b[0m       \u001b[32m69.4438\u001b[0m  0.0155\n",
      "      8      \u001b[36m220.6822\u001b[0m       \u001b[32m50.8597\u001b[0m  0.0151\n",
      "      4      \u001b[36m284.8959\u001b[0m       59.3485  0.0155\n",
      "      8      \u001b[36m272.4774\u001b[0m       \u001b[32m64.0473\u001b[0m  0.0154\n",
      "      3      319.6830       54.7824  0.0381\n",
      "      5      304.0725       55.4451  0.0330\n",
      "      7      \u001b[36m260.6340\u001b[0m       \u001b[32m67.7576\u001b[0m  0.0185\n",
      "      9      \u001b[36m217.2887\u001b[0m       \u001b[32m50.4311\u001b[0m  0.0152\n",
      "      2      251.1837       \u001b[32m50.2017\u001b[0m  0.0342\n",
      "      5      285.2124       59.6351  0.0156\n",
      "      9      273.6867       64.1356  0.0158\n",
      "      3      \u001b[36m283.0215\u001b[0m       \u001b[32m61.0980\u001b[0m  0.0459\n",
      "     10      219.0337       \u001b[32m50.3478\u001b[0m  0.0154\n",
      "      8      \u001b[36m256.6747\u001b[0m       \u001b[32m67.0298\u001b[0m  0.0193\n",
      "      6      \u001b[36m282.7630\u001b[0m       60.1607  0.0185\n",
      "     10      \u001b[36m267.0262\u001b[0m       64.3728  0.0187\n",
      "Restoring best model from epoch 8.\n",
      "      6      310.4925       55.9868  0.0347\n",
      "      9      \u001b[36m254.0027\u001b[0m       67.1935  0.0172\n",
      "      4      \u001b[36m300.5870\u001b[0m       56.5921  0.0454\n",
      "      3      \u001b[36m237.1688\u001b[0m       52.0435  0.0416\n",
      "      7      \u001b[36m279.8569\u001b[0m       60.1575  0.0227\n",
      "     10      \u001b[36m252.4213\u001b[0m       67.7220  0.0165\n",
      "Restoring best model from epoch 8.\n",
      "      4      \u001b[36m277.9787\u001b[0m       62.3518  0.0437\n",
      "      8      283.7394       60.1131  0.0177\n",
      "      7      \u001b[36m289.4119\u001b[0m       56.7953  0.0339\n",
      "      4      \u001b[36m226.3032\u001b[0m       58.8219  0.0342\n",
      "      9      \u001b[36m278.7387\u001b[0m       60.1789  0.0230\n",
      "      5      309.5764       59.3023  0.0464\n",
      "      5      \u001b[36m274.6773\u001b[0m       63.5991  0.0429\n",
      "      8      290.9001       57.9299  0.0347\n",
      "     10      \u001b[36m274.0525\u001b[0m       60.3415  0.0160\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      5      265.7914       58.5294  0.0401\n",
      "      6      317.7191       60.5141  0.0336\n",
      "      9      \u001b[36m285.2865\u001b[0m       60.5957  0.0340\n",
      "      6      \u001b[36m271.4918\u001b[0m       64.8445  0.0379\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m297.8549\u001b[0m       \u001b[32m61.9017\u001b[0m  0.0412\n",
      "      6      242.1250       55.8995  0.0385\n",
      "      7      303.9755       60.7741  0.0392\n",
      "     10      \u001b[36m284.6587\u001b[0m       65.1031  0.0345\n",
      "Restoring best model from epoch 2.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m315.5978\u001b[0m       \u001b[32m69.3026\u001b[0m  0.0261\n",
      "      7      \u001b[36m269.5268\u001b[0m       67.1635  0.0365\n",
      "      7      235.6194       53.6266  0.0356\n",
      "      2      \u001b[36m307.8005\u001b[0m       \u001b[32m65.5818\u001b[0m  0.0210\n",
      "      2      301.0867       \u001b[32m61.5085\u001b[0m  0.0424\n",
      "      8      312.8282       61.4772  0.0363\n",
      "      3      \u001b[36m303.3003\u001b[0m       \u001b[32m62.7861\u001b[0m  0.0191\n",
      "      8      269.8849       69.5183  0.0416\n",
      "      8      235.2730       54.0189  0.0365\n",
      "      3      315.7383       \u001b[32m59.8825\u001b[0m  0.0347\n",
      "      4      308.2562       \u001b[32m61.0517\u001b[0m  0.0185\n",
      "      9      303.9839       62.1813  0.0359\n",
      "      9      271.7255       72.7957  0.0329\n",
      "      5      \u001b[36m302.4211\u001b[0m       \u001b[32m61.0159\u001b[0m  0.0169\n",
      "      4      299.8586       59.9324  0.0320\n",
      "      9      232.7006       58.0558  0.0355\n",
      "     10      306.2123       63.9168  0.0318\n",
      "      6      \u001b[36m295.9891\u001b[0m       61.1900  0.0161\n",
      "Restoring best model from epoch 2.\n",
      "     10      \u001b[36m266.1034\u001b[0m       75.8991  0.0337\n",
      "      7      \u001b[36m294.5229\u001b[0m       61.4734  0.0160\n",
      "Restoring best model from epoch 3.\n",
      "      5      \u001b[36m293.3293\u001b[0m       61.8558  0.0352\n",
      "     10      226.8387       64.0234  0.0340\n",
      "Restoring best model from epoch 2.\n",
      "      8      \u001b[36m288.4388\u001b[0m       61.9449  0.0197\n",
      "      9      \u001b[36m287.9058\u001b[0m       62.3673  0.0166\n",
      "      6      \u001b[36m283.5663\u001b[0m       64.9999  0.0331\n",
      "     10      289.5648       62.7404  0.0159\n",
      "Restoring best model from epoch 5.\n",
      "      7      293.1217       68.3771  0.0324\n",
      "      8      285.4458       69.1158  0.0360\n",
      "      9      \u001b[36m281.3806\u001b[0m       69.2844  0.0327\n",
      "     10      \u001b[36m277.4897\u001b[0m       68.2852  0.0383\n",
      "Restoring best model from epoch 3.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m286.7832\u001b[0m       \u001b[32m82.3825\u001b[0m  0.0309\n",
      "      2      \u001b[36m283.7719\u001b[0m       \u001b[32m81.6693\u001b[0m  0.0281\n",
      "      3      296.2969       82.1913  0.0257\n",
      "      4      \u001b[36m266.8550\u001b[0m       82.2389  0.0365\n",
      "      5      284.0021       81.7648  0.0302\n",
      "      6      268.3279       82.9280  0.0306\n",
      "      7      268.0801       87.8846  0.0239\n",
      "      8      \u001b[36m265.7218\u001b[0m       94.6916  0.0261\n",
      "      9      \u001b[36m260.7380\u001b[0m       94.8593  0.0266\n",
      "     10      262.0309       94.7909  0.0313\n",
      "Restoring best model from epoch 2.\n",
      "y_train breslow final [-1.000e+00 -2.000e+00 -3.000e+00 -3.000e+00 -3.000e+00 -3.000e+00\n",
      " -3.000e+00 -4.000e+00 -4.000e+00 -6.000e+00 -7.000e+00 -7.000e+00\n",
      " -7.000e+00 -7.000e+00 -7.000e+00 -7.000e+00  7.000e+00 -8.000e+00\n",
      " -1.000e+01 -1.400e+01 -1.500e+01 -1.700e+01  2.300e+01 -2.300e+01\n",
      "  3.700e+01 -3.900e+01 -5.000e+01 -5.500e+01 -5.800e+01 -6.200e+01\n",
      " -7.000e+01 -7.200e+01 -7.300e+01 -7.400e+01 -7.600e+01 -7.700e+01\n",
      " -8.200e+01  9.600e+01  1.110e+02 -1.120e+02 -1.140e+02 -1.220e+02\n",
      " -1.340e+02 -1.390e+02  1.550e+02  1.620e+02 -1.660e+02 -1.690e+02\n",
      " -1.740e+02  1.780e+02 -1.790e+02 -1.820e+02 -1.840e+02 -1.900e+02\n",
      " -1.940e+02 -1.940e+02  1.940e+02  1.990e+02 -2.030e+02 -2.040e+02\n",
      "  2.050e+02 -2.070e+02  2.140e+02  2.280e+02 -2.300e+02  2.340e+02\n",
      "  2.400e+02  2.410e+02 -2.420e+02  2.420e+02 -2.420e+02 -2.490e+02\n",
      " -2.570e+02 -2.790e+02 -2.860e+02 -2.870e+02 -2.920e+02 -3.010e+02\n",
      " -3.130e+02  3.150e+02 -3.200e+02 -3.260e+02 -3.330e+02 -3.360e+02\n",
      " -3.370e+02 -3.420e+02 -3.430e+02  3.490e+02  3.510e+02 -3.540e+02\n",
      "  3.540e+02 -3.680e+02 -3.720e+02  3.720e+02 -3.720e+02 -3.840e+02\n",
      "  3.880e+02 -3.950e+02 -4.030e+02 -4.050e+02 -4.070e+02 -4.110e+02\n",
      " -4.140e+02 -4.160e+02 -4.170e+02 -4.180e+02 -4.190e+02 -4.270e+02\n",
      " -4.310e+02 -4.340e+02  4.380e+02 -4.380e+02 -4.420e+02 -4.420e+02\n",
      " -4.420e+02 -4.430e+02  4.440e+02 -4.490e+02 -4.540e+02 -4.540e+02\n",
      " -4.550e+02 -4.570e+02 -4.580e+02 -4.610e+02  4.660e+02 -4.670e+02\n",
      " -4.710e+02 -4.780e+02 -4.820e+02 -4.860e+02 -4.870e+02 -4.870e+02\n",
      " -4.910e+02  4.920e+02 -4.920e+02  4.920e+02 -4.940e+02 -4.940e+02\n",
      " -4.970e+02 -5.020e+02 -5.030e+02 -5.080e+02 -5.090e+02  5.120e+02\n",
      " -5.120e+02 -5.150e+02 -5.230e+02 -5.230e+02 -5.260e+02 -5.300e+02\n",
      "  5.310e+02 -5.310e+02 -5.320e+02 -5.330e+02  5.380e+02 -5.440e+02\n",
      "  5.470e+02 -5.480e+02  5.610e+02 -5.660e+02 -5.670e+02 -5.670e+02\n",
      " -5.690e+02 -5.690e+02 -5.730e+02  5.760e+02 -5.760e+02  5.780e+02\n",
      " -5.820e+02 -5.840e+02 -5.850e+02 -5.880e+02  5.920e+02 -6.040e+02\n",
      "  6.050e+02 -6.090e+02 -6.090e+02 -6.110e+02 -6.110e+02 -6.150e+02\n",
      " -6.220e+02 -6.230e+02 -6.270e+02 -6.290e+02 -6.290e+02 -6.300e+02\n",
      " -6.380e+02  6.390e+02 -6.500e+02 -6.510e+02 -6.510e+02 -6.550e+02\n",
      " -6.710e+02 -6.770e+02 -6.780e+02 -6.860e+02 -7.050e+02 -7.060e+02\n",
      " -7.060e+02  7.090e+02 -7.140e+02 -7.150e+02 -7.180e+02  7.220e+02\n",
      "  7.270e+02 -7.350e+02 -7.360e+02 -7.380e+02  7.420e+02 -7.430e+02\n",
      " -7.480e+02  7.580e+02 -7.580e+02 -7.600e+02 -7.720e+02  7.750e+02\n",
      "  7.750e+02 -7.850e+02 -7.870e+02  7.880e+02 -7.920e+02 -7.950e+02\n",
      " -7.960e+02 -8.050e+02  8.140e+02  8.190e+02  8.210e+02 -8.260e+02\n",
      " -8.320e+02 -8.370e+02 -8.460e+02 -8.540e+02 -8.600e+02 -8.620e+02\n",
      " -8.630e+02 -8.680e+02 -8.780e+02 -8.890e+02 -9.080e+02 -9.080e+02\n",
      " -9.080e+02 -9.140e+02 -9.140e+02 -9.160e+02  9.190e+02 -9.190e+02\n",
      " -9.260e+02  9.330e+02 -9.350e+02 -9.390e+02 -9.460e+02  9.540e+02\n",
      " -9.550e+02 -9.560e+02 -9.620e+02  9.620e+02 -9.640e+02 -9.640e+02\n",
      " -9.680e+02  9.840e+02  9.870e+02 -9.920e+02 -9.920e+02 -9.930e+02\n",
      " -1.004e+03  1.011e+03 -1.012e+03 -1.021e+03 -1.026e+03  1.033e+03\n",
      " -1.040e+03 -1.058e+03 -1.069e+03 -1.076e+03 -1.079e+03 -1.099e+03\n",
      "  1.106e+03 -1.112e+03 -1.115e+03 -1.116e+03 -1.120e+03  1.120e+03\n",
      "  1.120e+03 -1.127e+03 -1.130e+03 -1.137e+03 -1.139e+03 -1.147e+03\n",
      " -1.164e+03 -1.173e+03  1.183e+03 -1.189e+03 -1.201e+03  1.209e+03\n",
      " -1.213e+03 -1.217e+03 -1.219e+03  1.220e+03 -1.222e+03 -1.227e+03\n",
      " -1.229e+03 -1.236e+03  1.242e+03 -1.245e+03 -1.250e+03  1.251e+03\n",
      " -1.257e+03  1.262e+03 -1.277e+03 -1.279e+03 -1.294e+03 -1.294e+03\n",
      " -1.300e+03 -1.301e+03 -1.314e+03 -1.320e+03  1.335e+03 -1.337e+03\n",
      "  1.339e+03 -1.341e+03  1.351e+03 -1.354e+03 -1.359e+03 -1.382e+03\n",
      " -1.387e+03 -1.393e+03 -1.397e+03 -1.399e+03  1.401e+03 -1.412e+03\n",
      " -1.421e+03 -1.426e+03 -1.428e+03 -1.453e+03 -1.458e+03 -1.470e+03\n",
      "  1.481e+03  1.491e+03 -1.494e+03 -1.500e+03 -1.519e+03  1.525e+03\n",
      " -1.540e+03 -1.567e+03 -1.568e+03  1.578e+03  1.585e+03 -1.588e+03\n",
      " -1.631e+03  1.666e+03 -1.706e+03 -1.721e+03 -1.752e+03 -1.796e+03\n",
      " -1.806e+03 -1.828e+03 -1.834e+03 -1.868e+03  1.886e+03  1.891e+03\n",
      "  1.933e+03 -1.943e+03 -1.989e+03  2.000e+03  2.052e+03 -2.078e+03\n",
      " -2.107e+03 -2.218e+03 -2.219e+03  2.235e+03  2.282e+03  2.286e+03\n",
      " -2.287e+03 -2.289e+03 -2.289e+03  2.379e+03 -2.381e+03  2.433e+03\n",
      " -2.493e+03 -2.565e+03 -2.585e+03 -2.602e+03 -2.650e+03  2.660e+03\n",
      " -2.702e+03 -2.761e+03  2.835e+03  2.875e+03 -2.893e+03  2.907e+03\n",
      " -2.918e+03  2.988e+03 -3.013e+03  3.200e+03 -3.253e+03  3.470e+03\n",
      "  3.571e+03 -3.725e+03 -3.733e+03 -3.761e+03  4.068e+03 -4.113e+03\n",
      "  4.229e+03  4.412e+03  4.445e+03  4.695e+03  5.166e+03 -5.255e+03\n",
      " -5.546e+03]\n",
      "Concordance Index 0.7501702610669694\n",
      "Integrated Brier Score: 0.15299701494399198\n",
      "y_train breslow final [-1.000e+00 -2.000e+00 -3.000e+00 -3.000e+00 -3.000e+00 -3.000e+00\n",
      " -3.000e+00 -4.000e+00 -4.000e+00 -6.000e+00 -7.000e+00 -7.000e+00\n",
      " -7.000e+00 -7.000e+00 -7.000e+00 -7.000e+00  7.000e+00 -8.000e+00\n",
      " -1.000e+01 -1.400e+01 -1.500e+01 -1.700e+01  2.300e+01 -2.300e+01\n",
      "  3.700e+01 -3.900e+01 -5.000e+01 -5.500e+01 -5.800e+01 -6.200e+01\n",
      " -7.000e+01 -7.200e+01 -7.300e+01 -7.400e+01 -7.600e+01 -7.700e+01\n",
      " -8.200e+01  9.600e+01  1.110e+02 -1.120e+02 -1.140e+02 -1.220e+02\n",
      " -1.340e+02 -1.390e+02  1.550e+02  1.620e+02 -1.660e+02 -1.690e+02\n",
      " -1.740e+02  1.780e+02 -1.790e+02 -1.820e+02 -1.840e+02 -1.900e+02\n",
      " -1.940e+02 -1.940e+02  1.940e+02  1.990e+02 -2.030e+02 -2.040e+02\n",
      "  2.050e+02 -2.070e+02  2.140e+02  2.280e+02 -2.300e+02  2.340e+02\n",
      "  2.400e+02  2.410e+02 -2.420e+02  2.420e+02 -2.420e+02 -2.490e+02\n",
      " -2.570e+02 -2.790e+02 -2.860e+02 -2.870e+02 -2.920e+02 -3.010e+02\n",
      " -3.130e+02  3.150e+02 -3.200e+02 -3.260e+02 -3.330e+02 -3.360e+02\n",
      " -3.370e+02 -3.420e+02 -3.430e+02  3.490e+02  3.510e+02 -3.540e+02\n",
      "  3.540e+02 -3.680e+02 -3.720e+02  3.720e+02 -3.720e+02 -3.840e+02\n",
      "  3.880e+02 -3.950e+02 -4.030e+02 -4.050e+02 -4.070e+02 -4.110e+02\n",
      " -4.140e+02 -4.160e+02 -4.170e+02 -4.180e+02 -4.190e+02 -4.270e+02\n",
      " -4.310e+02 -4.340e+02  4.380e+02 -4.380e+02 -4.420e+02 -4.420e+02\n",
      " -4.420e+02 -4.430e+02  4.440e+02 -4.490e+02 -4.540e+02 -4.540e+02\n",
      " -4.550e+02 -4.570e+02 -4.580e+02 -4.610e+02  4.660e+02 -4.670e+02\n",
      " -4.710e+02 -4.780e+02 -4.820e+02 -4.860e+02 -4.870e+02 -4.870e+02\n",
      " -4.910e+02  4.920e+02 -4.920e+02  4.920e+02 -4.940e+02 -4.940e+02\n",
      " -4.970e+02 -5.020e+02 -5.030e+02 -5.080e+02 -5.090e+02  5.120e+02\n",
      " -5.120e+02 -5.150e+02 -5.230e+02 -5.230e+02 -5.260e+02 -5.300e+02\n",
      "  5.310e+02 -5.310e+02 -5.320e+02 -5.330e+02  5.380e+02 -5.440e+02\n",
      "  5.470e+02 -5.480e+02  5.610e+02 -5.660e+02 -5.670e+02 -5.670e+02\n",
      " -5.690e+02 -5.690e+02 -5.730e+02  5.760e+02 -5.760e+02  5.780e+02\n",
      " -5.820e+02 -5.840e+02 -5.850e+02 -5.880e+02  5.920e+02 -6.040e+02\n",
      "  6.050e+02 -6.090e+02 -6.090e+02 -6.110e+02 -6.110e+02 -6.150e+02\n",
      " -6.220e+02 -6.230e+02 -6.270e+02 -6.290e+02 -6.290e+02 -6.300e+02\n",
      " -6.380e+02  6.390e+02 -6.500e+02 -6.510e+02 -6.510e+02 -6.550e+02\n",
      " -6.710e+02 -6.770e+02 -6.780e+02 -6.860e+02 -7.050e+02 -7.060e+02\n",
      " -7.060e+02  7.090e+02 -7.140e+02 -7.150e+02 -7.180e+02  7.220e+02\n",
      "  7.270e+02 -7.350e+02 -7.360e+02 -7.380e+02  7.420e+02 -7.430e+02\n",
      " -7.480e+02  7.580e+02 -7.580e+02 -7.600e+02 -7.720e+02  7.750e+02\n",
      "  7.750e+02 -7.850e+02 -7.870e+02  7.880e+02 -7.920e+02 -7.950e+02\n",
      " -7.960e+02 -8.050e+02  8.140e+02  8.190e+02  8.210e+02 -8.260e+02\n",
      " -8.320e+02 -8.370e+02 -8.460e+02 -8.540e+02 -8.600e+02 -8.620e+02\n",
      " -8.630e+02 -8.680e+02 -8.780e+02 -8.890e+02 -9.080e+02 -9.080e+02\n",
      " -9.080e+02 -9.140e+02 -9.140e+02 -9.160e+02  9.190e+02 -9.190e+02\n",
      " -9.260e+02  9.330e+02 -9.350e+02 -9.390e+02 -9.460e+02  9.540e+02\n",
      " -9.550e+02 -9.560e+02 -9.620e+02  9.620e+02 -9.640e+02 -9.640e+02\n",
      " -9.680e+02  9.840e+02  9.870e+02 -9.920e+02 -9.920e+02 -9.930e+02\n",
      " -1.004e+03  1.011e+03 -1.012e+03 -1.021e+03 -1.026e+03  1.033e+03\n",
      " -1.040e+03 -1.058e+03 -1.069e+03 -1.076e+03 -1.079e+03 -1.099e+03\n",
      "  1.106e+03 -1.112e+03 -1.115e+03 -1.116e+03 -1.120e+03  1.120e+03\n",
      "  1.120e+03 -1.127e+03 -1.130e+03 -1.137e+03 -1.139e+03 -1.147e+03\n",
      " -1.164e+03 -1.173e+03  1.183e+03 -1.189e+03 -1.201e+03  1.209e+03\n",
      " -1.213e+03 -1.217e+03 -1.219e+03  1.220e+03 -1.222e+03 -1.227e+03\n",
      " -1.229e+03 -1.236e+03  1.242e+03 -1.245e+03 -1.250e+03  1.251e+03\n",
      " -1.257e+03  1.262e+03 -1.277e+03 -1.279e+03 -1.294e+03 -1.294e+03\n",
      " -1.300e+03 -1.301e+03 -1.314e+03 -1.320e+03  1.335e+03 -1.337e+03\n",
      "  1.339e+03 -1.341e+03  1.351e+03 -1.354e+03 -1.359e+03 -1.382e+03\n",
      " -1.387e+03 -1.393e+03 -1.397e+03 -1.399e+03  1.401e+03 -1.412e+03\n",
      " -1.421e+03 -1.426e+03 -1.428e+03 -1.453e+03 -1.458e+03 -1.470e+03\n",
      "  1.481e+03  1.491e+03 -1.494e+03 -1.500e+03 -1.519e+03  1.525e+03\n",
      " -1.540e+03 -1.567e+03 -1.568e+03  1.578e+03  1.585e+03 -1.588e+03\n",
      " -1.631e+03  1.666e+03 -1.706e+03 -1.721e+03 -1.752e+03 -1.796e+03\n",
      " -1.806e+03 -1.828e+03 -1.834e+03 -1.868e+03  1.886e+03  1.891e+03\n",
      "  1.933e+03 -1.943e+03 -1.989e+03  2.000e+03  2.052e+03 -2.078e+03\n",
      " -2.107e+03 -2.218e+03 -2.219e+03  2.235e+03  2.282e+03  2.286e+03\n",
      " -2.287e+03 -2.289e+03 -2.289e+03  2.379e+03 -2.381e+03  2.433e+03\n",
      " -2.493e+03 -2.565e+03 -2.585e+03 -2.602e+03 -2.650e+03  2.660e+03\n",
      " -2.702e+03 -2.761e+03  2.835e+03  2.875e+03 -2.893e+03  2.907e+03\n",
      " -2.918e+03  2.988e+03 -3.013e+03  3.200e+03 -3.253e+03  3.470e+03\n",
      "  3.571e+03 -3.725e+03 -3.733e+03 -3.761e+03  4.068e+03 -4.113e+03\n",
      "  4.229e+03  4.412e+03  4.445e+03  4.695e+03  5.166e+03 -5.255e+03\n",
      " -5.546e+03]\n",
      "durations 3.0 6423.0\n",
      "Concordance Index 0.7900262467191601\n",
      "Integrated Brier Score: 0.21232466022383634\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m281.9138\u001b[0m       \u001b[32m37.4212\u001b[0m  0.0166\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2      \u001b[36m266.8640\u001b[0m       39.6981  0.0296\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      3      \u001b[36m259.4031\u001b[0m       41.4051  0.0188\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m333.6614\u001b[0m       \u001b[32m34.5210\u001b[0m  0.0317\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m298.8596\u001b[0m       \u001b[32m39.2470\u001b[0m  0.0536\n",
      "      4      261.9504       40.6771  0.0219\n",
      "      2      \u001b[36m317.9318\u001b[0m       37.3440  0.0264\n",
      "      5      \u001b[36m257.9617\u001b[0m       39.1480  0.0221\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m310.2913\u001b[0m       \u001b[32m55.2969\u001b[0m  0.0608\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      3      \u001b[36m301.7883\u001b[0m       40.4402  0.0236\n",
      "      6      \u001b[36m256.4911\u001b[0m       37.4796  0.0237\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2      \u001b[36m294.8813\u001b[0m       39.4097  0.0528\n",
      "      7      \u001b[36m251.7775\u001b[0m       \u001b[32m35.8668\u001b[0m  0.0212\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      4      \u001b[36m300.7210\u001b[0m       40.8687  0.0242\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2      311.8649       \u001b[32m55.2656\u001b[0m  0.0445\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      5      300.7277       39.2181  0.0177\n",
      "      8      \u001b[36m250.8088\u001b[0m       \u001b[32m34.2660\u001b[0m  0.0201\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m311.6055\u001b[0m       \u001b[32m57.1805\u001b[0m  0.0420\n",
      "      3      316.7008       \u001b[32m39.2112\u001b[0m  0.0457\n",
      "      6      \u001b[36m299.3726\u001b[0m       37.2347  0.0181\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m318.7769\u001b[0m       \u001b[32m31.1281\u001b[0m  0.0604\n",
      "      9      \u001b[36m241.6511\u001b[0m       \u001b[32m33.2698\u001b[0m  0.0208\n",
      "      2      \u001b[36m298.7340\u001b[0m       62.0259  0.0205\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m282.8721\u001b[0m       \u001b[32m29.8063\u001b[0m  0.0400\n",
      "      3      318.1941       \u001b[32m54.1647\u001b[0m  0.0367\n",
      "     10      246.2707       \u001b[32m32.7647\u001b[0m  0.0165\n",
      "      3      304.2736       65.6669  0.0189\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m315.0806\u001b[0m       \u001b[32m42.7097\u001b[0m  0.0260\n",
      "      7      301.0640       35.5535  0.0323\n",
      "      4      298.4264       39.5425  0.0456\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m304.4489\u001b[0m       \u001b[32m50.0963\u001b[0m  0.0489\n",
      "      2      \u001b[36m314.6760\u001b[0m       \u001b[32m31.0595\u001b[0m  0.0394\n",
      "      4      \u001b[36m296.5084\u001b[0m       67.8642  0.0184\n",
      "      4      \u001b[36m302.4290\u001b[0m       56.0739  0.0347\n",
      "      2      \u001b[36m291.9597\u001b[0m       46.6243  0.0212\n",
      "      2      \u001b[36m271.6689\u001b[0m       30.7611  0.0372\n",
      "      8      \u001b[36m295.0519\u001b[0m       \u001b[32m34.2112\u001b[0m  0.0202\n",
      "      5      \u001b[36m290.1528\u001b[0m       70.1919  0.0210\n",
      "      3      \u001b[36m284.2267\u001b[0m       50.2753  0.0198\n",
      "      9      297.3621       \u001b[32m33.4594\u001b[0m  0.0201\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      3      342.3747       31.4954  0.0412\n",
      "      6      \u001b[36m284.5574\u001b[0m       70.8100  0.0174\n",
      "      5      \u001b[36m293.8457\u001b[0m       39.8208  0.0504\n",
      "      2      \u001b[36m294.2480\u001b[0m       \u001b[32m49.5893\u001b[0m  0.0484\n",
      "      5      302.6015       61.2195  0.0378\n",
      "      4      \u001b[36m280.0392\u001b[0m       50.8463  0.0186\n",
      "      3      279.8605       33.3906  0.0409\n",
      "     10      \u001b[36m292.6948\u001b[0m       \u001b[32m33.0248\u001b[0m  0.0195\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m294.7024\u001b[0m       \u001b[32m63.3505\u001b[0m  0.0217\n",
      "      7      \u001b[36m282.9170\u001b[0m       70.5431  0.0198\n",
      "      5      282.3275       49.2283  0.0175\n",
      "      2      \u001b[36m282.0066\u001b[0m       \u001b[32m60.2531\u001b[0m  0.0180\n",
      "      6      303.1556       65.3175  0.0331\n",
      "      4      328.3008       31.4704  0.0370\n",
      "      6      280.1398       47.7650  0.0154\n",
      "      8      \u001b[36m278.4778\u001b[0m       69.7424  0.0188\n",
      "      6      \u001b[36m285.5688\u001b[0m       40.2629  0.0425\n",
      "      3      311.8595       \u001b[32m48.1196\u001b[0m  0.0409\n",
      "      4      279.6125       33.6723  0.0427\n",
      "      3      283.9646       \u001b[32m56.3140\u001b[0m  0.0196\n",
      "      7      \u001b[36m275.7691\u001b[0m       46.3432  0.0156\n",
      "      9      280.5895       68.4956  0.0151\n",
      "      8      \u001b[36m272.4132\u001b[0m       45.5977  0.0164\n",
      "     10      283.2715       67.0630  0.0174\n",
      "Restoring best model from epoch 1.\n",
      "      5      324.8690       31.5093  0.0385\n",
      "      4      \u001b[36m279.3990\u001b[0m       \u001b[32m53.9883\u001b[0m  0.0219\n",
      "      7      \u001b[36m292.2295\u001b[0m       65.2645  0.0407\n",
      "      4      \u001b[36m289.0527\u001b[0m       \u001b[32m47.0299\u001b[0m  0.0332\n",
      "      5      288.8905       34.1132  0.0318\n",
      "      7      295.0113       40.7775  0.0375\n",
      "      9      274.7251       45.1771  0.0141\n",
      "     10      275.6894       44.7464  0.0147\n",
      "Restoring best model from epoch 1.\n",
      "      5      \u001b[36m276.8145\u001b[0m       \u001b[32m52.5848\u001b[0m  0.0235\n",
      "      8      299.2374       63.3886  0.0312\n",
      "      6      \u001b[36m309.9448\u001b[0m       32.1975  0.0326\n",
      "      5      294.6633       47.1763  0.0317\n",
      "      6      \u001b[36m265.5041\u001b[0m       35.0290  0.0300\n",
      "      8      \u001b[36m285.5410\u001b[0m       41.3543  0.0367\n",
      "      6      278.0220       \u001b[32m52.2220\u001b[0m  0.0240\n",
      "      9      293.1026       62.7345  0.0314\n",
      "      7      \u001b[36m274.7314\u001b[0m       52.3836  0.0167\n",
      "      7      268.8679       33.2821  0.0308\n",
      "      7      315.9295       33.4556  0.0382\n",
      "      6      291.8757       48.3898  0.0379\n",
      "      9      \u001b[36m277.4949\u001b[0m       42.1242  0.0385\n",
      "     10      \u001b[36m291.3026\u001b[0m       62.6157  0.0337\n",
      "Restoring best model from epoch 3.\n",
      "      8      265.8654       32.7685  0.0323\n",
      "      8      \u001b[36m307.6908\u001b[0m       35.2645  0.0319\n",
      "      7      \u001b[36m284.3652\u001b[0m       50.1164  0.0344\n",
      "      8      \u001b[36m263.5633\u001b[0m       52.5917  0.0455\n",
      "     10      288.1909       42.0613  0.0363\n",
      "Restoring best model from epoch 3.\n",
      "      9      270.7897       52.7138  0.0161\n",
      "      9      \u001b[36m258.9778\u001b[0m       33.6464  0.0316\n",
      "      9      \u001b[36m297.1552\u001b[0m       37.6604  0.0335\n",
      "      8      \u001b[36m272.5557\u001b[0m       52.7655  0.0350\n",
      "     10      266.4547       52.6642  0.0155\n",
      "Restoring best model from epoch 6.\n",
      "     10      260.9403       34.2598  0.0317\n",
      "Restoring best model from epoch 1.\n",
      "     10      298.7528       40.1827  0.0342\n",
      "Restoring best model from epoch 2.\n",
      "      9      276.0231       56.1682  0.0325\n",
      "     10      278.2365       59.0163  0.0307\n",
      "Restoring best model from epoch 4.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m268.7900\u001b[0m       \u001b[32m61.4325\u001b[0m  0.0264\n",
      "      2      \u001b[36m266.6418\u001b[0m       61.4989  0.0264\n",
      "      3      273.8399       63.0853  0.0323\n",
      "      4      \u001b[36m263.7018\u001b[0m       63.0999  0.0274\n",
      "      5      \u001b[36m247.7236\u001b[0m       \u001b[32m60.0933\u001b[0m  0.0257\n",
      "      6      253.3917       61.3167  0.0260\n",
      "      7      263.8598       \u001b[32m59.3402\u001b[0m  0.0266\n",
      "      8      256.8646       \u001b[32m57.4098\u001b[0m  0.0312\n",
      "      9      255.2772       \u001b[32m56.2344\u001b[0m  0.0263\n",
      "     10      260.3735       \u001b[32m55.9334\u001b[0m  0.0256\n",
      "y_train breslow final [-1.000e+00 -2.000e+00 -3.000e+00 -3.000e+00 -3.000e+00 -3.000e+00\n",
      " -3.000e+00 -3.000e+00 -3.000e+00 -3.000e+00 -3.000e+00 -4.000e+00\n",
      " -4.000e+00 -5.000e+00 -6.000e+00 -7.000e+00 -7.000e+00 -7.000e+00\n",
      " -7.000e+00  7.000e+00 -1.000e+01 -1.500e+01  3.700e+01 -3.900e+01\n",
      " -5.000e+01 -5.500e+01 -5.800e+01 -6.200e+01 -6.200e+01 -6.300e+01\n",
      " -7.200e+01 -7.200e+01 -7.300e+01 -7.400e+01 -7.600e+01 -7.700e+01\n",
      " -8.200e+01 -8.400e+01 -9.000e+01 -1.050e+02  1.110e+02 -1.120e+02\n",
      "  1.130e+02 -1.140e+02 -1.220e+02 -1.340e+02 -1.390e+02 -1.530e+02\n",
      "  1.550e+02 -1.620e+02  1.620e+02 -1.660e+02 -1.690e+02  1.780e+02\n",
      " -1.790e+02 -1.820e+02 -1.840e+02 -1.900e+02 -1.940e+02 -1.940e+02\n",
      "  1.990e+02 -1.990e+02 -2.030e+02 -2.040e+02 -2.070e+02 -2.100e+02\n",
      "  2.140e+02  2.280e+02 -2.300e+02 -2.310e+02  2.340e+02  2.400e+02\n",
      "  2.410e+02 -2.420e+02  2.420e+02 -2.420e+02 -2.430e+02  2.450e+02\n",
      " -2.490e+02 -2.570e+02  2.690e+02 -2.740e+02 -2.790e+02 -2.870e+02\n",
      " -2.920e+02 -3.010e+02 -3.130e+02  3.150e+02  3.160e+02 -3.170e+02\n",
      " -3.260e+02 -3.280e+02 -3.360e+02 -3.370e+02 -3.420e+02 -3.430e+02\n",
      "  3.470e+02  3.490e+02  3.510e+02 -3.540e+02  3.540e+02 -3.680e+02\n",
      " -3.720e+02  3.780e+02 -3.840e+02  3.880e+02 -3.950e+02  3.980e+02\n",
      " -4.050e+02 -4.070e+02 -4.080e+02 -4.110e+02 -4.140e+02 -4.140e+02\n",
      " -4.170e+02 -4.180e+02 -4.190e+02 -4.270e+02 -4.280e+02 -4.310e+02\n",
      " -4.330e+02 -4.340e+02  4.350e+02  4.380e+02 -4.380e+02 -4.380e+02\n",
      " -4.420e+02 -4.420e+02 -4.420e+02  4.440e+02 -4.490e+02 -4.510e+02\n",
      " -4.540e+02 -4.540e+02 -4.550e+02 -4.550e+02  4.560e+02 -4.570e+02\n",
      " -4.580e+02 -4.610e+02 -4.610e+02  4.660e+02 -4.670e+02 -4.710e+02\n",
      " -4.820e+02 -4.870e+02 -4.870e+02 -4.910e+02  4.920e+02 -4.920e+02\n",
      "  4.920e+02 -4.940e+02 -4.970e+02 -5.000e+02 -5.020e+02 -5.030e+02\n",
      " -5.080e+02 -5.090e+02  5.120e+02 -5.120e+02 -5.160e+02 -5.220e+02\n",
      " -5.260e+02  5.310e+02 -5.330e+02  5.370e+02  5.380e+02 -5.420e+02\n",
      " -5.440e+02 -5.440e+02 -5.480e+02  5.590e+02  5.610e+02 -5.630e+02\n",
      " -5.640e+02 -5.660e+02 -5.660e+02 -5.660e+02 -5.670e+02 -5.690e+02\n",
      " -5.690e+02 -5.710e+02 -5.710e+02 -5.730e+02  5.760e+02  5.780e+02\n",
      " -5.850e+02 -5.880e+02  5.920e+02 -5.960e+02 -5.990e+02  6.050e+02\n",
      "  6.070e+02 -6.080e+02 -6.090e+02 -6.110e+02 -6.110e+02 -6.150e+02\n",
      " -6.220e+02 -6.290e+02 -6.290e+02 -6.300e+02 -6.330e+02 -6.380e+02\n",
      "  6.480e+02  6.480e+02 -6.500e+02 -6.510e+02 -6.560e+02 -6.770e+02\n",
      " -6.780e+02  6.820e+02 -6.850e+02 -6.860e+02 -7.050e+02 -7.060e+02\n",
      " -7.060e+02  7.090e+02 -7.140e+02 -7.180e+02 -7.210e+02  7.220e+02\n",
      " -7.240e+02 -7.380e+02  7.420e+02 -7.430e+02 -7.540e+02 -7.600e+02\n",
      " -7.720e+02  7.750e+02  7.750e+02 -7.770e+02 -7.850e+02  7.880e+02\n",
      " -7.950e+02 -7.960e+02 -8.000e+02 -8.050e+02  8.140e+02  8.140e+02\n",
      "  8.190e+02  8.210e+02 -8.260e+02 -8.320e+02 -8.350e+02 -8.370e+02\n",
      " -8.460e+02 -8.460e+02 -8.540e+02 -8.540e+02 -8.620e+02 -8.630e+02\n",
      " -8.680e+02 -8.780e+02  8.860e+02 -8.890e+02 -9.000e+02 -9.080e+02\n",
      " -9.080e+02 -9.140e+02 -9.160e+02 -9.190e+02 -9.330e+02 -9.350e+02\n",
      " -9.390e+02 -9.490e+02  9.540e+02 -9.550e+02 -9.560e+02  9.610e+02\n",
      " -9.620e+02 -9.640e+02 -9.680e+02  9.840e+02 -9.920e+02 -9.930e+02\n",
      " -1.004e+03  1.011e+03 -1.012e+03 -1.021e+03 -1.026e+03 -1.032e+03\n",
      " -1.058e+03 -1.078e+03 -1.099e+03 -1.112e+03 -1.115e+03 -1.116e+03\n",
      "  1.120e+03  1.120e+03 -1.127e+03 -1.130e+03  1.137e+03 -1.137e+03\n",
      " -1.147e+03  1.152e+03 -1.164e+03  1.183e+03 -1.189e+03 -1.191e+03\n",
      " -1.201e+03 -1.201e+03  1.209e+03 -1.210e+03 -1.213e+03 -1.217e+03\n",
      " -1.219e+03  1.220e+03 -1.227e+03 -1.229e+03 -1.236e+03  1.242e+03\n",
      " -1.250e+03  1.251e+03 -1.257e+03  1.262e+03 -1.277e+03 -1.294e+03\n",
      " -1.300e+03 -1.301e+03 -1.314e+03 -1.320e+03  1.335e+03 -1.337e+03\n",
      "  1.339e+03 -1.341e+03  1.351e+03 -1.382e+03 -1.397e+03 -1.399e+03\n",
      "  1.401e+03 -1.401e+03 -1.412e+03 -1.421e+03 -1.426e+03 -1.426e+03\n",
      " -1.453e+03 -1.458e+03 -1.469e+03 -1.470e+03  1.491e+03 -1.494e+03\n",
      " -1.500e+03 -1.519e+03  1.525e+03 -1.540e+03  1.547e+03 -1.553e+03\n",
      " -1.567e+03  1.578e+03  1.585e+03 -1.588e+03 -1.631e+03 -1.650e+03\n",
      "  1.666e+03 -1.706e+03 -1.721e+03 -1.752e+03  1.762e+03 -1.796e+03\n",
      " -1.806e+03 -1.828e+03  1.891e+03  1.915e+03  1.933e+03 -1.943e+03\n",
      "  2.000e+03  2.052e+03 -2.107e+03 -2.218e+03 -2.219e+03  2.282e+03\n",
      "  2.286e+03 -2.287e+03 -2.289e+03 -2.289e+03  2.379e+03 -2.381e+03\n",
      " -2.565e+03 -2.602e+03 -2.650e+03  2.660e+03 -2.702e+03 -2.761e+03\n",
      " -2.772e+03  2.835e+03 -2.860e+03 -2.869e+03  2.875e+03 -2.893e+03\n",
      " -2.918e+03 -2.918e+03  2.988e+03 -3.000e+03 -3.013e+03  3.470e+03\n",
      " -3.574e+03 -3.725e+03 -3.733e+03  3.978e+03  4.084e+03 -4.113e+03\n",
      "  4.229e+03  4.412e+03  4.695e+03 -4.752e+03 -5.255e+03 -5.546e+03\n",
      " -6.423e+03]\n",
      "Concordance Index 0.8298012517843417\n",
      "Integrated Brier Score: 0.19093646885379534\n",
      "y_train breslow final [-1.000e+00 -2.000e+00 -3.000e+00 -3.000e+00 -3.000e+00 -3.000e+00\n",
      " -3.000e+00 -3.000e+00 -3.000e+00 -3.000e+00 -3.000e+00 -4.000e+00\n",
      " -4.000e+00 -5.000e+00 -6.000e+00 -7.000e+00 -7.000e+00 -7.000e+00\n",
      " -7.000e+00  7.000e+00 -1.000e+01 -1.500e+01  3.700e+01 -3.900e+01\n",
      " -5.000e+01 -5.500e+01 -5.800e+01 -6.200e+01 -6.200e+01 -6.300e+01\n",
      " -7.200e+01 -7.200e+01 -7.300e+01 -7.400e+01 -7.600e+01 -7.700e+01\n",
      " -8.200e+01 -8.400e+01 -9.000e+01 -1.050e+02  1.110e+02 -1.120e+02\n",
      "  1.130e+02 -1.140e+02 -1.220e+02 -1.340e+02 -1.390e+02 -1.530e+02\n",
      "  1.550e+02 -1.620e+02  1.620e+02 -1.660e+02 -1.690e+02  1.780e+02\n",
      " -1.790e+02 -1.820e+02 -1.840e+02 -1.900e+02 -1.940e+02 -1.940e+02\n",
      "  1.990e+02 -1.990e+02 -2.030e+02 -2.040e+02 -2.070e+02 -2.100e+02\n",
      "  2.140e+02  2.280e+02 -2.300e+02 -2.310e+02  2.340e+02  2.400e+02\n",
      "  2.410e+02 -2.420e+02  2.420e+02 -2.420e+02 -2.430e+02  2.450e+02\n",
      " -2.490e+02 -2.570e+02  2.690e+02 -2.740e+02 -2.790e+02 -2.870e+02\n",
      " -2.920e+02 -3.010e+02 -3.130e+02  3.150e+02  3.160e+02 -3.170e+02\n",
      " -3.260e+02 -3.280e+02 -3.360e+02 -3.370e+02 -3.420e+02 -3.430e+02\n",
      "  3.470e+02  3.490e+02  3.510e+02 -3.540e+02  3.540e+02 -3.680e+02\n",
      " -3.720e+02  3.780e+02 -3.840e+02  3.880e+02 -3.950e+02  3.980e+02\n",
      " -4.050e+02 -4.070e+02 -4.080e+02 -4.110e+02 -4.140e+02 -4.140e+02\n",
      " -4.170e+02 -4.180e+02 -4.190e+02 -4.270e+02 -4.280e+02 -4.310e+02\n",
      " -4.330e+02 -4.340e+02  4.350e+02  4.380e+02 -4.380e+02 -4.380e+02\n",
      " -4.420e+02 -4.420e+02 -4.420e+02  4.440e+02 -4.490e+02 -4.510e+02\n",
      " -4.540e+02 -4.540e+02 -4.550e+02 -4.550e+02  4.560e+02 -4.570e+02\n",
      " -4.580e+02 -4.610e+02 -4.610e+02  4.660e+02 -4.670e+02 -4.710e+02\n",
      " -4.820e+02 -4.870e+02 -4.870e+02 -4.910e+02  4.920e+02 -4.920e+02\n",
      "  4.920e+02 -4.940e+02 -4.970e+02 -5.000e+02 -5.020e+02 -5.030e+02\n",
      " -5.080e+02 -5.090e+02  5.120e+02 -5.120e+02 -5.160e+02 -5.220e+02\n",
      " -5.260e+02  5.310e+02 -5.330e+02  5.370e+02  5.380e+02 -5.420e+02\n",
      " -5.440e+02 -5.440e+02 -5.480e+02  5.590e+02  5.610e+02 -5.630e+02\n",
      " -5.640e+02 -5.660e+02 -5.660e+02 -5.660e+02 -5.670e+02 -5.690e+02\n",
      " -5.690e+02 -5.710e+02 -5.710e+02 -5.730e+02  5.760e+02  5.780e+02\n",
      " -5.850e+02 -5.880e+02  5.920e+02 -5.960e+02 -5.990e+02  6.050e+02\n",
      "  6.070e+02 -6.080e+02 -6.090e+02 -6.110e+02 -6.110e+02 -6.150e+02\n",
      " -6.220e+02 -6.290e+02 -6.290e+02 -6.300e+02 -6.330e+02 -6.380e+02\n",
      "  6.480e+02  6.480e+02 -6.500e+02 -6.510e+02 -6.560e+02 -6.770e+02\n",
      " -6.780e+02  6.820e+02 -6.850e+02 -6.860e+02 -7.050e+02 -7.060e+02\n",
      " -7.060e+02  7.090e+02 -7.140e+02 -7.180e+02 -7.210e+02  7.220e+02\n",
      " -7.240e+02 -7.380e+02  7.420e+02 -7.430e+02 -7.540e+02 -7.600e+02\n",
      " -7.720e+02  7.750e+02  7.750e+02 -7.770e+02 -7.850e+02  7.880e+02\n",
      " -7.950e+02 -7.960e+02 -8.000e+02 -8.050e+02  8.140e+02  8.140e+02\n",
      "  8.190e+02  8.210e+02 -8.260e+02 -8.320e+02 -8.350e+02 -8.370e+02\n",
      " -8.460e+02 -8.460e+02 -8.540e+02 -8.540e+02 -8.620e+02 -8.630e+02\n",
      " -8.680e+02 -8.780e+02  8.860e+02 -8.890e+02 -9.000e+02 -9.080e+02\n",
      " -9.080e+02 -9.140e+02 -9.160e+02 -9.190e+02 -9.330e+02 -9.350e+02\n",
      " -9.390e+02 -9.490e+02  9.540e+02 -9.550e+02 -9.560e+02  9.610e+02\n",
      " -9.620e+02 -9.640e+02 -9.680e+02  9.840e+02 -9.920e+02 -9.930e+02\n",
      " -1.004e+03  1.011e+03 -1.012e+03 -1.021e+03 -1.026e+03 -1.032e+03\n",
      " -1.058e+03 -1.078e+03 -1.099e+03 -1.112e+03 -1.115e+03 -1.116e+03\n",
      "  1.120e+03  1.120e+03 -1.127e+03 -1.130e+03  1.137e+03 -1.137e+03\n",
      " -1.147e+03  1.152e+03 -1.164e+03  1.183e+03 -1.189e+03 -1.191e+03\n",
      " -1.201e+03 -1.201e+03  1.209e+03 -1.210e+03 -1.213e+03 -1.217e+03\n",
      " -1.219e+03  1.220e+03 -1.227e+03 -1.229e+03 -1.236e+03  1.242e+03\n",
      " -1.250e+03  1.251e+03 -1.257e+03  1.262e+03 -1.277e+03 -1.294e+03\n",
      " -1.300e+03 -1.301e+03 -1.314e+03 -1.320e+03  1.335e+03 -1.337e+03\n",
      "  1.339e+03 -1.341e+03  1.351e+03 -1.382e+03 -1.397e+03 -1.399e+03\n",
      "  1.401e+03 -1.401e+03 -1.412e+03 -1.421e+03 -1.426e+03 -1.426e+03\n",
      " -1.453e+03 -1.458e+03 -1.469e+03 -1.470e+03  1.491e+03 -1.494e+03\n",
      " -1.500e+03 -1.519e+03  1.525e+03 -1.540e+03  1.547e+03 -1.553e+03\n",
      " -1.567e+03  1.578e+03  1.585e+03 -1.588e+03 -1.631e+03 -1.650e+03\n",
      "  1.666e+03 -1.706e+03 -1.721e+03 -1.752e+03  1.762e+03 -1.796e+03\n",
      " -1.806e+03 -1.828e+03  1.891e+03  1.915e+03  1.933e+03 -1.943e+03\n",
      "  2.000e+03  2.052e+03 -2.107e+03 -2.218e+03 -2.219e+03  2.282e+03\n",
      "  2.286e+03 -2.287e+03 -2.289e+03 -2.289e+03  2.379e+03 -2.381e+03\n",
      " -2.565e+03 -2.602e+03 -2.650e+03  2.660e+03 -2.702e+03 -2.761e+03\n",
      " -2.772e+03  2.835e+03 -2.860e+03 -2.869e+03  2.875e+03 -2.893e+03\n",
      " -2.918e+03 -2.918e+03  2.988e+03 -3.000e+03 -3.013e+03  3.470e+03\n",
      " -3.574e+03 -3.725e+03 -3.733e+03  3.978e+03  4.084e+03 -4.113e+03\n",
      "  4.229e+03  4.412e+03  4.695e+03 -4.752e+03 -5.255e+03 -5.546e+03\n",
      " -6.423e+03]\n",
      "durations 4.0 5166.0\n",
      "Concordance Index 0.693939393939394\n",
      "Integrated Brier Score: 0.1901732538104823\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m296.6620\u001b[0m       \u001b[32m61.0815\u001b[0m  0.0183\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m322.7235\u001b[0m       \u001b[32m54.2901\u001b[0m  0.0177\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      2      \u001b[36m280.8637\u001b[0m       64.5277  0.0216\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2      \u001b[36m306.7453\u001b[0m       57.1687  0.0164\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      3      \u001b[36m278.7928\u001b[0m       71.7672  0.0164\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m294.4927\u001b[0m       \u001b[32m34.3038\u001b[0m  0.0225\n",
      "      3      \u001b[36m297.3498\u001b[0m       62.7811  0.0176\n",
      "      4      \u001b[36m277.7374\u001b[0m       73.1530  0.0158\n",
      "      2      \u001b[36m270.9352\u001b[0m       37.8311  0.0159\n",
      "      4      \u001b[36m296.8681\u001b[0m       64.2017  0.0156\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m314.5132\u001b[0m       \u001b[32m44.9476\u001b[0m  0.0389\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m314.3935\u001b[0m       \u001b[32m68.1542\u001b[0m  0.0237\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m314.9076\u001b[0m       \u001b[32m46.3057\u001b[0m  0.0409\n",
      "      5      280.2386       69.4431  0.0233\n",
      "      3      \u001b[36m259.8002\u001b[0m       40.4809  0.0213\n",
      "      5      \u001b[36m294.6154\u001b[0m       62.5807  0.0205\n",
      "      2      \u001b[36m304.1792\u001b[0m       \u001b[32m60.1114\u001b[0m  0.0176\n",
      "      4      \u001b[36m257.3942\u001b[0m       39.9796  0.0151\n",
      "      6      \u001b[36m268.1517\u001b[0m       64.4243  0.0221\n",
      "      3      305.3885       \u001b[32m55.5371\u001b[0m  0.0158\n",
      "      6      \u001b[36m288.7068\u001b[0m       59.6812  0.0192\n",
      "      2      328.1547       45.1841  0.0409\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      2      \u001b[36m311.1781\u001b[0m       \u001b[32m46.0855\u001b[0m  0.0326\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      7      276.0267       \u001b[32m59.0181\u001b[0m  0.0150\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      4      307.2072       \u001b[32m51.0362\u001b[0m  0.0160\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      7      293.9347       55.9925  0.0162\n",
      "      5      \u001b[36m247.6918\u001b[0m       39.0796  0.0279\n",
      "      8      \u001b[36m267.6758\u001b[0m       \u001b[32m55.4256\u001b[0m  0.0151\n",
      "      3      316.9718       47.3078  0.0316\n",
      "      5      \u001b[36m294.6452\u001b[0m       \u001b[32m48.4096\u001b[0m  0.0197\n",
      "      8      289.7942       \u001b[32m51.9130\u001b[0m  0.0211\n",
      "      6      \u001b[36m244.5179\u001b[0m       37.9332  0.0186\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m327.8753\u001b[0m       \u001b[32m59.7747\u001b[0m  0.0236\n",
      "      9      \u001b[36m260.8582\u001b[0m       \u001b[32m53.3863\u001b[0m  0.0220\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m286.5614\u001b[0m       \u001b[32m51.3446\u001b[0m  0.0450\n",
      "      6      295.8387       \u001b[32m47.1426\u001b[0m  0.0194\n",
      "      9      \u001b[36m282.2798\u001b[0m       \u001b[32m49.5408\u001b[0m  0.0218\n",
      "      7      253.6448       35.8946  0.0183\n",
      "      3      329.1399       \u001b[32m44.7434\u001b[0m  0.0554\n",
      "      2      \u001b[36m311.3496\u001b[0m       61.2218  0.0271\n",
      "     10      262.4114       \u001b[32m52.2771\u001b[0m  0.0257\n",
      "      7      296.2811       \u001b[32m46.4747\u001b[0m  0.0198\n",
      "      4      \u001b[36m308.7866\u001b[0m       49.8432  0.0452\n",
      "     10      \u001b[36m279.3295\u001b[0m       \u001b[32m48.1821\u001b[0m  0.0176\n",
      "      8      251.6568       \u001b[32m33.2411\u001b[0m  0.0189\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      3      \u001b[36m297.1117\u001b[0m       63.6081  0.0184\n",
      "      8      \u001b[36m289.6674\u001b[0m       \u001b[32m46.2301\u001b[0m  0.0159\n",
      "      9      245.3439       \u001b[32m31.2441\u001b[0m  0.0150\n",
      "      2      \u001b[36m279.1547\u001b[0m       \u001b[32m51.1524\u001b[0m  0.0486\n",
      "      4      301.9676       60.7007  0.0184\n",
      "      4      329.6984       44.9504  0.0488\n",
      "     10      248.5759       \u001b[32m30.3375\u001b[0m  0.0167\n",
      "      9      291.2519       46.2780  0.0224\n",
      "      5      313.7761       50.2378  0.0467\n",
      "      5      299.5859       \u001b[32m56.4097\u001b[0m  0.0193\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m310.7908\u001b[0m       \u001b[32m45.9862\u001b[0m  0.0451\n",
      "     10      294.3340       46.3497  0.0172\n",
      "Restoring best model from epoch 8.\n",
      "      6      \u001b[36m293.1790\u001b[0m       \u001b[32m53.0588\u001b[0m  0.0156\n",
      "      5      316.8522       45.4241  0.0392\n",
      "      3      293.2205       52.8962  0.0472\n",
      "      7      294.9462       \u001b[32m49.9678\u001b[0m  0.0202\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      2      \u001b[36m304.1550\u001b[0m       \u001b[32m45.8913\u001b[0m  0.0386\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      6      341.6154       51.0858  0.0580\n",
      "      6      320.0408       48.2965  0.0317\n",
      "      3      328.7849       48.0329  0.0343\n",
      "      4      315.4343       52.0384  0.0566\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m280.4248\u001b[0m       \u001b[32m33.1545\u001b[0m  0.0530\n",
      "      7      318.9771       50.6817  0.0486\n",
      "      8      293.7052       \u001b[32m48.2320\u001b[0m  0.0676\n",
      "      7      317.5929       48.4971  0.0540\n",
      "      4      337.4815       46.7801  0.0337\n",
      "      9      \u001b[36m291.9304\u001b[0m       \u001b[32m47.2207\u001b[0m  0.0158\n",
      "      5      284.7925       \u001b[32m50.5755\u001b[0m  0.0494\n",
      "      2      \u001b[36m266.2764\u001b[0m       \u001b[32m30.9155\u001b[0m  0.0316\n",
      "     10      \u001b[36m290.1258\u001b[0m       \u001b[32m46.7829\u001b[0m  0.0148\n",
      "      8      316.3834       50.7317  0.0345\n",
      "      8      308.8701       46.7462  0.0333\n",
      "      5      320.8499       46.0283  0.0319\n",
      "      6      295.0319       \u001b[32m49.6260\u001b[0m  0.0317\n",
      "      3      289.7849       \u001b[32m30.5677\u001b[0m  0.0303\n",
      "      9      \u001b[36m309.9560\u001b[0m       49.3784  0.0293\n",
      "      9      \u001b[36m297.7748\u001b[0m       46.2960  0.0306\n",
      "      6      310.9373       46.6246  0.0307\n",
      "     10      \u001b[36m299.2502\u001b[0m       48.0413  0.0288\n",
      "      7      286.6859       \u001b[32m48.9065\u001b[0m  0.0306\n",
      "Restoring best model from epoch 3.\n",
      "      4      306.4554       \u001b[32m29.8010\u001b[0m  0.0305\n",
      "     10      313.2151       46.6431  0.0316\n",
      "Restoring best model from epoch 1.\n",
      "      7      309.9928       47.6606  0.0308\n",
      "      8      279.8620       49.3722  0.0305\n",
      "      5      280.4480       29.9340  0.0299\n",
      "      8      \u001b[36m299.1354\u001b[0m       49.9682  0.0297\n",
      "      6      273.4179       30.3180  0.0300\n",
      "      9      281.1421       50.7813  0.0332\n",
      "      9      305.8485       52.8896  0.0304\n",
      "      7      268.0335       31.0522  0.0295\n",
      "     10      279.5854       53.1128  0.0306\n",
      "Restoring best model from epoch 7.\n",
      "     10      \u001b[36m292.6815\u001b[0m       56.4146  0.0303\n",
      "Restoring best model from epoch 2.\n",
      "      8      \u001b[36m258.2922\u001b[0m       31.1614  0.0306\n",
      "      9      258.5411       31.2563  0.0308\n",
      "     10      270.1947       31.0307  0.0318\n",
      "Restoring best model from epoch 4.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m313.3059\u001b[0m       \u001b[32m58.9460\u001b[0m  0.0163\n",
      "      2      \u001b[36m292.6972\u001b[0m       59.9874  0.0162\n",
      "      3      \u001b[36m289.9590\u001b[0m       62.2384  0.0153\n",
      "      4      \u001b[36m287.0759\u001b[0m       61.0914  0.0155\n",
      "      5      290.4034       \u001b[32m58.4855\u001b[0m  0.0182\n",
      "      6      \u001b[36m276.8840\u001b[0m       \u001b[32m56.5361\u001b[0m  0.0166\n",
      "      7      281.2873       \u001b[32m55.2650\u001b[0m  0.0161\n",
      "      8      280.6227       \u001b[32m54.8571\u001b[0m  0.0156\n",
      "      9      \u001b[36m274.8106\u001b[0m       \u001b[32m54.7452\u001b[0m  0.0176\n",
      "     10      \u001b[36m271.1913\u001b[0m       \u001b[32m54.7229\u001b[0m  0.0160\n",
      "y_train breslow final [-1.000e+00 -3.000e+00 -3.000e+00 -3.000e+00 -3.000e+00 -3.000e+00\n",
      " -3.000e+00 -3.000e+00 -4.000e+00 -4.000e+00 -4.000e+00 -5.000e+00\n",
      " -7.000e+00 -7.000e+00 -7.000e+00 -7.000e+00 -7.000e+00  7.000e+00\n",
      " -8.000e+00 -1.000e+01 -1.400e+01 -1.500e+01 -1.700e+01  2.300e+01\n",
      " -2.300e+01  3.700e+01 -5.000e+01 -5.500e+01 -5.800e+01 -6.200e+01\n",
      " -6.200e+01 -6.300e+01 -7.000e+01 -7.200e+01 -7.200e+01 -7.300e+01\n",
      " -7.600e+01 -7.700e+01 -8.200e+01 -8.400e+01 -9.000e+01  9.600e+01\n",
      " -1.050e+02 -1.120e+02  1.130e+02 -1.140e+02 -1.220e+02 -1.340e+02\n",
      " -1.390e+02 -1.530e+02  1.550e+02 -1.620e+02  1.620e+02 -1.690e+02\n",
      " -1.740e+02 -1.790e+02 -1.820e+02 -1.840e+02 -1.940e+02  1.940e+02\n",
      "  1.990e+02 -1.990e+02 -2.030e+02 -2.040e+02  2.050e+02 -2.070e+02\n",
      " -2.100e+02  2.140e+02  2.280e+02 -2.310e+02  2.340e+02 -2.420e+02\n",
      "  2.420e+02 -2.420e+02 -2.430e+02  2.450e+02 -2.490e+02 -2.570e+02\n",
      "  2.690e+02 -2.740e+02 -2.790e+02 -2.860e+02 -2.870e+02 -3.010e+02\n",
      " -3.130e+02  3.150e+02  3.160e+02 -3.170e+02 -3.200e+02 -3.260e+02\n",
      " -3.280e+02 -3.330e+02 -3.420e+02  3.470e+02  3.490e+02  3.510e+02\n",
      " -3.540e+02  3.540e+02 -3.680e+02 -3.720e+02  3.720e+02 -3.720e+02\n",
      "  3.780e+02 -3.840e+02  3.880e+02 -3.950e+02  3.980e+02 -4.030e+02\n",
      " -4.050e+02 -4.070e+02 -4.080e+02 -4.110e+02 -4.140e+02 -4.140e+02\n",
      " -4.160e+02 -4.180e+02 -4.270e+02 -4.280e+02 -4.330e+02 -4.340e+02\n",
      "  4.350e+02 -4.380e+02 -4.380e+02 -4.420e+02 -4.420e+02 -4.420e+02\n",
      " -4.430e+02  4.440e+02 -4.490e+02 -4.510e+02 -4.550e+02  4.560e+02\n",
      " -4.580e+02 -4.610e+02 -4.610e+02  4.660e+02 -4.670e+02 -4.670e+02\n",
      " -4.710e+02 -4.780e+02 -4.820e+02 -4.860e+02 -4.870e+02  4.920e+02\n",
      " -4.940e+02 -4.970e+02 -5.000e+02 -5.030e+02 -5.090e+02 -5.120e+02\n",
      " -5.150e+02 -5.160e+02 -5.220e+02 -5.230e+02 -5.230e+02 -5.300e+02\n",
      " -5.310e+02 -5.320e+02 -5.330e+02 -5.330e+02  5.370e+02 -5.420e+02\n",
      " -5.440e+02  5.470e+02 -5.480e+02  5.590e+02 -5.630e+02 -5.640e+02\n",
      " -5.660e+02 -5.660e+02 -5.670e+02 -5.670e+02 -5.690e+02 -5.690e+02\n",
      " -5.710e+02 -5.710e+02 -5.730e+02 -5.760e+02  5.780e+02 -5.820e+02\n",
      " -5.840e+02  5.920e+02 -5.960e+02 -5.990e+02 -6.040e+02  6.050e+02\n",
      "  6.070e+02 -6.080e+02 -6.090e+02 -6.090e+02 -6.110e+02 -6.110e+02\n",
      " -6.230e+02 -6.270e+02 -6.330e+02 -6.380e+02  6.390e+02  6.480e+02\n",
      "  6.480e+02 -6.500e+02 -6.510e+02 -6.510e+02 -6.550e+02 -6.560e+02\n",
      " -6.710e+02  6.820e+02 -6.850e+02 -6.860e+02 -7.060e+02 -7.060e+02\n",
      "  7.090e+02 -7.140e+02 -7.150e+02 -7.210e+02 -7.240e+02  7.270e+02\n",
      " -7.350e+02 -7.360e+02 -7.380e+02  7.420e+02 -7.430e+02 -7.480e+02\n",
      " -7.540e+02  7.580e+02 -7.580e+02 -7.600e+02 -7.720e+02  7.750e+02\n",
      " -7.770e+02 -7.850e+02 -7.870e+02  7.880e+02 -7.920e+02 -7.950e+02\n",
      " -7.960e+02 -8.000e+02 -8.050e+02  8.140e+02  8.140e+02  8.190e+02\n",
      "  8.210e+02 -8.260e+02 -8.320e+02 -8.350e+02 -8.370e+02 -8.460e+02\n",
      " -8.540e+02 -8.600e+02 -8.630e+02 -8.680e+02 -8.780e+02  8.860e+02\n",
      " -9.000e+02 -9.080e+02 -9.080e+02 -9.140e+02 -9.140e+02 -9.160e+02\n",
      "  9.190e+02 -9.190e+02 -9.260e+02  9.330e+02 -9.330e+02 -9.350e+02\n",
      " -9.460e+02 -9.490e+02 -9.550e+02  9.610e+02  9.620e+02 -9.640e+02\n",
      " -9.640e+02 -9.680e+02  9.840e+02  9.870e+02 -9.920e+02 -1.004e+03\n",
      " -1.012e+03 -1.026e+03 -1.032e+03  1.033e+03 -1.040e+03 -1.058e+03\n",
      " -1.069e+03 -1.076e+03 -1.078e+03 -1.079e+03 -1.099e+03  1.106e+03\n",
      " -1.112e+03 -1.115e+03 -1.120e+03  1.120e+03 -1.127e+03 -1.130e+03\n",
      "  1.137e+03 -1.139e+03  1.152e+03 -1.173e+03 -1.189e+03 -1.191e+03\n",
      " -1.201e+03 -1.201e+03 -1.210e+03 -1.213e+03 -1.217e+03 -1.219e+03\n",
      "  1.220e+03 -1.222e+03 -1.227e+03 -1.229e+03 -1.236e+03  1.242e+03\n",
      " -1.245e+03  1.251e+03 -1.257e+03  1.262e+03 -1.279e+03 -1.294e+03\n",
      " -1.300e+03 -1.314e+03 -1.320e+03  1.335e+03  1.351e+03 -1.354e+03\n",
      " -1.359e+03 -1.382e+03 -1.387e+03 -1.393e+03 -1.397e+03 -1.401e+03\n",
      " -1.412e+03 -1.421e+03 -1.426e+03 -1.426e+03 -1.428e+03 -1.453e+03\n",
      " -1.469e+03 -1.470e+03  1.481e+03  1.491e+03 -1.519e+03  1.525e+03\n",
      " -1.540e+03  1.547e+03 -1.553e+03 -1.568e+03  1.578e+03  1.585e+03\n",
      " -1.588e+03 -1.631e+03 -1.650e+03  1.666e+03 -1.721e+03 -1.752e+03\n",
      "  1.762e+03 -1.806e+03 -1.828e+03 -1.834e+03 -1.868e+03  1.886e+03\n",
      "  1.915e+03 -1.943e+03 -1.989e+03  2.052e+03 -2.078e+03 -2.107e+03\n",
      " -2.218e+03 -2.219e+03  2.235e+03  2.282e+03  2.286e+03 -2.289e+03\n",
      "  2.379e+03 -2.381e+03  2.433e+03 -2.493e+03 -2.585e+03 -2.650e+03\n",
      "  2.660e+03 -2.772e+03 -2.860e+03 -2.869e+03  2.875e+03 -2.893e+03\n",
      "  2.907e+03 -2.918e+03 -2.918e+03  2.988e+03 -3.000e+03 -3.013e+03\n",
      "  3.200e+03 -3.253e+03  3.470e+03  3.571e+03 -3.574e+03 -3.725e+03\n",
      " -3.733e+03 -3.761e+03  3.978e+03  4.068e+03  4.084e+03 -4.113e+03\n",
      "  4.412e+03  4.445e+03  4.695e+03 -4.752e+03  5.166e+03 -5.546e+03\n",
      " -6.423e+03]\n",
      "Concordance Index 0.7782943978226355\n",
      "Integrated Brier Score: 0.16649168382355736\n",
      "y_train breslow final [-1.000e+00 -3.000e+00 -3.000e+00 -3.000e+00 -3.000e+00 -3.000e+00\n",
      " -3.000e+00 -3.000e+00 -4.000e+00 -4.000e+00 -4.000e+00 -5.000e+00\n",
      " -7.000e+00 -7.000e+00 -7.000e+00 -7.000e+00 -7.000e+00  7.000e+00\n",
      " -8.000e+00 -1.000e+01 -1.400e+01 -1.500e+01 -1.700e+01  2.300e+01\n",
      " -2.300e+01  3.700e+01 -5.000e+01 -5.500e+01 -5.800e+01 -6.200e+01\n",
      " -6.200e+01 -6.300e+01 -7.000e+01 -7.200e+01 -7.200e+01 -7.300e+01\n",
      " -7.600e+01 -7.700e+01 -8.200e+01 -8.400e+01 -9.000e+01  9.600e+01\n",
      " -1.050e+02 -1.120e+02  1.130e+02 -1.140e+02 -1.220e+02 -1.340e+02\n",
      " -1.390e+02 -1.530e+02  1.550e+02 -1.620e+02  1.620e+02 -1.690e+02\n",
      " -1.740e+02 -1.790e+02 -1.820e+02 -1.840e+02 -1.940e+02  1.940e+02\n",
      "  1.990e+02 -1.990e+02 -2.030e+02 -2.040e+02  2.050e+02 -2.070e+02\n",
      " -2.100e+02  2.140e+02  2.280e+02 -2.310e+02  2.340e+02 -2.420e+02\n",
      "  2.420e+02 -2.420e+02 -2.430e+02  2.450e+02 -2.490e+02 -2.570e+02\n",
      "  2.690e+02 -2.740e+02 -2.790e+02 -2.860e+02 -2.870e+02 -3.010e+02\n",
      " -3.130e+02  3.150e+02  3.160e+02 -3.170e+02 -3.200e+02 -3.260e+02\n",
      " -3.280e+02 -3.330e+02 -3.420e+02  3.470e+02  3.490e+02  3.510e+02\n",
      " -3.540e+02  3.540e+02 -3.680e+02 -3.720e+02  3.720e+02 -3.720e+02\n",
      "  3.780e+02 -3.840e+02  3.880e+02 -3.950e+02  3.980e+02 -4.030e+02\n",
      " -4.050e+02 -4.070e+02 -4.080e+02 -4.110e+02 -4.140e+02 -4.140e+02\n",
      " -4.160e+02 -4.180e+02 -4.270e+02 -4.280e+02 -4.330e+02 -4.340e+02\n",
      "  4.350e+02 -4.380e+02 -4.380e+02 -4.420e+02 -4.420e+02 -4.420e+02\n",
      " -4.430e+02  4.440e+02 -4.490e+02 -4.510e+02 -4.550e+02  4.560e+02\n",
      " -4.580e+02 -4.610e+02 -4.610e+02  4.660e+02 -4.670e+02 -4.670e+02\n",
      " -4.710e+02 -4.780e+02 -4.820e+02 -4.860e+02 -4.870e+02  4.920e+02\n",
      " -4.940e+02 -4.970e+02 -5.000e+02 -5.030e+02 -5.090e+02 -5.120e+02\n",
      " -5.150e+02 -5.160e+02 -5.220e+02 -5.230e+02 -5.230e+02 -5.300e+02\n",
      " -5.310e+02 -5.320e+02 -5.330e+02 -5.330e+02  5.370e+02 -5.420e+02\n",
      " -5.440e+02  5.470e+02 -5.480e+02  5.590e+02 -5.630e+02 -5.640e+02\n",
      " -5.660e+02 -5.660e+02 -5.670e+02 -5.670e+02 -5.690e+02 -5.690e+02\n",
      " -5.710e+02 -5.710e+02 -5.730e+02 -5.760e+02  5.780e+02 -5.820e+02\n",
      " -5.840e+02  5.920e+02 -5.960e+02 -5.990e+02 -6.040e+02  6.050e+02\n",
      "  6.070e+02 -6.080e+02 -6.090e+02 -6.090e+02 -6.110e+02 -6.110e+02\n",
      " -6.230e+02 -6.270e+02 -6.330e+02 -6.380e+02  6.390e+02  6.480e+02\n",
      "  6.480e+02 -6.500e+02 -6.510e+02 -6.510e+02 -6.550e+02 -6.560e+02\n",
      " -6.710e+02  6.820e+02 -6.850e+02 -6.860e+02 -7.060e+02 -7.060e+02\n",
      "  7.090e+02 -7.140e+02 -7.150e+02 -7.210e+02 -7.240e+02  7.270e+02\n",
      " -7.350e+02 -7.360e+02 -7.380e+02  7.420e+02 -7.430e+02 -7.480e+02\n",
      " -7.540e+02  7.580e+02 -7.580e+02 -7.600e+02 -7.720e+02  7.750e+02\n",
      " -7.770e+02 -7.850e+02 -7.870e+02  7.880e+02 -7.920e+02 -7.950e+02\n",
      " -7.960e+02 -8.000e+02 -8.050e+02  8.140e+02  8.140e+02  8.190e+02\n",
      "  8.210e+02 -8.260e+02 -8.320e+02 -8.350e+02 -8.370e+02 -8.460e+02\n",
      " -8.540e+02 -8.600e+02 -8.630e+02 -8.680e+02 -8.780e+02  8.860e+02\n",
      " -9.000e+02 -9.080e+02 -9.080e+02 -9.140e+02 -9.140e+02 -9.160e+02\n",
      "  9.190e+02 -9.190e+02 -9.260e+02  9.330e+02 -9.330e+02 -9.350e+02\n",
      " -9.460e+02 -9.490e+02 -9.550e+02  9.610e+02  9.620e+02 -9.640e+02\n",
      " -9.640e+02 -9.680e+02  9.840e+02  9.870e+02 -9.920e+02 -1.004e+03\n",
      " -1.012e+03 -1.026e+03 -1.032e+03  1.033e+03 -1.040e+03 -1.058e+03\n",
      " -1.069e+03 -1.076e+03 -1.078e+03 -1.079e+03 -1.099e+03  1.106e+03\n",
      " -1.112e+03 -1.115e+03 -1.120e+03  1.120e+03 -1.127e+03 -1.130e+03\n",
      "  1.137e+03 -1.139e+03  1.152e+03 -1.173e+03 -1.189e+03 -1.191e+03\n",
      " -1.201e+03 -1.201e+03 -1.210e+03 -1.213e+03 -1.217e+03 -1.219e+03\n",
      "  1.220e+03 -1.222e+03 -1.227e+03 -1.229e+03 -1.236e+03  1.242e+03\n",
      " -1.245e+03  1.251e+03 -1.257e+03  1.262e+03 -1.279e+03 -1.294e+03\n",
      " -1.300e+03 -1.314e+03 -1.320e+03  1.335e+03  1.351e+03 -1.354e+03\n",
      " -1.359e+03 -1.382e+03 -1.387e+03 -1.393e+03 -1.397e+03 -1.401e+03\n",
      " -1.412e+03 -1.421e+03 -1.426e+03 -1.426e+03 -1.428e+03 -1.453e+03\n",
      " -1.469e+03 -1.470e+03  1.481e+03  1.491e+03 -1.519e+03  1.525e+03\n",
      " -1.540e+03  1.547e+03 -1.553e+03 -1.568e+03  1.578e+03  1.585e+03\n",
      " -1.588e+03 -1.631e+03 -1.650e+03  1.666e+03 -1.721e+03 -1.752e+03\n",
      "  1.762e+03 -1.806e+03 -1.828e+03 -1.834e+03 -1.868e+03  1.886e+03\n",
      "  1.915e+03 -1.943e+03 -1.989e+03  2.052e+03 -2.078e+03 -2.107e+03\n",
      " -2.218e+03 -2.219e+03  2.235e+03  2.282e+03  2.286e+03 -2.289e+03\n",
      "  2.379e+03 -2.381e+03  2.433e+03 -2.493e+03 -2.585e+03 -2.650e+03\n",
      "  2.660e+03 -2.772e+03 -2.860e+03 -2.869e+03  2.875e+03 -2.893e+03\n",
      "  2.907e+03 -2.918e+03 -2.918e+03  2.988e+03 -3.000e+03 -3.013e+03\n",
      "  3.200e+03 -3.253e+03  3.470e+03  3.571e+03 -3.574e+03 -3.725e+03\n",
      " -3.733e+03 -3.761e+03  3.978e+03  4.068e+03  4.084e+03 -4.113e+03\n",
      "  4.412e+03  4.445e+03  4.695e+03 -4.752e+03  5.166e+03 -5.546e+03\n",
      " -6.423e+03]\n",
      "durations 2.0 5255.0\n",
      "Concordance Index 0.7480586712683348\n",
      "Integrated Brier Score: 0.2572629896680649\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m318.3043\u001b[0m       \u001b[32m48.6001\u001b[0m  0.0247\n",
      "      2      \u001b[36m308.6640\u001b[0m       52.3736  0.0183\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m300.1771\u001b[0m       \u001b[32m56.0442\u001b[0m  0.0404\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      3      \u001b[36m300.8819\u001b[0m       53.1274  0.0180\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m320.9962\u001b[0m       \u001b[32m53.3545\u001b[0m  0.0191\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      4      301.0423       51.3230  0.0176\n",
      "      2      \u001b[36m294.0026\u001b[0m       \u001b[32m55.9455\u001b[0m  0.0392\n",
      "      2      \u001b[36m304.4691\u001b[0m       55.1059  0.0248\n",
      "      5      \u001b[36m295.7060\u001b[0m       49.3155  0.0225\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m279.0018\u001b[0m       \u001b[32m65.2798\u001b[0m  0.0280\n",
      "      3      \u001b[36m303.1859\u001b[0m       \u001b[32m51.7010\u001b[0m  0.0252\n",
      "      6      298.4922       \u001b[32m47.4330\u001b[0m  0.0226\n",
      "      2      \u001b[36m267.2525\u001b[0m       66.3924  0.0279\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      4      \u001b[36m294.1727\u001b[0m       \u001b[32m48.3022\u001b[0m  0.0222\n",
      "      7      \u001b[36m294.5839\u001b[0m       \u001b[32m46.0396\u001b[0m  0.0164\n",
      "      3      303.7996       \u001b[32m55.4102\u001b[0m  0.0579\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      3      269.4791       65.5733  0.0165\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      8      \u001b[36m291.2444\u001b[0m       \u001b[32m44.9562\u001b[0m  0.0172\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      5      \u001b[36m287.2241\u001b[0m       \u001b[32m46.0550\u001b[0m  0.0249\n",
      "      4      \u001b[36m263.9122\u001b[0m       \u001b[32m63.7554\u001b[0m  0.0223\n",
      "Re-initializing criterion.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m276.0334\u001b[0m       \u001b[32m35.3526\u001b[0m  0.0226\n",
      "      9      293.8919       \u001b[32m44.1239\u001b[0m  0.0162\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      4      \u001b[36m291.7277\u001b[0m       55.8926  0.0425\n",
      "      6      288.9971       \u001b[32m44.3019\u001b[0m  0.0190\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m278.5595\u001b[0m       \u001b[32m53.3014\u001b[0m  0.0455\n",
      "      5      \u001b[36m259.2560\u001b[0m       \u001b[32m62.2260\u001b[0m  0.0240\n",
      "     10      \u001b[36m288.2061\u001b[0m       \u001b[32m43.5424\u001b[0m  0.0167\n",
      "      2      \u001b[36m258.4037\u001b[0m       35.7972  0.0220\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m299.3241\u001b[0m       \u001b[32m66.0167\u001b[0m  0.0292\n",
      "      7      \u001b[36m284.6226\u001b[0m       \u001b[32m43.4818\u001b[0m  0.0212\n",
      "      6      \u001b[36m258.9880\u001b[0m       \u001b[32m60.2813\u001b[0m  0.0243\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      5      295.2678       57.5975  0.0445\n",
      "      8      \u001b[36m283.2364\u001b[0m       \u001b[32m42.7869\u001b[0m  0.0220\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      3      \u001b[36m249.6215\u001b[0m       38.0967  0.0365\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m306.1967\u001b[0m       \u001b[32m40.5855\u001b[0m  0.0600\n",
      "      2      \u001b[36m287.0804\u001b[0m       \u001b[32m65.8413\u001b[0m  0.0410\n",
      "      7      \u001b[36m257.9433\u001b[0m       \u001b[32m59.2558\u001b[0m  0.0299\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m270.7758\u001b[0m       \u001b[32m32.1199\u001b[0m  0.0678\n",
      "      9      285.9771       \u001b[32m42.4275\u001b[0m  0.0184\n",
      "      4      \u001b[36m241.2680\u001b[0m       40.6888  0.0187\n",
      "      2      283.5812       \u001b[32m52.4223\u001b[0m  0.0651\n",
      "      8      \u001b[36m254.1703\u001b[0m       \u001b[32m58.7081\u001b[0m  0.0187\n",
      "     10      \u001b[36m276.1175\u001b[0m       \u001b[32m42.2358\u001b[0m  0.0171\n",
      "      6      \u001b[36m286.2527\u001b[0m       59.7392  0.0395\n",
      "      5      \u001b[36m240.1893\u001b[0m       42.8161  0.0176\n",
      "      3      \u001b[36m286.2492\u001b[0m       \u001b[32m64.3098\u001b[0m  0.0284\n",
      "      2      \u001b[36m299.4616\u001b[0m       40.6225  0.0456\n",
      "      2      \u001b[36m258.2225\u001b[0m       \u001b[32m31.7223\u001b[0m  0.0338\n",
      "      9      254.4220       \u001b[32m58.4981\u001b[0m  0.0186\n",
      "      6      241.4545       42.8326  0.0161\n",
      "      4      \u001b[36m281.7458\u001b[0m       \u001b[32m62.1369\u001b[0m  0.0148\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m317.3459\u001b[0m       \u001b[32m44.8641\u001b[0m  0.0588\n",
      "     10      \u001b[36m254.0497\u001b[0m       \u001b[32m58.3565\u001b[0m  0.0158\n",
      "      7      \u001b[36m227.9968\u001b[0m       42.9678  0.0157\n",
      "      3      285.2231       55.0833  0.0455\n",
      "      5      \u001b[36m280.4817\u001b[0m       \u001b[32m59.6115\u001b[0m  0.0167\n",
      "      7      289.4154       61.9987  0.0408\n",
      "      3      324.4206       42.2192  0.0317\n",
      "      3      269.9467       32.9686  0.0424\n",
      "      8      231.9625       42.1474  0.0228\n",
      "      6      \u001b[36m275.7192\u001b[0m       \u001b[32m57.3741\u001b[0m  0.0259\n",
      "      2      \u001b[36m300.8158\u001b[0m       \u001b[32m44.8407\u001b[0m  0.0369\n",
      "      4      \u001b[36m272.1632\u001b[0m       57.9422  0.0326\n",
      "      8      \u001b[36m279.3582\u001b[0m       64.3021  0.0339\n",
      "      4      317.5638       43.7474  0.0351\n",
      "      9      231.2826       40.4613  0.0222\n",
      "      7      276.4807       \u001b[32m56.0096\u001b[0m  0.0284\n",
      "     10      228.6669       39.2486  0.0158\n",
      "Restoring best model from epoch 1.\n",
      "      4      \u001b[36m257.3812\u001b[0m       35.2035  0.0439\n",
      "      3      323.7857       \u001b[32m44.7208\u001b[0m  0.0350\n",
      "      5      276.3324       60.1487  0.0387\n",
      "      9      281.7123       66.4345  0.0346\n",
      "      8      276.5365       \u001b[32m55.2837\u001b[0m  0.0224\n",
      "      5      301.0577       46.2675  0.0396\n",
      "      9      \u001b[36m272.6441\u001b[0m       \u001b[32m55.0865\u001b[0m  0.0156\n",
      "     10      280.0192       69.1511  0.0355\n",
      "      5      264.3093       33.7252  0.0441\n",
      "Restoring best model from epoch 3.\n",
      "      6      \u001b[36m263.7979\u001b[0m       62.7408  0.0405\n",
      "     10      273.8440       55.1283  0.0180\n",
      "      4      337.5938       \u001b[32m44.4342\u001b[0m  0.0468\n",
      "Restoring best model from epoch 9.\n",
      "      6      327.1858       44.1798  0.0400\n",
      "      6      \u001b[36m249.5008\u001b[0m       34.1156  0.0324\n",
      "      7      268.9257       64.5012  0.0334\n",
      "      5      311.1432       \u001b[32m44.3221\u001b[0m  0.0320\n",
      "      7      \u001b[36m296.6220\u001b[0m       43.7255  0.0307\n",
      "      7      \u001b[36m247.5111\u001b[0m       36.0221  0.0308\n",
      "      8      266.4593       68.3748  0.0315\n",
      "      6      301.7818       45.3621  0.0319\n",
      "      8      297.5233       44.5662  0.0318\n",
      "      8      265.0013       37.6648  0.0300\n",
      "      9      \u001b[36m260.5343\u001b[0m       72.4170  0.0308\n",
      "      7      \u001b[36m298.8134\u001b[0m       47.1760  0.0315\n",
      "      9      \u001b[36m288.1953\u001b[0m       45.7194  0.0325\n",
      "      9      \u001b[36m243.8889\u001b[0m       39.0765  0.0292\n",
      "     10      261.5859       78.2393  0.0299\n",
      "Restoring best model from epoch 2.\n",
      "      8      305.4134       49.9469  0.0291\n",
      "     10      293.6507       45.9867  0.0287\n",
      "Restoring best model from epoch 1.\n",
      "     10      248.7488       39.2137  0.0283\n",
      "Restoring best model from epoch 2.\n",
      "      9      299.8306       52.7202  0.0291\n",
      "     10      301.8152       54.4930  0.0282\n",
      "Restoring best model from epoch 5.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m288.2824\u001b[0m       \u001b[32m54.8925\u001b[0m  0.0349\n",
      "      2      \u001b[36m284.5228\u001b[0m       \u001b[32m54.4504\u001b[0m  0.0299\n",
      "      3      298.4055       \u001b[32m52.8440\u001b[0m  0.0348\n",
      "      4      303.8228       53.3970  0.0274\n",
      "      5      293.0258       54.2894  0.0278\n",
      "      6      \u001b[36m261.0721\u001b[0m       54.3083  0.0448\n",
      "      7      278.3136       \u001b[32m52.0302\u001b[0m  0.0622\n",
      "      8      283.9834       \u001b[32m49.9033\u001b[0m  0.0304\n",
      "      9      268.1412       \u001b[32m49.3290\u001b[0m  0.0436\n",
      "     10      273.6880       49.5049  0.0289\n",
      "Restoring best model from epoch 9.\n",
      "y_train breslow final [-2.000e+00 -3.000e+00 -3.000e+00 -3.000e+00 -3.000e+00 -3.000e+00\n",
      " -3.000e+00 -3.000e+00 -3.000e+00 -4.000e+00 -4.000e+00 -4.000e+00\n",
      " -5.000e+00 -6.000e+00 -7.000e+00 -7.000e+00 -7.000e+00 -7.000e+00\n",
      " -7.000e+00 -8.000e+00 -1.000e+01 -1.400e+01 -1.500e+01 -1.700e+01\n",
      "  2.300e+01 -2.300e+01 -3.900e+01 -5.000e+01 -5.800e+01 -6.200e+01\n",
      " -6.200e+01 -6.300e+01 -7.000e+01 -7.200e+01 -7.200e+01 -7.400e+01\n",
      " -7.600e+01 -7.700e+01 -8.400e+01 -9.000e+01  9.600e+01 -1.050e+02\n",
      "  1.110e+02  1.130e+02 -1.140e+02 -1.220e+02 -1.340e+02 -1.530e+02\n",
      "  1.550e+02 -1.620e+02  1.620e+02 -1.660e+02 -1.690e+02 -1.740e+02\n",
      "  1.780e+02 -1.900e+02 -1.940e+02  1.940e+02 -1.990e+02 -2.040e+02\n",
      "  2.050e+02 -2.070e+02 -2.100e+02 -2.300e+02 -2.310e+02  2.400e+02\n",
      "  2.410e+02  2.420e+02 -2.420e+02 -2.430e+02  2.450e+02 -2.490e+02\n",
      " -2.570e+02  2.690e+02 -2.740e+02 -2.860e+02 -2.870e+02 -2.920e+02\n",
      "  3.150e+02  3.160e+02 -3.170e+02 -3.200e+02 -3.280e+02 -3.330e+02\n",
      " -3.360e+02 -3.370e+02 -3.430e+02  3.470e+02  3.510e+02  3.540e+02\n",
      " -3.680e+02  3.720e+02 -3.720e+02  3.780e+02 -3.840e+02  3.880e+02\n",
      " -3.950e+02  3.980e+02 -4.030e+02 -4.050e+02 -4.070e+02 -4.080e+02\n",
      " -4.140e+02 -4.160e+02 -4.170e+02 -4.190e+02 -4.270e+02 -4.280e+02\n",
      " -4.310e+02 -4.330e+02  4.350e+02  4.380e+02 -4.380e+02 -4.420e+02\n",
      " -4.420e+02 -4.430e+02  4.440e+02 -4.510e+02 -4.540e+02 -4.540e+02\n",
      " -4.550e+02 -4.550e+02  4.560e+02 -4.570e+02 -4.610e+02  4.660e+02\n",
      " -4.670e+02 -4.670e+02 -4.780e+02 -4.860e+02 -4.870e+02 -4.870e+02\n",
      " -4.910e+02  4.920e+02 -4.920e+02  4.920e+02 -4.940e+02 -4.940e+02\n",
      " -4.970e+02 -5.000e+02 -5.020e+02 -5.030e+02 -5.080e+02  5.120e+02\n",
      " -5.120e+02 -5.150e+02 -5.160e+02 -5.220e+02 -5.230e+02 -5.230e+02\n",
      " -5.260e+02 -5.300e+02  5.310e+02 -5.310e+02 -5.320e+02 -5.330e+02\n",
      " -5.330e+02  5.370e+02  5.380e+02 -5.420e+02 -5.440e+02 -5.440e+02\n",
      "  5.470e+02 -5.480e+02  5.590e+02  5.610e+02 -5.630e+02 -5.640e+02\n",
      " -5.660e+02 -5.660e+02 -5.660e+02 -5.670e+02 -5.670e+02 -5.690e+02\n",
      " -5.690e+02 -5.710e+02 -5.710e+02 -5.730e+02  5.760e+02 -5.760e+02\n",
      " -5.820e+02 -5.840e+02 -5.850e+02 -5.880e+02  5.920e+02 -5.960e+02\n",
      " -5.990e+02 -6.040e+02  6.070e+02 -6.080e+02 -6.090e+02 -6.150e+02\n",
      " -6.220e+02 -6.230e+02 -6.270e+02 -6.290e+02 -6.290e+02 -6.300e+02\n",
      " -6.330e+02 -6.380e+02  6.390e+02  6.480e+02  6.480e+02 -6.500e+02\n",
      " -6.510e+02 -6.550e+02 -6.560e+02 -6.710e+02 -6.770e+02 -6.780e+02\n",
      "  6.820e+02 -6.850e+02 -7.050e+02  7.090e+02 -7.150e+02 -7.180e+02\n",
      " -7.210e+02  7.220e+02 -7.240e+02  7.270e+02 -7.350e+02 -7.360e+02\n",
      " -7.430e+02 -7.480e+02 -7.540e+02  7.580e+02 -7.580e+02 -7.720e+02\n",
      "  7.750e+02 -7.770e+02 -7.870e+02 -7.920e+02 -8.000e+02 -8.050e+02\n",
      "  8.140e+02 -8.320e+02 -8.350e+02 -8.460e+02 -8.460e+02 -8.540e+02\n",
      " -8.540e+02 -8.600e+02 -8.620e+02 -8.630e+02 -8.780e+02  8.860e+02\n",
      " -8.890e+02 -9.000e+02 -9.080e+02 -9.080e+02 -9.080e+02 -9.140e+02\n",
      " -9.140e+02 -9.160e+02  9.190e+02 -9.190e+02 -9.260e+02  9.330e+02\n",
      " -9.330e+02 -9.390e+02 -9.460e+02 -9.490e+02  9.540e+02 -9.550e+02\n",
      " -9.560e+02  9.610e+02 -9.620e+02  9.620e+02 -9.640e+02 -9.680e+02\n",
      "  9.870e+02 -9.920e+02 -9.920e+02 -9.930e+02  1.011e+03 -1.012e+03\n",
      " -1.021e+03 -1.032e+03  1.033e+03 -1.040e+03 -1.069e+03 -1.076e+03\n",
      " -1.078e+03 -1.079e+03 -1.099e+03  1.106e+03 -1.112e+03 -1.115e+03\n",
      " -1.116e+03 -1.120e+03  1.120e+03 -1.130e+03  1.137e+03 -1.137e+03\n",
      " -1.139e+03 -1.147e+03  1.152e+03 -1.164e+03 -1.173e+03  1.183e+03\n",
      " -1.191e+03 -1.201e+03 -1.201e+03  1.209e+03 -1.210e+03 -1.213e+03\n",
      " -1.217e+03 -1.219e+03  1.220e+03 -1.222e+03 -1.227e+03  1.242e+03\n",
      " -1.245e+03 -1.250e+03  1.251e+03  1.262e+03 -1.277e+03 -1.279e+03\n",
      " -1.294e+03 -1.294e+03 -1.301e+03 -1.337e+03  1.339e+03 -1.341e+03\n",
      " -1.354e+03 -1.359e+03 -1.382e+03 -1.387e+03 -1.393e+03 -1.399e+03\n",
      "  1.401e+03 -1.401e+03 -1.412e+03 -1.421e+03 -1.426e+03 -1.426e+03\n",
      " -1.428e+03 -1.453e+03 -1.458e+03 -1.469e+03 -1.470e+03  1.481e+03\n",
      "  1.491e+03 -1.494e+03 -1.500e+03 -1.519e+03  1.525e+03  1.547e+03\n",
      " -1.553e+03 -1.567e+03 -1.568e+03  1.578e+03  1.585e+03 -1.650e+03\n",
      " -1.706e+03  1.762e+03 -1.796e+03 -1.806e+03 -1.828e+03 -1.834e+03\n",
      " -1.868e+03  1.886e+03  1.891e+03  1.915e+03  1.933e+03 -1.989e+03\n",
      "  2.000e+03  2.052e+03 -2.078e+03 -2.107e+03 -2.218e+03  2.235e+03\n",
      "  2.286e+03 -2.287e+03 -2.289e+03 -2.289e+03  2.433e+03 -2.493e+03\n",
      " -2.565e+03 -2.585e+03 -2.602e+03  2.660e+03 -2.702e+03 -2.761e+03\n",
      " -2.772e+03  2.835e+03 -2.860e+03 -2.869e+03  2.907e+03 -2.918e+03\n",
      " -3.000e+03  3.200e+03 -3.253e+03  3.470e+03  3.571e+03 -3.574e+03\n",
      " -3.733e+03 -3.761e+03  3.978e+03  4.068e+03  4.084e+03 -4.113e+03\n",
      "  4.229e+03  4.445e+03  4.695e+03 -4.752e+03  5.166e+03 -5.255e+03\n",
      " -6.423e+03]\n",
      "Concordance Index 0.791435682769544\n",
      "Integrated Brier Score: 0.24657128405266168\n",
      "y_train breslow final [-2.000e+00 -3.000e+00 -3.000e+00 -3.000e+00 -3.000e+00 -3.000e+00\n",
      " -3.000e+00 -3.000e+00 -3.000e+00 -4.000e+00 -4.000e+00 -4.000e+00\n",
      " -5.000e+00 -6.000e+00 -7.000e+00 -7.000e+00 -7.000e+00 -7.000e+00\n",
      " -7.000e+00 -8.000e+00 -1.000e+01 -1.400e+01 -1.500e+01 -1.700e+01\n",
      "  2.300e+01 -2.300e+01 -3.900e+01 -5.000e+01 -5.800e+01 -6.200e+01\n",
      " -6.200e+01 -6.300e+01 -7.000e+01 -7.200e+01 -7.200e+01 -7.400e+01\n",
      " -7.600e+01 -7.700e+01 -8.400e+01 -9.000e+01  9.600e+01 -1.050e+02\n",
      "  1.110e+02  1.130e+02 -1.140e+02 -1.220e+02 -1.340e+02 -1.530e+02\n",
      "  1.550e+02 -1.620e+02  1.620e+02 -1.660e+02 -1.690e+02 -1.740e+02\n",
      "  1.780e+02 -1.900e+02 -1.940e+02  1.940e+02 -1.990e+02 -2.040e+02\n",
      "  2.050e+02 -2.070e+02 -2.100e+02 -2.300e+02 -2.310e+02  2.400e+02\n",
      "  2.410e+02  2.420e+02 -2.420e+02 -2.430e+02  2.450e+02 -2.490e+02\n",
      " -2.570e+02  2.690e+02 -2.740e+02 -2.860e+02 -2.870e+02 -2.920e+02\n",
      "  3.150e+02  3.160e+02 -3.170e+02 -3.200e+02 -3.280e+02 -3.330e+02\n",
      " -3.360e+02 -3.370e+02 -3.430e+02  3.470e+02  3.510e+02  3.540e+02\n",
      " -3.680e+02  3.720e+02 -3.720e+02  3.780e+02 -3.840e+02  3.880e+02\n",
      " -3.950e+02  3.980e+02 -4.030e+02 -4.050e+02 -4.070e+02 -4.080e+02\n",
      " -4.140e+02 -4.160e+02 -4.170e+02 -4.190e+02 -4.270e+02 -4.280e+02\n",
      " -4.310e+02 -4.330e+02  4.350e+02  4.380e+02 -4.380e+02 -4.420e+02\n",
      " -4.420e+02 -4.430e+02  4.440e+02 -4.510e+02 -4.540e+02 -4.540e+02\n",
      " -4.550e+02 -4.550e+02  4.560e+02 -4.570e+02 -4.610e+02  4.660e+02\n",
      " -4.670e+02 -4.670e+02 -4.780e+02 -4.860e+02 -4.870e+02 -4.870e+02\n",
      " -4.910e+02  4.920e+02 -4.920e+02  4.920e+02 -4.940e+02 -4.940e+02\n",
      " -4.970e+02 -5.000e+02 -5.020e+02 -5.030e+02 -5.080e+02  5.120e+02\n",
      " -5.120e+02 -5.150e+02 -5.160e+02 -5.220e+02 -5.230e+02 -5.230e+02\n",
      " -5.260e+02 -5.300e+02  5.310e+02 -5.310e+02 -5.320e+02 -5.330e+02\n",
      " -5.330e+02  5.370e+02  5.380e+02 -5.420e+02 -5.440e+02 -5.440e+02\n",
      "  5.470e+02 -5.480e+02  5.590e+02  5.610e+02 -5.630e+02 -5.640e+02\n",
      " -5.660e+02 -5.660e+02 -5.660e+02 -5.670e+02 -5.670e+02 -5.690e+02\n",
      " -5.690e+02 -5.710e+02 -5.710e+02 -5.730e+02  5.760e+02 -5.760e+02\n",
      " -5.820e+02 -5.840e+02 -5.850e+02 -5.880e+02  5.920e+02 -5.960e+02\n",
      " -5.990e+02 -6.040e+02  6.070e+02 -6.080e+02 -6.090e+02 -6.150e+02\n",
      " -6.220e+02 -6.230e+02 -6.270e+02 -6.290e+02 -6.290e+02 -6.300e+02\n",
      " -6.330e+02 -6.380e+02  6.390e+02  6.480e+02  6.480e+02 -6.500e+02\n",
      " -6.510e+02 -6.550e+02 -6.560e+02 -6.710e+02 -6.770e+02 -6.780e+02\n",
      "  6.820e+02 -6.850e+02 -7.050e+02  7.090e+02 -7.150e+02 -7.180e+02\n",
      " -7.210e+02  7.220e+02 -7.240e+02  7.270e+02 -7.350e+02 -7.360e+02\n",
      " -7.430e+02 -7.480e+02 -7.540e+02  7.580e+02 -7.580e+02 -7.720e+02\n",
      "  7.750e+02 -7.770e+02 -7.870e+02 -7.920e+02 -8.000e+02 -8.050e+02\n",
      "  8.140e+02 -8.320e+02 -8.350e+02 -8.460e+02 -8.460e+02 -8.540e+02\n",
      " -8.540e+02 -8.600e+02 -8.620e+02 -8.630e+02 -8.780e+02  8.860e+02\n",
      " -8.890e+02 -9.000e+02 -9.080e+02 -9.080e+02 -9.080e+02 -9.140e+02\n",
      " -9.140e+02 -9.160e+02  9.190e+02 -9.190e+02 -9.260e+02  9.330e+02\n",
      " -9.330e+02 -9.390e+02 -9.460e+02 -9.490e+02  9.540e+02 -9.550e+02\n",
      " -9.560e+02  9.610e+02 -9.620e+02  9.620e+02 -9.640e+02 -9.680e+02\n",
      "  9.870e+02 -9.920e+02 -9.920e+02 -9.930e+02  1.011e+03 -1.012e+03\n",
      " -1.021e+03 -1.032e+03  1.033e+03 -1.040e+03 -1.069e+03 -1.076e+03\n",
      " -1.078e+03 -1.079e+03 -1.099e+03  1.106e+03 -1.112e+03 -1.115e+03\n",
      " -1.116e+03 -1.120e+03  1.120e+03 -1.130e+03  1.137e+03 -1.137e+03\n",
      " -1.139e+03 -1.147e+03  1.152e+03 -1.164e+03 -1.173e+03  1.183e+03\n",
      " -1.191e+03 -1.201e+03 -1.201e+03  1.209e+03 -1.210e+03 -1.213e+03\n",
      " -1.217e+03 -1.219e+03  1.220e+03 -1.222e+03 -1.227e+03  1.242e+03\n",
      " -1.245e+03 -1.250e+03  1.251e+03  1.262e+03 -1.277e+03 -1.279e+03\n",
      " -1.294e+03 -1.294e+03 -1.301e+03 -1.337e+03  1.339e+03 -1.341e+03\n",
      " -1.354e+03 -1.359e+03 -1.382e+03 -1.387e+03 -1.393e+03 -1.399e+03\n",
      "  1.401e+03 -1.401e+03 -1.412e+03 -1.421e+03 -1.426e+03 -1.426e+03\n",
      " -1.428e+03 -1.453e+03 -1.458e+03 -1.469e+03 -1.470e+03  1.481e+03\n",
      "  1.491e+03 -1.494e+03 -1.500e+03 -1.519e+03  1.525e+03  1.547e+03\n",
      " -1.553e+03 -1.567e+03 -1.568e+03  1.578e+03  1.585e+03 -1.650e+03\n",
      " -1.706e+03  1.762e+03 -1.796e+03 -1.806e+03 -1.828e+03 -1.834e+03\n",
      " -1.868e+03  1.886e+03  1.891e+03  1.915e+03  1.933e+03 -1.989e+03\n",
      "  2.000e+03  2.052e+03 -2.078e+03 -2.107e+03 -2.218e+03  2.235e+03\n",
      "  2.286e+03 -2.287e+03 -2.289e+03 -2.289e+03  2.433e+03 -2.493e+03\n",
      " -2.565e+03 -2.585e+03 -2.602e+03  2.660e+03 -2.702e+03 -2.761e+03\n",
      " -2.772e+03  2.835e+03 -2.860e+03 -2.869e+03  2.907e+03 -2.918e+03\n",
      " -3.000e+03  3.200e+03 -3.253e+03  3.470e+03  3.571e+03 -3.574e+03\n",
      " -3.733e+03 -3.761e+03  3.978e+03  4.068e+03  4.084e+03 -4.113e+03\n",
      "  4.229e+03  4.445e+03  4.695e+03 -4.752e+03  5.166e+03 -5.255e+03\n",
      " -6.423e+03]\n",
      "durations 1.0 5546.0\n",
      "Concordance Index 0.7370689655172413\n",
      "Integrated Brier Score: 0.19089521928669276\n",
      "split gex_?|100130426      float32\n",
      "gex_?|100133144      float32\n",
      "gex_?|100134869      float32\n",
      "gex_?|10357          float32\n",
      "gex_?|10431          float32\n",
      "                      ...   \n",
      "gex_ZYG11A|440590    float32\n",
      "gex_ZYG11B|79699     float32\n",
      "gex_ZYX|7791         float32\n",
      "gex_ZZEF1|23140      float32\n",
      "gex_ZZZ3|26009       float32\n",
      "Length: 20531, dtype: object\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m108.6649\u001b[0m  0.0031\n",
      "      2      108.6649  0.0020\n",
      "      3      108.6649  0.0015\n",
      "      4      108.6649  0.0018\n",
      "      5      108.6649  0.0016\n",
      "      6      108.6649  0.0019\n",
      "      7      108.6649  0.0017\n",
      "      8      108.6649  0.0046\n",
      "      9      108.6649  0.0024\n",
      "     10      108.6649  0.0028\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m227.0142\u001b[0m  0.0029\n",
      "      2      227.0142  0.0017\n",
      "      3      227.0142  0.0014\n",
      "      4      227.0142  0.0013\n",
      "      5      227.0142  0.0013\n",
      "      6      227.0142  0.0013\n",
      "      7      227.0142  0.0012\n",
      "      8      227.0142  0.0034\n",
      "      9      227.0142  0.0013\n",
      "     10      227.0142  0.0014\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m43.2787\u001b[0m  0.0031\n",
      "      2       43.2787  0.0023\n",
      "      3       43.2787  0.0021\n",
      "      4       43.2787  0.0021\n",
      "      5       43.2787  0.0020\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      6       43.2787  0.0022\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m134.9939\u001b[0m  0.0024\n",
      "      7       43.2787  0.0035\n",
      "      2      134.9939  0.0014\n",
      "      3      134.9939  0.0015\n",
      "      8       43.2787  0.0022\n",
      "      4      134.9939  0.0013\n",
      "      9       43.2787  0.0023\n",
      "      5      134.9939  0.0013\n",
      "      6      134.9939  0.0014\n",
      "     10       43.2787  0.0022\n",
      "      7      134.9939  0.0013\n",
      "Restoring best model from epoch 1.\n",
      "      8      134.9939  0.0013\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.      9      134.9939  0.0016\n",
      "\n",
      "     10      134.9939  0.0016\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m38.9687\u001b[0m  0.0078\n",
      "      2       38.9687  0.0057\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m39.1040\u001b[0m  0.0072\n",
      "      3       38.9687  0.0028\n",
      "      2       39.1040  0.0037\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m104.0364\u001b[0m  0.0101\n",
      "      4       38.9687  0.0037\n",
      "      2      104.0364  0.0026\n",
      "      3       39.1040  0.0028\n",
      "      5       38.9687  0.0032\n",
      "      3      104.0364  0.0021\n",
      "      4       39.1040  0.0024\n",
      "      4      104.0364  0.0017\n",
      "      6       38.9687  0.0026\n",
      "      5      104.0364  0.0019\n",
      "      5       39.1040  0.0034\n",
      "      7       38.9687  0.0020\n",
      "      6      104.0364  0.0027\n",
      "      6       39.1040  0.0028\n",
      "      8       38.9687  0.0037\n",
      "      7      104.0364  0.0013\n",
      "      7       39.1040  0.0029\n",
      "      8      104.0364  0.0018\n",
      "      9       38.9687  0.0027\n",
      "      8       39.1040  0.0021\n",
      "      9      104.0364  0.0018\n",
      "     10       38.9687  0.0025\n",
      "      9       39.1040  0.0020\n",
      "Restoring best model from epoch 1.\n",
      "     10      104.0364  0.0023\n",
      "Restoring best model from epoch 1.\n",
      "     10       39.1040  0.0020\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m105.6685\u001b[0m  0.0028\n",
      "      2      105.6685  0.0023\n",
      "      3      105.6685  0.0016\n",
      "      4      105.6685  0.0016\n",
      "      5      105.6685  0.0016\n",
      "      6      105.6685  0.0014\n",
      "      7      105.6685  0.0013\n",
      "      8      105.6685  0.0013\n",
      "      9      105.6685  0.0012\n",
      "     10      105.6685  0.0012\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m43.1511\u001b[0m  0.0095\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m28.1560\u001b[0m  0.0088\n",
      "      2       43.1511  0.0032\n",
      "      2       28.1560  0.0026\n",
      "      3       43.1511  0.0031\n",
      "      3       28.1560  0.0029\n",
      "      4       43.1511  0.0027\n",
      "      4       28.1560  0.0025\n",
      "      5       43.1511  0.0023\n",
      "      5       28.1560  0.0021\n",
      "      6       43.1511  0.0021\n",
      "      6       28.1560  0.0020\n",
      "      7       43.1511  0.0020\n",
      "      7       28.1560  0.0024\n",
      "      8       28.1560  0.0021\n",
      "      8       43.1511  0.0030\n",
      "      9       28.1560  0.0019\n",
      "      9       43.1511  0.0023\n",
      "     10       28.1560  0.0019\n",
      "Restoring best model from epoch 1.\n",
      "     10       43.1511  0.0021\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m72.0151\u001b[0m  0.0035\n",
      "      2       72.0151  0.0034\n",
      "      3       72.0151  0.0031\n",
      "      4       72.0151  0.0027\n",
      "      5       72.0151  0.0028\n",
      "      6       72.0151  0.0030\n",
      "      7       72.0151  0.0128\n",
      "      8       72.0151  0.0053\n",
      "      9       72.0151  0.0030\n",
      "     10       72.0151  0.0037\n",
      "Restoring best model from epoch 1.\n",
      "y_train breslow final [-1.000e+00 -6.000e+00 -6.000e+00 -8.000e+00 -9.000e+00 -9.000e+00\n",
      "  9.000e+00 -1.000e+01  1.100e+01  1.200e+01 -1.200e+01  1.400e+01\n",
      "  1.600e+01 -2.000e+01 -2.000e+01 -2.100e+01 -2.300e+01  2.700e+01\n",
      " -3.000e+01  3.100e+01  3.400e+01  4.600e+01  5.600e+01  5.600e+01\n",
      "  6.500e+01  6.500e+01  6.700e+01 -7.900e+01  8.700e+01  9.100e+01\n",
      " -9.400e+01  9.700e+01  1.010e+02  1.020e+02  1.030e+02  1.070e+02\n",
      "  1.150e+02  1.290e+02  1.290e+02  1.400e+02 -1.410e+02  1.530e+02\n",
      "  1.710e+02  1.710e+02 -1.800e+02 -1.830e+02  1.940e+02 -2.110e+02\n",
      "  2.140e+02  2.170e+02  2.230e+02 -2.290e+02  2.330e+02  2.470e+02\n",
      " -2.480e+02 -2.500e+02  2.620e+02  2.780e+02  2.790e+02  2.830e+02\n",
      " -2.910e+02  2.960e+02  2.990e+02  3.000e+02  3.000e+02 -3.030e+02\n",
      "  3.030e+02  3.040e+02  3.080e+02  3.150e+02 -3.220e+02 -3.270e+02\n",
      " -3.280e+02 -3.300e+02  3.440e+02 -3.450e+02 -3.470e+02  3.470e+02\n",
      "  3.490e+02 -3.540e+02 -3.570e+02 -3.590e+02  3.590e+02 -3.610e+02\n",
      " -3.610e+02 -3.630e+02 -3.630e+02 -3.650e+02  3.650e+02  3.660e+02\n",
      " -3.660e+02 -3.720e+02  3.810e+02 -3.820e+02 -3.870e+02 -3.870e+02\n",
      " -3.900e+02  3.940e+02 -3.950e+02 -3.950e+02 -3.960e+02 -3.990e+02\n",
      " -4.060e+02 -4.060e+02 -4.080e+02 -4.090e+02  4.100e+02  4.120e+02\n",
      " -4.120e+02  4.150e+02 -4.150e+02  4.190e+02 -4.230e+02 -4.250e+02\n",
      "  4.250e+02 -4.270e+02 -4.300e+02  4.380e+02 -4.440e+02 -4.470e+02\n",
      " -4.490e+02  4.520e+02 -4.580e+02 -4.680e+02 -4.720e+02 -4.740e+02\n",
      " -4.760e+02 -4.780e+02 -4.800e+02 -4.860e+02 -5.200e+02 -5.380e+02\n",
      "  5.470e+02 -5.520e+02 -5.540e+02 -5.550e+02 -5.550e+02  5.550e+02\n",
      "  5.560e+02  5.580e+02 -5.620e+02 -5.660e+02 -5.700e+02 -5.740e+02\n",
      " -5.750e+02  5.810e+02 -5.850e+02 -5.870e+02 -5.880e+02  5.960e+02\n",
      "  6.010e+02 -6.020e+02 -6.080e+02  6.120e+02 -6.150e+02 -6.210e+02\n",
      "  6.270e+02 -6.300e+02 -6.310e+02 -6.320e+02 -6.360e+02 -6.380e+02\n",
      " -6.400e+02  6.430e+02  6.490e+02 -6.550e+02 -6.580e+02  6.600e+02\n",
      " -6.710e+02 -6.720e+02 -6.730e+02  6.880e+02  6.930e+02 -6.970e+02\n",
      " -6.980e+02 -7.060e+02  7.110e+02 -7.220e+02  7.240e+02 -7.290e+02\n",
      " -7.440e+02 -7.470e+02 -7.560e+02  7.570e+02 -7.630e+02 -7.800e+02\n",
      " -7.820e+02 -7.840e+02  8.020e+02 -8.100e+02 -8.190e+02  8.270e+02\n",
      "  8.370e+02 -8.480e+02 -8.480e+02 -8.490e+02 -8.600e+02 -8.790e+02\n",
      " -8.980e+02 -8.980e+02  8.990e+02 -9.100e+02 -9.170e+02 -9.280e+02\n",
      "  9.310e+02 -9.420e+02 -9.510e+02 -9.890e+02  1.005e+03 -1.008e+03\n",
      " -1.030e+03 -1.049e+03 -1.066e+03 -1.067e+03 -1.091e+03 -1.098e+03\n",
      " -1.115e+03  1.135e+03 -1.145e+03  1.147e+03 -1.168e+03  1.210e+03\n",
      " -1.219e+03  1.229e+03 -1.241e+03 -1.242e+03 -1.295e+03 -1.339e+03\n",
      " -1.345e+03 -1.351e+03 -1.363e+03  1.386e+03  1.397e+03  1.423e+03\n",
      " -1.424e+03 -1.450e+03 -1.452e+03  1.490e+03 -1.495e+03 -1.516e+03\n",
      " -1.531e+03 -1.531e+03 -1.553e+03 -1.562e+03 -1.567e+03 -1.618e+03\n",
      "  1.622e+03  1.624e+03 -1.633e+03 -1.636e+03  1.685e+03  1.694e+03\n",
      " -1.711e+03 -1.718e+03 -1.731e+03 -1.804e+03 -1.823e+03 -1.855e+03\n",
      " -1.876e+03 -1.900e+03 -1.939e+03 -1.970e+03 -2.015e+03 -2.017e+03\n",
      " -2.102e+03  2.116e+03  2.131e+03 -2.184e+03 -2.202e+03 -2.245e+03\n",
      " -2.301e+03 -2.301e+03 -2.317e+03 -2.324e+03 -2.398e+03 -2.412e+03\n",
      " -2.415e+03 -2.425e+03 -2.442e+03 -2.455e+03  2.456e+03  2.486e+03\n",
      "  2.532e+03  2.542e+03 -2.728e+03 -2.746e+03 -2.752e+03  3.125e+03\n",
      "  3.258e+03 -3.437e+03 -3.478e+03 -3.675e+03]\n",
      "Concordance Index 0.42395982783357244\n",
      "Integrated Brier Score: 0.2138522210894207\n",
      "y_train breslow final [-1.000e+00 -6.000e+00 -6.000e+00 -8.000e+00 -9.000e+00 -9.000e+00\n",
      "  9.000e+00 -1.000e+01  1.100e+01  1.200e+01 -1.200e+01  1.400e+01\n",
      "  1.600e+01 -2.000e+01 -2.000e+01 -2.100e+01 -2.300e+01  2.700e+01\n",
      " -3.000e+01  3.100e+01  3.400e+01  4.600e+01  5.600e+01  5.600e+01\n",
      "  6.500e+01  6.500e+01  6.700e+01 -7.900e+01  8.700e+01  9.100e+01\n",
      " -9.400e+01  9.700e+01  1.010e+02  1.020e+02  1.030e+02  1.070e+02\n",
      "  1.150e+02  1.290e+02  1.290e+02  1.400e+02 -1.410e+02  1.530e+02\n",
      "  1.710e+02  1.710e+02 -1.800e+02 -1.830e+02  1.940e+02 -2.110e+02\n",
      "  2.140e+02  2.170e+02  2.230e+02 -2.290e+02  2.330e+02  2.470e+02\n",
      " -2.480e+02 -2.500e+02  2.620e+02  2.780e+02  2.790e+02  2.830e+02\n",
      " -2.910e+02  2.960e+02  2.990e+02  3.000e+02  3.000e+02 -3.030e+02\n",
      "  3.030e+02  3.040e+02  3.080e+02  3.150e+02 -3.220e+02 -3.270e+02\n",
      " -3.280e+02 -3.300e+02  3.440e+02 -3.450e+02 -3.470e+02  3.470e+02\n",
      "  3.490e+02 -3.540e+02 -3.570e+02 -3.590e+02  3.590e+02 -3.610e+02\n",
      " -3.610e+02 -3.630e+02 -3.630e+02 -3.650e+02  3.650e+02  3.660e+02\n",
      " -3.660e+02 -3.720e+02  3.810e+02 -3.820e+02 -3.870e+02 -3.870e+02\n",
      " -3.900e+02  3.940e+02 -3.950e+02 -3.950e+02 -3.960e+02 -3.990e+02\n",
      " -4.060e+02 -4.060e+02 -4.080e+02 -4.090e+02  4.100e+02  4.120e+02\n",
      " -4.120e+02  4.150e+02 -4.150e+02  4.190e+02 -4.230e+02 -4.250e+02\n",
      "  4.250e+02 -4.270e+02 -4.300e+02  4.380e+02 -4.440e+02 -4.470e+02\n",
      " -4.490e+02  4.520e+02 -4.580e+02 -4.680e+02 -4.720e+02 -4.740e+02\n",
      " -4.760e+02 -4.780e+02 -4.800e+02 -4.860e+02 -5.200e+02 -5.380e+02\n",
      "  5.470e+02 -5.520e+02 -5.540e+02 -5.550e+02 -5.550e+02  5.550e+02\n",
      "  5.560e+02  5.580e+02 -5.620e+02 -5.660e+02 -5.700e+02 -5.740e+02\n",
      " -5.750e+02  5.810e+02 -5.850e+02 -5.870e+02 -5.880e+02  5.960e+02\n",
      "  6.010e+02 -6.020e+02 -6.080e+02  6.120e+02 -6.150e+02 -6.210e+02\n",
      "  6.270e+02 -6.300e+02 -6.310e+02 -6.320e+02 -6.360e+02 -6.380e+02\n",
      " -6.400e+02  6.430e+02  6.490e+02 -6.550e+02 -6.580e+02  6.600e+02\n",
      " -6.710e+02 -6.720e+02 -6.730e+02  6.880e+02  6.930e+02 -6.970e+02\n",
      " -6.980e+02 -7.060e+02  7.110e+02 -7.220e+02  7.240e+02 -7.290e+02\n",
      " -7.440e+02 -7.470e+02 -7.560e+02  7.570e+02 -7.630e+02 -7.800e+02\n",
      " -7.820e+02 -7.840e+02  8.020e+02 -8.100e+02 -8.190e+02  8.270e+02\n",
      "  8.370e+02 -8.480e+02 -8.480e+02 -8.490e+02 -8.600e+02 -8.790e+02\n",
      " -8.980e+02 -8.980e+02  8.990e+02 -9.100e+02 -9.170e+02 -9.280e+02\n",
      "  9.310e+02 -9.420e+02 -9.510e+02 -9.890e+02  1.005e+03 -1.008e+03\n",
      " -1.030e+03 -1.049e+03 -1.066e+03 -1.067e+03 -1.091e+03 -1.098e+03\n",
      " -1.115e+03  1.135e+03 -1.145e+03  1.147e+03 -1.168e+03  1.210e+03\n",
      " -1.219e+03  1.229e+03 -1.241e+03 -1.242e+03 -1.295e+03 -1.339e+03\n",
      " -1.345e+03 -1.351e+03 -1.363e+03  1.386e+03  1.397e+03  1.423e+03\n",
      " -1.424e+03 -1.450e+03 -1.452e+03  1.490e+03 -1.495e+03 -1.516e+03\n",
      " -1.531e+03 -1.531e+03 -1.553e+03 -1.562e+03 -1.567e+03 -1.618e+03\n",
      "  1.622e+03  1.624e+03 -1.633e+03 -1.636e+03  1.685e+03  1.694e+03\n",
      " -1.711e+03 -1.718e+03 -1.731e+03 -1.804e+03 -1.823e+03 -1.855e+03\n",
      " -1.876e+03 -1.900e+03 -1.939e+03 -1.970e+03 -2.015e+03 -2.017e+03\n",
      " -2.102e+03  2.116e+03  2.131e+03 -2.184e+03 -2.202e+03 -2.245e+03\n",
      " -2.301e+03 -2.301e+03 -2.317e+03 -2.324e+03 -2.398e+03 -2.412e+03\n",
      " -2.415e+03 -2.425e+03 -2.442e+03 -2.455e+03  2.456e+03  2.486e+03\n",
      "  2.532e+03  2.542e+03 -2.728e+03 -2.746e+03 -2.752e+03  3.125e+03\n",
      "  3.258e+03 -3.437e+03 -3.478e+03 -3.675e+03]\n",
      "durations 6.0 3308.0\n",
      "Concordance Index 0.40293809024134314\n",
      "Integrated Brier Score: 0.23239838944936206\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m94.2737\u001b[0m       \u001b[32m61.1214\u001b[0m  0.0282\n",
      "      2      103.8769       72.2923  0.0213\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m76.1220\u001b[0m  0.0019\n",
      "      3       95.9607       82.2049  0.0190\n",
      "      2       76.1220  0.0015\n",
      "      3       76.1220  0.0015\n",
      "      4       76.1220  0.0015\n",
      "      5       76.1220  0.0015\n",
      "      6       76.1220  0.0015\n",
      "      7       76.1220  0.0016\n",
      "      8       76.1220  0.0015\n",
      "      9       76.1220  0.0015\n",
      "     10       76.1220  0.0015\n",
      "Restoring best model from epoch 1.\n",
      "      4       \u001b[36m91.2805\u001b[0m       70.9381  0.0194\n",
      "      5       \u001b[36m87.0519\u001b[0m       63.9872  0.0195\n",
      "      6       \u001b[36m84.5073\u001b[0m       67.6952  0.0241\n",
      "      7       88.8223       69.9458  0.0218\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      8       \u001b[36m82.1128\u001b[0m       76.3580  0.0238\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m50.6735\u001b[0m  0.0030\n",
      "      2       50.6735  0.0018\n",
      "      9       \u001b[36m79.6457\u001b[0m       79.6250  0.0229\n",
      "      3       50.6735  0.0016\n",
      "      4       50.6735  0.0016\n",
      "      5       50.6735  0.0015\n",
      "      6       50.6735  0.0017\n",
      "      7       50.6735  0.0016\n",
      "      8       50.6735  0.0016\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m96.4630\u001b[0m       \u001b[32m64.7343\u001b[0m  0.0355\n",
      "      9       50.6735  0.0015\n",
      "     10       50.6735  0.0082\n",
      "Restoring best model from epoch 1.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m85.3198\u001b[0m       \u001b[32m37.6900\u001b[0m  0.0234\n",
      "     10       80.7385       81.2338  0.0218\n",
      "Restoring best model from epoch 1.\n",
      "      2      105.1557       67.1533  0.0286\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2       92.8787       56.4595  0.0709\n",
      "      3       \u001b[36m93.6251\u001b[0m       66.7123  0.0539\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m79.8812\u001b[0m  0.0068\n",
      "      2       79.8812  0.0044\n",
      "      3       79.8812  0.0030\n",
      "      4       79.8812  0.0026\n",
      "      5       79.8812  0.0018\n",
      "      6       79.8812  0.0029\n",
      "      7       79.8812  0.0038\n",
      "      8       79.8812  0.0032\n",
      "      9       79.8812  0.0026\n",
      "     10       79.8812  0.0034\n",
      "Restoring best model from epoch 1.\n",
      "      4       \u001b[36m86.7165\u001b[0m       69.2996  0.0481\n",
      "      3       \u001b[36m82.7526\u001b[0m       56.3737  0.0513\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m93.8668\u001b[0m       \u001b[32m65.8543\u001b[0m  0.0689\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m103.8310\u001b[0m  0.0032\n",
      "      2      103.8310  0.0024\n",
      "      3      103.8310  0.0018\n",
      "      4      103.8310  0.0032\n",
      "      5       \u001b[36m82.0666\u001b[0m       65.0222  0.0293\n",
      "      4       84.2267       44.8853  0.0312\n",
      "      5      103.8310  0.0071\n",
      "      6      103.8310  0.0022\n",
      "      7      103.8310  0.0040\n",
      "      8      103.8310  0.0031\n",
      "      9      103.8310  0.0019\n",
      "     10      103.8310  0.0036\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      2       \u001b[36m93.7988\u001b[0m       \u001b[32m64.7265\u001b[0m  0.0357\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      5       \u001b[36m73.3532\u001b[0m       52.3148  0.0244\n",
      "      6       \u001b[36m77.3634\u001b[0m       69.0879  0.0292\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m91.4503\u001b[0m  0.0044\n",
      "      2       91.4503  0.0037\n",
      "      3       91.4503  0.0040\n",
      "      4       91.4503  0.0057\n",
      "      5       91.4503  0.0026\n",
      "      3       97.9326       83.8345  0.0311\n",
      "      6       \u001b[36m72.7204\u001b[0m       58.4083  0.0290\n",
      "      6       91.4503  0.0037\n",
      "      7       91.4503  0.0020\n",
      "      7       78.6516       73.4796  0.0319\n",
      "      8       91.4503  0.0027\n",
      "      9       91.4503  0.0029\n",
      "     10       91.4503  0.0039\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      7       \u001b[36m70.2738\u001b[0m       62.0922  0.0225\n",
      "      8       78.0704       78.5951  0.0229\n",
      "      4       \u001b[36m86.3446\u001b[0m       66.3579  0.0276\n",
      "      8       \u001b[36m68.7432\u001b[0m       56.7071  0.0231\n",
      "      9       80.9421       82.0414  0.0239\n",
      "      5       \u001b[36m84.7396\u001b[0m       69.8963  0.0245\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m66.6878\u001b[0m       \u001b[32m34.9824\u001b[0m  0.0298\n",
      "      9       69.5897       50.6400  0.0210\n",
      "      6       \u001b[36m82.0910\u001b[0m       67.6900  0.0194\n",
      "     10       79.3756       79.1616  0.0205\n",
      "Restoring best model from epoch 1.\n",
      "      2       77.2961       \u001b[32m34.2271\u001b[0m  0.0208\n",
      "     10       69.2707       44.1218  0.0192\n",
      "Restoring best model from epoch 1.\n",
      "      7       \u001b[36m78.7118\u001b[0m       73.8065  0.0192\n",
      "      3       76.4438       \u001b[32m31.9912\u001b[0m  0.0192\n",
      "      8       78.8460       74.8468  0.0196\n",
      "      4       \u001b[36m66.5023\u001b[0m       34.2109  0.0193\n",
      "      9       83.9028       70.7008  0.0185\n",
      "      5       \u001b[36m62.1910\u001b[0m       33.7436  0.0192\n",
      "     10       \u001b[36m76.0155\u001b[0m       70.2429  0.0192\n",
      "Restoring best model from epoch 2.\n",
      "      6       \u001b[36m61.4838\u001b[0m       \u001b[32m30.7991\u001b[0m  0.0199\n",
      "      7       \u001b[36m60.0991\u001b[0m       \u001b[32m27.7879\u001b[0m  0.0208\n",
      "      8       \u001b[36m56.5216\u001b[0m       30.0803  0.0188\n",
      "      9       \u001b[36m56.1621\u001b[0m       30.9762  0.0179\n",
      "     10       59.7571       29.5569  0.0171\n",
      "Restoring best model from epoch 7.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m90.6719\u001b[0m       \u001b[32m86.0494\u001b[0m  0.0296\n",
      "      2       \u001b[36m89.3168\u001b[0m       \u001b[32m74.5653\u001b[0m  0.0263\n",
      "      3       \u001b[36m76.9509\u001b[0m       75.8184  0.0265\n",
      "      4       \u001b[36m75.4425\u001b[0m       76.0317  0.0339\n",
      "      5       \u001b[36m74.9530\u001b[0m       75.9685  0.0328\n",
      "      6       \u001b[36m71.5097\u001b[0m       77.4808  0.0289\n",
      "      7       \u001b[36m70.5835\u001b[0m       74.9090  0.0415\n",
      "      8       \u001b[36m67.7622\u001b[0m       \u001b[32m74.5455\u001b[0m  0.0316\n",
      "      9       68.7601       76.6213  0.0306\n",
      "     10       71.3713       75.4778  0.0448\n",
      "Restoring best model from epoch 8.\n",
      "y_train breslow final [   -6.    -6.    -6.    -8.    -9.    -9.     9.    11.    12.   -12.\n",
      "    14.   -15.    16.    19.   -20.   -21.   -22.   -23.   -30.    31.\n",
      "    34.    36.   -44.    46.    52.    56.    56.    65.    65.    67.\n",
      "   -79.    87.    91.    91.   -94.    97.   101.   103.   107.   115.\n",
      "   129.   129.  -137.  -141.   153.  -170.   171.   171.  -180.  -183.\n",
      "   194.   195.  -211.   217.  -219.   233.   247.  -248.  -260.   262.\n",
      "   262.   272.   278.   283.  -291.   296.   299.   300.   304.   308.\n",
      "  -314.  -322.  -327.  -328.  -330.  -341.   344.  -345.  -347.   347.\n",
      "  -354.  -357.  -359.  -361.  -363.  -365.   365.   366.  -366.  -372.\n",
      "   373.  -382.  -387.  -387.  -390.   394.  -395.  -395.  -396.  -400.\n",
      "  -406.  -406.  -408.  -409.   410.   412.  -412.   415.   416.   419.\n",
      "  -423.  -425.   425.   432.  -436.   438.  -444.  -447.   452.  -453.\n",
      "  -468.   469.  -472.  -474.  -478.  -480.  -486.  -500.  -507.  -512.\n",
      "  -519.  -520.  -520.   535.  -538.   547.  -552.  -555.  -555.   555.\n",
      "   556.  -564.  -566.  -570.  -575.  -579.   581.  -585.  -587.  -588.\n",
      "  -594.  -601.   601.  -608.  -608.   612.  -615.  -621.   627.  -630.\n",
      "   633.  -636.  -638.   639.  -640.   643.  -644.  -655.  -662.  -671.\n",
      "  -672.  -673.  -680.   688.  -693.   693.  -697.  -698.  -698.  -701.\n",
      "  -706.   711.  -719.  -722.   724.  -728.  -729.  -747.  -756.   757.\n",
      "  -763.   765.   768.   770.  -782.  -799.   802.  -810.  -816.  -819.\n",
      "   827.  -829.   837.   837.  -848.  -848.   848.  -854.  -860.  -898.\n",
      "  -898.  -906.  -910.  -917.  -925.  -928.  -951.  -989.  1005. -1008.\n",
      " -1030. -1066. -1085. -1085.  1088. -1091. -1145.  1147.  1149. -1168.\n",
      "  1210.  1229. -1231. -1233. -1242.  1271. -1295. -1302. -1339. -1351.\n",
      " -1363.  1372.  1386.  1423. -1424. -1450. -1516. -1531. -1553.  1560.\n",
      " -1562. -1567. -1618.  1624. -1633. -1636.  1685.  1694. -1779.  1791.\n",
      " -1823.  1852. -1855. -1876. -1900. -1939. -1970. -1989. -2015. -2017.\n",
      " -2028.  2116.  2131. -2184. -2202. -2245. -2301. -2301. -2317. -2398.\n",
      " -2415. -2425. -2442. -2455. -2513.  2542. -2728. -2752.  3258. -3308.\n",
      " -3478. -3675.]\n",
      "Concordance Index 0.6807735426008968\n",
      "Integrated Brier Score: 0.19392011029563777\n",
      "y_train breslow final [   -6.    -6.    -6.    -8.    -9.    -9.     9.    11.    12.   -12.\n",
      "    14.   -15.    16.    19.   -20.   -21.   -22.   -23.   -30.    31.\n",
      "    34.    36.   -44.    46.    52.    56.    56.    65.    65.    67.\n",
      "   -79.    87.    91.    91.   -94.    97.   101.   103.   107.   115.\n",
      "   129.   129.  -137.  -141.   153.  -170.   171.   171.  -180.  -183.\n",
      "   194.   195.  -211.   217.  -219.   233.   247.  -248.  -260.   262.\n",
      "   262.   272.   278.   283.  -291.   296.   299.   300.   304.   308.\n",
      "  -314.  -322.  -327.  -328.  -330.  -341.   344.  -345.  -347.   347.\n",
      "  -354.  -357.  -359.  -361.  -363.  -365.   365.   366.  -366.  -372.\n",
      "   373.  -382.  -387.  -387.  -390.   394.  -395.  -395.  -396.  -400.\n",
      "  -406.  -406.  -408.  -409.   410.   412.  -412.   415.   416.   419.\n",
      "  -423.  -425.   425.   432.  -436.   438.  -444.  -447.   452.  -453.\n",
      "  -468.   469.  -472.  -474.  -478.  -480.  -486.  -500.  -507.  -512.\n",
      "  -519.  -520.  -520.   535.  -538.   547.  -552.  -555.  -555.   555.\n",
      "   556.  -564.  -566.  -570.  -575.  -579.   581.  -585.  -587.  -588.\n",
      "  -594.  -601.   601.  -608.  -608.   612.  -615.  -621.   627.  -630.\n",
      "   633.  -636.  -638.   639.  -640.   643.  -644.  -655.  -662.  -671.\n",
      "  -672.  -673.  -680.   688.  -693.   693.  -697.  -698.  -698.  -701.\n",
      "  -706.   711.  -719.  -722.   724.  -728.  -729.  -747.  -756.   757.\n",
      "  -763.   765.   768.   770.  -782.  -799.   802.  -810.  -816.  -819.\n",
      "   827.  -829.   837.   837.  -848.  -848.   848.  -854.  -860.  -898.\n",
      "  -898.  -906.  -910.  -917.  -925.  -928.  -951.  -989.  1005. -1008.\n",
      " -1030. -1066. -1085. -1085.  1088. -1091. -1145.  1147.  1149. -1168.\n",
      "  1210.  1229. -1231. -1233. -1242.  1271. -1295. -1302. -1339. -1351.\n",
      " -1363.  1372.  1386.  1423. -1424. -1450. -1516. -1531. -1553.  1560.\n",
      " -1562. -1567. -1618.  1624. -1633. -1636.  1685.  1694. -1779.  1791.\n",
      " -1823.  1852. -1855. -1876. -1900. -1939. -1970. -1989. -2015. -2017.\n",
      " -2028.  2116.  2131. -2184. -2202. -2245. -2301. -2301. -2317. -2398.\n",
      " -2415. -2425. -2442. -2455. -2513.  2542. -2728. -2752.  3258. -3308.\n",
      " -3478. -3675.]\n",
      "durations 1.0 3437.0\n",
      "Concordance Index 0.5038535645472062\n",
      "Integrated Brier Score: 0.26039842556165516\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m91.5263\u001b[0m       \u001b[32m45.9286\u001b[0m  0.0323\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m87.7755\u001b[0m       \u001b[32m56.7340\u001b[0m  0.0235\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2       98.6869       \u001b[32m45.8067\u001b[0m  0.0238\n",
      "      2       96.7904       \u001b[32m52.6623\u001b[0m  0.0230\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m94.9881\u001b[0m       \u001b[32m61.4736\u001b[0m  0.0361\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m87.3506\u001b[0m       \u001b[32m51.8882\u001b[0m  0.0262\n",
      "      3       \u001b[36m87.4198\u001b[0m       \u001b[32m44.3054\u001b[0m  0.0253\n",
      "      2      120.8801       \u001b[32m58.6708\u001b[0m  0.0325\n",
      "      3       \u001b[36m79.4037\u001b[0m       53.0329  0.0406\n",
      "      2      113.2243       52.1927  0.0196\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m74.5557\u001b[0m       \u001b[32m41.9619\u001b[0m  0.0320\n",
      "      3       \u001b[36m90.7183\u001b[0m       \u001b[32m58.1428\u001b[0m  0.0207\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      4       \u001b[36m77.4159\u001b[0m       44.6590  0.0373\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m48.8760\u001b[0m  0.0021\n",
      "      4       \u001b[36m70.7908\u001b[0m       56.4234  0.0248\n",
      "      2       48.8760  0.0016\n",
      "      3       48.8760  0.0028\n",
      "      3       \u001b[36m76.8629\u001b[0m       \u001b[32m51.2101\u001b[0m  0.0264\n",
      "      4       48.8760  0.0042\n",
      "      5       48.8760  0.0035\n",
      "      6       48.8760  0.0017\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      7       48.8760  0.0015\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2       78.3964       \u001b[32m41.3462\u001b[0m  0.0228\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      8       48.8760  0.0015\n",
      "      4       \u001b[36m84.1875\u001b[0m       \u001b[32m55.6250\u001b[0m  0.0267\n",
      "      9       48.8760  0.0015\n",
      "      5       \u001b[36m69.9741\u001b[0m       62.7523  0.0200\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m73.8013\u001b[0m  0.0021\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m54.8693\u001b[0m  0.0027\n",
      "     10       48.8760  0.0015\n",
      "Restoring best model from epoch 1.\n",
      "      5       \u001b[36m73.6767\u001b[0m       46.3402  0.0240\n",
      "      2       73.8013  0.0017\n",
      "      2       54.8693  0.0024\n",
      "      3       73.8013  0.0016\n",
      "      3       54.8693  0.0017\n",
      "      4       73.8013  0.0016\n",
      "      4       54.8693  0.0016\n",
      "      5       73.8013  0.0016\n",
      "      5       54.8693  0.0015\n",
      "      6       73.8013  0.0016\n",
      "      6       54.8693  0.0015\n",
      "      7       73.8013  0.0015\n",
      "      4       79.8356       \u001b[32m50.8957\u001b[0m  0.0208\n",
      "      7       54.8693  0.0015\n",
      "      8       73.8013  0.0015\n",
      "      8       54.8693  0.0015\n",
      "      9       73.8013  0.0015\n",
      "      9       54.8693  0.0015\n",
      "     10       73.8013  0.0015\n",
      "Restoring best model from epoch 1.\n",
      "     10       54.8693  0.0017\n",
      "Restoring best model from epoch 1.\n",
      "      5       \u001b[36m81.6594\u001b[0m       61.3545  0.0188\n",
      "      3       \u001b[36m74.5115\u001b[0m       \u001b[32m40.7213\u001b[0m  0.0212\n",
      "      6       \u001b[36m68.4907\u001b[0m       68.7211  0.0290\n",
      "      5       \u001b[36m73.9506\u001b[0m       51.9323  0.0202\n",
      "      6       76.1582       \u001b[32m43.8809\u001b[0m  0.0299\n",
      "      4       \u001b[36m67.7899\u001b[0m       47.6441  0.0276\n",
      "      7       72.8057       70.6841  0.0188\n",
      "      6       81.8970       62.0672  0.0312\n",
      "      6       75.7695       54.1422  0.0189\n",
      "      7       75.6137       46.4858  0.0225\n",
      "      5       \u001b[36m64.0759\u001b[0m       43.3642  0.0187\n",
      "      8       72.0602       68.9589  0.0192\n",
      "      7       \u001b[36m81.4591\u001b[0m       60.7365  0.0188\n",
      "      7       \u001b[36m73.6012\u001b[0m       61.1029  0.0262\n",
      "      8       \u001b[36m72.5037\u001b[0m       50.7155  0.0265\n",
      "      8       \u001b[36m81.4449\u001b[0m       61.1345  0.0185\n",
      "      9       68.5133       65.7975  0.0218\n",
      "      6       64.4300       \u001b[32m40.5251\u001b[0m  0.0299\n",
      "      8       73.6895       65.3010  0.0196\n",
      "      9       74.0057       52.4253  0.0198\n",
      "      9       \u001b[36m79.2055\u001b[0m       62.4115  0.0185\n",
      "     10       70.6895       61.6603  0.0227\n",
      "Restoring best model from epoch 2.\n",
      "      7       66.2531       43.3039  0.0219\n",
      "     10       \u001b[36m71.2107\u001b[0m       52.3971  0.0189\n",
      "Restoring best model from epoch 6.\n",
      "      9       \u001b[36m71.6240\u001b[0m       63.5874  0.0273\n",
      "     10       79.7816       61.7578  0.0184\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Restoring best model from epoch 4.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m58.7365\u001b[0m  0.0027\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m66.0023\u001b[0m  0.0022\n",
      "      2       58.7365  0.0017\n",
      "      2       66.0023  0.0018\n",
      "      3       58.7365  0.0016\n",
      "      3       66.0023  0.0019\n",
      "      4       58.7365  0.0015\n",
      "      4       66.0023  0.0017\n",
      "      5       58.7365  0.0015\n",
      "      8       \u001b[36m60.8606\u001b[0m       47.2550  0.0191\n",
      "      5       66.0023  0.0016\n",
      "      6       58.7365  0.0015\n",
      "      6       66.0023  0.0016\n",
      "      7       58.7365  0.0017\n",
      "      7       66.0023  0.0015\n",
      "      8       58.7365  0.0015\n",
      "     10       \u001b[36m68.9859\u001b[0m       61.3709  0.0188\n",
      "Restoring best model from epoch 4.\n",
      "      8       66.0023  0.0016\n",
      "      9       58.7365  0.0015\n",
      "      9       66.0023  0.0016\n",
      "     10       58.7365  0.0016\n",
      "Restoring best model from epoch 1.\n",
      "     10       66.0023  0.0016\n",
      "Restoring best model from epoch 1.\n",
      "      9       \u001b[36m58.5132\u001b[0m       52.7539  0.0187\n",
      "     10       \u001b[36m58.1347\u001b[0m       55.6038  0.0185\n",
      "Restoring best model from epoch 6.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m87.2111\u001b[0m      \u001b[32m115.6963\u001b[0m  0.0255\n",
      "      2       \u001b[36m75.9286\u001b[0m       \u001b[32m82.6274\u001b[0m  0.0351\n",
      "      3       \u001b[36m68.0291\u001b[0m       86.0409  0.0262\n",
      "      4       \u001b[36m62.5814\u001b[0m       93.9815  0.0264\n",
      "      5       62.8815       97.0042  0.0301\n",
      "      6       \u001b[36m62.2715\u001b[0m       97.3583  0.0275\n",
      "      7       \u001b[36m61.7191\u001b[0m       96.4469  0.0896\n",
      "      8       \u001b[36m59.8906\u001b[0m      100.2536  0.0413\n",
      "      9       60.5728       99.2482  0.0257\n",
      "     10       \u001b[36m57.1725\u001b[0m      101.0965  0.0244\n",
      "Restoring best model from epoch 2.\n",
      "y_train breslow final [-1.000e+00 -6.000e+00 -6.000e+00 -6.000e+00 -9.000e+00  9.000e+00\n",
      " -1.000e+01  1.100e+01  1.200e+01 -1.200e+01 -1.500e+01  1.600e+01\n",
      "  1.900e+01 -2.000e+01 -2.000e+01 -2.100e+01 -2.200e+01 -2.300e+01\n",
      "  2.700e+01 -3.000e+01  3.100e+01  3.600e+01 -4.400e+01  4.600e+01\n",
      "  5.200e+01  5.600e+01  6.500e+01  6.500e+01  6.700e+01  8.700e+01\n",
      "  9.100e+01 -9.400e+01  9.700e+01  1.010e+02  1.020e+02  1.030e+02\n",
      "  1.070e+02  1.290e+02  1.290e+02 -1.370e+02  1.400e+02 -1.410e+02\n",
      " -1.700e+02  1.710e+02  1.710e+02 -1.830e+02  1.940e+02  1.950e+02\n",
      " -2.110e+02  2.140e+02  2.170e+02 -2.190e+02  2.230e+02 -2.290e+02\n",
      "  2.330e+02  2.470e+02 -2.480e+02 -2.500e+02 -2.600e+02  2.620e+02\n",
      "  2.620e+02  2.720e+02  2.790e+02 -2.910e+02  2.990e+02  3.000e+02\n",
      "  3.000e+02 -3.030e+02  3.030e+02  3.040e+02 -3.140e+02  3.150e+02\n",
      " -3.280e+02 -3.300e+02 -3.410e+02 -3.450e+02 -3.470e+02  3.470e+02\n",
      "  3.490e+02 -3.540e+02 -3.570e+02  3.590e+02 -3.610e+02 -3.610e+02\n",
      " -3.630e+02 -3.630e+02  3.650e+02 -3.660e+02 -3.720e+02  3.730e+02\n",
      "  3.810e+02 -3.820e+02 -3.870e+02 -3.870e+02 -3.900e+02 -3.960e+02\n",
      " -3.990e+02 -4.000e+02 -4.060e+02 -4.080e+02  4.100e+02 -4.120e+02\n",
      " -4.150e+02  4.160e+02 -4.230e+02  4.250e+02 -4.270e+02 -4.300e+02\n",
      "  4.320e+02 -4.360e+02  4.380e+02 -4.490e+02  4.520e+02 -4.530e+02\n",
      " -4.580e+02  4.690e+02 -4.720e+02 -4.740e+02 -4.760e+02 -4.800e+02\n",
      " -5.000e+02 -5.070e+02 -5.120e+02 -5.190e+02 -5.200e+02  5.350e+02\n",
      " -5.520e+02 -5.540e+02 -5.550e+02 -5.550e+02  5.560e+02  5.580e+02\n",
      " -5.620e+02 -5.640e+02 -5.700e+02 -5.740e+02 -5.790e+02 -5.850e+02\n",
      " -5.870e+02 -5.880e+02 -5.940e+02  5.960e+02 -6.010e+02  6.010e+02\n",
      " -6.020e+02 -6.080e+02  6.270e+02 -6.300e+02 -6.310e+02 -6.320e+02\n",
      "  6.330e+02 -6.360e+02 -6.380e+02  6.390e+02  6.430e+02 -6.440e+02\n",
      "  6.490e+02 -6.580e+02  6.600e+02 -6.620e+02 -6.720e+02 -6.800e+02\n",
      " -6.930e+02 -6.970e+02 -6.980e+02 -6.980e+02 -7.010e+02 -7.060e+02\n",
      "  7.110e+02 -7.190e+02  7.240e+02 -7.280e+02 -7.290e+02 -7.440e+02\n",
      " -7.560e+02 -7.630e+02  7.650e+02  7.680e+02  7.700e+02 -7.800e+02\n",
      " -7.820e+02 -7.840e+02 -7.990e+02  8.020e+02 -8.100e+02 -8.160e+02\n",
      "  8.270e+02 -8.290e+02  8.370e+02  8.370e+02 -8.480e+02 -8.480e+02\n",
      "  8.480e+02 -8.490e+02 -8.540e+02 -8.600e+02 -8.790e+02 -8.980e+02\n",
      "  8.990e+02 -9.060e+02 -9.100e+02 -9.170e+02 -9.250e+02  9.310e+02\n",
      " -9.420e+02 -9.890e+02 -1.008e+03 -1.030e+03 -1.049e+03 -1.066e+03\n",
      " -1.067e+03 -1.085e+03 -1.085e+03  1.088e+03 -1.098e+03 -1.115e+03\n",
      "  1.135e+03 -1.145e+03  1.147e+03  1.149e+03 -1.168e+03 -1.219e+03\n",
      "  1.229e+03 -1.231e+03 -1.233e+03 -1.241e+03 -1.242e+03  1.271e+03\n",
      " -1.302e+03 -1.339e+03 -1.345e+03 -1.351e+03  1.372e+03  1.386e+03\n",
      "  1.397e+03  1.423e+03 -1.424e+03 -1.450e+03 -1.452e+03  1.490e+03\n",
      " -1.495e+03 -1.531e+03 -1.531e+03 -1.553e+03  1.560e+03 -1.562e+03\n",
      " -1.567e+03 -1.618e+03  1.622e+03  1.624e+03 -1.633e+03 -1.636e+03\n",
      "  1.694e+03 -1.711e+03 -1.718e+03 -1.731e+03 -1.779e+03  1.791e+03\n",
      " -1.804e+03 -1.823e+03  1.852e+03 -1.855e+03 -1.876e+03 -1.900e+03\n",
      " -1.939e+03 -1.989e+03 -2.015e+03 -2.017e+03 -2.028e+03 -2.102e+03\n",
      "  2.116e+03  2.131e+03 -2.184e+03 -2.245e+03 -2.324e+03 -2.398e+03\n",
      " -2.412e+03 -2.415e+03 -2.425e+03 -2.455e+03  2.456e+03  2.486e+03\n",
      " -2.513e+03  2.532e+03  2.542e+03 -2.728e+03 -2.746e+03  3.125e+03\n",
      "  3.258e+03 -3.308e+03 -3.437e+03 -3.478e+03]\n",
      "Concordance Index 0.637225981838163\n",
      "Integrated Brier Score: 0.1945932569709806\n",
      "y_train breslow final [-1.000e+00 -6.000e+00 -6.000e+00 -6.000e+00 -9.000e+00  9.000e+00\n",
      " -1.000e+01  1.100e+01  1.200e+01 -1.200e+01 -1.500e+01  1.600e+01\n",
      "  1.900e+01 -2.000e+01 -2.000e+01 -2.100e+01 -2.200e+01 -2.300e+01\n",
      "  2.700e+01 -3.000e+01  3.100e+01  3.600e+01 -4.400e+01  4.600e+01\n",
      "  5.200e+01  5.600e+01  6.500e+01  6.500e+01  6.700e+01  8.700e+01\n",
      "  9.100e+01 -9.400e+01  9.700e+01  1.010e+02  1.020e+02  1.030e+02\n",
      "  1.070e+02  1.290e+02  1.290e+02 -1.370e+02  1.400e+02 -1.410e+02\n",
      " -1.700e+02  1.710e+02  1.710e+02 -1.830e+02  1.940e+02  1.950e+02\n",
      " -2.110e+02  2.140e+02  2.170e+02 -2.190e+02  2.230e+02 -2.290e+02\n",
      "  2.330e+02  2.470e+02 -2.480e+02 -2.500e+02 -2.600e+02  2.620e+02\n",
      "  2.620e+02  2.720e+02  2.790e+02 -2.910e+02  2.990e+02  3.000e+02\n",
      "  3.000e+02 -3.030e+02  3.030e+02  3.040e+02 -3.140e+02  3.150e+02\n",
      " -3.280e+02 -3.300e+02 -3.410e+02 -3.450e+02 -3.470e+02  3.470e+02\n",
      "  3.490e+02 -3.540e+02 -3.570e+02  3.590e+02 -3.610e+02 -3.610e+02\n",
      " -3.630e+02 -3.630e+02  3.650e+02 -3.660e+02 -3.720e+02  3.730e+02\n",
      "  3.810e+02 -3.820e+02 -3.870e+02 -3.870e+02 -3.900e+02 -3.960e+02\n",
      " -3.990e+02 -4.000e+02 -4.060e+02 -4.080e+02  4.100e+02 -4.120e+02\n",
      " -4.150e+02  4.160e+02 -4.230e+02  4.250e+02 -4.270e+02 -4.300e+02\n",
      "  4.320e+02 -4.360e+02  4.380e+02 -4.490e+02  4.520e+02 -4.530e+02\n",
      " -4.580e+02  4.690e+02 -4.720e+02 -4.740e+02 -4.760e+02 -4.800e+02\n",
      " -5.000e+02 -5.070e+02 -5.120e+02 -5.190e+02 -5.200e+02  5.350e+02\n",
      " -5.520e+02 -5.540e+02 -5.550e+02 -5.550e+02  5.560e+02  5.580e+02\n",
      " -5.620e+02 -5.640e+02 -5.700e+02 -5.740e+02 -5.790e+02 -5.850e+02\n",
      " -5.870e+02 -5.880e+02 -5.940e+02  5.960e+02 -6.010e+02  6.010e+02\n",
      " -6.020e+02 -6.080e+02  6.270e+02 -6.300e+02 -6.310e+02 -6.320e+02\n",
      "  6.330e+02 -6.360e+02 -6.380e+02  6.390e+02  6.430e+02 -6.440e+02\n",
      "  6.490e+02 -6.580e+02  6.600e+02 -6.620e+02 -6.720e+02 -6.800e+02\n",
      " -6.930e+02 -6.970e+02 -6.980e+02 -6.980e+02 -7.010e+02 -7.060e+02\n",
      "  7.110e+02 -7.190e+02  7.240e+02 -7.280e+02 -7.290e+02 -7.440e+02\n",
      " -7.560e+02 -7.630e+02  7.650e+02  7.680e+02  7.700e+02 -7.800e+02\n",
      " -7.820e+02 -7.840e+02 -7.990e+02  8.020e+02 -8.100e+02 -8.160e+02\n",
      "  8.270e+02 -8.290e+02  8.370e+02  8.370e+02 -8.480e+02 -8.480e+02\n",
      "  8.480e+02 -8.490e+02 -8.540e+02 -8.600e+02 -8.790e+02 -8.980e+02\n",
      "  8.990e+02 -9.060e+02 -9.100e+02 -9.170e+02 -9.250e+02  9.310e+02\n",
      " -9.420e+02 -9.890e+02 -1.008e+03 -1.030e+03 -1.049e+03 -1.066e+03\n",
      " -1.067e+03 -1.085e+03 -1.085e+03  1.088e+03 -1.098e+03 -1.115e+03\n",
      "  1.135e+03 -1.145e+03  1.147e+03  1.149e+03 -1.168e+03 -1.219e+03\n",
      "  1.229e+03 -1.231e+03 -1.233e+03 -1.241e+03 -1.242e+03  1.271e+03\n",
      " -1.302e+03 -1.339e+03 -1.345e+03 -1.351e+03  1.372e+03  1.386e+03\n",
      "  1.397e+03  1.423e+03 -1.424e+03 -1.450e+03 -1.452e+03  1.490e+03\n",
      " -1.495e+03 -1.531e+03 -1.531e+03 -1.553e+03  1.560e+03 -1.562e+03\n",
      " -1.567e+03 -1.618e+03  1.622e+03  1.624e+03 -1.633e+03 -1.636e+03\n",
      "  1.694e+03 -1.711e+03 -1.718e+03 -1.731e+03 -1.779e+03  1.791e+03\n",
      " -1.804e+03 -1.823e+03  1.852e+03 -1.855e+03 -1.876e+03 -1.900e+03\n",
      " -1.939e+03 -1.989e+03 -2.015e+03 -2.017e+03 -2.028e+03 -2.102e+03\n",
      "  2.116e+03  2.131e+03 -2.184e+03 -2.245e+03 -2.324e+03 -2.398e+03\n",
      " -2.412e+03 -2.415e+03 -2.425e+03 -2.455e+03  2.456e+03  2.486e+03\n",
      " -2.513e+03  2.532e+03  2.542e+03 -2.728e+03 -2.746e+03  3.125e+03\n",
      "  3.258e+03 -3.308e+03 -3.437e+03 -3.478e+03]\n",
      "durations 8.0 3675.0\n",
      "Concordance Index 0.6073253833049403\n",
      "Integrated Brier Score: 0.3481800414570151\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m88.1864\u001b[0m       \u001b[32m34.5227\u001b[0m  0.0212\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m94.5338\u001b[0m       \u001b[32m49.7107\u001b[0m  0.0323\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2       \u001b[36m86.8064\u001b[0m       41.4669  0.0205\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m43.0469\u001b[0m  0.0021\n",
      "      2       97.6491       53.8235  0.0205\n",
      "      2       43.0469  0.0016\n",
      "      3       43.0469  0.0016\n",
      "      4       43.0469  0.0015\n",
      "      5       43.0469  0.0022\n",
      "      6       43.0469  0.0051\n",
      "      7       43.0469  0.0016\n",
      "      8       43.0469  0.0015\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      9       43.0469  0.0015\n",
      "     10       43.0469  0.0015\n",
      "Restoring best model from epoch 1.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m87.2827\u001b[0m       \u001b[32m54.8223\u001b[0m  0.0204\n",
      "      3       \u001b[36m83.1017\u001b[0m       51.0113  0.0254\n",
      "      3      106.7271       65.2859  0.0250\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m38.6365\u001b[0m  0.0019\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      2       38.6365  0.0016\n",
      "      3       38.6365  0.0015\n",
      "      4       38.6365  0.0015\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      5       38.6365  0.0018\n",
      "      6       38.6365  0.0025\n",
      "      7       38.6365  0.0016\n",
      "      8       38.6365  0.0015\n",
      "      9       38.6365  0.0015\n",
      "      4       \u001b[36m73.1451\u001b[0m       41.3343  0.0226\n",
      "     10       38.6365  0.0015\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2       99.0125       \u001b[32m54.0909\u001b[0m  0.0233\n",
      "      4       \u001b[36m87.2227\u001b[0m       68.7970  0.0237\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m55.1507\u001b[0m  0.0021\n",
      "      2       55.1507  0.0018\n",
      "      3       55.1507  0.0017\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      4       55.1507  0.0018\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      5       55.1507  0.0019\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m57.6925\u001b[0m  0.0019\n",
      "      6       55.1507  0.0026\n",
      "      2       57.6925  0.0020\n",
      "      7       55.1507  0.0018\n",
      "      3       57.6925  0.0016\n",
      "      8       55.1507  0.0017\n",
      "      4       57.6925  0.0016\n",
      "      9       55.1507  0.0019\n",
      "      5       57.6925  0.0015\n",
      "     10       55.1507  0.0018\n",
      "Restoring best model from epoch 1.\n",
      "      5       \u001b[36m72.0274\u001b[0m       38.1514  0.0265\n",
      "      6       57.6925  0.0015\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      3       \u001b[36m78.0239\u001b[0m       \u001b[32m51.1858\u001b[0m  0.0249\n",
      "      5       \u001b[36m83.6444\u001b[0m       62.5464  0.0274\n",
      "      7       57.6925  0.0054\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      8       57.6925  0.0017\n",
      "      9       57.6925  0.0016\n",
      "     10       57.6925  0.0016\n",
      "Restoring best model from epoch 1.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m66.4293\u001b[0m       \u001b[32m32.0925\u001b[0m  0.0395\n",
      "      6       73.4577       47.1101  0.0249\n",
      "      4       \u001b[36m74.1893\u001b[0m       \u001b[32m50.2001\u001b[0m  0.0262\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m83.5044\u001b[0m       \u001b[32m44.1831\u001b[0m  0.0229\n",
      "      6       \u001b[36m81.3227\u001b[0m       56.9850  0.0274\n",
      "      2       77.9687       \u001b[32m31.4234\u001b[0m  0.0196\n",
      "      2       \u001b[36m81.3647\u001b[0m       45.6047  0.0197\n",
      "      7       \u001b[36m68.2042\u001b[0m       62.0815  0.0271\n",
      "      5       \u001b[36m73.9740\u001b[0m       54.9355  0.0208\n",
      "      7       81.6690       58.0754  0.0207\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m53.3164\u001b[0m  0.0034\n",
      "      2       53.3164  0.0019\n",
      "      3       53.3164  0.0016\n",
      "      4       53.3164  0.0016\n",
      "      8       69.9513       65.6202  0.0189\n",
      "      5       53.3164  0.0015\n",
      "      3       75.8397       34.6875  0.0371\n",
      "      6       74.0621       55.9304  0.0200\n",
      "      6       53.3164  0.0015\n",
      "      3       86.9452       50.3430  0.0222\n",
      "      7       53.3164  0.0016\n",
      "      8       \u001b[36m78.2118\u001b[0m       66.0580  0.0225\n",
      "      8       53.3164  0.0015\n",
      "      9       53.3164  0.0015\n",
      "     10       53.3164  0.0015\n",
      "Restoring best model from epoch 1.\n",
      "      9       \u001b[36m67.7397\u001b[0m       64.1041  0.0192\n",
      "      7       \u001b[36m72.7318\u001b[0m       55.1020  0.0191\n",
      "      4       \u001b[36m61.6209\u001b[0m       37.3671  0.0248\n",
      "      9       \u001b[36m77.1074\u001b[0m       67.4703  0.0261\n",
      "      4       \u001b[36m70.4929\u001b[0m       52.2739  0.0282\n",
      "     10       \u001b[36m67.5690\u001b[0m       58.7327  0.0198\n",
      "Restoring best model from epoch 1.\n",
      "      5       \u001b[36m60.2671\u001b[0m       37.1569  0.0189\n",
      "      8       73.5243       54.7967  0.0244\n",
      "      5       \u001b[36m70.1441\u001b[0m       48.8551  0.0192\n",
      "     10       \u001b[36m74.2004\u001b[0m       63.8617  0.0194\n",
      "Restoring best model from epoch 1.\n",
      "      6       \u001b[36m59.9370\u001b[0m       37.7667  0.0186\n",
      "      9       \u001b[36m72.6699\u001b[0m       54.6376  0.0197\n",
      "      6       \u001b[36m67.9635\u001b[0m       51.8976  0.0181\n",
      "      7       \u001b[36m57.4156\u001b[0m       43.2356  0.0191\n",
      "     10       \u001b[36m69.4915\u001b[0m       53.2097  0.0184\n",
      "Restoring best model from epoch 4.\n",
      "      7       \u001b[36m67.0213\u001b[0m       56.5088  0.0178\n",
      "      8       57.4454       45.1230  0.0177\n",
      "      8       \u001b[36m62.7614\u001b[0m       64.0785  0.0178\n",
      "      9       \u001b[36m57.2101\u001b[0m       46.7757  0.0188\n",
      "      9       71.7151       60.8627  0.0178\n",
      "     10       \u001b[36m54.6874\u001b[0m       48.4428  0.0178\n",
      "Restoring best model from epoch 2.\n",
      "     10       65.5638       54.2031  0.0174\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m81.6346\u001b[0m       \u001b[32m85.8323\u001b[0m  0.0254\n",
      "      2       95.7766       \u001b[32m79.3715\u001b[0m  0.0259\n",
      "      3       \u001b[36m75.3850\u001b[0m       89.0641  0.0316\n",
      "      4       \u001b[36m74.9882\u001b[0m       80.8888  0.0272\n",
      "      5       75.9001       79.6680  0.0278\n",
      "      6       \u001b[36m73.8643\u001b[0m       83.4989  0.0315\n",
      "      7       \u001b[36m73.0395\u001b[0m       89.9369  0.0271\n",
      "      8       \u001b[36m72.6105\u001b[0m       86.5405  0.0255\n",
      "      9       \u001b[36m69.1581\u001b[0m       82.5690  0.0319\n",
      "     10       69.6094       83.8091  0.0260\n",
      "Restoring best model from epoch 2.\n",
      "y_train breslow final [-1.000e+00 -6.000e+00 -6.000e+00 -8.000e+00 -9.000e+00 -9.000e+00\n",
      "  9.000e+00 -1.000e+01  1.100e+01  1.200e+01 -1.200e+01  1.400e+01\n",
      " -1.500e+01  1.600e+01  1.900e+01 -2.000e+01 -2.000e+01 -2.100e+01\n",
      " -2.200e+01 -2.300e+01  2.700e+01  3.400e+01  3.600e+01 -4.400e+01\n",
      "  4.600e+01  5.200e+01  5.600e+01  5.600e+01  6.500e+01 -7.900e+01\n",
      "  8.700e+01  9.100e+01  9.100e+01 -9.400e+01  1.020e+02  1.030e+02\n",
      "  1.150e+02  1.290e+02 -1.370e+02  1.400e+02 -1.410e+02  1.530e+02\n",
      " -1.700e+02  1.710e+02  1.710e+02 -1.800e+02  1.940e+02  1.950e+02\n",
      " -2.110e+02  2.140e+02  2.170e+02 -2.190e+02  2.230e+02 -2.290e+02\n",
      " -2.500e+02 -2.600e+02  2.620e+02  2.720e+02  2.780e+02  2.790e+02\n",
      "  2.830e+02  2.960e+02  3.000e+02  3.000e+02 -3.030e+02  3.030e+02\n",
      "  3.080e+02 -3.140e+02  3.150e+02 -3.220e+02 -3.270e+02 -3.410e+02\n",
      "  3.440e+02  3.490e+02 -3.570e+02 -3.590e+02  3.590e+02 -3.610e+02\n",
      " -3.610e+02 -3.630e+02 -3.630e+02 -3.650e+02  3.650e+02  3.660e+02\n",
      "  3.730e+02  3.810e+02 -3.870e+02 -3.900e+02  3.940e+02 -3.950e+02\n",
      " -3.950e+02 -3.990e+02 -4.000e+02 -4.060e+02 -4.090e+02  4.100e+02\n",
      "  4.120e+02 -4.120e+02  4.150e+02 -4.150e+02  4.160e+02  4.190e+02\n",
      " -4.230e+02 -4.250e+02  4.250e+02 -4.270e+02 -4.300e+02  4.320e+02\n",
      " -4.360e+02  4.380e+02 -4.440e+02 -4.470e+02 -4.490e+02 -4.530e+02\n",
      " -4.580e+02 -4.680e+02  4.690e+02 -4.760e+02 -4.780e+02 -4.800e+02\n",
      " -4.860e+02 -5.000e+02 -5.070e+02 -5.120e+02 -5.190e+02 -5.200e+02\n",
      " -5.200e+02  5.350e+02 -5.380e+02  5.470e+02 -5.520e+02 -5.540e+02\n",
      "  5.550e+02  5.580e+02 -5.620e+02 -5.640e+02 -5.660e+02 -5.740e+02\n",
      " -5.750e+02 -5.790e+02  5.810e+02 -5.870e+02 -5.940e+02  5.960e+02\n",
      " -6.010e+02  6.010e+02 -6.020e+02 -6.080e+02 -6.080e+02  6.120e+02\n",
      " -6.150e+02 -6.210e+02  6.270e+02 -6.310e+02 -6.320e+02  6.330e+02\n",
      "  6.390e+02 -6.400e+02 -6.440e+02  6.490e+02 -6.550e+02 -6.580e+02\n",
      "  6.600e+02 -6.620e+02 -6.710e+02 -6.720e+02 -6.730e+02 -6.800e+02\n",
      "  6.880e+02 -6.930e+02  6.930e+02 -6.970e+02 -6.980e+02 -7.010e+02\n",
      " -7.190e+02 -7.220e+02  7.240e+02 -7.280e+02 -7.440e+02 -7.470e+02\n",
      " -7.560e+02  7.570e+02  7.650e+02  7.680e+02  7.700e+02 -7.800e+02\n",
      " -7.840e+02 -7.990e+02 -8.160e+02 -8.190e+02  8.270e+02 -8.290e+02\n",
      "  8.370e+02  8.370e+02 -8.480e+02  8.480e+02 -8.490e+02 -8.540e+02\n",
      " -8.600e+02 -8.790e+02 -8.980e+02  8.990e+02 -9.060e+02 -9.250e+02\n",
      " -9.280e+02  9.310e+02 -9.420e+02 -9.510e+02 -9.890e+02  1.005e+03\n",
      " -1.008e+03 -1.030e+03 -1.049e+03 -1.067e+03 -1.085e+03 -1.085e+03\n",
      "  1.088e+03 -1.091e+03 -1.098e+03 -1.115e+03  1.135e+03 -1.145e+03\n",
      "  1.147e+03  1.149e+03 -1.168e+03  1.210e+03 -1.219e+03 -1.231e+03\n",
      " -1.233e+03 -1.241e+03 -1.242e+03  1.271e+03 -1.295e+03 -1.302e+03\n",
      " -1.345e+03 -1.351e+03 -1.363e+03  1.372e+03  1.397e+03 -1.424e+03\n",
      " -1.450e+03 -1.452e+03  1.490e+03 -1.495e+03 -1.516e+03 -1.531e+03\n",
      " -1.531e+03  1.560e+03 -1.562e+03 -1.567e+03 -1.618e+03  1.622e+03\n",
      " -1.633e+03  1.685e+03 -1.711e+03 -1.718e+03 -1.731e+03 -1.779e+03\n",
      "  1.791e+03 -1.804e+03 -1.823e+03  1.852e+03 -1.876e+03 -1.939e+03\n",
      " -1.970e+03 -1.989e+03 -2.017e+03 -2.028e+03 -2.102e+03 -2.202e+03\n",
      " -2.301e+03 -2.301e+03 -2.317e+03 -2.324e+03 -2.398e+03 -2.412e+03\n",
      " -2.415e+03 -2.425e+03 -2.442e+03 -2.455e+03  2.456e+03  2.486e+03\n",
      " -2.513e+03  2.532e+03  2.542e+03 -2.728e+03 -2.746e+03 -2.752e+03\n",
      "  3.125e+03 -3.308e+03 -3.437e+03 -3.675e+03]\n",
      "Concordance Index 0.6126000111900632\n",
      "Integrated Brier Score: 0.21881497385023083\n",
      "y_train breslow final [-1.000e+00 -6.000e+00 -6.000e+00 -8.000e+00 -9.000e+00 -9.000e+00\n",
      "  9.000e+00 -1.000e+01  1.100e+01  1.200e+01 -1.200e+01  1.400e+01\n",
      " -1.500e+01  1.600e+01  1.900e+01 -2.000e+01 -2.000e+01 -2.100e+01\n",
      " -2.200e+01 -2.300e+01  2.700e+01  3.400e+01  3.600e+01 -4.400e+01\n",
      "  4.600e+01  5.200e+01  5.600e+01  5.600e+01  6.500e+01 -7.900e+01\n",
      "  8.700e+01  9.100e+01  9.100e+01 -9.400e+01  1.020e+02  1.030e+02\n",
      "  1.150e+02  1.290e+02 -1.370e+02  1.400e+02 -1.410e+02  1.530e+02\n",
      " -1.700e+02  1.710e+02  1.710e+02 -1.800e+02  1.940e+02  1.950e+02\n",
      " -2.110e+02  2.140e+02  2.170e+02 -2.190e+02  2.230e+02 -2.290e+02\n",
      " -2.500e+02 -2.600e+02  2.620e+02  2.720e+02  2.780e+02  2.790e+02\n",
      "  2.830e+02  2.960e+02  3.000e+02  3.000e+02 -3.030e+02  3.030e+02\n",
      "  3.080e+02 -3.140e+02  3.150e+02 -3.220e+02 -3.270e+02 -3.410e+02\n",
      "  3.440e+02  3.490e+02 -3.570e+02 -3.590e+02  3.590e+02 -3.610e+02\n",
      " -3.610e+02 -3.630e+02 -3.630e+02 -3.650e+02  3.650e+02  3.660e+02\n",
      "  3.730e+02  3.810e+02 -3.870e+02 -3.900e+02  3.940e+02 -3.950e+02\n",
      " -3.950e+02 -3.990e+02 -4.000e+02 -4.060e+02 -4.090e+02  4.100e+02\n",
      "  4.120e+02 -4.120e+02  4.150e+02 -4.150e+02  4.160e+02  4.190e+02\n",
      " -4.230e+02 -4.250e+02  4.250e+02 -4.270e+02 -4.300e+02  4.320e+02\n",
      " -4.360e+02  4.380e+02 -4.440e+02 -4.470e+02 -4.490e+02 -4.530e+02\n",
      " -4.580e+02 -4.680e+02  4.690e+02 -4.760e+02 -4.780e+02 -4.800e+02\n",
      " -4.860e+02 -5.000e+02 -5.070e+02 -5.120e+02 -5.190e+02 -5.200e+02\n",
      " -5.200e+02  5.350e+02 -5.380e+02  5.470e+02 -5.520e+02 -5.540e+02\n",
      "  5.550e+02  5.580e+02 -5.620e+02 -5.640e+02 -5.660e+02 -5.740e+02\n",
      " -5.750e+02 -5.790e+02  5.810e+02 -5.870e+02 -5.940e+02  5.960e+02\n",
      " -6.010e+02  6.010e+02 -6.020e+02 -6.080e+02 -6.080e+02  6.120e+02\n",
      " -6.150e+02 -6.210e+02  6.270e+02 -6.310e+02 -6.320e+02  6.330e+02\n",
      "  6.390e+02 -6.400e+02 -6.440e+02  6.490e+02 -6.550e+02 -6.580e+02\n",
      "  6.600e+02 -6.620e+02 -6.710e+02 -6.720e+02 -6.730e+02 -6.800e+02\n",
      "  6.880e+02 -6.930e+02  6.930e+02 -6.970e+02 -6.980e+02 -7.010e+02\n",
      " -7.190e+02 -7.220e+02  7.240e+02 -7.280e+02 -7.440e+02 -7.470e+02\n",
      " -7.560e+02  7.570e+02  7.650e+02  7.680e+02  7.700e+02 -7.800e+02\n",
      " -7.840e+02 -7.990e+02 -8.160e+02 -8.190e+02  8.270e+02 -8.290e+02\n",
      "  8.370e+02  8.370e+02 -8.480e+02  8.480e+02 -8.490e+02 -8.540e+02\n",
      " -8.600e+02 -8.790e+02 -8.980e+02  8.990e+02 -9.060e+02 -9.250e+02\n",
      " -9.280e+02  9.310e+02 -9.420e+02 -9.510e+02 -9.890e+02  1.005e+03\n",
      " -1.008e+03 -1.030e+03 -1.049e+03 -1.067e+03 -1.085e+03 -1.085e+03\n",
      "  1.088e+03 -1.091e+03 -1.098e+03 -1.115e+03  1.135e+03 -1.145e+03\n",
      "  1.147e+03  1.149e+03 -1.168e+03  1.210e+03 -1.219e+03 -1.231e+03\n",
      " -1.233e+03 -1.241e+03 -1.242e+03  1.271e+03 -1.295e+03 -1.302e+03\n",
      " -1.345e+03 -1.351e+03 -1.363e+03  1.372e+03  1.397e+03 -1.424e+03\n",
      " -1.450e+03 -1.452e+03  1.490e+03 -1.495e+03 -1.516e+03 -1.531e+03\n",
      " -1.531e+03  1.560e+03 -1.562e+03 -1.567e+03 -1.618e+03  1.622e+03\n",
      " -1.633e+03  1.685e+03 -1.711e+03 -1.718e+03 -1.731e+03 -1.779e+03\n",
      "  1.791e+03 -1.804e+03 -1.823e+03  1.852e+03 -1.876e+03 -1.939e+03\n",
      " -1.970e+03 -1.989e+03 -2.017e+03 -2.028e+03 -2.102e+03 -2.202e+03\n",
      " -2.301e+03 -2.301e+03 -2.317e+03 -2.324e+03 -2.398e+03 -2.412e+03\n",
      " -2.415e+03 -2.425e+03 -2.442e+03 -2.455e+03  2.456e+03  2.486e+03\n",
      " -2.513e+03  2.532e+03  2.542e+03 -2.728e+03 -2.746e+03 -2.752e+03\n",
      "  3.125e+03 -3.308e+03 -3.437e+03 -3.675e+03]\n",
      "durations 6.0 3478.0\n",
      "Concordance Index 0.6135496183206107\n",
      "Integrated Brier Score: 0.23100105582354022\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m87.3648\u001b[0m       \u001b[32m57.0719\u001b[0m  0.0287\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m78.6338\u001b[0m       \u001b[32m49.0361\u001b[0m  0.0238\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m91.4023\u001b[0m       \u001b[32m43.5801\u001b[0m  0.0380\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m85.4792\u001b[0m       \u001b[32m57.9818\u001b[0m  0.0243\n",
      "      2       97.4728       57.0726  0.0200\n",
      "      2       \u001b[36m72.4663\u001b[0m       54.7904  0.0225\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m81.0048\u001b[0m       \u001b[32m30.7022\u001b[0m  0.0238\n",
      "      2       \u001b[36m85.6734\u001b[0m       46.2364  0.0204\n",
      "      2      106.0620       \u001b[32m42.6553\u001b[0m  0.0203\n",
      "      3       \u001b[36m72.1194\u001b[0m       49.4562  0.0207\n",
      "      3       \u001b[36m80.4421\u001b[0m       58.0147  0.0259\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      2       \u001b[36m76.5426\u001b[0m       \u001b[32m30.3486\u001b[0m  0.0292\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      4       \u001b[36m68.5459\u001b[0m       54.8313  0.0205\n",
      "      3       \u001b[36m84.5541\u001b[0m       51.6048  0.0282\n",
      "      4       \u001b[36m71.0251\u001b[0m       61.0023  0.0195\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m45.1769\u001b[0m  0.0074\n",
      "      2       45.1769  0.0016\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m60.6944\u001b[0m  0.0023\n",
      "      3       45.1769  0.0015\n",
      "      2       60.6944  0.0018\n",
      "      3       87.3132       \u001b[32m40.8301\u001b[0m  0.0324\n",
      "      4       45.1769  0.0015\n",
      "      3       60.6944  0.0016\n",
      "      5       45.1769  0.0015\n",
      "      4       60.6944  0.0015\n",
      "      6       45.1769  0.0016\n",
      "      5       60.6944  0.0019\n",
      "      7       45.1769  0.0015\n",
      "      8       45.1769  0.0015\n",
      "      6       60.6944  0.0021\n",
      "      9       45.1769  0.0015\n",
      "     10       45.1769  0.0015\n",
      "Restoring best model from epoch 1.\n",
      "      5       \u001b[36m63.1693\u001b[0m       65.4513  0.0200\n",
      "      4       \u001b[36m79.9909\u001b[0m       58.4660  0.0199\n",
      "      3      103.1621       35.3856  0.0274\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      7       60.6944  0.0042\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m75.4047\u001b[0m  0.0020\n",
      "      8       60.6944  0.0046\n",
      "      2       75.4047  0.0016\n",
      "      9       60.6944  0.0016\n",
      "      3       75.4047  0.0016\n",
      "     10       60.6944  0.0016\n",
      "Restoring best model from epoch 1.\n",
      "      4       75.4047  0.0016\n",
      "      5       75.4047  0.0016\n",
      "      5       73.2788       66.4246  0.0320\n",
      "      6       75.4047  0.0015\n",
      "      7       75.4047  0.0016\n",
      "      8       75.4047  0.0016\n",
      "      9       75.4047  0.0017\n",
      "      6       66.1126       70.8612  0.0194\n",
      "      5       \u001b[36m77.3834\u001b[0m       69.7214  0.0202\n",
      "     10       75.4047  0.0016\n",
      "Restoring best model from epoch 1.\n",
      "      4       \u001b[36m78.9011\u001b[0m       52.2689  0.0263\n",
      "      4       \u001b[36m66.8954\u001b[0m       32.1523  0.0207\n",
      "      6       75.3565       70.4105  0.0196\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m49.7565\u001b[0m  0.0020\n",
      "      5       \u001b[36m66.3055\u001b[0m       34.7061  0.0183\n",
      "      5       \u001b[36m76.8157\u001b[0m       49.2310  0.0192\n",
      "      2       49.7565  0.0021\n",
      "      6       \u001b[36m74.6032\u001b[0m       71.8051  0.0238\n",
      "      7       \u001b[36m61.3942\u001b[0m       67.9548  0.0247\n",
      "      3       49.7565  0.0068\n",
      "      4       49.7565  0.0019\n",
      "      5       49.7565  0.0017\n",
      "      7       71.0296       73.5804  0.0212\n",
      "      6       49.7565  0.0016\n",
      "      7       49.7565  0.0016\n",
      "      8       49.7565  0.0015\n",
      "      6       \u001b[36m65.7529\u001b[0m       42.7860  0.0183\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m54.3401\u001b[0m  0.0168\n",
      "      9       49.7565  0.0015\n",
      "      2       54.3401  0.0018\n",
      "     10       49.7565  0.0015\n",
      "Restoring best model from epoch 1.\n",
      "      6       \u001b[36m76.3052\u001b[0m       46.7969  0.0209\n",
      "      3       54.3401  0.0015\n",
      "      4       54.3401  0.0015\n",
      "      5       54.3401  0.0015\n",
      "      8       \u001b[36m57.5774\u001b[0m       68.2539  0.0222\n",
      "      6       54.3401  0.0015\n",
      "      7       54.3401  0.0017\n",
      "      7       \u001b[36m71.8591\u001b[0m       72.9864  0.0260\n",
      "      8       54.3401  0.0014\n",
      "      9       54.3401  0.0014\n",
      "     10       54.3401  0.0014\n",
      "      8       \u001b[36m70.6445\u001b[0m       76.7043  0.0234\n",
      "Restoring best model from epoch 1.\n",
      "      7       \u001b[36m65.7009\u001b[0m       44.8774  0.0208\n",
      "      7       \u001b[36m73.6970\u001b[0m       47.6990  0.0234\n",
      "      8       72.6466       78.7322  0.0248\n",
      "      9       \u001b[36m70.5915\u001b[0m       76.8254  0.0187\n",
      "      9       60.0264       68.5052  0.0351\n",
      "      8       \u001b[36m72.9575\u001b[0m       48.4951  0.0184      8       \u001b[36m64.3259\u001b[0m       40.0914  0.0238\n",
      "\n",
      "     10       \u001b[36m66.6079\u001b[0m       77.1060  0.0193\n",
      "Restoring best model from epoch 1.\n",
      "      9       \u001b[36m69.1274\u001b[0m       87.9682  0.0216\n",
      "     10       59.4307       69.8250  0.0189\n",
      "Restoring best model from epoch 1.\n",
      "      9       \u001b[36m72.0941\u001b[0m       48.7326  0.0184\n",
      "      9       \u001b[36m60.2900\u001b[0m       38.9036  0.0287\n",
      "     10       69.4857       93.2234  0.0190\n",
      "Restoring best model from epoch 1.\n",
      "     10       \u001b[36m70.8649\u001b[0m       47.7876  0.0189\n",
      "Restoring best model from epoch 3.\n",
      "     10       62.8850       41.5600  0.0186\n",
      "Restoring best model from epoch 2.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m89.8284\u001b[0m       \u001b[32m55.9932\u001b[0m  0.0262\n",
      "      2       \u001b[36m86.9492\u001b[0m       58.1476  0.0306\n",
      "      3       \u001b[36m76.8322\u001b[0m       \u001b[32m54.9231\u001b[0m  0.0291\n",
      "      4       \u001b[36m76.6607\u001b[0m       59.7144  0.0264\n",
      "      5       \u001b[36m74.2142\u001b[0m       57.2657  0.0268\n",
      "      6       \u001b[36m70.1827\u001b[0m       55.2283  0.0272\n",
      "      7       71.2592       \u001b[32m53.7724\u001b[0m  0.0271\n",
      "      8       \u001b[36m68.6630\u001b[0m       \u001b[32m52.8778\u001b[0m  0.0494\n",
      "      9       69.1566       52.9262  0.0267\n",
      "     10       69.1414       \u001b[32m51.8647\u001b[0m  0.0264\n",
      "y_train breslow final [-1.000e+00 -6.000e+00 -6.000e+00 -8.000e+00 -9.000e+00 -1.000e+01\n",
      "  1.400e+01 -1.500e+01  1.900e+01 -2.000e+01 -2.200e+01  2.700e+01\n",
      " -3.000e+01  3.100e+01  3.400e+01  3.600e+01 -4.400e+01  5.200e+01\n",
      "  5.600e+01  6.500e+01  6.700e+01 -7.900e+01  9.100e+01  9.100e+01\n",
      "  9.700e+01  1.010e+02  1.020e+02  1.070e+02  1.150e+02  1.290e+02\n",
      " -1.370e+02  1.400e+02  1.530e+02 -1.700e+02 -1.800e+02 -1.830e+02\n",
      "  1.950e+02  2.140e+02 -2.190e+02  2.230e+02 -2.290e+02  2.330e+02\n",
      "  2.470e+02 -2.480e+02 -2.500e+02 -2.600e+02  2.620e+02  2.620e+02\n",
      "  2.720e+02  2.780e+02  2.790e+02  2.830e+02 -2.910e+02  2.960e+02\n",
      "  2.990e+02  3.000e+02 -3.030e+02  3.030e+02  3.040e+02  3.080e+02\n",
      " -3.140e+02  3.150e+02 -3.220e+02 -3.270e+02 -3.280e+02 -3.300e+02\n",
      " -3.410e+02  3.440e+02 -3.450e+02 -3.470e+02  3.470e+02  3.490e+02\n",
      " -3.540e+02 -3.590e+02  3.590e+02 -3.610e+02 -3.630e+02 -3.650e+02\n",
      "  3.660e+02 -3.660e+02 -3.720e+02  3.730e+02  3.810e+02 -3.820e+02\n",
      " -3.870e+02  3.940e+02 -3.950e+02 -3.950e+02 -3.960e+02 -3.990e+02\n",
      " -4.000e+02 -4.060e+02 -4.060e+02 -4.080e+02 -4.090e+02  4.120e+02\n",
      "  4.150e+02 -4.150e+02  4.160e+02  4.190e+02 -4.250e+02 -4.270e+02\n",
      " -4.300e+02  4.320e+02 -4.360e+02 -4.440e+02 -4.470e+02 -4.490e+02\n",
      "  4.520e+02 -4.530e+02 -4.580e+02 -4.680e+02  4.690e+02 -4.720e+02\n",
      " -4.740e+02 -4.760e+02 -4.780e+02 -4.860e+02 -5.000e+02 -5.070e+02\n",
      " -5.120e+02 -5.190e+02 -5.200e+02 -5.200e+02  5.350e+02 -5.380e+02\n",
      "  5.470e+02 -5.540e+02 -5.550e+02 -5.550e+02  5.550e+02  5.560e+02\n",
      "  5.580e+02 -5.620e+02 -5.640e+02 -5.660e+02 -5.700e+02 -5.740e+02\n",
      " -5.750e+02 -5.790e+02  5.810e+02 -5.850e+02 -5.880e+02 -5.940e+02\n",
      "  5.960e+02 -6.010e+02 -6.020e+02 -6.080e+02 -6.080e+02  6.120e+02\n",
      " -6.150e+02 -6.210e+02 -6.300e+02 -6.310e+02 -6.320e+02  6.330e+02\n",
      " -6.360e+02 -6.380e+02  6.390e+02 -6.400e+02  6.430e+02 -6.440e+02\n",
      "  6.490e+02 -6.550e+02 -6.580e+02  6.600e+02 -6.620e+02 -6.710e+02\n",
      " -6.730e+02 -6.800e+02  6.880e+02 -6.930e+02  6.930e+02 -6.980e+02\n",
      " -6.980e+02 -7.010e+02 -7.060e+02  7.110e+02 -7.190e+02 -7.220e+02\n",
      " -7.280e+02 -7.290e+02 -7.440e+02 -7.470e+02  7.570e+02 -7.630e+02\n",
      "  7.650e+02  7.680e+02  7.700e+02 -7.800e+02 -7.820e+02 -7.840e+02\n",
      " -7.990e+02  8.020e+02 -8.100e+02 -8.160e+02 -8.190e+02 -8.290e+02\n",
      "  8.370e+02 -8.480e+02  8.480e+02 -8.490e+02 -8.540e+02 -8.790e+02\n",
      " -8.980e+02 -8.980e+02  8.990e+02 -9.060e+02 -9.100e+02 -9.170e+02\n",
      " -9.250e+02 -9.280e+02  9.310e+02 -9.420e+02 -9.510e+02  1.005e+03\n",
      " -1.049e+03 -1.066e+03 -1.067e+03 -1.085e+03 -1.085e+03  1.088e+03\n",
      " -1.091e+03 -1.098e+03 -1.115e+03  1.135e+03  1.149e+03  1.210e+03\n",
      " -1.219e+03  1.229e+03 -1.231e+03 -1.233e+03 -1.241e+03  1.271e+03\n",
      " -1.295e+03 -1.302e+03 -1.339e+03 -1.345e+03 -1.363e+03  1.372e+03\n",
      "  1.386e+03  1.397e+03  1.423e+03 -1.452e+03  1.490e+03 -1.495e+03\n",
      " -1.516e+03 -1.531e+03 -1.553e+03  1.560e+03  1.622e+03  1.624e+03\n",
      " -1.636e+03  1.685e+03  1.694e+03 -1.711e+03 -1.718e+03 -1.731e+03\n",
      " -1.779e+03  1.791e+03 -1.804e+03  1.852e+03 -1.855e+03 -1.900e+03\n",
      " -1.970e+03 -1.989e+03 -2.015e+03 -2.028e+03 -2.102e+03  2.116e+03\n",
      "  2.131e+03 -2.184e+03 -2.202e+03 -2.245e+03 -2.301e+03 -2.301e+03\n",
      " -2.317e+03 -2.324e+03 -2.412e+03 -2.442e+03  2.456e+03  2.486e+03\n",
      " -2.513e+03  2.532e+03 -2.746e+03 -2.752e+03  3.125e+03  3.258e+03\n",
      " -3.308e+03 -3.437e+03 -3.478e+03 -3.675e+03]\n",
      "Concordance Index 0.6504265960576641\n",
      "Integrated Brier Score: 0.23376135153022182\n",
      "y_train breslow final [-1.000e+00 -6.000e+00 -6.000e+00 -8.000e+00 -9.000e+00 -1.000e+01\n",
      "  1.400e+01 -1.500e+01  1.900e+01 -2.000e+01 -2.200e+01  2.700e+01\n",
      " -3.000e+01  3.100e+01  3.400e+01  3.600e+01 -4.400e+01  5.200e+01\n",
      "  5.600e+01  6.500e+01  6.700e+01 -7.900e+01  9.100e+01  9.100e+01\n",
      "  9.700e+01  1.010e+02  1.020e+02  1.070e+02  1.150e+02  1.290e+02\n",
      " -1.370e+02  1.400e+02  1.530e+02 -1.700e+02 -1.800e+02 -1.830e+02\n",
      "  1.950e+02  2.140e+02 -2.190e+02  2.230e+02 -2.290e+02  2.330e+02\n",
      "  2.470e+02 -2.480e+02 -2.500e+02 -2.600e+02  2.620e+02  2.620e+02\n",
      "  2.720e+02  2.780e+02  2.790e+02  2.830e+02 -2.910e+02  2.960e+02\n",
      "  2.990e+02  3.000e+02 -3.030e+02  3.030e+02  3.040e+02  3.080e+02\n",
      " -3.140e+02  3.150e+02 -3.220e+02 -3.270e+02 -3.280e+02 -3.300e+02\n",
      " -3.410e+02  3.440e+02 -3.450e+02 -3.470e+02  3.470e+02  3.490e+02\n",
      " -3.540e+02 -3.590e+02  3.590e+02 -3.610e+02 -3.630e+02 -3.650e+02\n",
      "  3.660e+02 -3.660e+02 -3.720e+02  3.730e+02  3.810e+02 -3.820e+02\n",
      " -3.870e+02  3.940e+02 -3.950e+02 -3.950e+02 -3.960e+02 -3.990e+02\n",
      " -4.000e+02 -4.060e+02 -4.060e+02 -4.080e+02 -4.090e+02  4.120e+02\n",
      "  4.150e+02 -4.150e+02  4.160e+02  4.190e+02 -4.250e+02 -4.270e+02\n",
      " -4.300e+02  4.320e+02 -4.360e+02 -4.440e+02 -4.470e+02 -4.490e+02\n",
      "  4.520e+02 -4.530e+02 -4.580e+02 -4.680e+02  4.690e+02 -4.720e+02\n",
      " -4.740e+02 -4.760e+02 -4.780e+02 -4.860e+02 -5.000e+02 -5.070e+02\n",
      " -5.120e+02 -5.190e+02 -5.200e+02 -5.200e+02  5.350e+02 -5.380e+02\n",
      "  5.470e+02 -5.540e+02 -5.550e+02 -5.550e+02  5.550e+02  5.560e+02\n",
      "  5.580e+02 -5.620e+02 -5.640e+02 -5.660e+02 -5.700e+02 -5.740e+02\n",
      " -5.750e+02 -5.790e+02  5.810e+02 -5.850e+02 -5.880e+02 -5.940e+02\n",
      "  5.960e+02 -6.010e+02 -6.020e+02 -6.080e+02 -6.080e+02  6.120e+02\n",
      " -6.150e+02 -6.210e+02 -6.300e+02 -6.310e+02 -6.320e+02  6.330e+02\n",
      " -6.360e+02 -6.380e+02  6.390e+02 -6.400e+02  6.430e+02 -6.440e+02\n",
      "  6.490e+02 -6.550e+02 -6.580e+02  6.600e+02 -6.620e+02 -6.710e+02\n",
      " -6.730e+02 -6.800e+02  6.880e+02 -6.930e+02  6.930e+02 -6.980e+02\n",
      " -6.980e+02 -7.010e+02 -7.060e+02  7.110e+02 -7.190e+02 -7.220e+02\n",
      " -7.280e+02 -7.290e+02 -7.440e+02 -7.470e+02  7.570e+02 -7.630e+02\n",
      "  7.650e+02  7.680e+02  7.700e+02 -7.800e+02 -7.820e+02 -7.840e+02\n",
      " -7.990e+02  8.020e+02 -8.100e+02 -8.160e+02 -8.190e+02 -8.290e+02\n",
      "  8.370e+02 -8.480e+02  8.480e+02 -8.490e+02 -8.540e+02 -8.790e+02\n",
      " -8.980e+02 -8.980e+02  8.990e+02 -9.060e+02 -9.100e+02 -9.170e+02\n",
      " -9.250e+02 -9.280e+02  9.310e+02 -9.420e+02 -9.510e+02  1.005e+03\n",
      " -1.049e+03 -1.066e+03 -1.067e+03 -1.085e+03 -1.085e+03  1.088e+03\n",
      " -1.091e+03 -1.098e+03 -1.115e+03  1.135e+03  1.149e+03  1.210e+03\n",
      " -1.219e+03  1.229e+03 -1.231e+03 -1.233e+03 -1.241e+03  1.271e+03\n",
      " -1.295e+03 -1.302e+03 -1.339e+03 -1.345e+03 -1.363e+03  1.372e+03\n",
      "  1.386e+03  1.397e+03  1.423e+03 -1.452e+03  1.490e+03 -1.495e+03\n",
      " -1.516e+03 -1.531e+03 -1.553e+03  1.560e+03  1.622e+03  1.624e+03\n",
      " -1.636e+03  1.685e+03  1.694e+03 -1.711e+03 -1.718e+03 -1.731e+03\n",
      " -1.779e+03  1.791e+03 -1.804e+03  1.852e+03 -1.855e+03 -1.900e+03\n",
      " -1.970e+03 -1.989e+03 -2.015e+03 -2.028e+03 -2.102e+03  2.116e+03\n",
      "  2.131e+03 -2.184e+03 -2.202e+03 -2.245e+03 -2.301e+03 -2.301e+03\n",
      " -2.317e+03 -2.324e+03 -2.412e+03 -2.442e+03  2.456e+03  2.486e+03\n",
      " -2.513e+03  2.532e+03 -2.746e+03 -2.752e+03  3.125e+03  3.258e+03\n",
      " -3.308e+03 -3.437e+03 -3.478e+03 -3.675e+03]\n",
      "durations 6.0 2728.0\n",
      "Concordance Index 0.46197874080130824\n",
      "Integrated Brier Score: 0.2587434531376844\n",
      "split gex_?|100130426      float32\n",
      "gex_?|100133144      float32\n",
      "gex_?|100134869      float32\n",
      "gex_?|10357          float32\n",
      "gex_?|10431          float32\n",
      "                      ...   \n",
      "gex_ZYG11A|440590    float32\n",
      "gex_ZYG11B|79699     float32\n",
      "gex_ZYX|7791         float32\n",
      "gex_ZZEF1|23140      float32\n",
      "gex_ZZZ3|26009       float32\n",
      "Length: 20531, dtype: object\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m78.4935\u001b[0m       \u001b[32m65.2231\u001b[0m  0.0539\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2       \u001b[36m77.7246\u001b[0m       72.7132  0.0410\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m86.0741\u001b[0m      \u001b[32m104.8977\u001b[0m  0.0677\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      3       \u001b[36m65.3192\u001b[0m       72.2444  0.0393\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m123.1672\u001b[0m  0.0032\n",
      "      2      123.1672  0.0022\n",
      "      2       91.3622       \u001b[32m90.1479\u001b[0m  0.0407\n",
      "      3      123.1672  0.0023\n",
      "      4      123.1672  0.0021\n",
      "      5      123.1672  0.0029\n",
      "      6      123.1672  0.0022\n",
      "      7      123.1672  0.0021\n",
      "      8      123.1672  0.0023\n",
      "      9      123.1672  0.0030\n",
      "     10      123.1672  0.0033\n",
      "Restoring best model from epoch 1.\n",
      "      4       67.6041       \u001b[32m60.6803\u001b[0m  0.0402\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      3       \u001b[36m78.1194\u001b[0m       94.4173  0.0483\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m126.3815\u001b[0m  0.0033\n",
      "      2      126.3815  0.0023\n",
      "      3      126.3815  0.0022\n",
      "      4      126.3815  0.0021\n",
      "      5      126.3815  0.0021\n",
      "      6      126.3815  0.0021\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      7      126.3815  0.0023\n",
      "      5       \u001b[36m64.5945\u001b[0m       60.8919  0.0362\n",
      "      8      126.3815  0.0023\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m80.4462\u001b[0m  0.0030\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m125.6218\u001b[0m  0.0032\n",
      "      9      126.3815  0.0023\n",
      "      2       80.4462  0.0024\n",
      "      2      125.6218  0.0026\n",
      "     10      126.3815  0.0025\n",
      "Restoring best model from epoch 1.\n",
      "      3       80.4462  0.0021\n",
      "      3      125.6218  0.0022\n",
      "      4       80.4462  0.0020\n",
      "      4      125.6218  0.0022\n",
      "      5       80.4462  0.0019\n",
      "      5      125.6218  0.0021\n",
      "      6      125.6218  0.0021\n",
      "      6       80.4462  0.0038\n",
      "      7      125.6218  0.0021\n",
      "      4       \u001b[36m76.5230\u001b[0m       93.5324  0.0365\n",
      "      7       80.4462  0.0023\n",
      "      8      125.6218  0.0021\n",
      "      8       80.4462  0.0020\n",
      "      9      125.6218  0.0021\n",
      "      9       80.4462  0.0019\n",
      "     10      125.6218  0.0021\n",
      "Restoring best model from epoch 1.\n",
      "     10       80.4462  0.0019\n",
      "Restoring best model from epoch 1.\n",
      "      6       \u001b[36m62.0000\u001b[0m       61.9176  0.0357\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m114.7610\u001b[0m  0.0032\n",
      "      2      114.7610  0.0022\n",
      "      3      114.7610  0.0021\n",
      "      4      114.7610  0.0020\n",
      "      5       \u001b[36m76.2381\u001b[0m       92.6502  0.0381\n",
      "      5      114.7610  0.0023\n",
      "      6      114.7610  0.0020\n",
      "      7      114.7610  0.0020\n",
      "      8      114.7610  0.0020\n",
      "      9      114.7610  0.0021\n",
      "     10      114.7610  0.0024\n",
      "Restoring best model from epoch 1.\n",
      "      7       \u001b[36m61.5020\u001b[0m       64.0410  0.0359\n",
      "      6       \u001b[36m73.2318\u001b[0m       93.4207  0.0362\n",
      "      8       \u001b[36m60.3643\u001b[0m       64.0837  0.0340\n",
      "      7       \u001b[36m72.6310\u001b[0m       94.7481  0.0382\n",
      "      9       61.4115       61.9762  0.0329\n",
      "      8       \u001b[36m72.5930\u001b[0m       93.7042  0.0347\n",
      "     10       \u001b[36m60.0989\u001b[0m       61.9289  0.0320\n",
      "Restoring best model from epoch 4.\n",
      "      9       \u001b[36m70.3215\u001b[0m       95.5987  0.0333\n",
      "     10       \u001b[36m69.4385\u001b[0m       99.6681  0.0337\n",
      "Restoring best model from epoch 2.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m132.4423\u001b[0m  0.0035\n",
      "      2      132.4423  0.0035\n",
      "      3      132.4423  0.0029\n",
      "      4      132.4423  0.0029\n",
      "      5      132.4423  0.0034\n",
      "      6      132.4423  0.0032\n",
      "      7      132.4423  0.0030\n",
      "      8      132.4423  0.0029\n",
      "      9      132.4423  0.0033\n",
      "     10      132.4423  0.0038\n",
      "Restoring best model from epoch 1.\n",
      "y_train breslow final [  -11.   -15.    18.    19.    22.   -24.   -28.    33.   -35.   -44.\n",
      "   -44.   -50.   -60.   -62.    62.    74.   -79.   -84.    87.    91.\n",
      "    97.    99.   116.   118.  -119.   121.   124.  -129.  -131.  -133.\n",
      "  -134.   139.  -139.  -141.  -151.   154.   161.   164.  -165.   167.\n",
      "  -174.   176.   179.  -179.  -182.  -184.  -186.   189.   193.  -202.\n",
      "   210.  -225.   237.   243.   244.   244.  -256.   257.   258.   260.\n",
      "  -264.   275.   281.   282.  -285.  -287.   291.   300.   303.  -307.\n",
      "   307.   308.  -310.   321.   321.  -323.   336.   339.   340.   343.\n",
      "  -351.  -353.   354.  -354.  -365.   370.  -372.   375.  -377.  -385.\n",
      "   385.  -385.  -408.   409.  -409.  -411.   414.  -414.  -415.  -417.\n",
      "  -418.  -422.  -423.  -424.  -426.  -426.  -427.   428.   434.  -435.\n",
      "  -435.  -440.   442.  -442.   444.   444.  -446.  -448.  -455.   460.\n",
      "  -466.   468.  -476.  -476.  -476.  -477.  -478.  -481.  -486.  -487.\n",
      "   488.  -499.   500.   503.  -515.  -520.  -522.  -526.  -534.  -536.\n",
      "  -537.  -539.  -540.  -541.  -545.  -546.   550.  -551.  -553.   561.\n",
      "  -564.  -565.  -567.  -568.  -568.  -573.   574.  -578.  -578.  -578.\n",
      "   586.  -591.   593.   594.  -595.  -596.   598.  -599.  -600.  -601.\n",
      "  -603.  -603.  -603.   607.  -608.  -609.  -610.  -617.  -624.   624.\n",
      "   625.  -626.   626.  -626.  -629.  -630.  -634.  -651.  -652.  -652.\n",
      "  -653.   656.  -658.   666.  -669.  -670.  -670.  -674.   677.  -677.\n",
      "  -683.  -689.  -690.   694.   697.  -701.   701.   702.  -704.  -704.\n",
      "  -705.   711.  -719.   719.  -724.   731.   737.  -741.  -747.   760.\n",
      "   761.  -761.  -775.   777.  -791.  -791.   800.  -805.   807.  -813.\n",
      "  -824.   826.  -829.  -830.  -839.  -842.  -845.  -852.   855.  -862.\n",
      "  -863.  -864.   864.  -866.   869.  -872.   879.  -882.  -882.  -889.\n",
      "   896.  -896.   896.  -904.   905.  -910.  -912.  -913.   922.  -930.\n",
      "  -938.  -944.  -947.  -949.   949.   952.   976.  -977.  -988.  -993.\n",
      "   995.  -997. -1013.  1026. -1036. -1040.  1043.  1046. -1060. -1071.\n",
      " -1072.  1073. -1079.  1081. -1097.  1115. -1118. -1119. -1125. -1126.\n",
      " -1126. -1130. -1148. -1157. -1159. -1163.  1167.  1171. -1175. -1178.\n",
      " -1189.  1194.  1209.  1215. -1216.  1229. -1233.  1235. -1246.  1258.\n",
      "  1265.  1268. -1272. -1280.  1288. -1289.  1293. -1301. -1305. -1324.\n",
      " -1351.  1357. -1367. -1369.  1379. -1400.  1421. -1429. -1431. -1432.\n",
      " -1442.  1454. -1479.  1498.  1499.  1501.  1516. -1523.  1528.  1531.\n",
      " -1559. -1617. -1621.  1622.  1632.  1653. -1683. -1700.  1725. -1728.\n",
      " -1750.  1778.  1790.  1798.  1830. -1847. -1864. -1870. -1893. -1932.\n",
      " -1965. -1974.  2027. -2065. -2067. -2109. -2137. -2161.  2174. -2199.\n",
      " -2224. -2248. -2261.  2318. -2368. -2449. -2488. -2515. -2595. -2616.\n",
      "  2617.  2620. -2696. -2823. -2832. -2973. -3059. -3094.  3169. -3261.\n",
      " -3305.  3361. -3674. -3759. -3940. -4765.  4961. -4992. -6732. -7062.\n",
      " -7248.]\n",
      "Concordance Index 0.44903106343581395\n",
      "Integrated Brier Score: 0.34839459487483165\n",
      "y_train breslow final [  -11.   -15.    18.    19.    22.   -24.   -28.    33.   -35.   -44.\n",
      "   -44.   -50.   -60.   -62.    62.    74.   -79.   -84.    87.    91.\n",
      "    97.    99.   116.   118.  -119.   121.   124.  -129.  -131.  -133.\n",
      "  -134.   139.  -139.  -141.  -151.   154.   161.   164.  -165.   167.\n",
      "  -174.   176.   179.  -179.  -182.  -184.  -186.   189.   193.  -202.\n",
      "   210.  -225.   237.   243.   244.   244.  -256.   257.   258.   260.\n",
      "  -264.   275.   281.   282.  -285.  -287.   291.   300.   303.  -307.\n",
      "   307.   308.  -310.   321.   321.  -323.   336.   339.   340.   343.\n",
      "  -351.  -353.   354.  -354.  -365.   370.  -372.   375.  -377.  -385.\n",
      "   385.  -385.  -408.   409.  -409.  -411.   414.  -414.  -415.  -417.\n",
      "  -418.  -422.  -423.  -424.  -426.  -426.  -427.   428.   434.  -435.\n",
      "  -435.  -440.   442.  -442.   444.   444.  -446.  -448.  -455.   460.\n",
      "  -466.   468.  -476.  -476.  -476.  -477.  -478.  -481.  -486.  -487.\n",
      "   488.  -499.   500.   503.  -515.  -520.  -522.  -526.  -534.  -536.\n",
      "  -537.  -539.  -540.  -541.  -545.  -546.   550.  -551.  -553.   561.\n",
      "  -564.  -565.  -567.  -568.  -568.  -573.   574.  -578.  -578.  -578.\n",
      "   586.  -591.   593.   594.  -595.  -596.   598.  -599.  -600.  -601.\n",
      "  -603.  -603.  -603.   607.  -608.  -609.  -610.  -617.  -624.   624.\n",
      "   625.  -626.   626.  -626.  -629.  -630.  -634.  -651.  -652.  -652.\n",
      "  -653.   656.  -658.   666.  -669.  -670.  -670.  -674.   677.  -677.\n",
      "  -683.  -689.  -690.   694.   697.  -701.   701.   702.  -704.  -704.\n",
      "  -705.   711.  -719.   719.  -724.   731.   737.  -741.  -747.   760.\n",
      "   761.  -761.  -775.   777.  -791.  -791.   800.  -805.   807.  -813.\n",
      "  -824.   826.  -829.  -830.  -839.  -842.  -845.  -852.   855.  -862.\n",
      "  -863.  -864.   864.  -866.   869.  -872.   879.  -882.  -882.  -889.\n",
      "   896.  -896.   896.  -904.   905.  -910.  -912.  -913.   922.  -930.\n",
      "  -938.  -944.  -947.  -949.   949.   952.   976.  -977.  -988.  -993.\n",
      "   995.  -997. -1013.  1026. -1036. -1040.  1043.  1046. -1060. -1071.\n",
      " -1072.  1073. -1079.  1081. -1097.  1115. -1118. -1119. -1125. -1126.\n",
      " -1126. -1130. -1148. -1157. -1159. -1163.  1167.  1171. -1175. -1178.\n",
      " -1189.  1194.  1209.  1215. -1216.  1229. -1233.  1235. -1246.  1258.\n",
      "  1265.  1268. -1272. -1280.  1288. -1289.  1293. -1301. -1305. -1324.\n",
      " -1351.  1357. -1367. -1369.  1379. -1400.  1421. -1429. -1431. -1432.\n",
      " -1442.  1454. -1479.  1498.  1499.  1501.  1516. -1523.  1528.  1531.\n",
      " -1559. -1617. -1621.  1622.  1632.  1653. -1683. -1700.  1725. -1728.\n",
      " -1750.  1778.  1790.  1798.  1830. -1847. -1864. -1870. -1893. -1932.\n",
      " -1965. -1974.  2027. -2065. -2067. -2109. -2137. -2161.  2174. -2199.\n",
      " -2224. -2248. -2261.  2318. -2368. -2449. -2488. -2515. -2595. -2616.\n",
      "  2617.  2620. -2696. -2823. -2832. -2973. -3059. -3094.  3169. -3261.\n",
      " -3305.  3361. -3674. -3759. -3940. -4765.  4961. -4992. -6732. -7062.\n",
      " -7248.]\n",
      "durations 4.0 3635.0\n",
      "Concordance Index 0.32077393075356414\n",
      "Integrated Brier Score: 0.4152810230831341\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m240.5898\u001b[0m  0.0024\n",
      "      2      240.5898  0.0019\n",
      "      3      240.5898  0.0019\n",
      "      4      240.5898  0.0019\n",
      "      5      240.5898  0.0018\n",
      "      6      240.5898  0.0018\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      7      240.5898  0.0033\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m220.3230\u001b[0m  0.0022\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2      220.3230  0.0020\n",
      "      3      220.3230  0.0019\n",
      "      8      240.5898  0.0054\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m281.2724\u001b[0m  0.0023\n",
      "      4      220.3230  0.0018\n",
      "      2      281.2724  0.0019\n",
      "      5      220.3230  0.0020\n",
      "      3      281.2724  0.0018\n",
      "      9      240.5898  0.0052\n",
      "      6      220.3230  0.0018\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      4      281.2724  0.0018\n",
      "     10      240.5898  0.0020\n",
      "Restoring best model from epoch 1.\n",
      "      7      220.3230  0.0018\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      5      281.2724  0.0018\n",
      "      8      220.3230  0.0018\n",
      "      6      281.2724  0.0017\n",
      "      9      220.3230  0.0018\n",
      "      7      281.2724  0.0018\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m80.0431\u001b[0m  0.0030\n",
      "     10      220.3230  0.0018\n",
      "Restoring best model from epoch 1.\n",
      "      8      281.2724  0.0018\n",
      "      9      281.2724  0.0017\n",
      "     10      281.2724  0.0017\n",
      "Restoring best model from epoch 1.\n",
      "      2       80.0431  0.0068\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      3       80.0431  0.0029\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      4       80.0431  0.0027\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m93.1200\u001b[0m  0.0030\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      5       80.0431  0.0027\n",
      "      2       93.1200  0.0027\n",
      "      6       80.0431  0.0026\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m73.4860\u001b[0m  0.0029\n",
      "      3       93.1200  0.0027\n",
      "      2       73.4860  0.0027\n",
      "      4       93.1200  0.0027\n",
      "      7       80.0431  0.0050\n",
      "      3       73.4860  0.0026\n",
      "      5       93.1200  0.0027\n",
      "      4       73.4860  0.0029\n",
      "      6       93.1200  0.0026\n",
      "      8       80.0431  0.0065\n",
      "      5       73.4860  0.0027\n",
      "      7       93.1200  0.0026\n",
      "      9       80.0431  0.0028\n",
      "      6       73.4860  0.0026\n",
      "      8       93.1200  0.0028\n",
      "     10       80.0431  0.0027\n",
      "Restoring best model from epoch 1.\n",
      "      9       93.1200  0.0026\n",
      "      7       73.4860  0.0028\n",
      "     10       93.1200  0.0026\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      8       73.4860  0.0027\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      9       73.4860  0.0029\n",
      "     10       73.4860  0.0026\n",
      "Restoring best model from epoch 1.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m88.2628\u001b[0m  0.0029\n",
      "      2       88.2628  0.0033\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      3       88.2628  0.0027\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      4       88.2628  0.0027\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m227.6306\u001b[0m  0.0027\n",
      "      2      227.6306  0.0020\n",
      "      5       88.2628  0.0029\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m92.3360\u001b[0m  0.0029\n",
      "      3      227.6306  0.0018\n",
      "      6       88.2628  0.0032\n",
      "      4      227.6306  0.0018\n",
      "      2       92.3360  0.0030\n",
      "      5      227.6306  0.0017\n",
      "      7       88.2628  0.0027\n",
      "      6      227.6306  0.0018\n",
      "      8       88.2628  0.0026\n",
      "      3       92.3360  0.0045\n",
      "      7      227.6306  0.0017\n",
      "      9       88.2628  0.0030\n",
      "      8      227.6306  0.0022\n",
      "     10       88.2628  0.0029\n",
      "Restoring best model from epoch 1.\n",
      "      4       92.3360  0.0074\n",
      "      9      227.6306  0.0047\n",
      "     10      227.6306  0.0020\n",
      "Restoring best model from epoch 1.\n",
      "      5       92.3360  0.0027\n",
      "      6       92.3360  0.0027\n",
      "      7       92.3360  0.0026\n",
      "      8       92.3360  0.0043\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      9       92.3360  0.0050\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m242.5562\u001b[0m  0.0026\n",
      "     10       92.3360  0.0031\n",
      "      2      242.5562  0.0017\n",
      "Restoring best model from epoch 1.\n",
      "      3      242.5562  0.0017\n",
      "      4      242.5562  0.0017\n",
      "      5      242.5562  0.0017\n",
      "      6      242.5562  0.0017\n",
      "      7      242.5562  0.0020\n",
      "      8      242.5562  0.0017\n",
      "      9      242.5562  0.0017\n",
      "     10      242.5562  0.0017\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m88.5232\u001b[0m  0.0031\n",
      "      2       88.5232  0.0034\n",
      "      3       88.5232  0.0038\n",
      "      4       88.5232  0.0036\n",
      "      5       88.5232  0.0037\n",
      "      6       88.5232  0.0038\n",
      "      7       88.5232  0.0036\n",
      "      8       88.5232  0.0034\n",
      "      9       88.5232  0.0053\n",
      "     10       88.5232  0.0050\n",
      "Restoring best model from epoch 1.\n",
      "y_train breslow final [ 4.000e+00 -1.100e+01 -1.300e+01 -1.400e+01 -1.500e+01  1.900e+01\n",
      "  2.200e+01 -2.800e+01  3.300e+01 -3.600e+01  3.800e+01 -4.800e+01\n",
      "  5.800e+01 -6.200e+01  7.400e+01 -7.900e+01 -8.400e+01 -8.400e+01\n",
      "  8.700e+01  9.100e+01  9.700e+01  1.160e+02  1.190e+02 -1.190e+02\n",
      "  1.240e+02 -1.290e+02 -1.310e+02 -1.340e+02  1.390e+02 -1.390e+02\n",
      " -1.410e+02 -1.510e+02  1.610e+02  1.640e+02  1.670e+02  1.710e+02\n",
      "  1.730e+02 -1.740e+02  1.760e+02 -1.820e+02 -1.840e+02 -1.860e+02\n",
      "  1.870e+02  1.890e+02  1.930e+02  2.100e+02 -2.180e+02 -2.240e+02\n",
      " -2.250e+02 -2.300e+02  2.430e+02  2.440e+02  2.500e+02 -2.560e+02\n",
      "  2.570e+02 -2.600e+02 -2.640e+02  2.680e+02  2.740e+02  2.810e+02\n",
      "  2.820e+02  2.910e+02  3.000e+02  3.030e+02 -3.070e+02  3.080e+02\n",
      " -3.100e+02  3.210e+02  3.210e+02 -3.230e+02  3.360e+02  3.360e+02\n",
      "  3.390e+02  3.430e+02 -3.510e+02  3.540e+02 -3.540e+02 -3.650e+02\n",
      "  3.700e+02  3.750e+02  3.760e+02 -3.850e+02  3.850e+02 -3.850e+02\n",
      "  4.090e+02 -4.090e+02 -4.110e+02  4.140e+02 -4.150e+02 -4.150e+02\n",
      " -4.160e+02 -4.170e+02 -4.180e+02 -4.220e+02 -4.260e+02 -4.260e+02\n",
      " -4.270e+02  4.280e+02  4.290e+02  4.340e+02  4.340e+02 -4.350e+02\n",
      " -4.350e+02 -4.350e+02  4.420e+02  4.440e+02  4.440e+02 -4.460e+02\n",
      " -4.480e+02 -4.550e+02 -4.570e+02  4.570e+02  4.600e+02 -4.620e+02\n",
      "  4.640e+02 -4.660e+02 -4.670e+02  4.680e+02  4.680e+02  4.690e+02\n",
      " -4.760e+02 -4.760e+02 -4.780e+02  4.780e+02 -4.810e+02 -4.840e+02\n",
      " -4.860e+02 -4.870e+02  4.880e+02 -4.920e+02 -4.990e+02  5.000e+02\n",
      "  5.030e+02 -5.050e+02 -5.130e+02 -5.150e+02 -5.200e+02 -5.220e+02\n",
      " -5.260e+02 -5.310e+02 -5.360e+02 -5.370e+02 -5.390e+02 -5.400e+02\n",
      " -5.410e+02 -5.460e+02 -5.470e+02  5.500e+02 -5.520e+02 -5.530e+02\n",
      "  5.570e+02  5.610e+02 -5.640e+02 -5.650e+02 -5.670e+02 -5.680e+02\n",
      " -5.680e+02 -5.680e+02 -5.730e+02  5.740e+02 -5.780e+02 -5.780e+02\n",
      " -5.780e+02  5.820e+02  5.860e+02 -5.910e+02 -5.920e+02 -5.960e+02\n",
      " -6.000e+02 -6.030e+02 -6.030e+02 -6.050e+02 -6.050e+02 -6.070e+02\n",
      "  6.070e+02 -6.080e+02 -6.090e+02 -6.100e+02 -6.100e+02 -6.100e+02\n",
      " -6.140e+02  6.140e+02 -6.170e+02 -6.170e+02 -6.240e+02  6.250e+02\n",
      " -6.260e+02  6.260e+02 -6.260e+02  6.280e+02 -6.290e+02 -6.510e+02\n",
      " -6.520e+02 -6.520e+02 -6.530e+02  6.530e+02  6.560e+02 -6.570e+02\n",
      " -6.580e+02 -6.580e+02 -6.640e+02  6.660e+02 -6.690e+02 -6.700e+02\n",
      " -6.740e+02  6.770e+02 -6.770e+02 -6.890e+02 -6.900e+02 -6.910e+02\n",
      "  6.940e+02 -7.010e+02  7.010e+02  7.020e+02 -7.040e+02 -7.040e+02\n",
      " -7.050e+02  7.110e+02 -7.180e+02 -7.190e+02  7.190e+02 -7.240e+02\n",
      " -7.280e+02 -7.300e+02  7.310e+02 -7.390e+02 -7.400e+02 -7.410e+02\n",
      " -7.470e+02  7.600e+02 -7.610e+02  7.610e+02 -7.730e+02 -7.750e+02\n",
      "  7.770e+02 -7.910e+02 -7.910e+02 -7.910e+02 -7.910e+02 -8.000e+02\n",
      " -8.050e+02 -8.060e+02  8.080e+02 -8.240e+02 -8.240e+02 -8.270e+02\n",
      " -8.390e+02 -8.420e+02 -8.450e+02 -8.520e+02  8.550e+02 -8.620e+02\n",
      " -8.630e+02 -8.640e+02  8.640e+02 -8.660e+02 -8.720e+02  8.790e+02\n",
      " -8.820e+02 -8.820e+02 -8.880e+02 -8.890e+02 -8.890e+02  8.960e+02\n",
      "  8.960e+02 -9.040e+02  9.050e+02 -9.100e+02 -9.120e+02 -9.130e+02\n",
      "  9.290e+02 -9.300e+02 -9.380e+02 -9.440e+02  9.490e+02  9.500e+02\n",
      "  9.520e+02 -9.770e+02  9.870e+02 -9.880e+02  9.950e+02  9.950e+02\n",
      " -9.970e+02  9.990e+02 -1.013e+03  1.026e+03 -1.036e+03 -1.040e+03\n",
      "  1.043e+03  1.046e+03 -1.060e+03  1.073e+03 -1.079e+03  1.081e+03\n",
      " -1.097e+03  1.115e+03 -1.118e+03 -1.119e+03 -1.126e+03 -1.130e+03\n",
      "  1.135e+03  1.147e+03 -1.157e+03 -1.159e+03 -1.163e+03  1.171e+03\n",
      " -1.175e+03 -1.178e+03 -1.189e+03 -1.189e+03  1.194e+03  1.197e+03\n",
      "  1.215e+03 -1.216e+03  1.229e+03 -1.233e+03  1.235e+03 -1.239e+03\n",
      " -1.246e+03  1.258e+03 -1.272e+03 -1.280e+03 -1.285e+03  1.288e+03\n",
      " -1.289e+03 -1.301e+03 -1.305e+03 -1.324e+03 -1.333e+03  1.357e+03\n",
      " -1.367e+03 -1.369e+03 -1.400e+03 -1.431e+03 -1.432e+03 -1.442e+03\n",
      "  1.454e+03 -1.474e+03  1.492e+03  1.499e+03  1.501e+03 -1.523e+03\n",
      "  1.528e+03  1.531e+03 -1.559e+03  1.600e+03 -1.617e+03 -1.621e+03\n",
      "  1.632e+03  1.653e+03 -1.683e+03 -1.700e+03  1.725e+03 -1.728e+03\n",
      "  1.778e+03  1.790e+03  1.798e+03  1.830e+03 -1.847e+03 -1.870e+03\n",
      " -1.893e+03 -1.932e+03 -1.974e+03  2.027e+03 -2.065e+03 -2.109e+03\n",
      " -2.137e+03 -2.161e+03  2.174e+03 -2.199e+03 -2.224e+03 -2.248e+03\n",
      " -2.261e+03  2.318e+03 -2.360e+03 -2.368e+03  2.393e+03 -2.488e+03\n",
      " -2.515e+03 -2.590e+03 -2.595e+03 -2.616e+03  2.620e+03 -2.676e+03\n",
      "  2.681e+03 -2.696e+03 -2.823e+03 -2.973e+03 -3.094e+03  3.169e+03\n",
      " -3.261e+03 -3.305e+03  3.361e+03 -3.635e+03 -3.759e+03 -3.940e+03\n",
      "  4.961e+03 -4.992e+03 -6.732e+03 -7.062e+03 -7.248e+03]\n",
      "Concordance Index 0.5331231278574807\n",
      "Integrated Brier Score: 0.198614059458693\n",
      "y_train breslow final [ 4.000e+00 -1.100e+01 -1.300e+01 -1.400e+01 -1.500e+01  1.900e+01\n",
      "  2.200e+01 -2.800e+01  3.300e+01 -3.600e+01  3.800e+01 -4.800e+01\n",
      "  5.800e+01 -6.200e+01  7.400e+01 -7.900e+01 -8.400e+01 -8.400e+01\n",
      "  8.700e+01  9.100e+01  9.700e+01  1.160e+02  1.190e+02 -1.190e+02\n",
      "  1.240e+02 -1.290e+02 -1.310e+02 -1.340e+02  1.390e+02 -1.390e+02\n",
      " -1.410e+02 -1.510e+02  1.610e+02  1.640e+02  1.670e+02  1.710e+02\n",
      "  1.730e+02 -1.740e+02  1.760e+02 -1.820e+02 -1.840e+02 -1.860e+02\n",
      "  1.870e+02  1.890e+02  1.930e+02  2.100e+02 -2.180e+02 -2.240e+02\n",
      " -2.250e+02 -2.300e+02  2.430e+02  2.440e+02  2.500e+02 -2.560e+02\n",
      "  2.570e+02 -2.600e+02 -2.640e+02  2.680e+02  2.740e+02  2.810e+02\n",
      "  2.820e+02  2.910e+02  3.000e+02  3.030e+02 -3.070e+02  3.080e+02\n",
      " -3.100e+02  3.210e+02  3.210e+02 -3.230e+02  3.360e+02  3.360e+02\n",
      "  3.390e+02  3.430e+02 -3.510e+02  3.540e+02 -3.540e+02 -3.650e+02\n",
      "  3.700e+02  3.750e+02  3.760e+02 -3.850e+02  3.850e+02 -3.850e+02\n",
      "  4.090e+02 -4.090e+02 -4.110e+02  4.140e+02 -4.150e+02 -4.150e+02\n",
      " -4.160e+02 -4.170e+02 -4.180e+02 -4.220e+02 -4.260e+02 -4.260e+02\n",
      " -4.270e+02  4.280e+02  4.290e+02  4.340e+02  4.340e+02 -4.350e+02\n",
      " -4.350e+02 -4.350e+02  4.420e+02  4.440e+02  4.440e+02 -4.460e+02\n",
      " -4.480e+02 -4.550e+02 -4.570e+02  4.570e+02  4.600e+02 -4.620e+02\n",
      "  4.640e+02 -4.660e+02 -4.670e+02  4.680e+02  4.680e+02  4.690e+02\n",
      " -4.760e+02 -4.760e+02 -4.780e+02  4.780e+02 -4.810e+02 -4.840e+02\n",
      " -4.860e+02 -4.870e+02  4.880e+02 -4.920e+02 -4.990e+02  5.000e+02\n",
      "  5.030e+02 -5.050e+02 -5.130e+02 -5.150e+02 -5.200e+02 -5.220e+02\n",
      " -5.260e+02 -5.310e+02 -5.360e+02 -5.370e+02 -5.390e+02 -5.400e+02\n",
      " -5.410e+02 -5.460e+02 -5.470e+02  5.500e+02 -5.520e+02 -5.530e+02\n",
      "  5.570e+02  5.610e+02 -5.640e+02 -5.650e+02 -5.670e+02 -5.680e+02\n",
      " -5.680e+02 -5.680e+02 -5.730e+02  5.740e+02 -5.780e+02 -5.780e+02\n",
      " -5.780e+02  5.820e+02  5.860e+02 -5.910e+02 -5.920e+02 -5.960e+02\n",
      " -6.000e+02 -6.030e+02 -6.030e+02 -6.050e+02 -6.050e+02 -6.070e+02\n",
      "  6.070e+02 -6.080e+02 -6.090e+02 -6.100e+02 -6.100e+02 -6.100e+02\n",
      " -6.140e+02  6.140e+02 -6.170e+02 -6.170e+02 -6.240e+02  6.250e+02\n",
      " -6.260e+02  6.260e+02 -6.260e+02  6.280e+02 -6.290e+02 -6.510e+02\n",
      " -6.520e+02 -6.520e+02 -6.530e+02  6.530e+02  6.560e+02 -6.570e+02\n",
      " -6.580e+02 -6.580e+02 -6.640e+02  6.660e+02 -6.690e+02 -6.700e+02\n",
      " -6.740e+02  6.770e+02 -6.770e+02 -6.890e+02 -6.900e+02 -6.910e+02\n",
      "  6.940e+02 -7.010e+02  7.010e+02  7.020e+02 -7.040e+02 -7.040e+02\n",
      " -7.050e+02  7.110e+02 -7.180e+02 -7.190e+02  7.190e+02 -7.240e+02\n",
      " -7.280e+02 -7.300e+02  7.310e+02 -7.390e+02 -7.400e+02 -7.410e+02\n",
      " -7.470e+02  7.600e+02 -7.610e+02  7.610e+02 -7.730e+02 -7.750e+02\n",
      "  7.770e+02 -7.910e+02 -7.910e+02 -7.910e+02 -7.910e+02 -8.000e+02\n",
      " -8.050e+02 -8.060e+02  8.080e+02 -8.240e+02 -8.240e+02 -8.270e+02\n",
      " -8.390e+02 -8.420e+02 -8.450e+02 -8.520e+02  8.550e+02 -8.620e+02\n",
      " -8.630e+02 -8.640e+02  8.640e+02 -8.660e+02 -8.720e+02  8.790e+02\n",
      " -8.820e+02 -8.820e+02 -8.880e+02 -8.890e+02 -8.890e+02  8.960e+02\n",
      "  8.960e+02 -9.040e+02  9.050e+02 -9.100e+02 -9.120e+02 -9.130e+02\n",
      "  9.290e+02 -9.300e+02 -9.380e+02 -9.440e+02  9.490e+02  9.500e+02\n",
      "  9.520e+02 -9.770e+02  9.870e+02 -9.880e+02  9.950e+02  9.950e+02\n",
      " -9.970e+02  9.990e+02 -1.013e+03  1.026e+03 -1.036e+03 -1.040e+03\n",
      "  1.043e+03  1.046e+03 -1.060e+03  1.073e+03 -1.079e+03  1.081e+03\n",
      " -1.097e+03  1.115e+03 -1.118e+03 -1.119e+03 -1.126e+03 -1.130e+03\n",
      "  1.135e+03  1.147e+03 -1.157e+03 -1.159e+03 -1.163e+03  1.171e+03\n",
      " -1.175e+03 -1.178e+03 -1.189e+03 -1.189e+03  1.194e+03  1.197e+03\n",
      "  1.215e+03 -1.216e+03  1.229e+03 -1.233e+03  1.235e+03 -1.239e+03\n",
      " -1.246e+03  1.258e+03 -1.272e+03 -1.280e+03 -1.285e+03  1.288e+03\n",
      " -1.289e+03 -1.301e+03 -1.305e+03 -1.324e+03 -1.333e+03  1.357e+03\n",
      " -1.367e+03 -1.369e+03 -1.400e+03 -1.431e+03 -1.432e+03 -1.442e+03\n",
      "  1.454e+03 -1.474e+03  1.492e+03  1.499e+03  1.501e+03 -1.523e+03\n",
      "  1.528e+03  1.531e+03 -1.559e+03  1.600e+03 -1.617e+03 -1.621e+03\n",
      "  1.632e+03  1.653e+03 -1.683e+03 -1.700e+03  1.725e+03 -1.728e+03\n",
      "  1.778e+03  1.790e+03  1.798e+03  1.830e+03 -1.847e+03 -1.870e+03\n",
      " -1.893e+03 -1.932e+03 -1.974e+03  2.027e+03 -2.065e+03 -2.109e+03\n",
      " -2.137e+03 -2.161e+03  2.174e+03 -2.199e+03 -2.224e+03 -2.248e+03\n",
      " -2.261e+03  2.318e+03 -2.360e+03 -2.368e+03  2.393e+03 -2.488e+03\n",
      " -2.515e+03 -2.590e+03 -2.595e+03 -2.616e+03  2.620e+03 -2.676e+03\n",
      "  2.681e+03 -2.696e+03 -2.823e+03 -2.973e+03 -3.094e+03  3.169e+03\n",
      " -3.261e+03 -3.305e+03  3.361e+03 -3.635e+03 -3.759e+03 -3.940e+03\n",
      "  4.961e+03 -4.992e+03 -6.732e+03 -7.062e+03 -7.248e+03]\n",
      "durations 18.0 4765.0\n",
      "Concordance Index 0.4452164617851416\n",
      "Integrated Brier Score: 0.20887635731409174\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m217.4252\u001b[0m  0.0025\n",
      "      2      217.4252  0.0019\n",
      "      3      217.4252  0.0018\n",
      "      4      217.4252  0.0017\n",
      "      5      217.4252  0.0028\n",
      "      6      217.4252  0.0050\n",
      "      7      217.4252  0.0037\n",
      "      8      217.4252  0.0020\n",
      "      9      217.4252  0.0018\n",
      "     10      217.4252  0.0017\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m169.4287\u001b[0m  0.0029\n",
      "      2      169.4287  0.0021\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      3      169.4287  0.0020\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      4      169.4287  0.0019\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      5      169.4287  0.0018\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m380.2364\u001b[0m  0.0031\n",
      "      6      169.4287  0.0017\n",
      "      2      380.2364  0.0019\n",
      "      7      169.4287  0.0017\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m63.2013\u001b[0m  0.0035\n",
      "      3      380.2364  0.0018\n",
      "      8      169.4287  0.0023\n",
      "      4      380.2364  0.0020\n",
      "      2       63.2013  0.0031\n",
      "      9      169.4287  0.0018\n",
      "      5      380.2364  0.0021\n",
      "     10      169.4287  0.0017\n",
      "Restoring best model from epoch 1.\n",
      "      6      380.2364  0.0022\n",
      "      3       63.2013  0.0065\n",
      "      7      380.2364  0.0044\n",
      "      4       63.2013  0.0028\n",
      "      8      380.2364  0.0032\n",
      "      5       63.2013  0.0027\n",
      "      9      380.2364  0.0018\n",
      "      6       63.2013  0.0027\n",
      "     10      380.2364  0.0019\n",
      "Restoring best model from epoch 1.\n",
      "      7       63.2013  0.0027\n",
      "      8       63.2013  0.0027\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      9       63.2013  0.0027\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "     10       63.2013  0.0027\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m85.7124\u001b[0m  0.0086\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m167.3575\u001b[0m  0.0075\n",
      "      2       85.7124  0.0046\n",
      "      2      167.3575  0.0033\n",
      "      3       85.7124  0.0029\n",
      "      3      167.3575  0.0018\n",
      "      4       85.7124  0.0027\n",
      "      4      167.3575  0.0018\n",
      "      5      167.3575  0.0017\n",
      "      5       85.7124  0.0027\n",
      "      6      167.3575  0.0017\n",
      "      6       85.7124  0.0027\n",
      "      7      167.3575  0.0017\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      8      167.3575  0.0017\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      7       85.7124  0.0027\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      9      167.3575  0.0018\n",
      "      8       85.7124  0.0027\n",
      "     10      167.3575  0.0017\n",
      "Restoring best model from epoch 1.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m171.4089\u001b[0m  0.0028\n",
      "      9       85.7124  0.0027\n",
      "      2      171.4089  0.0020\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m79.3446\u001b[0m  0.0037\n",
      "      3      171.4089  0.0020\n",
      "     10       85.7124  0.0026\n",
      "Restoring best model from epoch 1.\n",
      "      4      171.4089  0.0017\n",
      "      2       79.3446  0.0028\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      5      171.4089  0.0016\n",
      "      3       79.3446  0.0027\n",
      "      6      171.4089  0.0016\n",
      "      7      171.4089  0.0016\n",
      "      4       79.3446  0.0026\n",
      "      8      171.4089  0.0016\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      9      171.4089  0.0016\n",
      "     10      171.4089  0.0016\n",
      "Restoring best model from epoch 1.\n",
      "      5       79.3446  0.0066\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m68.0304\u001b[0m  0.0042\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      6       79.3446  0.0035\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2       68.0304  0.0030\n",
      "      3       68.0304  0.0027\n",
      "      7       79.3446  0.0057\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m68.2078\u001b[0m  0.0041\n",
      "      4       68.0304  0.0026\n",
      "      8       79.3446  0.0030\n",
      "      2       68.2078  0.0027\n",
      "      5       68.0304  0.0026\n",
      "      3       68.2078  0.0026\n",
      "      6       68.0304  0.0026\n",
      "      9       79.3446  0.0046\n",
      "      7       68.0304  0.0026\n",
      "      4       68.2078  0.0026\n",
      "      5       68.2078  0.0026\n",
      "     10       79.3446  0.0051\n",
      "Restoring best model from epoch 1.\n",
      "      6       68.2078  0.0026\n",
      "      8       68.0304  0.0025\n",
      "      7       68.2078  0.0026\n",
      "      9       68.0304  0.0029\n",
      "      8       68.2078  0.0026\n",
      "     10       68.0304  0.0026\n",
      "Restoring best model from epoch 1.\n",
      "      9       68.2078  0.0026\n",
      "     10       68.2078  0.0026\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m103.1231\u001b[0m  0.0038\n",
      "      2      103.1231  0.0035\n",
      "      3      103.1231  0.0036\n",
      "      4      103.1231  0.0032\n",
      "      5      103.1231  0.0034\n",
      "      6      103.1231  0.0042\n",
      "      7      103.1231  0.0054\n",
      "      8      103.1231  0.0058\n",
      "      9      103.1231  0.0186\n",
      "     10      103.1231  0.0058\n",
      "Restoring best model from epoch 1.\n",
      "y_train breslow final [ 4.000e+00 -1.300e+01 -1.400e+01  1.800e+01  1.900e+01 -2.400e+01\n",
      " -2.800e+01  3.300e+01 -3.500e+01 -3.600e+01  3.800e+01 -4.400e+01\n",
      " -4.400e+01 -4.800e+01 -5.000e+01  5.800e+01 -6.000e+01 -6.200e+01\n",
      "  6.200e+01  7.400e+01 -8.400e+01  9.700e+01  9.900e+01  1.160e+02\n",
      "  1.180e+02  1.190e+02  1.210e+02  1.240e+02 -1.330e+02 -1.340e+02\n",
      " -1.390e+02 -1.410e+02  1.540e+02  1.610e+02  1.640e+02 -1.650e+02\n",
      "  1.670e+02  1.710e+02  1.730e+02  1.760e+02  1.790e+02 -1.790e+02\n",
      " -1.820e+02 -1.840e+02 -1.860e+02 -1.860e+02  1.870e+02  1.890e+02\n",
      "  1.930e+02 -2.020e+02  2.100e+02 -2.180e+02 -2.240e+02 -2.250e+02\n",
      " -2.300e+02  2.370e+02  2.440e+02  2.500e+02 -2.560e+02  2.570e+02\n",
      "  2.580e+02  2.600e+02 -2.600e+02 -2.640e+02  2.680e+02  2.740e+02\n",
      "  2.750e+02  2.810e+02 -2.850e+02 -2.870e+02  2.910e+02  3.030e+02\n",
      " -3.070e+02  3.070e+02  3.080e+02  3.210e+02 -3.230e+02  3.360e+02\n",
      "  3.360e+02  3.400e+02  3.430e+02 -3.510e+02 -3.530e+02  3.540e+02\n",
      " -3.540e+02 -3.720e+02  3.750e+02  3.760e+02 -3.770e+02 -3.850e+02\n",
      "  3.850e+02 -4.080e+02 -4.090e+02 -4.110e+02  4.140e+02 -4.140e+02\n",
      " -4.150e+02 -4.150e+02 -4.160e+02 -4.170e+02 -4.180e+02 -4.230e+02\n",
      " -4.240e+02  4.280e+02  4.290e+02  4.340e+02 -4.350e+02 -4.350e+02\n",
      " -4.400e+02  4.420e+02 -4.420e+02  4.440e+02 -4.480e+02 -4.480e+02\n",
      " -4.550e+02 -4.570e+02  4.570e+02 -4.620e+02  4.640e+02 -4.660e+02\n",
      " -4.670e+02  4.680e+02  4.680e+02  4.690e+02 -4.760e+02 -4.760e+02\n",
      " -4.760e+02 -4.770e+02 -4.780e+02  4.780e+02 -4.810e+02 -4.840e+02\n",
      " -4.920e+02 -4.990e+02  5.000e+02  5.030e+02 -5.050e+02 -5.130e+02\n",
      " -5.150e+02 -5.150e+02 -5.200e+02 -5.260e+02 -5.310e+02 -5.340e+02\n",
      " -5.360e+02 -5.370e+02 -5.400e+02 -5.410e+02 -5.450e+02 -5.470e+02\n",
      "  5.500e+02 -5.510e+02 -5.520e+02 -5.530e+02  5.570e+02  5.610e+02\n",
      " -5.650e+02 -5.680e+02 -5.680e+02 -5.680e+02 -5.730e+02  5.740e+02\n",
      "  5.820e+02  5.860e+02 -5.920e+02  5.930e+02  5.940e+02 -5.950e+02\n",
      "  5.980e+02 -5.990e+02 -6.000e+02 -6.010e+02 -6.030e+02 -6.030e+02\n",
      " -6.050e+02 -6.050e+02 -6.070e+02  6.070e+02 -6.080e+02 -6.090e+02\n",
      " -6.100e+02 -6.100e+02 -6.100e+02 -6.140e+02  6.140e+02 -6.170e+02\n",
      " -6.170e+02  6.240e+02  6.250e+02 -6.260e+02  6.260e+02 -6.260e+02\n",
      "  6.280e+02 -6.290e+02 -6.300e+02 -6.340e+02 -6.510e+02 -6.510e+02\n",
      " -6.520e+02 -6.530e+02  6.530e+02  6.560e+02 -6.570e+02 -6.580e+02\n",
      " -6.580e+02 -6.640e+02 -6.690e+02 -6.700e+02 -6.700e+02 -6.740e+02\n",
      " -6.830e+02 -6.900e+02 -6.910e+02  6.970e+02 -7.010e+02  7.010e+02\n",
      " -7.040e+02 -7.040e+02 -7.050e+02 -7.050e+02 -7.180e+02 -7.190e+02\n",
      "  7.190e+02 -7.240e+02 -7.280e+02 -7.300e+02  7.310e+02  7.370e+02\n",
      " -7.390e+02 -7.400e+02 -7.410e+02 -7.470e+02 -7.610e+02  7.610e+02\n",
      " -7.610e+02 -7.730e+02 -7.750e+02  7.770e+02 -7.910e+02 -7.910e+02\n",
      " -7.910e+02 -7.910e+02  8.000e+02 -8.000e+02 -8.060e+02  8.070e+02\n",
      "  8.080e+02 -8.130e+02 -8.240e+02  8.260e+02 -8.270e+02 -8.290e+02\n",
      " -8.300e+02 -8.390e+02 -8.450e+02 -8.520e+02  8.550e+02 -8.630e+02\n",
      "  8.640e+02  8.690e+02 -8.720e+02  8.790e+02 -8.820e+02 -8.820e+02\n",
      " -8.880e+02 -8.890e+02  8.960e+02 -8.960e+02  8.960e+02 -9.040e+02\n",
      " -9.130e+02  9.220e+02  9.290e+02 -9.380e+02 -9.440e+02 -9.470e+02\n",
      " -9.490e+02  9.490e+02  9.500e+02  9.520e+02  9.760e+02 -9.770e+02\n",
      "  9.870e+02 -9.930e+02  9.950e+02  9.990e+02 -1.013e+03  1.026e+03\n",
      " -1.036e+03 -1.040e+03 -1.060e+03 -1.071e+03 -1.072e+03  1.073e+03\n",
      "  1.081e+03 -1.118e+03 -1.119e+03 -1.125e+03 -1.126e+03 -1.126e+03\n",
      "  1.135e+03  1.147e+03 -1.148e+03 -1.157e+03 -1.159e+03 -1.163e+03\n",
      "  1.167e+03  1.171e+03 -1.178e+03 -1.189e+03 -1.189e+03  1.194e+03\n",
      "  1.197e+03  1.209e+03  1.215e+03 -1.216e+03  1.229e+03 -1.233e+03\n",
      "  1.235e+03 -1.239e+03 -1.246e+03  1.258e+03  1.265e+03  1.268e+03\n",
      " -1.272e+03 -1.280e+03 -1.285e+03 -1.289e+03  1.293e+03 -1.305e+03\n",
      " -1.324e+03 -1.333e+03 -1.351e+03  1.357e+03 -1.367e+03 -1.369e+03\n",
      "  1.379e+03 -1.400e+03  1.421e+03 -1.429e+03 -1.432e+03 -1.474e+03\n",
      " -1.479e+03  1.492e+03  1.498e+03  1.499e+03  1.501e+03  1.516e+03\n",
      " -1.523e+03  1.528e+03  1.531e+03 -1.559e+03  1.600e+03 -1.621e+03\n",
      "  1.622e+03  1.632e+03  1.653e+03 -1.700e+03  1.725e+03 -1.728e+03\n",
      " -1.750e+03  1.798e+03 -1.847e+03 -1.864e+03 -1.893e+03 -1.932e+03\n",
      " -1.965e+03 -1.974e+03 -2.065e+03 -2.067e+03 -2.137e+03 -2.161e+03\n",
      "  2.174e+03 -2.199e+03 -2.224e+03 -2.248e+03 -2.261e+03  2.318e+03\n",
      " -2.360e+03 -2.368e+03  2.393e+03 -2.449e+03 -2.488e+03 -2.515e+03\n",
      " -2.590e+03 -2.595e+03  2.617e+03 -2.676e+03  2.681e+03 -2.696e+03\n",
      " -2.832e+03 -2.973e+03 -3.059e+03 -3.094e+03 -3.261e+03 -3.305e+03\n",
      "  3.361e+03 -3.635e+03 -3.674e+03 -3.940e+03 -4.765e+03 -6.732e+03]\n",
      "Concordance Index 0.5396232339089482\n",
      "Integrated Brier Score: 0.21176809709646272\n",
      "y_train breslow final [ 4.000e+00 -1.300e+01 -1.400e+01  1.800e+01  1.900e+01 -2.400e+01\n",
      " -2.800e+01  3.300e+01 -3.500e+01 -3.600e+01  3.800e+01 -4.400e+01\n",
      " -4.400e+01 -4.800e+01 -5.000e+01  5.800e+01 -6.000e+01 -6.200e+01\n",
      "  6.200e+01  7.400e+01 -8.400e+01  9.700e+01  9.900e+01  1.160e+02\n",
      "  1.180e+02  1.190e+02  1.210e+02  1.240e+02 -1.330e+02 -1.340e+02\n",
      " -1.390e+02 -1.410e+02  1.540e+02  1.610e+02  1.640e+02 -1.650e+02\n",
      "  1.670e+02  1.710e+02  1.730e+02  1.760e+02  1.790e+02 -1.790e+02\n",
      " -1.820e+02 -1.840e+02 -1.860e+02 -1.860e+02  1.870e+02  1.890e+02\n",
      "  1.930e+02 -2.020e+02  2.100e+02 -2.180e+02 -2.240e+02 -2.250e+02\n",
      " -2.300e+02  2.370e+02  2.440e+02  2.500e+02 -2.560e+02  2.570e+02\n",
      "  2.580e+02  2.600e+02 -2.600e+02 -2.640e+02  2.680e+02  2.740e+02\n",
      "  2.750e+02  2.810e+02 -2.850e+02 -2.870e+02  2.910e+02  3.030e+02\n",
      " -3.070e+02  3.070e+02  3.080e+02  3.210e+02 -3.230e+02  3.360e+02\n",
      "  3.360e+02  3.400e+02  3.430e+02 -3.510e+02 -3.530e+02  3.540e+02\n",
      " -3.540e+02 -3.720e+02  3.750e+02  3.760e+02 -3.770e+02 -3.850e+02\n",
      "  3.850e+02 -4.080e+02 -4.090e+02 -4.110e+02  4.140e+02 -4.140e+02\n",
      " -4.150e+02 -4.150e+02 -4.160e+02 -4.170e+02 -4.180e+02 -4.230e+02\n",
      " -4.240e+02  4.280e+02  4.290e+02  4.340e+02 -4.350e+02 -4.350e+02\n",
      " -4.400e+02  4.420e+02 -4.420e+02  4.440e+02 -4.480e+02 -4.480e+02\n",
      " -4.550e+02 -4.570e+02  4.570e+02 -4.620e+02  4.640e+02 -4.660e+02\n",
      " -4.670e+02  4.680e+02  4.680e+02  4.690e+02 -4.760e+02 -4.760e+02\n",
      " -4.760e+02 -4.770e+02 -4.780e+02  4.780e+02 -4.810e+02 -4.840e+02\n",
      " -4.920e+02 -4.990e+02  5.000e+02  5.030e+02 -5.050e+02 -5.130e+02\n",
      " -5.150e+02 -5.150e+02 -5.200e+02 -5.260e+02 -5.310e+02 -5.340e+02\n",
      " -5.360e+02 -5.370e+02 -5.400e+02 -5.410e+02 -5.450e+02 -5.470e+02\n",
      "  5.500e+02 -5.510e+02 -5.520e+02 -5.530e+02  5.570e+02  5.610e+02\n",
      " -5.650e+02 -5.680e+02 -5.680e+02 -5.680e+02 -5.730e+02  5.740e+02\n",
      "  5.820e+02  5.860e+02 -5.920e+02  5.930e+02  5.940e+02 -5.950e+02\n",
      "  5.980e+02 -5.990e+02 -6.000e+02 -6.010e+02 -6.030e+02 -6.030e+02\n",
      " -6.050e+02 -6.050e+02 -6.070e+02  6.070e+02 -6.080e+02 -6.090e+02\n",
      " -6.100e+02 -6.100e+02 -6.100e+02 -6.140e+02  6.140e+02 -6.170e+02\n",
      " -6.170e+02  6.240e+02  6.250e+02 -6.260e+02  6.260e+02 -6.260e+02\n",
      "  6.280e+02 -6.290e+02 -6.300e+02 -6.340e+02 -6.510e+02 -6.510e+02\n",
      " -6.520e+02 -6.530e+02  6.530e+02  6.560e+02 -6.570e+02 -6.580e+02\n",
      " -6.580e+02 -6.640e+02 -6.690e+02 -6.700e+02 -6.700e+02 -6.740e+02\n",
      " -6.830e+02 -6.900e+02 -6.910e+02  6.970e+02 -7.010e+02  7.010e+02\n",
      " -7.040e+02 -7.040e+02 -7.050e+02 -7.050e+02 -7.180e+02 -7.190e+02\n",
      "  7.190e+02 -7.240e+02 -7.280e+02 -7.300e+02  7.310e+02  7.370e+02\n",
      " -7.390e+02 -7.400e+02 -7.410e+02 -7.470e+02 -7.610e+02  7.610e+02\n",
      " -7.610e+02 -7.730e+02 -7.750e+02  7.770e+02 -7.910e+02 -7.910e+02\n",
      " -7.910e+02 -7.910e+02  8.000e+02 -8.000e+02 -8.060e+02  8.070e+02\n",
      "  8.080e+02 -8.130e+02 -8.240e+02  8.260e+02 -8.270e+02 -8.290e+02\n",
      " -8.300e+02 -8.390e+02 -8.450e+02 -8.520e+02  8.550e+02 -8.630e+02\n",
      "  8.640e+02  8.690e+02 -8.720e+02  8.790e+02 -8.820e+02 -8.820e+02\n",
      " -8.880e+02 -8.890e+02  8.960e+02 -8.960e+02  8.960e+02 -9.040e+02\n",
      " -9.130e+02  9.220e+02  9.290e+02 -9.380e+02 -9.440e+02 -9.470e+02\n",
      " -9.490e+02  9.490e+02  9.500e+02  9.520e+02  9.760e+02 -9.770e+02\n",
      "  9.870e+02 -9.930e+02  9.950e+02  9.990e+02 -1.013e+03  1.026e+03\n",
      " -1.036e+03 -1.040e+03 -1.060e+03 -1.071e+03 -1.072e+03  1.073e+03\n",
      "  1.081e+03 -1.118e+03 -1.119e+03 -1.125e+03 -1.126e+03 -1.126e+03\n",
      "  1.135e+03  1.147e+03 -1.148e+03 -1.157e+03 -1.159e+03 -1.163e+03\n",
      "  1.167e+03  1.171e+03 -1.178e+03 -1.189e+03 -1.189e+03  1.194e+03\n",
      "  1.197e+03  1.209e+03  1.215e+03 -1.216e+03  1.229e+03 -1.233e+03\n",
      "  1.235e+03 -1.239e+03 -1.246e+03  1.258e+03  1.265e+03  1.268e+03\n",
      " -1.272e+03 -1.280e+03 -1.285e+03 -1.289e+03  1.293e+03 -1.305e+03\n",
      " -1.324e+03 -1.333e+03 -1.351e+03  1.357e+03 -1.367e+03 -1.369e+03\n",
      "  1.379e+03 -1.400e+03  1.421e+03 -1.429e+03 -1.432e+03 -1.474e+03\n",
      " -1.479e+03  1.492e+03  1.498e+03  1.499e+03  1.501e+03  1.516e+03\n",
      " -1.523e+03  1.528e+03  1.531e+03 -1.559e+03  1.600e+03 -1.621e+03\n",
      "  1.622e+03  1.632e+03  1.653e+03 -1.700e+03  1.725e+03 -1.728e+03\n",
      " -1.750e+03  1.798e+03 -1.847e+03 -1.864e+03 -1.893e+03 -1.932e+03\n",
      " -1.965e+03 -1.974e+03 -2.065e+03 -2.067e+03 -2.137e+03 -2.161e+03\n",
      "  2.174e+03 -2.199e+03 -2.224e+03 -2.248e+03 -2.261e+03  2.318e+03\n",
      " -2.360e+03 -2.368e+03  2.393e+03 -2.449e+03 -2.488e+03 -2.515e+03\n",
      " -2.590e+03 -2.595e+03  2.617e+03 -2.676e+03  2.681e+03 -2.696e+03\n",
      " -2.832e+03 -2.973e+03 -3.059e+03 -3.094e+03 -3.261e+03 -3.305e+03\n",
      "  3.361e+03 -3.635e+03 -3.674e+03 -3.940e+03 -4.765e+03 -6.732e+03]\n",
      "durations 11.0 7248.0\n",
      "Concordance Index 0.42637362637362636\n",
      "Integrated Brier Score: 0.2060413750774309\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m269.3496\u001b[0m  0.0026\n",
      "      2      269.3496  0.0020\n",
      "      3      269.3496  0.0019\n",
      "      4      269.3496  0.0018\n",
      "      5      269.3496  0.0030\n",
      "      6      269.3496  0.0053\n",
      "      7      269.3496  0.0052\n",
      "      8      269.3496  0.0022\n",
      "      9      269.3496  0.0019\n",
      "     10      269.3496  0.0018\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m202.6476\u001b[0m  0.0032\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      2      202.6476  0.0021\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      3      202.6476  0.0028\n",
      "      4      202.6476  0.0019\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m269.7728\u001b[0m  0.0035\n",
      "      5      202.6476  0.0018\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m295.9162\u001b[0m  0.0058\n",
      "      2      269.7728  0.0026\n",
      "      6      202.6476  0.0018\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      3      269.7728  0.0019\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      7      202.6476  0.0024\n",
      "      4      269.7728  0.0018\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      2      295.9162  0.0055\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      5      269.7728  0.0018\n",
      "      3      295.9162  0.0019\n",
      "      6      269.7728  0.0017\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m79.5009\u001b[0m  0.0038\n",
      "      4      295.9162  0.0018\n",
      "      8      202.6476  0.0055\n",
      "      5      295.9162  0.0018\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m236.6923\u001b[0m  0.0037\n",
      "      2       79.5009  0.0028\n",
      "      2      236.6923  0.0020\n",
      "      9      202.6476  0.0038\n",
      "      6      295.9162  0.0025\n",
      "      7      269.7728  0.0055\n",
      "      3       79.5009  0.0027\n",
      "      3      236.6923  0.0019\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "     10      202.6476  0.0027\n",
      "Restoring best model from epoch 1.\n",
      "      4      236.6923  0.0018\n",
      "      4       79.5009  0.0029\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      5      236.6923  0.0018\n",
      "      8      269.7728  0.0053\n",
      "      7      295.9162  0.0051\n",
      "      5       79.5009  0.0027\n",
      "      6      236.6923  0.0018\n",
      "      9      269.7728  0.0019\n",
      "      7      236.6923  0.0018\n",
      "      6       79.5009  0.0029\n",
      "     10      269.7728  0.0018\n",
      "      8      295.9162  0.0040\n",
      "Restoring best model from epoch 1.\n",
      "      8      236.6923  0.0019\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m85.7973\u001b[0m  0.0043\n",
      "      7       79.5009  0.0027\n",
      "      9      236.6923  0.0023\n",
      "      9      295.9162  0.0036\n",
      "      2       85.7973  0.0029\n",
      "      8       79.5009  0.0026\n",
      "     10      236.6923  0.0019\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      9       79.5009  0.0027\n",
      "      3       85.7973  0.0028\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "     10      295.9162  0.0038\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "     10       79.5009  0.0026\n",
      "      4       85.7973  0.0027\n",
      "Restoring best model from epoch 1.\n",
      "      5       85.7973  0.0027\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m89.6978\u001b[0m  0.0038\n",
      "      6       85.7973  0.0026\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m106.0212\u001b[0m  0.0056\n",
      "      7       85.7973  0.0028\n",
      "      2       89.6978  0.0030\n",
      "      2      106.0212  0.0030\n",
      "      3       89.6978  0.0027\n",
      "      8       85.7973  0.0028\n",
      "      9       85.7973  0.0027\n",
      "      4       89.6978  0.0028\n",
      "      3      106.0212  0.0046\n",
      "     10       85.7973  0.0027\n",
      "Restoring best model from epoch 1.\n",
      "      5       89.6978  0.0037\n",
      "      4      106.0212  0.0044\n",
      "      6       89.6978  0.0031\n",
      "      5      106.0212  0.0028\n",
      "      7       89.6978  0.0029\n",
      "      8       89.6978  0.0028\n",
      "      6      106.0212  0.0057\n",
      "      7      106.0212  0.0029\n",
      "      9       89.6978  0.0071\n",
      "      8      106.0212  0.0029\n",
      "      9      106.0212  0.0027\n",
      "     10       89.6978  0.0040\n",
      "Restoring best model from epoch 1.\n",
      "     10      106.0212  0.0027\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m88.8809\u001b[0m  0.0038\n",
      "      2       88.8809  0.0028\n",
      "      3       88.8809  0.0027\n",
      "      4       88.8809  0.0026\n",
      "      5       88.8809  0.0026\n",
      "      6       88.8809  0.0026\n",
      "      7       88.8809  0.0026\n",
      "      8       88.8809  0.0026\n",
      "      9       88.8809  0.0026\n",
      "     10       88.8809  0.0026\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m112.7514\u001b[0m  0.0046\n",
      "      2      112.7514  0.0035\n",
      "      3      112.7514  0.0034\n",
      "      4      112.7514  0.0045\n",
      "      5      112.7514  0.0036\n",
      "      6      112.7514  0.0042\n",
      "      7      112.7514  0.0065\n",
      "      8      112.7514  0.0053\n",
      "      9      112.7514  0.0048\n",
      "     10      112.7514  0.0058\n",
      "Restoring best model from epoch 1.\n",
      "y_train breslow final [ 4.000e+00 -1.100e+01 -1.300e+01 -1.400e+01 -1.500e+01  1.800e+01\n",
      "  1.900e+01  2.200e+01 -2.400e+01  3.300e+01 -3.500e+01 -3.600e+01\n",
      "  3.800e+01 -4.400e+01 -4.400e+01 -4.800e+01 -5.000e+01  5.800e+01\n",
      " -6.000e+01  6.200e+01 -7.900e+01 -8.400e+01 -8.400e+01  8.700e+01\n",
      "  9.100e+01  9.700e+01  9.900e+01  1.180e+02  1.190e+02 -1.190e+02\n",
      "  1.210e+02 -1.290e+02 -1.310e+02 -1.330e+02 -1.340e+02  1.390e+02\n",
      " -1.410e+02 -1.510e+02  1.540e+02  1.610e+02 -1.650e+02  1.710e+02\n",
      "  1.730e+02 -1.740e+02  1.790e+02 -1.790e+02 -1.820e+02 -1.860e+02\n",
      " -1.860e+02  1.870e+02  1.890e+02 -2.020e+02  2.100e+02 -2.180e+02\n",
      " -2.240e+02 -2.300e+02  2.370e+02  2.430e+02  2.440e+02  2.440e+02\n",
      "  2.500e+02  2.580e+02  2.600e+02 -2.600e+02 -2.640e+02  2.680e+02\n",
      "  2.740e+02  2.750e+02  2.820e+02 -2.850e+02 -2.870e+02  3.000e+02\n",
      " -3.070e+02  3.070e+02 -3.100e+02  3.210e+02  3.210e+02  3.360e+02\n",
      "  3.360e+02  3.390e+02  3.400e+02  3.430e+02 -3.510e+02 -3.530e+02\n",
      "  3.540e+02 -3.650e+02  3.700e+02 -3.720e+02  3.750e+02  3.760e+02\n",
      " -3.770e+02 -3.850e+02  3.850e+02 -3.850e+02 -4.080e+02  4.090e+02\n",
      " -4.140e+02 -4.150e+02 -4.150e+02 -4.160e+02 -4.180e+02 -4.220e+02\n",
      " -4.230e+02 -4.240e+02 -4.260e+02 -4.260e+02 -4.270e+02  4.290e+02\n",
      "  4.340e+02  4.340e+02 -4.350e+02 -4.350e+02 -4.350e+02 -4.400e+02\n",
      "  4.420e+02 -4.420e+02  4.440e+02 -4.460e+02 -4.480e+02 -4.480e+02\n",
      " -4.570e+02  4.570e+02  4.600e+02 -4.620e+02  4.640e+02 -4.670e+02\n",
      "  4.680e+02  4.690e+02 -4.760e+02 -4.760e+02 -4.760e+02 -4.770e+02\n",
      "  4.780e+02 -4.810e+02 -4.840e+02 -4.860e+02 -4.870e+02  4.880e+02\n",
      " -4.920e+02 -5.050e+02 -5.130e+02 -5.150e+02 -5.150e+02 -5.200e+02\n",
      " -5.220e+02 -5.310e+02 -5.340e+02 -5.360e+02 -5.390e+02 -5.400e+02\n",
      " -5.410e+02 -5.450e+02 -5.460e+02 -5.470e+02 -5.510e+02 -5.520e+02\n",
      " -5.530e+02  5.570e+02  5.610e+02 -5.640e+02 -5.650e+02 -5.670e+02\n",
      " -5.680e+02  5.740e+02 -5.780e+02 -5.780e+02 -5.780e+02  5.820e+02\n",
      " -5.910e+02 -5.920e+02  5.930e+02  5.940e+02 -5.950e+02 -5.960e+02\n",
      "  5.980e+02 -5.990e+02 -6.000e+02 -6.010e+02 -6.030e+02 -6.030e+02\n",
      " -6.030e+02 -6.050e+02 -6.050e+02 -6.070e+02 -6.090e+02 -6.100e+02\n",
      " -6.100e+02 -6.100e+02 -6.140e+02  6.140e+02 -6.170e+02 -6.170e+02\n",
      " -6.240e+02  6.240e+02  6.280e+02 -6.290e+02 -6.300e+02 -6.340e+02\n",
      " -6.510e+02 -6.510e+02 -6.520e+02  6.530e+02 -6.570e+02 -6.580e+02\n",
      " -6.640e+02  6.660e+02 -6.700e+02 -6.700e+02  6.770e+02 -6.770e+02\n",
      " -6.830e+02 -6.890e+02 -6.900e+02 -6.910e+02  6.940e+02  6.970e+02\n",
      " -7.010e+02  7.020e+02 -7.040e+02 -7.050e+02 -7.050e+02  7.110e+02\n",
      " -7.180e+02  7.190e+02 -7.240e+02 -7.280e+02 -7.300e+02  7.310e+02\n",
      "  7.370e+02 -7.390e+02 -7.400e+02  7.600e+02 -7.610e+02 -7.610e+02\n",
      " -7.730e+02 -7.750e+02  7.770e+02 -7.910e+02 -7.910e+02  8.000e+02\n",
      " -8.000e+02 -8.050e+02 -8.060e+02  8.070e+02  8.080e+02 -8.130e+02\n",
      " -8.240e+02 -8.240e+02  8.260e+02 -8.270e+02 -8.290e+02 -8.300e+02\n",
      " -8.390e+02 -8.420e+02 -8.620e+02 -8.640e+02 -8.660e+02  8.690e+02\n",
      "  8.790e+02 -8.820e+02 -8.880e+02 -8.890e+02 -8.890e+02  8.960e+02\n",
      " -8.960e+02 -9.040e+02  9.050e+02 -9.100e+02 -9.120e+02  9.220e+02\n",
      "  9.290e+02 -9.300e+02 -9.470e+02 -9.490e+02  9.490e+02  9.500e+02\n",
      "  9.520e+02  9.760e+02 -9.770e+02  9.870e+02 -9.880e+02 -9.930e+02\n",
      "  9.950e+02  9.950e+02 -9.970e+02  9.990e+02 -1.040e+03  1.043e+03\n",
      "  1.046e+03 -1.071e+03 -1.072e+03  1.073e+03 -1.079e+03  1.081e+03\n",
      " -1.097e+03  1.115e+03 -1.119e+03 -1.125e+03 -1.126e+03 -1.130e+03\n",
      "  1.135e+03  1.147e+03 -1.148e+03 -1.157e+03 -1.159e+03 -1.163e+03\n",
      "  1.167e+03  1.171e+03 -1.175e+03 -1.189e+03 -1.189e+03  1.197e+03\n",
      "  1.209e+03  1.215e+03 -1.216e+03 -1.233e+03  1.235e+03 -1.239e+03\n",
      " -1.246e+03  1.258e+03  1.265e+03  1.268e+03 -1.280e+03 -1.285e+03\n",
      "  1.288e+03  1.293e+03 -1.301e+03 -1.324e+03 -1.333e+03 -1.351e+03\n",
      "  1.357e+03 -1.367e+03 -1.369e+03  1.379e+03 -1.400e+03  1.421e+03\n",
      " -1.429e+03 -1.431e+03 -1.432e+03 -1.442e+03  1.454e+03 -1.474e+03\n",
      " -1.479e+03  1.492e+03  1.498e+03  1.499e+03  1.516e+03  1.528e+03\n",
      "  1.531e+03 -1.559e+03  1.600e+03 -1.617e+03  1.622e+03  1.632e+03\n",
      " -1.683e+03 -1.700e+03  1.725e+03 -1.728e+03 -1.750e+03  1.778e+03\n",
      "  1.790e+03  1.830e+03 -1.864e+03 -1.870e+03 -1.932e+03 -1.965e+03\n",
      " -1.974e+03  2.027e+03 -2.067e+03 -2.109e+03 -2.224e+03 -2.261e+03\n",
      "  2.318e+03 -2.360e+03 -2.368e+03  2.393e+03 -2.449e+03 -2.590e+03\n",
      " -2.616e+03  2.617e+03  2.620e+03 -2.676e+03  2.681e+03 -2.696e+03\n",
      " -2.823e+03 -2.832e+03 -2.973e+03 -3.059e+03 -3.094e+03  3.169e+03\n",
      " -3.261e+03 -3.305e+03  3.361e+03 -3.635e+03 -3.674e+03 -3.759e+03\n",
      " -4.765e+03  4.961e+03 -4.992e+03 -6.732e+03 -7.062e+03 -7.248e+03]\n",
      "Concordance Index 0.5507056550640165\n",
      "Integrated Brier Score: 0.1949739406615247\n",
      "y_train breslow final [ 4.000e+00 -1.100e+01 -1.300e+01 -1.400e+01 -1.500e+01  1.800e+01\n",
      "  1.900e+01  2.200e+01 -2.400e+01  3.300e+01 -3.500e+01 -3.600e+01\n",
      "  3.800e+01 -4.400e+01 -4.400e+01 -4.800e+01 -5.000e+01  5.800e+01\n",
      " -6.000e+01  6.200e+01 -7.900e+01 -8.400e+01 -8.400e+01  8.700e+01\n",
      "  9.100e+01  9.700e+01  9.900e+01  1.180e+02  1.190e+02 -1.190e+02\n",
      "  1.210e+02 -1.290e+02 -1.310e+02 -1.330e+02 -1.340e+02  1.390e+02\n",
      " -1.410e+02 -1.510e+02  1.540e+02  1.610e+02 -1.650e+02  1.710e+02\n",
      "  1.730e+02 -1.740e+02  1.790e+02 -1.790e+02 -1.820e+02 -1.860e+02\n",
      " -1.860e+02  1.870e+02  1.890e+02 -2.020e+02  2.100e+02 -2.180e+02\n",
      " -2.240e+02 -2.300e+02  2.370e+02  2.430e+02  2.440e+02  2.440e+02\n",
      "  2.500e+02  2.580e+02  2.600e+02 -2.600e+02 -2.640e+02  2.680e+02\n",
      "  2.740e+02  2.750e+02  2.820e+02 -2.850e+02 -2.870e+02  3.000e+02\n",
      " -3.070e+02  3.070e+02 -3.100e+02  3.210e+02  3.210e+02  3.360e+02\n",
      "  3.360e+02  3.390e+02  3.400e+02  3.430e+02 -3.510e+02 -3.530e+02\n",
      "  3.540e+02 -3.650e+02  3.700e+02 -3.720e+02  3.750e+02  3.760e+02\n",
      " -3.770e+02 -3.850e+02  3.850e+02 -3.850e+02 -4.080e+02  4.090e+02\n",
      " -4.140e+02 -4.150e+02 -4.150e+02 -4.160e+02 -4.180e+02 -4.220e+02\n",
      " -4.230e+02 -4.240e+02 -4.260e+02 -4.260e+02 -4.270e+02  4.290e+02\n",
      "  4.340e+02  4.340e+02 -4.350e+02 -4.350e+02 -4.350e+02 -4.400e+02\n",
      "  4.420e+02 -4.420e+02  4.440e+02 -4.460e+02 -4.480e+02 -4.480e+02\n",
      " -4.570e+02  4.570e+02  4.600e+02 -4.620e+02  4.640e+02 -4.670e+02\n",
      "  4.680e+02  4.690e+02 -4.760e+02 -4.760e+02 -4.760e+02 -4.770e+02\n",
      "  4.780e+02 -4.810e+02 -4.840e+02 -4.860e+02 -4.870e+02  4.880e+02\n",
      " -4.920e+02 -5.050e+02 -5.130e+02 -5.150e+02 -5.150e+02 -5.200e+02\n",
      " -5.220e+02 -5.310e+02 -5.340e+02 -5.360e+02 -5.390e+02 -5.400e+02\n",
      " -5.410e+02 -5.450e+02 -5.460e+02 -5.470e+02 -5.510e+02 -5.520e+02\n",
      " -5.530e+02  5.570e+02  5.610e+02 -5.640e+02 -5.650e+02 -5.670e+02\n",
      " -5.680e+02  5.740e+02 -5.780e+02 -5.780e+02 -5.780e+02  5.820e+02\n",
      " -5.910e+02 -5.920e+02  5.930e+02  5.940e+02 -5.950e+02 -5.960e+02\n",
      "  5.980e+02 -5.990e+02 -6.000e+02 -6.010e+02 -6.030e+02 -6.030e+02\n",
      " -6.030e+02 -6.050e+02 -6.050e+02 -6.070e+02 -6.090e+02 -6.100e+02\n",
      " -6.100e+02 -6.100e+02 -6.140e+02  6.140e+02 -6.170e+02 -6.170e+02\n",
      " -6.240e+02  6.240e+02  6.280e+02 -6.290e+02 -6.300e+02 -6.340e+02\n",
      " -6.510e+02 -6.510e+02 -6.520e+02  6.530e+02 -6.570e+02 -6.580e+02\n",
      " -6.640e+02  6.660e+02 -6.700e+02 -6.700e+02  6.770e+02 -6.770e+02\n",
      " -6.830e+02 -6.890e+02 -6.900e+02 -6.910e+02  6.940e+02  6.970e+02\n",
      " -7.010e+02  7.020e+02 -7.040e+02 -7.050e+02 -7.050e+02  7.110e+02\n",
      " -7.180e+02  7.190e+02 -7.240e+02 -7.280e+02 -7.300e+02  7.310e+02\n",
      "  7.370e+02 -7.390e+02 -7.400e+02  7.600e+02 -7.610e+02 -7.610e+02\n",
      " -7.730e+02 -7.750e+02  7.770e+02 -7.910e+02 -7.910e+02  8.000e+02\n",
      " -8.000e+02 -8.050e+02 -8.060e+02  8.070e+02  8.080e+02 -8.130e+02\n",
      " -8.240e+02 -8.240e+02  8.260e+02 -8.270e+02 -8.290e+02 -8.300e+02\n",
      " -8.390e+02 -8.420e+02 -8.620e+02 -8.640e+02 -8.660e+02  8.690e+02\n",
      "  8.790e+02 -8.820e+02 -8.880e+02 -8.890e+02 -8.890e+02  8.960e+02\n",
      " -8.960e+02 -9.040e+02  9.050e+02 -9.100e+02 -9.120e+02  9.220e+02\n",
      "  9.290e+02 -9.300e+02 -9.470e+02 -9.490e+02  9.490e+02  9.500e+02\n",
      "  9.520e+02  9.760e+02 -9.770e+02  9.870e+02 -9.880e+02 -9.930e+02\n",
      "  9.950e+02  9.950e+02 -9.970e+02  9.990e+02 -1.040e+03  1.043e+03\n",
      "  1.046e+03 -1.071e+03 -1.072e+03  1.073e+03 -1.079e+03  1.081e+03\n",
      " -1.097e+03  1.115e+03 -1.119e+03 -1.125e+03 -1.126e+03 -1.130e+03\n",
      "  1.135e+03  1.147e+03 -1.148e+03 -1.157e+03 -1.159e+03 -1.163e+03\n",
      "  1.167e+03  1.171e+03 -1.175e+03 -1.189e+03 -1.189e+03  1.197e+03\n",
      "  1.209e+03  1.215e+03 -1.216e+03 -1.233e+03  1.235e+03 -1.239e+03\n",
      " -1.246e+03  1.258e+03  1.265e+03  1.268e+03 -1.280e+03 -1.285e+03\n",
      "  1.288e+03  1.293e+03 -1.301e+03 -1.324e+03 -1.333e+03 -1.351e+03\n",
      "  1.357e+03 -1.367e+03 -1.369e+03  1.379e+03 -1.400e+03  1.421e+03\n",
      " -1.429e+03 -1.431e+03 -1.432e+03 -1.442e+03  1.454e+03 -1.474e+03\n",
      " -1.479e+03  1.492e+03  1.498e+03  1.499e+03  1.516e+03  1.528e+03\n",
      "  1.531e+03 -1.559e+03  1.600e+03 -1.617e+03  1.622e+03  1.632e+03\n",
      " -1.683e+03 -1.700e+03  1.725e+03 -1.728e+03 -1.750e+03  1.778e+03\n",
      "  1.790e+03  1.830e+03 -1.864e+03 -1.870e+03 -1.932e+03 -1.965e+03\n",
      " -1.974e+03  2.027e+03 -2.067e+03 -2.109e+03 -2.224e+03 -2.261e+03\n",
      "  2.318e+03 -2.360e+03 -2.368e+03  2.393e+03 -2.449e+03 -2.590e+03\n",
      " -2.616e+03  2.617e+03  2.620e+03 -2.676e+03  2.681e+03 -2.696e+03\n",
      " -2.823e+03 -2.832e+03 -2.973e+03 -3.059e+03 -3.094e+03  3.169e+03\n",
      " -3.261e+03 -3.305e+03  3.361e+03 -3.635e+03 -3.674e+03 -3.759e+03\n",
      " -4.765e+03  4.961e+03 -4.992e+03 -6.732e+03 -7.062e+03 -7.248e+03]\n",
      "durations 28.0 3940.0\n",
      "Concordance Index 0.5995370370370371\n",
      "Integrated Brier Score: 0.21360030064729016\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m145.6896\u001b[0m  0.0021\n",
      "      2      145.6896  0.0018\n",
      "      3      145.6896  0.0017\n",
      "      4      145.6896  0.0018\n",
      "      5      145.6896  0.0018\n",
      "      6      145.6896  0.0018\n",
      "      7      145.6896  0.0017\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      8      145.6896  0.0017\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      9      145.6896  0.0017\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "     10      145.6896  0.0016\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m54.9228\u001b[0m  0.0031\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m291.3726\u001b[0m  0.0022\n",
      "      2       54.9228  0.0038\n",
      "      2      291.3726  0.0019\n",
      "      3       54.9228  0.0027\n",
      "      3      291.3726  0.0018\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      4      291.3726  0.0018\n",
      "      4       54.9228  0.0026\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m240.9685\u001b[0m  0.0022\n",
      "      5      291.3726  0.0041\n",
      "      5       54.9228  0.0040\n",
      "      2      240.9685  0.0021\n",
      "      6      291.3726  0.0019\n",
      "      3      240.9685  0.0018\n",
      "      6       54.9228  0.0027\n",
      "      7      291.3726  0.0018\n",
      "      4      240.9685  0.0017\n",
      "      7       54.9228  0.0026\n",
      "      5      240.9685  0.0017\n",
      "      8      291.3726  0.0018\n",
      "      9      291.3726  0.0019\n",
      "      8       54.9228  0.0026\n",
      "      6      240.9685  0.0039\n",
      "     10      291.3726  0.0018\n",
      "Restoring best model from epoch 1.\n",
      "      7      240.9685  0.0020\n",
      "      8      240.9685  0.0018\n",
      "      9       54.9228  0.0060\n",
      "      9      240.9685  0.0018\n",
      "     10      240.9685  0.0017\n",
      "Restoring best model from epoch 1.\n",
      "     10       54.9228  0.0058\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m80.6819\u001b[0m  0.0038\n",
      "      2       80.6819  0.0029\n",
      "      3       80.6819  0.0027\n",
      "      4       80.6819  0.0071\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      5       80.6819  0.0028\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      6       80.6819  0.0036\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      7       80.6819  0.0041\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m64.8211\u001b[0m  0.0053\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m98.3683\u001b[0m  0.0032\n",
      "      8       80.6819  0.0027\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      2       98.3683  0.0030\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      9       80.6819  0.0038\n",
      "      2       64.8211  0.0071\n",
      "      3       98.3683  0.0051\n",
      "      3       64.8211  0.0032\n",
      "     10       80.6819  0.0051\n",
      "Restoring best model from epoch 1.\n",
      "      4       98.3683  0.0035\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m123.0941\u001b[0m  0.0073\n",
      "      4       64.8211  0.0043\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      5       98.3683  0.0043\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2      123.0941  0.0020\n",
      "      3      123.0941  0.0017\n",
      "      5       64.8211  0.0035\n",
      "      6       98.3683  0.0040\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m100.0883\u001b[0m  0.0031\n",
      "      4      123.0941  0.0024\n",
      "      6       64.8211  0.0027\n",
      "      2      100.0883  0.0028\n",
      "      7       64.8211  0.0026\n",
      "      7       98.3683  0.0040\n",
      "      5      123.0941  0.0045\n",
      "      8       64.8211  0.0027\n",
      "      3      100.0883  0.0036\n",
      "      6      123.0941  0.0021\n",
      "      8       98.3683  0.0042\n",
      "      9       64.8211  0.0026\n",
      "      9       98.3683  0.0030\n",
      "      4      100.0883  0.0046\n",
      "      7      123.0941  0.0054\n",
      "     10       64.8211  0.0046\n",
      "     10       98.3683  0.0032\n",
      "Restoring best model from epoch 1.\n",
      "      8      123.0941  0.0017\n",
      "Restoring best model from epoch 1.\n",
      "      5      100.0883  0.0035\n",
      "      9      123.0941  0.0016\n",
      "     10      123.0941  0.0016\n",
      "      6      100.0883  0.0030\n",
      "Restoring best model from epoch 1.\n",
      "      7      100.0883  0.0031\n",
      "      8      100.0883  0.0032\n",
      "      9      100.0883  0.0028\n",
      "     10      100.0883  0.0031\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m314.5165\u001b[0m  0.0067\n",
      "      2      314.5165  0.0027\n",
      "      3      314.5165  0.0020\n",
      "      4      314.5165  0.0018\n",
      "      5      314.5165  0.0018\n",
      "      6      314.5165  0.0018\n",
      "      7      314.5165  0.0018\n",
      "      8      314.5165  0.0017\n",
      "      9      314.5165  0.0017\n",
      "     10      314.5165  0.0017\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m115.1936\u001b[0m  0.0036\n",
      "      2      115.1936  0.0037\n",
      "      3      115.1936  0.0039\n",
      "      4      115.1936  0.0044\n",
      "      5      115.1936  0.0039\n",
      "      6      115.1936  0.0044\n",
      "      7      115.1936  0.0069\n",
      "      8      115.1936  0.0060\n",
      "      9      115.1936  0.0060\n",
      "     10      115.1936  0.0039\n",
      "Restoring best model from epoch 1.\n",
      "y_train breslow final [ 4.000e+00 -1.100e+01 -1.300e+01 -1.400e+01 -1.500e+01  1.800e+01\n",
      "  2.200e+01 -2.400e+01 -2.800e+01 -3.500e+01 -3.600e+01  3.800e+01\n",
      " -4.400e+01 -4.400e+01 -4.800e+01 -5.000e+01  5.800e+01 -6.000e+01\n",
      " -6.200e+01  6.200e+01  7.400e+01 -7.900e+01 -8.400e+01 -8.400e+01\n",
      "  8.700e+01  9.100e+01  9.900e+01  1.160e+02  1.180e+02  1.190e+02\n",
      " -1.190e+02  1.210e+02  1.240e+02 -1.290e+02 -1.310e+02 -1.330e+02\n",
      "  1.390e+02 -1.390e+02 -1.510e+02  1.540e+02  1.640e+02 -1.650e+02\n",
      "  1.670e+02  1.710e+02  1.730e+02 -1.740e+02  1.760e+02  1.790e+02\n",
      " -1.790e+02 -1.840e+02 -1.860e+02 -1.860e+02  1.870e+02  1.930e+02\n",
      " -2.020e+02 -2.180e+02 -2.240e+02 -2.250e+02 -2.300e+02  2.370e+02\n",
      "  2.430e+02  2.440e+02  2.440e+02  2.500e+02 -2.560e+02  2.570e+02\n",
      "  2.580e+02  2.600e+02 -2.600e+02  2.680e+02  2.740e+02  2.750e+02\n",
      "  2.810e+02  2.820e+02 -2.850e+02 -2.870e+02  2.910e+02  3.000e+02\n",
      "  3.030e+02  3.070e+02  3.080e+02 -3.100e+02  3.210e+02 -3.230e+02\n",
      "  3.360e+02  3.390e+02  3.400e+02 -3.530e+02 -3.540e+02 -3.650e+02\n",
      "  3.700e+02 -3.720e+02  3.760e+02 -3.770e+02 -3.850e+02 -4.080e+02\n",
      "  4.090e+02 -4.090e+02 -4.110e+02  4.140e+02 -4.140e+02 -4.150e+02\n",
      " -4.160e+02 -4.170e+02 -4.220e+02 -4.230e+02 -4.240e+02 -4.260e+02\n",
      " -4.260e+02 -4.270e+02  4.280e+02  4.290e+02  4.340e+02  4.340e+02\n",
      " -4.350e+02 -4.350e+02 -4.400e+02 -4.420e+02  4.440e+02  4.440e+02\n",
      " -4.460e+02 -4.480e+02 -4.480e+02 -4.550e+02 -4.570e+02  4.570e+02\n",
      "  4.600e+02 -4.620e+02  4.640e+02 -4.660e+02 -4.670e+02  4.680e+02\n",
      "  4.680e+02  4.690e+02 -4.760e+02 -4.770e+02 -4.780e+02  4.780e+02\n",
      " -4.840e+02 -4.860e+02 -4.870e+02  4.880e+02 -4.920e+02 -4.990e+02\n",
      "  5.000e+02  5.030e+02 -5.050e+02 -5.130e+02 -5.150e+02 -5.150e+02\n",
      " -5.220e+02 -5.260e+02 -5.310e+02 -5.340e+02 -5.370e+02 -5.390e+02\n",
      " -5.450e+02 -5.460e+02 -5.470e+02  5.500e+02 -5.510e+02 -5.520e+02\n",
      "  5.570e+02 -5.640e+02 -5.670e+02 -5.680e+02 -5.680e+02 -5.680e+02\n",
      " -5.730e+02 -5.780e+02 -5.780e+02 -5.780e+02  5.820e+02  5.860e+02\n",
      " -5.910e+02 -5.920e+02  5.930e+02  5.940e+02 -5.950e+02 -5.960e+02\n",
      "  5.980e+02 -5.990e+02 -6.010e+02 -6.030e+02 -6.030e+02 -6.050e+02\n",
      " -6.050e+02 -6.070e+02  6.070e+02 -6.080e+02 -6.100e+02 -6.100e+02\n",
      " -6.140e+02  6.140e+02 -6.170e+02 -6.240e+02  6.240e+02  6.250e+02\n",
      " -6.260e+02  6.260e+02 -6.260e+02  6.280e+02 -6.300e+02 -6.340e+02\n",
      " -6.510e+02 -6.510e+02 -6.520e+02 -6.520e+02 -6.530e+02  6.530e+02\n",
      "  6.560e+02 -6.570e+02 -6.580e+02 -6.580e+02 -6.640e+02  6.660e+02\n",
      " -6.690e+02 -6.700e+02 -6.740e+02  6.770e+02 -6.770e+02 -6.830e+02\n",
      " -6.890e+02 -6.910e+02  6.940e+02  6.970e+02  7.010e+02  7.020e+02\n",
      " -7.040e+02 -7.050e+02 -7.050e+02  7.110e+02 -7.180e+02 -7.190e+02\n",
      " -7.280e+02 -7.300e+02  7.370e+02 -7.390e+02 -7.400e+02 -7.410e+02\n",
      " -7.470e+02  7.600e+02 -7.610e+02  7.610e+02 -7.610e+02 -7.730e+02\n",
      " -7.910e+02 -7.910e+02 -7.910e+02 -7.910e+02  8.000e+02 -8.000e+02\n",
      " -8.050e+02 -8.060e+02  8.070e+02  8.080e+02 -8.130e+02 -8.240e+02\n",
      " -8.240e+02  8.260e+02 -8.270e+02 -8.290e+02 -8.300e+02 -8.420e+02\n",
      " -8.450e+02 -8.520e+02  8.550e+02 -8.620e+02 -8.630e+02 -8.640e+02\n",
      "  8.640e+02 -8.660e+02  8.690e+02 -8.720e+02 -8.820e+02 -8.880e+02\n",
      " -8.890e+02 -8.890e+02 -8.960e+02  8.960e+02  9.050e+02 -9.100e+02\n",
      " -9.120e+02 -9.130e+02  9.220e+02  9.290e+02 -9.300e+02 -9.380e+02\n",
      " -9.440e+02 -9.470e+02 -9.490e+02  9.500e+02  9.760e+02  9.870e+02\n",
      " -9.880e+02 -9.930e+02  9.950e+02  9.950e+02 -9.970e+02  9.990e+02\n",
      " -1.013e+03  1.026e+03 -1.036e+03  1.043e+03  1.046e+03 -1.060e+03\n",
      " -1.071e+03 -1.072e+03 -1.079e+03 -1.097e+03  1.115e+03 -1.118e+03\n",
      " -1.125e+03 -1.126e+03 -1.126e+03 -1.130e+03  1.135e+03  1.147e+03\n",
      " -1.148e+03  1.167e+03 -1.175e+03 -1.178e+03 -1.189e+03  1.194e+03\n",
      "  1.197e+03  1.209e+03  1.229e+03 -1.239e+03  1.265e+03  1.268e+03\n",
      " -1.272e+03 -1.285e+03  1.288e+03 -1.289e+03  1.293e+03 -1.301e+03\n",
      " -1.305e+03 -1.333e+03 -1.351e+03  1.379e+03  1.421e+03 -1.429e+03\n",
      " -1.431e+03 -1.442e+03  1.454e+03 -1.474e+03 -1.479e+03  1.492e+03\n",
      "  1.498e+03  1.501e+03  1.516e+03 -1.523e+03  1.600e+03 -1.617e+03\n",
      " -1.621e+03  1.622e+03  1.653e+03 -1.683e+03 -1.750e+03  1.778e+03\n",
      "  1.790e+03  1.798e+03  1.830e+03 -1.847e+03 -1.864e+03 -1.870e+03\n",
      " -1.893e+03 -1.965e+03  2.027e+03 -2.065e+03 -2.067e+03 -2.109e+03\n",
      " -2.137e+03 -2.161e+03  2.174e+03 -2.199e+03 -2.248e+03 -2.360e+03\n",
      "  2.393e+03 -2.449e+03 -2.488e+03 -2.515e+03 -2.590e+03 -2.595e+03\n",
      " -2.616e+03  2.617e+03  2.620e+03 -2.676e+03  2.681e+03 -2.823e+03\n",
      " -2.832e+03 -3.059e+03  3.169e+03 -3.635e+03 -3.674e+03 -3.759e+03\n",
      " -3.940e+03 -4.765e+03  4.961e+03 -4.992e+03 -7.062e+03 -7.248e+03]\n",
      "Concordance Index 0.42617073717230564\n",
      "Integrated Brier Score: 0.19247649330686062\n",
      "y_train breslow final [ 4.000e+00 -1.100e+01 -1.300e+01 -1.400e+01 -1.500e+01  1.800e+01\n",
      "  2.200e+01 -2.400e+01 -2.800e+01 -3.500e+01 -3.600e+01  3.800e+01\n",
      " -4.400e+01 -4.400e+01 -4.800e+01 -5.000e+01  5.800e+01 -6.000e+01\n",
      " -6.200e+01  6.200e+01  7.400e+01 -7.900e+01 -8.400e+01 -8.400e+01\n",
      "  8.700e+01  9.100e+01  9.900e+01  1.160e+02  1.180e+02  1.190e+02\n",
      " -1.190e+02  1.210e+02  1.240e+02 -1.290e+02 -1.310e+02 -1.330e+02\n",
      "  1.390e+02 -1.390e+02 -1.510e+02  1.540e+02  1.640e+02 -1.650e+02\n",
      "  1.670e+02  1.710e+02  1.730e+02 -1.740e+02  1.760e+02  1.790e+02\n",
      " -1.790e+02 -1.840e+02 -1.860e+02 -1.860e+02  1.870e+02  1.930e+02\n",
      " -2.020e+02 -2.180e+02 -2.240e+02 -2.250e+02 -2.300e+02  2.370e+02\n",
      "  2.430e+02  2.440e+02  2.440e+02  2.500e+02 -2.560e+02  2.570e+02\n",
      "  2.580e+02  2.600e+02 -2.600e+02  2.680e+02  2.740e+02  2.750e+02\n",
      "  2.810e+02  2.820e+02 -2.850e+02 -2.870e+02  2.910e+02  3.000e+02\n",
      "  3.030e+02  3.070e+02  3.080e+02 -3.100e+02  3.210e+02 -3.230e+02\n",
      "  3.360e+02  3.390e+02  3.400e+02 -3.530e+02 -3.540e+02 -3.650e+02\n",
      "  3.700e+02 -3.720e+02  3.760e+02 -3.770e+02 -3.850e+02 -4.080e+02\n",
      "  4.090e+02 -4.090e+02 -4.110e+02  4.140e+02 -4.140e+02 -4.150e+02\n",
      " -4.160e+02 -4.170e+02 -4.220e+02 -4.230e+02 -4.240e+02 -4.260e+02\n",
      " -4.260e+02 -4.270e+02  4.280e+02  4.290e+02  4.340e+02  4.340e+02\n",
      " -4.350e+02 -4.350e+02 -4.400e+02 -4.420e+02  4.440e+02  4.440e+02\n",
      " -4.460e+02 -4.480e+02 -4.480e+02 -4.550e+02 -4.570e+02  4.570e+02\n",
      "  4.600e+02 -4.620e+02  4.640e+02 -4.660e+02 -4.670e+02  4.680e+02\n",
      "  4.680e+02  4.690e+02 -4.760e+02 -4.770e+02 -4.780e+02  4.780e+02\n",
      " -4.840e+02 -4.860e+02 -4.870e+02  4.880e+02 -4.920e+02 -4.990e+02\n",
      "  5.000e+02  5.030e+02 -5.050e+02 -5.130e+02 -5.150e+02 -5.150e+02\n",
      " -5.220e+02 -5.260e+02 -5.310e+02 -5.340e+02 -5.370e+02 -5.390e+02\n",
      " -5.450e+02 -5.460e+02 -5.470e+02  5.500e+02 -5.510e+02 -5.520e+02\n",
      "  5.570e+02 -5.640e+02 -5.670e+02 -5.680e+02 -5.680e+02 -5.680e+02\n",
      " -5.730e+02 -5.780e+02 -5.780e+02 -5.780e+02  5.820e+02  5.860e+02\n",
      " -5.910e+02 -5.920e+02  5.930e+02  5.940e+02 -5.950e+02 -5.960e+02\n",
      "  5.980e+02 -5.990e+02 -6.010e+02 -6.030e+02 -6.030e+02 -6.050e+02\n",
      " -6.050e+02 -6.070e+02  6.070e+02 -6.080e+02 -6.100e+02 -6.100e+02\n",
      " -6.140e+02  6.140e+02 -6.170e+02 -6.240e+02  6.240e+02  6.250e+02\n",
      " -6.260e+02  6.260e+02 -6.260e+02  6.280e+02 -6.300e+02 -6.340e+02\n",
      " -6.510e+02 -6.510e+02 -6.520e+02 -6.520e+02 -6.530e+02  6.530e+02\n",
      "  6.560e+02 -6.570e+02 -6.580e+02 -6.580e+02 -6.640e+02  6.660e+02\n",
      " -6.690e+02 -6.700e+02 -6.740e+02  6.770e+02 -6.770e+02 -6.830e+02\n",
      " -6.890e+02 -6.910e+02  6.940e+02  6.970e+02  7.010e+02  7.020e+02\n",
      " -7.040e+02 -7.050e+02 -7.050e+02  7.110e+02 -7.180e+02 -7.190e+02\n",
      " -7.280e+02 -7.300e+02  7.370e+02 -7.390e+02 -7.400e+02 -7.410e+02\n",
      " -7.470e+02  7.600e+02 -7.610e+02  7.610e+02 -7.610e+02 -7.730e+02\n",
      " -7.910e+02 -7.910e+02 -7.910e+02 -7.910e+02  8.000e+02 -8.000e+02\n",
      " -8.050e+02 -8.060e+02  8.070e+02  8.080e+02 -8.130e+02 -8.240e+02\n",
      " -8.240e+02  8.260e+02 -8.270e+02 -8.290e+02 -8.300e+02 -8.420e+02\n",
      " -8.450e+02 -8.520e+02  8.550e+02 -8.620e+02 -8.630e+02 -8.640e+02\n",
      "  8.640e+02 -8.660e+02  8.690e+02 -8.720e+02 -8.820e+02 -8.880e+02\n",
      " -8.890e+02 -8.890e+02 -8.960e+02  8.960e+02  9.050e+02 -9.100e+02\n",
      " -9.120e+02 -9.130e+02  9.220e+02  9.290e+02 -9.300e+02 -9.380e+02\n",
      " -9.440e+02 -9.470e+02 -9.490e+02  9.500e+02  9.760e+02  9.870e+02\n",
      " -9.880e+02 -9.930e+02  9.950e+02  9.950e+02 -9.970e+02  9.990e+02\n",
      " -1.013e+03  1.026e+03 -1.036e+03  1.043e+03  1.046e+03 -1.060e+03\n",
      " -1.071e+03 -1.072e+03 -1.079e+03 -1.097e+03  1.115e+03 -1.118e+03\n",
      " -1.125e+03 -1.126e+03 -1.126e+03 -1.130e+03  1.135e+03  1.147e+03\n",
      " -1.148e+03  1.167e+03 -1.175e+03 -1.178e+03 -1.189e+03  1.194e+03\n",
      "  1.197e+03  1.209e+03  1.229e+03 -1.239e+03  1.265e+03  1.268e+03\n",
      " -1.272e+03 -1.285e+03  1.288e+03 -1.289e+03  1.293e+03 -1.301e+03\n",
      " -1.305e+03 -1.333e+03 -1.351e+03  1.379e+03  1.421e+03 -1.429e+03\n",
      " -1.431e+03 -1.442e+03  1.454e+03 -1.474e+03 -1.479e+03  1.492e+03\n",
      "  1.498e+03  1.501e+03  1.516e+03 -1.523e+03  1.600e+03 -1.617e+03\n",
      " -1.621e+03  1.622e+03  1.653e+03 -1.683e+03 -1.750e+03  1.778e+03\n",
      "  1.790e+03  1.798e+03  1.830e+03 -1.847e+03 -1.864e+03 -1.870e+03\n",
      " -1.893e+03 -1.965e+03  2.027e+03 -2.065e+03 -2.067e+03 -2.109e+03\n",
      " -2.137e+03 -2.161e+03  2.174e+03 -2.199e+03 -2.248e+03 -2.360e+03\n",
      "  2.393e+03 -2.449e+03 -2.488e+03 -2.515e+03 -2.590e+03 -2.595e+03\n",
      " -2.616e+03  2.617e+03  2.620e+03 -2.676e+03  2.681e+03 -2.823e+03\n",
      " -2.832e+03 -3.059e+03  3.169e+03 -3.635e+03 -3.674e+03 -3.759e+03\n",
      " -3.940e+03 -4.765e+03  4.961e+03 -4.992e+03 -7.062e+03 -7.248e+03]\n",
      "durations 19.0 6732.0\n",
      "Concordance Index 0.4679226069246436\n",
      "Integrated Brier Score: 0.18435247712338215\n",
      "split gex_?|100130426      float32\n",
      "gex_?|100133144      float32\n",
      "gex_?|100134869      float32\n",
      "gex_?|10357          float32\n",
      "gex_?|10431          float32\n",
      "                      ...   \n",
      "gex_ZYG11A|440590    float32\n",
      "gex_ZYG11B|79699     float32\n",
      "gex_ZYX|7791         float32\n",
      "gex_ZZEF1|23140      float32\n",
      "gex_ZZZ3|26009       float32\n",
      "Length: 20531, dtype: object\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m278.0762\u001b[0m  0.0027\n",
      "      2      278.0762  0.0020\n",
      "      3      278.0762  0.0018\n",
      "      4      278.0762  0.0018\n",
      "      5      278.0762  0.0024\n",
      "      6      278.0762  0.0018\n",
      "      7      278.0762  0.0066\n",
      "      8      278.0762  0.0032\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m265.8808\u001b[0m  0.0029\n",
      "      9      278.0762  0.0065\n",
      "      2      265.8808  0.0020\n",
      "     10      278.0762  0.0018\n",
      "Restoring best model from epoch 1.\n",
      "      3      265.8808  0.0018\n",
      "      4      265.8808  0.0031\n",
      "      5      265.8808  0.0028\n",
      "      6      265.8808  0.0018\n",
      "      7      265.8808  0.0018\n",
      "      8      265.8808  0.0018\n",
      "      9      265.8808  0.0018\n",
      "     10      265.8808  0.0020\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m470.2995\u001b[0m  0.0031\n",
      "      2      470.2995  0.0019\n",
      "      3      470.2995  0.0019\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m249.9177\u001b[0m  0.0032  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "\n",
      "      1       \u001b[36m93.5572\u001b[0m  0.0041\n",
      "      2      249.9177  0.0027\n",
      "      4      470.2995  0.0034\n",
      "      2       93.5572  0.0042\n",
      "      5      470.2995  0.0026\n",
      "      3      249.9177  0.0042\n",
      "      6      470.2995  0.0020\n",
      "      3       93.5572  0.0038\n",
      "      4      249.9177  0.0020\n",
      "      7      470.2995  0.0020\n",
      "      5      249.9177  0.0019\n",
      "      4       93.5572  0.0032\n",
      "      6      249.9177  0.0018\n",
      "      8      470.2995  0.0031\n",
      "      5       93.5572  0.0028\n",
      "      7      249.9177  0.0018\n",
      "      9      470.2995  0.0020\n",
      "      8      249.9177  0.0017\n",
      "      6       93.5572  0.0027\n",
      "     10      470.2995  0.0019\n",
      "Restoring best model from epoch 1.\n",
      "      9      249.9177  0.0017\n",
      "      7       93.5572  0.0027\n",
      "      8       93.5572  0.0028\n",
      "     10      249.9177  0.0051\n",
      "Restoring best model from epoch 1.\n",
      "      9       93.5572  0.0029\n",
      "     10       93.5572  0.0028\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m492.6743\u001b[0m  0.0028\n",
      "      2      492.6743  0.0019\n",
      "      3      492.6743  0.0018\n",
      "      4      492.6743  0.0017\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      5      492.6743  0.0017\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      6      492.6743  0.0024\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      7      492.6743  0.0050\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m97.7922\u001b[0m  0.0053\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      8      492.6743  0.0039\n",
      "      2       97.7922  0.0029\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      3       97.7922  0.0027\n",
      "      9      492.6743  0.0033\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m109.6825\u001b[0m  0.0070\n",
      "     10      492.6743  0.0018\n",
      "      4       97.7922  0.0026\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m105.7274\u001b[0m  0.0069\n",
      "Restoring best model from epoch 1.\n",
      "      2      109.6825  0.0028\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m108.6187\u001b[0m  0.0040\n",
      "      5       97.7922  0.0026\n",
      "      3      109.6825  0.0027\n",
      "      2      105.7274  0.0029\n",
      "      2      108.6187  0.0028\n",
      "      6       97.7922  0.0030\n",
      "      3      105.7274  0.0027\n",
      "      4      109.6825  0.0027\n",
      "      3      108.6187  0.0028\n",
      "      4      105.7274  0.0027\n",
      "      7       97.7922  0.0044\n",
      "      4      108.6187  0.0028\n",
      "      5      105.7274  0.0027\n",
      "      5      108.6187  0.0027\n",
      "      8       97.7922  0.0029\n",
      "      5      109.6825  0.0073\n",
      "      6      105.7274  0.0027\n",
      "      6      108.6187  0.0027\n",
      "      9       97.7922  0.0027\n",
      "      7      105.7274  0.0027\n",
      "     10       97.7922  0.0026\n",
      "      6      109.6825  0.0040\n",
      "      7      108.6187  0.0027\n",
      "Restoring best model from epoch 1.\n",
      "      8      105.7274  0.0026\n",
      "      8      108.6187  0.0027\n",
      "      7      109.6825  0.0027\n",
      "      9      105.7274  0.0027\n",
      "      9      108.6187  0.0027\n",
      "      8      109.6825  0.0027\n",
      "     10      105.7274  0.0027\n",
      "Restoring best model from epoch 1.\n",
      "      9      109.6825  0.0026\n",
      "     10      108.6187  0.0027\n",
      "Restoring best model from epoch 1.\n",
      "     10      109.6825  0.0026\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m151.2519\u001b[0m  0.0034\n",
      "      2      151.2519  0.0041\n",
      "      3      151.2519  0.0037\n",
      "      4      151.2519  0.0036\n",
      "      5      151.2519  0.0035\n",
      "      6      151.2519  0.0035\n",
      "      7      151.2519  0.0056\n",
      "      8      151.2519  0.0049\n",
      "      9      151.2519  0.0053\n",
      "     10      151.2519  0.0044\n",
      "Restoring best model from epoch 1.\n",
      "y_train breslow final [ 2.000e+00  3.000e+00 -4.000e+00  5.000e+00  5.000e+00  6.000e+00\n",
      " -8.000e+00  9.000e+00 -1.200e+01  1.200e+01 -1.200e+01 -1.300e+01\n",
      " -1.300e+01 -1.500e+01  1.700e+01  2.300e+01  2.400e+01 -2.800e+01\n",
      " -3.000e+01 -3.000e+01 -3.000e+01 -3.100e+01 -3.100e+01  3.400e+01\n",
      " -3.500e+01 -4.200e+01  4.700e+01  5.300e+01  5.900e+01 -6.000e+01\n",
      " -6.100e+01 -6.100e+01 -6.100e+01 -6.100e+01 -6.100e+01 -6.100e+01\n",
      " -6.100e+01 -7.000e+01  8.000e+01 -8.300e+01  8.400e+01  8.500e+01\n",
      "  8.800e+01 -8.900e+01  8.900e+01  9.200e+01  9.400e+01 -9.700e+01\n",
      " -1.030e+02 -1.050e+02 -1.060e+02  1.090e+02 -1.110e+02 -1.110e+02\n",
      "  1.160e+02 -1.210e+02 -1.220e+02 -1.220e+02 -1.220e+02 -1.220e+02\n",
      "  1.230e+02 -1.280e+02  1.310e+02  1.360e+02  1.380e+02  1.430e+02\n",
      "  1.460e+02 -1.500e+02  1.510e+02 -1.530e+02  1.530e+02 -1.600e+02\n",
      "  1.610e+02  1.660e+02  1.660e+02 -1.730e+02  1.800e+02  1.880e+02\n",
      "  1.950e+02  1.980e+02  1.980e+02 -2.020e+02 -2.070e+02 -2.110e+02\n",
      "  2.110e+02  2.120e+02  2.150e+02  2.230e+02 -2.270e+02 -2.360e+02\n",
      " -2.380e+02 -2.420e+02 -2.440e+02  2.660e+02  2.740e+02 -2.890e+02\n",
      "  2.910e+02  2.940e+02  2.990e+02  3.020e+02  3.070e+02  3.110e+02\n",
      "  3.150e+02  3.220e+02 -3.240e+02  3.290e+02  3.400e+02  3.400e+02\n",
      "  3.450e+02 -3.530e+02 -3.580e+02 -3.580e+02  3.600e+02  3.650e+02\n",
      "  3.660e+02 -3.660e+02 -3.670e+02 -3.700e+02  3.710e+02  3.720e+02\n",
      " -3.770e+02 -3.780e+02  3.830e+02  3.830e+02  3.970e+02  3.990e+02\n",
      " -4.010e+02  4.020e+02 -4.050e+02 -4.060e+02 -4.070e+02  4.080e+02\n",
      "  4.120e+02  4.180e+02 -4.200e+02 -4.230e+02 -4.280e+02  4.290e+02\n",
      " -4.300e+02 -4.320e+02 -4.400e+02  4.420e+02  4.450e+02 -4.480e+02\n",
      "  4.480e+02  4.540e+02  4.550e+02  4.560e+02 -4.810e+02 -4.820e+02\n",
      "  4.900e+02 -4.920e+02 -4.980e+02  5.010e+02 -5.100e+02 -5.100e+02\n",
      " -5.110e+02  5.150e+02  5.160e+02 -5.170e+02  5.390e+02  5.430e+02\n",
      "  5.520e+02 -5.550e+02 -5.570e+02  5.590e+02 -5.590e+02  5.620e+02\n",
      " -5.650e+02 -5.700e+02 -5.760e+02 -5.780e+02 -5.780e+02 -5.790e+02\n",
      " -5.810e+02 -5.850e+02 -5.890e+02 -5.960e+02 -6.000e+02 -6.010e+02\n",
      "  6.010e+02  6.040e+02 -6.080e+02 -6.150e+02 -6.180e+02  6.230e+02\n",
      " -6.300e+02  6.320e+02 -6.370e+02 -6.400e+02 -6.410e+02 -6.420e+02\n",
      "  6.450e+02 -6.600e+02 -6.660e+02  6.690e+02  6.780e+02  6.840e+02\n",
      "  6.870e+02 -6.880e+02 -6.990e+02 -7.000e+02  7.080e+02  7.160e+02\n",
      " -7.170e+02  7.340e+02 -7.340e+02  7.400e+02 -7.410e+02 -7.570e+02\n",
      " -7.580e+02 -7.620e+02 -7.650e+02 -7.690e+02  7.700e+02 -7.900e+02\n",
      "  8.030e+02 -8.040e+02 -8.040e+02 -8.160e+02 -8.180e+02 -8.220e+02\n",
      "  8.220e+02  8.260e+02 -8.260e+02 -8.330e+02  8.350e+02  8.400e+02\n",
      " -8.420e+02 -8.490e+02 -8.670e+02  8.810e+02 -9.080e+02 -9.100e+02\n",
      " -9.110e+02 -9.110e+02  9.160e+02  9.210e+02  9.270e+02  9.330e+02\n",
      "  9.370e+02 -9.370e+02 -9.420e+02  9.510e+02 -9.520e+02 -9.600e+02\n",
      "  9.650e+02  9.730e+02  9.740e+02 -9.830e+02 -9.950e+02  1.000e+03\n",
      "  1.001e+03  1.006e+03 -1.007e+03 -1.032e+03  1.045e+03 -1.050e+03\n",
      "  1.057e+03 -1.058e+03 -1.072e+03  1.075e+03 -1.092e+03  1.097e+03\n",
      " -1.100e+03 -1.106e+03  1.107e+03 -1.111e+03 -1.113e+03  1.114e+03\n",
      "  1.143e+03  1.150e+03 -1.160e+03 -1.176e+03 -1.180e+03 -1.182e+03\n",
      "  1.189e+03  1.190e+03 -1.196e+03 -1.212e+03 -1.217e+03 -1.224e+03\n",
      " -1.259e+03 -1.260e+03 -1.268e+03 -1.280e+03 -1.297e+03 -1.311e+03\n",
      "  1.315e+03 -1.336e+03  1.338e+03  1.344e+03  1.346e+03 -1.361e+03\n",
      " -1.386e+03  1.423e+03  1.426e+03  1.470e+03 -1.475e+03  1.485e+03\n",
      " -1.492e+03 -1.499e+03 -1.505e+03 -1.519e+03 -1.531e+03 -1.541e+03\n",
      " -1.602e+03 -1.639e+03  1.640e+03 -1.645e+03  1.655e+03  1.656e+03\n",
      "  1.679e+03 -1.682e+03 -1.690e+03  1.695e+03  1.713e+03 -1.723e+03\n",
      " -1.731e+03  1.736e+03 -1.743e+03 -1.784e+03 -1.824e+03 -1.834e+03\n",
      "  1.841e+03 -1.845e+03  1.856e+03 -1.884e+03  1.912e+03 -1.927e+03\n",
      "  1.933e+03  1.953e+03 -1.963e+03  1.975e+03  1.984e+03 -1.992e+03\n",
      " -1.997e+03 -2.023e+03 -2.024e+03 -2.073e+03 -2.133e+03  2.133e+03\n",
      " -2.134e+03 -2.148e+03  2.160e+03 -2.167e+03  2.224e+03 -2.227e+03\n",
      " -2.271e+03  2.284e+03  2.304e+03 -2.336e+03  2.378e+03 -2.381e+03\n",
      "  2.409e+03 -2.419e+03 -2.447e+03 -2.471e+03 -2.542e+03 -2.589e+03\n",
      "  2.625e+03  2.639e+03 -2.645e+03  2.680e+03  2.803e+03 -2.811e+03\n",
      " -2.820e+03  2.945e+03  2.979e+03 -3.016e+03 -3.108e+03 -3.123e+03\n",
      " -3.166e+03 -3.189e+03  3.253e+03  3.376e+03 -3.387e+03 -3.576e+03\n",
      "  3.600e+03 -3.636e+03 -3.644e+03 -3.724e+03 -3.747e+03 -3.850e+03\n",
      "  3.924e+03 -4.026e+03 -4.053e+03 -4.068e+03 -4.261e+03 -4.570e+03\n",
      "  4.601e+03 -4.694e+03 -4.765e+03  5.287e+03]\n",
      "Concordance Index 0.539204913644944\n",
      "Integrated Brier Score: 0.19272873148231495\n",
      "y_train breslow final [ 2.000e+00  3.000e+00 -4.000e+00  5.000e+00  5.000e+00  6.000e+00\n",
      " -8.000e+00  9.000e+00 -1.200e+01  1.200e+01 -1.200e+01 -1.300e+01\n",
      " -1.300e+01 -1.500e+01  1.700e+01  2.300e+01  2.400e+01 -2.800e+01\n",
      " -3.000e+01 -3.000e+01 -3.000e+01 -3.100e+01 -3.100e+01  3.400e+01\n",
      " -3.500e+01 -4.200e+01  4.700e+01  5.300e+01  5.900e+01 -6.000e+01\n",
      " -6.100e+01 -6.100e+01 -6.100e+01 -6.100e+01 -6.100e+01 -6.100e+01\n",
      " -6.100e+01 -7.000e+01  8.000e+01 -8.300e+01  8.400e+01  8.500e+01\n",
      "  8.800e+01 -8.900e+01  8.900e+01  9.200e+01  9.400e+01 -9.700e+01\n",
      " -1.030e+02 -1.050e+02 -1.060e+02  1.090e+02 -1.110e+02 -1.110e+02\n",
      "  1.160e+02 -1.210e+02 -1.220e+02 -1.220e+02 -1.220e+02 -1.220e+02\n",
      "  1.230e+02 -1.280e+02  1.310e+02  1.360e+02  1.380e+02  1.430e+02\n",
      "  1.460e+02 -1.500e+02  1.510e+02 -1.530e+02  1.530e+02 -1.600e+02\n",
      "  1.610e+02  1.660e+02  1.660e+02 -1.730e+02  1.800e+02  1.880e+02\n",
      "  1.950e+02  1.980e+02  1.980e+02 -2.020e+02 -2.070e+02 -2.110e+02\n",
      "  2.110e+02  2.120e+02  2.150e+02  2.230e+02 -2.270e+02 -2.360e+02\n",
      " -2.380e+02 -2.420e+02 -2.440e+02  2.660e+02  2.740e+02 -2.890e+02\n",
      "  2.910e+02  2.940e+02  2.990e+02  3.020e+02  3.070e+02  3.110e+02\n",
      "  3.150e+02  3.220e+02 -3.240e+02  3.290e+02  3.400e+02  3.400e+02\n",
      "  3.450e+02 -3.530e+02 -3.580e+02 -3.580e+02  3.600e+02  3.650e+02\n",
      "  3.660e+02 -3.660e+02 -3.670e+02 -3.700e+02  3.710e+02  3.720e+02\n",
      " -3.770e+02 -3.780e+02  3.830e+02  3.830e+02  3.970e+02  3.990e+02\n",
      " -4.010e+02  4.020e+02 -4.050e+02 -4.060e+02 -4.070e+02  4.080e+02\n",
      "  4.120e+02  4.180e+02 -4.200e+02 -4.230e+02 -4.280e+02  4.290e+02\n",
      " -4.300e+02 -4.320e+02 -4.400e+02  4.420e+02  4.450e+02 -4.480e+02\n",
      "  4.480e+02  4.540e+02  4.550e+02  4.560e+02 -4.810e+02 -4.820e+02\n",
      "  4.900e+02 -4.920e+02 -4.980e+02  5.010e+02 -5.100e+02 -5.100e+02\n",
      " -5.110e+02  5.150e+02  5.160e+02 -5.170e+02  5.390e+02  5.430e+02\n",
      "  5.520e+02 -5.550e+02 -5.570e+02  5.590e+02 -5.590e+02  5.620e+02\n",
      " -5.650e+02 -5.700e+02 -5.760e+02 -5.780e+02 -5.780e+02 -5.790e+02\n",
      " -5.810e+02 -5.850e+02 -5.890e+02 -5.960e+02 -6.000e+02 -6.010e+02\n",
      "  6.010e+02  6.040e+02 -6.080e+02 -6.150e+02 -6.180e+02  6.230e+02\n",
      " -6.300e+02  6.320e+02 -6.370e+02 -6.400e+02 -6.410e+02 -6.420e+02\n",
      "  6.450e+02 -6.600e+02 -6.660e+02  6.690e+02  6.780e+02  6.840e+02\n",
      "  6.870e+02 -6.880e+02 -6.990e+02 -7.000e+02  7.080e+02  7.160e+02\n",
      " -7.170e+02  7.340e+02 -7.340e+02  7.400e+02 -7.410e+02 -7.570e+02\n",
      " -7.580e+02 -7.620e+02 -7.650e+02 -7.690e+02  7.700e+02 -7.900e+02\n",
      "  8.030e+02 -8.040e+02 -8.040e+02 -8.160e+02 -8.180e+02 -8.220e+02\n",
      "  8.220e+02  8.260e+02 -8.260e+02 -8.330e+02  8.350e+02  8.400e+02\n",
      " -8.420e+02 -8.490e+02 -8.670e+02  8.810e+02 -9.080e+02 -9.100e+02\n",
      " -9.110e+02 -9.110e+02  9.160e+02  9.210e+02  9.270e+02  9.330e+02\n",
      "  9.370e+02 -9.370e+02 -9.420e+02  9.510e+02 -9.520e+02 -9.600e+02\n",
      "  9.650e+02  9.730e+02  9.740e+02 -9.830e+02 -9.950e+02  1.000e+03\n",
      "  1.001e+03  1.006e+03 -1.007e+03 -1.032e+03  1.045e+03 -1.050e+03\n",
      "  1.057e+03 -1.058e+03 -1.072e+03  1.075e+03 -1.092e+03  1.097e+03\n",
      " -1.100e+03 -1.106e+03  1.107e+03 -1.111e+03 -1.113e+03  1.114e+03\n",
      "  1.143e+03  1.150e+03 -1.160e+03 -1.176e+03 -1.180e+03 -1.182e+03\n",
      "  1.189e+03  1.190e+03 -1.196e+03 -1.212e+03 -1.217e+03 -1.224e+03\n",
      " -1.259e+03 -1.260e+03 -1.268e+03 -1.280e+03 -1.297e+03 -1.311e+03\n",
      "  1.315e+03 -1.336e+03  1.338e+03  1.344e+03  1.346e+03 -1.361e+03\n",
      " -1.386e+03  1.423e+03  1.426e+03  1.470e+03 -1.475e+03  1.485e+03\n",
      " -1.492e+03 -1.499e+03 -1.505e+03 -1.519e+03 -1.531e+03 -1.541e+03\n",
      " -1.602e+03 -1.639e+03  1.640e+03 -1.645e+03  1.655e+03  1.656e+03\n",
      "  1.679e+03 -1.682e+03 -1.690e+03  1.695e+03  1.713e+03 -1.723e+03\n",
      " -1.731e+03  1.736e+03 -1.743e+03 -1.784e+03 -1.824e+03 -1.834e+03\n",
      "  1.841e+03 -1.845e+03  1.856e+03 -1.884e+03  1.912e+03 -1.927e+03\n",
      "  1.933e+03  1.953e+03 -1.963e+03  1.975e+03  1.984e+03 -1.992e+03\n",
      " -1.997e+03 -2.023e+03 -2.024e+03 -2.073e+03 -2.133e+03  2.133e+03\n",
      " -2.134e+03 -2.148e+03  2.160e+03 -2.167e+03  2.224e+03 -2.227e+03\n",
      " -2.271e+03  2.284e+03  2.304e+03 -2.336e+03  2.378e+03 -2.381e+03\n",
      "  2.409e+03 -2.419e+03 -2.447e+03 -2.471e+03 -2.542e+03 -2.589e+03\n",
      "  2.625e+03  2.639e+03 -2.645e+03  2.680e+03  2.803e+03 -2.811e+03\n",
      " -2.820e+03  2.945e+03  2.979e+03 -3.016e+03 -3.108e+03 -3.123e+03\n",
      " -3.166e+03 -3.189e+03  3.253e+03  3.376e+03 -3.387e+03 -3.576e+03\n",
      "  3.600e+03 -3.636e+03 -3.644e+03 -3.724e+03 -3.747e+03 -3.850e+03\n",
      "  3.924e+03 -4.026e+03 -4.053e+03 -4.068e+03 -4.261e+03 -4.570e+03\n",
      "  4.601e+03 -4.694e+03 -4.765e+03  5.287e+03]\n",
      "durations 1.0 3838.0\n",
      "Concordance Index 0.44848225737494657\n",
      "Integrated Brier Score: 0.19718916458333732\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m297.8870\u001b[0m  0.0038\n",
      "      2      297.8870  0.0072\n",
      "      3      297.8870  0.0031\n",
      "      4      297.8870  0.0049\n",
      "      5      297.8870  0.0033\n",
      "      6      297.8870  0.0019\n",
      "      7      297.8870  0.0017\n",
      "      8      297.8870  0.0017\n",
      "      9      297.8870  0.0017\n",
      "     10      297.8870  0.0017\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m247.4434\u001b[0m  0.0023\n",
      "      2      247.4434  0.0019\n",
      "      3      247.4434  0.0018\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      4      247.4434  0.0018\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m78.2312\u001b[0m  0.0039\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m223.7235\u001b[0m  0.0020\n",
      "      2       78.2312  0.0029\n",
      "      2      223.7235  0.0018\n",
      "      5      247.4434  0.0054\n",
      "      3       78.2312  0.0027      3      223.7235  0.0019\n",
      "\n",
      "      6      247.4434  0.0018\n",
      "      4      223.7235  0.0017\n",
      "      4       78.2312  0.0027\n",
      "      7      247.4434  0.0018\n",
      "      5      223.7235  0.0016\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      6      223.7235  0.0016\n",
      "      8      247.4434  0.0019\n",
      "      5       78.2312  0.0026\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      7      223.7235  0.0016\n",
      "      9      247.4434  0.0020\n",
      "      6       78.2312  0.0026\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      8      223.7235  0.0016\n",
      "     10      247.4434  0.0018\n",
      "Restoring best model from epoch 1.\n",
      "      7       78.2312  0.0026\n",
      "      9      223.7235  0.0020\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m91.6354\u001b[0m  0.0053\n",
      "     10      223.7235  0.0020\n",
      "Restoring best model from epoch 1.\n",
      "      8       78.2312  0.0026\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2       91.6354  0.0029\n",
      "      9       78.2312  0.0026\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m82.5905\u001b[0m  0.0072\n",
      "      3       91.6354  0.0027\n",
      "     10       78.2312  0.0026\n",
      "Restoring best model from epoch 1.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m206.0622\u001b[0m  0.0026\n",
      "      2       82.5905  0.0029\n",
      "      2      206.0622  0.0018\n",
      "      4       91.6354  0.0026\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      3      206.0622  0.0017\n",
      "      3       82.5905  0.0026\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      4      206.0622  0.0017\n",
      "      4       82.5905  0.0026\n",
      "      5       91.6354  0.0053\n",
      "      5      206.0622  0.0017\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      6      206.0622  0.0017\n",
      "      5       82.5905  0.0026\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m98.5682\u001b[0m  0.0032\n",
      "      6       91.6354  0.0028\n",
      "      7      206.0622  0.0023\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m266.3711\u001b[0m  0.0026\n",
      "      2       98.5682  0.0027\n",
      "      6       82.5905  0.0029\n",
      "      7       91.6354  0.0026\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      8      206.0622  0.0018\n",
      "      2      266.3711  0.0018\n",
      "      7       82.5905  0.0026\n",
      "      3       98.5682  0.0028\n",
      "      8       91.6354  0.0026\n",
      "      9      206.0622  0.0018\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      3      266.3711  0.0017\n",
      "     10      206.0622  0.0017\n",
      "Restoring best model from epoch 1.\n",
      "      4      266.3711  0.0019\n",
      "      8       82.5905  0.0026\n",
      "      9       91.6354  0.0026\n",
      "      4       98.5682  0.0040\n",
      "      9       82.5905  0.0025\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m86.6718\u001b[0m  0.0030\n",
      "     10       91.6354  0.0026\n",
      "Restoring best model from epoch 1.\n",
      "      5       98.5682  0.0027\n",
      "      5      266.3711  0.0039\n",
      "     10       82.5905  0.0025\n",
      "Restoring best model from epoch 1.\n",
      "      2       86.6718  0.0028\n",
      "      6       98.5682  0.0026\n",
      "      6      266.3711  0.0031\n",
      "      3       86.6718  0.0027\n",
      "      7      266.3711  0.0017\n",
      "      7       98.5682  0.0027\n",
      "      8      266.3711  0.0017\n",
      "      4       86.6718  0.0027\n",
      "      8       98.5682  0.0027\n",
      "      9      266.3711  0.0016\n",
      "      5       86.6718  0.0027\n",
      "     10      266.3711  0.0016\n",
      "Restoring best model from epoch 1.\n",
      "      9       98.5682  0.0026\n",
      "      6       86.6718  0.0026\n",
      "      7       86.6718  0.0026\n",
      "      8       86.6718  0.0026\n",
      "      9       86.6718  0.0031\n",
      "     10       98.5682  0.0109\n",
      "Restoring best model from epoch 1.\n",
      "     10       86.6718  0.0026\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m111.3153\u001b[0m  0.0034\n",
      "      2      111.3153  0.0036\n",
      "      3      111.3153  0.0036\n",
      "      4      111.3153  0.0032\n",
      "      5      111.3153  0.0036\n",
      "      6      111.3153  0.0036\n",
      "      7      111.3153  0.0060\n",
      "      8      111.3153  0.0055\n",
      "      9      111.3153  0.0051\n",
      "     10      111.3153  0.0039\n",
      "Restoring best model from epoch 1.\n",
      "y_train breslow final [ 1.000e+00  2.000e+00  3.000e+00  6.000e+00 -1.200e+01  1.200e+01\n",
      " -1.200e+01  1.200e+01 -1.300e+01 -1.500e+01  1.700e+01  2.300e+01\n",
      "  2.400e+01 -3.000e+01 -3.000e+01 -3.100e+01 -3.100e+01  3.400e+01\n",
      " -3.700e+01 -4.100e+01  5.200e+01  5.300e+01 -5.500e+01  5.900e+01\n",
      " -6.000e+01 -6.100e+01 -6.100e+01 -6.100e+01 -6.100e+01 -6.100e+01\n",
      " -6.100e+01 -6.100e+01  6.100e+01  8.000e+01 -8.200e+01 -8.300e+01\n",
      "  8.400e+01 -8.900e+01  8.900e+01  9.200e+01  9.400e+01 -9.700e+01\n",
      " -1.030e+02 -1.050e+02 -1.060e+02  1.090e+02 -1.110e+02 -1.110e+02\n",
      "  1.130e+02  1.160e+02 -1.210e+02 -1.220e+02 -1.220e+02  1.230e+02\n",
      "  1.230e+02 -1.280e+02  1.310e+02 -1.350e+02  1.360e+02  1.380e+02\n",
      " -1.500e+02  1.510e+02  1.530e+02  1.610e+02  1.660e+02  1.660e+02\n",
      " -1.730e+02  1.800e+02  1.880e+02  1.950e+02  1.980e+02  1.980e+02\n",
      " -2.020e+02  2.150e+02  2.230e+02 -2.270e+02 -2.360e+02  2.360e+02\n",
      " -2.420e+02 -2.470e+02  2.610e+02  2.660e+02  2.740e+02  2.760e+02\n",
      "  2.840e+02 -2.890e+02  2.910e+02  2.940e+02 -2.960e+02  3.060e+02\n",
      "  3.070e+02  3.150e+02  3.220e+02 -3.240e+02  3.290e+02  3.400e+02\n",
      "  3.450e+02  3.450e+02  3.510e+02 -3.530e+02  3.570e+02  3.580e+02\n",
      " -3.580e+02 -3.580e+02  3.580e+02  3.600e+02  3.650e+02  3.660e+02\n",
      " -3.700e+02  3.720e+02 -3.750e+02 -3.770e+02 -3.780e+02  3.830e+02\n",
      "  3.870e+02 -3.920e+02 -3.960e+02  3.970e+02 -4.010e+02  4.020e+02\n",
      "  4.030e+02 -4.050e+02  4.080e+02  4.120e+02  4.180e+02  4.280e+02\n",
      " -4.280e+02 -4.280e+02 -4.300e+02 -4.320e+02 -4.400e+02  4.450e+02\n",
      " -4.480e+02  4.480e+02  4.540e+02 -4.550e+02  4.560e+02 -4.650e+02\n",
      " -4.680e+02  4.740e+02 -4.810e+02 -4.820e+02  4.900e+02 -4.910e+02\n",
      " -4.930e+02 -4.980e+02  5.060e+02 -5.100e+02 -5.100e+02 -5.110e+02\n",
      "  5.150e+02  5.160e+02  5.190e+02 -5.310e+02  5.320e+02 -5.380e+02\n",
      "  5.390e+02  5.430e+02  5.440e+02  5.500e+02  5.520e+02 -5.550e+02\n",
      " -5.570e+02  5.590e+02  5.620e+02 -5.700e+02  5.730e+02 -5.780e+02\n",
      " -5.780e+02 -5.790e+02 -5.810e+02 -5.850e+02  5.870e+02 -5.960e+02\n",
      " -6.000e+02  6.010e+02  6.040e+02 -6.080e+02 -6.160e+02  6.230e+02\n",
      " -6.300e+02  6.320e+02 -6.390e+02 -6.400e+02 -6.400e+02 -6.410e+02\n",
      "  6.450e+02 -6.490e+02 -6.520e+02 -6.530e+02 -6.660e+02  6.670e+02\n",
      " -6.710e+02  6.780e+02 -6.830e+02  6.840e+02  6.870e+02 -6.880e+02\n",
      "  6.920e+02 -6.990e+02 -6.990e+02 -6.990e+02 -7.000e+02  7.080e+02\n",
      "  7.160e+02 -7.170e+02 -7.340e+02  7.400e+02 -7.410e+02 -7.570e+02\n",
      " -7.590e+02 -7.620e+02 -7.650e+02 -7.690e+02 -7.900e+02 -8.040e+02\n",
      " -8.100e+02 -8.150e+02 -8.160e+02 -8.220e+02  8.220e+02  8.260e+02\n",
      "  8.260e+02 -8.260e+02  8.270e+02  8.350e+02  8.400e+02 -8.420e+02\n",
      " -8.490e+02 -8.620e+02 -8.670e+02  8.990e+02 -9.080e+02 -9.100e+02\n",
      " -9.100e+02 -9.110e+02 -9.110e+02 -9.110e+02 -9.130e+02  9.160e+02\n",
      " -9.230e+02 -9.250e+02  9.270e+02 -9.320e+02  9.330e+02 -9.370e+02\n",
      " -9.420e+02  9.510e+02  9.620e+02  9.650e+02  9.740e+02 -9.790e+02\n",
      " -9.830e+02 -9.900e+02 -9.950e+02  1.000e+03  1.001e+03  1.006e+03\n",
      " -1.007e+03 -1.011e+03 -1.031e+03 -1.032e+03 -1.038e+03 -1.052e+03\n",
      "  1.058e+03 -1.058e+03 -1.063e+03  1.067e+03 -1.072e+03  1.075e+03\n",
      " -1.092e+03  1.097e+03 -1.100e+03 -1.106e+03 -1.111e+03 -1.113e+03\n",
      "  1.114e+03  1.143e+03  1.150e+03  1.154e+03 -1.160e+03  1.161e+03\n",
      " -1.176e+03 -1.180e+03 -1.182e+03  1.190e+03 -1.196e+03 -1.212e+03\n",
      " -1.217e+03 -1.223e+03 -1.224e+03 -1.244e+03 -1.260e+03 -1.268e+03\n",
      " -1.280e+03 -1.297e+03 -1.311e+03  1.315e+03  1.335e+03 -1.336e+03\n",
      "  1.338e+03  1.346e+03  1.423e+03  1.426e+03  1.470e+03 -1.475e+03\n",
      " -1.492e+03 -1.505e+03 -1.519e+03 -1.531e+03 -1.541e+03 -1.645e+03\n",
      "  1.655e+03  1.679e+03 -1.682e+03 -1.690e+03  1.695e+03  1.713e+03\n",
      " -1.723e+03 -1.743e+03 -1.784e+03 -1.824e+03 -1.834e+03 -1.841e+03\n",
      " -1.845e+03  1.856e+03  1.874e+03 -1.884e+03 -1.927e+03  1.933e+03\n",
      "  1.953e+03 -1.963e+03  1.975e+03  1.984e+03 -1.992e+03 -1.997e+03\n",
      " -2.023e+03 -2.024e+03 -2.026e+03 -2.073e+03 -2.080e+03  2.086e+03\n",
      " -2.133e+03  2.133e+03 -2.134e+03 -2.142e+03 -2.148e+03  2.160e+03\n",
      " -2.165e+03 -2.167e+03  2.170e+03 -2.271e+03  2.284e+03  2.304e+03\n",
      " -2.336e+03  2.378e+03  2.409e+03 -2.447e+03 -2.471e+03 -2.510e+03\n",
      " -2.589e+03  2.625e+03  2.639e+03 -2.645e+03  2.680e+03 -2.716e+03\n",
      " -2.811e+03 -2.820e+03  2.945e+03  2.979e+03 -3.016e+03 -3.108e+03\n",
      " -3.123e+03  3.149e+03 -3.166e+03 -3.189e+03  3.253e+03 -3.387e+03\n",
      " -3.576e+03  3.600e+03 -3.636e+03 -3.724e+03 -3.747e+03  3.838e+03\n",
      " -3.850e+03  3.924e+03 -4.053e+03 -4.068e+03 -4.261e+03 -4.570e+03\n",
      "  4.601e+03 -4.694e+03 -4.765e+03  5.287e+03]\n",
      "Concordance Index 0.5434966793298532\n",
      "Integrated Brier Score: 0.18527250435322287\n",
      "y_train breslow final [ 1.000e+00  2.000e+00  3.000e+00  6.000e+00 -1.200e+01  1.200e+01\n",
      " -1.200e+01  1.200e+01 -1.300e+01 -1.500e+01  1.700e+01  2.300e+01\n",
      "  2.400e+01 -3.000e+01 -3.000e+01 -3.100e+01 -3.100e+01  3.400e+01\n",
      " -3.700e+01 -4.100e+01  5.200e+01  5.300e+01 -5.500e+01  5.900e+01\n",
      " -6.000e+01 -6.100e+01 -6.100e+01 -6.100e+01 -6.100e+01 -6.100e+01\n",
      " -6.100e+01 -6.100e+01  6.100e+01  8.000e+01 -8.200e+01 -8.300e+01\n",
      "  8.400e+01 -8.900e+01  8.900e+01  9.200e+01  9.400e+01 -9.700e+01\n",
      " -1.030e+02 -1.050e+02 -1.060e+02  1.090e+02 -1.110e+02 -1.110e+02\n",
      "  1.130e+02  1.160e+02 -1.210e+02 -1.220e+02 -1.220e+02  1.230e+02\n",
      "  1.230e+02 -1.280e+02  1.310e+02 -1.350e+02  1.360e+02  1.380e+02\n",
      " -1.500e+02  1.510e+02  1.530e+02  1.610e+02  1.660e+02  1.660e+02\n",
      " -1.730e+02  1.800e+02  1.880e+02  1.950e+02  1.980e+02  1.980e+02\n",
      " -2.020e+02  2.150e+02  2.230e+02 -2.270e+02 -2.360e+02  2.360e+02\n",
      " -2.420e+02 -2.470e+02  2.610e+02  2.660e+02  2.740e+02  2.760e+02\n",
      "  2.840e+02 -2.890e+02  2.910e+02  2.940e+02 -2.960e+02  3.060e+02\n",
      "  3.070e+02  3.150e+02  3.220e+02 -3.240e+02  3.290e+02  3.400e+02\n",
      "  3.450e+02  3.450e+02  3.510e+02 -3.530e+02  3.570e+02  3.580e+02\n",
      " -3.580e+02 -3.580e+02  3.580e+02  3.600e+02  3.650e+02  3.660e+02\n",
      " -3.700e+02  3.720e+02 -3.750e+02 -3.770e+02 -3.780e+02  3.830e+02\n",
      "  3.870e+02 -3.920e+02 -3.960e+02  3.970e+02 -4.010e+02  4.020e+02\n",
      "  4.030e+02 -4.050e+02  4.080e+02  4.120e+02  4.180e+02  4.280e+02\n",
      " -4.280e+02 -4.280e+02 -4.300e+02 -4.320e+02 -4.400e+02  4.450e+02\n",
      " -4.480e+02  4.480e+02  4.540e+02 -4.550e+02  4.560e+02 -4.650e+02\n",
      " -4.680e+02  4.740e+02 -4.810e+02 -4.820e+02  4.900e+02 -4.910e+02\n",
      " -4.930e+02 -4.980e+02  5.060e+02 -5.100e+02 -5.100e+02 -5.110e+02\n",
      "  5.150e+02  5.160e+02  5.190e+02 -5.310e+02  5.320e+02 -5.380e+02\n",
      "  5.390e+02  5.430e+02  5.440e+02  5.500e+02  5.520e+02 -5.550e+02\n",
      " -5.570e+02  5.590e+02  5.620e+02 -5.700e+02  5.730e+02 -5.780e+02\n",
      " -5.780e+02 -5.790e+02 -5.810e+02 -5.850e+02  5.870e+02 -5.960e+02\n",
      " -6.000e+02  6.010e+02  6.040e+02 -6.080e+02 -6.160e+02  6.230e+02\n",
      " -6.300e+02  6.320e+02 -6.390e+02 -6.400e+02 -6.400e+02 -6.410e+02\n",
      "  6.450e+02 -6.490e+02 -6.520e+02 -6.530e+02 -6.660e+02  6.670e+02\n",
      " -6.710e+02  6.780e+02 -6.830e+02  6.840e+02  6.870e+02 -6.880e+02\n",
      "  6.920e+02 -6.990e+02 -6.990e+02 -6.990e+02 -7.000e+02  7.080e+02\n",
      "  7.160e+02 -7.170e+02 -7.340e+02  7.400e+02 -7.410e+02 -7.570e+02\n",
      " -7.590e+02 -7.620e+02 -7.650e+02 -7.690e+02 -7.900e+02 -8.040e+02\n",
      " -8.100e+02 -8.150e+02 -8.160e+02 -8.220e+02  8.220e+02  8.260e+02\n",
      "  8.260e+02 -8.260e+02  8.270e+02  8.350e+02  8.400e+02 -8.420e+02\n",
      " -8.490e+02 -8.620e+02 -8.670e+02  8.990e+02 -9.080e+02 -9.100e+02\n",
      " -9.100e+02 -9.110e+02 -9.110e+02 -9.110e+02 -9.130e+02  9.160e+02\n",
      " -9.230e+02 -9.250e+02  9.270e+02 -9.320e+02  9.330e+02 -9.370e+02\n",
      " -9.420e+02  9.510e+02  9.620e+02  9.650e+02  9.740e+02 -9.790e+02\n",
      " -9.830e+02 -9.900e+02 -9.950e+02  1.000e+03  1.001e+03  1.006e+03\n",
      " -1.007e+03 -1.011e+03 -1.031e+03 -1.032e+03 -1.038e+03 -1.052e+03\n",
      "  1.058e+03 -1.058e+03 -1.063e+03  1.067e+03 -1.072e+03  1.075e+03\n",
      " -1.092e+03  1.097e+03 -1.100e+03 -1.106e+03 -1.111e+03 -1.113e+03\n",
      "  1.114e+03  1.143e+03  1.150e+03  1.154e+03 -1.160e+03  1.161e+03\n",
      " -1.176e+03 -1.180e+03 -1.182e+03  1.190e+03 -1.196e+03 -1.212e+03\n",
      " -1.217e+03 -1.223e+03 -1.224e+03 -1.244e+03 -1.260e+03 -1.268e+03\n",
      " -1.280e+03 -1.297e+03 -1.311e+03  1.315e+03  1.335e+03 -1.336e+03\n",
      "  1.338e+03  1.346e+03  1.423e+03  1.426e+03  1.470e+03 -1.475e+03\n",
      " -1.492e+03 -1.505e+03 -1.519e+03 -1.531e+03 -1.541e+03 -1.645e+03\n",
      "  1.655e+03  1.679e+03 -1.682e+03 -1.690e+03  1.695e+03  1.713e+03\n",
      " -1.723e+03 -1.743e+03 -1.784e+03 -1.824e+03 -1.834e+03 -1.841e+03\n",
      " -1.845e+03  1.856e+03  1.874e+03 -1.884e+03 -1.927e+03  1.933e+03\n",
      "  1.953e+03 -1.963e+03  1.975e+03  1.984e+03 -1.992e+03 -1.997e+03\n",
      " -2.023e+03 -2.024e+03 -2.026e+03 -2.073e+03 -2.080e+03  2.086e+03\n",
      " -2.133e+03  2.133e+03 -2.134e+03 -2.142e+03 -2.148e+03  2.160e+03\n",
      " -2.165e+03 -2.167e+03  2.170e+03 -2.271e+03  2.284e+03  2.304e+03\n",
      " -2.336e+03  2.378e+03  2.409e+03 -2.447e+03 -2.471e+03 -2.510e+03\n",
      " -2.589e+03  2.625e+03  2.639e+03 -2.645e+03  2.680e+03 -2.716e+03\n",
      " -2.811e+03 -2.820e+03  2.945e+03  2.979e+03 -3.016e+03 -3.108e+03\n",
      " -3.123e+03  3.149e+03 -3.166e+03 -3.189e+03  3.253e+03 -3.387e+03\n",
      " -3.576e+03  3.600e+03 -3.636e+03 -3.724e+03 -3.747e+03  3.838e+03\n",
      " -3.850e+03  3.924e+03 -4.053e+03 -4.068e+03 -4.261e+03 -4.570e+03\n",
      "  4.601e+03 -4.694e+03 -4.765e+03  5.287e+03]\n",
      "durations 4.0 4026.0\n",
      "Concordance Index 0.4574938574938575\n",
      "Integrated Brier Score: 0.2003813492438247\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m238.2901\u001b[0m  0.0021\n",
      "      2      238.2901  0.0018\n",
      "      3      238.2901  0.0017\n",
      "      4      238.2901  0.0017\n",
      "      5      238.2901  0.0017\n",
      "      6      238.2901  0.0016\n",
      "      7      238.2901  0.0016\n",
      "      8      238.2901  0.0016\n",
      "      9      238.2901  0.0016\n",
      "     10      238.2901  0.0016\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m166.5340\u001b[0m  0.0021\n",
      "      2      166.5340  0.0018\n",
      "      3      166.5340  0.0017\n",
      "      4      166.5340  0.0017\n",
      "      5      166.5340  0.0016\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      6      166.5340  0.0017\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      7      166.5340  0.0017\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m221.2088\u001b[0m  0.0022\n",
      "      8      166.5340  0.0016\n",
      "      9      166.5340  0.0016\n",
      "      2      221.2088  0.0019\n",
      "     10      166.5340  0.0016\n",
      "Restoring best model from epoch 1.\n",
      "      3      221.2088  0.0017\n",
      "      4      221.2088  0.0017\n",
      "      5      221.2088  0.0017\n",
      "      6      221.2088  0.0017\n",
      "      7      221.2088  0.0017\n",
      "      8      221.2088  0.0017\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      9      221.2088  0.0017Re-initializing criterion.\n",
      "\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m262.3083\u001b[0m  0.0025\n",
      "     10      221.2088  0.0045\n",
      "Restoring best model from epoch 1.\n",
      "      2      262.3083  0.0018\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      3      262.3083  0.0018\n",
      "      4      262.3083  0.0018\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      5      262.3083  0.0017\n",
      "      6      262.3083  0.0017\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m71.1638\u001b[0m  0.0029\n",
      "      7      262.3083  0.0018\n",
      "      8      262.3083  0.0017\n",
      "      2       71.1638  0.0032\n",
      "      9      262.3083  0.0017\n",
      "      3       71.1638  0.0028\n",
      "     10      262.3083  0.0017\n",
      "Restoring best model from epoch 1.\n",
      "      4       71.1638  0.0025\n",
      "      5       71.1638  0.0026\n",
      "      6       71.1638  0.0027\n",
      "      7       71.1638  0.0026\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      8       71.1638  0.0025\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      9       71.1638  0.0025\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m93.4060\u001b[0m  0.0029\n",
      "      2       93.4060  0.0027\n",
      "     10       71.1638  0.0063\n",
      "Restoring best model from epoch 1.\n",
      "      3       93.4060  0.0027\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m79.5770\u001b[0m  0.0070\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2       79.5770  0.0027\n",
      "      4       93.4060  0.0057\n",
      "      3       79.5770  0.0025\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m69.1095\u001b[0m  0.0028\n",
      "      2       69.1095  0.0027\n",
      "      4       79.5770  0.0029\n",
      "      5       93.4060  0.0057\n",
      "      5       79.5770  0.0027\n",
      "      3       69.1095  0.0047\n",
      "      6       93.4060  0.0028\n",
      "      6       79.5770  0.0026\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      7       93.4060  0.0027\n",
      "      7       79.5770  0.0026\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      8       79.5770  0.0025\n",
      "      8       93.4060  0.0027\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m83.9120\u001b[0m  0.0032\n",
      "      9       79.5770  0.0025\n",
      "      4       69.1095  0.0085\n",
      "      9       93.4060  0.0028\n",
      "     10       79.5770  0.0025\n",
      "      2       83.9120  0.0029\n",
      "      5       69.1095  0.0026\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "     10       93.4060  0.0026\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      3       83.9120  0.0026\n",
      "      6       69.1095  0.0025\n",
      "      7       69.1095  0.0025\n",
      "      4       83.9120  0.0026\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m231.0837\u001b[0m  0.0048\n",
      "      8       69.1095  0.0026\n",
      "      5       83.9120  0.0026\n",
      "      9       69.1095  0.0025\n",
      "      6       83.9120  0.0026\n",
      "      2      231.0837  0.0035\n",
      "     10       69.1095  0.0024\n",
      "Restoring best model from epoch 1.\n",
      "      7       83.9120  0.0026\n",
      "      3      231.0837  0.0020\n",
      "      4      231.0837  0.0018\n",
      "      8       83.9120  0.0026\n",
      "      5      231.0837  0.0017\n",
      "      9       83.9120  0.0026\n",
      "      6      231.0837  0.0017\n",
      "      7      231.0837  0.0019\n",
      "     10       83.9120  0.0029\n",
      "Restoring best model from epoch 1.\n",
      "      8      231.0837  0.0019\n",
      "      9      231.0837  0.0018\n",
      "     10      231.0837  0.0017\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m99.3288\u001b[0m  0.0034\n",
      "      2       99.3288  0.0046\n",
      "      3       99.3288  0.0035\n",
      "      4       99.3288  0.0033\n",
      "      5       99.3288  0.0036\n",
      "      6       99.3288  0.0036\n",
      "      7       99.3288  0.0036\n",
      "      8       99.3288  0.0030\n",
      "      9       99.3288  0.0031\n",
      "     10       99.3288  0.0051\n",
      "Restoring best model from epoch 1.\n",
      "y_train breslow final [ 1.000e+00  2.000e+00  3.000e+00 -4.000e+00  5.000e+00  5.000e+00\n",
      "  6.000e+00 -8.000e+00  9.000e+00 -1.200e+01  1.200e+01  1.200e+01\n",
      " -1.300e+01 -1.500e+01  2.300e+01  2.400e+01 -2.800e+01 -3.000e+01\n",
      " -3.000e+01 -3.000e+01 -3.100e+01 -3.100e+01  3.400e+01 -3.500e+01\n",
      " -3.700e+01 -4.100e+01 -4.200e+01  4.700e+01  5.200e+01  5.300e+01\n",
      " -5.500e+01  5.900e+01 -6.000e+01 -6.100e+01 -6.100e+01 -6.100e+01\n",
      " -6.100e+01 -6.100e+01 -6.100e+01  6.100e+01 -7.000e+01  8.000e+01\n",
      " -8.200e+01 -8.300e+01  8.500e+01  8.800e+01  8.900e+01  9.200e+01\n",
      "  9.400e+01 -9.700e+01 -1.030e+02 -1.060e+02 -1.060e+02  1.090e+02\n",
      " -1.110e+02  1.130e+02 -1.210e+02 -1.220e+02 -1.220e+02 -1.220e+02\n",
      " -1.220e+02  1.230e+02  1.230e+02  1.310e+02 -1.350e+02  1.430e+02\n",
      "  1.460e+02 -1.500e+02  1.510e+02 -1.530e+02  1.530e+02 -1.600e+02\n",
      "  1.660e+02 -1.730e+02  1.880e+02  1.950e+02 -2.070e+02 -2.110e+02\n",
      "  2.110e+02  2.120e+02 -2.270e+02 -2.360e+02  2.360e+02 -2.380e+02\n",
      " -2.440e+02 -2.470e+02  2.610e+02  2.660e+02  2.760e+02  2.840e+02\n",
      " -2.890e+02  2.910e+02 -2.960e+02  2.990e+02  3.020e+02  3.060e+02\n",
      "  3.070e+02  3.110e+02  3.220e+02  3.290e+02  3.400e+02  3.450e+02\n",
      "  3.450e+02  3.510e+02 -3.530e+02  3.570e+02  3.580e+02 -3.580e+02\n",
      " -3.580e+02  3.580e+02  3.600e+02  3.650e+02  3.660e+02 -3.660e+02\n",
      " -3.670e+02 -3.700e+02  3.710e+02 -3.750e+02 -3.770e+02 -3.780e+02\n",
      "  3.830e+02  3.830e+02  3.870e+02 -3.920e+02 -3.960e+02  3.970e+02\n",
      "  3.990e+02 -4.010e+02  4.030e+02 -4.050e+02 -4.050e+02 -4.060e+02\n",
      " -4.070e+02  4.080e+02  4.180e+02 -4.200e+02 -4.230e+02  4.280e+02\n",
      " -4.280e+02  4.290e+02 -4.300e+02 -4.320e+02  4.420e+02  4.450e+02\n",
      " -4.550e+02  4.550e+02  4.560e+02 -4.650e+02 -4.680e+02  4.740e+02\n",
      " -4.810e+02 -4.820e+02 -4.910e+02 -4.920e+02 -4.930e+02 -4.980e+02\n",
      "  5.010e+02  5.060e+02 -5.100e+02 -5.110e+02  5.150e+02 -5.170e+02\n",
      "  5.190e+02 -5.310e+02  5.320e+02 -5.380e+02  5.390e+02  5.430e+02\n",
      "  5.440e+02  5.500e+02  5.520e+02 -5.570e+02  5.590e+02 -5.590e+02\n",
      "  5.620e+02 -5.650e+02 -5.700e+02  5.730e+02 -5.760e+02 -5.780e+02\n",
      " -5.780e+02 -5.810e+02 -5.850e+02  5.870e+02 -5.890e+02 -6.010e+02\n",
      "  6.040e+02 -6.080e+02 -6.150e+02 -6.160e+02 -6.180e+02  6.230e+02\n",
      " -6.300e+02  6.320e+02 -6.370e+02 -6.390e+02 -6.400e+02 -6.400e+02\n",
      " -6.410e+02 -6.420e+02 -6.490e+02 -6.520e+02 -6.530e+02 -6.600e+02\n",
      " -6.660e+02  6.670e+02  6.690e+02 -6.710e+02 -6.830e+02  6.840e+02\n",
      "  6.870e+02  6.920e+02 -6.990e+02 -6.990e+02 -6.990e+02  7.080e+02\n",
      "  7.160e+02 -7.170e+02  7.340e+02  7.400e+02 -7.580e+02 -7.590e+02\n",
      " -7.690e+02  7.700e+02 -7.900e+02  8.030e+02 -8.040e+02 -8.040e+02\n",
      " -8.100e+02 -8.150e+02 -8.160e+02 -8.180e+02 -8.220e+02  8.220e+02\n",
      "  8.260e+02  8.270e+02 -8.330e+02  8.350e+02  8.400e+02 -8.490e+02\n",
      " -8.620e+02 -8.670e+02  8.810e+02  8.990e+02 -9.080e+02 -9.100e+02\n",
      " -9.100e+02 -9.110e+02 -9.110e+02 -9.130e+02  9.160e+02  9.210e+02\n",
      " -9.230e+02 -9.250e+02 -9.320e+02  9.330e+02  9.370e+02 -9.370e+02\n",
      " -9.420e+02  9.510e+02 -9.520e+02 -9.600e+02  9.620e+02  9.730e+02\n",
      "  9.740e+02 -9.790e+02 -9.830e+02 -9.900e+02 -9.950e+02  1.000e+03\n",
      "  1.001e+03  1.006e+03 -1.007e+03 -1.011e+03 -1.031e+03 -1.032e+03\n",
      " -1.038e+03  1.045e+03 -1.050e+03 -1.052e+03  1.057e+03  1.058e+03\n",
      " -1.063e+03  1.067e+03 -1.100e+03 -1.106e+03  1.107e+03 -1.111e+03\n",
      " -1.113e+03  1.114e+03  1.143e+03  1.154e+03  1.161e+03 -1.176e+03\n",
      "  1.189e+03  1.190e+03 -1.196e+03 -1.212e+03 -1.217e+03 -1.223e+03\n",
      " -1.244e+03 -1.259e+03 -1.260e+03 -1.280e+03 -1.311e+03  1.315e+03\n",
      "  1.335e+03 -1.336e+03  1.338e+03  1.344e+03  1.346e+03 -1.361e+03\n",
      " -1.386e+03  1.423e+03  1.426e+03  1.470e+03 -1.475e+03  1.485e+03\n",
      " -1.499e+03 -1.519e+03 -1.531e+03 -1.541e+03 -1.602e+03 -1.639e+03\n",
      "  1.640e+03  1.656e+03  1.679e+03 -1.690e+03  1.695e+03  1.713e+03\n",
      " -1.723e+03 -1.731e+03  1.736e+03 -1.784e+03 -1.841e+03  1.841e+03\n",
      "  1.874e+03 -1.884e+03  1.912e+03  1.933e+03  1.953e+03 -1.963e+03\n",
      "  1.975e+03  1.984e+03 -1.992e+03 -1.997e+03 -2.023e+03 -2.024e+03\n",
      " -2.026e+03 -2.080e+03  2.086e+03 -2.133e+03  2.133e+03 -2.142e+03\n",
      " -2.148e+03  2.160e+03 -2.165e+03  2.170e+03  2.224e+03 -2.227e+03\n",
      " -2.271e+03  2.284e+03  2.304e+03 -2.336e+03  2.378e+03 -2.381e+03\n",
      " -2.419e+03 -2.447e+03 -2.471e+03 -2.510e+03 -2.542e+03 -2.716e+03\n",
      "  2.803e+03 -2.820e+03  2.979e+03 -3.016e+03 -3.123e+03  3.149e+03\n",
      " -3.189e+03  3.253e+03  3.376e+03 -3.387e+03 -3.636e+03 -3.644e+03\n",
      " -3.747e+03  3.838e+03 -3.850e+03 -4.026e+03 -4.053e+03 -4.261e+03\n",
      " -4.570e+03  4.601e+03 -4.694e+03 -4.765e+03]\n",
      "Concordance Index 0.4926478905101233\n",
      "Integrated Brier Score: 0.1932948382239094\n",
      "y_train breslow final [ 1.000e+00  2.000e+00  3.000e+00 -4.000e+00  5.000e+00  5.000e+00\n",
      "  6.000e+00 -8.000e+00  9.000e+00 -1.200e+01  1.200e+01  1.200e+01\n",
      " -1.300e+01 -1.500e+01  2.300e+01  2.400e+01 -2.800e+01 -3.000e+01\n",
      " -3.000e+01 -3.000e+01 -3.100e+01 -3.100e+01  3.400e+01 -3.500e+01\n",
      " -3.700e+01 -4.100e+01 -4.200e+01  4.700e+01  5.200e+01  5.300e+01\n",
      " -5.500e+01  5.900e+01 -6.000e+01 -6.100e+01 -6.100e+01 -6.100e+01\n",
      " -6.100e+01 -6.100e+01 -6.100e+01  6.100e+01 -7.000e+01  8.000e+01\n",
      " -8.200e+01 -8.300e+01  8.500e+01  8.800e+01  8.900e+01  9.200e+01\n",
      "  9.400e+01 -9.700e+01 -1.030e+02 -1.060e+02 -1.060e+02  1.090e+02\n",
      " -1.110e+02  1.130e+02 -1.210e+02 -1.220e+02 -1.220e+02 -1.220e+02\n",
      " -1.220e+02  1.230e+02  1.230e+02  1.310e+02 -1.350e+02  1.430e+02\n",
      "  1.460e+02 -1.500e+02  1.510e+02 -1.530e+02  1.530e+02 -1.600e+02\n",
      "  1.660e+02 -1.730e+02  1.880e+02  1.950e+02 -2.070e+02 -2.110e+02\n",
      "  2.110e+02  2.120e+02 -2.270e+02 -2.360e+02  2.360e+02 -2.380e+02\n",
      " -2.440e+02 -2.470e+02  2.610e+02  2.660e+02  2.760e+02  2.840e+02\n",
      " -2.890e+02  2.910e+02 -2.960e+02  2.990e+02  3.020e+02  3.060e+02\n",
      "  3.070e+02  3.110e+02  3.220e+02  3.290e+02  3.400e+02  3.450e+02\n",
      "  3.450e+02  3.510e+02 -3.530e+02  3.570e+02  3.580e+02 -3.580e+02\n",
      " -3.580e+02  3.580e+02  3.600e+02  3.650e+02  3.660e+02 -3.660e+02\n",
      " -3.670e+02 -3.700e+02  3.710e+02 -3.750e+02 -3.770e+02 -3.780e+02\n",
      "  3.830e+02  3.830e+02  3.870e+02 -3.920e+02 -3.960e+02  3.970e+02\n",
      "  3.990e+02 -4.010e+02  4.030e+02 -4.050e+02 -4.050e+02 -4.060e+02\n",
      " -4.070e+02  4.080e+02  4.180e+02 -4.200e+02 -4.230e+02  4.280e+02\n",
      " -4.280e+02  4.290e+02 -4.300e+02 -4.320e+02  4.420e+02  4.450e+02\n",
      " -4.550e+02  4.550e+02  4.560e+02 -4.650e+02 -4.680e+02  4.740e+02\n",
      " -4.810e+02 -4.820e+02 -4.910e+02 -4.920e+02 -4.930e+02 -4.980e+02\n",
      "  5.010e+02  5.060e+02 -5.100e+02 -5.110e+02  5.150e+02 -5.170e+02\n",
      "  5.190e+02 -5.310e+02  5.320e+02 -5.380e+02  5.390e+02  5.430e+02\n",
      "  5.440e+02  5.500e+02  5.520e+02 -5.570e+02  5.590e+02 -5.590e+02\n",
      "  5.620e+02 -5.650e+02 -5.700e+02  5.730e+02 -5.760e+02 -5.780e+02\n",
      " -5.780e+02 -5.810e+02 -5.850e+02  5.870e+02 -5.890e+02 -6.010e+02\n",
      "  6.040e+02 -6.080e+02 -6.150e+02 -6.160e+02 -6.180e+02  6.230e+02\n",
      " -6.300e+02  6.320e+02 -6.370e+02 -6.390e+02 -6.400e+02 -6.400e+02\n",
      " -6.410e+02 -6.420e+02 -6.490e+02 -6.520e+02 -6.530e+02 -6.600e+02\n",
      " -6.660e+02  6.670e+02  6.690e+02 -6.710e+02 -6.830e+02  6.840e+02\n",
      "  6.870e+02  6.920e+02 -6.990e+02 -6.990e+02 -6.990e+02  7.080e+02\n",
      "  7.160e+02 -7.170e+02  7.340e+02  7.400e+02 -7.580e+02 -7.590e+02\n",
      " -7.690e+02  7.700e+02 -7.900e+02  8.030e+02 -8.040e+02 -8.040e+02\n",
      " -8.100e+02 -8.150e+02 -8.160e+02 -8.180e+02 -8.220e+02  8.220e+02\n",
      "  8.260e+02  8.270e+02 -8.330e+02  8.350e+02  8.400e+02 -8.490e+02\n",
      " -8.620e+02 -8.670e+02  8.810e+02  8.990e+02 -9.080e+02 -9.100e+02\n",
      " -9.100e+02 -9.110e+02 -9.110e+02 -9.130e+02  9.160e+02  9.210e+02\n",
      " -9.230e+02 -9.250e+02 -9.320e+02  9.330e+02  9.370e+02 -9.370e+02\n",
      " -9.420e+02  9.510e+02 -9.520e+02 -9.600e+02  9.620e+02  9.730e+02\n",
      "  9.740e+02 -9.790e+02 -9.830e+02 -9.900e+02 -9.950e+02  1.000e+03\n",
      "  1.001e+03  1.006e+03 -1.007e+03 -1.011e+03 -1.031e+03 -1.032e+03\n",
      " -1.038e+03  1.045e+03 -1.050e+03 -1.052e+03  1.057e+03  1.058e+03\n",
      " -1.063e+03  1.067e+03 -1.100e+03 -1.106e+03  1.107e+03 -1.111e+03\n",
      " -1.113e+03  1.114e+03  1.143e+03  1.154e+03  1.161e+03 -1.176e+03\n",
      "  1.189e+03  1.190e+03 -1.196e+03 -1.212e+03 -1.217e+03 -1.223e+03\n",
      " -1.244e+03 -1.259e+03 -1.260e+03 -1.280e+03 -1.311e+03  1.315e+03\n",
      "  1.335e+03 -1.336e+03  1.338e+03  1.344e+03  1.346e+03 -1.361e+03\n",
      " -1.386e+03  1.423e+03  1.426e+03  1.470e+03 -1.475e+03  1.485e+03\n",
      " -1.499e+03 -1.519e+03 -1.531e+03 -1.541e+03 -1.602e+03 -1.639e+03\n",
      "  1.640e+03  1.656e+03  1.679e+03 -1.690e+03  1.695e+03  1.713e+03\n",
      " -1.723e+03 -1.731e+03  1.736e+03 -1.784e+03 -1.841e+03  1.841e+03\n",
      "  1.874e+03 -1.884e+03  1.912e+03  1.933e+03  1.953e+03 -1.963e+03\n",
      "  1.975e+03  1.984e+03 -1.992e+03 -1.997e+03 -2.023e+03 -2.024e+03\n",
      " -2.026e+03 -2.080e+03  2.086e+03 -2.133e+03  2.133e+03 -2.142e+03\n",
      " -2.148e+03  2.160e+03 -2.165e+03  2.170e+03  2.224e+03 -2.227e+03\n",
      " -2.271e+03  2.284e+03  2.304e+03 -2.336e+03  2.378e+03 -2.381e+03\n",
      " -2.419e+03 -2.447e+03 -2.471e+03 -2.510e+03 -2.542e+03 -2.716e+03\n",
      "  2.803e+03 -2.820e+03  2.979e+03 -3.016e+03 -3.123e+03  3.149e+03\n",
      " -3.189e+03  3.253e+03  3.376e+03 -3.387e+03 -3.636e+03 -3.644e+03\n",
      " -3.747e+03  3.838e+03 -3.850e+03 -4.026e+03 -4.053e+03 -4.261e+03\n",
      " -4.570e+03  4.601e+03 -4.694e+03 -4.765e+03]\n",
      "durations 12.0 5287.0\n",
      "Concordance Index 0.50709067468844\n",
      "Integrated Brier Score: 0.18994193636499657\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m382.0060\u001b[0m  0.0026\n",
      "      2      382.0060  0.0020\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      3      382.0060  0.0020\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      4      382.0060  0.0019\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m106.7526\u001b[0m  0.0031\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      5      382.0060  0.0044\n",
      "      2      106.7526  0.0028\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m266.5128\u001b[0m  0.0026\n",
      "      3      106.7526  0.0028\n",
      "  epoch    valid_loss     dur      6      382.0060  0.0052\n",
      "\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m178.5278\u001b[0m  0.0024\n",
      "      2      266.5128  0.0020\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m269.4631\u001b[0m  0.0023\n",
      "      4      106.7526  0.0028\n",
      "      3      266.5128  0.0019\n",
      "      7      382.0060  0.0022\n",
      "      2      269.4631  0.0019\n",
      "      4      266.5128  0.0018\n",
      "      8      382.0060  0.0019\n",
      "      3      269.4631  0.0018\n",
      "      5      106.7526  0.0029\n",
      "      2      178.5278  0.0051\n",
      "      5      266.5128  0.0019\n",
      "      4      269.4631  0.0018\n",
      "      9      382.0060  0.0018\n",
      "      6      266.5128  0.0018\n",
      "      5      269.4631  0.0018\n",
      "     10      382.0060  0.0019\n",
      "Restoring best model from epoch 1.\n",
      "      7      266.5128  0.0018\n",
      "      6      269.4631  0.0018\n",
      "      3      178.5278  0.0046\n",
      "      8      266.5128  0.0018\n",
      "      7      269.4631  0.0017\n",
      "      4      178.5278  0.0020\n",
      "      6      106.7526  0.0074\n",
      "      9      266.5128  0.0018\n",
      "      8      269.4631  0.0017\n",
      "      5      178.5278  0.0018\n",
      "     10      266.5128  0.0018\n",
      "Restoring best model from epoch 1.\n",
      "      9      269.4631  0.0017\n",
      "      6      178.5278  0.0018\n",
      "      7      106.7526  0.0038\n",
      "     10      269.4631  0.0017\n",
      "Restoring best model from epoch 1.\n",
      "      7      178.5278  0.0018\n",
      "      8      106.7526  0.0028\n",
      "      8      178.5278  0.0018\n",
      "      9      178.5278  0.0018\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      9      106.7526  0.0028\n",
      "     10      178.5278  0.0018\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing criterion.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing optimizer.\n",
      "     10      106.7526  0.0027\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m87.3122\u001b[0m  0.0031\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m264.6351\u001b[0m  0.0023\n",
      "      2       87.3122  0.0027\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m116.3670\u001b[0m  0.0033\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m119.3021\u001b[0m  0.0030\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m94.8276\u001b[0m  0.0032      3       87.3122  0.0029\n",
      "      2      116.3670  0.0028\n",
      "      2      264.6351  0.0044\n",
      "\n",
      "      2      119.3021  0.0028\n",
      "      3      264.6351  0.0019\n",
      "      4       87.3122  0.0028\n",
      "      2       94.8276  0.0028\n",
      "      3      116.3670  0.0044\n",
      "      5       87.3122  0.0028\n",
      "      4      264.6351  0.0027\n",
      "      3       94.8276  0.0026\n",
      "      4      116.3670  0.0027\n",
      "      6       87.3122  0.0027\n",
      "      4       94.8276  0.0032\n",
      "      3      119.3021  0.0077\n",
      "      5      116.3670  0.0026\n",
      "      7       87.3122  0.0027\n",
      "      5       94.8276  0.0027\n",
      "      6      116.3670  0.0026\n",
      "      4      119.3021  0.0039\n",
      "      8       87.3122  0.0026\n",
      "      5      264.6351  0.0086\n",
      "      6       94.8276  0.0026\n",
      "      6      264.6351  0.0020\n",
      "      5      119.3021  0.0027\n",
      "      9       87.3122  0.0026\n",
      "      7      116.3670  0.0039\n",
      "      7       94.8276  0.0026\n",
      "      7      264.6351  0.0019\n",
      "      6      119.3021  0.0027\n",
      "     10       87.3122  0.0025\n",
      "      8      116.3670  0.0028\n",
      "Restoring best model from epoch 1.\n",
      "      8      264.6351  0.0019\n",
      "      8       94.8276  0.0026\n",
      "      9      264.6351  0.0018      9      116.3670  0.0027\n",
      "\n",
      "      7      119.3021  0.0031\n",
      "      9       94.8276  0.0026\n",
      "     10      264.6351  0.0022\n",
      "Restoring best model from epoch 1.\n",
      "      8      119.3021  0.0026\n",
      "     10       94.8276  0.0026\n",
      "     10      116.3670  0.0036\n",
      "Restoring best model from epoch 1.\n",
      "Restoring best model from epoch 1.\n",
      "      9      119.3021  0.0026\n",
      "     10      119.3021  0.0026\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m135.1295\u001b[0m  0.0034\n",
      "      2      135.1295  0.0039\n",
      "      3      135.1295  0.0037\n",
      "      4      135.1295  0.0043\n",
      "      5      135.1295  0.0048\n",
      "      6      135.1295  0.0041\n",
      "      7      135.1295  0.0080\n",
      "      8      135.1295  0.0076\n",
      "      9      135.1295  0.0039\n",
      "     10      135.1295  0.0044\n",
      "Restoring best model from epoch 1.\n",
      "y_train breslow final [ 1.000e+00  2.000e+00 -4.000e+00  5.000e+00  5.000e+00  6.000e+00\n",
      " -8.000e+00  9.000e+00 -1.200e+01  1.200e+01 -1.200e+01  1.200e+01\n",
      " -1.300e+01 -1.300e+01  1.700e+01  2.400e+01 -2.800e+01 -3.000e+01\n",
      " -3.000e+01 -3.000e+01 -3.500e+01 -3.700e+01 -4.100e+01 -4.200e+01\n",
      "  4.700e+01  5.200e+01  5.300e+01 -5.500e+01 -6.100e+01 -6.100e+01\n",
      " -6.100e+01  6.100e+01 -7.000e+01  8.000e+01 -8.200e+01  8.400e+01\n",
      "  8.500e+01  8.800e+01 -8.900e+01  9.200e+01 -9.700e+01 -1.050e+02\n",
      " -1.060e+02 -1.060e+02 -1.110e+02  1.130e+02  1.160e+02 -1.210e+02\n",
      " -1.220e+02 -1.220e+02  1.230e+02  1.230e+02 -1.280e+02 -1.350e+02\n",
      "  1.360e+02  1.380e+02  1.430e+02  1.460e+02 -1.530e+02  1.530e+02\n",
      " -1.600e+02  1.610e+02  1.660e+02  1.660e+02  1.800e+02  1.950e+02\n",
      "  1.980e+02  1.980e+02 -2.020e+02 -2.070e+02 -2.110e+02  2.110e+02\n",
      "  2.120e+02  2.150e+02  2.230e+02 -2.270e+02 -2.360e+02  2.360e+02\n",
      " -2.380e+02 -2.420e+02 -2.440e+02 -2.470e+02  2.610e+02  2.660e+02\n",
      "  2.740e+02  2.760e+02  2.840e+02 -2.890e+02  2.910e+02  2.940e+02\n",
      " -2.960e+02  2.990e+02  3.020e+02  3.060e+02  3.070e+02  3.110e+02\n",
      "  3.150e+02 -3.240e+02  3.290e+02  3.400e+02  3.400e+02  3.450e+02\n",
      "  3.450e+02  3.510e+02 -3.530e+02  3.570e+02  3.580e+02  3.580e+02\n",
      "  3.650e+02 -3.660e+02 -3.670e+02  3.710e+02  3.720e+02 -3.750e+02\n",
      " -3.780e+02  3.830e+02  3.830e+02  3.870e+02 -3.920e+02 -3.960e+02\n",
      "  3.970e+02  3.990e+02 -4.010e+02  4.020e+02  4.030e+02 -4.050e+02\n",
      " -4.050e+02 -4.060e+02 -4.070e+02  4.120e+02 -4.200e+02 -4.230e+02\n",
      "  4.280e+02 -4.280e+02 -4.280e+02  4.290e+02 -4.320e+02 -4.400e+02\n",
      "  4.420e+02 -4.480e+02  4.480e+02  4.540e+02 -4.550e+02  4.550e+02\n",
      " -4.650e+02 -4.680e+02  4.740e+02 -4.810e+02  4.900e+02 -4.910e+02\n",
      " -4.920e+02 -4.930e+02  5.010e+02  5.060e+02 -5.100e+02 -5.100e+02\n",
      "  5.160e+02 -5.170e+02  5.190e+02 -5.310e+02  5.320e+02 -5.380e+02\n",
      "  5.390e+02  5.440e+02  5.500e+02  5.520e+02 -5.550e+02 -5.570e+02\n",
      "  5.590e+02 -5.590e+02 -5.650e+02  5.730e+02 -5.760e+02 -5.780e+02\n",
      " -5.790e+02  5.870e+02 -5.890e+02 -5.960e+02 -6.000e+02 -6.010e+02\n",
      "  6.010e+02 -6.080e+02 -6.150e+02 -6.160e+02 -6.180e+02  6.230e+02\n",
      " -6.370e+02 -6.390e+02 -6.400e+02 -6.420e+02  6.450e+02 -6.490e+02\n",
      " -6.520e+02 -6.530e+02 -6.600e+02 -6.660e+02  6.670e+02  6.690e+02\n",
      " -6.710e+02  6.780e+02 -6.830e+02  6.870e+02 -6.880e+02  6.920e+02\n",
      " -6.990e+02 -6.990e+02 -6.990e+02 -7.000e+02  7.080e+02  7.340e+02\n",
      " -7.340e+02 -7.410e+02 -7.570e+02 -7.580e+02 -7.590e+02 -7.620e+02\n",
      " -7.650e+02  7.700e+02  8.030e+02 -8.040e+02 -8.100e+02 -8.150e+02\n",
      " -8.180e+02 -8.220e+02  8.260e+02  8.260e+02 -8.260e+02  8.270e+02\n",
      " -8.330e+02 -8.420e+02 -8.620e+02 -8.670e+02  8.810e+02  8.990e+02\n",
      " -9.080e+02 -9.100e+02 -9.110e+02 -9.110e+02 -9.130e+02  9.160e+02\n",
      "  9.210e+02 -9.230e+02 -9.250e+02  9.270e+02 -9.320e+02  9.370e+02\n",
      " -9.370e+02 -9.520e+02 -9.600e+02  9.620e+02  9.650e+02  9.730e+02\n",
      " -9.790e+02 -9.830e+02 -9.900e+02 -9.950e+02  1.000e+03  1.006e+03\n",
      " -1.011e+03 -1.031e+03 -1.038e+03  1.045e+03 -1.050e+03 -1.052e+03\n",
      "  1.057e+03  1.058e+03 -1.058e+03 -1.063e+03  1.067e+03 -1.072e+03\n",
      "  1.075e+03 -1.092e+03  1.097e+03 -1.100e+03  1.107e+03 -1.111e+03\n",
      " -1.113e+03  1.114e+03  1.143e+03  1.150e+03  1.154e+03 -1.160e+03\n",
      "  1.161e+03 -1.180e+03 -1.182e+03  1.189e+03  1.190e+03 -1.212e+03\n",
      " -1.217e+03 -1.223e+03 -1.224e+03 -1.244e+03 -1.259e+03 -1.260e+03\n",
      " -1.268e+03 -1.297e+03 -1.311e+03  1.335e+03  1.344e+03  1.346e+03\n",
      " -1.361e+03 -1.386e+03  1.426e+03  1.470e+03 -1.475e+03  1.485e+03\n",
      " -1.492e+03 -1.499e+03 -1.505e+03 -1.519e+03 -1.541e+03 -1.602e+03\n",
      " -1.639e+03  1.640e+03 -1.645e+03  1.655e+03  1.656e+03  1.679e+03\n",
      " -1.682e+03  1.695e+03 -1.723e+03 -1.731e+03  1.736e+03 -1.743e+03\n",
      " -1.784e+03 -1.824e+03 -1.834e+03 -1.841e+03  1.841e+03 -1.845e+03\n",
      "  1.856e+03  1.874e+03 -1.884e+03  1.912e+03 -1.927e+03  1.953e+03\n",
      "  1.975e+03 -1.992e+03 -1.997e+03 -2.023e+03 -2.024e+03 -2.026e+03\n",
      " -2.073e+03 -2.080e+03  2.086e+03 -2.134e+03 -2.142e+03 -2.148e+03\n",
      "  2.160e+03 -2.165e+03 -2.167e+03  2.170e+03  2.224e+03 -2.227e+03\n",
      " -2.271e+03 -2.336e+03 -2.381e+03  2.409e+03 -2.419e+03 -2.447e+03\n",
      " -2.471e+03 -2.510e+03 -2.542e+03 -2.589e+03  2.625e+03  2.639e+03\n",
      " -2.645e+03  2.680e+03 -2.716e+03  2.803e+03 -2.811e+03  2.945e+03\n",
      "  2.979e+03 -3.016e+03 -3.108e+03 -3.123e+03  3.149e+03 -3.166e+03\n",
      " -3.189e+03  3.253e+03  3.376e+03 -3.576e+03  3.600e+03 -3.644e+03\n",
      " -3.724e+03 -3.747e+03  3.838e+03 -3.850e+03  3.924e+03 -4.026e+03\n",
      " -4.053e+03 -4.068e+03  4.601e+03 -4.694e+03  5.287e+03]\n",
      "Concordance Index 0.5320161996497373\n",
      "Integrated Brier Score: 0.17791216964965256\n",
      "y_train breslow final [ 1.000e+00  2.000e+00 -4.000e+00  5.000e+00  5.000e+00  6.000e+00\n",
      " -8.000e+00  9.000e+00 -1.200e+01  1.200e+01 -1.200e+01  1.200e+01\n",
      " -1.300e+01 -1.300e+01  1.700e+01  2.400e+01 -2.800e+01 -3.000e+01\n",
      " -3.000e+01 -3.000e+01 -3.500e+01 -3.700e+01 -4.100e+01 -4.200e+01\n",
      "  4.700e+01  5.200e+01  5.300e+01 -5.500e+01 -6.100e+01 -6.100e+01\n",
      " -6.100e+01  6.100e+01 -7.000e+01  8.000e+01 -8.200e+01  8.400e+01\n",
      "  8.500e+01  8.800e+01 -8.900e+01  9.200e+01 -9.700e+01 -1.050e+02\n",
      " -1.060e+02 -1.060e+02 -1.110e+02  1.130e+02  1.160e+02 -1.210e+02\n",
      " -1.220e+02 -1.220e+02  1.230e+02  1.230e+02 -1.280e+02 -1.350e+02\n",
      "  1.360e+02  1.380e+02  1.430e+02  1.460e+02 -1.530e+02  1.530e+02\n",
      " -1.600e+02  1.610e+02  1.660e+02  1.660e+02  1.800e+02  1.950e+02\n",
      "  1.980e+02  1.980e+02 -2.020e+02 -2.070e+02 -2.110e+02  2.110e+02\n",
      "  2.120e+02  2.150e+02  2.230e+02 -2.270e+02 -2.360e+02  2.360e+02\n",
      " -2.380e+02 -2.420e+02 -2.440e+02 -2.470e+02  2.610e+02  2.660e+02\n",
      "  2.740e+02  2.760e+02  2.840e+02 -2.890e+02  2.910e+02  2.940e+02\n",
      " -2.960e+02  2.990e+02  3.020e+02  3.060e+02  3.070e+02  3.110e+02\n",
      "  3.150e+02 -3.240e+02  3.290e+02  3.400e+02  3.400e+02  3.450e+02\n",
      "  3.450e+02  3.510e+02 -3.530e+02  3.570e+02  3.580e+02  3.580e+02\n",
      "  3.650e+02 -3.660e+02 -3.670e+02  3.710e+02  3.720e+02 -3.750e+02\n",
      " -3.780e+02  3.830e+02  3.830e+02  3.870e+02 -3.920e+02 -3.960e+02\n",
      "  3.970e+02  3.990e+02 -4.010e+02  4.020e+02  4.030e+02 -4.050e+02\n",
      " -4.050e+02 -4.060e+02 -4.070e+02  4.120e+02 -4.200e+02 -4.230e+02\n",
      "  4.280e+02 -4.280e+02 -4.280e+02  4.290e+02 -4.320e+02 -4.400e+02\n",
      "  4.420e+02 -4.480e+02  4.480e+02  4.540e+02 -4.550e+02  4.550e+02\n",
      " -4.650e+02 -4.680e+02  4.740e+02 -4.810e+02  4.900e+02 -4.910e+02\n",
      " -4.920e+02 -4.930e+02  5.010e+02  5.060e+02 -5.100e+02 -5.100e+02\n",
      "  5.160e+02 -5.170e+02  5.190e+02 -5.310e+02  5.320e+02 -5.380e+02\n",
      "  5.390e+02  5.440e+02  5.500e+02  5.520e+02 -5.550e+02 -5.570e+02\n",
      "  5.590e+02 -5.590e+02 -5.650e+02  5.730e+02 -5.760e+02 -5.780e+02\n",
      " -5.790e+02  5.870e+02 -5.890e+02 -5.960e+02 -6.000e+02 -6.010e+02\n",
      "  6.010e+02 -6.080e+02 -6.150e+02 -6.160e+02 -6.180e+02  6.230e+02\n",
      " -6.370e+02 -6.390e+02 -6.400e+02 -6.420e+02  6.450e+02 -6.490e+02\n",
      " -6.520e+02 -6.530e+02 -6.600e+02 -6.660e+02  6.670e+02  6.690e+02\n",
      " -6.710e+02  6.780e+02 -6.830e+02  6.870e+02 -6.880e+02  6.920e+02\n",
      " -6.990e+02 -6.990e+02 -6.990e+02 -7.000e+02  7.080e+02  7.340e+02\n",
      " -7.340e+02 -7.410e+02 -7.570e+02 -7.580e+02 -7.590e+02 -7.620e+02\n",
      " -7.650e+02  7.700e+02  8.030e+02 -8.040e+02 -8.100e+02 -8.150e+02\n",
      " -8.180e+02 -8.220e+02  8.260e+02  8.260e+02 -8.260e+02  8.270e+02\n",
      " -8.330e+02 -8.420e+02 -8.620e+02 -8.670e+02  8.810e+02  8.990e+02\n",
      " -9.080e+02 -9.100e+02 -9.110e+02 -9.110e+02 -9.130e+02  9.160e+02\n",
      "  9.210e+02 -9.230e+02 -9.250e+02  9.270e+02 -9.320e+02  9.370e+02\n",
      " -9.370e+02 -9.520e+02 -9.600e+02  9.620e+02  9.650e+02  9.730e+02\n",
      " -9.790e+02 -9.830e+02 -9.900e+02 -9.950e+02  1.000e+03  1.006e+03\n",
      " -1.011e+03 -1.031e+03 -1.038e+03  1.045e+03 -1.050e+03 -1.052e+03\n",
      "  1.057e+03  1.058e+03 -1.058e+03 -1.063e+03  1.067e+03 -1.072e+03\n",
      "  1.075e+03 -1.092e+03  1.097e+03 -1.100e+03  1.107e+03 -1.111e+03\n",
      " -1.113e+03  1.114e+03  1.143e+03  1.150e+03  1.154e+03 -1.160e+03\n",
      "  1.161e+03 -1.180e+03 -1.182e+03  1.189e+03  1.190e+03 -1.212e+03\n",
      " -1.217e+03 -1.223e+03 -1.224e+03 -1.244e+03 -1.259e+03 -1.260e+03\n",
      " -1.268e+03 -1.297e+03 -1.311e+03  1.335e+03  1.344e+03  1.346e+03\n",
      " -1.361e+03 -1.386e+03  1.426e+03  1.470e+03 -1.475e+03  1.485e+03\n",
      " -1.492e+03 -1.499e+03 -1.505e+03 -1.519e+03 -1.541e+03 -1.602e+03\n",
      " -1.639e+03  1.640e+03 -1.645e+03  1.655e+03  1.656e+03  1.679e+03\n",
      " -1.682e+03  1.695e+03 -1.723e+03 -1.731e+03  1.736e+03 -1.743e+03\n",
      " -1.784e+03 -1.824e+03 -1.834e+03 -1.841e+03  1.841e+03 -1.845e+03\n",
      "  1.856e+03  1.874e+03 -1.884e+03  1.912e+03 -1.927e+03  1.953e+03\n",
      "  1.975e+03 -1.992e+03 -1.997e+03 -2.023e+03 -2.024e+03 -2.026e+03\n",
      " -2.073e+03 -2.080e+03  2.086e+03 -2.134e+03 -2.142e+03 -2.148e+03\n",
      "  2.160e+03 -2.165e+03 -2.167e+03  2.170e+03  2.224e+03 -2.227e+03\n",
      " -2.271e+03 -2.336e+03 -2.381e+03  2.409e+03 -2.419e+03 -2.447e+03\n",
      " -2.471e+03 -2.510e+03 -2.542e+03 -2.589e+03  2.625e+03  2.639e+03\n",
      " -2.645e+03  2.680e+03 -2.716e+03  2.803e+03 -2.811e+03  2.945e+03\n",
      "  2.979e+03 -3.016e+03 -3.108e+03 -3.123e+03  3.149e+03 -3.166e+03\n",
      " -3.189e+03  3.253e+03  3.376e+03 -3.576e+03  3.600e+03 -3.644e+03\n",
      " -3.724e+03 -3.747e+03  3.838e+03 -3.850e+03  3.924e+03 -4.026e+03\n",
      " -4.053e+03 -4.068e+03  4.601e+03 -4.694e+03  5.287e+03]\n",
      "durations 3.0 4765.0\n",
      "Concordance Index 0.5825621042377009\n",
      "Integrated Brier Score: 0.19109648565934462\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m221.5611\u001b[0m  0.0024\n",
      "      2      221.5611  0.0020\n",
      "      3      221.5611  0.0019\n",
      "      4      221.5611  0.0018\n",
      "      5      221.5611  0.0024\n",
      "      6      221.5611  0.0020\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      7      221.5611  0.0019\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "\n",
      "Re-initializing optimizer.\n",
      "      8      221.5611  0.0019\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m313.8798\u001b[0m  0.0024\n",
      "      9      221.5611  0.0018\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2      313.8798  0.0019\n",
      "     10      221.5611  0.0018\n",
      "Restoring best model from epoch 1.\n",
      "      3      313.8798  0.0018\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m237.4372\u001b[0m  0.0051\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m111.4244\u001b[0m  0.0031\n",
      "      4      313.8798  0.0018\n",
      "      2      237.4372  0.0020\n",
      "      2      111.4244  0.0028\n",
      "      5      313.8798  0.0019\n",
      "      3      237.4372  0.0019\n",
      "      6      313.8798  0.0019\n",
      "      3      111.4244  0.0028\n",
      "      4      237.4372  0.0019\n",
      "      7      313.8798  0.0017\n",
      "      5      237.4372  0.0018\n",
      "      4      111.4244  0.0027\n",
      "      8      313.8798  0.0017\n",
      "      6      237.4372  0.0018\n",
      "      9      313.8798  0.0017\n",
      "      5      111.4244  0.0026\n",
      "      7      237.4372  0.0017\n",
      "     10      313.8798  0.0017\n",
      "Restoring best model from epoch 1.\n",
      "      8      237.4372  0.0020\n",
      "      9      237.4372  0.0021      6      111.4244  0.0041\n",
      "\n",
      "     10      237.4372  0.0018\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      7      111.4244  0.0086\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m98.6412\u001b[0m  0.0032\n",
      "      8      111.4244  0.0030\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m109.9196\u001b[0m  0.0032\n",
      "      2       98.6412  0.0027\n",
      "      9      111.4244  0.0027\n",
      "      2      109.9196  0.0029\n",
      "      3       98.6412  0.0027\n",
      "     10      111.4244  0.0026\n",
      "Restoring best model from epoch 1.\n",
      "      3      109.9196  0.0027\n",
      "      4       98.6412  0.0027\n",
      "      4      109.9196  0.0027\n",
      "      5       98.6412  0.0026\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      5      109.9196  0.0027\n",
      "      6       98.6412  0.0026\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      7       98.6412  0.0026\n",
      "      8       98.6412  0.0026\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m99.6905\u001b[0m  0.0030\n",
      "      6      109.9196  0.0062\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      9       98.6412  0.0026\n",
      "      2       99.6905  0.0027\n",
      "     10       98.6412  0.0025\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Restoring best model from epoch 1.\n",
      "      3       99.6905  0.0026\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m246.9321\u001b[0m  0.0025\n",
      "      7      109.9196  0.0057\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2      246.9321  0.0018\n",
      "      4       99.6905  0.0026\n",
      "      3      246.9321  0.0017\n",
      "      8      109.9196  0.0028\n",
      "      5       99.6905  0.0026\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m92.1767\u001b[0m  0.0031\n",
      "      4      246.9321  0.0018\n",
      "      9      109.9196  0.0027\n",
      "      5      246.9321  0.0018\n",
      "      6       99.6905  0.0026\n",
      "      2       92.1767  0.0028\n",
      "     10      109.9196  0.0027\n",
      "Restoring best model from epoch 1.\n",
      "      7       99.6905  0.0026\n",
      "      6      246.9321  0.0028\n",
      "      3       92.1767  0.0026\n",
      "      8       99.6905  0.0026\n",
      "      4       92.1767  0.0026\n",
      "      7      246.9321  0.0048\n",
      "      9       99.6905  0.0026\n",
      "      5       92.1767  0.0026\n",
      "     10       99.6905  0.0026\n",
      "Restoring best model from epoch 1.\n",
      "      6       92.1767  0.0026\n",
      "      8      246.9321  0.0035\n",
      "      7       92.1767  0.0026\n",
      "      9      246.9321  0.0020\n",
      "     10      246.9321  0.0018\n",
      "Restoring best model from epoch 1.\n",
      "      8       92.1767  0.0026\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      9       92.1767  0.0039\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m260.5679\u001b[0m  0.0023\n",
      "      2      260.5679  0.0018\n",
      "      3      260.5679  0.0039\n",
      "     10       92.1767  0.0069\n",
      "Restoring best model from epoch 1.\n",
      "      4      260.5679  0.0020\n",
      "      5      260.5679  0.0017\n",
      "      6      260.5679  0.0017\n",
      "      7      260.5679  0.0017\n",
      "      8      260.5679  0.0032\n",
      "      9      260.5679  0.0044\n",
      "     10      260.5679  0.0018\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m107.2824\u001b[0m  0.0031\n",
      "      2      107.2824  0.0037\n",
      "      3      107.2824  0.0035\n",
      "      4      107.2824  0.0035\n",
      "      5      107.2824  0.0035\n",
      "      6      107.2824  0.0047\n",
      "      7      107.2824  0.0055\n",
      "      8      107.2824  0.0047\n",
      "      9      107.2824  0.0084\n",
      "     10      107.2824  0.0105\n",
      "Restoring best model from epoch 1.\n",
      "y_train breslow final [ 1.000e+00  3.000e+00 -4.000e+00  5.000e+00  5.000e+00 -8.000e+00\n",
      "  9.000e+00 -1.200e+01  1.200e+01 -1.300e+01 -1.300e+01 -1.500e+01\n",
      "  1.700e+01  2.300e+01 -2.800e+01 -3.000e+01 -3.100e+01 -3.100e+01\n",
      "  3.400e+01 -3.500e+01 -3.700e+01 -4.100e+01 -4.200e+01  4.700e+01\n",
      "  5.200e+01 -5.500e+01  5.900e+01 -6.000e+01 -6.100e+01 -6.100e+01\n",
      " -6.100e+01 -6.100e+01 -6.100e+01  6.100e+01 -7.000e+01 -8.200e+01\n",
      " -8.300e+01  8.400e+01  8.500e+01  8.800e+01 -8.900e+01  8.900e+01\n",
      "  9.400e+01 -1.030e+02 -1.050e+02 -1.060e+02 -1.060e+02  1.090e+02\n",
      " -1.110e+02 -1.110e+02  1.130e+02  1.160e+02 -1.220e+02 -1.220e+02\n",
      " -1.220e+02 -1.220e+02  1.230e+02 -1.280e+02  1.310e+02 -1.350e+02\n",
      "  1.360e+02  1.380e+02  1.430e+02  1.460e+02 -1.500e+02  1.510e+02\n",
      " -1.530e+02 -1.600e+02  1.610e+02  1.660e+02 -1.730e+02  1.800e+02\n",
      "  1.880e+02  1.980e+02  1.980e+02 -2.020e+02 -2.070e+02 -2.110e+02\n",
      "  2.110e+02  2.120e+02  2.150e+02  2.230e+02  2.360e+02 -2.380e+02\n",
      " -2.420e+02 -2.440e+02 -2.470e+02  2.610e+02  2.740e+02  2.760e+02\n",
      "  2.840e+02  2.940e+02 -2.960e+02  2.990e+02  3.020e+02  3.060e+02\n",
      "  3.110e+02  3.150e+02  3.220e+02 -3.240e+02  3.400e+02  3.400e+02\n",
      "  3.450e+02  3.510e+02  3.570e+02  3.580e+02 -3.580e+02 -3.580e+02\n",
      "  3.580e+02  3.600e+02  3.660e+02 -3.660e+02 -3.670e+02 -3.700e+02\n",
      "  3.710e+02  3.720e+02 -3.750e+02 -3.770e+02  3.830e+02  3.870e+02\n",
      " -3.920e+02 -3.960e+02  3.990e+02  4.020e+02  4.030e+02 -4.050e+02\n",
      " -4.050e+02 -4.060e+02 -4.070e+02  4.080e+02  4.120e+02  4.180e+02\n",
      " -4.200e+02 -4.230e+02  4.280e+02 -4.280e+02 -4.280e+02  4.290e+02\n",
      " -4.300e+02 -4.400e+02  4.420e+02  4.450e+02 -4.480e+02  4.480e+02\n",
      "  4.540e+02 -4.550e+02  4.550e+02  4.560e+02 -4.650e+02 -4.680e+02\n",
      "  4.740e+02 -4.820e+02  4.900e+02 -4.910e+02 -4.920e+02 -4.930e+02\n",
      " -4.980e+02  5.010e+02  5.060e+02 -5.100e+02 -5.110e+02  5.150e+02\n",
      "  5.160e+02 -5.170e+02  5.190e+02 -5.310e+02  5.320e+02 -5.380e+02\n",
      "  5.430e+02  5.440e+02  5.500e+02 -5.550e+02 -5.590e+02  5.620e+02\n",
      " -5.650e+02 -5.700e+02  5.730e+02 -5.760e+02 -5.780e+02 -5.790e+02\n",
      " -5.810e+02 -5.850e+02  5.870e+02 -5.890e+02 -5.960e+02 -6.000e+02\n",
      " -6.010e+02  6.010e+02  6.040e+02 -6.150e+02 -6.160e+02 -6.180e+02\n",
      " -6.300e+02  6.320e+02 -6.370e+02 -6.390e+02 -6.400e+02 -6.400e+02\n",
      " -6.410e+02 -6.420e+02  6.450e+02 -6.490e+02 -6.520e+02 -6.530e+02\n",
      " -6.600e+02  6.670e+02  6.690e+02 -6.710e+02  6.780e+02 -6.830e+02\n",
      "  6.840e+02 -6.880e+02  6.920e+02 -6.990e+02 -6.990e+02 -7.000e+02\n",
      "  7.160e+02 -7.170e+02  7.340e+02 -7.340e+02  7.400e+02 -7.410e+02\n",
      " -7.570e+02 -7.580e+02 -7.590e+02 -7.620e+02 -7.650e+02 -7.690e+02\n",
      "  7.700e+02 -7.900e+02  8.030e+02 -8.040e+02 -8.040e+02 -8.100e+02\n",
      " -8.150e+02 -8.160e+02 -8.180e+02  8.220e+02  8.260e+02  8.260e+02\n",
      " -8.260e+02  8.270e+02 -8.330e+02  8.350e+02  8.400e+02 -8.420e+02\n",
      " -8.490e+02 -8.620e+02  8.810e+02  8.990e+02 -9.100e+02 -9.100e+02\n",
      " -9.110e+02 -9.110e+02 -9.110e+02 -9.130e+02  9.210e+02 -9.230e+02\n",
      " -9.250e+02  9.270e+02 -9.320e+02  9.330e+02  9.370e+02 -9.420e+02\n",
      "  9.510e+02 -9.520e+02 -9.600e+02  9.620e+02  9.650e+02  9.730e+02\n",
      "  9.740e+02 -9.790e+02 -9.900e+02  1.001e+03 -1.007e+03 -1.011e+03\n",
      " -1.031e+03 -1.032e+03 -1.038e+03  1.045e+03 -1.050e+03 -1.052e+03\n",
      "  1.057e+03  1.058e+03 -1.058e+03 -1.063e+03  1.067e+03 -1.072e+03\n",
      "  1.075e+03 -1.092e+03  1.097e+03 -1.106e+03  1.107e+03  1.150e+03\n",
      "  1.154e+03 -1.160e+03  1.161e+03 -1.176e+03 -1.180e+03 -1.182e+03\n",
      "  1.189e+03 -1.196e+03 -1.223e+03 -1.224e+03 -1.244e+03 -1.259e+03\n",
      " -1.268e+03 -1.280e+03 -1.297e+03  1.315e+03  1.335e+03 -1.336e+03\n",
      "  1.338e+03  1.344e+03 -1.361e+03 -1.386e+03  1.423e+03  1.485e+03\n",
      " -1.492e+03 -1.499e+03 -1.505e+03 -1.531e+03 -1.602e+03 -1.639e+03\n",
      "  1.640e+03 -1.645e+03  1.655e+03  1.656e+03 -1.682e+03 -1.690e+03\n",
      "  1.713e+03 -1.731e+03  1.736e+03 -1.743e+03 -1.824e+03 -1.834e+03\n",
      " -1.841e+03  1.841e+03 -1.845e+03  1.856e+03  1.874e+03  1.912e+03\n",
      " -1.927e+03  1.933e+03 -1.963e+03  1.984e+03 -2.026e+03 -2.073e+03\n",
      " -2.080e+03  2.086e+03 -2.133e+03  2.133e+03 -2.134e+03 -2.142e+03\n",
      " -2.165e+03 -2.167e+03  2.170e+03  2.224e+03 -2.227e+03  2.284e+03\n",
      "  2.304e+03  2.378e+03 -2.381e+03  2.409e+03 -2.419e+03 -2.510e+03\n",
      " -2.542e+03 -2.589e+03  2.625e+03  2.639e+03 -2.645e+03  2.680e+03\n",
      " -2.716e+03  2.803e+03 -2.811e+03 -2.820e+03  2.945e+03 -3.108e+03\n",
      "  3.149e+03 -3.166e+03  3.376e+03 -3.387e+03 -3.576e+03  3.600e+03\n",
      " -3.636e+03 -3.644e+03 -3.724e+03  3.838e+03  3.924e+03 -4.026e+03\n",
      " -4.068e+03 -4.261e+03 -4.570e+03 -4.765e+03  5.287e+03]\n",
      "Concordance Index 0.5364797146173664\n",
      "Integrated Brier Score: 0.1780845053962166\n",
      "y_train breslow final [ 1.000e+00  3.000e+00 -4.000e+00  5.000e+00  5.000e+00 -8.000e+00\n",
      "  9.000e+00 -1.200e+01  1.200e+01 -1.300e+01 -1.300e+01 -1.500e+01\n",
      "  1.700e+01  2.300e+01 -2.800e+01 -3.000e+01 -3.100e+01 -3.100e+01\n",
      "  3.400e+01 -3.500e+01 -3.700e+01 -4.100e+01 -4.200e+01  4.700e+01\n",
      "  5.200e+01 -5.500e+01  5.900e+01 -6.000e+01 -6.100e+01 -6.100e+01\n",
      " -6.100e+01 -6.100e+01 -6.100e+01  6.100e+01 -7.000e+01 -8.200e+01\n",
      " -8.300e+01  8.400e+01  8.500e+01  8.800e+01 -8.900e+01  8.900e+01\n",
      "  9.400e+01 -1.030e+02 -1.050e+02 -1.060e+02 -1.060e+02  1.090e+02\n",
      " -1.110e+02 -1.110e+02  1.130e+02  1.160e+02 -1.220e+02 -1.220e+02\n",
      " -1.220e+02 -1.220e+02  1.230e+02 -1.280e+02  1.310e+02 -1.350e+02\n",
      "  1.360e+02  1.380e+02  1.430e+02  1.460e+02 -1.500e+02  1.510e+02\n",
      " -1.530e+02 -1.600e+02  1.610e+02  1.660e+02 -1.730e+02  1.800e+02\n",
      "  1.880e+02  1.980e+02  1.980e+02 -2.020e+02 -2.070e+02 -2.110e+02\n",
      "  2.110e+02  2.120e+02  2.150e+02  2.230e+02  2.360e+02 -2.380e+02\n",
      " -2.420e+02 -2.440e+02 -2.470e+02  2.610e+02  2.740e+02  2.760e+02\n",
      "  2.840e+02  2.940e+02 -2.960e+02  2.990e+02  3.020e+02  3.060e+02\n",
      "  3.110e+02  3.150e+02  3.220e+02 -3.240e+02  3.400e+02  3.400e+02\n",
      "  3.450e+02  3.510e+02  3.570e+02  3.580e+02 -3.580e+02 -3.580e+02\n",
      "  3.580e+02  3.600e+02  3.660e+02 -3.660e+02 -3.670e+02 -3.700e+02\n",
      "  3.710e+02  3.720e+02 -3.750e+02 -3.770e+02  3.830e+02  3.870e+02\n",
      " -3.920e+02 -3.960e+02  3.990e+02  4.020e+02  4.030e+02 -4.050e+02\n",
      " -4.050e+02 -4.060e+02 -4.070e+02  4.080e+02  4.120e+02  4.180e+02\n",
      " -4.200e+02 -4.230e+02  4.280e+02 -4.280e+02 -4.280e+02  4.290e+02\n",
      " -4.300e+02 -4.400e+02  4.420e+02  4.450e+02 -4.480e+02  4.480e+02\n",
      "  4.540e+02 -4.550e+02  4.550e+02  4.560e+02 -4.650e+02 -4.680e+02\n",
      "  4.740e+02 -4.820e+02  4.900e+02 -4.910e+02 -4.920e+02 -4.930e+02\n",
      " -4.980e+02  5.010e+02  5.060e+02 -5.100e+02 -5.110e+02  5.150e+02\n",
      "  5.160e+02 -5.170e+02  5.190e+02 -5.310e+02  5.320e+02 -5.380e+02\n",
      "  5.430e+02  5.440e+02  5.500e+02 -5.550e+02 -5.590e+02  5.620e+02\n",
      " -5.650e+02 -5.700e+02  5.730e+02 -5.760e+02 -5.780e+02 -5.790e+02\n",
      " -5.810e+02 -5.850e+02  5.870e+02 -5.890e+02 -5.960e+02 -6.000e+02\n",
      " -6.010e+02  6.010e+02  6.040e+02 -6.150e+02 -6.160e+02 -6.180e+02\n",
      " -6.300e+02  6.320e+02 -6.370e+02 -6.390e+02 -6.400e+02 -6.400e+02\n",
      " -6.410e+02 -6.420e+02  6.450e+02 -6.490e+02 -6.520e+02 -6.530e+02\n",
      " -6.600e+02  6.670e+02  6.690e+02 -6.710e+02  6.780e+02 -6.830e+02\n",
      "  6.840e+02 -6.880e+02  6.920e+02 -6.990e+02 -6.990e+02 -7.000e+02\n",
      "  7.160e+02 -7.170e+02  7.340e+02 -7.340e+02  7.400e+02 -7.410e+02\n",
      " -7.570e+02 -7.580e+02 -7.590e+02 -7.620e+02 -7.650e+02 -7.690e+02\n",
      "  7.700e+02 -7.900e+02  8.030e+02 -8.040e+02 -8.040e+02 -8.100e+02\n",
      " -8.150e+02 -8.160e+02 -8.180e+02  8.220e+02  8.260e+02  8.260e+02\n",
      " -8.260e+02  8.270e+02 -8.330e+02  8.350e+02  8.400e+02 -8.420e+02\n",
      " -8.490e+02 -8.620e+02  8.810e+02  8.990e+02 -9.100e+02 -9.100e+02\n",
      " -9.110e+02 -9.110e+02 -9.110e+02 -9.130e+02  9.210e+02 -9.230e+02\n",
      " -9.250e+02  9.270e+02 -9.320e+02  9.330e+02  9.370e+02 -9.420e+02\n",
      "  9.510e+02 -9.520e+02 -9.600e+02  9.620e+02  9.650e+02  9.730e+02\n",
      "  9.740e+02 -9.790e+02 -9.900e+02  1.001e+03 -1.007e+03 -1.011e+03\n",
      " -1.031e+03 -1.032e+03 -1.038e+03  1.045e+03 -1.050e+03 -1.052e+03\n",
      "  1.057e+03  1.058e+03 -1.058e+03 -1.063e+03  1.067e+03 -1.072e+03\n",
      "  1.075e+03 -1.092e+03  1.097e+03 -1.106e+03  1.107e+03  1.150e+03\n",
      "  1.154e+03 -1.160e+03  1.161e+03 -1.176e+03 -1.180e+03 -1.182e+03\n",
      "  1.189e+03 -1.196e+03 -1.223e+03 -1.224e+03 -1.244e+03 -1.259e+03\n",
      " -1.268e+03 -1.280e+03 -1.297e+03  1.315e+03  1.335e+03 -1.336e+03\n",
      "  1.338e+03  1.344e+03 -1.361e+03 -1.386e+03  1.423e+03  1.485e+03\n",
      " -1.492e+03 -1.499e+03 -1.505e+03 -1.531e+03 -1.602e+03 -1.639e+03\n",
      "  1.640e+03 -1.645e+03  1.655e+03  1.656e+03 -1.682e+03 -1.690e+03\n",
      "  1.713e+03 -1.731e+03  1.736e+03 -1.743e+03 -1.824e+03 -1.834e+03\n",
      " -1.841e+03  1.841e+03 -1.845e+03  1.856e+03  1.874e+03  1.912e+03\n",
      " -1.927e+03  1.933e+03 -1.963e+03  1.984e+03 -2.026e+03 -2.073e+03\n",
      " -2.080e+03  2.086e+03 -2.133e+03  2.133e+03 -2.134e+03 -2.142e+03\n",
      " -2.165e+03 -2.167e+03  2.170e+03  2.224e+03 -2.227e+03  2.284e+03\n",
      "  2.304e+03  2.378e+03 -2.381e+03  2.409e+03 -2.419e+03 -2.510e+03\n",
      " -2.542e+03 -2.589e+03  2.625e+03  2.639e+03 -2.645e+03  2.680e+03\n",
      " -2.716e+03  2.803e+03 -2.811e+03 -2.820e+03  2.945e+03 -3.108e+03\n",
      "  3.149e+03 -3.166e+03  3.376e+03 -3.387e+03 -3.576e+03  3.600e+03\n",
      " -3.636e+03 -3.644e+03 -3.724e+03  3.838e+03  3.924e+03 -4.026e+03\n",
      " -4.068e+03 -4.261e+03 -4.570e+03 -4.765e+03  5.287e+03]\n",
      "durations 2.0 4694.0\n",
      "Concordance Index 0.4883127921801955\n",
      "Integrated Brier Score: 0.2152095568137767\n",
      "split gex_?|100133144      float32\n",
      "gex_?|100134869      float32\n",
      "gex_?|10357          float32\n",
      "gex_?|10431          float32\n",
      "gex_?|155060         float32\n",
      "                      ...   \n",
      "gex_ZYG11A|440590    float32\n",
      "gex_ZYG11B|79699     float32\n",
      "gex_ZYX|7791         float32\n",
      "gex_ZZEF1|23140      float32\n",
      "gex_ZZZ3|26009       float32\n",
      "Length: 19076, dtype: object\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m285.8373\u001b[0m  0.0018\n",
      "      2      285.8373  0.0015\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      3      285.8373  0.0014\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      4      285.8373  0.0016\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m206.0049\u001b[0m  0.0018\n",
      "      5      285.8373  0.0013\n",
      "      6      285.8373  0.0013\n",
      "      2      206.0049  0.0014\n",
      "      3      206.0049  0.0012      7      285.8373  0.0012\n",
      "\n",
      "      4      206.0049  0.0012\n",
      "      8      285.8373  0.0012\n",
      "      5      206.0049  0.0012\n",
      "      9      285.8373  0.0012\n",
      "      6      206.0049  0.0012\n",
      "     10      285.8373  0.0012\n",
      "Restoring best model from epoch 1.\n",
      "      7      206.0049  0.0012\n",
      "      8      206.0049  0.0012\n",
      "      9      206.0049  0.0012\n",
      "     10      206.0049  0.0012\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m64.3283\u001b[0m  0.0024\n",
      "      2       64.3283  0.0020\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m228.8663\u001b[0m  0.0038\n",
      "      2      228.8663  0.0023\n",
      "      3       64.3283  0.0031\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      3      228.8663  0.0014\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      4      228.8663  0.0012\n",
      "      5      228.8663  0.0012\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m213.1623\u001b[0m  0.0021\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m229.2789\u001b[0m  0.0015\n",
      "      6      228.8663  0.0012\n",
      "      2      213.1623  0.0013\n",
      "      4       64.3283  0.0053\n",
      "      2      229.2789  0.0013\n",
      "      7      228.8663  0.0012\n",
      "      3      229.2789  0.0012\n",
      "      8      228.8663  0.0012\n",
      "      4      229.2789  0.0012\n",
      "      5       64.3283  0.0034\n",
      "      9      228.8663  0.0016\n",
      "      3      213.1623  0.0036\n",
      "      5      229.2789  0.0012\n",
      "     10      228.8663  0.0013\n",
      "Restoring best model from epoch 1.\n",
      "      6      229.2789  0.0012\n",
      "      6       64.3283  0.0020\n",
      "      7      229.2789  0.0012\n",
      "      4      213.1623  0.0033\n",
      "      7       64.3283  0.0019\n",
      "      8      229.2789  0.0012\n",
      "      8       64.3283  0.0018\n",
      "      9      229.2789  0.0016\n",
      "      5      213.1623  0.0031\n",
      "      9       64.3283  0.0019\n",
      "      6      213.1623  0.0016\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "     10       64.3283  0.0018\n",
      "      7      213.1623  0.0012\n",
      "Restoring best model from epoch 1.\n",
      "     10      229.2789  0.0037\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      8      213.1623  0.0012\n",
      "      9      213.1623  0.0012\n",
      "     10      213.1623  0.0012\n",
      "Restoring best model from epoch 1.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m64.6868\u001b[0m  0.0022\n",
      "      2       64.6868  0.0020\n",
      "      3       64.6868  0.0018\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      4       64.6868  0.0018\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      5       64.6868  0.0018\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      6       64.6868  0.0026\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m50.6923\u001b[0m  0.0021\n",
      "      2       50.6923  0.0018\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m65.0720\u001b[0m  0.0021\n",
      "      3       50.6923  0.0018\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      7       64.6868  0.0052\n",
      "      4       50.6923  0.0018\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m54.0541\u001b[0m  0.0020\n",
      "      5       50.6923  0.0017\n",
      "      2       65.0720  0.0051\n",
      "      8       64.6868  0.0036\n",
      "      2       54.0541  0.0018\n",
      "      6       50.6923  0.0017\n",
      "      3       65.0720  0.0021\n",
      "      9       64.6868  0.0019\n",
      "      3       54.0541  0.0018\n",
      "      7       50.6923  0.0020\n",
      "      4       65.0720  0.0019\n",
      "     10       64.6868  0.0018\n",
      "Restoring best model from epoch 1.\n",
      "      4       54.0541  0.0018\n",
      "      8       50.6923  0.0017\n",
      "      5       65.0720  0.0018\n",
      "      5       54.0541  0.0018\n",
      "      9       50.6923  0.0017\n",
      "      6       65.0720  0.0018\n",
      "      6       54.0541  0.0018\n",
      "     10       50.6923  0.0017\n",
      "Restoring best model from epoch 1.\n",
      "      7       54.0541  0.0018\n",
      "      7       65.0720  0.0030\n",
      "      8       54.0541  0.0042\n",
      "      8       65.0720  0.0049\n",
      "      9       65.0720  0.0036\n",
      "      9       54.0541  0.0052\n",
      "     10       65.0720  0.0020\n",
      "Restoring best model from epoch 1.\n",
      "     10       54.0541  0.0024\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m88.3661\u001b[0m  0.0033\n",
      "      2       88.3661  0.0025\n",
      "      3       88.3661  0.0026\n",
      "      4       88.3661  0.0025\n",
      "      5       88.3661  0.0025\n",
      "      6       88.3661  0.0026\n",
      "      7       88.3661  0.0027\n",
      "      8       88.3661  0.0039\n",
      "      9       88.3661  0.0046\n",
      "     10       88.3661  0.0041\n",
      "Restoring best model from epoch 1.\n",
      "y_train breslow final [    9.    11.   -16.    24.    24.    31.    36.    53.    61.   -61.\n",
      "    65.    74.    75.    90.   -92.    92.   -98.  -106.  -113.  -122.\n",
      "  -130.  -133.  -147.  -148.  -165.  -168.  -168.  -183.  -186.   186.\n",
      "  -190.  -192.  -194.   197.  -204.  -238.  -239.  -243.  -253.  -255.\n",
      "   260.  -260.  -268.  -277.   286.   304.  -343.   346.  -360.  -361.\n",
      "   365.  -379.   395.   396.  -420.  -441.  -442.  -454.   455.   457.\n",
      "   479.   493.   506.  -508.   518.   524.   528.  -529.   547.  -547.\n",
      "   555.   562.   563.   565.   568.  -571.  -576.  -581.   594.   608.\n",
      "   624.   627.   629.   637.  -643.  -655.   663.   676.   676.   679.\n",
      "  -686.   695.   731.   737.  -741.  -751.   760.  -761.  -763.  -783.\n",
      "   787.  -816.   820.  -837.   840.  -847.  -848.   857.   863.  -875.\n",
      "   883.  -885.   887.  -918.  -928.  -932.  -932.   949.  -967.   976.\n",
      "  1004.  1018.  1024.  1032.  1033.  1039.  1046. -1053.  1058.  1059.\n",
      "  1064.  1069.  1088.  1089.  1091.  1106.  1123. -1127.  1155.  1157.\n",
      "  1161.  1162.  1163.  1187.  1189. -1207.  1213.  1229.  1249.  1249.\n",
      "  1259.  1278.  1279.  1321.  1324.  1329.  1341. -1342.  1348.  1354.\n",
      "  1354. -1357.  1364. -1364.  1366. -1372.  1384. -1386.  1446.  1448.\n",
      "  1451.  1484.  1484.  1492.  1516.  1562. -1573.  1579.  1583.  1620.\n",
      "  1646. -1684.  1688.  1699.  1720.  1721. -1722.  1736.  1746. -1752.\n",
      "  1757.  1767.  1769. -1785.  1799.  1815.  1875.  1891. -1900. -1919.\n",
      "  1955.  1977. -1977. -1993.  1993. -1998.  2009.  2012.  2012. -2032.\n",
      " -2078.  2089. -2143.  2148. -2182.  2182.  2218.  2342.  2345. -2369.\n",
      "  2400.  2467. -2535. -2614.  2621.  2634.  2648. -2661. -2661.  2692.\n",
      "  2717. -3096.  3224. -3325.  3337. -3525. -3532. -3825. -3871.  4624.\n",
      " -5481.]\n",
      "Concordance Index 0.5112287748767573\n",
      "Integrated Brier Score: 0.12593525890414564\n",
      "y_train breslow final [    9.    11.   -16.    24.    24.    31.    36.    53.    61.   -61.\n",
      "    65.    74.    75.    90.   -92.    92.   -98.  -106.  -113.  -122.\n",
      "  -130.  -133.  -147.  -148.  -165.  -168.  -168.  -183.  -186.   186.\n",
      "  -190.  -192.  -194.   197.  -204.  -238.  -239.  -243.  -253.  -255.\n",
      "   260.  -260.  -268.  -277.   286.   304.  -343.   346.  -360.  -361.\n",
      "   365.  -379.   395.   396.  -420.  -441.  -442.  -454.   455.   457.\n",
      "   479.   493.   506.  -508.   518.   524.   528.  -529.   547.  -547.\n",
      "   555.   562.   563.   565.   568.  -571.  -576.  -581.   594.   608.\n",
      "   624.   627.   629.   637.  -643.  -655.   663.   676.   676.   679.\n",
      "  -686.   695.   731.   737.  -741.  -751.   760.  -761.  -763.  -783.\n",
      "   787.  -816.   820.  -837.   840.  -847.  -848.   857.   863.  -875.\n",
      "   883.  -885.   887.  -918.  -928.  -932.  -932.   949.  -967.   976.\n",
      "  1004.  1018.  1024.  1032.  1033.  1039.  1046. -1053.  1058.  1059.\n",
      "  1064.  1069.  1088.  1089.  1091.  1106.  1123. -1127.  1155.  1157.\n",
      "  1161.  1162.  1163.  1187.  1189. -1207.  1213.  1229.  1249.  1249.\n",
      "  1259.  1278.  1279.  1321.  1324.  1329.  1341. -1342.  1348.  1354.\n",
      "  1354. -1357.  1364. -1364.  1366. -1372.  1384. -1386.  1446.  1448.\n",
      "  1451.  1484.  1484.  1492.  1516.  1562. -1573.  1579.  1583.  1620.\n",
      "  1646. -1684.  1688.  1699.  1720.  1721. -1722.  1736.  1746. -1752.\n",
      "  1757.  1767.  1769. -1785.  1799.  1815.  1875.  1891. -1900. -1919.\n",
      "  1955.  1977. -1977. -1993.  1993. -1998.  2009.  2012.  2012. -2032.\n",
      " -2078.  2089. -2143.  2148. -2182.  2182.  2218.  2342.  2345. -2369.\n",
      "  2400.  2467. -2535. -2614.  2621.  2634.  2648. -2661. -2661.  2692.\n",
      "  2717. -3096.  3224. -3325.  3337. -3525. -3532. -3825. -3871.  4624.\n",
      " -5481.]\n",
      "durations 8.0 4424.0\n",
      "Concordance Index 0.4504331087584216\n",
      "Integrated Brier Score: 0.1369150026528061\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      1       \u001b[36m92.2424\u001b[0m  0.0019\n",
      "      2       92.2424  0.0016\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m99.1561\u001b[0m  0.0019\n",
      "      3       92.2424  0.0015\n",
      "      2       99.1561  0.0015\n",
      "      3       99.1561  0.0015\n",
      "      4       99.1561  0.0015\n",
      "      4       92.2424  0.0045\n",
      "      5       99.1561  0.0014\n",
      "      6       99.1561  0.0015\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m118.4719\u001b[0m       \u001b[32m59.4361\u001b[0m  0.0195\n",
      "      5       92.2424  0.0026\n",
      "      7       99.1561  0.0015\n",
      "      6       92.2424  0.0020\n",
      "      8       99.1561  0.0024\n",
      "      7       92.2424  0.0016\n",
      "      9       99.1561  0.0016\n",
      "      8       92.2424  0.0015\n",
      "     10       99.1561  0.0015\n",
      "Restoring best model from epoch 1.\n",
      "      9       92.2424  0.0015\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m126.3319\u001b[0m       \u001b[32m56.8191\u001b[0m  0.0227\n",
      "     10       92.2424  0.0015\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2      121.1934       \u001b[32m59.0241\u001b[0m  0.0177\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2      \u001b[36m123.7240\u001b[0m       56.8264  0.0187\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m100.1616\u001b[0m  0.0018\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m158.6947\u001b[0m  0.0019\n",
      "      2      100.1616  0.0018\n",
      "      3      100.1616  0.0015\n",
      "      2      158.6947  0.0031\n",
      "      4      100.1616  0.0015\n",
      "      3      158.6947  0.0015\n",
      "      5      100.1616  0.0016\n",
      "      4      158.6947  0.0015\n",
      "      5      158.6947  0.0015\n",
      "      3      124.6728       59.5369  0.0210\n",
      "      6      100.1616  0.0028\n",
      "      6      158.6947  0.0015\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m134.1572\u001b[0m       \u001b[32m69.5116\u001b[0m  0.0216\n",
      "      7      100.1616  0.0016\n",
      "      7      158.6947  0.0015\n",
      "      8      100.1616  0.0015\n",
      "      8      158.6947  0.0017\n",
      "      3      126.6631       56.8381  0.0177\n",
      "      9      100.1616  0.0014\n",
      "      9      158.6947  0.0015\n",
      "     10      100.1616  0.0014\n",
      "Restoring best model from epoch 1.\n",
      "     10      158.6947  0.0015\n",
      "Restoring best model from epoch 1.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m153.2639\u001b[0m       \u001b[32m62.1304\u001b[0m  0.0291\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m116.0390\u001b[0m       \u001b[32m59.5515\u001b[0m  0.0275\n",
      "      4      119.9361       60.5005  0.0180\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2      \u001b[36m132.8178\u001b[0m       70.3646  0.0183\n",
      "      4      \u001b[36m122.5079\u001b[0m       \u001b[32m56.6205\u001b[0m  0.0167\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m81.8221\u001b[0m  0.0019\n",
      "      2       81.8221  0.0015\n",
      "      3       81.8221  0.0014\n",
      "      4       81.8221  0.0014\n",
      "      5       81.8221  0.0014\n",
      "      2      \u001b[36m148.9755\u001b[0m       62.7136  0.0172\n",
      "      6       81.8221  0.0015\n",
      "      7       81.8221  0.0015\n",
      "      8       81.8221  0.0014\n",
      "      2      121.3279       59.6068  0.0173\n",
      "      3      \u001b[36m130.6193\u001b[0m       71.5120  0.0150\n",
      "      9       81.8221  0.0014\n",
      "     10       81.8221  0.0014\n",
      "      5      118.4725       60.5643  0.0198\n",
      "Restoring best model from epoch 1.\n",
      "      5      \u001b[36m116.3440\u001b[0m       \u001b[32m56.5112\u001b[0m  0.0158\n",
      "      3      \u001b[36m144.4933\u001b[0m       63.7660  0.0182\n",
      "      4      \u001b[36m125.8904\u001b[0m       70.9864  0.0186\n",
      "      6      \u001b[36m118.0115\u001b[0m       60.0361  0.0171\n",
      "      3      116.9353       59.6836  0.0245\n",
      "      6      \u001b[36m114.7520\u001b[0m       56.6597  0.0200\n",
      "      4      145.4314       64.0239  0.0160\n",
      "      5      131.5942       70.1187  0.0139\n",
      "      7      \u001b[36m114.7052\u001b[0m       59.5511  0.0147\n",
      "      4      \u001b[36m113.6904\u001b[0m       60.1643  0.0179\n",
      "      7      118.3229       57.5784  0.0227\n",
      "      8      \u001b[36m112.8365\u001b[0m       59.6205  0.0153\n",
      "      6      126.0068       69.9834  0.0185\n",
      "      5      \u001b[36m140.6613\u001b[0m       63.9663  0.0226\n",
      "      5      \u001b[36m113.2397\u001b[0m       60.0157  0.0154\n",
      "      8      117.2998       59.0188  0.0141\n",
      "      9      \u001b[36m111.5585\u001b[0m       59.8625  0.0155\n",
      "      7      \u001b[36m124.0760\u001b[0m       69.9730  0.0160\n",
      "      6      \u001b[36m135.7023\u001b[0m       63.5268  0.0158\n",
      "      6      \u001b[36m113.1491\u001b[0m       59.9107  0.0154\n",
      "      9      117.8957       59.7450  0.0139\n",
      "     10      \u001b[36m111.0127\u001b[0m       60.5650  0.0146\n",
      "Restoring best model from epoch 2.\n",
      "      8      126.7894       70.0349  0.0156\n",
      "      7      140.8382       63.1534  0.0158\n",
      "      7      115.3875       59.9489  0.0147\n",
      "     10      117.4707       60.0513  0.0137\n",
      "Restoring best model from epoch 5.\n",
      "      9      127.1114       70.1227  0.0145\n",
      "      8      139.4206       63.6843  0.0152\n",
      "      8      \u001b[36m109.0696\u001b[0m       60.1747  0.0141\n",
      "     10      128.2009       70.2458  0.0138\n",
      "Restoring best model from epoch 1.\n",
      "      9      137.2264       64.3071  0.0150\n",
      "      9      111.2429       60.4985  0.0142\n",
      "     10      \u001b[36m134.8715\u001b[0m       65.3740  0.0151\n",
      "Restoring best model from epoch 1.\n",
      "     10      111.4275       60.8924  0.0141\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m135.2347\u001b[0m       \u001b[32m79.8057\u001b[0m  0.0209\n",
      "      2      \u001b[36m134.7177\u001b[0m       80.6520  0.0222\n",
      "      3      \u001b[36m132.9444\u001b[0m       80.4747  0.0324\n",
      "      4      133.4091       80.8618  0.0248\n",
      "      5      133.2246       81.0759  0.0274\n",
      "      6      \u001b[36m130.1185\u001b[0m       80.8026  0.0248\n",
      "      7      \u001b[36m128.7290\u001b[0m       80.3571  0.0247\n",
      "      8      130.3871       81.1404  0.0286\n",
      "      9      \u001b[36m128.3670\u001b[0m       80.8378  0.0237\n",
      "     10      128.6961       79.9033  0.0254\n",
      "Restoring best model from epoch 1.\n",
      "y_train breslow final [    8.    11.    24.   -24.    24.    25.    31.    36.    53.    61.\n",
      "    65.   -67.    75.   -81.    91.   -98.  -106.  -113.  -130.   138.\n",
      "  -145.  -147.  -148.  -163.  -165.  -168.  -176.  -176.  -181.  -183.\n",
      "  -186.   186.  -190.  -192.  -194.  -220.  -238.  -239.  -243.  -253.\n",
      "  -255.   260.  -260.  -268.   286.   304.   304.   336.  -338.  -343.\n",
      "   346.  -360.  -361.   394.   395.   396.  -420.  -441.   455.   479.\n",
      "   493.   515.   518.   531.   542.  -547.  -547.   555.   562.   563.\n",
      "   565.   568.  -581.   608.   624.   627.   629.   637.   637.  -643.\n",
      "  -655.   676.   676.   679.  -684.  -686.   695.   728.   731.   737.\n",
      "   760.  -763.  -772.  -783.   787.  -816.   820.   820.   821.  -837.\n",
      "   840.  -847.  -848.   857.   863.   868.   883.  -885.   887.   914.\n",
      "  -915.  -918.   919.  -928.  -932.  -932.   949.   951.  -952.  -956.\n",
      "   962.  -967.   976.  1004.  1018.  1024.  1024.  1032.  1033.  1035.\n",
      "  1039. -1053.  1058.  1069.  1082.  1089.  1091.  1102.  1104.  1106.\n",
      "  1123. -1127.  1155.  1157.  1162.  1163.  1187.  1189. -1212.  1213.\n",
      "  1249.  1249.  1259.  1319.  1324.  1329. -1342.  1348.  1354.  1354.\n",
      " -1357.  1364. -1364.  1369. -1372.  1384. -1386.  1399.  1446.  1451.\n",
      "  1470.  1483.  1484.  1484.  1492.  1516.  1562. -1573.  1579.  1579.\n",
      "  1583.  1620.  1646.  1650. -1684.  1699.  1720.  1721. -1722.  1736.\n",
      " -1752.  1767. -1785.  1799. -1844. -1882.  1891. -1900. -1919.  1947.\n",
      "  1977. -1977. -1993.  1993. -1998.  2012.  2012. -2032.  2049. -2078.\n",
      " -2143.  2148. -2182.  2182.  2218. -2329. -2338.  2342.  2345.  2400.\n",
      " -2464.  2467. -2535. -2614.  2621.  2634.  2648. -2661. -2661.  2688.\n",
      "  2717.  2742. -3096.  3224. -3325.  3337. -3525. -3825. -3871. -4424.\n",
      "  4624.]\n",
      "Concordance Index 0.557712924359738\n",
      "Integrated Brier Score: 0.13808939641899534\n",
      "y_train breslow final [    8.    11.    24.   -24.    24.    25.    31.    36.    53.    61.\n",
      "    65.   -67.    75.   -81.    91.   -98.  -106.  -113.  -130.   138.\n",
      "  -145.  -147.  -148.  -163.  -165.  -168.  -176.  -176.  -181.  -183.\n",
      "  -186.   186.  -190.  -192.  -194.  -220.  -238.  -239.  -243.  -253.\n",
      "  -255.   260.  -260.  -268.   286.   304.   304.   336.  -338.  -343.\n",
      "   346.  -360.  -361.   394.   395.   396.  -420.  -441.   455.   479.\n",
      "   493.   515.   518.   531.   542.  -547.  -547.   555.   562.   563.\n",
      "   565.   568.  -581.   608.   624.   627.   629.   637.   637.  -643.\n",
      "  -655.   676.   676.   679.  -684.  -686.   695.   728.   731.   737.\n",
      "   760.  -763.  -772.  -783.   787.  -816.   820.   820.   821.  -837.\n",
      "   840.  -847.  -848.   857.   863.   868.   883.  -885.   887.   914.\n",
      "  -915.  -918.   919.  -928.  -932.  -932.   949.   951.  -952.  -956.\n",
      "   962.  -967.   976.  1004.  1018.  1024.  1024.  1032.  1033.  1035.\n",
      "  1039. -1053.  1058.  1069.  1082.  1089.  1091.  1102.  1104.  1106.\n",
      "  1123. -1127.  1155.  1157.  1162.  1163.  1187.  1189. -1212.  1213.\n",
      "  1249.  1249.  1259.  1319.  1324.  1329. -1342.  1348.  1354.  1354.\n",
      " -1357.  1364. -1364.  1369. -1372.  1384. -1386.  1399.  1446.  1451.\n",
      "  1470.  1483.  1484.  1484.  1492.  1516.  1562. -1573.  1579.  1579.\n",
      "  1583.  1620.  1646.  1650. -1684.  1699.  1720.  1721. -1722.  1736.\n",
      " -1752.  1767. -1785.  1799. -1844. -1882.  1891. -1900. -1919.  1947.\n",
      "  1977. -1977. -1993.  1993. -1998.  2012.  2012. -2032.  2049. -2078.\n",
      " -2143.  2148. -2182.  2182.  2218. -2329. -2338.  2342.  2345.  2400.\n",
      " -2464.  2467. -2535. -2614.  2621.  2634.  2648. -2661. -2661.  2688.\n",
      "  2717.  2742. -3096.  3224. -3325.  3337. -3525. -3825. -3871. -4424.\n",
      "  4624.]\n",
      "durations 9.0 5481.0\n",
      "Concordance Index 0.46394984326018807\n",
      "Integrated Brier Score: 0.12205484132759518\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m125.9997\u001b[0m       \u001b[32m62.8889\u001b[0m  0.0172\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      2      129.9440       \u001b[32m62.6899\u001b[0m  0.0153\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m89.7164\u001b[0m  0.0020\n",
      "      2       89.7164  0.0019\n",
      "      3       89.7164  0.0016\n",
      "      4       89.7164  0.0022\n",
      "      5       89.7164  0.0015\n",
      "      6       89.7164  0.0015\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      7       89.7164  0.0015\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      8       89.7164  0.0015\n",
      "      9       89.7164  0.0014\n",
      "     10       89.7164  0.0014\n",
      "Restoring best model from epoch 1.\n",
      "      3      127.3050       62.9077  0.0195\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m146.0988\u001b[0m       \u001b[32m68.7135\u001b[0m  0.0225\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m93.5786\u001b[0m  0.0025\n",
      "      2       93.5786  0.0020\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      4      127.5517       63.2087  0.0175\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      3       93.5786  0.0015\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m113.5559\u001b[0m       \u001b[32m55.8220\u001b[0m  0.0230\n",
      "      4       93.5786  0.0014\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m115.5190\u001b[0m  0.0018\n",
      "      5       93.5786  0.0014\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      2      115.5190  0.0015\n",
      "      2      147.3405       \u001b[32m68.3167\u001b[0m  0.0166\n",
      "      6       93.5786  0.0014\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      3      115.5190  0.0015\n",
      "      7       93.5786  0.0014\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      4      115.5190  0.0014\n",
      "      5      115.5190  0.0014\n",
      "      6      115.5190  0.0014\n",
      "      8       93.5786  0.0041\n",
      "      7      115.5190  0.0016\n",
      "      8      115.5190  0.0017\n",
      "      9       93.5786  0.0035\n",
      "      5      \u001b[36m122.8862\u001b[0m       63.7026  0.0161\n",
      "      9      115.5190  0.0015\n",
      "     10      115.5190  0.0015\n",
      "      2      120.8912       \u001b[32m55.7609\u001b[0m  0.0156\n",
      "Restoring best model from epoch 1.\n",
      "     10       93.5786  0.0034\n",
      "Restoring best model from epoch 1.\n",
      "      3      \u001b[36m142.6236\u001b[0m       \u001b[32m67.7503\u001b[0m  0.0167\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m130.8852\u001b[0m       \u001b[32m67.0344\u001b[0m  0.0171\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m121.0109\u001b[0m       \u001b[32m67.4750\u001b[0m  0.0217\n",
      "      6      \u001b[36m120.5396\u001b[0m       63.6329  0.0180\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m99.1665\u001b[0m  0.0019\n",
      "      2      131.9957       67.6920  0.0173\n",
      "      2       99.1665  0.0015\n",
      "      3      \u001b[36m113.4632\u001b[0m       56.3533  0.0239\n",
      "      3       99.1665  0.0014\n",
      "      4      142.6787       67.8386  0.0213\n",
      "      4       99.1665  0.0039\n",
      "      5       99.1665  0.0015\n",
      "      2      128.3091       \u001b[32m67.1575\u001b[0m  0.0167\n",
      "      6       99.1665  0.0015\n",
      "      7       99.1665  0.0014\n",
      "      8       99.1665  0.0014\n",
      "      7      121.8127       63.6424  0.0206\n",
      "      9       99.1665  0.0014\n",
      "     10       99.1665  0.0014\n",
      "Restoring best model from epoch 1.\n",
      "      3      134.9913       69.1051  0.0171\n",
      "      5      \u001b[36m140.2601\u001b[0m       68.3950  0.0156\n",
      "      4      \u001b[36m112.6148\u001b[0m       56.0343  0.0222\n",
      "      3      \u001b[36m116.8292\u001b[0m       \u001b[32m66.8643\u001b[0m  0.0170\n",
      "      8      123.1415       63.4651  0.0146\n",
      "      6      \u001b[36m138.0845\u001b[0m       68.4732  0.0149\n",
      "      4      135.4915       68.7991  0.0178\n",
      "      9      120.8688       63.4704  0.0139\n",
      "      4      118.6030       66.9793  0.0165\n",
      "      5      \u001b[36m108.2288\u001b[0m       \u001b[32m55.6743\u001b[0m  0.0194\n",
      "      5      \u001b[36m127.7508\u001b[0m       68.2731  0.0139\n",
      "      7      140.2068       67.9940  0.0155\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m93.5177\u001b[0m  0.0023\n",
      "     10      \u001b[36m119.8311\u001b[0m       63.6163  0.0154\n",
      "Restoring best model from epoch 2.\n",
      "      6      \u001b[36m105.5608\u001b[0m       \u001b[32m55.3724\u001b[0m  0.0143\n",
      "      2       93.5177  0.0015\n",
      "      5      \u001b[36m113.3683\u001b[0m       67.1737  0.0173\n",
      "      3       93.5177  0.0015\n",
      "      4       93.5177  0.0014\n",
      "      5       93.5177  0.0015\n",
      "      6      \u001b[36m126.8871\u001b[0m       \u001b[32m66.2140\u001b[0m  0.0151\n",
      "      6       93.5177  0.0014\n",
      "      8      139.4381       \u001b[32m67.3634\u001b[0m  0.0146\n",
      "      7       93.5177  0.0014\n",
      "      8       93.5177  0.0014\n",
      "      9       93.5177  0.0016\n",
      "     10       93.5177  0.0014\n",
      "Restoring best model from epoch 1.\n",
      "      7      106.6883       \u001b[32m55.1339\u001b[0m  0.0149\n",
      "      6      \u001b[36m109.5641\u001b[0m       67.0064  0.0155\n",
      "      9      \u001b[36m136.9314\u001b[0m       \u001b[32m66.7450\u001b[0m  0.0149\n",
      "      7      \u001b[36m123.8860\u001b[0m       \u001b[32m64.6706\u001b[0m  0.0201\n",
      "      8      107.3076       55.2568  0.0153\n",
      "      7      114.2591       66.8844  0.0166\n",
      "     10      140.5327       \u001b[32m66.2092\u001b[0m  0.0143\n",
      "      8      \u001b[36m122.6456\u001b[0m       \u001b[32m64.3584\u001b[0m  0.0141\n",
      "      9      110.3511       55.9460  0.0146\n",
      "      8      116.9201       66.9310  0.0146\n",
      "      9      126.8768       64.6737  0.0141\n",
      "     10      \u001b[36m103.9116\u001b[0m       56.1089  0.0142\n",
      "Restoring best model from epoch 7.\n",
      "      9      111.7356       67.1520  0.0145\n",
      "     10      123.6602       65.5080  0.0177\n",
      "Restoring best model from epoch 8.\n",
      "     10      114.1425       67.2080  0.0166\n",
      "Restoring best model from epoch 3.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m132.2568\u001b[0m       \u001b[32m94.6259\u001b[0m  0.0231\n",
      "      2      \u001b[36m126.3354\u001b[0m       96.4886  0.0256\n",
      "      3      \u001b[36m121.2341\u001b[0m       98.7458  0.0256\n",
      "      4      \u001b[36m119.2772\u001b[0m      100.2612  0.0251\n",
      "      5      120.0181      100.6239  0.0252\n",
      "      6      122.4993       98.2865  0.0245\n",
      "      7      \u001b[36m118.5593\u001b[0m       96.4409  0.0263\n",
      "      8      121.5658       96.3031  0.0267\n",
      "      9      119.2566       98.8388  0.0260\n",
      "     10      \u001b[36m118.2216\u001b[0m      101.6828  0.0246\n",
      "Restoring best model from epoch 1.\n",
      "y_train breslow final [    8.     9.    11.   -16.    24.   -24.    24.    25.    31.    31.\n",
      "    36.   -61.    65.   -67.    74.    75.   -81.    90.    91.   -92.\n",
      "    92.   -98.  -113.  -122.  -130.  -133.   138.  -145.  -148.  -163.\n",
      "  -168.  -168.  -176.  -176.  -181.  -183.  -186.  -192.   197.  -204.\n",
      "  -220.  -238.  -243.  -253.   260.  -260.  -260.  -277.   286.   304.\n",
      "   304.   336.  -338.  -343.   346.  -360.  -361.   365.  -379.   394.\n",
      "   395.   396.  -420.  -442.  -454.   457.   479.   493.   506.  -508.\n",
      "   515.   518.   524.   528.  -529.   531.   542.   547.  -547.   555.\n",
      "   563.  -571.  -576.  -581.   594.   608.   629.   637.   637.  -643.\n",
      "  -655.   663.   676.   676.  -684.  -686.   695.   728.   731.   737.\n",
      "  -741.  -751.   760.  -761.  -763.  -772.  -783.   820.   820.   821.\n",
      "  -837.  -848.   857.   863.   868.  -875.   883.  -885.   887.   914.\n",
      "  -915.  -918.   919.  -928.  -932.  -932.   951.  -952.  -956.   962.\n",
      "  -967.  1004.  1024.  1024.  1032.  1033.  1035.  1039.  1046.  1059.\n",
      "  1064.  1069.  1082.  1088.  1089.  1091.  1102.  1104.  1106. -1127.\n",
      "  1161.  1162.  1189. -1207. -1212.  1213.  1229.  1249.  1259.  1278.\n",
      "  1279.  1319.  1321.  1324.  1329.  1341. -1342.  1348.  1354.  1354.\n",
      " -1357.  1366.  1369.  1384. -1386.  1399.  1446.  1448.  1470.  1483.\n",
      "  1516.  1562.  1579.  1583.  1583.  1620.  1646.  1650. -1684.  1688.\n",
      "  1699.  1720.  1721. -1722.  1736.  1746.  1757.  1769. -1785.  1799.\n",
      "  1815. -1844.  1875. -1882. -1900.  1947.  1955.  1977. -1977. -1993.\n",
      "  1993. -1998.  2009.  2012.  2012. -2032.  2049. -2078.  2089.  2148.\n",
      " -2182.  2182.  2218. -2329. -2338. -2369.  2400. -2464. -2535.  2634.\n",
      "  2648.  2688.  2692.  2742. -3096. -3325.  3337. -3525. -3532. -3871.\n",
      " -4424. -5481.]\n",
      "Concordance Index 0.504730713245997\n",
      "Integrated Brier Score: 0.12513580362848592\n",
      "y_train breslow final [    8.     9.    11.   -16.    24.   -24.    24.    25.    31.    31.\n",
      "    36.   -61.    65.   -67.    74.    75.   -81.    90.    91.   -92.\n",
      "    92.   -98.  -113.  -122.  -130.  -133.   138.  -145.  -148.  -163.\n",
      "  -168.  -168.  -176.  -176.  -181.  -183.  -186.  -192.   197.  -204.\n",
      "  -220.  -238.  -243.  -253.   260.  -260.  -260.  -277.   286.   304.\n",
      "   304.   336.  -338.  -343.   346.  -360.  -361.   365.  -379.   394.\n",
      "   395.   396.  -420.  -442.  -454.   457.   479.   493.   506.  -508.\n",
      "   515.   518.   524.   528.  -529.   531.   542.   547.  -547.   555.\n",
      "   563.  -571.  -576.  -581.   594.   608.   629.   637.   637.  -643.\n",
      "  -655.   663.   676.   676.  -684.  -686.   695.   728.   731.   737.\n",
      "  -741.  -751.   760.  -761.  -763.  -772.  -783.   820.   820.   821.\n",
      "  -837.  -848.   857.   863.   868.  -875.   883.  -885.   887.   914.\n",
      "  -915.  -918.   919.  -928.  -932.  -932.   951.  -952.  -956.   962.\n",
      "  -967.  1004.  1024.  1024.  1032.  1033.  1035.  1039.  1046.  1059.\n",
      "  1064.  1069.  1082.  1088.  1089.  1091.  1102.  1104.  1106. -1127.\n",
      "  1161.  1162.  1189. -1207. -1212.  1213.  1229.  1249.  1259.  1278.\n",
      "  1279.  1319.  1321.  1324.  1329.  1341. -1342.  1348.  1354.  1354.\n",
      " -1357.  1366.  1369.  1384. -1386.  1399.  1446.  1448.  1470.  1483.\n",
      "  1516.  1562.  1579.  1583.  1583.  1620.  1646.  1650. -1684.  1688.\n",
      "  1699.  1720.  1721. -1722.  1736.  1746.  1757.  1769. -1785.  1799.\n",
      "  1815. -1844.  1875. -1882. -1900.  1947.  1955.  1977. -1977. -1993.\n",
      "  1993. -1998.  2009.  2012.  2012. -2032.  2049. -2078.  2089.  2148.\n",
      " -2182.  2182.  2218. -2329. -2338. -2369.  2400. -2464. -2535.  2634.\n",
      "  2648.  2688.  2692.  2742. -3096. -3325.  3337. -3525. -3532. -3871.\n",
      " -4424. -5481.]\n",
      "durations 53.0 4624.0\n",
      "Concordance Index 0.5986394557823129\n",
      "Integrated Brier Score: 0.14867696884507353\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m114.6016\u001b[0m  0.0019\n",
      "      2      114.6016  0.0016\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      3      114.6016  0.0015\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      4      114.6016  0.0015\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      5      114.6016  0.0014\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m122.1499\u001b[0m  0.0028\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m85.6513\u001b[0m  0.0018\n",
      "      6      114.6016  0.0019\n",
      "      2      122.1499  0.0017\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2       85.6513  0.0015\n",
      "      7      114.6016  0.0016\n",
      "      3      122.1499  0.0015\n",
      "      3       85.6513  0.0014\n",
      "      8      114.6016  0.0015\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      4      122.1499  0.0015\n",
      "      4       85.6513  0.0014\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      9      114.6016  0.0014\n",
      "      5      122.1499  0.0015\n",
      "      5       85.6513  0.0014\n",
      "     10      114.6016  0.0014\n",
      "Restoring best model from epoch 1.\n",
      "      6       85.6513  0.0013\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m92.6744\u001b[0m  0.0017\n",
      "      6      122.1499  0.0015\n",
      "      7       85.6513  0.0013\n",
      "      7      122.1499  0.0015\n",
      "      2       92.6744  0.0015\n",
      "      8       85.6513  0.0013\n",
      "      8      122.1499  0.0015\n",
      "      3       92.6744  0.0015\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m106.7299\u001b[0m  0.0088\n",
      "      9       85.6513  0.0013\n",
      "      4       92.6744  0.0014\n",
      "      9      122.1499  0.0015\n",
      "      2      106.7299  0.0017\n",
      "     10       85.6513  0.0013\n",
      "Restoring best model from epoch 1.\n",
      "      5       92.6744  0.0014\n",
      "     10      122.1499  0.0015\n",
      "Restoring best model from epoch 1.\n",
      "      3      106.7299  0.0016\n",
      "      6       92.6744  0.0014\n",
      "      4      106.7299  0.0015\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m131.7001\u001b[0m       \u001b[32m66.0897\u001b[0m  0.0189\n",
      "      7       92.6744  0.0014\n",
      "      5      106.7299  0.0019\n",
      "      8       92.6744  0.0014\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      9       92.6744  0.0015\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "     10       92.6744  0.0014\n",
      "Restoring best model from epoch 1.\n",
      "      6      106.7299  0.0034\n",
      "      7      106.7299  0.0016\n",
      "      8      106.7299  0.0015\n",
      "      9      106.7299  0.0015\n",
      "     10      106.7299  0.0014\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2      \u001b[36m125.2952\u001b[0m       \u001b[32m66.0557\u001b[0m  0.0165\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m119.0860\u001b[0m       \u001b[32m56.3334\u001b[0m  0.0178\n",
      "      3      \u001b[36m124.2541\u001b[0m       \u001b[32m65.8447\u001b[0m  0.0148\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m149.2146\u001b[0m       \u001b[32m81.7028\u001b[0m  0.0218\n",
      "      2      125.9342       57.0196  0.0154\n",
      "      4      125.5329       65.9722  0.0252\n",
      "      3      124.5739       57.4301  0.0176\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      2      \u001b[36m146.3984\u001b[0m       \u001b[32m81.5918\u001b[0m  0.0312\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      4      120.9782       56.7012  0.0176\n",
      "      5      \u001b[36m120.2838\u001b[0m       \u001b[32m65.6648\u001b[0m  0.0187\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m131.3354\u001b[0m       \u001b[32m68.0468\u001b[0m  0.0180\n",
      "      3      \u001b[36m143.4162\u001b[0m       82.0683  0.0177\n",
      "      6      \u001b[36m119.2651\u001b[0m       \u001b[32m65.1017\u001b[0m  0.0159\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m126.5384\u001b[0m       \u001b[32m68.8408\u001b[0m  0.0216\n",
      "      2      \u001b[36m130.0367\u001b[0m       68.2432  0.0220\n",
      "      5      \u001b[36m115.0592\u001b[0m       56.6849  0.0259\n",
      "      4      \u001b[36m136.3241\u001b[0m       82.4483  0.0179\n",
      "      7      119.4609       \u001b[32m64.8436\u001b[0m  0.0180\n",
      "      2      129.8699       \u001b[32m68.0651\u001b[0m  0.0190\n",
      "      6      \u001b[36m112.6655\u001b[0m       56.5339  0.0175\n",
      "      3      132.3979       68.5443  0.0179\n",
      "      5      \u001b[36m134.6179\u001b[0m       82.7759  0.0188\n",
      "      3      128.4809       \u001b[32m67.9140\u001b[0m  0.0153\n",
      "      8      123.9889       64.8658  0.0195\n",
      "      7      114.4547       \u001b[32m56.0040\u001b[0m  0.0136\n",
      "      4      \u001b[36m127.9231\u001b[0m       68.6009  0.0152\n",
      "      6      \u001b[36m134.5989\u001b[0m       83.0037  0.0162\n",
      "      4      \u001b[36m122.6986\u001b[0m       68.1650  0.0153\n",
      "      9      124.2307       65.1138  0.0146\n",
      "      8      114.8128       56.1458  0.0133\n",
      "      5      \u001b[36m123.0377\u001b[0m       68.7139  0.0154\n",
      "      7      135.8584       83.3356  0.0145\n",
      "      9      115.6994       56.3241  0.0131\n",
      "      5      \u001b[36m119.8525\u001b[0m       68.2251  0.0158\n",
      "     10      \u001b[36m118.7027\u001b[0m       65.0492  0.0148\n",
      "Restoring best model from epoch 7.\n",
      "      6      \u001b[36m122.3582\u001b[0m       68.6905  0.0152\n",
      "      8      \u001b[36m131.8470\u001b[0m       83.8517  0.0138\n",
      "     10      113.1349       56.8144  0.0131\n",
      "Restoring best model from epoch 7.\n",
      "      6      120.9922       68.4771  0.0143\n",
      "      7      \u001b[36m120.0026\u001b[0m       68.7173  0.0140\n",
      "      9      \u001b[36m130.7938\u001b[0m       84.5253  0.0137\n",
      "      7      122.3187       68.7368  0.0144\n",
      "      8      \u001b[36m117.5450\u001b[0m       68.6517  0.0136\n",
      "     10      133.9637       84.5463  0.0137\n",
      "Restoring best model from epoch 2.\n",
      "      8      \u001b[36m116.3710\u001b[0m       68.9392  0.0135\n",
      "      9      119.1195       68.7708  0.0145\n",
      "      9      116.4900       69.0230  0.0148\n",
      "     10      118.1747       68.9016  0.0141\n",
      "Restoring best model from epoch 1.\n",
      "     10      120.3522       68.6544  0.0143\n",
      "Restoring best model from epoch 3.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m126.1997\u001b[0m       \u001b[32m95.2681\u001b[0m  0.0220\n",
      "      2      \u001b[36m123.4166\u001b[0m       95.9735  0.0219\n",
      "      3      127.7623       96.8527  0.0248\n",
      "      4      \u001b[36m123.3723\u001b[0m       97.5123  0.0286\n",
      "      5      \u001b[36m122.1125\u001b[0m       96.5691  0.0249\n",
      "      6      \u001b[36m120.0233\u001b[0m       95.9038  0.0260\n",
      "      7      121.5213       95.8076  0.0261\n",
      "      8      \u001b[36m119.3352\u001b[0m       95.8768  0.0253\n",
      "      9      119.9030       95.8691  0.0264\n",
      "     10      \u001b[36m118.9732\u001b[0m       95.8860  0.0263\n",
      "Restoring best model from epoch 1.\n",
      "y_train breslow final [    8.     9.    11.   -16.   -24.    24.    25.    31.    31.    53.\n",
      "    61.   -61.    65.   -67.    74.    75.   -81.    90.    91.   -92.\n",
      "    92.   -98.  -106.  -113.  -122.  -133.   138.  -145.  -147.  -148.\n",
      "  -163.  -165.  -168.  -168.  -176.  -176.  -181.  -186.   186.  -190.\n",
      "  -194.   197.  -204.  -220.  -239.  -255.   260.  -260.  -260.  -268.\n",
      "  -277.   304.   336.  -338.  -343.  -361.   365.  -379.   394.   395.\n",
      "  -441.  -442.  -454.   455.   457.   506.  -508.   515.   518.   524.\n",
      "   528.  -529.   531.   542.   547.  -547.  -547.   562.   565.   568.\n",
      "  -571.  -576.   594.   624.   627.   629.   637.   663.   676.   676.\n",
      "   679.  -684.  -686.   695.   728.   731.   737.  -741.  -751.  -761.\n",
      "  -763.  -772.   787.  -816.   820.   821.   840.  -847.   868.  -875.\n",
      "   883.   914.  -915.  -918.   919.  -932.   949.   951.  -952.  -956.\n",
      "   962.  -967.   976.  1004.  1018.  1024.  1035.  1046. -1053.  1058.\n",
      "  1059.  1064.  1069.  1082.  1088.  1102.  1104.  1123. -1127.  1155.\n",
      "  1157.  1161.  1163.  1187.  1189. -1207. -1212.  1213.  1229.  1249.\n",
      "  1259.  1278.  1279.  1319.  1321.  1341. -1342.  1348.  1364. -1364.\n",
      "  1366.  1369. -1372.  1384.  1399.  1446.  1448.  1451.  1470.  1483.\n",
      "  1484.  1484.  1492.  1516.  1562. -1573.  1579.  1579.  1583.  1583.\n",
      "  1620.  1650. -1684.  1688.  1699.  1721. -1722.  1736.  1746. -1752.\n",
      "  1757.  1767.  1769. -1785.  1799.  1815. -1844.  1875. -1882.  1891.\n",
      " -1919.  1947.  1955.  1977. -1993. -1998.  2009.  2012.  2012. -2032.\n",
      "  2049.  2089. -2143.  2148. -2182.  2218. -2329. -2338.  2342.  2345.\n",
      " -2369. -2464.  2467. -2535. -2614.  2621.  2648. -2661. -2661.  2688.\n",
      "  2692.  2717.  2742. -3096.  3224.  3337. -3525. -3532. -3825. -4424.\n",
      "  4624. -5481.]\n",
      "Concordance Index 0.4993367900639093\n",
      "Integrated Brier Score: 0.12382283162537086\n",
      "y_train breslow final [    8.     9.    11.   -16.   -24.    24.    25.    31.    31.    53.\n",
      "    61.   -61.    65.   -67.    74.    75.   -81.    90.    91.   -92.\n",
      "    92.   -98.  -106.  -113.  -122.  -133.   138.  -145.  -147.  -148.\n",
      "  -163.  -165.  -168.  -168.  -176.  -176.  -181.  -186.   186.  -190.\n",
      "  -194.   197.  -204.  -220.  -239.  -255.   260.  -260.  -260.  -268.\n",
      "  -277.   304.   336.  -338.  -343.  -361.   365.  -379.   394.   395.\n",
      "  -441.  -442.  -454.   455.   457.   506.  -508.   515.   518.   524.\n",
      "   528.  -529.   531.   542.   547.  -547.  -547.   562.   565.   568.\n",
      "  -571.  -576.   594.   624.   627.   629.   637.   663.   676.   676.\n",
      "   679.  -684.  -686.   695.   728.   731.   737.  -741.  -751.  -761.\n",
      "  -763.  -772.   787.  -816.   820.   821.   840.  -847.   868.  -875.\n",
      "   883.   914.  -915.  -918.   919.  -932.   949.   951.  -952.  -956.\n",
      "   962.  -967.   976.  1004.  1018.  1024.  1035.  1046. -1053.  1058.\n",
      "  1059.  1064.  1069.  1082.  1088.  1102.  1104.  1123. -1127.  1155.\n",
      "  1157.  1161.  1163.  1187.  1189. -1207. -1212.  1213.  1229.  1249.\n",
      "  1259.  1278.  1279.  1319.  1321.  1341. -1342.  1348.  1364. -1364.\n",
      "  1366.  1369. -1372.  1384.  1399.  1446.  1448.  1451.  1470.  1483.\n",
      "  1484.  1484.  1492.  1516.  1562. -1573.  1579.  1579.  1583.  1583.\n",
      "  1620.  1650. -1684.  1688.  1699.  1721. -1722.  1736.  1746. -1752.\n",
      "  1757.  1767.  1769. -1785.  1799.  1815. -1844.  1875. -1882.  1891.\n",
      " -1919.  1947.  1955.  1977. -1993. -1998.  2009.  2012.  2012. -2032.\n",
      "  2049.  2089. -2143.  2148. -2182.  2218. -2329. -2338.  2342.  2345.\n",
      " -2369. -2464.  2467. -2535. -2614.  2621.  2648. -2661. -2661.  2688.\n",
      "  2692.  2717.  2742. -3096.  3224.  3337. -3525. -3532. -3825. -4424.\n",
      "  4624. -5481.]\n",
      "durations 24.0 3871.0\n",
      "Concordance Index 0.5556660039761432\n",
      "Integrated Brier Score: 0.1483175205283391\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m85.7843\u001b[0m  0.0019\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2       85.7843  0.0017\n",
      "      3       85.7843  0.0016\n",
      "      4       85.7843  0.0016\n",
      "      5       85.7843  0.0016\n",
      "      6       85.7843  0.0015\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      7       85.7843  0.0015\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m130.2763\u001b[0m       \u001b[32m71.0165\u001b[0m  0.0162\n",
      "      8       85.7843  0.0015\n",
      "      9       85.7843  0.0015\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m112.8716\u001b[0m  0.0018\n",
      "     10       85.7843  0.0015\n",
      "Restoring best model from epoch 1.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m122.9347\u001b[0m       \u001b[32m69.7345\u001b[0m  0.0160\n",
      "      2      112.8716  0.0038\n",
      "      3      112.8716  0.0015\n",
      "      4      112.8716  0.0015\n",
      "      5      112.8716  0.0015\n",
      "      6      112.8716  0.0014\n",
      "      7      112.8716  0.0014\n",
      "      8      112.8716  0.0014\n",
      "      9      112.8716  0.0014\n",
      "     10      112.8716  0.0014\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2      130.4636       71.1063  0.0219\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m97.8365\u001b[0m  0.0017\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      2       97.8365  0.0015\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      3       97.8365  0.0014\n",
      "      2      127.2070       70.3407  0.0234\n",
      "      4       97.8365  0.0014\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m105.5291\u001b[0m  0.0019\n",
      "      5       97.8365  0.0014\n",
      "      2      105.5291  0.0015\n",
      "      6       97.8365  0.0014\n",
      "      3      105.5291  0.0014\n",
      "      7       97.8365  0.0014\n",
      "      4      105.5291  0.0014\n",
      "      8       97.8365  0.0014\n",
      "      5      105.5291  0.0014\n",
      "      9       97.8365  0.0018\n",
      "     10       97.8365  0.0014\n",
      "Restoring best model from epoch 1.\n",
      "      6      105.5291  0.0029\n",
      "      3      \u001b[36m127.4506\u001b[0m       \u001b[32m70.8462\u001b[0m  0.0153\n",
      "      7      105.5291  0.0015\n",
      "      8      105.5291  0.0014\n",
      "      9      105.5291  0.0014\n",
      "     10      105.5291  0.0014\n",
      "Restoring best model from epoch 1.\n",
      "      3      \u001b[36m117.8725\u001b[0m       \u001b[32m69.3222\u001b[0m  0.0197\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      4      \u001b[36m127.2819\u001b[0m       \u001b[32m70.3955\u001b[0m  0.0154\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      4      117.8751       \u001b[32m69.1004\u001b[0m  0.0143\n",
      "      5      \u001b[36m124.8343\u001b[0m       70.6378  0.0147\n",
      "      5      \u001b[36m114.5832\u001b[0m       69.5341  0.0163\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m144.2834\u001b[0m       \u001b[32m79.3210\u001b[0m  0.0235\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2      152.4972       79.5292  0.0168\n",
      "      6      \u001b[36m111.9461\u001b[0m       70.3762  0.0199\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      6      127.1179       70.4362  0.0264\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m118.5857\u001b[0m       \u001b[32m59.3570\u001b[0m  0.0155\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m93.1003\u001b[0m  0.0037\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2       93.1003  0.0016\n",
      "      3       93.1003  0.0017\n",
      "      4       93.1003  0.0015\n",
      "      3      146.6706       \u001b[32m78.6886\u001b[0m  0.0151\n",
      "      5       93.1003  0.0014\n",
      "      7      113.0348       71.0794  0.0162\n",
      "      6       93.1003  0.0014\n",
      "      2      \u001b[36m118.0629\u001b[0m       59.8759  0.0145\n",
      "      7       93.1003  0.0014\n",
      "      8       93.1003  0.0014\n",
      "      9       93.1003  0.0014\n",
      "     10       93.1003  0.0014\n",
      "Restoring best model from epoch 1.\n",
      "      7      \u001b[36m122.6684\u001b[0m       \u001b[32m70.3671\u001b[0m  0.0211\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m118.6800\u001b[0m       \u001b[32m74.7528\u001b[0m  0.0177\n",
      "      4      \u001b[36m142.6069\u001b[0m       \u001b[32m78.4630\u001b[0m  0.0148\n",
      "      8      114.2787       72.1414  0.0157\n",
      "      3      121.1626       \u001b[32m59.2103\u001b[0m  0.0152\n",
      "      8      124.2869       70.8165  0.0152\n",
      "      2      131.1586       75.3068  0.0147\n",
      "      4      119.7960       \u001b[32m58.8488\u001b[0m  0.0141\n",
      "      9      \u001b[36m111.8376\u001b[0m       73.0254  0.0161\n",
      "      5      \u001b[36m141.6480\u001b[0m       \u001b[32m78.4551\u001b[0m  0.0179\n",
      "      9      \u001b[36m121.5718\u001b[0m       71.2501  0.0148\n",
      "      3      121.8401       75.2708  0.0138\n",
      "      5      \u001b[36m116.3147\u001b[0m       59.1418  0.0137\n",
      "     10      113.0088       73.9198  0.0144\n",
      "Restoring best model from epoch 4.\n",
      "      6      \u001b[36m137.7687\u001b[0m       78.4761  0.0162\n",
      "     10      \u001b[36m120.1392\u001b[0m       71.5189  0.0138\n",
      "Restoring best model from epoch 7.\n",
      "      4      \u001b[36m115.3653\u001b[0m       75.2646  0.0133\n",
      "      6      \u001b[36m113.7816\u001b[0m       59.5298  0.0141\n",
      "      7      138.5373       78.5292  0.0160\n",
      "      5      \u001b[36m114.3657\u001b[0m       75.8127  0.0138\n",
      "      7      \u001b[36m110.2226\u001b[0m       59.7767  0.0138\n",
      "      8      142.9631       78.4665  0.0150\n",
      "      6      \u001b[36m112.5063\u001b[0m       75.8037  0.0135\n",
      "      8      111.4365       59.5977  0.0140\n",
      "      7      112.8354       76.3328  0.0134\n",
      "      9      \u001b[36m137.4767\u001b[0m       \u001b[32m78.4243\u001b[0m  0.0160\n",
      "      9      \u001b[36m110.1230\u001b[0m       59.2453  0.0152\n",
      "      8      114.4610       77.0940  0.0137\n",
      "     10      140.5675       \u001b[32m78.3797\u001b[0m  0.0155\n",
      "     10      111.8156       58.9046  0.0142\n",
      "Restoring best model from epoch 4.\n",
      "      9      113.7365       78.1079  0.0143\n",
      "     10      \u001b[36m109.9453\u001b[0m       79.2999  0.0136\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m134.4720\u001b[0m       \u001b[32m98.1383\u001b[0m  0.0227\n",
      "      2      \u001b[36m133.6069\u001b[0m       98.4257  0.0241\n",
      "      3      \u001b[36m127.8532\u001b[0m       98.6853  0.0273\n",
      "      4      \u001b[36m127.6586\u001b[0m       \u001b[32m97.4660\u001b[0m  0.0257\n",
      "      5      \u001b[36m126.5004\u001b[0m       97.7175  0.0245\n",
      "      6      \u001b[36m124.4548\u001b[0m       97.8862  0.0249\n",
      "      7      \u001b[36m124.2302\u001b[0m       98.0215  0.0253\n",
      "      8      \u001b[36m124.1744\u001b[0m       98.2436  0.0243\n",
      "      9      \u001b[36m121.7308\u001b[0m       98.4714  0.0258\n",
      "     10      125.3679       98.4511  0.0238\n",
      "Restoring best model from epoch 4.\n",
      "y_train breslow final [    8.     9.   -16.    24.   -24.    25.    31.    31.    36.    53.\n",
      "    61.   -61.   -67.    74.   -81.    90.    91.   -92.    92.  -106.\n",
      "  -122.  -130.  -133.   138.  -145.  -147.  -163.  -165.  -168.  -176.\n",
      "  -176.  -181.  -183.   186.  -190.  -192.  -194.   197.  -204.  -220.\n",
      "  -238.  -239.  -243.  -253.  -255.  -260.  -260.  -268.  -277.   286.\n",
      "   304.   304.   336.  -338.   346.  -360.   365.  -379.   394.   396.\n",
      "  -420.  -441.  -442.  -454.   455.   457.   479.   493.   506.  -508.\n",
      "   515.   524.   528.  -529.   531.   542.   547.  -547.  -547.   555.\n",
      "   562.   563.   565.   568.  -571.  -576.  -581.   594.   608.   624.\n",
      "   627.   637.   637.  -643.  -655.   663.   679.  -684.   728.  -741.\n",
      "  -751.   760.  -761.  -772.  -783.   787.  -816.   820.   820.   821.\n",
      "  -837.   840.  -847.  -848.   857.   863.   868.  -875.  -885.   887.\n",
      "   914.  -915.   919.  -928.  -932.   949.   951.  -952.  -956.   962.\n",
      "   976.  1018.  1024.  1024.  1032.  1033.  1035.  1039.  1046. -1053.\n",
      "  1058.  1059.  1064.  1082.  1088.  1089.  1091.  1102.  1104.  1106.\n",
      "  1123.  1155.  1157.  1161.  1162.  1163.  1187. -1207. -1212.  1229.\n",
      "  1249.  1249.  1278.  1279.  1319.  1321.  1324.  1329.  1341.  1354.\n",
      "  1354. -1357.  1364. -1364.  1366.  1369. -1372. -1386.  1399.  1448.\n",
      "  1451.  1470.  1483.  1484.  1484.  1492. -1573.  1579.  1579.  1583.\n",
      "  1583.  1646.  1650.  1688.  1720.  1746. -1752.  1757.  1767.  1769.\n",
      "  1815. -1844.  1875. -1882.  1891. -1900. -1919.  1947.  1955. -1977.\n",
      "  1993.  2009.  2049. -2078.  2089. -2143.  2182. -2329. -2338.  2342.\n",
      "  2345. -2369.  2400. -2464.  2467. -2614.  2621.  2634. -2661. -2661.\n",
      "  2688.  2692.  2717.  2742.  3224. -3325. -3532. -3825. -3871. -4424.\n",
      "  4624. -5481.]\n",
      "Concordance Index 0.5784681636419563\n",
      "Integrated Brier Score: 0.115815105435137\n",
      "y_train breslow final [    8.     9.   -16.    24.   -24.    25.    31.    31.    36.    53.\n",
      "    61.   -61.   -67.    74.   -81.    90.    91.   -92.    92.  -106.\n",
      "  -122.  -130.  -133.   138.  -145.  -147.  -163.  -165.  -168.  -176.\n",
      "  -176.  -181.  -183.   186.  -190.  -192.  -194.   197.  -204.  -220.\n",
      "  -238.  -239.  -243.  -253.  -255.  -260.  -260.  -268.  -277.   286.\n",
      "   304.   304.   336.  -338.   346.  -360.   365.  -379.   394.   396.\n",
      "  -420.  -441.  -442.  -454.   455.   457.   479.   493.   506.  -508.\n",
      "   515.   524.   528.  -529.   531.   542.   547.  -547.  -547.   555.\n",
      "   562.   563.   565.   568.  -571.  -576.  -581.   594.   608.   624.\n",
      "   627.   637.   637.  -643.  -655.   663.   679.  -684.   728.  -741.\n",
      "  -751.   760.  -761.  -772.  -783.   787.  -816.   820.   820.   821.\n",
      "  -837.   840.  -847.  -848.   857.   863.   868.  -875.  -885.   887.\n",
      "   914.  -915.   919.  -928.  -932.   949.   951.  -952.  -956.   962.\n",
      "   976.  1018.  1024.  1024.  1032.  1033.  1035.  1039.  1046. -1053.\n",
      "  1058.  1059.  1064.  1082.  1088.  1089.  1091.  1102.  1104.  1106.\n",
      "  1123.  1155.  1157.  1161.  1162.  1163.  1187. -1207. -1212.  1229.\n",
      "  1249.  1249.  1278.  1279.  1319.  1321.  1324.  1329.  1341.  1354.\n",
      "  1354. -1357.  1364. -1364.  1366.  1369. -1372. -1386.  1399.  1448.\n",
      "  1451.  1470.  1483.  1484.  1484.  1492. -1573.  1579.  1579.  1583.\n",
      "  1583.  1646.  1650.  1688.  1720.  1746. -1752.  1757.  1767.  1769.\n",
      "  1815. -1844.  1875. -1882.  1891. -1900. -1919.  1947.  1955. -1977.\n",
      "  1993.  2009.  2049. -2078.  2089. -2143.  2182. -2329. -2338.  2342.\n",
      "  2345. -2369.  2400. -2464.  2467. -2614.  2621.  2634. -2661. -2661.\n",
      "  2688.  2692.  2717.  2742.  3224. -3325. -3532. -3825. -3871. -4424.\n",
      "  4624. -5481.]\n",
      "durations 11.0 3525.0\n",
      "Concordance Index 0.46441947565543074\n",
      "Integrated Brier Score: 0.16736086653943127\n",
      "split gex_?|100133144      float32\n",
      "gex_?|100134869      float32\n",
      "gex_?|10357          float32\n",
      "gex_?|10431          float32\n",
      "gex_?|155060         float32\n",
      "                      ...   \n",
      "gex_ZYG11A|440590    float32\n",
      "gex_ZYG11B|79699     float32\n",
      "gex_ZYX|7791         float32\n",
      "gex_ZZEF1|23140      float32\n",
      "gex_ZZZ3|26009       float32\n",
      "Length: 19076, dtype: object\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m103.1867\u001b[0m       \u001b[32m70.7945\u001b[0m  0.0265  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m93.0851\u001b[0m  0.0029\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m65.7177\u001b[0m  0.0029\n",
      "      2       93.0851  0.0019\n",
      "\n",
      "      2       65.7177  0.0018\n",
      "      3       93.0851  0.0017\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      3       65.7177  0.0017\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      4       93.0851  0.0017\n",
      "      4       65.7177  0.0016\n",
      "      5       93.0851  0.0017\n",
      "      5       65.7177  0.0016\n",
      "      6       93.0851  0.0016\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      6       65.7177  0.0016\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      7       93.0851  0.0016\n",
      "      7       65.7177  0.0018\n",
      "      8       93.0851  0.0016\n",
      "      8       65.7177  0.0016\n",
      "      9       93.0851  0.0018\n",
      "      9       65.7177  0.0016\n",
      "     10       93.0851  0.0016\n",
      "Restoring best model from epoch 1.\n",
      "     10       65.7177  0.0016\n",
      "Restoring best model from epoch 1.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m72.6665\u001b[0m       \u001b[32m61.2078\u001b[0m  0.0345\n",
      "      2       \u001b[36m94.0598\u001b[0m       71.1716  0.0246\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m79.8742\u001b[0m       \u001b[32m58.7206\u001b[0m  0.0313\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2       76.2402       61.4541  0.0212\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m93.3092\u001b[0m       \u001b[32m59.4480\u001b[0m  0.0400\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m99.6256\u001b[0m  0.0048\n",
      "      2       99.6256  0.0022\n",
      "      3       94.3410       \u001b[32m70.3652\u001b[0m  0.0291\n",
      "      3       99.6256  0.0053\n",
      "      4       99.6256  0.0059\n",
      "      5       99.6256  0.0023\n",
      "      2       \u001b[36m77.0694\u001b[0m       \u001b[32m57.7184\u001b[0m  0.0262\n",
      "      6       99.6256  0.0023\n",
      "      7       99.6256  0.0020\n",
      "      3       76.2971       61.9463  0.0314\n",
      "      8       99.6256  0.0018\n",
      "      9       99.6256  0.0018\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "     10       99.6256  0.0018\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2       \u001b[36m90.8967\u001b[0m       60.0605  0.0377\n",
      "      4       \u001b[36m92.2331\u001b[0m       \u001b[32m69.9882\u001b[0m  0.0331\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m88.2670\u001b[0m  0.0054\n",
      "      3       78.4815       57.8651  0.0266\n",
      "      2       88.2670  0.0041\n",
      "      4       \u001b[36m72.1217\u001b[0m       \u001b[32m60.8191\u001b[0m  0.0289\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      3       88.2670  0.0062\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      4       88.2670  0.0027\n",
      "      3       \u001b[36m85.5500\u001b[0m       59.5662  0.0251\n",
      "      5       88.2670  0.0040\n",
      "      6       88.2670  0.0019\n",
      "      5       92.3820       70.2303  0.0255\n",
      "      7       88.2670  0.0017\n",
      "      8       88.2670  0.0017\n",
      "      9       88.2670  0.0032\n",
      "      4       78.4955       \u001b[32m57.6297\u001b[0m  0.0264\n",
      "     10       88.2670  0.0038\n",
      "Restoring best model from epoch 1.\n",
      "      5       \u001b[36m70.7367\u001b[0m       \u001b[32m59.9935\u001b[0m  0.0275\n",
      "      4       88.0093       59.4945  0.0230\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m84.7424\u001b[0m       \u001b[32m49.6530\u001b[0m  0.0240\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      6       92.5855       70.6692  0.0236\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m70.1510\u001b[0m  0.0029\n",
      "      2       70.1510  0.0018\n",
      "      3       70.1510  0.0017\n",
      "      4       70.1510  0.0024\n",
      "      5       70.1510  0.0020\n",
      "      5       \u001b[36m73.4695\u001b[0m       57.9073  0.0297\n",
      "      6       70.1510  0.0038\n",
      "      7       70.1510  0.0018\n",
      "      5       89.9795       59.6716  0.0234\n",
      "      6       \u001b[36m69.0216\u001b[0m       60.1445  0.0243\n",
      "      8       70.1510  0.0017\n",
      "      2       87.5891       \u001b[32m49.5916\u001b[0m  0.0249\n",
      "      9       70.1510  0.0029\n",
      "     10       70.1510  0.0018\n",
      "Restoring best model from epoch 1.\n",
      "      7       \u001b[36m91.8067\u001b[0m       70.9722  0.0286\n",
      "      7       \u001b[36m67.6747\u001b[0m       60.9628  0.0224\n",
      "      6       87.4272       59.7611  0.0242\n",
      "      3       \u001b[36m80.7720\u001b[0m       49.8893  0.0228\n",
      "      6       75.9037       58.1180  0.0356\n",
      "      8       92.4673       70.9046  0.0208\n",
      "      7       \u001b[36m84.3912\u001b[0m       60.0977  0.0231\n",
      "      8       68.6842       61.6860  0.0274\n",
      "      4       82.2651       49.7196  0.0275\n",
      "      9       \u001b[36m89.5949\u001b[0m       70.8955  0.0245\n",
      "      7       75.0853       58.5544  0.0296\n",
      "      8       85.6857       60.5883  0.0220\n",
      "      9       68.0825       62.6734  0.0232\n",
      "      8       74.7814       58.7416  0.0209\n",
      "     10       90.0377       70.8310  0.0278\n",
      "Restoring best model from epoch 4.\n",
      "      9       85.4119       60.7456  0.0195\n",
      "      5       81.3825       \u001b[32m49.4483\u001b[0m  0.0380\n",
      "     10       70.4328       62.7393  0.0203\n",
      "Restoring best model from epoch 5.\n",
      "      9       \u001b[36m72.9465\u001b[0m       58.5914  0.0251\n",
      "     10       85.5664       61.3400  0.0197\n",
      "Restoring best model from epoch 1.\n",
      "      6       \u001b[36m79.4060\u001b[0m       \u001b[32m49.0713\u001b[0m  0.0196\n",
      "     10       \u001b[36m72.0705\u001b[0m       58.7426  0.0191\n",
      "Restoring best model from epoch 4.\n",
      "      7       81.2684       49.2749  0.0189\n",
      "      8       79.6388       49.4868  0.0197\n",
      "      9       \u001b[36m78.3424\u001b[0m       49.2962  0.0182\n",
      "     10       \u001b[36m78.0995\u001b[0m       49.4732  0.0196\n",
      "Restoring best model from epoch 6.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m85.1418\u001b[0m       \u001b[32m78.9404\u001b[0m  0.0223\n",
      "      2       85.5113       \u001b[32m78.2037\u001b[0m  0.0213\n",
      "      3       87.9863       \u001b[32m77.3304\u001b[0m  0.0230\n",
      "      4       86.6684       77.5062  0.0263\n",
      "      5       \u001b[36m82.3903\u001b[0m       77.9687  0.0266\n",
      "      6       \u001b[36m80.2688\u001b[0m       78.3505  0.0688\n",
      "      7       81.0428       78.5923  0.0218\n",
      "      8       80.3628       78.1551  0.0217\n",
      "      9       \u001b[36m80.0253\u001b[0m       78.3339  0.0259\n",
      "     10       \u001b[36m77.8434\u001b[0m       79.1132  0.0252\n",
      "Restoring best model from epoch 3.\n",
      "y_train breslow final [ 3.000e+00  8.000e+00 -8.000e+00 -1.400e+01 -1.600e+01 -1.700e+01\n",
      " -2.100e+01 -2.100e+01  2.200e+01 -2.300e+01  2.400e+01 -2.900e+01\n",
      "  3.000e+01 -3.000e+01 -3.100e+01 -3.500e+01  4.500e+01 -4.600e+01\n",
      "  5.200e+01  5.700e+01  6.100e+01 -6.400e+01  7.600e+01 -7.700e+01\n",
      "  8.100e+01 -9.900e+01  1.050e+02  1.050e+02  1.060e+02  1.130e+02\n",
      " -1.130e+02  1.220e+02 -1.310e+02  1.410e+02  1.530e+02  1.560e+02\n",
      " -1.560e+02  1.660e+02  1.680e+02 -1.700e+02  1.740e+02 -1.760e+02\n",
      "  1.800e+02  1.800e+02 -1.830e+02  1.850e+02  1.850e+02  1.880e+02\n",
      "  1.910e+02  1.920e+02 -1.980e+02 -1.980e+02  2.000e+02  2.010e+02\n",
      "  2.050e+02  2.120e+02  2.150e+02  2.180e+02 -2.250e+02 -2.290e+02\n",
      "  2.350e+02  2.430e+02 -2.450e+02  2.450e+02  2.500e+02  2.550e+02\n",
      "  2.620e+02  2.720e+02  2.730e+02 -2.730e+02  2.750e+02  2.760e+02\n",
      "  2.790e+02 -2.800e+02  2.810e+02  2.840e+02 -2.870e+02 -2.880e+02\n",
      "  2.890e+02  2.910e+02  2.940e+02  2.960e+02  3.000e+02  3.000e+02\n",
      " -3.230e+02 -3.250e+02 -3.350e+02  3.360e+02  3.410e+02 -3.440e+02\n",
      " -3.440e+02 -3.460e+02  3.480e+02  3.480e+02  3.530e+02  3.540e+02\n",
      "  3.560e+02 -3.560e+02 -3.580e+02  3.580e+02  3.590e+02 -3.650e+02\n",
      "  3.660e+02 -3.670e+02 -3.680e+02  3.700e+02 -3.710e+02 -3.740e+02\n",
      " -3.750e+02  3.760e+02  3.770e+02  3.780e+02 -3.780e+02 -3.780e+02\n",
      " -3.790e+02 -3.810e+02 -3.830e+02 -3.830e+02 -3.830e+02 -3.880e+02\n",
      "  3.890e+02 -3.890e+02 -3.890e+02 -3.900e+02 -3.920e+02 -3.940e+02\n",
      " -3.960e+02  3.960e+02  3.980e+02 -4.000e+02  4.010e+02  4.030e+02\n",
      "  4.060e+02 -4.080e+02 -4.110e+02 -4.110e+02 -4.130e+02 -4.160e+02\n",
      " -4.190e+02  4.220e+02  4.220e+02  4.260e+02 -4.270e+02 -4.280e+02\n",
      "  4.280e+02 -4.310e+02 -4.340e+02  4.390e+02 -4.400e+02  4.460e+02\n",
      " -4.500e+02  4.570e+02 -4.630e+02  4.740e+02  4.760e+02  4.770e+02\n",
      " -4.790e+02 -4.860e+02  4.910e+02  4.910e+02 -4.910e+02 -4.940e+02\n",
      "  4.960e+02 -5.000e+02  5.070e+02  5.130e+02 -5.190e+02 -5.210e+02\n",
      " -5.230e+02 -5.250e+02  5.430e+02 -5.430e+02  5.490e+02  5.540e+02\n",
      "  5.580e+02 -5.590e+02  5.600e+02 -5.660e+02  5.700e+02 -5.730e+02\n",
      " -5.770e+02 -5.790e+02 -5.800e+02 -5.820e+02  5.880e+02 -5.930e+02\n",
      " -5.940e+02 -5.950e+02 -6.000e+02 -6.070e+02 -6.130e+02 -6.150e+02\n",
      "  6.180e+02 -6.210e+02 -6.250e+02 -6.280e+02  6.330e+02  6.350e+02\n",
      " -6.360e+02  6.400e+02 -6.440e+02 -6.470e+02 -6.500e+02  6.520e+02\n",
      "  6.610e+02 -6.640e+02 -6.660e+02  6.690e+02 -6.750e+02  6.750e+02\n",
      " -6.780e+02 -6.790e+02 -6.940e+02 -6.990e+02  7.120e+02 -7.240e+02\n",
      " -7.250e+02 -7.360e+02 -7.380e+02 -7.390e+02 -7.420e+02 -7.520e+02\n",
      " -7.540e+02  7.620e+02  7.790e+02  7.820e+02 -7.850e+02  7.920e+02\n",
      "  7.940e+02  8.010e+02 -8.120e+02 -8.130e+02 -8.190e+02 -8.250e+02\n",
      "  8.320e+02 -8.380e+02 -8.380e+02 -8.560e+02 -8.620e+02 -8.640e+02\n",
      "  8.690e+02  8.740e+02 -8.810e+02  8.810e+02 -8.820e+02 -9.000e+02\n",
      " -9.120e+02 -9.120e+02 -9.280e+02  9.400e+02 -9.400e+02 -9.420e+02\n",
      " -9.420e+02 -9.460e+02 -9.490e+02 -9.510e+02 -9.720e+02 -9.810e+02\n",
      " -9.890e+02 -9.910e+02 -1.000e+03 -1.002e+03 -1.010e+03 -1.016e+03\n",
      " -1.023e+03 -1.038e+03  1.043e+03 -1.055e+03 -1.072e+03  1.095e+03\n",
      " -1.100e+03 -1.106e+03 -1.108e+03 -1.124e+03 -1.132e+03 -1.133e+03\n",
      " -1.138e+03 -1.145e+03  1.153e+03 -1.160e+03 -1.184e+03 -1.190e+03\n",
      " -1.200e+03 -1.210e+03 -1.223e+03 -1.236e+03  1.294e+03 -1.297e+03\n",
      " -1.319e+03 -1.328e+03 -1.367e+03 -1.431e+03 -1.484e+03 -1.588e+03\n",
      " -1.645e+03  1.686e+03 -1.690e+03  1.747e+03 -1.765e+03 -1.851e+03\n",
      " -1.862e+03 -1.918e+03 -1.935e+03 -1.964e+03 -2.032e+03  2.100e+03\n",
      "  2.197e+03 -2.267e+03 -3.196e+03 -3.519e+03 -3.720e+03]\n",
      "Concordance Index 0.6547366635569503\n",
      "Integrated Brier Score: 0.2069823157694685\n",
      "y_train breslow final [ 3.000e+00  8.000e+00 -8.000e+00 -1.400e+01 -1.600e+01 -1.700e+01\n",
      " -2.100e+01 -2.100e+01  2.200e+01 -2.300e+01  2.400e+01 -2.900e+01\n",
      "  3.000e+01 -3.000e+01 -3.100e+01 -3.500e+01  4.500e+01 -4.600e+01\n",
      "  5.200e+01  5.700e+01  6.100e+01 -6.400e+01  7.600e+01 -7.700e+01\n",
      "  8.100e+01 -9.900e+01  1.050e+02  1.050e+02  1.060e+02  1.130e+02\n",
      " -1.130e+02  1.220e+02 -1.310e+02  1.410e+02  1.530e+02  1.560e+02\n",
      " -1.560e+02  1.660e+02  1.680e+02 -1.700e+02  1.740e+02 -1.760e+02\n",
      "  1.800e+02  1.800e+02 -1.830e+02  1.850e+02  1.850e+02  1.880e+02\n",
      "  1.910e+02  1.920e+02 -1.980e+02 -1.980e+02  2.000e+02  2.010e+02\n",
      "  2.050e+02  2.120e+02  2.150e+02  2.180e+02 -2.250e+02 -2.290e+02\n",
      "  2.350e+02  2.430e+02 -2.450e+02  2.450e+02  2.500e+02  2.550e+02\n",
      "  2.620e+02  2.720e+02  2.730e+02 -2.730e+02  2.750e+02  2.760e+02\n",
      "  2.790e+02 -2.800e+02  2.810e+02  2.840e+02 -2.870e+02 -2.880e+02\n",
      "  2.890e+02  2.910e+02  2.940e+02  2.960e+02  3.000e+02  3.000e+02\n",
      " -3.230e+02 -3.250e+02 -3.350e+02  3.360e+02  3.410e+02 -3.440e+02\n",
      " -3.440e+02 -3.460e+02  3.480e+02  3.480e+02  3.530e+02  3.540e+02\n",
      "  3.560e+02 -3.560e+02 -3.580e+02  3.580e+02  3.590e+02 -3.650e+02\n",
      "  3.660e+02 -3.670e+02 -3.680e+02  3.700e+02 -3.710e+02 -3.740e+02\n",
      " -3.750e+02  3.760e+02  3.770e+02  3.780e+02 -3.780e+02 -3.780e+02\n",
      " -3.790e+02 -3.810e+02 -3.830e+02 -3.830e+02 -3.830e+02 -3.880e+02\n",
      "  3.890e+02 -3.890e+02 -3.890e+02 -3.900e+02 -3.920e+02 -3.940e+02\n",
      " -3.960e+02  3.960e+02  3.980e+02 -4.000e+02  4.010e+02  4.030e+02\n",
      "  4.060e+02 -4.080e+02 -4.110e+02 -4.110e+02 -4.130e+02 -4.160e+02\n",
      " -4.190e+02  4.220e+02  4.220e+02  4.260e+02 -4.270e+02 -4.280e+02\n",
      "  4.280e+02 -4.310e+02 -4.340e+02  4.390e+02 -4.400e+02  4.460e+02\n",
      " -4.500e+02  4.570e+02 -4.630e+02  4.740e+02  4.760e+02  4.770e+02\n",
      " -4.790e+02 -4.860e+02  4.910e+02  4.910e+02 -4.910e+02 -4.940e+02\n",
      "  4.960e+02 -5.000e+02  5.070e+02  5.130e+02 -5.190e+02 -5.210e+02\n",
      " -5.230e+02 -5.250e+02  5.430e+02 -5.430e+02  5.490e+02  5.540e+02\n",
      "  5.580e+02 -5.590e+02  5.600e+02 -5.660e+02  5.700e+02 -5.730e+02\n",
      " -5.770e+02 -5.790e+02 -5.800e+02 -5.820e+02  5.880e+02 -5.930e+02\n",
      " -5.940e+02 -5.950e+02 -6.000e+02 -6.070e+02 -6.130e+02 -6.150e+02\n",
      "  6.180e+02 -6.210e+02 -6.250e+02 -6.280e+02  6.330e+02  6.350e+02\n",
      " -6.360e+02  6.400e+02 -6.440e+02 -6.470e+02 -6.500e+02  6.520e+02\n",
      "  6.610e+02 -6.640e+02 -6.660e+02  6.690e+02 -6.750e+02  6.750e+02\n",
      " -6.780e+02 -6.790e+02 -6.940e+02 -6.990e+02  7.120e+02 -7.240e+02\n",
      " -7.250e+02 -7.360e+02 -7.380e+02 -7.390e+02 -7.420e+02 -7.520e+02\n",
      " -7.540e+02  7.620e+02  7.790e+02  7.820e+02 -7.850e+02  7.920e+02\n",
      "  7.940e+02  8.010e+02 -8.120e+02 -8.130e+02 -8.190e+02 -8.250e+02\n",
      "  8.320e+02 -8.380e+02 -8.380e+02 -8.560e+02 -8.620e+02 -8.640e+02\n",
      "  8.690e+02  8.740e+02 -8.810e+02  8.810e+02 -8.820e+02 -9.000e+02\n",
      " -9.120e+02 -9.120e+02 -9.280e+02  9.400e+02 -9.400e+02 -9.420e+02\n",
      " -9.420e+02 -9.460e+02 -9.490e+02 -9.510e+02 -9.720e+02 -9.810e+02\n",
      " -9.890e+02 -9.910e+02 -1.000e+03 -1.002e+03 -1.010e+03 -1.016e+03\n",
      " -1.023e+03 -1.038e+03  1.043e+03 -1.055e+03 -1.072e+03  1.095e+03\n",
      " -1.100e+03 -1.106e+03 -1.108e+03 -1.124e+03 -1.132e+03 -1.133e+03\n",
      " -1.138e+03 -1.145e+03  1.153e+03 -1.160e+03 -1.184e+03 -1.190e+03\n",
      " -1.200e+03 -1.210e+03 -1.223e+03 -1.236e+03  1.294e+03 -1.297e+03\n",
      " -1.319e+03 -1.328e+03 -1.367e+03 -1.431e+03 -1.484e+03 -1.588e+03\n",
      " -1.645e+03  1.686e+03 -1.690e+03  1.747e+03 -1.765e+03 -1.851e+03\n",
      " -1.862e+03 -1.918e+03 -1.935e+03 -1.964e+03 -2.032e+03  2.100e+03\n",
      "  2.197e+03 -2.267e+03 -3.196e+03 -3.519e+03 -3.720e+03]\n",
      "durations 7.0 3540.0\n",
      "Concordance Index 0.432580424366872\n",
      "Integrated Brier Score: 0.2598385022937182\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m59.2621\u001b[0m  0.0018\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2       59.2621  0.0016\n",
      "      3       59.2621  0.0014\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m79.6638\u001b[0m  0.0017\n",
      "      4       59.2621  0.0014\n",
      "      2       79.6638  0.0015\n",
      "      5       59.2621  0.0014\n",
      "      3       79.6638  0.0014\n",
      "      6       59.2621  0.0014\n",
      "      4       79.6638  0.0014\n",
      "      7       59.2621  0.0015\n",
      "      5       79.6638  0.0014\n",
      "      8       59.2621  0.0014\n",
      "      6       79.6638  0.0014\n",
      "      9       59.2621  0.0014\n",
      "      7       79.6638  0.0014\n",
      "     10       59.2621  0.0014\n",
      "Restoring best model from epoch 1.\n",
      "      8       79.6638  0.0014\n",
      "      9       79.6638  0.0014\n",
      "     10       79.6638  0.0014\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m200.9013\u001b[0m  0.0019\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m203.7951\u001b[0m  0.0020\n",
      "      2      200.9013  0.0016\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1           nan  0.0017\n",
      "      2      203.7951  0.0016\n",
      "      3      200.9013  0.0015\n",
      "      2           nan  0.0015\n",
      "      3      203.7951  0.0019\n",
      "      4      200.9013  0.0015\n",
      "      3           nan  0.0015\n",
      "      5      200.9013  0.0014\n",
      "      4      203.7951  0.0016\n",
      "      4           nan  0.0014\n",
      "      6      200.9013  0.0014\n",
      "      5      203.7951  0.0016\n",
      "      5           nan  0.0014\n",
      "      7      200.9013  0.0014\n",
      "      6      203.7951  0.0015\n",
      "      6           nan  0.0014\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      8      200.9013  0.0014\n",
      "      7      203.7951  0.0015\n",
      "      7           nan  0.0014\n",
      "      9      200.9013  0.0014\n",
      "      8           nan  0.0014\n",
      "      8      203.7951  0.0015\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m70.4469\u001b[0m  0.0016\n",
      "     10      200.9013  0.0014\n",
      "Restoring best model from epoch 1.\n",
      "      9           nan  0.0014\n",
      "      9      203.7951  0.0016\n",
      "      2       70.4469  0.0015\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "      3       70.4469  0.0014\n",
      "     10      203.7951  0.0015\n",
      "Restoring best model from epoch 1.\n",
      "      4       70.4469  0.0014\n",
      "      5       70.4469  0.0014\n",
      "      6       70.4469  0.0025\n",
      "      7       70.4469  0.0046\n",
      "      8       70.4469  0.0036\n",
      "      9       70.4469  0.0016\n",
      "     10       70.4469  0.0014\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m114.5243\u001b[0m  0.0018\n",
      "      2      114.5243  0.0028\n",
      "      3      114.5243  0.0044\n",
      "      4      114.5243  0.0016\n",
      "      5      114.5243  0.0014\n",
      "      6      114.5243  0.0014\n",
      "      7      114.5243  0.0039\n",
      "      8      114.5243  0.0017\n",
      "      9      114.5243  0.0014\n",
      "     10      114.5243  0.0014\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m140.7403\u001b[0m  0.0018\n",
      "      2      140.7403  0.0016\n",
      "      3      140.7403  0.0016\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      4      140.7403  0.0015\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m150.1609\u001b[0m  0.0017\n",
      "      5      140.7403  0.0022\n",
      "      2      150.1609  0.0029\n",
      "      6      140.7403  0.0024\n",
      "      3      150.1609  0.0015\n",
      "      4      150.1609  0.0014\n",
      "      7      140.7403  0.0032\n",
      "      5      150.1609  0.0014\n",
      "      8      140.7403  0.0016\n",
      "      6      150.1609  0.0014\n",
      "      9      140.7403  0.0016\n",
      "      7      150.1609  0.0014\n",
      "     10      140.7403  0.0015\n",
      "Restoring best model from epoch 1.\n",
      "      8      150.1609  0.0014\n",
      "      9      150.1609  0.0014\n",
      "     10      150.1609  0.0014\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m222.9811\u001b[0m  0.0023\n",
      "      2      222.9811  0.0015\n",
      "      3      222.9811  0.0014\n",
      "      4      222.9811  0.0016\n",
      "      5      222.9811  0.0014\n",
      "      6      222.9811  0.0014\n",
      "      7      222.9811  0.0014\n",
      "      8      222.9811  0.0014\n",
      "      9      222.9811  0.0013\n",
      "     10      222.9811  0.0014\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m138.4464\u001b[0m  0.0023\n",
      "      2      138.4464  0.0025\n",
      "      3      138.4464  0.0026\n",
      "      4      138.4464  0.0025\n",
      "      5      138.4464  0.0021\n",
      "      6      138.4464  0.0023\n",
      "      7      138.4464  0.0024\n",
      "      8      138.4464  0.0026\n",
      "      9      138.4464  0.0025\n",
      "     10      138.4464  0.0025\n",
      "Restoring best model from epoch 1.\n",
      "y_train breslow final [ 3.000e+00 -7.000e+00  8.000e+00 -8.000e+00 -2.000e+01 -2.100e+01\n",
      " -2.100e+01  2.200e+01 -2.300e+01  2.400e+01 -2.900e+01  3.000e+01\n",
      "  3.100e+01 -3.100e+01 -3.400e+01 -3.500e+01  4.500e+01  5.200e+01\n",
      "  5.700e+01  6.100e+01 -6.400e+01  6.700e+01  7.600e+01  8.100e+01\n",
      "  8.100e+01  8.200e+01 -9.200e+01 -9.900e+01  1.030e+02  1.050e+02\n",
      "  1.050e+02  1.060e+02  1.130e+02 -1.130e+02  1.220e+02  1.240e+02\n",
      "  1.240e+02  1.320e+02  1.380e+02  1.410e+02  1.530e+02 -1.560e+02\n",
      " -1.640e+02  1.680e+02 -1.700e+02 -1.760e+02  1.800e+02 -1.830e+02\n",
      "  1.850e+02  1.850e+02  1.880e+02 -1.890e+02  1.910e+02  1.920e+02\n",
      " -1.980e+02 -2.000e+02 -2.000e+02  2.000e+02  2.010e+02  2.050e+02\n",
      "  2.120e+02  2.180e+02 -2.240e+02 -2.290e+02  2.290e+02  2.350e+02\n",
      " -2.430e+02  2.430e+02  2.430e+02  2.500e+02  2.550e+02  2.590e+02\n",
      "  2.620e+02  2.720e+02 -2.730e+02  2.740e+02  2.760e+02  2.790e+02\n",
      " -2.800e+02 -2.800e+02  2.810e+02  2.820e+02  2.840e+02  2.840e+02\n",
      " -2.880e+02  2.890e+02  2.910e+02  2.920e+02  2.940e+02  2.960e+02\n",
      "  3.000e+02  3.120e+02 -3.230e+02 -3.250e+02 -3.370e+02  3.410e+02\n",
      "  3.420e+02 -3.420e+02 -3.440e+02  3.440e+02  3.480e+02  3.530e+02\n",
      "  3.540e+02 -3.560e+02  3.560e+02 -3.560e+02 -3.580e+02  3.580e+02\n",
      " -3.650e+02  3.660e+02 -3.670e+02 -3.680e+02 -3.710e+02 -3.740e+02\n",
      " -3.740e+02 -3.750e+02 -3.750e+02  3.760e+02 -3.770e+02  3.780e+02\n",
      " -3.780e+02 -3.790e+02 -3.830e+02 -3.840e+02 -3.850e+02 -3.880e+02\n",
      "  3.890e+02 -3.890e+02 -3.890e+02 -3.920e+02 -3.940e+02 -3.960e+02\n",
      "  3.980e+02  4.010e+02  4.060e+02  4.060e+02 -4.080e+02 -4.110e+02\n",
      " -4.110e+02 -4.130e+02 -4.160e+02 -4.190e+02  4.220e+02 -4.270e+02\n",
      " -4.280e+02  4.280e+02 -4.340e+02  4.390e+02 -4.400e+02 -4.490e+02\n",
      " -4.630e+02  4.660e+02 -4.680e+02  4.760e+02 -4.760e+02  4.770e+02\n",
      " -4.790e+02 -4.850e+02 -4.850e+02 -4.860e+02  4.910e+02  4.910e+02\n",
      " -4.910e+02 -4.940e+02  4.960e+02 -5.000e+02  5.070e+02 -5.110e+02\n",
      "  5.130e+02 -5.230e+02 -5.230e+02  5.260e+02  5.330e+02 -5.430e+02\n",
      "  5.490e+02  5.520e+02  5.540e+02  5.580e+02 -5.590e+02  5.600e+02\n",
      "  5.620e+02 -5.640e+02 -5.660e+02  5.700e+02 -5.720e+02 -5.730e+02\n",
      " -5.790e+02 -5.800e+02 -5.820e+02  5.880e+02 -5.930e+02 -5.940e+02\n",
      " -5.940e+02 -6.000e+02 -6.070e+02  6.070e+02 -6.130e+02 -6.150e+02\n",
      " -6.160e+02  6.180e+02 -6.210e+02 -6.250e+02 -6.280e+02  6.330e+02\n",
      "  6.350e+02 -6.360e+02 -6.410e+02 -6.430e+02 -6.470e+02 -6.500e+02\n",
      "  6.520e+02 -6.640e+02  6.690e+02 -6.750e+02 -6.780e+02 -6.780e+02\n",
      " -6.790e+02 -6.900e+02 -6.920e+02 -6.940e+02 -6.990e+02 -7.250e+02\n",
      " -7.360e+02 -7.380e+02 -7.390e+02 -7.420e+02 -7.520e+02  7.620e+02\n",
      "  7.660e+02  7.820e+02 -7.850e+02  7.920e+02  8.010e+02  8.050e+02\n",
      " -8.120e+02 -8.130e+02 -8.380e+02 -8.380e+02 -8.560e+02 -8.620e+02\n",
      " -8.640e+02  8.690e+02  8.740e+02 -8.810e+02  8.810e+02 -8.950e+02\n",
      " -8.990e+02 -9.000e+02 -9.120e+02 -9.280e+02  9.400e+02 -9.400e+02\n",
      " -9.420e+02 -9.460e+02 -9.490e+02 -9.510e+02 -9.720e+02 -9.910e+02\n",
      " -9.970e+02 -1.000e+03 -1.010e+03 -1.016e+03 -1.023e+03 -1.038e+03\n",
      "  1.043e+03 -1.055e+03 -1.072e+03 -1.083e+03 -1.090e+03  1.095e+03\n",
      " -1.106e+03 -1.108e+03 -1.124e+03 -1.132e+03 -1.133e+03 -1.138e+03\n",
      " -1.145e+03  1.153e+03 -1.184e+03 -1.190e+03 -1.210e+03 -1.236e+03\n",
      "  1.294e+03 -1.297e+03 -1.319e+03 -1.328e+03 -1.367e+03 -1.389e+03\n",
      "  1.407e+03 -1.431e+03 -1.484e+03 -1.588e+03 -1.645e+03 -1.646e+03\n",
      "  1.686e+03 -1.690e+03  1.747e+03 -1.765e+03  1.811e+03 -1.851e+03\n",
      " -1.862e+03 -1.935e+03 -2.032e+03  2.100e+03 -2.171e+03  2.197e+03\n",
      " -2.351e+03 -3.196e+03 -3.519e+03 -3.540e+03 -3.720e+03]\n",
      "Concordance Index 0.48521303258145365\n",
      "Integrated Brier Score: 0.43741644608263525\n",
      "y_train breslow final [ 3.000e+00 -7.000e+00  8.000e+00 -8.000e+00 -2.000e+01 -2.100e+01\n",
      " -2.100e+01  2.200e+01 -2.300e+01  2.400e+01 -2.900e+01  3.000e+01\n",
      "  3.100e+01 -3.100e+01 -3.400e+01 -3.500e+01  4.500e+01  5.200e+01\n",
      "  5.700e+01  6.100e+01 -6.400e+01  6.700e+01  7.600e+01  8.100e+01\n",
      "  8.100e+01  8.200e+01 -9.200e+01 -9.900e+01  1.030e+02  1.050e+02\n",
      "  1.050e+02  1.060e+02  1.130e+02 -1.130e+02  1.220e+02  1.240e+02\n",
      "  1.240e+02  1.320e+02  1.380e+02  1.410e+02  1.530e+02 -1.560e+02\n",
      " -1.640e+02  1.680e+02 -1.700e+02 -1.760e+02  1.800e+02 -1.830e+02\n",
      "  1.850e+02  1.850e+02  1.880e+02 -1.890e+02  1.910e+02  1.920e+02\n",
      " -1.980e+02 -2.000e+02 -2.000e+02  2.000e+02  2.010e+02  2.050e+02\n",
      "  2.120e+02  2.180e+02 -2.240e+02 -2.290e+02  2.290e+02  2.350e+02\n",
      " -2.430e+02  2.430e+02  2.430e+02  2.500e+02  2.550e+02  2.590e+02\n",
      "  2.620e+02  2.720e+02 -2.730e+02  2.740e+02  2.760e+02  2.790e+02\n",
      " -2.800e+02 -2.800e+02  2.810e+02  2.820e+02  2.840e+02  2.840e+02\n",
      " -2.880e+02  2.890e+02  2.910e+02  2.920e+02  2.940e+02  2.960e+02\n",
      "  3.000e+02  3.120e+02 -3.230e+02 -3.250e+02 -3.370e+02  3.410e+02\n",
      "  3.420e+02 -3.420e+02 -3.440e+02  3.440e+02  3.480e+02  3.530e+02\n",
      "  3.540e+02 -3.560e+02  3.560e+02 -3.560e+02 -3.580e+02  3.580e+02\n",
      " -3.650e+02  3.660e+02 -3.670e+02 -3.680e+02 -3.710e+02 -3.740e+02\n",
      " -3.740e+02 -3.750e+02 -3.750e+02  3.760e+02 -3.770e+02  3.780e+02\n",
      " -3.780e+02 -3.790e+02 -3.830e+02 -3.840e+02 -3.850e+02 -3.880e+02\n",
      "  3.890e+02 -3.890e+02 -3.890e+02 -3.920e+02 -3.940e+02 -3.960e+02\n",
      "  3.980e+02  4.010e+02  4.060e+02  4.060e+02 -4.080e+02 -4.110e+02\n",
      " -4.110e+02 -4.130e+02 -4.160e+02 -4.190e+02  4.220e+02 -4.270e+02\n",
      " -4.280e+02  4.280e+02 -4.340e+02  4.390e+02 -4.400e+02 -4.490e+02\n",
      " -4.630e+02  4.660e+02 -4.680e+02  4.760e+02 -4.760e+02  4.770e+02\n",
      " -4.790e+02 -4.850e+02 -4.850e+02 -4.860e+02  4.910e+02  4.910e+02\n",
      " -4.910e+02 -4.940e+02  4.960e+02 -5.000e+02  5.070e+02 -5.110e+02\n",
      "  5.130e+02 -5.230e+02 -5.230e+02  5.260e+02  5.330e+02 -5.430e+02\n",
      "  5.490e+02  5.520e+02  5.540e+02  5.580e+02 -5.590e+02  5.600e+02\n",
      "  5.620e+02 -5.640e+02 -5.660e+02  5.700e+02 -5.720e+02 -5.730e+02\n",
      " -5.790e+02 -5.800e+02 -5.820e+02  5.880e+02 -5.930e+02 -5.940e+02\n",
      " -5.940e+02 -6.000e+02 -6.070e+02  6.070e+02 -6.130e+02 -6.150e+02\n",
      " -6.160e+02  6.180e+02 -6.210e+02 -6.250e+02 -6.280e+02  6.330e+02\n",
      "  6.350e+02 -6.360e+02 -6.410e+02 -6.430e+02 -6.470e+02 -6.500e+02\n",
      "  6.520e+02 -6.640e+02  6.690e+02 -6.750e+02 -6.780e+02 -6.780e+02\n",
      " -6.790e+02 -6.900e+02 -6.920e+02 -6.940e+02 -6.990e+02 -7.250e+02\n",
      " -7.360e+02 -7.380e+02 -7.390e+02 -7.420e+02 -7.520e+02  7.620e+02\n",
      "  7.660e+02  7.820e+02 -7.850e+02  7.920e+02  8.010e+02  8.050e+02\n",
      " -8.120e+02 -8.130e+02 -8.380e+02 -8.380e+02 -8.560e+02 -8.620e+02\n",
      " -8.640e+02  8.690e+02  8.740e+02 -8.810e+02  8.810e+02 -8.950e+02\n",
      " -8.990e+02 -9.000e+02 -9.120e+02 -9.280e+02  9.400e+02 -9.400e+02\n",
      " -9.420e+02 -9.460e+02 -9.490e+02 -9.510e+02 -9.720e+02 -9.910e+02\n",
      " -9.970e+02 -1.000e+03 -1.010e+03 -1.016e+03 -1.023e+03 -1.038e+03\n",
      "  1.043e+03 -1.055e+03 -1.072e+03 -1.083e+03 -1.090e+03  1.095e+03\n",
      " -1.106e+03 -1.108e+03 -1.124e+03 -1.132e+03 -1.133e+03 -1.138e+03\n",
      " -1.145e+03  1.153e+03 -1.184e+03 -1.190e+03 -1.210e+03 -1.236e+03\n",
      "  1.294e+03 -1.297e+03 -1.319e+03 -1.328e+03 -1.367e+03 -1.389e+03\n",
      "  1.407e+03 -1.431e+03 -1.484e+03 -1.588e+03 -1.645e+03 -1.646e+03\n",
      "  1.686e+03 -1.690e+03  1.747e+03 -1.765e+03  1.811e+03 -1.851e+03\n",
      " -1.862e+03 -1.935e+03 -2.032e+03  2.100e+03 -2.171e+03  2.197e+03\n",
      " -2.351e+03 -3.196e+03 -3.519e+03 -3.540e+03 -3.720e+03]\n",
      "durations 14.0 2267.0\n",
      "Concordance Index 0.49336283185840707\n",
      "Integrated Brier Score: 0.33808183663286206\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m278.6340\u001b[0m  0.0020\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1           nan  0.0019\n",
      "      2      278.6340  0.0016\n",
      "      2           nan  0.0016\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      3      278.6340  0.0020\n",
      "      4      278.6340  0.0017\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m67.6404\u001b[0m  0.0020\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      5      278.6340  0.0015\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2       67.6404  0.0016\n",
      "      6      278.6340  0.0015\n",
      "      3       67.6404  0.0015\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m105.9626\u001b[0m  0.0017\n",
      "      7      278.6340  0.0014\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      3           nan  0.0029\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      4       67.6404  0.0015\n",
      "      2      105.9626  0.0016\n",
      "      8      278.6340  0.0016\n",
      "      4           nan  0.0018\n",
      "      5       67.6404  0.0016\n",
      "      3      105.9626  0.0015\n",
      "      9      278.6340  0.0015\n",
      "      5           nan  0.0017\n",
      "      6       67.6404  0.0017\n",
      "      4      105.9626  0.0016\n",
      "     10      278.6340  0.0015\n",
      "Restoring best model from epoch 1.\n",
      "      6           nan  0.0015\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m90.6806\u001b[0m  0.0034\n",
      "      7       67.6404  0.0016      5      105.9626  0.0015\n",
      "\n",
      "      7           nan  0.0015\n",
      "      8       67.6404  0.0015\n",
      "      6      105.9626  0.0014\n",
      "      8           nan  0.0015\n",
      "      2       90.6806  0.0032\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      9       67.6404  0.0015\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      7      105.9626  0.0017\n",
      "      3       90.6806  0.0015\n",
      "     10       67.6404  0.0015\n",
      "Restoring best model from epoch 1.\n",
      "      8      105.9626  0.0015\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m110.5265\u001b[0m  0.0018\n",
      "      4       90.6806  0.0015\n",
      "      9      105.9626  0.0015\n",
      "      5       90.6806  0.0015\n",
      "      2      110.5265  0.0016\n",
      "     10      105.9626  0.0015\n",
      "Restoring best model from epoch 1.\n",
      "      6       90.6806  0.0015\n",
      "      3      110.5265  0.0016\n",
      "      7       90.6806  0.0014\n",
      "      4      110.5265  0.0015\n",
      "      8       90.6806  0.0014\n",
      "      5      110.5265  0.0015\n",
      "      9       90.6806  0.0014\n",
      "      6      110.5265  0.0015\n",
      "     10       90.6806  0.0014\n",
      "Restoring best model from epoch 1.\n",
      "      9           nan  0.0146\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "      7      110.5265  0.0036\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m407.3492\u001b[0m  0.0018\n",
      "      8      110.5265  0.0043\n",
      "      2      407.3492  0.0016\n",
      "      3      407.3492  0.0015\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      9      110.5265  0.0026\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      4      407.3492  0.0015\n",
      "     10      110.5265  0.0016\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Restoring best model from epoch 1.\n",
      "      5      407.3492  0.0015\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m124.5032\u001b[0m  0.0018\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m406.0663\u001b[0m  0.0017\n",
      "      6      407.3492  0.0015\n",
      "      2      406.0663  0.0015\n",
      "      7      407.3492  0.0015\n",
      "      3      406.0663  0.0015\n",
      "      2      124.5032  0.0033\n",
      "      8      407.3492  0.0015\n",
      "      4      406.0663  0.0015\n",
      "      9      407.3492  0.0015\n",
      "      3      124.5032  0.0028\n",
      "      5      406.0663  0.0014\n",
      "     10      407.3492  0.0015\n",
      "Restoring best model from epoch 1.\n",
      "      6      406.0663  0.0014\n",
      "      7      406.0663  0.0016\n",
      "      4      124.5032  0.0030\n",
      "      8      406.0663  0.0016\n",
      "      9      406.0663  0.0015\n",
      "     10      406.0663  0.0014\n",
      "Restoring best model from epoch 1.\n",
      "      5      124.5032  0.0047\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      6      124.5032  0.0053\n",
      "      7      124.5032  0.0025\n",
      "      8      124.5032  0.0015\n",
      "      9      124.5032  0.0015\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m422.5891\u001b[0m  0.0053\n",
      "     10      124.5032  0.0015\n",
      "Restoring best model from epoch 1.\n",
      "      2      422.5891  0.0019\n",
      "      3      422.5891  0.0016\n",
      "      4      422.5891  0.0015\n",
      "      5      422.5891  0.0015\n",
      "      6      422.5891  0.0015\n",
      "      7      422.5891  0.0015\n",
      "      8      422.5891  0.0015\n",
      "      9      422.5891  0.0040\n",
      "     10      422.5891  0.0034\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m142.6711\u001b[0m  0.0021\n",
      "      2      142.6711  0.0024\n",
      "      3      142.6711  0.0026\n",
      "      4      142.6711  0.0023\n",
      "      5      142.6711  0.0023\n",
      "      6      142.6711  0.0023\n",
      "      7      142.6711  0.0026\n",
      "      8      142.6711  0.0026\n",
      "      9      142.6711  0.0020\n",
      "     10      142.6711  0.0038\n",
      "Restoring best model from epoch 1.\n",
      "y_train breslow final [ 3.000e+00 -7.000e+00 -8.000e+00 -1.400e+01 -1.600e+01 -1.700e+01\n",
      " -2.000e+01 -2.100e+01 -2.100e+01 -2.300e+01  3.000e+01 -3.000e+01\n",
      "  3.100e+01 -3.100e+01 -3.400e+01 -3.500e+01  4.500e+01 -4.600e+01\n",
      "  5.200e+01  6.100e+01 -6.400e+01  6.700e+01 -7.700e+01  8.100e+01\n",
      "  8.100e+01  8.200e+01 -9.200e+01 -9.900e+01  1.030e+02  1.050e+02\n",
      "  1.050e+02  1.060e+02 -1.130e+02  1.220e+02  1.240e+02  1.240e+02\n",
      " -1.310e+02  1.320e+02  1.380e+02  1.410e+02  1.560e+02 -1.640e+02\n",
      "  1.660e+02  1.680e+02 -1.700e+02  1.740e+02 -1.760e+02  1.800e+02\n",
      "  1.800e+02  1.850e+02  1.850e+02  1.880e+02 -1.890e+02  1.910e+02\n",
      " -1.980e+02 -2.000e+02 -2.000e+02  2.000e+02  2.000e+02  2.010e+02\n",
      "  2.120e+02  2.150e+02 -2.240e+02 -2.250e+02  2.290e+02  2.350e+02\n",
      " -2.430e+02  2.430e+02 -2.450e+02  2.450e+02  2.550e+02  2.590e+02\n",
      "  2.620e+02  2.720e+02  2.730e+02  2.740e+02  2.750e+02  2.790e+02\n",
      " -2.800e+02 -2.800e+02  2.820e+02  2.840e+02  2.840e+02 -2.870e+02\n",
      " -2.880e+02  2.890e+02  2.910e+02  2.920e+02  2.940e+02  3.000e+02\n",
      "  3.120e+02 -3.250e+02 -3.350e+02  3.360e+02 -3.370e+02  3.410e+02\n",
      "  3.420e+02 -3.420e+02 -3.440e+02  3.440e+02 -3.460e+02  3.480e+02\n",
      "  3.480e+02  3.540e+02 -3.560e+02  3.560e+02 -3.580e+02  3.580e+02\n",
      "  3.590e+02  3.660e+02 -3.670e+02  3.700e+02 -3.710e+02 -3.740e+02\n",
      " -3.750e+02 -3.750e+02  3.770e+02 -3.770e+02  3.780e+02 -3.780e+02\n",
      " -3.780e+02 -3.790e+02 -3.810e+02 -3.830e+02 -3.830e+02 -3.840e+02\n",
      " -3.850e+02  3.890e+02 -3.890e+02 -3.900e+02 -3.920e+02 -3.940e+02\n",
      " -3.960e+02  3.960e+02  3.980e+02 -4.000e+02  4.010e+02  4.030e+02\n",
      "  4.060e+02  4.060e+02 -4.080e+02 -4.130e+02 -4.160e+02 -4.190e+02\n",
      "  4.220e+02  4.220e+02  4.260e+02 -4.270e+02 -4.270e+02 -4.280e+02\n",
      " -4.310e+02 -4.340e+02  4.460e+02 -4.490e+02 -4.500e+02  4.570e+02\n",
      " -4.630e+02  4.660e+02 -4.680e+02  4.740e+02 -4.760e+02  4.770e+02\n",
      " -4.790e+02 -4.850e+02 -4.850e+02 -4.860e+02 -4.910e+02  4.960e+02\n",
      "  5.070e+02 -5.110e+02 -5.190e+02 -5.210e+02 -5.230e+02 -5.250e+02\n",
      "  5.260e+02  5.330e+02  5.430e+02 -5.430e+02  5.490e+02  5.520e+02\n",
      "  5.540e+02  5.540e+02  5.600e+02  5.620e+02 -5.640e+02 -5.660e+02\n",
      "  5.700e+02 -5.720e+02 -5.770e+02 -5.790e+02 -5.800e+02 -5.820e+02\n",
      "  5.880e+02 -5.930e+02 -5.940e+02 -5.940e+02 -5.950e+02 -6.000e+02\n",
      " -6.070e+02  6.070e+02 -6.130e+02 -6.150e+02 -6.160e+02  6.180e+02\n",
      " -6.250e+02 -6.280e+02  6.350e+02 -6.360e+02  6.400e+02 -6.410e+02\n",
      " -6.430e+02 -6.440e+02 -6.470e+02 -6.500e+02  6.520e+02  6.610e+02\n",
      " -6.640e+02 -6.660e+02  6.690e+02  6.750e+02 -6.780e+02 -6.780e+02\n",
      " -6.900e+02 -6.920e+02 -6.940e+02  7.120e+02 -7.240e+02 -7.250e+02\n",
      " -7.360e+02 -7.380e+02 -7.390e+02 -7.420e+02 -7.520e+02 -7.540e+02\n",
      "  7.660e+02  7.790e+02  7.820e+02 -7.850e+02  7.920e+02  7.940e+02\n",
      "  8.010e+02  8.050e+02 -8.120e+02 -8.130e+02 -8.190e+02 -8.250e+02\n",
      "  8.320e+02 -8.560e+02  8.690e+02  8.740e+02 -8.810e+02  8.810e+02\n",
      " -8.820e+02 -8.950e+02 -8.990e+02 -9.000e+02 -9.120e+02 -9.280e+02\n",
      "  9.400e+02 -9.400e+02 -9.420e+02 -9.420e+02 -9.460e+02 -9.510e+02\n",
      " -9.720e+02 -9.810e+02 -9.890e+02 -9.910e+02 -9.970e+02 -1.000e+03\n",
      " -1.002e+03 -1.010e+03 -1.016e+03 -1.038e+03 -1.083e+03 -1.090e+03\n",
      " -1.100e+03 -1.106e+03 -1.108e+03 -1.132e+03 -1.138e+03  1.153e+03\n",
      " -1.160e+03 -1.184e+03 -1.190e+03 -1.200e+03 -1.210e+03 -1.223e+03\n",
      "  1.294e+03 -1.297e+03 -1.319e+03 -1.328e+03 -1.389e+03  1.407e+03\n",
      " -1.431e+03 -1.588e+03 -1.645e+03 -1.646e+03  1.747e+03  1.811e+03\n",
      " -1.851e+03 -1.862e+03 -1.918e+03 -1.964e+03 -2.032e+03 -2.171e+03\n",
      "  2.197e+03 -2.267e+03 -2.351e+03 -3.519e+03 -3.540e+03]\n",
      "Concordance Index 0.5113797093995114\n",
      "Integrated Brier Score: 0.49145884279410046\n",
      "y_train breslow final [ 3.000e+00 -7.000e+00 -8.000e+00 -1.400e+01 -1.600e+01 -1.700e+01\n",
      " -2.000e+01 -2.100e+01 -2.100e+01 -2.300e+01  3.000e+01 -3.000e+01\n",
      "  3.100e+01 -3.100e+01 -3.400e+01 -3.500e+01  4.500e+01 -4.600e+01\n",
      "  5.200e+01  6.100e+01 -6.400e+01  6.700e+01 -7.700e+01  8.100e+01\n",
      "  8.100e+01  8.200e+01 -9.200e+01 -9.900e+01  1.030e+02  1.050e+02\n",
      "  1.050e+02  1.060e+02 -1.130e+02  1.220e+02  1.240e+02  1.240e+02\n",
      " -1.310e+02  1.320e+02  1.380e+02  1.410e+02  1.560e+02 -1.640e+02\n",
      "  1.660e+02  1.680e+02 -1.700e+02  1.740e+02 -1.760e+02  1.800e+02\n",
      "  1.800e+02  1.850e+02  1.850e+02  1.880e+02 -1.890e+02  1.910e+02\n",
      " -1.980e+02 -2.000e+02 -2.000e+02  2.000e+02  2.000e+02  2.010e+02\n",
      "  2.120e+02  2.150e+02 -2.240e+02 -2.250e+02  2.290e+02  2.350e+02\n",
      " -2.430e+02  2.430e+02 -2.450e+02  2.450e+02  2.550e+02  2.590e+02\n",
      "  2.620e+02  2.720e+02  2.730e+02  2.740e+02  2.750e+02  2.790e+02\n",
      " -2.800e+02 -2.800e+02  2.820e+02  2.840e+02  2.840e+02 -2.870e+02\n",
      " -2.880e+02  2.890e+02  2.910e+02  2.920e+02  2.940e+02  3.000e+02\n",
      "  3.120e+02 -3.250e+02 -3.350e+02  3.360e+02 -3.370e+02  3.410e+02\n",
      "  3.420e+02 -3.420e+02 -3.440e+02  3.440e+02 -3.460e+02  3.480e+02\n",
      "  3.480e+02  3.540e+02 -3.560e+02  3.560e+02 -3.580e+02  3.580e+02\n",
      "  3.590e+02  3.660e+02 -3.670e+02  3.700e+02 -3.710e+02 -3.740e+02\n",
      " -3.750e+02 -3.750e+02  3.770e+02 -3.770e+02  3.780e+02 -3.780e+02\n",
      " -3.780e+02 -3.790e+02 -3.810e+02 -3.830e+02 -3.830e+02 -3.840e+02\n",
      " -3.850e+02  3.890e+02 -3.890e+02 -3.900e+02 -3.920e+02 -3.940e+02\n",
      " -3.960e+02  3.960e+02  3.980e+02 -4.000e+02  4.010e+02  4.030e+02\n",
      "  4.060e+02  4.060e+02 -4.080e+02 -4.130e+02 -4.160e+02 -4.190e+02\n",
      "  4.220e+02  4.220e+02  4.260e+02 -4.270e+02 -4.270e+02 -4.280e+02\n",
      " -4.310e+02 -4.340e+02  4.460e+02 -4.490e+02 -4.500e+02  4.570e+02\n",
      " -4.630e+02  4.660e+02 -4.680e+02  4.740e+02 -4.760e+02  4.770e+02\n",
      " -4.790e+02 -4.850e+02 -4.850e+02 -4.860e+02 -4.910e+02  4.960e+02\n",
      "  5.070e+02 -5.110e+02 -5.190e+02 -5.210e+02 -5.230e+02 -5.250e+02\n",
      "  5.260e+02  5.330e+02  5.430e+02 -5.430e+02  5.490e+02  5.520e+02\n",
      "  5.540e+02  5.540e+02  5.600e+02  5.620e+02 -5.640e+02 -5.660e+02\n",
      "  5.700e+02 -5.720e+02 -5.770e+02 -5.790e+02 -5.800e+02 -5.820e+02\n",
      "  5.880e+02 -5.930e+02 -5.940e+02 -5.940e+02 -5.950e+02 -6.000e+02\n",
      " -6.070e+02  6.070e+02 -6.130e+02 -6.150e+02 -6.160e+02  6.180e+02\n",
      " -6.250e+02 -6.280e+02  6.350e+02 -6.360e+02  6.400e+02 -6.410e+02\n",
      " -6.430e+02 -6.440e+02 -6.470e+02 -6.500e+02  6.520e+02  6.610e+02\n",
      " -6.640e+02 -6.660e+02  6.690e+02  6.750e+02 -6.780e+02 -6.780e+02\n",
      " -6.900e+02 -6.920e+02 -6.940e+02  7.120e+02 -7.240e+02 -7.250e+02\n",
      " -7.360e+02 -7.380e+02 -7.390e+02 -7.420e+02 -7.520e+02 -7.540e+02\n",
      "  7.660e+02  7.790e+02  7.820e+02 -7.850e+02  7.920e+02  7.940e+02\n",
      "  8.010e+02  8.050e+02 -8.120e+02 -8.130e+02 -8.190e+02 -8.250e+02\n",
      "  8.320e+02 -8.560e+02  8.690e+02  8.740e+02 -8.810e+02  8.810e+02\n",
      " -8.820e+02 -8.950e+02 -8.990e+02 -9.000e+02 -9.120e+02 -9.280e+02\n",
      "  9.400e+02 -9.400e+02 -9.420e+02 -9.420e+02 -9.460e+02 -9.510e+02\n",
      " -9.720e+02 -9.810e+02 -9.890e+02 -9.910e+02 -9.970e+02 -1.000e+03\n",
      " -1.002e+03 -1.010e+03 -1.016e+03 -1.038e+03 -1.083e+03 -1.090e+03\n",
      " -1.100e+03 -1.106e+03 -1.108e+03 -1.132e+03 -1.138e+03  1.153e+03\n",
      " -1.160e+03 -1.184e+03 -1.190e+03 -1.200e+03 -1.210e+03 -1.223e+03\n",
      "  1.294e+03 -1.297e+03 -1.319e+03 -1.328e+03 -1.389e+03  1.407e+03\n",
      " -1.431e+03 -1.588e+03 -1.645e+03 -1.646e+03  1.747e+03  1.811e+03\n",
      " -1.851e+03 -1.862e+03 -1.918e+03 -1.964e+03 -2.032e+03 -2.171e+03\n",
      "  2.197e+03 -2.267e+03 -2.351e+03 -3.519e+03 -3.540e+03]\n",
      "durations 8.0 3720.0\n",
      "Concordance Index 0.524092409240924\n",
      "Integrated Brier Score: 0.4411507675105061\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m70.7648\u001b[0m  0.0018\n",
      "      2       70.7648  0.0015\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      3       70.7648  0.0015\n",
      "      4       70.7648  0.0014\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m263.4180\u001b[0m  0.0018\n",
      "      2      263.4180  0.0016\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      3      263.4180  0.0015\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      4      263.4180  0.0015\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      5       70.7648  0.0077\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m59.4779\u001b[0m  0.0034\n",
      "      5      263.4180  0.0037\n",
      "      6       70.7648  0.0016\n",
      "      2       59.4779  0.0015\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      6      263.4180  0.0016\n",
      "      7       70.7648  0.0015\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      3       59.4779  0.0015\n",
      "      8       70.7648  0.0015\n",
      "      7      263.4180  0.0015\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      4       59.4779  0.0016\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m87.9786\u001b[0m  0.0017\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      9       70.7648  0.0016\n",
      "      8      263.4180  0.0016\n",
      "      5       59.4779  0.0015\n",
      "      2       87.9786  0.0015\n",
      "     10       70.7648  0.0015\n",
      "      9      263.4180  0.0015\n",
      "Restoring best model from epoch 1.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m89.5197\u001b[0m  0.0018\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m90.4784\u001b[0m  0.0017\n",
      "      3       87.9786  0.0014\n",
      "     10      263.4180  0.0015\n",
      "Restoring best model from epoch 1.\n",
      "      6       59.4779  0.0024\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      2       89.5197  0.0016\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      4       87.9786  0.0014\n",
      "      2       90.4784  0.0018\n",
      "      7       59.4779  0.0015\n",
      "      3       89.5197  0.0015\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1           nan  0.0037\n",
      "      3       90.4784  0.0015\n",
      "      8       59.4779  0.0014\n",
      "      5       87.9786  0.0024\n",
      "      4       89.5197  0.0017\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m261.9019\u001b[0m  0.0025\n",
      "      2           nan  0.0017\n",
      "      4       90.4784  0.0015\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      9       59.4779  0.0014\n",
      "      6       87.9786  0.0015\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      5       89.5197  0.0015\n",
      "      2      261.9019  0.0016\n",
      "      3           nan  0.0014\n",
      "      5       90.4784  0.0014\n",
      "     10       59.4779  0.0014\n",
      "Restoring best model from epoch 1.\n",
      "      7       87.9786  0.0015\n",
      "      6       89.5197  0.0014\n",
      "      4           nan  0.0014\n",
      "      3      261.9019  0.0015\n",
      "      6       90.4784  0.0014\n",
      "      8       87.9786  0.0015\n",
      "      7       89.5197  0.0015\n",
      "      5           nan  0.0014\n",
      "      4      261.9019  0.0014\n",
      "      7       90.4784  0.0014\n",
      "      9       87.9786  0.0015\n",
      "      8       89.5197  0.0014\n",
      "      6           nan  0.0014\n",
      "      5      261.9019  0.0014\n",
      "      8       90.4784  0.0014\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m271.5542\u001b[0m  0.0048\n",
      "     10       87.9786  0.0015\n",
      "Restoring best model from epoch 1.\n",
      "      7           nan  0.0014\n",
      "      9       89.5197  0.0014\n",
      "      9       90.4784  0.0017\n",
      "      6      261.9019  0.0020\n",
      "      2      271.5542  0.0016\n",
      "      8           nan  0.0014\n",
      "     10       90.4784  0.0014\n",
      "Restoring best model from epoch 1.\n",
      "     10       89.5197  0.0023\n",
      "Restoring best model from epoch 1.\n",
      "      3      271.5542  0.0015\n",
      "      9           nan  0.0014\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "      4      271.5542  0.0015\n",
      "      5      271.5542  0.0015\n",
      "      6      271.5542  0.0015\n",
      "      7      261.9019  0.0046\n",
      "      8      261.9019  0.0019\n",
      "      7      271.5542  0.0024\n",
      "      9      261.9019  0.0015\n",
      "      8      271.5542  0.0016\n",
      "     10      261.9019  0.0014\n",
      "Restoring best model from epoch 1.\n",
      "      9      271.5542  0.0015\n",
      "     10      271.5542  0.0015\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m395.3599\u001b[0m  0.0049\n",
      "      2      395.3599  0.0028\n",
      "      3      395.3599  0.0016\n",
      "      4      395.3599  0.0015\n",
      "      5      395.3599  0.0015\n",
      "      6      395.3599  0.0014\n",
      "      7      395.3599  0.0014\n",
      "      8      395.3599  0.0020\n",
      "      9      395.3599  0.0015\n",
      "     10      395.3599  0.0015\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m114.0669\u001b[0m  0.0022\n",
      "      2      114.0669  0.0026\n",
      "      3      114.0669  0.0026\n",
      "      4      114.0669  0.0024\n",
      "      5      114.0669  0.0024\n",
      "      6      114.0669  0.0024\n",
      "      7      114.0669  0.0026\n",
      "      8      114.0669  0.0024\n",
      "      9      114.0669  0.0025\n",
      "     10      114.0669  0.0033\n",
      "Restoring best model from epoch 1.\n",
      "y_train breslow final [   -7.     8.    -8.   -14.   -16.   -17.   -20.    22.   -23.    24.\n",
      "   -29.   -30.    31.   -31.   -34.   -46.    52.    57.    61.   -64.\n",
      "    67.    76.   -77.    81.    81.    82.   -92.   103.   106.   113.\n",
      "  -113.   124.   124.  -131.   132.   138.   153.   156.  -156.  -164.\n",
      "   166.  -170.   174.   180.   180.  -183.   185.   188.  -189.   191.\n",
      "   192.  -198.  -198.  -200.  -200.   200.   200.   201.   205.   212.\n",
      "   215.   218.  -224.  -225.  -229.   229.   235.  -243.   243.   243.\n",
      "  -245.   245.   250.   259.   273.  -273.   274.   275.   276.  -280.\n",
      "   281.   282.   284.   284.  -287.   291.   292.   296.   300.   300.\n",
      "   312.  -323.  -335.   336.  -337.   342.  -342.  -344.  -344.   344.\n",
      "  -346.   348.   353.   354.  -356.   356.  -356.  -358.   358.   359.\n",
      "  -365.  -368.   370.  -371.  -374.  -374.  -375.   376.   377.  -377.\n",
      "   378.  -378.  -379.  -381.  -383.  -383.  -383.  -384.  -385.  -388.\n",
      "   389.  -389.  -390.  -392.  -394.   396.   398.  -400.   403.   406.\n",
      "  -408.  -411.  -411.  -413.  -419.   422.   422.   426.  -427.  -427.\n",
      "  -428.   428.  -431.   439.  -440.   446.  -449.  -450.   457.   466.\n",
      "  -468.   474.   476.  -476.  -485.  -485.  -486.   491.   491.  -494.\n",
      "   496.  -500.   507.  -511.   513.  -519.  -521.  -523.  -523.  -525.\n",
      "   526.   533.   543.   549.   552.   554.   554.   558.  -559.   560.\n",
      "   562.  -564.   570.  -572.  -573.  -577.  -580.  -582.  -594.  -595.\n",
      "  -600.  -607.   607.  -613.  -616.  -621.  -628.   633.   640.  -641.\n",
      "  -643.  -644.   661.  -664.  -666.   669.  -675.   675.  -678.  -679.\n",
      "  -690.  -692.  -699.   712.  -724.  -725.  -736.  -738.  -739.  -752.\n",
      "  -754.   762.   766.   779.   782.   792.   794.   801.   805.  -813.\n",
      "  -819.  -825.   832.  -838.  -838.  -862.  -864.   869.  -881.  -882.\n",
      "  -895.  -899.  -912.  -912.  -940.  -942.  -942.  -949.  -972.  -981.\n",
      "  -989.  -991.  -997. -1002. -1016. -1023. -1038.  1043. -1055. -1072.\n",
      " -1083. -1090.  1095. -1100. -1108. -1124. -1133. -1145. -1160. -1200.\n",
      " -1223. -1236.  1294. -1319. -1328. -1367. -1389.  1407. -1431. -1484.\n",
      " -1588. -1645. -1646.  1686. -1690. -1765.  1811. -1851. -1862. -1918.\n",
      " -1935. -1964. -2032.  2100. -2171. -2267. -2351. -3196. -3519. -3540.\n",
      " -3720.]\n",
      "Concordance Index 0.5020824479388015\n",
      "Integrated Brier Score: 0.42427822611005345\n",
      "y_train breslow final [   -7.     8.    -8.   -14.   -16.   -17.   -20.    22.   -23.    24.\n",
      "   -29.   -30.    31.   -31.   -34.   -46.    52.    57.    61.   -64.\n",
      "    67.    76.   -77.    81.    81.    82.   -92.   103.   106.   113.\n",
      "  -113.   124.   124.  -131.   132.   138.   153.   156.  -156.  -164.\n",
      "   166.  -170.   174.   180.   180.  -183.   185.   188.  -189.   191.\n",
      "   192.  -198.  -198.  -200.  -200.   200.   200.   201.   205.   212.\n",
      "   215.   218.  -224.  -225.  -229.   229.   235.  -243.   243.   243.\n",
      "  -245.   245.   250.   259.   273.  -273.   274.   275.   276.  -280.\n",
      "   281.   282.   284.   284.  -287.   291.   292.   296.   300.   300.\n",
      "   312.  -323.  -335.   336.  -337.   342.  -342.  -344.  -344.   344.\n",
      "  -346.   348.   353.   354.  -356.   356.  -356.  -358.   358.   359.\n",
      "  -365.  -368.   370.  -371.  -374.  -374.  -375.   376.   377.  -377.\n",
      "   378.  -378.  -379.  -381.  -383.  -383.  -383.  -384.  -385.  -388.\n",
      "   389.  -389.  -390.  -392.  -394.   396.   398.  -400.   403.   406.\n",
      "  -408.  -411.  -411.  -413.  -419.   422.   422.   426.  -427.  -427.\n",
      "  -428.   428.  -431.   439.  -440.   446.  -449.  -450.   457.   466.\n",
      "  -468.   474.   476.  -476.  -485.  -485.  -486.   491.   491.  -494.\n",
      "   496.  -500.   507.  -511.   513.  -519.  -521.  -523.  -523.  -525.\n",
      "   526.   533.   543.   549.   552.   554.   554.   558.  -559.   560.\n",
      "   562.  -564.   570.  -572.  -573.  -577.  -580.  -582.  -594.  -595.\n",
      "  -600.  -607.   607.  -613.  -616.  -621.  -628.   633.   640.  -641.\n",
      "  -643.  -644.   661.  -664.  -666.   669.  -675.   675.  -678.  -679.\n",
      "  -690.  -692.  -699.   712.  -724.  -725.  -736.  -738.  -739.  -752.\n",
      "  -754.   762.   766.   779.   782.   792.   794.   801.   805.  -813.\n",
      "  -819.  -825.   832.  -838.  -838.  -862.  -864.   869.  -881.  -882.\n",
      "  -895.  -899.  -912.  -912.  -940.  -942.  -942.  -949.  -972.  -981.\n",
      "  -989.  -991.  -997. -1002. -1016. -1023. -1038.  1043. -1055. -1072.\n",
      " -1083. -1090.  1095. -1100. -1108. -1124. -1133. -1145. -1160. -1200.\n",
      " -1223. -1236.  1294. -1319. -1328. -1367. -1389.  1407. -1431. -1484.\n",
      " -1588. -1645. -1646.  1686. -1690. -1765.  1811. -1851. -1862. -1918.\n",
      " -1935. -1964. -2032.  2100. -2171. -2267. -2351. -3196. -3519. -3540.\n",
      " -3720.]\n",
      "durations 3.0 2197.0\n",
      "Concordance Index 0.44214876033057854\n",
      "Integrated Brier Score: 0.34304240093124055\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m80.4718\u001b[0m  0.0018\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m80.0153\u001b[0m  0.0022\n",
      "      2       80.0153  0.0017\n",
      "      2       80.4718  0.0047\n",
      "      3       80.0153  0.0018\n",
      "      4       80.0153  0.0017\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      5       80.0153  0.0016\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      3       80.4718  0.0057\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m335.1528\u001b[0m  0.0017\n",
      "      6       80.0153  0.0034\n",
      "      2      335.1528  0.0016\n",
      "      4       80.4718  0.0038\n",
      "      7       80.0153  0.0015\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      3      335.1528  0.0015\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      5       80.4718  0.0027\n",
      "      4      335.1528  0.0015\n",
      "      8       80.0153  0.0036\n",
      "      6       80.4718  0.0017\n",
      "      5      335.1528  0.0021\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1           nan  0.0018      9       80.0153  0.0023\n",
      "\n",
      "      6      335.1528  0.0017\n",
      "      7       80.4718  0.0020\n",
      "      7      335.1528  0.0016\n",
      "      2           nan  0.0018\n",
      "      8       80.4718  0.0016\n",
      "     10       80.0153  0.0029\n",
      "Restoring best model from epoch 1.\n",
      "      8      335.1528  0.0015\n",
      "      3           nan  0.0016\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      9       80.4718  0.0015\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      9      335.1528  0.0014\n",
      "      4           nan  0.0016\n",
      "     10       80.4718  0.0015\n",
      "Restoring best model from epoch 1.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m243.6594\u001b[0m  0.0017\n",
      "      5           nan  0.0015\n",
      "     10      335.1528  0.0021\n",
      "Restoring best model from epoch 1.\n",
      "      6           nan  0.0015\n",
      "      2      243.6594  0.0015\n",
      "      3      243.6594  0.0015\n",
      "      7           nan  0.0022\n",
      "      4      243.6594  0.0015\n",
      "      8           nan  0.0017\n",
      "      5      243.6594  0.0014\n",
      "      9           nan  0.0015\n",
      "      6      243.6594  0.0020\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "      7      243.6594  0.0016\n",
      "      8      243.6594  0.0014\n",
      "      9      243.6594  0.0014\n",
      "     10      243.6594  0.0014\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m79.1298\u001b[0m  0.0017\n",
      "      2       79.1298  0.0015\n",
      "      3       79.1298  0.0014\n",
      "      4       79.1298  0.0015\n",
      "      5       79.1298  0.0015\n",
      "      6       79.1298  0.0014\n",
      "      7       79.1298  0.0014\n",
      "      8       79.1298  0.0014\n",
      "      9       79.1298  0.0014\n",
      "     10       79.1298  0.0014\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m247.8025\u001b[0m  0.0018\n",
      "      2      247.8025  0.0015\n",
      "      3      247.8025  0.0014\n",
      "      4      247.8025  0.0014\n",
      "      5      247.8025  0.0014\n",
      "      6      247.8025  0.0251\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      7      247.8025  0.0024\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m323.6726\u001b[0m  0.0018\n",
      "      2      323.6726  0.0016\n",
      "      8      247.8025  0.0035\n",
      "      3      323.6726  0.0015\n",
      "      9      247.8025  0.0016\n",
      "      4      323.6726  0.0015\n",
      "     10      247.8025  0.0014\n",
      "Restoring best model from epoch 1.\n",
      "      5      323.6726  0.0014\n",
      "      6      323.6726  0.0014\n",
      "      7      323.6726  0.0014\n",
      "      8      323.6726  0.0015\n",
      "      9      323.6726  0.0015\n",
      "     10      323.6726  0.0035\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m74.7726\u001b[0m  0.0018\n",
      "      2       74.7726  0.0015\n",
      "      3       74.7726  0.0015\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      4       74.7726  0.0015\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      5       74.7726  0.0014\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m88.2669\u001b[0m  0.0017\n",
      "      6       74.7726  0.0014\n",
      "      2       88.2669  0.0015\n",
      "      7       74.7726  0.0015\n",
      "      3       88.2669  0.0015\n",
      "      8       74.7726  0.0019\n",
      "      4       88.2669  0.0015\n",
      "      9       74.7726  0.0016\n",
      "      5       88.2669  0.0014\n",
      "     10       74.7726  0.0015\n",
      "Restoring best model from epoch 1.\n",
      "      6       88.2669  0.0014\n",
      "      7       88.2669  0.0014\n",
      "      8       88.2669  0.0014\n",
      "      9       88.2669  0.0014\n",
      "     10       88.2669  0.0014\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m130.0971\u001b[0m  0.0022\n",
      "      2      130.0971  0.0028\n",
      "      3      130.0971  0.0025\n",
      "      4      130.0971  0.0024\n",
      "      5      130.0971  0.0026\n",
      "      6      130.0971  0.0025\n",
      "      7      130.0971  0.0025\n",
      "      8      130.0971  0.0026\n",
      "      9      130.0971  0.0026\n",
      "     10      130.0971  0.0039\n",
      "Restoring best model from epoch 1.\n",
      "y_train breslow final [ 3.000e+00 -7.000e+00  8.000e+00 -1.400e+01 -1.600e+01 -1.700e+01\n",
      " -2.000e+01 -2.100e+01 -2.100e+01  2.200e+01  2.400e+01 -2.900e+01\n",
      "  3.000e+01 -3.000e+01  3.100e+01 -3.400e+01 -3.500e+01  4.500e+01\n",
      " -4.600e+01  5.700e+01  6.700e+01  7.600e+01 -7.700e+01  8.100e+01\n",
      "  8.200e+01 -9.200e+01 -9.900e+01  1.030e+02  1.050e+02  1.050e+02\n",
      "  1.130e+02  1.220e+02  1.240e+02  1.240e+02 -1.310e+02  1.320e+02\n",
      "  1.380e+02  1.410e+02  1.530e+02  1.560e+02 -1.560e+02 -1.640e+02\n",
      "  1.660e+02  1.680e+02  1.740e+02 -1.760e+02  1.800e+02 -1.830e+02\n",
      "  1.850e+02 -1.890e+02  1.920e+02 -1.980e+02 -1.980e+02 -2.000e+02\n",
      " -2.000e+02  2.000e+02  2.000e+02  2.050e+02  2.150e+02  2.180e+02\n",
      " -2.240e+02 -2.250e+02 -2.290e+02  2.290e+02 -2.430e+02  2.430e+02\n",
      "  2.430e+02 -2.450e+02  2.450e+02  2.500e+02  2.550e+02  2.590e+02\n",
      "  2.620e+02  2.720e+02  2.730e+02 -2.730e+02  2.740e+02  2.750e+02\n",
      "  2.760e+02  2.790e+02 -2.800e+02 -2.800e+02  2.810e+02  2.820e+02\n",
      "  2.840e+02 -2.870e+02 -2.880e+02  2.890e+02  2.920e+02  2.940e+02\n",
      "  2.960e+02  3.000e+02  3.000e+02  3.120e+02 -3.230e+02 -3.250e+02\n",
      " -3.350e+02  3.360e+02 -3.370e+02  3.410e+02  3.420e+02 -3.420e+02\n",
      " -3.440e+02 -3.440e+02  3.440e+02 -3.460e+02  3.480e+02  3.480e+02\n",
      "  3.530e+02 -3.560e+02 -3.560e+02  3.590e+02 -3.650e+02  3.660e+02\n",
      " -3.670e+02 -3.680e+02  3.700e+02 -3.740e+02 -3.740e+02 -3.750e+02\n",
      " -3.750e+02  3.760e+02  3.770e+02 -3.770e+02 -3.780e+02 -3.780e+02\n",
      " -3.810e+02 -3.830e+02 -3.830e+02 -3.830e+02 -3.840e+02 -3.850e+02\n",
      " -3.880e+02 -3.890e+02 -3.890e+02 -3.900e+02 -3.960e+02  3.960e+02\n",
      " -4.000e+02  4.010e+02  4.030e+02  4.060e+02  4.060e+02 -4.110e+02\n",
      " -4.110e+02 -4.160e+02  4.220e+02  4.260e+02 -4.270e+02 -4.270e+02\n",
      "  4.280e+02 -4.310e+02 -4.340e+02  4.390e+02 -4.400e+02  4.460e+02\n",
      " -4.490e+02 -4.500e+02  4.570e+02 -4.630e+02  4.660e+02 -4.680e+02\n",
      "  4.740e+02  4.760e+02 -4.760e+02  4.770e+02 -4.790e+02 -4.850e+02\n",
      " -4.850e+02  4.910e+02  4.910e+02 -4.910e+02 -4.940e+02 -5.000e+02\n",
      " -5.110e+02  5.130e+02 -5.190e+02 -5.210e+02 -5.230e+02 -5.230e+02\n",
      " -5.250e+02  5.260e+02  5.330e+02  5.430e+02 -5.430e+02  5.520e+02\n",
      "  5.540e+02  5.540e+02  5.580e+02 -5.590e+02  5.620e+02 -5.640e+02\n",
      " -5.660e+02 -5.720e+02 -5.730e+02 -5.770e+02 -5.790e+02  5.880e+02\n",
      " -5.930e+02 -5.940e+02 -5.940e+02 -5.950e+02  6.070e+02 -6.150e+02\n",
      " -6.160e+02  6.180e+02 -6.210e+02 -6.250e+02  6.330e+02  6.350e+02\n",
      " -6.360e+02  6.400e+02 -6.410e+02 -6.430e+02 -6.440e+02 -6.470e+02\n",
      " -6.500e+02  6.520e+02  6.610e+02 -6.660e+02 -6.750e+02  6.750e+02\n",
      " -6.780e+02 -6.780e+02 -6.790e+02 -6.900e+02 -6.920e+02 -6.940e+02\n",
      " -6.990e+02  7.120e+02 -7.240e+02 -7.420e+02 -7.540e+02  7.620e+02\n",
      "  7.660e+02  7.790e+02 -7.850e+02  7.940e+02  8.050e+02 -8.120e+02\n",
      " -8.190e+02 -8.250e+02  8.320e+02 -8.380e+02 -8.380e+02 -8.560e+02\n",
      " -8.620e+02 -8.640e+02  8.740e+02  8.810e+02 -8.820e+02 -8.950e+02\n",
      " -8.990e+02 -9.000e+02 -9.120e+02 -9.120e+02 -9.280e+02  9.400e+02\n",
      " -9.420e+02 -9.460e+02 -9.490e+02 -9.510e+02 -9.810e+02 -9.890e+02\n",
      " -9.970e+02 -1.000e+03 -1.002e+03 -1.010e+03 -1.023e+03  1.043e+03\n",
      " -1.055e+03 -1.072e+03 -1.083e+03 -1.090e+03  1.095e+03 -1.100e+03\n",
      " -1.106e+03 -1.124e+03 -1.132e+03 -1.133e+03 -1.138e+03 -1.145e+03\n",
      "  1.153e+03 -1.160e+03 -1.184e+03 -1.190e+03 -1.200e+03 -1.210e+03\n",
      " -1.223e+03 -1.236e+03 -1.297e+03 -1.367e+03 -1.389e+03  1.407e+03\n",
      " -1.484e+03 -1.646e+03  1.686e+03 -1.690e+03  1.747e+03 -1.765e+03\n",
      "  1.811e+03 -1.918e+03 -1.935e+03 -1.964e+03  2.100e+03 -2.171e+03\n",
      "  2.197e+03 -2.267e+03 -2.351e+03 -3.196e+03 -3.540e+03 -3.720e+03]\n",
      "Concordance Index 0.49982908904460777\n",
      "Integrated Brier Score: 0.3537206480491295\n",
      "y_train breslow final [ 3.000e+00 -7.000e+00  8.000e+00 -1.400e+01 -1.600e+01 -1.700e+01\n",
      " -2.000e+01 -2.100e+01 -2.100e+01  2.200e+01  2.400e+01 -2.900e+01\n",
      "  3.000e+01 -3.000e+01  3.100e+01 -3.400e+01 -3.500e+01  4.500e+01\n",
      " -4.600e+01  5.700e+01  6.700e+01  7.600e+01 -7.700e+01  8.100e+01\n",
      "  8.200e+01 -9.200e+01 -9.900e+01  1.030e+02  1.050e+02  1.050e+02\n",
      "  1.130e+02  1.220e+02  1.240e+02  1.240e+02 -1.310e+02  1.320e+02\n",
      "  1.380e+02  1.410e+02  1.530e+02  1.560e+02 -1.560e+02 -1.640e+02\n",
      "  1.660e+02  1.680e+02  1.740e+02 -1.760e+02  1.800e+02 -1.830e+02\n",
      "  1.850e+02 -1.890e+02  1.920e+02 -1.980e+02 -1.980e+02 -2.000e+02\n",
      " -2.000e+02  2.000e+02  2.000e+02  2.050e+02  2.150e+02  2.180e+02\n",
      " -2.240e+02 -2.250e+02 -2.290e+02  2.290e+02 -2.430e+02  2.430e+02\n",
      "  2.430e+02 -2.450e+02  2.450e+02  2.500e+02  2.550e+02  2.590e+02\n",
      "  2.620e+02  2.720e+02  2.730e+02 -2.730e+02  2.740e+02  2.750e+02\n",
      "  2.760e+02  2.790e+02 -2.800e+02 -2.800e+02  2.810e+02  2.820e+02\n",
      "  2.840e+02 -2.870e+02 -2.880e+02  2.890e+02  2.920e+02  2.940e+02\n",
      "  2.960e+02  3.000e+02  3.000e+02  3.120e+02 -3.230e+02 -3.250e+02\n",
      " -3.350e+02  3.360e+02 -3.370e+02  3.410e+02  3.420e+02 -3.420e+02\n",
      " -3.440e+02 -3.440e+02  3.440e+02 -3.460e+02  3.480e+02  3.480e+02\n",
      "  3.530e+02 -3.560e+02 -3.560e+02  3.590e+02 -3.650e+02  3.660e+02\n",
      " -3.670e+02 -3.680e+02  3.700e+02 -3.740e+02 -3.740e+02 -3.750e+02\n",
      " -3.750e+02  3.760e+02  3.770e+02 -3.770e+02 -3.780e+02 -3.780e+02\n",
      " -3.810e+02 -3.830e+02 -3.830e+02 -3.830e+02 -3.840e+02 -3.850e+02\n",
      " -3.880e+02 -3.890e+02 -3.890e+02 -3.900e+02 -3.960e+02  3.960e+02\n",
      " -4.000e+02  4.010e+02  4.030e+02  4.060e+02  4.060e+02 -4.110e+02\n",
      " -4.110e+02 -4.160e+02  4.220e+02  4.260e+02 -4.270e+02 -4.270e+02\n",
      "  4.280e+02 -4.310e+02 -4.340e+02  4.390e+02 -4.400e+02  4.460e+02\n",
      " -4.490e+02 -4.500e+02  4.570e+02 -4.630e+02  4.660e+02 -4.680e+02\n",
      "  4.740e+02  4.760e+02 -4.760e+02  4.770e+02 -4.790e+02 -4.850e+02\n",
      " -4.850e+02  4.910e+02  4.910e+02 -4.910e+02 -4.940e+02 -5.000e+02\n",
      " -5.110e+02  5.130e+02 -5.190e+02 -5.210e+02 -5.230e+02 -5.230e+02\n",
      " -5.250e+02  5.260e+02  5.330e+02  5.430e+02 -5.430e+02  5.520e+02\n",
      "  5.540e+02  5.540e+02  5.580e+02 -5.590e+02  5.620e+02 -5.640e+02\n",
      " -5.660e+02 -5.720e+02 -5.730e+02 -5.770e+02 -5.790e+02  5.880e+02\n",
      " -5.930e+02 -5.940e+02 -5.940e+02 -5.950e+02  6.070e+02 -6.150e+02\n",
      " -6.160e+02  6.180e+02 -6.210e+02 -6.250e+02  6.330e+02  6.350e+02\n",
      " -6.360e+02  6.400e+02 -6.410e+02 -6.430e+02 -6.440e+02 -6.470e+02\n",
      " -6.500e+02  6.520e+02  6.610e+02 -6.660e+02 -6.750e+02  6.750e+02\n",
      " -6.780e+02 -6.780e+02 -6.790e+02 -6.900e+02 -6.920e+02 -6.940e+02\n",
      " -6.990e+02  7.120e+02 -7.240e+02 -7.420e+02 -7.540e+02  7.620e+02\n",
      "  7.660e+02  7.790e+02 -7.850e+02  7.940e+02  8.050e+02 -8.120e+02\n",
      " -8.190e+02 -8.250e+02  8.320e+02 -8.380e+02 -8.380e+02 -8.560e+02\n",
      " -8.620e+02 -8.640e+02  8.740e+02  8.810e+02 -8.820e+02 -8.950e+02\n",
      " -8.990e+02 -9.000e+02 -9.120e+02 -9.120e+02 -9.280e+02  9.400e+02\n",
      " -9.420e+02 -9.460e+02 -9.490e+02 -9.510e+02 -9.810e+02 -9.890e+02\n",
      " -9.970e+02 -1.000e+03 -1.002e+03 -1.010e+03 -1.023e+03  1.043e+03\n",
      " -1.055e+03 -1.072e+03 -1.083e+03 -1.090e+03  1.095e+03 -1.100e+03\n",
      " -1.106e+03 -1.124e+03 -1.132e+03 -1.133e+03 -1.138e+03 -1.145e+03\n",
      "  1.153e+03 -1.160e+03 -1.184e+03 -1.190e+03 -1.200e+03 -1.210e+03\n",
      " -1.223e+03 -1.236e+03 -1.297e+03 -1.367e+03 -1.389e+03  1.407e+03\n",
      " -1.484e+03 -1.646e+03  1.686e+03 -1.690e+03  1.747e+03 -1.765e+03\n",
      "  1.811e+03 -1.918e+03 -1.935e+03 -1.964e+03  2.100e+03 -2.171e+03\n",
      "  2.197e+03 -2.267e+03 -2.351e+03 -3.196e+03 -3.540e+03 -3.720e+03]\n",
      "durations 8.0 3519.0\n",
      "Concordance Index 0.46622516556291393\n",
      "Integrated Brier Score: 0.4984426379373338\n"
     ]
    }
   ],
   "source": [
    "cancer_types = [\n",
    "    'BLCA',\n",
    "    'BRCA',\n",
    "    'HNSC',\n",
    "    'KIRC',\n",
    "    'LGG',\n",
    "    'LIHC',\n",
    "    'LUAD',\n",
    "    'LUSC',\n",
    "    'OV',\n",
    "    'STAD']\n",
    "import skorch.callbacks\n",
    "\n",
    "class InputShapeSetter(skorch.callbacks.Callback):\n",
    "    def on_train_begin(self, net, X, y):\n",
    "        net.set_params(module__input_units=X.shape[-1])\n",
    "\n",
    "for idx, cancer_type in enumerate(cancer_types):\n",
    "    # get name of current dataset\n",
    "    data = load_tcga(path=\"/Users/JUSC/Documents/xgbsurv/xgbsurv/datasets/data/\", cancer_type=cancer_type, as_frame=True)\n",
    "    X  = data.data #.astype(np.float32)\n",
    "    y = data.target #.values #.to_numpy()\n",
    "\n",
    "    X, y = sort_X_y_pandas(X, y)\n",
    "\n",
    "    net = NeuralNet(\n",
    "        SurvivalModel, \n",
    "        module__n_layers = 1,\n",
    "        module__input_units = X.shape[1],\n",
    "        #module__num_nodes = 32,\n",
    "        #module__dropout = 0.1, # these could also be removed\n",
    "        module__out_features = 1,\n",
    "        # for split sizes when result size = 1\n",
    "        iterator_train__drop_last=True,\n",
    "        #iterator_valid__drop_last=True,\n",
    "        criterion=EfronLoss,\n",
    "        optimizer=torch.optim.AdamW,\n",
    "        optimizer__weight_decay = 0.4,\n",
    "        batch_size=32, # separate train and valid->iterator_train__batch_size=128 and iterator_valid__batch_size=128 ?\n",
    "        callbacks=[\n",
    "            (\n",
    "                \"sched\",\n",
    "                LRScheduler(\n",
    "                    torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
    "                    monitor=\"valid_loss\",\n",
    "                    patience=5,\n",
    "                ),\n",
    "            ),\n",
    "            (\n",
    "                \"es\",\n",
    "                EarlyStopping(\n",
    "                    monitor=\"valid_loss\",\n",
    "                    patience=10,\n",
    "                    load_best=True,\n",
    "                ),\n",
    "            ),\n",
    "            (\"seed\", FixSeed(seed=42)),\n",
    "            (\"Input Shape Setter\",InputShapeSetter())\n",
    "        ],#[EarlyStopping(patience=10)],#,InputShapeSetter()],\n",
    "        #TODO: enable stratification, verify\n",
    "        train_split=ValidSplit(0.2), # might cause lower performance in metrics, explain in thesis\n",
    "        #lr=0.001,\n",
    "        #max_epochs=1, #0,#100\n",
    "        #train_split=None,\n",
    "        verbose=1\n",
    "    )\n",
    "    best_model,params, outer_scores, best_preds_train, best_preds_test, X_train, X_test, y_train, y_test = train_eval(X, y, net, n_iter, data.filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xgbsurv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
