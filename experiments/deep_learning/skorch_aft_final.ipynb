{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import savetxt\n",
    "from xgbsurv.datasets import (load_metabric, load_flchain, load_rgbsg, load_support, load_tcga)\n",
    "from xgbsurv.models.utils import sort_X_y_pandas, transform_back, transform\n",
    "from xgbsurv.models.eh_aft_final import get_cumulative_hazard_function_aft\n",
    "import torch\n",
    "from torch import nn\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold, train_test_split\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, LabelBinarizer, OneHotEncoder\n",
    "from sklearn.compose import make_column_transformer, make_column_selector\n",
    "from sklearn.decomposition import PCA\n",
    "from loss_functions_pytorch import AFTLoss, aft_likelihood_torch\n",
    "from skorch import NeuralNet\n",
    "from skorch.callbacks import EarlyStopping, Callback, LRScheduler\n",
    "import skorch.callbacks\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import check_cv\n",
    "from numbers import Number\n",
    "import torch.utils.data\n",
    "from skorch.utils import flatten\n",
    "from skorch.utils import is_pandas_ndframe\n",
    "from skorch.utils import check_indexing\n",
    "from skorch.utils import multi_indexing\n",
    "from skorch.utils import to_numpy\n",
    "from skorch.dataset import get_len\n",
    "from skorch.dataset import ValidSplit\n",
    "from pycox.evaluation import EvalSurv\n",
    "from scipy.stats import uniform as scuniform\n",
    "from scipy.stats import randint as scrandint\n",
    "from scipy.stats import loguniform as scloguniform\n",
    "import random\n",
    "import os\n",
    "#torch.set_default_dtype(torch.float64)\n",
    "#torch.set_default_tensor_type(torch.DoubleTensor)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters, put into function\n",
    "n_outer_splits = 5\n",
    "n_inner_splits = 5\n",
    "rand_state = 42\n",
    "n_iter = 1 #50\n",
    "early_stopping_rounds=10\n",
    "base_score = 0.0\n",
    "\n",
    "# set seed for scipy/numpy\n",
    "np.random.seed(rand_state)\n",
    "\n",
    "param_grid_aft = {\n",
    "    'estimator__module__n_layers': [1, 2, 4],\n",
    "    'estimator__module__num_nodes': [64, 128, 256, 512],\n",
    "    'estimator__module__dropout': scuniform(0.0,0.7),\n",
    "    'estimator__optimizer__weight_decay': [0.4, 0.2, 0.1, 0.05, 0.02, 0.01, 0],\n",
    "    'estimator__batch_size': [64, 128, 256, 512, 1024],\n",
    "    #lr not in paper because of learning rate finder\n",
    "    # note: setting learning rate higher would make exp(partial_hazard) explode\n",
    "    #'estimator__lr': scloguniform(0.001,0.01), # scheduler unten einbauen\n",
    "    # use callback instead\n",
    "    'estimator__lr':[0.01]\n",
    "    #'max_epochs':  scrandint(10,20), # corresponds to num_rounds\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_torch(seed=42):\n",
    "    \"\"\"Sets all seeds within torch and adjacent libraries.\n",
    "\n",
    "    Args:\n",
    "        seed: Random seed to be used by the seeding functions.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    #os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    #torch.backends.cudnn.benchmark = False\n",
    "    #torch.backends.cudnn.deterministic = True\n",
    "    return None\n",
    "\n",
    "\n",
    "class FixSeed(Callback):\n",
    "    def __init__(self, seed):\n",
    "        self.seed = seed\n",
    "\n",
    "    def initialize(self):\n",
    "        seed_torch(self.seed)\n",
    "        return super().initialize()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Scorer\n",
    "def custom_scoring_function(y_true, y_pred):\n",
    "\n",
    "        #y_true = torch.from_numpy(y_true)\n",
    "        if isinstance(y_pred, np.ndarray):\n",
    "            y_pred = torch.from_numpy(y_pred)\n",
    "        if isinstance(y_true, np.ndarray):\n",
    "            y_true = torch.from_numpy(y_true)\n",
    "        if isinstance(y_pred, pd.Series):\n",
    "            y_pred = torch.tensor(y_pred.values)\n",
    "        if isinstance(y_true, pd.Series):\n",
    "            y_true = torch.tensor(y_true.values)\n",
    "        #print('loss function y_pred', y_pred)\n",
    "        #print('loss function y_true', y_true)\n",
    "        score = aft_likelihood_torch(y_pred, y_true).to(torch.float32)\n",
    "        return score.numpy()\n",
    "\n",
    "scoring_function = make_scorer(custom_scoring_function, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set up Custom Splitter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Torch Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SurvivalModel(nn.Module):\n",
    "    def __init__(self, n_layers, input_units, num_nodes, dropout, out_features):\n",
    "        super(SurvivalModel, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.in_features = input_units\n",
    "        self.num_nodes = num_nodes\n",
    "        self.dropout = dropout\n",
    "        self.out_features = out_features\n",
    "        model = []\n",
    "        # first layer\n",
    "        model.append(torch.nn.Linear(input_units, num_nodes))\n",
    "        model.append(torch.nn.ReLU())\n",
    "        model.append(torch.nn.Dropout(dropout))\n",
    "        model.append(torch.nn.BatchNorm1d(num_nodes))\n",
    "\n",
    "        for i in range(n_layers-1):\n",
    "            model.append(torch.nn.Linear(num_nodes, num_nodes))\n",
    "            #init.kaiming_normal_(model[-1].weight, nonlinearity='relu')\n",
    "            model.append(torch.nn.ReLU())\n",
    "            model.append(torch.nn.Dropout(dropout))\n",
    "            model.append(torch.nn.BatchNorm1d(num_nodes))\n",
    "\n",
    "        # output layer\n",
    "        model.append(torch.nn.Linear(num_nodes, out_features))\n",
    "    \n",
    "        self.layers = nn.Sequential(*model)\n",
    "\n",
    "        # for layer in self.layers:\n",
    "        #     if isinstance(layer, nn.Linear):\n",
    "        #         #nn.init.uniform_(layer.weight, a=-0.5, b=0.5)\n",
    "        #         nn.init.kaiming_normal_(layer.weight)\n",
    "\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = X.to(torch.float32)\n",
    "        res = self.layers(X)\n",
    "        #print(res)\n",
    "        return res\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomStandardScaler(StandardScaler):\n",
    "    \n",
    "    def __init__(self, copy=True, with_mean=True, with_std=True):\n",
    "        super().__init__(copy=copy, with_mean=with_mean, with_std=with_std)\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return super().fit(X, y)\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X_transformed = super().transform(X, y)\n",
    "        return X_transformed.astype(np.float32)\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        X_transformed = super().fit_transform(X, y)\n",
    "        return X_transformed.astype(np.float32)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define stratified inner k-fold cross-validation\n",
    "class CustomSplit(StratifiedKFold):\n",
    "    def __init__(self, n_splits=5, shuffle=True, random_state=None):\n",
    "        super().__init__(n_splits=n_splits, shuffle=shuffle, random_state=random_state)\n",
    "\n",
    "    def split(self, X, y, groups=None):\n",
    "        print('split', X.dtypes)\n",
    "        try:\n",
    "            if y.shape[1]>1:\n",
    "                y = y[:,0]\n",
    "        except:\n",
    "            pass\n",
    "        bins = np.sign(y)\n",
    "        return super().split(X, bins, groups=groups)\n",
    "\n",
    "    def get_n_splits(self, X=None, y=None, groups=None):\n",
    "        return self.n_splits\n",
    "\n",
    "outer_custom_cv = CustomSplit(n_splits=n_outer_splits, shuffle=True, random_state=rand_state)\n",
    "inner_custom_cv = CustomSplit(n_splits=n_outer_splits, shuffle=True, random_state=rand_state)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Valid Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class CustomStandardScaler(StandardScaler):\n",
    "    \n",
    "    def __init__(self, copy=True, with_mean=True, with_std=True):\n",
    "        super().__init__(copy=copy, with_mean=with_mean, with_std=with_std)\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return super().fit(X, y)\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X_transformed = super().transform(X, y)\n",
    "        return X_transformed.astype(np.float32)\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        X_transformed = super().fit_transform(X, y)\n",
    "        return X_transformed.astype(np.float32)\n",
    "    \n",
    "class CustomValidSplit():\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            cv=5,\n",
    "            stratified=False,\n",
    "            random_state=None,\n",
    "    ):\n",
    "        self.stratified = stratified\n",
    "        self.random_state = random_state\n",
    "\n",
    "        if isinstance(cv, Number) and (cv <= 0):\n",
    "            raise ValueError(\"Numbers less than 0 are not allowed for cv \"\n",
    "                             \"but ValidSplit got {}\".format(cv))\n",
    "\n",
    "        if not self._is_float(cv) and random_state is not None:\n",
    "            raise ValueError(\n",
    "                \"Setting a random_state has no effect since cv is not a float. \"\n",
    "                \"You should leave random_state to its default (None), or set cv \"\n",
    "                \"to a float value.\",\n",
    "            )\n",
    "\n",
    "        self.cv = cv\n",
    "\n",
    "    def _is_stratified(self, cv):\n",
    "        return isinstance(cv, (StratifiedKFold, StratifiedShuffleSplit))\n",
    "\n",
    "    def _is_float(self, x):\n",
    "        if not isinstance(x, Number):\n",
    "            return False\n",
    "        return not float(x).is_integer()\n",
    "\n",
    "    def _check_cv_float(self):\n",
    "        cv_cls = StratifiedShuffleSplit if self.stratified else ShuffleSplit\n",
    "        return cv_cls(test_size=self.cv, random_state=self.random_state)\n",
    "\n",
    "    def _check_cv_non_float(self, y):\n",
    "        return check_cv(\n",
    "            self.cv,\n",
    "            y=y,\n",
    "            classifier=self.stratified,\n",
    "        )\n",
    "\n",
    "    def check_cv(self, y):\n",
    "        \"\"\"Resolve which cross validation strategy is used.\"\"\"\n",
    "        y_arr = None\n",
    "        if self.stratified:\n",
    "            # Try to convert y to numpy for sklearn's check_cv; if conversion\n",
    "            # doesn't work, still try.\n",
    "            try:\n",
    "                y_arr = to_numpy(y)\n",
    "            except (AttributeError, TypeError):\n",
    "                y_arr = y\n",
    "\n",
    "        if self._is_float(self.cv):\n",
    "            return self._check_cv_float()\n",
    "        return self._check_cv_non_float(y_arr)\n",
    "\n",
    "    def _is_regular(self, x):\n",
    "        return (x is None) or isinstance(x, np.ndarray) or is_pandas_ndframe(x)\n",
    "\n",
    "    def __call__(self, dataset, y=None, groups=None):\n",
    "        # key change here\n",
    "        y = np.sign(y)\n",
    "        bad_y_error = ValueError(\n",
    "            \"Stratified CV requires explicitly passing a suitable y.\")\n",
    "        if (y is None) and self.stratified:\n",
    "            raise bad_y_error\n",
    "\n",
    "        cv = self.check_cv(y)\n",
    "        if self.stratified and not self._is_stratified(cv):\n",
    "            raise bad_y_error\n",
    "\n",
    "        # pylint: disable=invalid-name\n",
    "        len_dataset = get_len(dataset)\n",
    "        if y is not None:\n",
    "            len_y = get_len(y)\n",
    "            if len_dataset != len_y:\n",
    "                raise ValueError(\"Cannot perform a CV split if dataset and y \"\n",
    "                                 \"have different lengths.\")\n",
    "\n",
    "        args = (np.arange(len_dataset),)\n",
    "        if self._is_stratified(cv):\n",
    "            args = args + (to_numpy(y),)\n",
    "\n",
    "        idx_train, idx_valid = next(iter(cv.split(*args, groups=groups)))\n",
    "        dataset_train = torch.utils.data.Subset(dataset, idx_train)\n",
    "        dataset_valid = torch.utils.data.Subset(dataset, idx_valid)\n",
    "        return dataset_train, dataset_valid\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Training Procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_eval(X, y, net, n_iter, filename):\n",
    "        model = 'aft_'\n",
    "        dataset_name = filename.split('_')[0]\n",
    "        # add IBS later\n",
    "        outer_scores = {'cindex_train_'+dataset_name:[], 'cindex_test_'+dataset_name:[],\n",
    "                        'ibs_train_'+dataset_name:[], 'ibs_test_'+dataset_name:[]}\n",
    "        best_params = {'best_params_'+dataset_name:[]}\n",
    "        best_model = {'best_model_'+dataset_name:[]}\n",
    "        ct = make_column_transformer(\n",
    "                #(OneHotEncoder(sparse_output=False), make_column_selector(dtype_include=['category', 'object'])),\n",
    "                (StandardScaler(), make_column_selector(dtype_include=['float32'],dtype_exclude=['category', 'object']))\n",
    "                ,remainder='passthrough')\n",
    "\n",
    "        pipe = Pipeline([('scaler',ct),\n",
    "                        ('estimator', net)])\n",
    "        rs = RandomizedSearchCV(pipe, param_grid_aft, scoring = scoring_function, n_jobs=-1, \n",
    "                                    n_iter=n_iter, refit=True)\n",
    "        for i, (train_index, test_index) in enumerate(outer_custom_cv.split(X, y)):\n",
    "                # Split data into training and testing sets for outer fold\n",
    "                X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "                y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "                X_train, y_train = sort_X_y_pandas(X_train, y_train)\n",
    "                X_test, y_test = sort_X_y_pandas(X_test, y_test)\n",
    "\n",
    "                # print(X_train.shape, type(X_train))\n",
    "                # print(y_train.shape, type(y_train))\n",
    "                # print(X_test.shape, type(X_test))\n",
    "                # print(y_test.shape, type(y_test))\n",
    "                # save splits and data\n",
    "                savetxt('splits/'+model+'train_index_'+str(i)+'_'+filename, train_index, delimiter=',')\n",
    "                savetxt('splits/'+model+'test_index_'+str(i)+'_'+filename, test_index, delimiter=',')\n",
    "                \n",
    "                #savetxt('splits/'model+'X_train_'+str(i)+'_'+filename, X_train, delimiter=',')\n",
    "                #savetxt('splits/'model+'X_test_'+str(i)+'_'+filename, X_test, delimiter=',')\n",
    "\n",
    "                #savetxt('splits/'model+'y_train_'+str(i)+'_'+filename, y_train, delimiter=',')\n",
    "                #savetxt('splits/'model+'y_test_'+str(i)+'_'+filename, y_test, delimiter=',')\n",
    "\n",
    "\n",
    "                rs.fit(X_train, y_train)\n",
    "                best_preds_train = rs.best_estimator_.predict(X_train)\n",
    "                best_preds_test = rs.best_estimator_.predict(X_test)\n",
    "\n",
    "                savetxt('predictions/'+model+'best_preds_train_'+str(i)+'_'+filename, best_preds_train, delimiter=',')\n",
    "                savetxt('predictions/'+model+'best_preds_test_'+str(i)+'_'+filename, best_preds_test, delimiter=',')\n",
    "\n",
    "                # save hyperparameter settings\n",
    "                params = rs.best_estimator_.get_params\n",
    "                best_params['best_params_'+dataset_name] += [rs.best_params_]\n",
    "                best_model['best_model_'+dataset_name] += [params]\n",
    "                try:\n",
    "                    cum_hazard_train = get_cumulative_hazard_function_aft(\n",
    "                            X_train.values, X_train.values, y_train.values, y_train.values,\n",
    "                            best_preds_train.reshape(-1), best_preds_train.reshape(-1)\n",
    "                            )\n",
    "\n",
    "                    df_survival_train = np.exp(-cum_hazard_train)\n",
    "                    durations_train, events_train = transform_back(y_train.values)\n",
    "                    time_grid_train = np.linspace(durations_train.min(), durations_train.max(), 100)\n",
    "                    ev = EvalSurv(df_survival_train, durations_train, events_train, censor_surv='km')\n",
    "                    print('Concordance Index',ev.concordance_td('antolini'))\n",
    "                    print('Integrated Brier Score:',ev.integrated_brier_score(time_grid_train))\n",
    "                    cindex_score_train = ev.concordance_td('antolini')\n",
    "                    ibs_score_train = ev.integrated_brier_score(time_grid_train)\n",
    "\n",
    "                    outer_scores['cindex_train_'+dataset_name] += [cindex_score_train]\n",
    "                    outer_scores['ibs_train_'+dataset_name] += [ibs_score_train]\n",
    "\n",
    "                except:\n",
    "                    outer_scores['cindex_train_'+dataset_name] += [np.nan]\n",
    "                    outer_scores['ibs_train_'+dataset_name] += [np.nan]\n",
    "                    \n",
    "                try:\n",
    "                    cum_hazard_test = get_cumulative_hazard_function_aft(\n",
    "                            X_train.values, X_test.values, y_train.values, y_test.values,\n",
    "                            best_preds_train.reshape(-1), best_preds_test.reshape(-1)\n",
    "                            )\n",
    "                    df_survival_test = np.exp(-cum_hazard_test)\n",
    "                    durations_test, events_test = transform_back(y_test.values)\n",
    "                    time_grid_test = np.linspace(durations_test.min(), durations_test.max(), 100)\n",
    "                    ev = EvalSurv(df_survival_test, durations_test, events_test, censor_surv='km')\n",
    "                    print('Concordance Index',ev.concordance_td('antolini'))\n",
    "                    print('Integrated Brier Score:',ev.integrated_brier_score(time_grid_test))\n",
    "                    cindex_score_test = ev.concordance_td('antolini')\n",
    "                    ibs_score_test = ev.integrated_brier_score(time_grid_test)\n",
    "\n",
    "                    outer_scores['cindex_test_'+dataset_name] += [cindex_score_test]\n",
    "                    outer_scores['ibs_test_'+dataset_name] += [ibs_score_test]\n",
    "                except: \n",
    "                    outer_scores['cindex_test_'+dataset_name] += [np.nan]\n",
    "                    outer_scores['ibs_test_'+dataset_name] += [np.nan]\n",
    "        \n",
    "        df_best_params = pd.DataFrame(best_params)\n",
    "        df_best_model = pd.DataFrame(best_model)\n",
    "        df_outer_scores = pd.DataFrame(outer_scores)\n",
    "        df_metrics = pd.concat([df_best_params,df_best_model,df_outer_scores], axis=1)\n",
    "        df_metrics.to_csv('metrics/'+model+'metric_summary'+str(i)+'_'+filename, index=False)\n",
    "        # cindex\n",
    "        df_agg_metrics_cindex = pd.DataFrame({'dataset':[dataset_name],\n",
    "                                              'cindex_train_mean':df_outer_scores['cindex_train_'+dataset_name].mean(),\n",
    "                                              'cindex_train_std':df_outer_scores['cindex_train_'+dataset_name].std(),\n",
    "                                              'cindex_test_mean':df_outer_scores['cindex_test_'+dataset_name].mean(),\n",
    "                                              'cindex_test_std':df_outer_scores['cindex_test_'+dataset_name].std() })\n",
    "        # IBS\n",
    "        df_agg_metrics_ibs = pd.DataFrame({'dataset':[dataset_name],\n",
    "                                              'ibs_train_mean':df_outer_scores['ibs_train_'+dataset_name].mean(),\n",
    "                                              'ibs_train_std':df_outer_scores['ibs_train_'+dataset_name].std(),\n",
    "                                              'ibs_test_mean':df_outer_scores['ibs_test_'+dataset_name].mean(),\n",
    "                                              'ibs_test_std':df_outer_scores['ibs_test_'+dataset_name].std() })\n",
    "        \n",
    "        return df_agg_metrics_cindex, df_agg_metrics_ibs, best_model, best_params, outer_scores, best_preds_train, best_preds_test \n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class InputShapeSetter(skorch.callbacks.Callback):\n",
    "    def on_train_begin(self, net, X, y):\n",
    "        net.set_params(module__input_units=X.shape[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_metabric\n",
      "split MKI67                float32\n",
      "EGFR                 float32\n",
      "PGR                  float32\n",
      "ERBB2                float32\n",
      "hormone_treatment      uint8\n",
      "radiotherapy           uint8\n",
      "chemotherapy           uint8\n",
      "ER_positive            uint8\n",
      "age                  float32\n",
      "dtype: object\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.8810\u001b[0m        \u001b[32m2.7399\u001b[0m  0.0392\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.0540\u001b[0m        \u001b[32m3.0088\u001b[0m  0.0368\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.2435\u001b[0m        \u001b[32m3.1972\u001b[0m  0.0399\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.2810\u001b[0m        \u001b[32m3.1906\u001b[0m  0.0392\n",
      "      2        2.8905        2.7429  0.0239\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.2285\u001b[0m        \u001b[32m3.1797\u001b[0m  0.0396\n",
      "      2        \u001b[36m3.0516\u001b[0m        3.0148  0.0223\n",
      "      2        3.2556        3.2047  0.0253\n",
      "      2        3.3319        3.2175  0.0245\n",
      "      3        2.9224        2.7474  0.0231\n",
      "      2        3.2916        \u001b[32m3.1736\u001b[0m  0.0225\n",
      "      3        3.1110        3.0115  0.0235\n",
      "      3        \u001b[36m3.2311\u001b[0m        \u001b[32m3.1735\u001b[0m  0.0247\n",
      "      3        3.4131        \u001b[32m3.1898\u001b[0m  0.0218\n",
      "      4        \u001b[36m2.8529\u001b[0m        2.7463  0.0246\n",
      "      3        3.2683        \u001b[32m3.1404\u001b[0m  0.0219\n",
      "      4        3.0960        3.0228  0.0247\n",
      "      4        \u001b[36m3.2027\u001b[0m        \u001b[32m3.1701\u001b[0m  0.0268\n",
      "      4        \u001b[36m3.2794\u001b[0m        3.2542  0.0235\n",
      "      5        \u001b[36m2.8092\u001b[0m        \u001b[32m2.7351\u001b[0m  0.0228\n",
      "      4        3.2515        3.1691  0.0226\n",
      "      5        \u001b[36m3.0075\u001b[0m        3.0553  0.0239\n",
      "      5        \u001b[36m3.1989\u001b[0m        \u001b[32m3.1659\u001b[0m  0.0252\n",
      "      5        \u001b[36m3.2377\u001b[0m        3.2514  0.0225\n",
      "      6        2.8247        2.7505  0.0240\n",
      "      5        \u001b[36m3.1735\u001b[0m        3.1788  0.0230\n",
      "      6        3.0375        3.0761  0.0224\n",
      "      6        3.2102        \u001b[32m3.1642\u001b[0m  0.0231\n",
      "      6        \u001b[36m3.2210\u001b[0m        3.2203  0.0240\n",
      "      7        \u001b[36m2.7815\u001b[0m        2.7915  0.0241\n",
      "      6        3.2064        3.1653  0.0238\n",
      "      7        3.0603        3.0477  0.0223\n",
      "      7        3.2098        3.1955  0.0234\n",
      "      7        3.2657        3.2318  0.0236\n",
      "      8        \u001b[36m2.7600\u001b[0m        2.8420  0.0221\n",
      "      7        3.1840        \u001b[32m3.1248\u001b[0m  0.0248\n",
      "      8        3.0362        3.0343  0.0238\n",
      "      8        \u001b[36m3.1534\u001b[0m        3.2723  0.0228\n",
      "      8        3.2327        3.3104  0.0237\n",
      "      9        2.7687        2.8390  0.0217\n",
      "      9        3.0086        3.0267  0.0233\n",
      "      8        3.1887        3.2092  0.0249\n",
      "      9        \u001b[36m3.1346\u001b[0m        3.3194  0.0234\n",
      "      9        \u001b[36m3.1796\u001b[0m        3.3676  0.0242\n",
      "     10        \u001b[36m2.7498\u001b[0m        2.8014  0.0253\n",
      "Restoring best model from epoch 5.\n",
      "     10        \u001b[36m3.0073\u001b[0m        3.0207  0.0246\n",
      "Restoring best model from epoch 1.\n",
      "      9        \u001b[36m3.1620\u001b[0m        3.2068  0.0242\n",
      "     10        3.1481        3.2914  0.0276\n",
      "Restoring best model from epoch 6.\n",
      "     10        3.1886        3.3114  0.0270\n",
      "Restoring best model from epoch 3.\n",
      "     10        \u001b[36m3.1338\u001b[0m        3.1737  0.0260\n",
      "Restoring best model from epoch 7.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.2137\u001b[0m        \u001b[32m3.0990\u001b[0m  0.0673\n",
      "      2        \u001b[36m3.1946\u001b[0m        3.1005  0.0420\n",
      "      3        \u001b[36m3.1432\u001b[0m        \u001b[32m3.0940\u001b[0m  0.0423\n",
      "      4        \u001b[36m3.1103\u001b[0m        3.1672  0.0416\n",
      "      5        \u001b[36m3.1057\u001b[0m        3.1568  0.0429\n",
      "      6        \u001b[36m3.0929\u001b[0m        3.1487  0.0437\n",
      "      7        3.1012        3.1678  0.0463\n",
      "      8        \u001b[36m3.0722\u001b[0m        3.1824  0.0461\n",
      "      9        \u001b[36m3.0691\u001b[0m        3.1502  0.0481\n",
      "     10        3.0876        3.1250  0.0439\n",
      "Restoring best model from epoch 3.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concordance Index 0.6423893842898083\n",
      "Integrated Brier Score: 0.17328718476262622\n",
      "Concordance Index 0.6387444942661006\n",
      "Integrated Brier Score: 0.17664044795686032\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.2479\u001b[0m        \u001b[32m3.1816\u001b[0m  0.0225\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.3017\u001b[0m        \u001b[32m3.1855\u001b[0m  0.0247\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.3175\u001b[0m        \u001b[32m3.1927\u001b[0m  0.0256\n",
      "      2        3.3072        \u001b[32m3.1728\u001b[0m  0.0233\n",
      "      2        \u001b[36m3.2882\u001b[0m        \u001b[32m3.1592\u001b[0m  0.0244\n",
      "      2        3.3811        3.2390  0.0249\n",
      "      3        3.2807        \u001b[32m3.1568\u001b[0m  0.0228\n",
      "      3        \u001b[36m3.2868\u001b[0m        3.1880  0.0243\n",
      "      3        3.3817        \u001b[32m3.1728\u001b[0m  0.0223\n",
      "      4        3.2507        3.1708  0.0223\n",
      "      4        \u001b[36m3.2621\u001b[0m        \u001b[32m3.1540\u001b[0m  0.0252\n",
      "      4        \u001b[36m3.2741\u001b[0m        3.1964  0.0287\n",
      "      5        \u001b[36m3.2204\u001b[0m        3.1626  0.0229\n",
      "      5        3.2714        3.1633  0.0221\n",
      "      5        3.3141        3.1868  0.0220\n",
      "      6        3.2361        3.1596  0.0216\n",
      "      6        \u001b[36m3.2228\u001b[0m        3.1763  0.0234\n",
      "      6        \u001b[36m3.2526\u001b[0m        3.2057  0.0211\n",
      "      7        \u001b[36m3.2135\u001b[0m        3.1736  0.0235\n",
      "      7        \u001b[36m3.2025\u001b[0m        3.1761  0.0221\n",
      "      7        3.2879        3.2088  0.0201\n",
      "      8        3.2182        3.1943  0.0220\n",
      "      8        3.2824        3.2807  0.0230\n",
      "      8        \u001b[36m3.2002\u001b[0m        3.2427  0.0379\n",
      "      9        3.2363        3.1987  0.0219\n",
      "      9        \u001b[36m3.1867\u001b[0m        3.3166  0.0198\n",
      "      9        3.2597        3.2895  0.0219\n",
      "     10        \u001b[36m3.1574\u001b[0m        3.2801  0.0202\n",
      "Restoring best model from epoch 3.\n",
      "     10        \u001b[36m3.1983\u001b[0m        3.1843  0.0224\n",
      "Restoring best model from epoch 4.\n",
      "     10        \u001b[36m3.2461\u001b[0m        3.2524  0.0223\n",
      "Restoring best model from epoch 3.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.0890\u001b[0m        \u001b[32m2.9992\u001b[0m  0.0322\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.8415\u001b[0m        \u001b[32m2.7489\u001b[0m  0.0331\n",
      "      2        3.1517        \u001b[32m2.9828\u001b[0m  0.0187\n",
      "      2        2.9648        \u001b[32m2.7140\u001b[0m  0.0201\n",
      "      3        3.1577        \u001b[32m2.9800\u001b[0m  0.0181\n",
      "      3        2.9318        2.7279  0.0204\n",
      "      4        3.1058        2.9862  0.0184\n",
      "      5        \u001b[36m3.0761\u001b[0m        3.0057  0.0178\n",
      "      4        2.8694        2.7628  0.0197\n",
      "      6        3.0990        3.0076  0.0180\n",
      "      5        2.8932        2.7626  0.0188\n",
      "      7        3.0778        3.0374  0.0174\n",
      "      6        2.8434        2.7775  0.0186\n",
      "      8        3.0768        3.0378  0.0177\n",
      "      7        2.8546        2.7830  0.0187\n",
      "      9        \u001b[36m3.0538\u001b[0m        3.0321  0.0174\n",
      "      8        2.8538        2.7872  0.0186\n",
      "     10        3.0571        3.0228  0.0175\n",
      "Restoring best model from epoch 3.\n",
      "      9        \u001b[36m2.8175\u001b[0m        2.7717  0.0184\n",
      "     10        \u001b[36m2.8150\u001b[0m        2.7614  0.0192\n",
      "Restoring best model from epoch 2.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.2174\u001b[0m        \u001b[32m3.1039\u001b[0m  0.0467\n",
      "      2        \u001b[36m3.1893\u001b[0m        \u001b[32m3.0961\u001b[0m  0.0434\n",
      "      3        \u001b[36m3.1658\u001b[0m        3.1219  0.0441\n",
      "      4        \u001b[36m3.1511\u001b[0m        3.0997  0.0432\n",
      "      5        3.1522        3.0978  0.0452\n",
      "      6        \u001b[36m3.1336\u001b[0m        3.1199  0.0441\n",
      "      7        \u001b[36m3.1229\u001b[0m        3.1256  0.0427\n",
      "      8        3.1233        3.1034  0.0433\n",
      "      9        \u001b[36m3.0961\u001b[0m        \u001b[32m3.0927\u001b[0m  0.0434\n",
      "     10        3.1155        \u001b[32m3.0883\u001b[0m  0.0425\n",
      "Concordance Index 0.653141736412802\n",
      "Integrated Brier Score: 0.1674008143831993\n",
      "Concordance Index 0.6298648940982123\n",
      "Integrated Brier Score: 0.17569719449921584\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.8958\u001b[0m        \u001b[32m2.7541\u001b[0m  0.0221\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.2790\u001b[0m        \u001b[32m3.1567\u001b[0m  0.0229\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.2806\u001b[0m        \u001b[32m3.2061\u001b[0m  0.0236\n",
      "      2        2.9775        \u001b[32m2.7443\u001b[0m  0.0214\n",
      "      2        3.2878        3.1695  0.0224\n",
      "      2        3.2969        3.2075  0.0228\n",
      "      3        2.9976        2.7445  0.0194\n",
      "      3        3.3209        \u001b[32m3.1230\u001b[0m  0.0226\n",
      "      4        2.9014        2.7491  0.0193\n",
      "      3        3.3718        \u001b[32m3.1377\u001b[0m  0.0232\n",
      "      5        \u001b[36m2.8667\u001b[0m        2.7926  0.0189\n",
      "      4        \u001b[36m3.2437\u001b[0m        3.1290  0.0222\n",
      "      4        3.3017        3.1438  0.0222\n",
      "      6        2.8695        2.7831  0.0197\n",
      "      5        \u001b[36m3.2236\u001b[0m        3.1399  0.0230\n",
      "      5        \u001b[36m3.2526\u001b[0m        3.1728  0.0223\n",
      "      7        \u001b[36m2.8645\u001b[0m        2.7758  0.0192\n",
      "      6        3.2579        3.1462  0.0230\n",
      "      6        \u001b[36m3.2385\u001b[0m        3.1863  0.0215\n",
      "      8        \u001b[36m2.8307\u001b[0m        2.7815  0.0188\n",
      "      7        3.2396        3.1242  0.0235\n",
      "      7        3.2516        3.1777  0.0238\n",
      "      9        \u001b[36m2.8153\u001b[0m        2.7705  0.0266\n",
      "      8        \u001b[36m3.2019\u001b[0m        3.1789  0.0258\n",
      "      8        3.2770        3.1772  0.0215\n",
      "     10        2.8763        2.7633  0.0252\n",
      "Restoring best model from epoch 2.\n",
      "      9        3.2484        3.1943  0.0201\n",
      "      9        \u001b[36m3.1722\u001b[0m        3.2302  0.0363\n",
      "     10        \u001b[36m3.2257\u001b[0m        3.1774  0.0199\n",
      "Restoring best model from epoch 3.\n",
      "     10        3.1911        3.1963  0.0207\n",
      "Restoring best model from epoch 3.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.0909\u001b[0m        \u001b[32m3.0131\u001b[0m  0.0417\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.2021\u001b[0m        \u001b[32m3.1717\u001b[0m  0.0416\n",
      "      2        3.1077        \u001b[32m3.0128\u001b[0m  0.0289\n",
      "      2        3.2625        \u001b[32m3.1498\u001b[0m  0.0285\n",
      "      3        3.1118        3.0499  0.0227\n",
      "      3        3.2722        \u001b[32m3.1405\u001b[0m  0.0237\n",
      "      4        \u001b[36m3.0635\u001b[0m        3.0658  0.0229\n",
      "      4        3.2466        3.1799  0.0228\n",
      "      5        3.0801        3.0438  0.0217\n",
      "      5        3.2319        3.1612  0.0223\n",
      "      6        \u001b[36m3.0617\u001b[0m        3.0179  0.0192\n",
      "      6        3.2326        \u001b[32m3.1383\u001b[0m  0.0188\n",
      "      7        3.0779        \u001b[32m3.0118\u001b[0m  0.0183\n",
      "      7        3.2061        \u001b[32m3.1202\u001b[0m  0.0184\n",
      "      8        3.0865        3.0316  0.0213\n",
      "      8        3.2139        3.1310  0.0231\n",
      "      9        \u001b[36m3.0400\u001b[0m        3.1505  0.0196\n",
      "      9        3.2049        3.1535  0.0194\n",
      "     10        \u001b[36m3.0167\u001b[0m        3.2017  0.0179\n",
      "Restoring best model from epoch 7.\n",
      "     10        \u001b[36m3.1855\u001b[0m        3.1421  0.0194\n",
      "Restoring best model from epoch 7.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.1997\u001b[0m        \u001b[32m3.1028\u001b[0m  0.0507\n",
      "      2        3.2031        \u001b[32m3.0902\u001b[0m  0.0430\n",
      "      3        \u001b[36m3.1384\u001b[0m        3.1030  0.0424\n",
      "      4        \u001b[36m3.1270\u001b[0m        3.1475  0.0429\n",
      "      5        \u001b[36m3.1087\u001b[0m        3.0979  0.0425\n",
      "      6        3.1101        \u001b[32m3.0785\u001b[0m  0.0428\n",
      "      7        3.1130        3.0978  0.0437\n",
      "      8        \u001b[36m3.1057\u001b[0m        3.1300  0.0419\n",
      "      9        3.1196        3.0792  0.0421\n",
      "     10        3.1078        \u001b[32m3.0681\u001b[0m  0.0425\n",
      "Concordance Index 0.6514888703380638\n",
      "Integrated Brier Score: 0.16820918737921484\n",
      "Concordance Index 0.6473445274864699\n",
      "Integrated Brier Score: 0.1723132202054857\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.8651\u001b[0m        \u001b[32m2.7584\u001b[0m  0.0224\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.0853\u001b[0m        \u001b[32m3.0048\u001b[0m  0.0233\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.2655\u001b[0m        \u001b[32m3.1635\u001b[0m  0.0259\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.2939\u001b[0m        \u001b[32m3.2335\u001b[0m  0.0238\n",
      "      2        2.9682        \u001b[32m2.7237\u001b[0m  0.0234\n",
      "      2        \u001b[36m3.0648\u001b[0m        \u001b[32m2.9921\u001b[0m  0.0229\n",
      "      2        3.3730        \u001b[32m3.2228\u001b[0m  0.0223\n",
      "      2        3.3059        \u001b[32m3.1484\u001b[0m  0.0261\n",
      "      3        2.9092        2.7486  0.0237\n",
      "      3        3.1267        3.0173  0.0224\n",
      "      3        3.3645        3.2458  0.0215\n",
      "      4        \u001b[36m2.8622\u001b[0m        2.7365  0.0239\n",
      "      3        \u001b[36m3.2635\u001b[0m        \u001b[32m3.1347\u001b[0m  0.0262\n",
      "      4        3.0851        3.0343  0.0234\n",
      "      4        3.3203        3.2243  0.0219\n",
      "      5        \u001b[36m2.8234\u001b[0m        2.7464  0.0212\n",
      "      4        3.3063        3.1460  0.0249\n",
      "      5        3.0744        3.0145  0.0227\n",
      "      5        3.3134        \u001b[32m3.2165\u001b[0m  0.0229\n",
      "      6        \u001b[36m2.7889\u001b[0m        2.7461  0.0206\n",
      "      6        \u001b[36m3.0270\u001b[0m        3.0052  0.0231\n",
      "      5        \u001b[36m3.2482\u001b[0m        3.1382  0.0245\n",
      "      6        \u001b[36m3.2677\u001b[0m        3.2190  0.0217\n",
      "      7        2.8219        2.8086  0.0215\n",
      "      7        3.0405        3.0088  0.0232\n",
      "      6        \u001b[36m3.2020\u001b[0m        3.1558  0.0246\n",
      "      8        \u001b[36m2.7596\u001b[0m        2.8869  0.0209\n",
      "      7        3.2897        3.2355  0.0257\n",
      "      8        3.0325        3.0333  0.0235\n",
      "      7        \u001b[36m3.1998\u001b[0m        3.1493  0.0254\n",
      "      9        2.7611        2.8455  0.0240\n",
      "      8        \u001b[36m3.2227\u001b[0m        3.2344  0.0240\n",
      "      9        3.0615        3.0199  0.0235\n",
      "      8        3.2066        \u001b[32m3.1297\u001b[0m  0.0266\n",
      "     10        2.7759        2.8120  0.0211\n",
      "Restoring best model from epoch 2.\n",
      "      9        3.2370        3.2298  0.0223\n",
      "     10        \u001b[36m2.9987\u001b[0m        3.0100  0.0222\n",
      "Restoring best model from epoch 2.\n",
      "      9        \u001b[36m3.1895\u001b[0m        3.1638  0.0232\n",
      "     10        3.2431        3.2221  0.0200\n",
      "Restoring best model from epoch 5.\n",
      "     10        \u001b[36m3.1825\u001b[0m        3.1974  0.0213\n",
      "Restoring best model from epoch 8.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.2486\u001b[0m        \u001b[32m3.1602\u001b[0m  0.0372\n",
      "      2        3.2642        3.1671  0.0190\n",
      "      3        \u001b[36m3.2372\u001b[0m        \u001b[32m3.1571\u001b[0m  0.0200\n",
      "      4        \u001b[36m3.2185\u001b[0m        \u001b[32m3.1466\u001b[0m  0.0209\n",
      "      5        \u001b[36m3.1763\u001b[0m        \u001b[32m3.1135\u001b[0m  0.0183\n",
      "      6        3.2088        \u001b[32m3.1050\u001b[0m  0.0183\n",
      "      7        3.1816        3.1267  0.0182\n",
      "      8        \u001b[36m3.1693\u001b[0m        3.1936  0.0184\n",
      "      9        3.1859        3.2206  0.0180\n",
      "     10        3.1713        3.2150  0.0182\n",
      "Restoring best model from epoch 6.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.2061\u001b[0m        \u001b[32m3.0890\u001b[0m  0.0479\n",
      "      2        \u001b[36m3.2049\u001b[0m        \u001b[32m3.0822\u001b[0m  0.0450\n",
      "      3        \u001b[36m3.1639\u001b[0m        3.0917  0.0440\n",
      "      4        \u001b[36m3.1552\u001b[0m        3.0963  0.0430\n",
      "      5        \u001b[36m3.1356\u001b[0m        3.2131  0.0566\n",
      "      6        \u001b[36m3.1142\u001b[0m        3.1681  0.0441\n",
      "      7        \u001b[36m3.1138\u001b[0m        3.1207  0.0466\n",
      "      8        \u001b[36m3.1051\u001b[0m        3.1545  0.0492\n",
      "      9        3.1146        3.1185  0.0495\n",
      "     10        3.1187        3.1041  0.0461\n",
      "Restoring best model from epoch 2.\n",
      "Concordance Index 0.6254949416958683\n",
      "Integrated Brier Score: 0.17689273746244893\n",
      "Concordance Index 0.6213636269921449\n",
      "Integrated Brier Score: 0.17146404168505183\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.8514\u001b[0m        \u001b[32m2.7581\u001b[0m  0.0226\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.0693\u001b[0m        \u001b[32m2.9783\u001b[0m  0.0264\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.2894\u001b[0m        \u001b[32m3.1741\u001b[0m  0.0270\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.2789\u001b[0m        \u001b[32m3.2111\u001b[0m  0.0272\n",
      "      2        2.9319        \u001b[32m2.7299\u001b[0m  0.0253\n",
      "      2        \u001b[36m3.0503\u001b[0m        3.0091  0.0265\n",
      "      2        3.3120        3.2014  0.0242\n",
      "      2        3.3193        \u001b[32m3.1338\u001b[0m  0.0261\n",
      "      3        2.9364        2.7346  0.0266\n",
      "      3        3.0992        3.0277  0.0271\n",
      "      3        3.3126        3.1928  0.0233\n",
      "      3        3.3568        3.2299  0.0241\n",
      "      4        \u001b[36m2.8304\u001b[0m        2.7563  0.0247\n",
      "      4        3.0768        3.0028  0.0256\n",
      "      4        \u001b[36m3.2814\u001b[0m        \u001b[32m3.1342\u001b[0m  0.0243\n",
      "      4        3.3413        3.2333  0.0221\n",
      "      5        \u001b[36m2.7841\u001b[0m        2.7877  0.0250\n",
      "      5        \u001b[36m3.2758\u001b[0m        3.1566  0.0241\n",
      "      5        \u001b[36m3.0460\u001b[0m        \u001b[32m2.9769\u001b[0m  0.0309\n",
      "      5        \u001b[36m3.2413\u001b[0m        3.1641  0.0230\n",
      "      6        2.8415        2.7592  0.0231\n",
      "      6        3.2854        3.1357  0.0234\n",
      "      6        3.0694        2.9887  0.0214\n",
      "      6        \u001b[36m3.2241\u001b[0m        3.1419  0.0259\n",
      "      7        2.8216        2.8069  0.0221\n",
      "      7        \u001b[36m3.0295\u001b[0m        2.9989  0.0219\n",
      "      7        \u001b[36m3.2457\u001b[0m        \u001b[32m3.1221\u001b[0m  0.0237\n",
      "      8        2.8069        2.8634  0.0243\n",
      "      7        3.2618        3.1655  0.0258\n",
      "      8        3.0313        3.0358  0.0239\n",
      "      8        \u001b[36m3.2088\u001b[0m        3.2132  0.0276\n",
      "      9        \u001b[36m2.7704\u001b[0m        2.8399  0.0256\n",
      "      8        3.2544        3.2163  0.0260\n",
      "      9        \u001b[36m3.0243\u001b[0m        3.1095  0.0219\n",
      "      9        3.2154        3.2770  0.0239\n",
      "     10        2.7780        2.8220  0.0238\n",
      "Restoring best model from epoch 2.\n",
      "      9        3.2543        3.2022  0.0240\n",
      "     10        \u001b[36m3.0082\u001b[0m        3.1373  0.0238\n",
      "Restoring best model from epoch 5.\n",
      "     10        \u001b[36m3.1940\u001b[0m        3.2717  0.0245\n",
      "Restoring best model from epoch 7.\n",
      "     10        \u001b[36m3.2219\u001b[0m        3.1935  0.0230\n",
      "Restoring best model from epoch 2.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.2731\u001b[0m        \u001b[32m3.1764\u001b[0m  0.0346\n",
      "      2        \u001b[36m3.2612\u001b[0m        3.2050  0.0224\n",
      "      3        3.3260        \u001b[32m3.1582\u001b[0m  0.0208\n",
      "      4        \u001b[36m3.2240\u001b[0m        \u001b[32m3.1339\u001b[0m  0.0197\n",
      "      5        3.2411        3.1570  0.0186\n",
      "      6        3.2446        3.1542  0.0187\n",
      "      7        3.2361        3.1414  0.0189\n",
      "      8        3.2529        3.1546  0.0190\n",
      "      9        3.2428        3.1645  0.0247\n",
      "     10        \u001b[36m3.2014\u001b[0m        3.1533  0.0201\n",
      "Restoring best model from epoch 4.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.2373\u001b[0m        \u001b[32m3.0454\u001b[0m  0.0472\n",
      "      2        \u001b[36m3.2051\u001b[0m        \u001b[32m3.0414\u001b[0m  0.0433\n",
      "      3        \u001b[36m3.1798\u001b[0m        3.0494  0.0425\n",
      "      4        \u001b[36m3.1338\u001b[0m        3.0735  0.0424\n",
      "      5        3.1394        3.0925  0.0415\n",
      "      6        \u001b[36m3.1277\u001b[0m        \u001b[32m3.0369\u001b[0m  0.0420\n",
      "      7        \u001b[36m3.1264\u001b[0m        \u001b[32m3.0240\u001b[0m  0.0489\n",
      "      8        \u001b[36m3.1106\u001b[0m        \u001b[32m3.0207\u001b[0m  0.0466\n",
      "      9        3.1252        3.0259  0.0421\n",
      "     10        \u001b[36m3.0765\u001b[0m        \u001b[32m3.0131\u001b[0m  0.0424\n",
      "Concordance Index 0.6593849681020734\n",
      "Integrated Brier Score: 0.17153118320268315\n",
      "Concordance Index 0.651619745082738\n",
      "Integrated Brier Score: 0.185588568623872\n",
      "load_flchain\n",
      "split age           float32\n",
      "sex             uint8\n",
      "sample_yr     float32\n",
      "kappa         float32\n",
      "lambda        float32\n",
      "flc_grp       float32\n",
      "creatinine    float32\n",
      "mgus            uint8\n",
      "dtype: object\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m1.6644\u001b[0m        \u001b[32m1.5976\u001b[0m  0.1578\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.1648\u001b[0m        \u001b[32m2.1994\u001b[0m  0.1611\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.8726\u001b[0m        \u001b[32m2.7467\u001b[0m  0.1667\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.1034\u001b[0m        \u001b[32m2.9653\u001b[0m  0.1666\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.1426\u001b[0m        \u001b[32m3.1695\u001b[0m  0.1711\n",
      "      2        \u001b[36m1.6408\u001b[0m        1.6239  0.1428\n",
      "      2        \u001b[36m2.1219\u001b[0m        \u001b[32m2.0925\u001b[0m  0.1541\n",
      "      2        \u001b[36m3.0239\u001b[0m        3.0113  0.1608\n",
      "      2        \u001b[36m2.8185\u001b[0m        2.8021  0.1774\n",
      "      2        \u001b[36m3.0659\u001b[0m        \u001b[32m3.0852\u001b[0m  0.2087\n",
      "      3        \u001b[36m2.1035\u001b[0m        2.1334  0.1357\n",
      "      3        \u001b[36m1.6147\u001b[0m        \u001b[32m1.5585\u001b[0m  0.1840\n",
      "      3        \u001b[36m3.0186\u001b[0m        2.9780  0.1419\n",
      "      3        \u001b[36m2.8084\u001b[0m        2.7816  0.1471\n",
      "      3        \u001b[36m3.0349\u001b[0m        \u001b[32m3.0138\u001b[0m  0.1457\n",
      "      4        \u001b[36m1.5885\u001b[0m        \u001b[32m1.5470\u001b[0m  0.1401\n",
      "      4        \u001b[36m3.0103\u001b[0m        \u001b[32m2.9400\u001b[0m  0.1483\n",
      "      4        \u001b[36m2.7961\u001b[0m        \u001b[32m2.7094\u001b[0m  0.1412\n",
      "      4        3.0396        \u001b[32m2.9775\u001b[0m  0.1360\n",
      "      4        \u001b[36m2.0886\u001b[0m        \u001b[32m2.0830\u001b[0m  0.2365\n",
      "      5        \u001b[36m1.5812\u001b[0m        \u001b[32m1.5419\u001b[0m  0.1378\n",
      "      5        \u001b[36m2.9790\u001b[0m        2.9605  0.1497\n",
      "      5        \u001b[36m2.7834\u001b[0m        \u001b[32m2.6970\u001b[0m  0.1579\n",
      "      5        2.0952        \u001b[32m2.0753\u001b[0m  0.1358\n",
      "      6        \u001b[36m1.5785\u001b[0m        \u001b[32m1.5383\u001b[0m  0.1295\n",
      "      5        \u001b[36m3.0315\u001b[0m        \u001b[32m2.9659\u001b[0m  0.2291\n",
      "      6        2.9965        \u001b[32m2.9097\u001b[0m  0.1444\n",
      "      6        2.7849        2.6990  0.1482\n",
      "      6        \u001b[36m2.0811\u001b[0m        \u001b[32m2.0645\u001b[0m  0.1364\n",
      "      7        \u001b[36m1.5698\u001b[0m        1.5414  0.1280\n",
      "      7        2.9943        2.9219  0.1280\n",
      "      7        \u001b[36m2.7702\u001b[0m        2.7032  0.1290\n",
      "      7        \u001b[36m2.0732\u001b[0m        \u001b[32m2.0449\u001b[0m  0.1373\n",
      "      8        1.5832        \u001b[32m1.5348\u001b[0m  0.1374\n",
      "      8        2.9880        2.9183  0.1352\n",
      "      8        \u001b[36m2.7669\u001b[0m        \u001b[32m2.6813\u001b[0m  0.1366\n",
      "      6        \u001b[36m3.0096\u001b[0m        2.9711  0.3136\n",
      "      8        \u001b[36m2.0718\u001b[0m        2.0655  0.1680\n",
      "      9        \u001b[36m1.5636\u001b[0m        1.5406  0.1589\n",
      "      9        2.9855        2.9112  0.1504\n",
      "      9        \u001b[36m2.7661\u001b[0m        2.6962  0.1359\n",
      "      7        \u001b[36m3.0065\u001b[0m        \u001b[32m2.9587\u001b[0m  0.1325\n",
      "      9        2.0720        \u001b[32m2.0411\u001b[0m  0.1354\n",
      "     10        \u001b[36m2.7564\u001b[0m        2.6888  0.1277\n",
      "Restoring best model from epoch 8.\n",
      "     10        2.9824        2.9098  0.1405\n",
      "Restoring best model from epoch 6.\n",
      "      8        \u001b[36m3.0020\u001b[0m        \u001b[32m2.9515\u001b[0m  0.1247\n",
      "     10        \u001b[36m2.0625\u001b[0m        2.0514  0.1333\n",
      "Restoring best model from epoch 9.\n",
      "     10        1.5665        1.5388  0.2367\n",
      "Restoring best model from epoch 8.\n",
      "      9        3.0056        2.9529  0.1132\n",
      "     10        3.0078        2.9533  0.1290\n",
      "Restoring best model from epoch 8.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.5725\u001b[0m        \u001b[32m2.6360\u001b[0m  0.1866\n",
      "      2        \u001b[36m2.5279\u001b[0m        \u001b[32m2.6242\u001b[0m  0.1752\n",
      "      3        \u001b[36m2.5092\u001b[0m        \u001b[32m2.5212\u001b[0m  0.1735\n",
      "      4        \u001b[36m2.4981\u001b[0m        \u001b[32m2.5172\u001b[0m  0.1760\n",
      "      5        \u001b[36m2.4911\u001b[0m        \u001b[32m2.4985\u001b[0m  0.1703\n",
      "      6        \u001b[36m2.4840\u001b[0m        \u001b[32m2.4829\u001b[0m  0.1933\n",
      "      7        \u001b[36m2.4799\u001b[0m        2.4933  0.1739\n",
      "      8        2.4823        2.4889  0.1978\n",
      "      9        2.4845        2.4992  0.1911\n",
      "     10        \u001b[36m2.4784\u001b[0m        2.4866  0.1819\n",
      "Restoring best model from epoch 6.\n"
     ]
    }
   ],
   "source": [
    "data_set_fns = [load_metabric, load_flchain, load_rgbsg, load_support] #load_metabric, , load_flchain, load_rgbsg, load_support, load_tcga]\n",
    "data_set_fns_str = ['load_metabric', 'load_flchain', 'load_rgbsg', 'load_support'] \n",
    "one_hot_dict = {'load_flchain': ['mgus'], 'load_support':['cancer'], 'load_rgbsg':['grade']}\n",
    "agg_metrics_cindex = []\n",
    "agg_metrics_ibs = []\n",
    "\n",
    "for idx, dataset in enumerate(data_set_fns):\n",
    "    # get name of current dataset\n",
    "    data = dataset(path=\"/Users/JUSC/Documents/xgbsurv/xgbsurv/datasets/data/\", as_frame=True)\n",
    "    X  = data.data #.astype(np.float32)\n",
    "    y = data.target #.values #.to_numpy()\n",
    "\n",
    "    print(data_set_fns_str[idx])\n",
    "    # if data_set_fns_str[idx] in one_hot_dict.keys():\n",
    "    #     X = pd.get_dummies(X, columns=one_hot_dict[data_set_fns_str[idx]])\n",
    "    X, y = sort_X_y_pandas(X, y)\n",
    "    \n",
    "    net = NeuralNet(\n",
    "        SurvivalModel, \n",
    "        module__n_layers = 1,\n",
    "        module__input_units = X.shape[1],\n",
    "        #module__num_nodes = 32,\n",
    "        #module__dropout = 0.1, # these could also be removed\n",
    "        module__out_features = 1,\n",
    "        # for split sizes when result size = 1\n",
    "        iterator_train__drop_last=True,\n",
    "        #iterator_valid__drop_last=True,\n",
    "        criterion=AFTLoss,\n",
    "        optimizer=torch.optim.AdamW,\n",
    "        optimizer__weight_decay = 0.4,\n",
    "        batch_size=32, # separate train and valid->iterator_train__batch_size=128 and iterator_valid__batch_size=128 ?\n",
    "        callbacks=[\n",
    "            (\n",
    "                \"sched\",\n",
    "                LRScheduler(\n",
    "                    torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
    "                    monitor=\"valid_loss\",\n",
    "                    patience=5,\n",
    "                ),\n",
    "            ),\n",
    "            (\n",
    "                \"es\",\n",
    "                EarlyStopping(\n",
    "                    monitor=\"valid_loss\",\n",
    "                    patience=early_stopping_rounds,\n",
    "                    load_best=True,\n",
    "                ),\n",
    "            ),\n",
    "            (\"seed\", FixSeed(seed=42)),\n",
    "            #(\"Input Shape Setter\",InputShapeSetter())\n",
    "        ],train_split = CustomValidSplit(0.2, stratified=True, random_state=rand_state), \n",
    "        #lr=0.001,\n",
    "        #max_epochs=1, #0,#100\n",
    "        #train_split=None,\n",
    "        verbose=2\n",
    "    )\n",
    "    df_agg_metrics_cindex, df_agg_metrics_ibs, best_model,params, outer_scores, best_preds_train, best_preds_test = train_eval(X, y, net, n_iter, data.filename)\n",
    "    agg_metrics_cindex.append(df_agg_metrics_cindex)\n",
    "    agg_metrics_ibs.append(df_agg_metrics_ibs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_aft_1_cindex = pd.concat([df for df in agg_metrics_cindex]).round(4)\n",
    "df_final_aft_1_cindex.to_csv('metrics/final_deep_learning_aft_1_cindex.csv', index=False)\n",
    "df_final_aft_1_cindex.to_csv('/Users/JUSC/Documents/644928e0fb7e147893e8ec15/05_thesis/tables/final_deep_learning_aft_1_cindex.csv', index=False)  #\n",
    "df_final_aft_1_cindex"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_aft_1_ibs = pd.concat([df for df in agg_metrics_ibs]).round(4)\n",
    "df_final_aft_1_ibs.to_csv('metrics/final_deep_learning_aft_1_ibs.csv', index=False)\n",
    "df_final_aft_1_ibs.to_csv('/Users/JUSC/Documents/644928e0fb7e147893e8ec15/05_thesis/tables/final_deep_learning_aft_1_ibs.csv', index=False) \n",
    "df_final_aft_1_ibs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TCGA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_aft_tcga = {\n",
    "    'estimator__module__n_layers': [1, 2, 4],\n",
    "    'estimator__module__num_nodes': [64, 128, 256, 512],\n",
    "    'estimator__module__dropout': scuniform(0.0,0.7),\n",
    "    'estimator__optimizer__weight_decay': [0.4, 0.2, 0.1, 0.05, 0.02, 0.01, 0],\n",
    "    'estimator__batch_size': [64, 128, 256, 512, 1024],\n",
    "    #lr not in paper because of learning rate finder\n",
    "    # note: setting learning rate higher would make exp(partial_hazard) explode\n",
    "    #'estimator__lr': scloguniform(0.001,0.01), # scheduler unten einbauen\n",
    "    # use callback instead\n",
    "    'estimator__lr':[0.01],\n",
    "    #'estimator__max_epochs':  scrandint(10,20), # corresponds to num_rounds\n",
    "    'pca__n_components': [8, 10, 12, 14, 16]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_eval(X, y, net, n_iter, filename):\n",
    "        model = 'aft_'\n",
    "        dataset_name = filename.split('_')[0]\n",
    "        # add IBS later\n",
    "        outer_scores = {'cindex_train_'+dataset_name:[], 'cindex_test_'+dataset_name:[],\n",
    "                        'ibs_train_'+dataset_name:[], 'ibs_test_'+dataset_name:[]}\n",
    "        best_params = {'best_params_'+dataset_name:[]}\n",
    "        best_model = {'best_model_'+dataset_name:[]}\n",
    "        ct = make_column_transformer(\n",
    "                #(OneHotEncoder(sparse_output=False), make_column_selector(dtype_include=['category', 'object']))\n",
    "                (StandardScaler(), make_column_selector(dtype_include=['float32']))\n",
    "                ,remainder='drop')\n",
    "        pipe = Pipeline([('scaler',ct),\n",
    "                         ('pca', PCA()),#n_components=10\n",
    "                        ('estimator', net)])\n",
    "        rs = RandomizedSearchCV(pipe, param_grid_aft_tcga, scoring = scoring_function, n_jobs=-1, \n",
    "                                    n_iter=2, refit=True)\n",
    "        for i, (train_index, test_index) in enumerate(outer_custom_cv.split(X, y)):\n",
    "                # Split data into training and testing sets for outer fold\n",
    "                X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "                y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "                X_train, y_train = sort_X_y_pandas(X_train, y_train)\n",
    "                X_test, y_test = sort_X_y_pandas(X_test, y_test)\n",
    "\n",
    "                #print(X_train.shape, type(X_train))\n",
    "                #print(y_train.shape, type(y_train))\n",
    "                #print(X_test.shape, type(X_test))\n",
    "                #print(y_test.shape, type(y_test))\n",
    "                # save splits and data\n",
    "                savetxt('splits/train_index_'+str(i)+'_'+filename, train_index, delimiter=',')\n",
    "                savetxt('splits/test_index_'+str(i)+'_'+filename, test_index, delimiter=',')\n",
    "                \n",
    "                # savetxt('splits/X_train_'+str(i)+'_'+filename, X_train, delimiter=',')\n",
    "                # savetxt('splits/X_test_'+str(i)+'_'+filename, X_test, delimiter=',')\n",
    "\n",
    "                # savetxt('splits/y_train_'+str(i)+'_'+filename, y_train, delimiter=',')\n",
    "                # savetxt('splits/y_test_'+str(i)+'_'+filename, y_test, delimiter=',')\n",
    "\n",
    "                rs.fit(X_train, y_train)\n",
    "                best_preds_train = rs.best_estimator_.predict(X_train)\n",
    "                best_preds_test = rs.best_estimator_.predict(X_test)\n",
    "                savetxt('predictions/'+model+'best_preds_train_'+str(i)+'_'+filename, best_preds_train, delimiter=',')\n",
    "                savetxt('predictions/'+model+'best_preds_test_'+str(i)+'_'+filename, best_preds_test, delimiter=',')\n",
    "\n",
    "                \n",
    "                # save hyperparameter settings\n",
    "                params = rs.best_estimator_.get_params\n",
    "                best_params['best_params_'+dataset_name] += [rs.best_params_]\n",
    "                best_model['best_model_'+dataset_name] += [params]\n",
    "                try:\n",
    "                    cum_hazard_train = get_cumulative_hazard_function_aft(\n",
    "                            X_train.values, X_train.values, y_train.values, y_train.values,\n",
    "                            best_preds_train.reshape(-1), best_preds_train.reshape(-1)\n",
    "                            )\n",
    "\n",
    "                    df_survival_train = np.exp(-cum_hazard_train)\n",
    "                    durations_train, events_train = transform_back(y_train.values)\n",
    "                    time_grid_train = np.linspace(durations_train.min(), durations_train.max(), 100)\n",
    "                    ev = EvalSurv(df_survival_train, durations_train, events_train, censor_surv='km')\n",
    "                    print('Concordance Index',ev.concordance_td('antolini'))\n",
    "                    print('Integrated Brier Score:',ev.integrated_brier_score(time_grid_train))\n",
    "                    cindex_score_train = ev.concordance_td('antolini')\n",
    "                    ibs_score_train = ev.integrated_brier_score(time_grid_train)\n",
    "\n",
    "                    outer_scores['cindex_train_'+dataset_name] += [cindex_score_train]\n",
    "                    outer_scores['ibs_train_'+dataset_name] += [ibs_score_train]\n",
    "\n",
    "                except:\n",
    "                    outer_scores['cindex_train_'+dataset_name] += [np.nan]\n",
    "                    outer_scores['ibs_train_'+dataset_name] += [np.nan]\n",
    "                    \n",
    "                try:\n",
    "                    cum_hazard_test = get_cumulative_hazard_function_aft(\n",
    "                            X_train.values, X_test.values, y_train.values, y_test.values,\n",
    "                            best_preds_train.reshape(-1), best_preds_test.reshape(-1)\n",
    "                            )\n",
    "                    df_survival_test = np.exp(-cum_hazard_test)\n",
    "                    durations_test, events_test = transform_back(y_test.values)\n",
    "                    print('durations',durations_test.min(), durations_test.max())\n",
    "                    time_grid_test = np.linspace(durations_test.min(), durations_test.max(), 100)\n",
    "                    ev = EvalSurv(df_survival_test, durations_test, events_test, censor_surv='km')\n",
    "                    print('Concordance Index',ev.concordance_td('antolini'))\n",
    "                    print('Integrated Brier Score:',ev.integrated_brier_score(time_grid_test))\n",
    "                    cindex_score_test = ev.concordance_td('antolini')\n",
    "                    ibs_score_test = ev.integrated_brier_score(time_grid_test)\n",
    "\n",
    "                    outer_scores['cindex_test_'+dataset_name] += [cindex_score_test]\n",
    "                    outer_scores['ibs_test_'+dataset_name] += [ibs_score_test]\n",
    "                except: \n",
    "                    outer_scores['cindex_test_'+dataset_name] += [np.nan]\n",
    "                    outer_scores['ibs_test_'+dataset_name] += [np.nan]\n",
    "            \n",
    "        df_best_params = pd.DataFrame(best_params)\n",
    "        df_best_model = pd.DataFrame(best_model)\n",
    "        df_outer_scores = pd.DataFrame(outer_scores)\n",
    "        df_metrics = pd.concat([df_best_params,df_best_model,df_outer_scores], axis=1)\n",
    "        df_metrics.to_csv('metrics/'+model+'metric_summary'+str(i)+'_'+filename, index=False)\n",
    "        # cindex\n",
    "        df_agg_metrics_cindex = pd.DataFrame({'dataset':[dataset_name],\n",
    "                                              'cindex_train_mean':df_outer_scores['cindex_train_'+dataset_name].mean(),\n",
    "                                              'cindex_train_std':df_outer_scores['cindex_train_'+dataset_name].std(),\n",
    "                                              'cindex_test_mean':df_outer_scores['cindex_test_'+dataset_name].mean(),\n",
    "                                              'cindex_test_std':df_outer_scores['cindex_test_'+dataset_name].std() })\n",
    "        # IBS\n",
    "        df_agg_metrics_ibs = pd.DataFrame({'dataset':[dataset_name],\n",
    "                                              'ibs_train_mean':df_outer_scores['ibs_train_'+dataset_name].mean(),\n",
    "                                              'ibs_train_std':df_outer_scores['ibs_train_'+dataset_name].std(),\n",
    "                                              'ibs_test_mean':df_outer_scores['ibs_test_'+dataset_name].mean(),\n",
    "                                              'ibs_test_std':df_outer_scores['ibs_test_'+dataset_name].std() })\n",
    "        \n",
    "        return df_agg_metrics_cindex, df_agg_metrics_ibs, best_model, best_params, outer_scores, best_preds_train, best_preds_test \n",
    "\n",
    "                \n",
    "#cv=inner_custom_cv,pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AHLoss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 38\u001b[0m\n\u001b[1;32m     24\u001b[0m y \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mtarget \u001b[39m#.values #.to_numpy()\u001b[39;00m\n\u001b[1;32m     26\u001b[0m X, y \u001b[39m=\u001b[39m sort_X_y_pandas(X, y)\n\u001b[1;32m     28\u001b[0m net \u001b[39m=\u001b[39m NeuralNet(\n\u001b[1;32m     29\u001b[0m     SurvivalModel, \n\u001b[1;32m     30\u001b[0m     module__n_layers \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m,\n\u001b[1;32m     31\u001b[0m     module__input_units \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m],\n\u001b[1;32m     32\u001b[0m     \u001b[39m#module__num_nodes = 32,\u001b[39;00m\n\u001b[1;32m     33\u001b[0m     \u001b[39m#module__dropout = 0.1, # these could also be removed\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     module__out_features \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m,\n\u001b[1;32m     35\u001b[0m     \u001b[39m# for split sizes when result size = 1\u001b[39;00m\n\u001b[1;32m     36\u001b[0m     iterator_train__drop_last\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     37\u001b[0m     \u001b[39m#iterator_valid__drop_last=True,\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m     criterion\u001b[39m=\u001b[39mAHLoss,\n\u001b[1;32m     39\u001b[0m     optimizer\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdamW,\n\u001b[1;32m     40\u001b[0m     optimizer__weight_decay \u001b[39m=\u001b[39m \u001b[39m0.4\u001b[39m,\n\u001b[1;32m     41\u001b[0m     batch_size\u001b[39m=\u001b[39m\u001b[39m32\u001b[39m, \u001b[39m# separate train and valid->iterator_train__batch_size=128 and iterator_valid__batch_size=128 ?\u001b[39;00m\n\u001b[1;32m     42\u001b[0m     callbacks\u001b[39m=\u001b[39m[\n\u001b[1;32m     43\u001b[0m         (\n\u001b[1;32m     44\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39msched\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     45\u001b[0m             LRScheduler(\n\u001b[1;32m     46\u001b[0m                 torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mlr_scheduler\u001b[39m.\u001b[39mReduceLROnPlateau,\n\u001b[1;32m     47\u001b[0m                 monitor\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mvalid_loss\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     48\u001b[0m                 patience\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m,\n\u001b[1;32m     49\u001b[0m             ),\n\u001b[1;32m     50\u001b[0m         ),\n\u001b[1;32m     51\u001b[0m         (\n\u001b[1;32m     52\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mes\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     53\u001b[0m             EarlyStopping(\n\u001b[1;32m     54\u001b[0m                 monitor\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mvalid_loss\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     55\u001b[0m                 patience\u001b[39m=\u001b[39mearly_stopping_rounds,\n\u001b[1;32m     56\u001b[0m                 load_best\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     57\u001b[0m             ),\n\u001b[1;32m     58\u001b[0m         ),\n\u001b[1;32m     59\u001b[0m         (\u001b[39m\"\u001b[39m\u001b[39mseed\u001b[39m\u001b[39m\"\u001b[39m, FixSeed(seed\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)),\n\u001b[1;32m     60\u001b[0m         (\u001b[39m\"\u001b[39m\u001b[39mInput Shape Setter\u001b[39m\u001b[39m\"\u001b[39m,InputShapeSetter())\n\u001b[1;32m     61\u001b[0m     ],\u001b[39m#[EarlyStopping(patience=10)],#,InputShapeSetter()],\u001b[39;00m\n\u001b[1;32m     62\u001b[0m     \u001b[39m#TODO: enable stratification, verify\u001b[39;00m\n\u001b[1;32m     63\u001b[0m     train_split \u001b[39m=\u001b[39m CustomValidSplit(\u001b[39m0.2\u001b[39m, stratified\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, random_state\u001b[39m=\u001b[39mrand_state), \n\u001b[1;32m     64\u001b[0m     \u001b[39m#max_epochs=1, #0,#100\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     \u001b[39m#train_split=None,\u001b[39;00m\n\u001b[1;32m     66\u001b[0m     verbose\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m     67\u001b[0m )\n\u001b[1;32m     68\u001b[0m df_agg_metrics_cindex, df_agg_metrics_ibs, best_model,params, outer_scores, best_preds_train, best_preds_test \u001b[39m=\u001b[39m train_eval(X, y, net, n_iter, data\u001b[39m.\u001b[39mfilename)\n\u001b[1;32m     69\u001b[0m agg_metrics_cindex\u001b[39m.\u001b[39mappend(df_agg_metrics_cindex)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'AHLoss' is not defined"
     ]
    }
   ],
   "source": [
    "cancer_types = [\n",
    "    'BLCA',\n",
    "    'BRCA',\n",
    "    'HNSC',\n",
    "    'KIRC',\n",
    "    'LGG',\n",
    "    'LIHC',\n",
    "    'LUAD',\n",
    "    'LUSC',\n",
    "    'OV',\n",
    "    'STAD']\n",
    "    \n",
    "agg_metrics_cindex = []\n",
    "agg_metrics_ibs = []\n",
    "\n",
    "class InputShapeSetter(skorch.callbacks.Callback):\n",
    "    def on_train_begin(self, net, X, y):\n",
    "        net.set_params(module__input_units=X.shape[-1])\n",
    "\n",
    "for idx, cancer_type in enumerate(cancer_types):\n",
    "    # get name of current dataset\n",
    "    data = load_tcga(path=\"/Users/JUSC/Documents/xgbsurv/xgbsurv/datasets/data/\", cancer_type=cancer_type, as_frame=True)\n",
    "    X  = data.data #.astype(np.float32)\n",
    "    y = data.target #.values #.to_numpy()\n",
    "\n",
    "    X, y = sort_X_y_pandas(X, y)\n",
    "\n",
    "    net = NeuralNet(\n",
    "        SurvivalModel, \n",
    "        module__n_layers = 1,\n",
    "        module__input_units = X.shape[1],\n",
    "        #module__num_nodes = 32,\n",
    "        #module__dropout = 0.1, # these could also be removed\n",
    "        module__out_features = 1,\n",
    "        # for split sizes when result size = 1\n",
    "        iterator_train__drop_last=True,\n",
    "        #iterator_valid__drop_last=True,\n",
    "        criterion=AHLoss,\n",
    "        optimizer=torch.optim.AdamW,\n",
    "        optimizer__weight_decay = 0.4,\n",
    "        batch_size=32, # separate train and valid->iterator_train__batch_size=128 and iterator_valid__batch_size=128 ?\n",
    "        callbacks=[\n",
    "            (\n",
    "                \"sched\",\n",
    "                LRScheduler(\n",
    "                    torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
    "                    monitor=\"valid_loss\",\n",
    "                    patience=5,\n",
    "                ),\n",
    "            ),\n",
    "            (\n",
    "                \"es\",\n",
    "                EarlyStopping(\n",
    "                    monitor=\"valid_loss\",\n",
    "                    patience=early_stopping_rounds,\n",
    "                    load_best=True,\n",
    "                ),\n",
    "            ),\n",
    "            (\"seed\", FixSeed(seed=42)),\n",
    "            (\"Input Shape Setter\",InputShapeSetter())\n",
    "        ],#[EarlyStopping(patience=10)],#,InputShapeSetter()],\n",
    "        #TODO: enable stratification, verify\n",
    "        train_split = CustomValidSplit(0.2, stratified=True, random_state=rand_state), \n",
    "        #max_epochs=1, #0,#100\n",
    "        #train_split=None,\n",
    "        verbose=1\n",
    "    )\n",
    "    df_agg_metrics_cindex, df_agg_metrics_ibs, best_model,params, outer_scores, best_preds_train, best_preds_test = train_eval(X, y, net, n_iter, data.filename)\n",
    "    agg_metrics_cindex.append(df_agg_metrics_cindex)\n",
    "    agg_metrics_ibs.append(df_agg_metrics_ibs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_aft_1_ibs = pd.concat([df for df in agg_metrics_ibs]).round(4)\n",
    "df_final_aft_1_ibs.to_csv('metrics/final_deep_learning_aft_tcga_ibs.csv', index=False)\n",
    "df_final_aft_1_ibs.to_csv('/Users/JUSC/Documents/644928e0fb7e147893e8ec15/05_thesis/tables/final_deep_learning_aft_tcga_ibs.csv', index=False) \n",
    "df_final_aft_1_ibs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_aft_1_ibs = pd.concat([df for df in agg_metrics_ibs]).round(4)\n",
    "df_final_aft_1_ibs.to_csv('metrics/final_deep_learning_aft_tcga_ibs.csv', index=False)\n",
    "df_final_aft_1_ibs.to_csv('/Users/JUSC/Documents/644928e0fb7e147893e8ec15/05_thesis/tables/final_deep_learning_aft_tcga_ibs.csv', index=False) \n",
    "df_final_aft_1_ibs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xgbsurv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
