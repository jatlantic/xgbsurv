{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import savetxt\n",
    "from xgbsurv.datasets import (load_metabric, load_flchain, load_rgbsg, load_support, load_tcga)\n",
    "from xgbsurv.models.utils import sort_X_y_pandas, transform_back, transform\n",
    "from xgbsurv.preprocessing.dataset_preprocessing import discretizer_df\n",
    "import torch\n",
    "from torch import nn\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold, train_test_split\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, LabelBinarizer, OneHotEncoder\n",
    "from sklearn.compose import make_column_transformer, make_column_selector\n",
    "from sklearn.decomposition import PCA\n",
    "from loss_functions_pytorch import DeephitLoss, deephit_likelihood_1_torch\n",
    "from skorch import NeuralNet, NeuralNetRegressor\n",
    "from skorch.callbacks import EarlyStopping, Callback, LRScheduler\n",
    "from skorch.dataset import ValidSplit\n",
    "from pycox.evaluation import EvalSurv\n",
    "from pycox.models import DeepHitSingle\n",
    "from scipy.stats import uniform as scuniform\n",
    "from scipy.stats import randint as scrandint\n",
    "from scipy.stats import loguniform as scloguniform\n",
    "import random\n",
    "from functools import partial\n",
    "import os\n",
    "#torch.set_default_dtype(torch.float64)\n",
    "#torch.set_default_tensor_type(torch.DoubleTensor)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters, put into function\n",
    "n_outer_splits = 5\n",
    "n_inner_splits = 5\n",
    "rand_state = 42\n",
    "n_iter = 10 # set to 50\n",
    "#n_iter_cind = 200\n",
    "early_stopping_rounds=15\n",
    "base_score = 0.0\n",
    "\n",
    "param_grid_breslow = {\n",
    "    'estimator__module__n_layers': [1, 2, 4],\n",
    "    'estimator__module__n_layers': [1, 2, 4],\n",
    "    'estimator__module__num_nodes': [64, 128, 256, 512],\n",
    "    'estimator__module__dropout': scuniform(0.0,0.7),\n",
    "    'estimator__optimizer__weight_decay': [0.4, 0.2, 0.1, 0.05, 0.02, 0.01, 0],\n",
    "    'estimator__batch_size': [64, 128, 256, 512, 1024],\n",
    "    #lr not in paper because of learning rate finder\n",
    "    # note: setting learning rate higher would make exp(partial_hazard) explode\n",
    "    #'estimator__lr': scloguniform(0.001,0.01), # scheduler unten einbauen\n",
    "    # use callback instead\n",
    "    'estimator__lr':[0.01]\n",
    "    #'max_epochs':  scrandint(10,20), # corresponds to num_rounds\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_torch(seed=42):\n",
    "    \"\"\"Sets all seeds within torch and adjacent libraries.\n",
    "\n",
    "    Args:\n",
    "        seed: Random seed to be used by the seeding functions.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    #os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    #torch.backends.cudnn.benchmark = False\n",
    "    #torch.backends.cudnn.deterministic = True\n",
    "    return None\n",
    "\n",
    "\n",
    "class FixSeed(Callback):\n",
    "    def __init__(self, seed):\n",
    "        self.seed = seed\n",
    "\n",
    "    def initialize(self):\n",
    "        seed_torch(self.seed)\n",
    "        return super().initialize()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = load_flchain(path=\"/Users/JUSC/Documents/xgbsurv/xgbsurv/datasets/data/\", as_frame=True)\n",
    "# print(type(data.data),type(data.target))\n",
    "# X, y = data.data, data.target\n",
    "# #X, y = data.data.astype(np.float64), data.target.astype(np.float64)\n",
    "# #one_hot_dict = {'load_flchain':[''], 'load_support':['cancer'], 'load_rgbsg':['grade']}\n",
    "# #X = pd.get_dummies(X, columns=['cancer'])\n",
    "# #X = pd.get_dummies(X, columns=['grade'])\n",
    "# #X = pd.get_dummies(X, columns=['mgus'])\n",
    "# X.mgus.value"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Scorer\n",
    "def custom_scoring_function(y_true, y_pred, duration_bins):\n",
    "\n",
    "        #y_true = torch.from_numpy(y_true)\n",
    "        if isinstance(y_pred, np.ndarray):\n",
    "            y_pred = torch.from_numpy(y_pred)\n",
    "        if isinstance(y_true, np.ndarray):\n",
    "            y_true = torch.from_numpy(y_true)\n",
    "        if isinstance(y_pred, pd.Series):\n",
    "            y_pred = torch.tensor(y_pred.values)\n",
    "        if isinstance(y_true, pd.Series):\n",
    "            y_true = torch.tensor(y_true.values)\n",
    "        print('score functions types',type(y_true), type(y_pred), type(duration_bins) )\n",
    "        score = deephit_likelihood_1_torch(y_true, y_pred, duration_bins).to(torch.float32)\n",
    "        return score.numpy()\n",
    "\n",
    "scoring_function = make_scorer(custom_scoring_function, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set up Custom Splitter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Torch Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SurvivalModel(nn.Module):\n",
    "    def __init__(self, n_layers, input_units, num_nodes, dropout, out_features):\n",
    "        super(SurvivalModel, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.in_features = input_units\n",
    "        self.num_nodes = num_nodes\n",
    "        self.dropout = dropout\n",
    "        self.out_features = out_features\n",
    "        model = []\n",
    "        # first layer\n",
    "        model.append(torch.nn.Linear(input_units, num_nodes))\n",
    "        model.append(torch.nn.ReLU())\n",
    "        model.append(torch.nn.Dropout(dropout))\n",
    "        model.append(torch.nn.BatchNorm1d(num_nodes))\n",
    "\n",
    "        for i in range(n_layers-1):\n",
    "            model.append(torch.nn.Linear(num_nodes, num_nodes))\n",
    "            #init.kaiming_normal_(model[-1].weight, nonlinearity='relu')\n",
    "            model.append(torch.nn.ReLU())\n",
    "            model.append(torch.nn.Dropout(dropout))\n",
    "            model.append(torch.nn.BatchNorm1d(num_nodes))\n",
    "\n",
    "        # output layer\n",
    "        model.append(torch.nn.Linear(num_nodes, out_features))\n",
    "        model.append(torch.nn.Softmax(dim=1))\n",
    "    \n",
    "        self.layers = nn.Sequential(*model)\n",
    "\n",
    "        # for layer in self.layers:\n",
    "        #     if isinstance(layer, nn.Linear):\n",
    "        #         #nn.init.uniform_(layer.weight, a=-0.5, b=0.5)\n",
    "        #         nn.init.kaiming_normal_(layer.weight)\n",
    "\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = X.to(torch.float32)\n",
    "        res = self.layers(X)\n",
    "        #print(res)\n",
    "        return res\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomStandardScaler(StandardScaler):\n",
    "    \n",
    "    def __init__(self, copy=True, with_mean=True, with_std=True):\n",
    "        super().__init__(copy=copy, with_mean=with_mean, with_std=with_std)\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return super().fit(X, y)\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X_transformed = super().transform(X, y)\n",
    "        return X_transformed.astype(np.float32)\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        X_transformed = super().fit_transform(X, y)\n",
    "        return X_transformed.astype(np.float32)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define stratified inner k-fold cross-validation\n",
    "class CustomSplit(StratifiedKFold):\n",
    "    def __init__(self, n_splits=5, shuffle=True, random_state=None):\n",
    "        super().__init__(n_splits=n_splits, shuffle=shuffle, random_state=random_state)\n",
    "\n",
    "    def split(self, X, y, groups=None):\n",
    "        print('split', X.dtypes)\n",
    "        try:\n",
    "            if y.shape[1]>1:\n",
    "                y = y[:,0]\n",
    "        except:\n",
    "            pass\n",
    "        bins = np.sign(y)\n",
    "        return super().split(X, bins, groups=groups)\n",
    "\n",
    "    def get_n_splits(self, X=None, y=None, groups=None):\n",
    "        return self.n_splits\n",
    "\n",
    "outer_custom_cv = CustomSplit(n_splits=n_outer_splits, shuffle=True, random_state=rand_state)\n",
    "inner_custom_cv = CustomSplit(n_splits=n_outer_splits, shuffle=True, random_state=rand_state)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Training Procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom transform allowing for zero time value due to deephit\n",
    "def transform(time, event):\n",
    "\n",
    "    #if isinstance(time, pd.Series):\n",
    "    #    time = time.to_numpy()\n",
    "    #    event = event.to_numpy()\n",
    "    event_mod = np.copy(event) \n",
    "    event_mod[event_mod==0] = -1\n",
    "    y = event_mod*time\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skorch.callbacks\n",
    "class InputShapeSetter(skorch.callbacks.Callback):\n",
    "    def on_train_begin(self, net, X, y):\n",
    "        net.set_params(module__input_units=X.shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split MKI67                float32\n",
      "EGFR                 float32\n",
      "PGR                  float32\n",
      "ERBB2                float32\n",
      "hormone_treatment    float32\n",
      "radiotherapy         float32\n",
      "chemotherapy         float32\n",
      "ER_positive          float32\n",
      "age                  float32\n",
      "dtype: object\n",
      "dtype X_train MKI67                float32\n",
      "EGFR                 float32\n",
      "PGR                  float32\n",
      "ERBB2                float32\n",
      "hormone_treatment    float32\n",
      "radiotherapy         float32\n",
      "chemotherapy         float32\n",
      "ER_positive          float32\n",
      "age                  float32\n",
      "dtype: object\n",
      "dtype y_train float64\n",
      "dtype X_test MKI67                float32\n",
      "EGFR                 float32\n",
      "PGR                  float32\n",
      "ERBB2                float32\n",
      "hormone_treatment    float32\n",
      "radiotherapy         float32\n",
      "chemotherapy         float32\n",
      "ER_positive          float32\n",
      "age                  float32\n",
      "dtype: object\n",
      "dtype y_test float32\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m741.8909\u001b[0m      \u001b[32m355.9173\u001b[0m  0.0211\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m670.0085\u001b[0m      \u001b[32m318.5809\u001b[0m  0.0271\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m820.4971\u001b[0m      \u001b[32m360.3320\u001b[0m  0.0167\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m833.5837\u001b[0m      \u001b[32m369.2114\u001b[0m  0.0168\n",
      "      2      \u001b[36m737.5864\u001b[0m      \u001b[32m355.0228\u001b[0m  0.0121\n",
      "      2      \u001b[36m816.0898\u001b[0m      \u001b[32m359.4577\u001b[0m  0.0090\n",
      "      2      \u001b[36m830.1618\u001b[0m      \u001b[32m368.4353\u001b[0m  0.0084\n",
      "      3      \u001b[36m731.2394\u001b[0m      \u001b[32m353.5354\u001b[0m  0.0108\n",
      "      2      \u001b[36m664.8253\u001b[0m      \u001b[32m317.7430\u001b[0m  0.0140\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m861.1353\u001b[0m      \u001b[32m386.5057\u001b[0m  0.0218\n",
      "      3      \u001b[36m824.3261\u001b[0m      \u001b[32m367.0256\u001b[0m  0.0087\n",
      "      3      \u001b[36m809.9357\u001b[0m      \u001b[32m357.9330\u001b[0m  0.0126\n",
      "      4      \u001b[36m721.8300\u001b[0m      \u001b[32m351.0711\u001b[0m  0.0106\n",
      "      3      \u001b[36m658.6845\u001b[0m      \u001b[32m316.2659\u001b[0m  0.0102\n",
      "      4      \u001b[36m813.2961\u001b[0m      \u001b[32m364.8864\u001b[0m  0.0109\n",
      "      2      \u001b[36m857.9140\u001b[0m      \u001b[32m385.6414\u001b[0m  0.0124\n",
      "      4      \u001b[36m799.0219\u001b[0m      \u001b[32m355.5632\u001b[0m  0.0102\n",
      "      4      \u001b[36m649.0469\u001b[0m      \u001b[32m313.8201\u001b[0m  0.0087\n",
      "      5      \u001b[36m711.4801\u001b[0m      \u001b[32m347.2600\u001b[0m  0.0105\n",
      "      3      \u001b[36m850.9550\u001b[0m      \u001b[32m384.0739\u001b[0m  0.0088\n",
      "      5      \u001b[36m806.0552\u001b[0m      \u001b[32m361.5681\u001b[0m  0.0095\n",
      "      5      \u001b[36m640.0460\u001b[0m      \u001b[32m310.0884\u001b[0m  0.0078\n",
      "      5      \u001b[36m789.9379\u001b[0m      \u001b[32m351.9647\u001b[0m  0.0100\n",
      "      6      \u001b[36m700.0729\u001b[0m      \u001b[32m341.8429\u001b[0m  0.0102\n",
      "      6      \u001b[36m625.3433\u001b[0m      \u001b[32m304.8763\u001b[0m  0.0079\n",
      "      6      \u001b[36m794.7333\u001b[0m      \u001b[32m356.8385\u001b[0m  0.0122\n",
      "      6      \u001b[36m779.1177\u001b[0m      \u001b[32m346.8934\u001b[0m  0.0101\n",
      "      4      \u001b[36m842.3817\u001b[0m      \u001b[32m381.8135\u001b[0m  0.0150\n",
      "      7      \u001b[36m686.5930\u001b[0m      \u001b[32m334.7202\u001b[0m  0.0104\n",
      "      7      \u001b[36m613.0325\u001b[0m      \u001b[32m298.1579\u001b[0m  0.0077\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m138.5837\u001b[0m      \u001b[32m129.5915\u001b[0m  0.1605\n",
      "      7      \u001b[36m760.9648\u001b[0m      \u001b[32m340.1217\u001b[0m  0.0088\n",
      "      7      \u001b[36m779.2615\u001b[0m      \u001b[32m350.5985\u001b[0m  0.0112\n",
      "      5      \u001b[36m833.6021\u001b[0m      \u001b[32m378.3706\u001b[0m  0.0089\n",
      "      8      \u001b[36m598.8688\u001b[0m      \u001b[32m290.4984\u001b[0m  0.0099\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m177.7330\u001b[0m      \u001b[32m149.5452\u001b[0m  0.1375\n",
      "      6      \u001b[36m822.5192\u001b[0m      \u001b[32m373.5259\u001b[0m  0.0078\n",
      "      8      \u001b[36m742.7184\u001b[0m      \u001b[32m332.1611\u001b[0m  0.0088\n",
      "      8      \u001b[36m669.5771\u001b[0m      \u001b[32m326.3208\u001b[0m  0.0144\n",
      "      8      \u001b[36m760.3982\u001b[0m      \u001b[32m343.0616\u001b[0m  0.0094\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m158.9610\u001b[0m      \u001b[32m145.0070\u001b[0m  0.1656\n",
      "      7      \u001b[36m808.7088\u001b[0m      \u001b[32m367.0719\u001b[0m  0.0076\n",
      "      9      \u001b[36m580.9378\u001b[0m      \u001b[32m282.5862\u001b[0m  0.0104\n",
      "      9      \u001b[36m725.3804\u001b[0m      \u001b[32m323.5087\u001b[0m  0.0077\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m173.6808\u001b[0m      \u001b[32m145.2425\u001b[0m  0.1605\n",
      "      9      \u001b[36m743.6280\u001b[0m      \u001b[32m334.8392\u001b[0m  0.0136\n",
      "      9      \u001b[36m652.4578\u001b[0m      \u001b[32m317.4254\u001b[0m  0.0147\n",
      "      8      \u001b[36m789.7410\u001b[0m      \u001b[32m359.3738\u001b[0m  0.0099\n",
      "     10      \u001b[36m568.4882\u001b[0m      \u001b[32m275.2464\u001b[0m  0.0112\n",
      "     10      \u001b[36m709.6294\u001b[0m      \u001b[32m315.2035\u001b[0m  0.0114\n",
      "     10      \u001b[36m727.9511\u001b[0m      \u001b[32m326.8578\u001b[0m  0.0086\n",
      "     10      \u001b[36m641.4172\u001b[0m      \u001b[32m309.0189\u001b[0m  0.0089\n",
      "      9      \u001b[36m775.5350\u001b[0m      \u001b[32m350.9219\u001b[0m  0.0079\n",
      "     11      \u001b[36m555.6241\u001b[0m      \u001b[32m269.2216\u001b[0m  0.0088\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m183.7770\u001b[0m      \u001b[32m154.8688\u001b[0m  0.1478\n",
      "     11      \u001b[36m695.6024\u001b[0m      \u001b[32m307.8515\u001b[0m  0.0122\n",
      "     11      \u001b[36m713.4920\u001b[0m      \u001b[32m319.6545\u001b[0m  0.0121\n",
      "     11      \u001b[36m629.3014\u001b[0m      \u001b[32m301.8559\u001b[0m  0.0124\n",
      "     12      \u001b[36m541.2137\u001b[0m      \u001b[32m264.8517\u001b[0m  0.0121\n",
      "     12      \u001b[36m700.8883\u001b[0m      \u001b[32m313.7096\u001b[0m  0.0107\n",
      "     10      \u001b[36m759.4995\u001b[0m      \u001b[32m342.9428\u001b[0m  0.0206\n",
      "     13      \u001b[36m532.1657\u001b[0m      \u001b[32m261.8060\u001b[0m  0.0082\n",
      "     12      \u001b[36m682.6979\u001b[0m      \u001b[32m301.8780\u001b[0m  0.0147\n",
      "     12      \u001b[36m617.4525\u001b[0m      \u001b[32m296.4121\u001b[0m  0.0180\n",
      "     13      \u001b[36m687.0945\u001b[0m      \u001b[32m308.8503\u001b[0m  0.0080\n",
      "     14      \u001b[36m528.1642\u001b[0m      \u001b[32m259.7790\u001b[0m  0.0083\n",
      "     11      \u001b[36m751.0245\u001b[0m      \u001b[32m335.4032\u001b[0m  0.0113\n",
      "     13      \u001b[36m669.3002\u001b[0m      \u001b[32m296.9869\u001b[0m  0.0135\n",
      "     13      \u001b[36m607.8342\u001b[0m      \u001b[32m292.4565\u001b[0m  0.0082\n",
      "     14      \u001b[36m679.5739\u001b[0m      \u001b[32m304.8300\u001b[0m  0.0113\n",
      "     15      \u001b[36m521.8041\u001b[0m      \u001b[32m258.4124\u001b[0m  0.0087\n",
      "     12      \u001b[36m729.9272\u001b[0m      \u001b[32m328.5475\u001b[0m  0.0086\n",
      "     14      \u001b[36m604.7018\u001b[0m      \u001b[32m289.6368\u001b[0m  0.0125\n",
      "     16      \u001b[36m518.9362\u001b[0m      \u001b[32m257.2789\u001b[0m  0.0107\n",
      "     14      \u001b[36m663.7407\u001b[0m      \u001b[32m293.0452\u001b[0m  0.0170\n",
      "     15      \u001b[36m673.2999\u001b[0m      \u001b[32m301.5394\u001b[0m  0.0141\n",
      "     13      \u001b[36m719.1232\u001b[0m      \u001b[32m322.3960\u001b[0m  0.0136\n",
      "     15      \u001b[36m599.1205\u001b[0m      \u001b[32m287.6207\u001b[0m  0.0089\n",
      "     15      \u001b[36m659.1057\u001b[0m      \u001b[32m289.9513\u001b[0m  0.0087\n",
      "     14      \u001b[36m709.6343\u001b[0m      \u001b[32m316.8622\u001b[0m  0.0082\n",
      "     16      \u001b[36m668.3315\u001b[0m      \u001b[32m298.7543\u001b[0m  0.0095\n",
      "     16      \u001b[36m653.5142\u001b[0m      \u001b[32m287.6027\u001b[0m  0.0077\n",
      "     17      \u001b[36m516.1185\u001b[0m      \u001b[32m256.3040\u001b[0m  0.0191\n",
      "     16      \u001b[36m593.7004\u001b[0m      \u001b[32m285.9444\u001b[0m  0.0110\n",
      "      2      \u001b[36m123.9717\u001b[0m      \u001b[32m123.9171\u001b[0m  0.1142\n",
      "     15      \u001b[36m701.8182\u001b[0m      \u001b[32m311.9871\u001b[0m  0.0106\n",
      "      2      \u001b[36m158.8667\u001b[0m      \u001b[32m142.6210\u001b[0m  0.1066\n",
      "     17      \u001b[36m663.1113\u001b[0m      \u001b[32m296.3703\u001b[0m  0.0104\n",
      "     17      \u001b[36m592.1503\u001b[0m      \u001b[32m284.5627\u001b[0m  0.0105\n",
      "      2      \u001b[36m144.8923\u001b[0m      \u001b[32m139.1823\u001b[0m  0.1105\n",
      "     17      \u001b[36m649.1981\u001b[0m      \u001b[32m285.8769\u001b[0m  0.0117\n",
      "     16      \u001b[36m693.1758\u001b[0m      \u001b[32m307.7712\u001b[0m  0.0079\n",
      "     18      \u001b[36m513.7621\u001b[0m      \u001b[32m255.3231\u001b[0m  0.0119\n",
      "     17      \u001b[36m683.0352\u001b[0m      \u001b[32m304.2640\u001b[0m  0.0080\n",
      "     18      \u001b[36m647.1454\u001b[0m      \u001b[32m284.4986\u001b[0m  0.0085\n",
      "     19      \u001b[36m511.0933\u001b[0m      \u001b[32m254.3889\u001b[0m  0.0080\n",
      "     18      \u001b[36m656.9312\u001b[0m      \u001b[32m294.1552\u001b[0m  0.0155\n",
      "     18      \u001b[36m589.3849\u001b[0m      \u001b[32m283.2633\u001b[0m  0.0117\n",
      "     18      \u001b[36m679.7475\u001b[0m      \u001b[32m301.1872\u001b[0m  0.0077\n",
      "     19      \u001b[36m638.5815\u001b[0m      \u001b[32m283.4026\u001b[0m  0.0078\n",
      "     20      \u001b[36m509.6771\u001b[0m      \u001b[32m253.4697\u001b[0m  0.0080\n",
      "     19      \u001b[36m649.9967\u001b[0m      \u001b[32m292.1891\u001b[0m  0.0089\n",
      "      2      \u001b[36m158.0368\u001b[0m      \u001b[32m140.4133\u001b[0m  0.1246\n",
      "     19      \u001b[36m670.9807\u001b[0m      \u001b[32m298.3602\u001b[0m  0.0076\n",
      "     21      \u001b[36m507.9247\u001b[0m      \u001b[32m252.5151\u001b[0m  0.0078\n",
      "     20      \u001b[36m647.9694\u001b[0m      \u001b[32m290.4725\u001b[0m  0.0082\n",
      "     19      \u001b[36m583.5123\u001b[0m      \u001b[32m282.1573\u001b[0m  0.0131\n",
      "     20      639.1797      \u001b[32m282.3947\u001b[0m  0.0098\n",
      "     22      \u001b[36m503.6346\u001b[0m      \u001b[32m251.6258\u001b[0m  0.0078\n",
      "     20      \u001b[36m666.2235\u001b[0m      \u001b[32m295.8909\u001b[0m  0.0101\n",
      "     21      \u001b[36m646.2576\u001b[0m      \u001b[32m288.9249\u001b[0m  0.0082\n",
      "     21      \u001b[36m636.2380\u001b[0m      \u001b[32m281.4560\u001b[0m  0.0083\n",
      "     20      \u001b[36m583.2910\u001b[0m      \u001b[32m281.1869\u001b[0m  0.0095\n",
      "     22      647.7765      \u001b[32m287.5326\u001b[0m  0.0079\n",
      "     21      \u001b[36m663.2359\u001b[0m      \u001b[32m293.8933\u001b[0m  0.0088\n",
      "     21      583.5715      \u001b[32m280.3414\u001b[0m  0.0094\n",
      "     23      506.1417      \u001b[32m250.8996\u001b[0m  0.0122\n",
      "     22      \u001b[36m632.6110\u001b[0m      \u001b[32m280.5515\u001b[0m  0.0143\n",
      "     22      \u001b[36m658.6103\u001b[0m      \u001b[32m292.1327\u001b[0m  0.0083\n",
      "     23      \u001b[36m642.7177\u001b[0m      \u001b[32m285.9709\u001b[0m  0.0096\n",
      "     24      506.4578      \u001b[32m250.2688\u001b[0m  0.0138\n",
      "     23      636.5925      \u001b[32m279.5731\u001b[0m  0.0094\n",
      "     22      584.4017      \u001b[32m279.6989\u001b[0m  0.0153\n",
      "     23      \u001b[36m654.9806\u001b[0m      \u001b[32m290.5297\u001b[0m  0.0111\n",
      "     24      643.2667      \u001b[32m284.2560\u001b[0m  0.0121\n",
      "     24      634.9988      \u001b[32m278.5278\u001b[0m  0.0091\n",
      "     24      \u001b[36m650.1606\u001b[0m      \u001b[32m289.1450\u001b[0m  0.0111\n",
      "     25      \u001b[36m637.0987\u001b[0m      \u001b[32m282.5051\u001b[0m  0.0109\n",
      "     25      \u001b[36m502.5587\u001b[0m      \u001b[32m249.7302\u001b[0m  0.0179\n",
      "     23      \u001b[36m582.3575\u001b[0m      \u001b[32m279.1723\u001b[0m  0.0157\n",
      "     25      652.8897      \u001b[32m288.0435\u001b[0m  0.0094\n",
      "     26      \u001b[36m635.4717\u001b[0m      \u001b[32m280.6808\u001b[0m  0.0123\n",
      "     25      \u001b[36m629.7713\u001b[0m      \u001b[32m277.4602\u001b[0m  0.0217\n",
      "     26      651.8950      \u001b[32m287.0383\u001b[0m  0.0131\n",
      "     24      \u001b[36m581.9877\u001b[0m      \u001b[32m278.6515\u001b[0m  0.0214\n",
      "     26      505.6178      \u001b[32m249.2713\u001b[0m  0.0196\n",
      "      3      \u001b[36m122.3871\u001b[0m      \u001b[32m122.6525\u001b[0m  0.1143\n",
      "     27      \u001b[36m633.3920\u001b[0m      \u001b[32m278.9144\u001b[0m  0.0209\n",
      "     26      630.4817      \u001b[32m276.4395\u001b[0m  0.0190\n",
      "      3      \u001b[36m143.3094\u001b[0m      \u001b[32m138.5955\u001b[0m  0.1178\n",
      "     27      \u001b[36m644.7987\u001b[0m      \u001b[32m286.1678\u001b[0m  0.0162\n",
      "     27      503.6851      \u001b[32m248.8319\u001b[0m  0.0171\n",
      "     25      \u001b[36m581.2120\u001b[0m      \u001b[32m278.0772\u001b[0m  0.0211\n",
      "     27      \u001b[36m625.0899\u001b[0m      \u001b[32m275.3779\u001b[0m  0.0123\n",
      "      3      \u001b[36m153.8572\u001b[0m      \u001b[32m137.2410\u001b[0m  0.1155\n",
      "     28      \u001b[36m629.7294\u001b[0m      \u001b[32m277.0455\u001b[0m  0.0237\n",
      "     28      \u001b[36m620.9402\u001b[0m      \u001b[32m274.1922\u001b[0m  0.0112\n",
      "     28      \u001b[36m500.4865\u001b[0m      \u001b[32m248.3891\u001b[0m  0.0263\n",
      "     28      645.0697      \u001b[32m285.2906\u001b[0m  0.0302\n",
      "      3      \u001b[36m156.3023\u001b[0m      \u001b[32m139.5034\u001b[0m  0.1603\n",
      "     29      622.4940      \u001b[32m272.9684\u001b[0m  0.0091\n",
      "     26      581.6665      \u001b[32m277.5201\u001b[0m  0.0258\n",
      "     29      631.3481      \u001b[32m275.3835\u001b[0m  0.0118\n",
      "     29      \u001b[36m640.1651\u001b[0m      \u001b[32m284.4201\u001b[0m  0.0082\n",
      "     29      502.6821      \u001b[32m247.9172\u001b[0m  0.0109\n",
      "     27      \u001b[36m579.4360\u001b[0m      \u001b[32m277.0587\u001b[0m  0.0085\n",
      "     30      630.0120      \u001b[32m274.0627\u001b[0m  0.0099\n",
      "     30      621.5526      \u001b[32m271.8709\u001b[0m  0.0149\n",
      "     28      \u001b[36m578.1705\u001b[0m      \u001b[32m276.6094\u001b[0m  0.0077\n",
      "     31      \u001b[36m625.6057\u001b[0m      \u001b[32m273.0172\u001b[0m  0.0080\n",
      "     30      \u001b[36m638.5848\u001b[0m      \u001b[32m283.3872\u001b[0m  0.0146\n",
      "     30      501.5246      \u001b[32m247.4605\u001b[0m  0.0149\n",
      "     29      579.5096      \u001b[32m276.2367\u001b[0m  0.0079\n",
      "      2      \u001b[36m164.4286\u001b[0m      \u001b[32m146.7619\u001b[0m  0.2609\n",
      "     32      \u001b[36m623.0867\u001b[0m      \u001b[32m271.8492\u001b[0m  0.0077\n",
      "     31      \u001b[36m619.2023\u001b[0m      \u001b[32m270.7810\u001b[0m  0.0148\n",
      "     31      \u001b[36m497.9055\u001b[0m      \u001b[32m246.9930\u001b[0m  0.0103\n",
      "     31      642.2979      \u001b[32m282.3353\u001b[0m  0.0120\n",
      "     30      \u001b[36m576.0540\u001b[0m      \u001b[32m275.9924\u001b[0m  0.0099\n",
      "     32      498.4997      \u001b[32m246.4606\u001b[0m  0.0080\n",
      "     32      \u001b[36m634.6410\u001b[0m      \u001b[32m281.1859\u001b[0m  0.0085\n",
      "     32      \u001b[36m616.7690\u001b[0m      \u001b[32m269.6362\u001b[0m  0.0135\n",
      "     31      576.9597      \u001b[32m275.8092\u001b[0m  0.0098\n",
      "     33      \u001b[36m622.9291\u001b[0m      \u001b[32m270.5872\u001b[0m  0.0205\n",
      "     33      498.7393      \u001b[32m245.9973\u001b[0m  0.0102\n",
      "     33      \u001b[36m612.3668\u001b[0m      \u001b[32m268.8155\u001b[0m  0.0099\n",
      "     32      \u001b[36m575.5038\u001b[0m      \u001b[32m275.6141\u001b[0m  0.0094\n",
      "     33      \u001b[36m633.2335\u001b[0m      \u001b[32m279.9460\u001b[0m  0.0135\n",
      "     34      \u001b[36m497.7433\u001b[0m      \u001b[32m245.5740\u001b[0m  0.0086\n",
      "     34      623.0005      \u001b[32m269.5138\u001b[0m  0.0148\n",
      "     34      616.6339      \u001b[32m268.3751\u001b[0m  0.0085\n",
      "     33      \u001b[36m574.7501\u001b[0m      \u001b[32m275.3952\u001b[0m  0.0121\n",
      "     34      \u001b[36m629.3429\u001b[0m      \u001b[32m278.2939\u001b[0m  0.0123\n",
      "     35      \u001b[36m618.8553\u001b[0m      \u001b[32m268.7930\u001b[0m  0.0080\n",
      "     35      \u001b[36m609.3897\u001b[0m      \u001b[32m268.1780\u001b[0m  0.0092\n",
      "     35      \u001b[36m497.7259\u001b[0m      \u001b[32m245.2379\u001b[0m  0.0136\n",
      "     35      \u001b[36m627.0570\u001b[0m      \u001b[32m276.7711\u001b[0m  0.0091\n",
      "     34      577.2570      \u001b[32m275.0913\u001b[0m  0.0119\n",
      "     36      \u001b[36m496.8120\u001b[0m      \u001b[32m244.9595\u001b[0m  0.0101\n",
      "     36      609.9430      \u001b[32m268.0428\u001b[0m  0.0122\n",
      "     36      \u001b[36m617.9441\u001b[0m      \u001b[32m268.4823\u001b[0m  0.0132\n",
      "     36      630.5699      \u001b[32m275.7891\u001b[0m  0.0082\n",
      "     35      \u001b[36m572.5738\u001b[0m      \u001b[32m274.6049\u001b[0m  0.0087\n",
      "      4      \u001b[36m122.2969\u001b[0m      \u001b[32m122.0499\u001b[0m  0.1386\n",
      "      4      \u001b[36m142.4974\u001b[0m      \u001b[32m135.8116\u001b[0m  0.1312\n",
      "     37      610.8378      \u001b[32m267.8540\u001b[0m  0.0082\n",
      "     37      \u001b[36m495.3490\u001b[0m      \u001b[32m244.6773\u001b[0m  0.0083\n",
      "     37      627.2914      \u001b[32m275.1393\u001b[0m  0.0077\n",
      "     36      573.5002      \u001b[32m274.0090\u001b[0m  0.0080\n",
      "     37      \u001b[36m615.2054\u001b[0m      \u001b[32m268.4087\u001b[0m  0.0179\n",
      "     38      \u001b[36m605.0416\u001b[0m      \u001b[32m267.6635\u001b[0m  0.0092\n",
      "      4      \u001b[36m154.5665\u001b[0m      \u001b[32m136.6005\u001b[0m  0.1069\n",
      "      4      \u001b[36m152.3792\u001b[0m      \u001b[32m134.2797\u001b[0m  0.1239\n",
      "     38      \u001b[36m626.9442\u001b[0m      \u001b[32m274.8481\u001b[0m  0.0104\n",
      "     37      \u001b[36m569.4202\u001b[0m      \u001b[32m273.2143\u001b[0m  0.0099\n",
      "     38      497.1127      \u001b[32m244.4279\u001b[0m  0.0147\n",
      "     38      \u001b[36m614.6459\u001b[0m      268.4202  0.0079\n",
      "     39      607.6523      \u001b[32m267.4279\u001b[0m  0.0104\n",
      "     39      \u001b[36m623.6188\u001b[0m      \u001b[32m274.7243\u001b[0m  0.0105\n",
      "     39      498.0296      \u001b[32m244.2472\u001b[0m  0.0082\n",
      "     39      \u001b[36m614.6312\u001b[0m      \u001b[32m268.2858\u001b[0m  0.0102\n",
      "     38      571.7030      \u001b[32m272.1699\u001b[0m  0.0149\n",
      "     40      607.0198      \u001b[32m267.2388\u001b[0m  0.0090\n",
      "     40      \u001b[36m619.6533\u001b[0m      \u001b[32m274.6862\u001b[0m  0.0080\n",
      "     40      \u001b[36m494.9183\u001b[0m      \u001b[32m244.0534\u001b[0m  0.0120\n",
      "     40      614.7482      \u001b[32m268.0047\u001b[0m  0.0079\n",
      "     39      569.6772      \u001b[32m270.9614\u001b[0m  0.0077\n",
      "     41      605.1918      \u001b[32m267.0658\u001b[0m  0.0109\n",
      "     41      \u001b[36m616.7436\u001b[0m      274.6946  0.0098\n",
      "     41      496.7265      \u001b[32m243.8717\u001b[0m  0.0088\n",
      "     40      \u001b[36m568.2612\u001b[0m      \u001b[32m269.8043\u001b[0m  0.0084\n",
      "     41      \u001b[36m613.3022\u001b[0m      \u001b[32m267.6813\u001b[0m  0.0091\n",
      "      3      \u001b[36m161.3666\u001b[0m      \u001b[32m144.7482\u001b[0m  0.1179\n",
      "     42      \u001b[36m603.6781\u001b[0m      \u001b[32m266.8123\u001b[0m  0.0079\n",
      "     42      \u001b[36m616.6019\u001b[0m      \u001b[32m274.6250\u001b[0m  0.0089\n",
      "     42      495.0382      \u001b[32m243.6950\u001b[0m  0.0079\n",
      "     42      \u001b[36m609.8754\u001b[0m      \u001b[32m267.4360\u001b[0m  0.0077\n",
      "     41      \u001b[36m568.1927\u001b[0m      \u001b[32m268.7210\u001b[0m  0.0118\n",
      "     43      \u001b[36m602.6790\u001b[0m      \u001b[32m266.4235\u001b[0m  0.0081\n",
      "     43      619.1053      \u001b[32m274.5883\u001b[0m  0.0091\n",
      "     43      495.1454      \u001b[32m243.4790\u001b[0m  0.0086\n",
      "     43      \u001b[36m608.8845\u001b[0m      \u001b[32m267.2295\u001b[0m  0.0083\n",
      "     42      \u001b[36m565.4998\u001b[0m      \u001b[32m267.4832\u001b[0m  0.0079\n",
      "     44      \u001b[36m616.0263\u001b[0m      \u001b[32m274.5548\u001b[0m  0.0078\n",
      "     44      603.8047      \u001b[32m265.8666\u001b[0m  0.0138\n",
      "     44      495.8859      \u001b[32m243.2693\u001b[0m  0.0109\n",
      "     44      609.5125      \u001b[32m267.1286\u001b[0m  0.0118\n",
      "     43      \u001b[36m563.5542\u001b[0m      \u001b[32m266.0404\u001b[0m  0.0094\n",
      "     45      \u001b[36m615.9799\u001b[0m      \u001b[32m274.5179\u001b[0m  0.0101\n",
      "     45      603.5890      \u001b[32m265.1670\u001b[0m  0.0087\n",
      "     45      495.1965      \u001b[32m243.0780\u001b[0m  0.0084\n",
      "     45      609.1979      \u001b[32m267.0566\u001b[0m  0.0075\n",
      "     44      564.9585      \u001b[32m264.9711\u001b[0m  0.0094\n",
      "     46      \u001b[36m601.4535\u001b[0m      \u001b[32m264.5208\u001b[0m  0.0093\n",
      "     46      \u001b[36m606.5040\u001b[0m      \u001b[32m266.9900\u001b[0m  0.0100\n",
      "     46      617.2489      \u001b[32m274.4416\u001b[0m  0.0147\n",
      "     46      \u001b[36m493.1545\u001b[0m      \u001b[32m242.8495\u001b[0m  0.0128\n",
      "     45      \u001b[36m562.5685\u001b[0m      \u001b[32m264.5068\u001b[0m  0.0112\n",
      "     47      \u001b[36m598.0514\u001b[0m      \u001b[32m263.9773\u001b[0m  0.0121\n",
      "     47      \u001b[36m610.9897\u001b[0m      \u001b[32m274.3870\u001b[0m  0.0082\n",
      "     47      \u001b[36m603.0646\u001b[0m      \u001b[32m266.9466\u001b[0m  0.0127\n",
      "     46      \u001b[36m561.9719\u001b[0m      \u001b[32m264.1448\u001b[0m  0.0078\n",
      "     47      494.6373      \u001b[32m242.6050\u001b[0m  0.0150\n",
      "     48      614.8483      \u001b[32m274.3372\u001b[0m  0.0087\n",
      "     48      600.3875      \u001b[32m263.5768\u001b[0m  0.0096\n",
      "     47      \u001b[36m559.9835\u001b[0m      \u001b[32m263.6099\u001b[0m  0.0080\n",
      "      5      \u001b[36m121.5355\u001b[0m      \u001b[32m121.8976\u001b[0m  0.1232\n",
      "      5      \u001b[36m140.7575\u001b[0m      \u001b[32m133.8060\u001b[0m  0.1223\n",
      "     48      605.1990      \u001b[32m266.8141\u001b[0m  0.0120\n",
      "      5      \u001b[36m150.2691\u001b[0m      \u001b[32m132.9725\u001b[0m  0.1123\n",
      "      5      \u001b[36m152.1512\u001b[0m      \u001b[32m133.5368\u001b[0m  0.1135\n",
      "     49      600.5425      \u001b[32m263.1701\u001b[0m  0.0082\n",
      "     49      \u001b[36m610.6176\u001b[0m      \u001b[32m274.2604\u001b[0m  0.0091\n",
      "     48      \u001b[36m556.7160\u001b[0m      \u001b[32m263.0385\u001b[0m  0.0080\n",
      "     48      495.5999      \u001b[32m242.4035\u001b[0m  0.0164\n",
      "     49      \u001b[36m600.7919\u001b[0m      \u001b[32m266.5491\u001b[0m  0.0081\n",
      "     50      \u001b[36m608.2072\u001b[0m      \u001b[32m274.2025\u001b[0m  0.0082\n",
      "     50      \u001b[36m596.3046\u001b[0m      \u001b[32m262.9189\u001b[0m  0.0110\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     49      494.7386      \u001b[32m242.0867\u001b[0m  0.0127\n",
      "     49      559.6008      \u001b[32m262.6746\u001b[0m  0.0201\n",
      "     50      601.3817      \u001b[32m266.2369\u001b[0m  0.0154\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     50      494.9152      \u001b[32m241.8275\u001b[0m  0.0103\n",
      "     50      \u001b[36m553.7913\u001b[0m      \u001b[32m262.4987\u001b[0m  0.0115\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "      4      \u001b[36m159.8376\u001b[0m      \u001b[32m141.3956\u001b[0m  0.1181\n",
      "      6      \u001b[36m121.4500\u001b[0m      \u001b[32m121.7812\u001b[0m  0.1000\n",
      "      6      \u001b[36m149.6520\u001b[0m      \u001b[32m131.3102\u001b[0m  0.1008\n",
      "      6      \u001b[36m139.4239\u001b[0m      \u001b[32m130.6671\u001b[0m  0.1068\n",
      "      6      \u001b[36m148.7011\u001b[0m      \u001b[32m131.4779\u001b[0m  0.1163\n",
      "      5      \u001b[36m156.3918\u001b[0m      \u001b[32m137.3950\u001b[0m  0.1175\n",
      "      7      121.5958      121.7838  0.0988\n",
      "      7      \u001b[36m148.3249\u001b[0m      131.6848  0.0976\n",
      "      7      \u001b[36m138.0243\u001b[0m      \u001b[32m129.6214\u001b[0m  0.0978\n",
      "      7      \u001b[36m147.4773\u001b[0m      \u001b[32m129.9501\u001b[0m  0.0958\n",
      "      6      \u001b[36m155.1874\u001b[0m      \u001b[32m137.1910\u001b[0m  0.0953\n",
      "      8      \u001b[36m121.4209\u001b[0m      122.3659  0.0944\n",
      "      8      \u001b[36m147.7440\u001b[0m      \u001b[32m130.7966\u001b[0m  0.0965\n",
      "      8      \u001b[36m135.7652\u001b[0m      \u001b[32m128.5572\u001b[0m  0.0945\n",
      "      8      \u001b[36m146.4012\u001b[0m      \u001b[32m129.2511\u001b[0m  0.0957\n",
      "      7      \u001b[36m153.2350\u001b[0m      \u001b[32m136.4804\u001b[0m  0.0946\n",
      "      9      121.5910      122.0816  0.0932\n",
      "      9      \u001b[36m146.6488\u001b[0m      130.9674  0.0938\n",
      "      9      135.8033      \u001b[32m128.0497\u001b[0m  0.0929\n",
      "      9      \u001b[36m145.5610\u001b[0m      130.1155  0.0959\n",
      "      8      \u001b[36m151.3507\u001b[0m      137.2186  0.0967\n",
      "     10      \u001b[36m121.1293\u001b[0m      \u001b[32m120.3243\u001b[0m  0.1046\n",
      "     10      \u001b[36m146.1512\u001b[0m      131.0876  0.1057\n",
      "     10      \u001b[36m135.3575\u001b[0m      128.0644  0.1062\n",
      "     10      \u001b[36m144.8896\u001b[0m      129.9868  0.0999\n",
      "      9      \u001b[36m151.0383\u001b[0m      136.8272  0.1101\n",
      "     11      121.4750      \u001b[32m120.2145\u001b[0m  0.1048\n",
      "     11      146.3545      131.1327  0.1047\n",
      "     11      \u001b[36m144.4980\u001b[0m      129.9118  0.0972\n",
      "     11      \u001b[36m135.0044\u001b[0m      \u001b[32m127.9656\u001b[0m  0.1038\n",
      "     10      \u001b[36m150.5578\u001b[0m      136.7260  0.0900\n",
      "     12      \u001b[36m120.8018\u001b[0m      120.5264  0.0897\n",
      "     12      \u001b[36m146.1291\u001b[0m      131.1313  0.0902\n",
      "     12      \u001b[36m134.9409\u001b[0m      128.0695  0.0882\n",
      "     12      \u001b[36m144.3670\u001b[0m      129.8256  0.0895\n",
      "     11      150.5643      \u001b[32m136.3561\u001b[0m  0.0889\n",
      "     13      121.0396      120.3180  0.0902\n",
      "     13      \u001b[36m145.5621\u001b[0m      130.9304  0.0893\n",
      "     13      \u001b[36m134.7301\u001b[0m      128.1027  0.0890\n",
      "     13      \u001b[36m144.1857\u001b[0m      129.7828  0.0886\n",
      "     12      \u001b[36m149.7728\u001b[0m      \u001b[32m136.1066\u001b[0m  0.0905\n",
      "     14      \u001b[36m120.1231\u001b[0m      120.5438  0.0905\n",
      "     14      145.5773      130.8199  0.0896\n",
      "     14      134.7517      128.1055  0.0895\n",
      "     14      144.4827      129.7649  0.0893\n",
      "     13      150.2654      \u001b[32m136.0422\u001b[0m  0.0884\n",
      "     15      120.6267      121.4872  0.0883\n",
      "     15      135.0485      128.2309  0.0877\n",
      "     15      144.8262      129.8052  0.0880\n",
      "     15      146.2930      130.9589  0.0886\n",
      "     14      150.1889      136.1184  0.0874\n",
      "     16      121.4615      120.8915  0.0879\n",
      "     16      \u001b[36m143.8154\u001b[0m      \u001b[32m129.1348\u001b[0m  0.0866\n",
      "     16      145.9450      130.9393  0.0870\n",
      "     16      134.8070      128.2110  0.0880\n",
      "     15      150.2109      \u001b[32m135.9700\u001b[0m  0.0866\n",
      "     17      120.8468      120.4481  0.0869\n",
      "     17      144.2778      129.2140  0.0865\n",
      "     17      134.8117      \u001b[32m127.7062\u001b[0m  0.0862\n",
      "     17      146.3640      130.9345  0.0868\n",
      "     16      150.0009      136.3063  0.0876\n",
      "     18      120.7252      \u001b[32m119.3230\u001b[0m  0.0869\n",
      "     18      143.8364      129.5804  0.0865\n",
      "     18      146.4367      130.8802  0.0862\n",
      "     18      134.8551      127.9578  0.0867\n",
      "     17      \u001b[36m149.6345\u001b[0m      136.4543  0.0868\n",
      "     19      120.3638      \u001b[32m119.2932\u001b[0m  0.0865\n",
      "     19      143.9597      129.5087  0.0862\n",
      "     19      145.6146      131.0790  0.0865\n",
      "     19      134.9166      128.1145  0.0864\n",
      "     18      \u001b[36m149.5257\u001b[0m      136.0958  0.0846\n",
      "     20      120.3094      \u001b[32m119.2110\u001b[0m  0.0867\n",
      "     20      143.9345      129.2221  0.0851\n",
      "     20      146.0974      131.2189  0.0862     20      135.0387      128.2003  0.0858\n",
      "\n",
      "     19      149.5696      136.0930  0.0865\n",
      "     21      144.0526      129.3205  0.0862\n",
      "     21      146.5067      131.1520  0.0865\n",
      "     21      \u001b[36m119.3716\u001b[0m      \u001b[32m118.8803\u001b[0m  0.0884\n",
      "     21      134.9168      127.8960  0.0868\n",
      "     20      149.5657      136.0931  0.0858\n",
      "     22      144.2527      129.3194  0.0872\n",
      "     22      134.8294      127.8048  0.0854\n",
      "     22      146.1855      130.8688  0.0863\n",
      "     22      119.4010      \u001b[32m118.1439\u001b[0m  0.0862\n",
      "     21      149.6955      136.0928  0.0866\n",
      "     23      134.9015      127.8976  0.0865\n",
      "     23      143.8607      129.2807  0.0875\n",
      "     23      146.1213      131.0249  0.0867\n",
      "     23      119.3752      119.2386  0.0870\n",
      "     22      149.8010      136.2882  0.0858\n",
      "     24      134.8783      \u001b[32m127.7004\u001b[0m  0.0855\n",
      "     24      145.9602      131.2477  0.0859\n",
      "     24      144.0712      129.2138  0.0870\n",
      "     24      119.4594      118.9470  0.0864\n",
      "     23      \u001b[36m149.4398\u001b[0m      136.5309  0.0859\n",
      "     25      135.2031      \u001b[32m127.5205\u001b[0m  0.0869\n",
      "     25      146.2560      131.0412  0.0866\n",
      "     25      \u001b[36m118.9125\u001b[0m      118.4259  0.0858\n",
      "     25      144.1507      129.3722  0.0866\n",
      "     24      149.5572      136.0986  0.0861\n",
      "     26      134.8921      127.6117  0.0861\n",
      "     26      119.9521      118.5968  0.0868\n",
      "     26      146.3889      \u001b[32m130.6971\u001b[0m  0.0869\n",
      "     26      143.9717      129.4900  0.0870\n",
      "     25      149.6132      136.1750  0.0865\n",
      "     27      134.9364      127.9264  0.0864\n",
      "     27      119.1603      118.9945  0.0856\n",
      "     27      146.0599      131.0614  0.0868\n",
      "     27      144.2089      129.3416  0.0865\n",
      "     26      149.7814      136.1756  0.0862\n",
      "     28      134.9411      127.6713  0.0866\n",
      "     28      119.5094      118.6688  0.0874\n",
      "     28      146.1697      131.2103  0.0865\n",
      "     28      143.8493      129.5884  0.0875\n",
      "     27      \u001b[36m149.3678\u001b[0m      136.1751  0.0860\n",
      "     29      \u001b[36m134.6468\u001b[0m      127.7019  0.0863\n",
      "     29      119.1924      118.8738  0.0862\n",
      "     29      146.2190      130.9873  0.0862\n",
      "     29      144.0480      129.5027  0.0866\n",
      "     28      \u001b[36m149.2510\u001b[0m      136.1662  0.0857\n",
      "     30      134.9333      127.7365  0.0857\n",
      "     30      146.1650      131.0347  0.0873\n",
      "     30      143.9237      129.6503  0.0863\n",
      "     30      119.7585      119.3108  0.0886\n",
      "     29      149.5106      136.5238  0.0872\n",
      "     31      134.7982      128.1051  0.0872\n",
      "     31      143.8352      129.2980  0.0863\n",
      "     31      146.4196      130.9342  0.0872\n",
      "     31      119.5218      118.6931  0.0870\n",
      "     30      149.5344      136.2728  0.0859\n",
      "     32      134.7056      128.2622  0.0857\n",
      "     32      143.8412      129.5509  0.0861\n",
      "     32      146.5258      130.8656  0.0864\n",
      "     32      119.0718      \u001b[32m117.1110\u001b[0m  0.0865\n",
      "     31      149.4515      136.1761  0.0860\n",
      "     33      134.6976      128.0657  0.0871\n",
      "     33      143.8908      129.5101  0.0868\n",
      "     33      145.7020      \u001b[32m130.5760\u001b[0m  0.0860\n",
      "     33      119.4660      \u001b[32m117.0836\u001b[0m  0.0874\n",
      "     32      149.3920      136.1754  0.0859\n",
      "     34      \u001b[36m134.4954\u001b[0m      128.2918  0.0860\n",
      "     34      143.9699      129.5123  0.0852\n",
      "     34      146.3854      \u001b[32m130.4917\u001b[0m  0.0861\n",
      "     34      119.1006      117.1324  0.0866\n",
      "     33      149.3584      136.1515  0.0862\n",
      "     35      134.8353      128.2795  0.0855\n",
      "     35      \u001b[36m143.7510\u001b[0m      129.2329  0.0866\n",
      "     35      146.4752      130.6382  0.0854\n",
      "     35      118.9395      117.2743  0.0870\n",
      "     34      149.3101      136.5293  0.0855\n",
      "     36      134.7071      128.3186  0.0874\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 16.\n",
      "     36      146.3395      130.8258  0.0861\n",
      "     36      119.0005      117.4732  0.0875\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 15.\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     37      134.8911      128.7138  0.0854\n",
      "     37      145.8676      131.0717  0.0860\n",
      "     37      \u001b[36m118.5710\u001b[0m      118.3210  0.0851\n",
      "     38      134.5961      128.7288  0.0850\n",
      "     38      146.0741      131.1159  0.0847\n",
      "     38      119.1200      117.4484  0.0858\n",
      "     39      134.9463      128.7001  0.0844\n",
      "     39      145.6285      131.0448  0.0837\n",
      "     39      119.0834      \u001b[32m116.8744\u001b[0m  0.0838\n",
      "     40      134.5858      128.6138  0.0840\n",
      "     40      145.9707      130.9579  0.0847\n",
      "     40      119.7627      118.5244  0.0847\n",
      "     41      \u001b[36m134.4587\u001b[0m      128.1354  0.0851\n",
      "     41      146.2194      131.0750  0.0852\n",
      "     41      119.5738      118.2560  0.0858\n",
      "     42      134.6381      128.0180  0.0836\n",
      "     42      145.9142      131.2401  0.0838\n",
      "     42      119.4399      117.1465  0.0844\n",
      "     43      134.6537      128.2751  0.0844\n",
      "     43      145.8996      131.3022  0.0834\n",
      "     43      119.2240      117.2983  0.0845\n",
      "     44      134.7257      128.2833  0.0852\n",
      "     44      145.9702      131.4722  0.0843\n",
      "     44      118.7652      118.2109  0.0853\n",
      "     45      145.8784      131.4264  0.0845\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 25.\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     45      119.2450      \u001b[32m116.4131\u001b[0m  0.0862\n",
      "     46      146.0860      131.2442  0.0841\n",
      "     46      119.4716      \u001b[32m116.2344\u001b[0m  0.0837\n",
      "     47      145.6684      131.1195  0.0847\n",
      "     47      119.3518      116.9904  0.0829\n",
      "     48      145.9886      131.2615  0.0827\n",
      "     48      119.3486      117.1612  0.0843\n",
      "     49      146.0185      131.3709  0.0840\n",
      "     49      119.3694      117.5308  0.0832\n",
      "     50      145.9108      131.1695  0.0847\n",
      "Restoring best model from epoch 34.\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     50      119.2220      117.7830  0.0842\n",
      "Restoring best model from epoch 46.\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m162.4325\u001b[0m      \u001b[32m137.7559\u001b[0m  0.0853\n",
      "      2      \u001b[36m148.1144\u001b[0m      \u001b[32m133.8907\u001b[0m  0.0805\n",
      "      3      \u001b[36m145.7156\u001b[0m      \u001b[32m132.0866\u001b[0m  0.0763\n",
      "      4      \u001b[36m143.5929\u001b[0m      \u001b[32m129.0306\u001b[0m  0.0770\n",
      "      5      \u001b[36m141.9336\u001b[0m      129.0484  0.0701\n",
      "      6      \u001b[36m140.3853\u001b[0m      \u001b[32m128.6638\u001b[0m  0.0934\n",
      "      7      \u001b[36m139.6719\u001b[0m      \u001b[32m128.2885\u001b[0m  0.0792\n",
      "      8      \u001b[36m138.8795\u001b[0m      128.4614  0.0722\n",
      "      9      139.2489      \u001b[32m128.2589\u001b[0m  0.0762\n",
      "     10      139.1601      \u001b[32m127.9711\u001b[0m  0.0730\n",
      "     11      \u001b[36m138.4826\u001b[0m      \u001b[32m127.5490\u001b[0m  0.0732\n",
      "     12      \u001b[36m138.3815\u001b[0m      \u001b[32m127.0352\u001b[0m  0.0722\n",
      "     13      \u001b[36m138.1330\u001b[0m      127.2423  0.0727\n",
      "     14      138.6039      127.2632  0.0735\n",
      "     15      138.8596      127.4332  0.0754\n",
      "     16      138.1393      127.4317  0.0722\n",
      "     17      138.5960      127.4716  0.0726\n",
      "     18      138.6435      127.9636  0.0791\n",
      "     19      138.2924      127.2757  0.0716\n",
      "     20      138.4048      \u001b[32m126.7709\u001b[0m  0.0733\n",
      "     21      138.3374      127.9442  0.0748\n",
      "     22      \u001b[36m138.0284\u001b[0m      128.1040  0.0713\n",
      "     23      138.6307      127.8096  0.0737\n",
      "     24      138.7653      128.2091  0.0718\n",
      "     25      138.6944      127.5091  0.0744\n",
      "     26      138.5009      127.2877  0.0726\n",
      "     27      138.2574      127.2624  0.0716\n",
      "     28      138.3147      127.0816  0.0738\n",
      "     29      138.3169      \u001b[32m126.7602\u001b[0m  0.0776\n",
      "     30      138.6914      126.9995  0.0753\n",
      "     31      138.3826      127.5222  0.0738\n",
      "     32      138.2011      127.3046  0.0746\n",
      "     33      138.1183      127.5379  0.0755\n",
      "     34      138.2731      127.4161  0.0729\n",
      "     35      138.3613      127.4507  0.0739\n",
      "     36      138.1480      127.5375  0.0740\n",
      "     37      138.3876      126.9733  0.0742\n",
      "     38      138.4990      126.9280  0.0763\n",
      "     39      138.3636      127.0062  0.0782\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 20.\n",
      "durations 0.76666665 337.03333\n",
      "dtype X_train MKI67                float32\n",
      "EGFR                 float32\n",
      "PGR                  float32\n",
      "ERBB2                float32\n",
      "hormone_treatment    float32\n",
      "radiotherapy         float32\n",
      "chemotherapy         float32\n",
      "ER_positive          float32\n",
      "age                  float32\n",
      "dtype: object\n",
      "dtype y_train float64\n",
      "dtype X_test MKI67                float32\n",
      "EGFR                 float32\n",
      "PGR                  float32\n",
      "ERBB2                float32\n",
      "hormone_treatment    float32\n",
      "radiotherapy         float32\n",
      "chemotherapy         float32\n",
      "ER_positive          float32\n",
      "age                  float32\n",
      "dtype: object\n",
      "dtype y_test float32\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m299.4162\u001b[0m  0.0029\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2      299.4162  0.0030\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m347.7088\u001b[0m  0.0029\n",
      "      3      299.4162  0.0026\n",
      "      2      347.7088  0.0026\n",
      "      4      299.4162  0.0025\n",
      "      3      347.7088  0.0026\n",
      "      5      299.4162  0.0036\n",
      "      4      347.7088  0.0025\n",
      "      6      299.4162  0.0026\n",
      "      5      347.7088  0.0025\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m171.0279\u001b[0m      \u001b[32m145.8990\u001b[0m  0.0381\n",
      "      7      299.4162  0.0025\n",
      "      6      347.7088  0.0025\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      8      299.4162  0.0029\n",
      "      7      347.7088  0.0028\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m176.1131\u001b[0m      \u001b[32m157.4615\u001b[0m  0.0373\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      8      347.7088  0.0025\n",
      "      9      299.4162  0.0027\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      9      347.7088  0.0025\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m402.4613\u001b[0m  0.0036\n",
      "     10      299.4162  0.0026\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m180.7781\u001b[0m      \u001b[32m168.6426\u001b[0m  0.0390\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m431.0018\u001b[0m  0.0031\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m153.0993\u001b[0m      \u001b[32m118.7904\u001b[0m  0.0439\n",
      "     10      347.7088  0.0025\n",
      "     11      299.4162  0.0025\n",
      "      2      402.4613  0.0036\n",
      "      2      431.0018  0.0026\n",
      "     11      347.7088  0.0024\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m417.7777\u001b[0m  0.0067\n",
      "     12      299.4162  0.0036\n",
      "      3      402.4613  0.0026\n",
      "      3      431.0018  0.0026\n",
      "      2      417.7777  0.0030\n",
      "     13      299.4162  0.0026\n",
      "      4      402.4613  0.0025\n",
      "     12      347.7088  0.0040\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m186.9828\u001b[0m      \u001b[32m172.9898\u001b[0m  0.0433\n",
      "      4      431.0018  0.0032\n",
      "     14      299.4162  0.0025\n",
      "     13      347.7088  0.0033\n",
      "      5      431.0018  0.0025\n",
      "     15      299.4162  0.0026\n",
      "     14      347.7088  0.0026\n",
      "      6      431.0018  0.0025\n",
      "      3      417.7777  0.0072\n",
      "      5      402.4613  0.0066\n",
      "     16      299.4162  0.0025\n",
      "     15      347.7088  0.0031\n",
      "      4      417.7777  0.0028\n",
      "      6      402.4613  0.0027\n",
      "     17      299.4162  0.0025\n",
      "      7      431.0018  0.0042\n",
      "     16      347.7088  0.0025\n",
      "      5      417.7777  0.0026\n",
      "     18      299.4162  0.0025\n",
      "      8      431.0018  0.0030\n",
      "     17      347.7088  0.0025\n",
      "      6      417.7777  0.0025\n",
      "      9      431.0018  0.0026\n",
      "      7      402.4613  0.0070\n",
      "     18      347.7088  0.0025\n",
      "      7      417.7777  0.0025\n",
      "     19      299.4162  0.0054\n",
      "      8      417.7777  0.0025\n",
      "     19      347.7088  0.0033\n",
      "      8      402.4613  0.0036\n",
      "     20      299.4162  0.0026\n",
      "      2      \u001b[36m149.6775\u001b[0m      \u001b[32m130.9871\u001b[0m  0.0410\n",
      "     10      431.0018  0.0057\n",
      "      9      417.7777  0.0028\n",
      "     20      347.7088  0.0026\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 1.\n",
      "      9      402.4613  0.0029\n",
      "     11      431.0018  0.0028\n",
      "     10      417.7777  0.0025\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 1.\n",
      "     10      402.4613  0.0025\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     12      431.0018  0.0026\n",
      "     11      417.7777  0.0025\n",
      "     11      402.4613  0.0025\n",
      "     12      417.7777  0.0025\n",
      "      2      \u001b[36m152.3912\u001b[0m      \u001b[32m148.3593\u001b[0m  0.0456\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     13      431.0018  0.0034\n",
      "     12      402.4613  0.0025\n",
      "      2      \u001b[36m157.9668\u001b[0m      \u001b[32m157.8840\u001b[0m  0.0423\n",
      "     13      417.7777  0.0030\n",
      "     14      431.0018  0.0027\n",
      "     13      402.4613  0.0028\n",
      "     14      417.7777  0.0025\n",
      "     15      431.0018  0.0026\n",
      "     14      402.4613  0.0025\n",
      "      2      \u001b[36m161.8294\u001b[0m      \u001b[32m161.2137\u001b[0m  0.0373\n",
      "     15      417.7777  0.0025\n",
      "     16      431.0018  0.0026\n",
      "     15      402.4613  0.0025\n",
      "     16      402.4613  0.0025\n",
      "     17      431.0018  0.0033\n",
      "     16      417.7777  0.0043\n",
      "      2      \u001b[36m132.3886\u001b[0m      \u001b[32m113.5171\u001b[0m  0.0526\n",
      "     17      402.4613  0.0026\n",
      "     17      417.7777  0.0026\n",
      "     18      431.0018  0.0029\n",
      "     18      402.4613  0.0025\n",
      "     18      417.7777  0.0025\n",
      "     19      431.0018  0.0026\n",
      "     19      402.4613  0.0025\n",
      "     19      417.7777  0.0026\n",
      "     20      431.0018  0.0025\n",
      "     20      402.4613  0.0025\n",
      "     20      417.7777  0.0025\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 1.\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 1.\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 1.\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "      3      \u001b[36m142.8884\u001b[0m      \u001b[32m126.2516\u001b[0m  0.0417\n",
      "      3      \u001b[36m146.6072\u001b[0m      \u001b[32m144.8268\u001b[0m  0.0384\n",
      "      3      \u001b[36m149.9588\u001b[0m      \u001b[32m147.7023\u001b[0m  0.0380\n",
      "      3      \u001b[36m154.5136\u001b[0m      \u001b[32m152.2888\u001b[0m  0.0383\n",
      "      3      \u001b[36m128.4883\u001b[0m      \u001b[32m111.8162\u001b[0m  0.0382\n",
      "      4      \u001b[36m140.6079\u001b[0m      \u001b[32m125.1061\u001b[0m  0.0364\n",
      "      4      \u001b[36m144.1813\u001b[0m      \u001b[32m143.0155\u001b[0m  0.0367\n",
      "      4      \u001b[36m148.0969\u001b[0m      \u001b[32m145.8169\u001b[0m  0.0358\n",
      "      4      \u001b[36m151.0033\u001b[0m      \u001b[32m149.8302\u001b[0m  0.0392\n",
      "      4      \u001b[36m127.1304\u001b[0m      112.0052  0.0367\n",
      "      5      \u001b[36m139.5425\u001b[0m      \u001b[32m124.8963\u001b[0m  0.0362\n",
      "      5      \u001b[36m143.6694\u001b[0m      \u001b[32m141.0012\u001b[0m  0.0361\n",
      "      5      \u001b[36m147.3095\u001b[0m      145.9006  0.0356\n",
      "      5      \u001b[36m149.7101\u001b[0m      150.2928  0.0379\n",
      "      5      \u001b[36m125.8646\u001b[0m      \u001b[32m110.7586\u001b[0m  0.0365\n",
      "      6      \u001b[36m138.7042\u001b[0m      \u001b[32m123.7264\u001b[0m  0.0360\n",
      "      6      \u001b[36m142.3186\u001b[0m      141.6663  0.0361\n",
      "      6      \u001b[36m145.1234\u001b[0m      \u001b[32m145.5843\u001b[0m  0.0366\n",
      "      6      \u001b[36m147.8845\u001b[0m      150.8937  0.0375\n",
      "      6      \u001b[36m125.4661\u001b[0m      111.8218  0.0371\n",
      "      7      \u001b[36m137.8162\u001b[0m      \u001b[32m123.5544\u001b[0m  0.0368\n",
      "      7      142.9258      141.6470  0.0354\n",
      "      7      \u001b[36m144.9586\u001b[0m      146.4007  0.0364\n",
      "      7      148.7437      151.8820  0.0371\n",
      "      7      125.9042      111.9541  0.0365\n",
      "      8      \u001b[36m137.5413\u001b[0m      \u001b[32m123.4306\u001b[0m  0.0364\n",
      "      8      143.0625      142.5169  0.0364\n",
      "      8      \u001b[36m144.7729\u001b[0m      \u001b[32m144.3948\u001b[0m  0.0369\n",
      "      8      149.0612      150.5666  0.0373\n",
      "      8      126.2376      111.4086  0.0374\n",
      "      9      \u001b[36m137.4322\u001b[0m      \u001b[32m123.2194\u001b[0m  0.0362\n",
      "      9      142.5742      142.3846  0.0360\n",
      "      9      144.8930      \u001b[32m144.2801\u001b[0m  0.0361\n",
      "      9      148.7448      \u001b[32m148.6625\u001b[0m  0.0362\n",
      "      9      \u001b[36m125.2851\u001b[0m      111.1911  0.0357\n",
      "     10      138.0998      123.5352  0.0357\n",
      "     10      142.7818      142.4001  0.0356\n",
      "     10      145.0189      144.6692  0.0367\n",
      "     10      125.3223      111.4130  0.0358\n",
      "     10      148.3272      150.1672  0.0376\n",
      "     11      \u001b[36m137.1963\u001b[0m      123.3144  0.0366\n",
      "     11      \u001b[36m142.3139\u001b[0m      \u001b[32m140.3864\u001b[0m  0.0358\n",
      "     11      \u001b[36m144.5389\u001b[0m      144.6821  0.0368\n",
      "     11      125.4898      111.2191  0.0357\n",
      "     11      148.6147      150.2807  0.0367\n",
      "     12      137.5104      123.4163  0.0365\n",
      "     12      \u001b[36m141.8252\u001b[0m      \u001b[32m139.7584\u001b[0m  0.0362\n",
      "     12      145.3848      144.3193  0.0363\n",
      "     12      125.4510      \u001b[32m110.6204\u001b[0m  0.0355\n",
      "     12      148.4329      150.3269  0.0373\n",
      "     13      137.3308      \u001b[32m123.1567\u001b[0m  0.0363\n",
      "     13      \u001b[36m141.7689\u001b[0m      \u001b[32m139.7409\u001b[0m  0.0359\n",
      "     13      \u001b[36m144.3120\u001b[0m      144.6407  0.0365\n",
      "     13      125.3233      110.7021  0.0356\n",
      "     13      148.6717      148.7342  0.0373\n",
      "     14      137.2981      \u001b[32m123.0943\u001b[0m  0.0361\n",
      "     14      \u001b[36m141.5594\u001b[0m      139.7891  0.0360\n",
      "     14      \u001b[36m144.0139\u001b[0m      145.7428  0.0357\n",
      "     14      \u001b[36m124.8084\u001b[0m      \u001b[32m110.3816\u001b[0m  0.0359\n",
      "     14      148.9792      \u001b[32m147.5040\u001b[0m  0.0373\n",
      "     15      \u001b[36m141.4741\u001b[0m      139.7867  0.0352\n",
      "     15      137.3639      123.2114  0.0366\n",
      "     15      144.0691      145.7344  0.0361\n",
      "     15      124.8291      \u001b[32m110.2610\u001b[0m  0.0358\n",
      "     15      149.1202      148.2768  0.0370\n",
      "     16      141.8467      139.8022  0.0363\n",
      "     16      137.9656      123.1402  0.0361\n",
      "     16      144.3310      145.6760  0.0357\n",
      "     16      125.1216      110.3174  0.0356\n",
      "     16      148.9311      148.2206  0.0369\n",
      "     17      141.9329      \u001b[32m138.6966\u001b[0m  0.0356\n",
      "     17      137.6285      123.2439  0.0366\n",
      "     17      144.5203      144.6348  0.0365\n",
      "     17      125.1737      \u001b[32m110.2265\u001b[0m  0.0357\n",
      "     17      149.0978      147.8833  0.0366\n",
      "     18      142.0714      \u001b[32m138.6149\u001b[0m  0.0357\n",
      "     18      137.6315      123.2749  0.0365\n",
      "     18      144.7496      \u001b[32m144.0063\u001b[0m  0.0360\n",
      "     18      125.0898      110.6223  0.0344\n",
      "     18      149.8377      147.7248  0.0370\n",
      "     19      \u001b[36m137.0454\u001b[0m      123.8088  0.0355\n",
      "     19      142.1527      138.8139  0.0361\n",
      "     19      144.7339      \u001b[32m143.6569\u001b[0m  0.0365\n",
      "     19      125.2208      110.4662  0.0357\n",
      "     19      148.5355      \u001b[32m147.1430\u001b[0m  0.0368\n",
      "     20      141.9381      139.7854  0.0352\n",
      "     20      137.5679      123.2710  0.0361\n",
      "     20      144.8961      144.0544  0.0364\n",
      "     20      125.0352      110.4672  0.0359\n",
      "     20      148.5990      147.3051  0.0369\n",
      "     21      137.1419      123.7591  0.0353\n",
      "     21      \u001b[36m141.4538\u001b[0m      140.6661  0.0363\n",
      "     21      144.8438      144.0312  0.0362\n",
      "     21      125.0419      110.4675  0.0360\n",
      "     21      148.6364      147.3356  0.0361\n",
      "     22      137.2847      123.8308  0.0359\n",
      "     22      141.9179      140.2963  0.0362\n",
      "     22      144.7653      144.0370  0.0369\n",
      "     22      \u001b[36m124.3632\u001b[0m      110.5375  0.0356\n",
      "     22      148.2922      147.2459  0.0370\n",
      "     23      137.7584      123.9748  0.0361\n",
      "     23      142.4676      142.5064  0.0360\n",
      "     23      144.5524      144.4656  0.0355\n",
      "     23      124.6884      110.5247  0.0356\n",
      "     23      148.8086      147.4734  0.0365\n",
      "     24      142.6074      143.8902  0.0355\n",
      "     24      137.3919      123.9891  0.0361\n",
      "     24      144.5445      144.5431  0.0357\n",
      "     24      124.5144      110.4523  0.0356\n",
      "     24      148.4615      148.0724  0.0376\n",
      "     25      137.3728      123.8999  0.0356\n",
      "     25      143.0321      143.1714  0.0362\n",
      "     25      144.5938      144.8937  0.0360\n",
      "     25      124.4270      110.4254  0.0361\n",
      "     25      148.2491      148.1422  0.0375\n",
      "     26      142.6125      139.3276  0.0357\n",
      "     26      137.1026      123.8712  0.0363\n",
      "     26      144.6834      144.0905  0.0364\n",
      "     26      124.9209      110.4159  0.0353\n",
      "     26      148.0582      147.6907  0.0373\n",
      "     27      142.4012      138.9466  0.0359\n",
      "     27      137.1447      123.8067  0.0360\n",
      "     27      144.9876      144.1769  0.0360\n",
      "     27      124.6312      110.3832  0.0359\n",
      "     27      148.1701      147.2449  0.0364\n",
      "     28      141.6324      138.9437  0.0354\n",
      "     28      \u001b[36m136.9897\u001b[0m      123.6606  0.0357\n",
      "     28      145.0100      144.0766  0.0359\n",
      "     28      124.6100      110.3709  0.0358\n",
      "     29      141.8277      138.6888  0.0355\n",
      "     28      148.4309      147.3358  0.0380\n",
      "     29      137.0436      124.2837  0.0363\n",
      "     29      145.2213      144.1046  0.0365\n",
      "     29      124.8018      110.3226  0.0371\n",
      "     30      141.7753      138.6600  0.0371\n",
      "     30      \u001b[36m136.9678\u001b[0m      124.4041  0.0365\n",
      "     29      148.3700      147.7158  0.0381\n",
      "     30      145.2849      144.0166  0.0364\n",
      "     30      \u001b[36m124.3439\u001b[0m      110.2865  0.0361\n",
      "     31      141.6468      139.0535  0.0367\n",
      "     31      137.6789      124.3800  0.0369\n",
      "     30      148.2829      148.3619  0.0371\n",
      "     31      144.9879      144.0976  0.0371\n",
      "     31      125.2941      110.2945  0.0358\n",
      "     32      141.8678      139.0557  0.0358\n",
      "     32      137.1254      124.0065  0.0350\n",
      "     31      148.6411      148.9441  0.0374\n",
      "     32      145.5378      144.1223  0.0362\n",
      "     32      125.0308      110.3083  0.0358\n",
      "     33      141.9284      138.9591  0.0364\n",
      "     33      137.2500      124.1092  0.0364\n",
      "     32      148.0069      148.7278  0.0371\n",
      "     33      145.2692      144.0824  0.0367\n",
      "     33      124.7072      110.2987  0.0366\n",
      "     34      \u001b[36m141.2952\u001b[0m      138.6369  0.0353\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 14.\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     33      \u001b[36m147.8007\u001b[0m      149.6338  0.0369\n",
      "     34      144.2249      144.0391  0.0359\n",
      "     34      124.8214      110.2943  0.0357\n",
      "     35      \u001b[36m141.2849\u001b[0m      138.6284  0.0357\n",
      "     34      148.0533      150.5861  0.0369\n",
      "     35      144.3234      144.0269  0.0361\n",
      "     35      124.5728      110.5053  0.0362\n",
      "     36      141.4289      \u001b[32m138.5813\u001b[0m  0.0365\n",
      "     35      148.3245      150.1079  0.0363\n",
      "     36      144.2582      144.0386  0.0357\n",
      "     36      124.9075      110.7423  0.0360\n",
      "     37      141.4103      \u001b[32m138.5087\u001b[0m  0.0358\n",
      "     36      148.4745      150.1465  0.0367\n",
      "     37      144.2925      144.0373  0.0356\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 17.\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     38      \u001b[36m141.2600\u001b[0m      \u001b[32m138.4991\u001b[0m  0.0366\n",
      "     37      148.1866      150.1297  0.0365\n",
      "     38      144.1788      144.0362  0.0365\n",
      "     39      141.3794      138.5286  0.0347\n",
      "     38      148.1408      150.1208  0.0357\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 19.\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     40      141.4827      138.6775  0.0345\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 19.\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     41      \u001b[36m141.2105\u001b[0m      138.6798  0.0341\n",
      "     42      141.5120      138.6221  0.0352\n",
      "     43      141.3105      138.9898  0.0374\n",
      "     44      141.4433      139.2211  0.0366\n",
      "     45      \u001b[36m141.1692\u001b[0m      139.0494  0.0365\n",
      "     46      141.5308      139.1514  0.0337\n",
      "     47      141.2226      138.8882  0.0334\n",
      "     48      141.4317      138.7727  0.0346\n",
      "     49      141.2362      138.7716  0.0335\n",
      "     50      141.5906      139.3490  0.0341\n",
      "Restoring best model from epoch 37.\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m170.4151\u001b[0m      \u001b[32m134.4920\u001b[0m  0.0490\n",
      "      2      \u001b[36m148.6332\u001b[0m      \u001b[32m129.8209\u001b[0m  0.0498\n",
      "      3      \u001b[36m141.9083\u001b[0m      \u001b[32m128.6267\u001b[0m  0.0472\n",
      "      4      \u001b[36m141.6659\u001b[0m      \u001b[32m127.9050\u001b[0m  0.0621\n",
      "      5      \u001b[36m140.9702\u001b[0m      \u001b[32m126.6655\u001b[0m  0.0460\n",
      "      6      \u001b[36m140.2553\u001b[0m      \u001b[32m124.3067\u001b[0m  0.0482\n",
      "      7      \u001b[36m139.7027\u001b[0m      \u001b[32m123.4942\u001b[0m  0.0463\n",
      "      8      \u001b[36m139.6257\u001b[0m      123.5068  0.0528\n",
      "      9      \u001b[36m139.5997\u001b[0m      123.7654  0.0465\n",
      "     10      139.6971      \u001b[32m123.2769\u001b[0m  0.0490\n",
      "     11      140.0275      \u001b[32m123.2269\u001b[0m  0.0492\n",
      "     12      140.1290      \u001b[32m123.1722\u001b[0m  0.0468\n",
      "     13      139.7340      123.3409  0.0452\n",
      "     14      \u001b[36m139.5735\u001b[0m      123.4861  0.0491\n",
      "     15      139.6925      123.4834  0.0467\n",
      "     16      \u001b[36m139.5266\u001b[0m      123.4803  0.0512\n",
      "     17      139.7293      123.4776  0.0509\n",
      "     18      139.6659      123.3952  0.0470\n",
      "     19      139.7299      123.6366  0.0481\n",
      "     20      139.7562      123.7240  0.0464\n",
      "     21      \u001b[36m139.3029\u001b[0m      123.8026  0.0479\n",
      "     22      139.9516      123.5481  0.0741\n",
      "     23      \u001b[36m139.1145\u001b[0m      123.5707  0.0519\n",
      "     24      139.4490      123.5663  0.0474\n",
      "     25      139.4269      123.6328  0.0506\n",
      "     26      139.5313      123.5521  0.0480\n",
      "     27      139.2388      124.0956  0.0454\n",
      "     28      139.7760      124.2882  0.0479\n",
      "     29      139.5417      123.4787  0.0455\n",
      "     30      139.2990      123.7256  0.0480\n",
      "     31      139.7074      123.7241  0.0469\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 12.\n",
      "durations 0.1 330.36667\n",
      "dtype X_train MKI67                float32\n",
      "EGFR                 float32\n",
      "PGR                  float32\n",
      "ERBB2                float32\n",
      "hormone_treatment    float32\n",
      "radiotherapy         float32\n",
      "chemotherapy         float32\n",
      "ER_positive          float32\n",
      "age                  float32\n",
      "dtype: object\n",
      "dtype y_train float64\n",
      "dtype X_test MKI67                float32\n",
      "EGFR                 float32\n",
      "PGR                  float32\n",
      "ERBB2                float32\n",
      "hormone_treatment    float32\n",
      "radiotherapy         float32\n",
      "chemotherapy         float32\n",
      "ER_positive          float32\n",
      "age                  float32\n",
      "dtype: object\n",
      "dtype y_test float32\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m305.9221\u001b[0m  0.0029\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2      305.9221  0.0028\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m349.7782\u001b[0m  0.0030\n",
      "      3      305.9221  0.0025\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2      349.7782  0.0032\n",
      "      4      305.9221  0.0025\n",
      "      3      349.7782  0.0027\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m404.5421\u001b[0m  0.0030\n",
      "      5      305.9221  0.0038\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m148.4878\u001b[0m      \u001b[32m122.6575\u001b[0m  0.0387\n",
      "      4      349.7782  0.0025\n",
      "      2      404.5421  0.0028\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      5      349.7782  0.0025\n",
      "      3      404.5421  0.0026\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      6      349.7782  0.0025\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m408.9187\u001b[0m  0.0031\n",
      "      4      404.5421  0.0025\n",
      "      6      305.9221  0.0083\n",
      "      7      349.7782  0.0025\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m168.3999\u001b[0m      \u001b[32m138.0229\u001b[0m  0.0434\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m452.8376\u001b[0m  0.0029\n",
      "      2      408.9187  0.0034\n",
      "      5      404.5421  0.0029\n",
      "      7      305.9221  0.0028\n",
      "      8      349.7782  0.0025\n",
      "      2      452.8376  0.0026\n",
      "      3      408.9187  0.0027\n",
      "      6      404.5421  0.0026\n",
      "      8      305.9221  0.0025\n",
      "      9      349.7782  0.0025\n",
      "      3      452.8376  0.0025\n",
      "      4      408.9187  0.0025\n",
      "      7      404.5421  0.0025\n",
      "      9      305.9221  0.0025\n",
      "     10      349.7782  0.0027\n",
      "      4      452.8376  0.0025\n",
      "      5      408.9187  0.0025\n",
      "      8      404.5421  0.0026\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m187.9530\u001b[0m      \u001b[32m182.1789\u001b[0m  0.0387\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m183.2902\u001b[0m      \u001b[32m162.6822\u001b[0m  0.0436\n",
      "     10      305.9221  0.0025\n",
      "     11      349.7782  0.0025\n",
      "      5      452.8376  0.0025\n",
      "      9      404.5421  0.0025\n",
      "      6      408.9187  0.0025\n",
      "     11      305.9221  0.0025\n",
      "     12      349.7782  0.0025\n",
      "      6      452.8376  0.0025\n",
      "     10      404.5421  0.0025\n",
      "     13      349.7782  0.0025\n",
      "     12      305.9221  0.0029\n",
      "      7      408.9187  0.0037\n",
      "      7      452.8376  0.0025\n",
      "     11      404.5421  0.0025\n",
      "     13      305.9221  0.0027\n",
      "      8      408.9187  0.0026\n",
      "      8      452.8376  0.0025\n",
      "     12      404.5421  0.0025\n",
      "     14      305.9221  0.0025\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m177.1013\u001b[0m      \u001b[32m159.4973\u001b[0m  0.0603\n",
      "      9      452.8376  0.0025\n",
      "     14      349.7782  0.0057\n",
      "      9      408.9187  0.0025\n",
      "     13      404.5421  0.0025\n",
      "     15      305.9221  0.0025\n",
      "     14      404.5421  0.0025\n",
      "     15      349.7782  0.0041\n",
      "     16      305.9221  0.0025\n",
      "     10      452.8376  0.0048\n",
      "     10      408.9187  0.0055\n",
      "     15      404.5421  0.0025\n",
      "     16      349.7782  0.0028\n",
      "     17      305.9221  0.0025\n",
      "     11      452.8376  0.0028\n",
      "     11      408.9187  0.0026\n",
      "     17      349.7782  0.0025\n",
      "     18      305.9221  0.0025\n",
      "     12      452.8376  0.0026\n",
      "     19      305.9221  0.0025\n",
      "     18      349.7782  0.0025\n",
      "     12      408.9187  0.0039\n",
      "     16      404.5421  0.0081\n",
      "     20      305.9221  0.0025\n",
      "     19      349.7782  0.0025\n",
      "     13      408.9187  0.0027\n",
      "     13      452.8376  0.0051\n",
      "     17      404.5421  0.0031\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 1.\n",
      "     20      349.7782  0.0025\n",
      "     14      408.9187  0.0025\n",
      "     14      452.8376  0.0026\n",
      "     18      404.5421  0.0025\n",
      "     15      408.9187  0.0026\n",
      "     15      452.8376  0.0025\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "      2      \u001b[36m129.8835\u001b[0m      \u001b[32m116.4430\u001b[0m  0.0529\n",
      "     19      404.5421  0.0025\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "     16      408.9187  0.0025\n",
      "Restoring best model from epoch 1.\n",
      "     16      452.8376  0.0027\n",
      "     20      404.5421  0.0025\n",
      "     17      408.9187  0.0025\n",
      "     17      452.8376  0.0025\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "      2      \u001b[36m161.3880\u001b[0m      \u001b[32m165.9693\u001b[0m  0.0392\n",
      "     18      408.9187  0.0025\n",
      "     18      452.8376  0.0025\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 1.\n",
      "     19      408.9187  0.0027\n",
      "     19      452.8376  0.0025\n",
      "      2      \u001b[36m155.5213\u001b[0m      \u001b[32m148.4680\u001b[0m  0.0410\n",
      "      2      \u001b[36m146.4331\u001b[0m      \u001b[32m130.6172\u001b[0m  0.0542\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     20      408.9187  0.0025\n",
      "     20      452.8376  0.0029\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 1.\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "      2      \u001b[36m152.0707\u001b[0m      \u001b[32m150.6730\u001b[0m  0.0377\n",
      "Restoring best model from epoch 1.\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "      3      \u001b[36m125.9686\u001b[0m      \u001b[32m114.2671\u001b[0m  0.0403\n",
      "      3      \u001b[36m154.6308\u001b[0m      \u001b[32m161.4510\u001b[0m  0.0420\n",
      "      3      \u001b[36m148.8674\u001b[0m      \u001b[32m144.5169\u001b[0m  0.0415\n",
      "      3      \u001b[36m145.9538\u001b[0m      \u001b[32m148.0509\u001b[0m  0.0376\n",
      "      3      \u001b[36m139.7103\u001b[0m      \u001b[32m126.1425\u001b[0m  0.0441\n",
      "      4      \u001b[36m125.1026\u001b[0m      \u001b[32m113.8357\u001b[0m  0.0408\n",
      "      4      \u001b[36m150.9795\u001b[0m      \u001b[32m157.2592\u001b[0m  0.0410\n",
      "      4      \u001b[36m143.7446\u001b[0m      \u001b[32m144.5871\u001b[0m  0.0376\n",
      "      4      \u001b[36m146.1983\u001b[0m      144.9807  0.0391\n",
      "      4      \u001b[36m137.1950\u001b[0m      126.2846  0.0384\n",
      "      5      \u001b[36m125.0223\u001b[0m      \u001b[32m113.1603\u001b[0m  0.0374\n",
      "      5      \u001b[36m149.0648\u001b[0m      \u001b[32m157.1052\u001b[0m  0.0371\n",
      "      5      \u001b[36m143.4784\u001b[0m      \u001b[32m143.4407\u001b[0m  0.0360\n",
      "      5      \u001b[36m136.9463\u001b[0m      127.1826  0.0364\n",
      "      5      \u001b[36m144.6677\u001b[0m      \u001b[32m143.8681\u001b[0m  0.0365\n",
      "      6      \u001b[36m123.4966\u001b[0m      \u001b[32m111.7893\u001b[0m  0.0365\n",
      "      6      \u001b[36m148.5817\u001b[0m      \u001b[32m156.8445\u001b[0m  0.0364\n",
      "      6      \u001b[36m136.3890\u001b[0m      127.9894  0.0371\n",
      "      6      \u001b[36m142.3793\u001b[0m      \u001b[32m142.9610\u001b[0m  0.0377\n",
      "      6      144.8892      \u001b[32m142.0787\u001b[0m  0.0375\n",
      "      7      \u001b[36m123.1620\u001b[0m      \u001b[32m111.7890\u001b[0m  0.0367\n",
      "      7      149.1880      157.6909  0.0361\n",
      "      7      137.1934      127.5307  0.0360\n",
      "      7      \u001b[36m142.0122\u001b[0m      143.0572  0.0373\n",
      "      7      \u001b[36m144.5763\u001b[0m      142.1903  0.0368\n",
      "      8      \u001b[36m122.5960\u001b[0m      112.3206  0.0362\n",
      "      8      149.6706      157.6350  0.0360\n",
      "      8      137.3706      127.2652  0.0354\n",
      "      8      144.5801      \u001b[32m141.8548\u001b[0m  0.0358\n",
      "      8      142.2976      \u001b[32m142.4588\u001b[0m  0.0372\n",
      "      9      \u001b[36m122.4993\u001b[0m      112.5636  0.0361\n",
      "      9      \u001b[36m148.1259\u001b[0m      \u001b[32m155.6512\u001b[0m  0.0360\n",
      "      9      136.8790      126.8076  0.0357\n",
      "      9      144.5950      142.3014  0.0360\n",
      "      9      \u001b[36m141.5272\u001b[0m      142.4682  0.0377\n",
      "     10      \u001b[36m122.3575\u001b[0m      112.1996  0.0366\n",
      "     10      \u001b[36m147.4363\u001b[0m      156.0284  0.0356\n",
      "     10      136.9226      126.6106  0.0365\n",
      "     10      144.9076      142.3640  0.0364\n",
      "     10      142.3719      142.6169  0.0368\n",
      "     11      123.0689      112.2367  0.0368\n",
      "     11      \u001b[36m147.0234\u001b[0m      \u001b[32m154.8057\u001b[0m  0.0365\n",
      "     11      136.5628      127.0165  0.0362\n",
      "     11      \u001b[36m144.5405\u001b[0m      142.4734  0.0380\n",
      "     11      142.0581      142.5393  0.0381\n",
      "     12      122.9465      112.6923  0.0366\n",
      "     12      147.1185      155.2183  0.0359\n",
      "     12      136.4173      127.3801  0.0358\n",
      "     12      144.5867      142.3793  0.0370\n",
      "     12      142.4253      142.4634  0.0382\n",
      "     13      122.7205      112.6922  0.0357\n",
      "     13      \u001b[36m136.1093\u001b[0m      127.0370  0.0358\n",
      "     13      147.0774      155.0180  0.0371\n",
      "     13      144.9514      142.6052  0.0359\n",
      "     13      141.9379      \u001b[32m141.9982\u001b[0m  0.0372\n",
      "     14      122.9767      112.8404  0.0357\n",
      "     14      137.4956      127.2372  0.0357\n",
      "     14      147.3894      157.7524  0.0357\n",
      "     14      144.8811      142.2398  0.0367\n",
      "     14      141.7135      \u001b[32m141.7163\u001b[0m  0.0374\n",
      "     15      122.9930      112.9093  0.0362\n",
      "     15      136.8641      126.2536  0.0356\n",
      "     15      147.7934      157.8222  0.0360\n",
      "     15      145.0393      142.4404  0.0359\n",
      "     15      141.8327      141.9836  0.0372\n",
      "     16      122.5107      112.4470  0.0362\n",
      "     16      137.0276      126.6887  0.0363\n",
      "     16      147.1784      158.1730  0.0367\n",
      "     16      145.0062      142.9807  0.0365\n",
      "     16      142.0413      \u001b[32m141.3629\u001b[0m  0.0377\n",
      "     17      122.9133      112.5190  0.0367\n",
      "     17      137.2263      126.5669  0.0368\n",
      "     17      147.5187      156.7119  0.0365\n",
      "     17      145.6216      143.2105  0.0366\n",
      "     17      \u001b[36m141.4772\u001b[0m      141.6381  0.0371\n",
      "     18      122.7356      112.5660  0.0366\n",
      "     18      136.7883      126.3566  0.0351\n",
      "     18      \u001b[36m146.7767\u001b[0m      155.3991  0.0358\n",
      "     18      144.9046      143.1579  0.0360\n",
      "     18      \u001b[36m141.4647\u001b[0m      141.5878  0.0372\n",
      "     19      123.1111      112.4565  0.0365\n",
      "     19      \u001b[36m135.8803\u001b[0m      \u001b[32m125.9169\u001b[0m  0.0357\n",
      "     19      148.0081      155.7251  0.0371\n",
      "     19      144.8903      143.2960  0.0371\n",
      "     19      141.7961      141.5475  0.0375\n",
      "     20      122.4538      112.4341  0.0362\n",
      "     20      135.8885      126.0999  0.0363\n",
      "     20      147.0889      \u001b[32m154.3793\u001b[0m  0.0359\n",
      "     20      145.0045      142.9137  0.0355\n",
      "     20      \u001b[36m141.4523\u001b[0m      141.8041  0.0377\n",
      "     21      122.7085      112.2659  0.0361\n",
      "     21      136.1550      \u001b[32m125.8011\u001b[0m  0.0363\n",
      "     21      146.9661      155.3011  0.0364\n",
      "     21      145.3955      143.0575  0.0364\n",
      "     21      141.4731      \u001b[32m141.3026\u001b[0m  0.0375\n",
      "     22      122.5240      112.2661  0.0368\n",
      "     22      \u001b[36m135.8746\u001b[0m      125.9687  0.0367\n",
      "     22      147.0138      155.5176  0.0364\n",
      "     22      144.9401      142.5438  0.0360\n",
      "     22      \u001b[36m141.3444\u001b[0m      141.5520  0.0365\n",
      "     23      122.8567      112.2657  0.0363\n",
      "     23      \u001b[36m135.7580\u001b[0m      125.9506  0.0362\n",
      "     23      147.0036      155.6479  0.0366\n",
      "     23      \u001b[36m144.3730\u001b[0m      142.5054  0.0359\n",
      "     23      \u001b[36m141.2066\u001b[0m      141.4941  0.0371\n",
      "     24      122.7657      112.3625  0.0362\n",
      "     24      \u001b[36m135.6032\u001b[0m      125.8206  0.0362\n",
      "     24      \u001b[36m146.6602\u001b[0m      155.4884  0.0355\n",
      "     24      144.3992      142.5967  0.0368\n",
      "     24      141.6503      142.0189  0.0373\n",
      "     25      122.5721      112.5941  0.0361\n",
      "     25      136.3472      125.9230  0.0362\n",
      "     25      146.8011      156.1113  0.0360\n",
      "     25      144.6778      142.4349  0.0363\n",
      "     25      141.4751      141.9033  0.0363\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 6.\n",
      "     26      \u001b[36m135.4801\u001b[0m      126.0082  0.0347\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     26      146.7856      156.4807  0.0359\n",
      "     26      144.4157      142.4291  0.0348\n",
      "     26      141.3690      141.9014  0.0386\n",
      "     27      136.3635      \u001b[32m125.7609\u001b[0m  0.0375\n",
      "     27      147.4826      156.4346  0.0375\n",
      "     27      \u001b[36m144.2091\u001b[0m      142.5019  0.0373\n",
      "     27      141.2073      141.7374  0.0369\n",
      "     28      136.7434      \u001b[32m125.5485\u001b[0m  0.0372\n",
      "     28      147.1581      156.5815  0.0365\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 8.\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     28      141.5308      141.5307  0.0370\n",
      "     29      136.1430      125.5776  0.0359\n",
      "     29      146.9191      156.5751  0.0363\n",
      "     29      141.4238      142.3515  0.0367\n",
      "     30      136.4280      125.6982  0.0355\n",
      "     30      147.0979      156.5795  0.0358\n",
      "     30      141.5749      141.8148  0.0359\n",
      "     31      147.2752      156.7129  0.0350\n",
      "     31      135.8697      \u001b[32m125.3001\u001b[0m  0.0362\n",
      "     31      141.9043      142.6536  0.0354\n",
      "     32      147.0352      157.0972  0.0361\n",
      "     32      135.9176      125.8536  0.0358\n",
      "     32      \u001b[36m141.1228\u001b[0m      142.2288  0.0365\n",
      "     33      146.7072      156.9580  0.0354\n",
      "     33      135.6958      126.2508  0.0351\n",
      "     33      141.2633      142.6843  0.0357\n",
      "     34      146.8541      156.7518  0.0355\n",
      "     34      135.6948      126.2054  0.0355\n",
      "     34      141.2716      141.7660  0.0378\n",
      "     35      136.2242      125.9304  0.0360\n",
      "     35      146.7276      156.2431  0.0368\n",
      "     35      141.3624      141.8746  0.0356\n",
      "     36      135.9492      126.3221  0.0352\n",
      "     36      146.7633      155.8325  0.0355\n",
      "     36      141.3234      141.8595  0.0362\n",
      "     37      136.0930      126.4120  0.0344\n",
      "     37      146.8202      155.8321  0.0350\n",
      "     37      141.5233      141.8396  0.0365\n",
      "     38      135.6636      126.5598  0.0344\n",
      "     38      146.8824      155.6723  0.0345\n",
      "     39      135.6510      125.7286  0.0358\n",
      "     38      141.7249      141.4739  0.0364\n",
      "     39      146.9985      156.1112  0.0353\n",
      "     40      135.8785      125.4583  0.0355\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 20.\n",
      "     39      141.6009      142.4500  0.0366\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     41      136.1141      125.4067  0.0350\n",
      "     40      141.8383      142.4540  0.0360\n",
      "     42      136.4088      126.0652  0.0349\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 21.\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     43      135.8084      126.1344  0.0344\n",
      "     44      135.8672      126.1042  0.0343\n",
      "     45      135.7730      126.1787  0.0343\n",
      "     46      135.9234      126.2396  0.0335\n",
      "     47      136.0329      126.2985  0.0337\n",
      "     48      136.4009      126.3636  0.0345\n",
      "     49      136.2196      126.3922  0.0342\n",
      "     50      136.4632      126.5837  0.0338\n",
      "Restoring best model from epoch 31.\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m170.9189\u001b[0m      \u001b[32m133.5154\u001b[0m  0.0513\n",
      "      2      \u001b[36m151.5585\u001b[0m      \u001b[32m129.5982\u001b[0m  0.0472\n",
      "      3      \u001b[36m146.8167\u001b[0m      \u001b[32m127.4790\u001b[0m  0.0482\n",
      "      4      \u001b[36m144.6238\u001b[0m      \u001b[32m124.5747\u001b[0m  0.0490\n",
      "      5      \u001b[36m142.5268\u001b[0m      \u001b[32m124.2551\u001b[0m  0.0480\n",
      "      6      \u001b[36m141.1448\u001b[0m      \u001b[32m123.0852\u001b[0m  0.0490\n",
      "      7      \u001b[36m140.5376\u001b[0m      123.3353  0.0469\n",
      "      8      \u001b[36m140.1430\u001b[0m      123.4303  0.0461\n",
      "      9      140.2095      123.4794  0.0481\n",
      "     10      140.3112      123.2810  0.0467\n",
      "     11      140.4875      123.1724  0.0492\n",
      "     12      140.6538      \u001b[32m122.8530\u001b[0m  0.0503\n",
      "     13      \u001b[36m139.9221\u001b[0m      122.9941  0.0584\n",
      "     14      139.9755      123.2849  0.0496\n",
      "     15      140.3506      123.7677  0.0496\n",
      "     16      140.9350      123.9912  0.0511\n",
      "     17      140.5508      123.8893  0.0467\n",
      "     18      140.4786      123.9044  0.0479\n",
      "     19      140.2499      123.7871  0.0486\n",
      "     20      140.5995      123.3155  0.0464\n",
      "     21      140.7200      123.2345  0.0475\n",
      "     22      140.2424      123.7171  0.0519\n",
      "     23      139.9748      124.0500  0.0457\n",
      "     24      140.0937      124.2093  0.0473\n",
      "     25      140.1452      124.0302  0.0449\n",
      "     26      140.1317      124.2711  0.0460\n",
      "     27      \u001b[36m139.6078\u001b[0m      124.2949  0.0465\n",
      "     28      139.6645      124.2936  0.0466\n",
      "     29      140.0872      124.0059  0.0497\n",
      "     30      139.6176      124.1560  0.0487\n",
      "     31      139.6371      123.5043  0.0479\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 12.\n",
      "durations 1.4333333 301.23334\n",
      "dtype X_train MKI67                float32\n",
      "EGFR                 float32\n",
      "PGR                  float32\n",
      "ERBB2                float32\n",
      "hormone_treatment    float32\n",
      "radiotherapy         float32\n",
      "chemotherapy         float32\n",
      "ER_positive          float32\n",
      "age                  float32\n",
      "dtype: object\n",
      "dtype y_train float64\n",
      "dtype X_test MKI67                float32\n",
      "EGFR                 float32\n",
      "PGR                  float32\n",
      "ERBB2                float32\n",
      "hormone_treatment    float32\n",
      "radiotherapy         float32\n",
      "chemotherapy         float32\n",
      "ER_positive          float32\n",
      "age                  float32\n",
      "dtype: object\n",
      "dtype y_test float32\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m321.3101\u001b[0m  0.0030\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m354.2971\u001b[0m  0.0030\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      2      321.3101  0.0054\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2      354.2971  0.0027\n",
      "      3      321.3101  0.0027\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m367.4011\u001b[0m  0.0029\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      4      321.3101  0.0027\n",
      "      3      354.2971  0.0031\n",
      "      2      367.4011  0.0026\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m144.0254\u001b[0m      \u001b[32m128.5034\u001b[0m  0.0379\n",
      "      4      354.2971  0.0025\n",
      "      5      321.3101  0.0026\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m384.9167\u001b[0m  0.0031\n",
      "      3      367.4011  0.0037\n",
      "      5      354.2971  0.0025\n",
      "      6      321.3101  0.0026\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2      384.9167  0.0028\n",
      "      6      354.2971  0.0025\n",
      "      4      367.4011  0.0027\n",
      "      7      321.3101  0.0025\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m165.5282\u001b[0m      \u001b[32m146.4522\u001b[0m  0.0425\n",
      "      3      384.9167  0.0026\n",
      "      7      354.2971  0.0026\n",
      "      5      367.4011  0.0026\n",
      "      4      384.9167  0.0025\n",
      "      8      354.2971  0.0025\n",
      "      8      321.3101  0.0052\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m411.2508\u001b[0m  0.0056\n",
      "      6      367.4011  0.0042\n",
      "      9      354.2971  0.0025\n",
      "      9      321.3101  0.0028\n",
      "      2      411.2508  0.0026\n",
      "      5      384.9167  0.0058\n",
      "     10      321.3101  0.0026\n",
      "      3      411.2508  0.0025\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m193.5916\u001b[0m      \u001b[32m170.8832\u001b[0m  0.0374\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m178.4782\u001b[0m      \u001b[32m151.5694\u001b[0m  0.0492\n",
      "     10      354.2971  0.0037\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m184.7493\u001b[0m      \u001b[32m159.7187\u001b[0m  0.0446\n",
      "      7      367.4011  0.0058\n",
      "      6      384.9167  0.0027\n",
      "     11      321.3101  0.0036\n",
      "      4      411.2508  0.0032\n",
      "      8      367.4011  0.0027\n",
      "      7      384.9167  0.0025\n",
      "     12      321.3101  0.0026\n",
      "      9      367.4011  0.0025\n",
      "      5      411.2508  0.0034\n",
      "      8      384.9167  0.0025\n",
      "     11      354.2971  0.0076\n",
      "     13      321.3101  0.0026\n",
      "     10      367.4011  0.0025\n",
      "      6      411.2508  0.0026\n",
      "     11      367.4011  0.0025\n",
      "      9      384.9167  0.0049\n",
      "     12      354.2971  0.0043\n",
      "      7      411.2508  0.0038\n",
      "     14      321.3101  0.0060\n",
      "     10      384.9167  0.0027\n",
      "     13      354.2971  0.0029\n",
      "     12      367.4011  0.0046\n",
      "     15      321.3101  0.0026\n",
      "     11      384.9167  0.0025\n",
      "      8      411.2508  0.0036\n",
      "     14      354.2971  0.0026\n",
      "     13      367.4011  0.0026\n",
      "     16      321.3101  0.0025\n",
      "     12      384.9167  0.0025\n",
      "      9      411.2508  0.0026\n",
      "     15      354.2971  0.0030\n",
      "     14      367.4011  0.0025\n",
      "     13      384.9167  0.0025\n",
      "     10      411.2508  0.0025\n",
      "     16      354.2971  0.0025\n",
      "     15      367.4011  0.0025\n",
      "     17      321.3101  0.0052\n",
      "     14      384.9167  0.0028\n",
      "     11      411.2508  0.0037\n",
      "     17      354.2971  0.0025\n",
      "     16      367.4011  0.0025\n",
      "     18      321.3101  0.0028\n",
      "     18      354.2971  0.0025\n",
      "     12      411.2508  0.0027\n",
      "     19      321.3101  0.0026\n",
      "      2      \u001b[36m125.9902\u001b[0m      \u001b[32m125.3338\u001b[0m  0.0505\n",
      "     15      384.9167  0.0058\n",
      "     13      411.2508  0.0025\n",
      "      2      \u001b[36m147.2102\u001b[0m      \u001b[32m134.3018\u001b[0m  0.0436\n",
      "     19      354.2971  0.0044\n",
      "     16      384.9167  0.0026\n",
      "     14      411.2508  0.0025\n",
      "     17      367.4011  0.0078\n",
      "     20      354.2971  0.0026\n",
      "     20      321.3101  0.0064\n",
      "     15      411.2508  0.0025\n",
      "     17      384.9167  0.0030\n",
      "      2      \u001b[36m164.8478\u001b[0m      \u001b[32m152.6060\u001b[0m  0.0393\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 1.\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "     18      367.4011  0.0042\n",
      "Restoring best model from epoch 1.     16      411.2508  0.0025\n",
      "\n",
      "     18      384.9167  0.0026\n",
      "     19      367.4011  0.0026\n",
      "     17      411.2508  0.0027\n",
      "     19      384.9167  0.0025\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     20      367.4011  0.0025\n",
      "     18      411.2508  0.0025\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     20      384.9167  0.0029\n",
      "      2      \u001b[36m158.4635\u001b[0m      \u001b[32m140.3153\u001b[0m  0.0463\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 1.\n",
      "     19      411.2508  0.0025\n",
      "      2      \u001b[36m158.5922\u001b[0m      \u001b[32m143.5460\u001b[0m  0.0473\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 1.\n",
      "     20      411.2508  0.0028\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 1.\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "      3      \u001b[36m141.8587\u001b[0m      \u001b[32m130.7288\u001b[0m  0.0386\n",
      "      3      \u001b[36m122.7637\u001b[0m      \u001b[32m122.1388\u001b[0m  0.0405\n",
      "      3      \u001b[36m157.0069\u001b[0m      \u001b[32m149.3229\u001b[0m  0.0378\n",
      "      3      \u001b[36m151.8860\u001b[0m      \u001b[32m136.2296\u001b[0m  0.0387\n",
      "      3      \u001b[36m152.9125\u001b[0m      \u001b[32m142.8109\u001b[0m  0.0392\n",
      "      4      \u001b[36m139.0227\u001b[0m      \u001b[32m129.0732\u001b[0m  0.0376\n",
      "      4      \u001b[36m121.7416\u001b[0m      \u001b[32m118.7048\u001b[0m  0.0374\n",
      "      4      \u001b[36m154.9822\u001b[0m      \u001b[32m147.2143\u001b[0m  0.0376\n",
      "      4      \u001b[36m147.6285\u001b[0m      \u001b[32m133.9602\u001b[0m  0.0369\n",
      "      4      \u001b[36m149.4732\u001b[0m      \u001b[32m138.5261\u001b[0m  0.0395\n",
      "      5      \u001b[36m121.2793\u001b[0m      118.7444  0.0362\n",
      "      5      \u001b[36m137.2948\u001b[0m      \u001b[32m128.8373\u001b[0m  0.0365\n",
      "      5      \u001b[36m153.2285\u001b[0m      \u001b[32m145.2257\u001b[0m  0.0385\n",
      "      5      \u001b[36m145.3530\u001b[0m      135.5556  0.0361\n",
      "      5      \u001b[36m148.3133\u001b[0m      \u001b[32m137.3116\u001b[0m  0.0394\n",
      "      6      \u001b[36m121.0187\u001b[0m      \u001b[32m118.0913\u001b[0m  0.0359\n",
      "      6      \u001b[36m136.5072\u001b[0m      \u001b[32m128.7862\u001b[0m  0.0365\n",
      "      6      \u001b[36m152.6984\u001b[0m      \u001b[32m144.8542\u001b[0m  0.0368\n",
      "      6      \u001b[36m144.5789\u001b[0m      134.5409  0.0367\n",
      "      6      \u001b[36m146.0614\u001b[0m      \u001b[32m136.4923\u001b[0m  0.0368\n",
      "      7      \u001b[36m120.6523\u001b[0m      \u001b[32m117.6021\u001b[0m  0.0369\n",
      "      7      136.6342      129.4254  0.0374\n",
      "      7      \u001b[36m151.0542\u001b[0m      144.9572  0.0359\n",
      "      7      \u001b[36m143.9082\u001b[0m      \u001b[32m132.8892\u001b[0m  0.0368\n",
      "      7      \u001b[36m145.3868\u001b[0m      136.6690  0.0367\n",
      "      8      \u001b[36m135.6397\u001b[0m      129.2704  0.0361\n",
      "      8      \u001b[36m120.1806\u001b[0m      \u001b[32m116.6603\u001b[0m  0.0361\n",
      "      8      151.5735      144.9821  0.0360\n",
      "      8      \u001b[36m143.2250\u001b[0m      133.7984  0.0366\n",
      "      8      \u001b[36m145.3605\u001b[0m      \u001b[32m135.8877\u001b[0m  0.0375\n",
      "      9      136.4700      129.7821  0.0363\n",
      "      9      120.7167      \u001b[32m116.3696\u001b[0m  0.0364\n",
      "      9      \u001b[36m150.9748\u001b[0m      \u001b[32m143.2260\u001b[0m  0.0357\n",
      "      9      144.2098      134.5267  0.0366\n",
      "      9      \u001b[36m144.8921\u001b[0m      \u001b[32m135.5956\u001b[0m  0.0371\n",
      "     10      \u001b[36m135.5606\u001b[0m      130.3178  0.0359\n",
      "     10      \u001b[36m120.1208\u001b[0m      116.6995  0.0369\n",
      "     10      151.2831      \u001b[32m142.6870\u001b[0m  0.0370\n",
      "     10      143.2776      \u001b[32m131.5787\u001b[0m  0.0364\n",
      "     10      144.9540      \u001b[32m135.5310\u001b[0m  0.0369\n",
      "     11      \u001b[36m134.9040\u001b[0m      130.3884  0.0369\n",
      "     11      120.5564      116.7009  0.0367\n",
      "     11      151.2852      \u001b[32m142.3080\u001b[0m  0.0365\n",
      "     11      \u001b[36m142.0943\u001b[0m      131.7051  0.0359\n",
      "     11      145.0815      135.5734  0.0381\n",
      "     12      135.0957      130.5219  0.0366\n",
      "     12      \u001b[36m120.0770\u001b[0m      117.6728  0.0369\n",
      "     12      \u001b[36m150.5884\u001b[0m      142.3709  0.0361\n",
      "     12      142.1535      \u001b[32m131.2276\u001b[0m  0.0374\n",
      "     12      \u001b[36m144.6349\u001b[0m      135.7413  0.0372\n",
      "     13      \u001b[36m134.6701\u001b[0m      130.5432  0.0360\n",
      "     13      120.6310      118.0211  0.0364\n",
      "     13      150.6919      142.8545  0.0365\n",
      "     13      \u001b[36m141.7443\u001b[0m      131.6564  0.0359\n",
      "     13      \u001b[36m144.3349\u001b[0m      136.1697  0.0373\n",
      "     14      \u001b[36m134.5387\u001b[0m      129.2370  0.0365\n",
      "     14      120.5362      118.1736  0.0366\n",
      "     14      150.7938      142.9219  0.0363\n",
      "     14      142.0550      131.4564  0.0359\n",
      "     14      144.8235      135.9612  0.0365\n",
      "     15      \u001b[36m134.4010\u001b[0m      129.2607  0.0361\n",
      "     15      120.3584      116.8598  0.0371\n",
      "     15      150.7172      142.8876  0.0373\n",
      "     15      \u001b[36m141.7220\u001b[0m      131.2490  0.0374\n",
      "     15      145.1931      135.8127  0.0378\n",
      "     16      \u001b[36m134.3792\u001b[0m      128.8225  0.0366\n",
      "     16      120.3738      117.2537  0.0370\n",
      "     16      \u001b[36m150.1370\u001b[0m      142.8776  0.0364\n",
      "     16      142.2608      \u001b[32m131.2104\u001b[0m  0.0367\n",
      "     16      144.9207      135.8367  0.0378\n",
      "     17      134.4194      129.1792  0.0364\n",
      "     17      120.3911      117.3705  0.0366\n",
      "     17      150.2278      142.8750  0.0369\n",
      "     17      141.8761      131.6896  0.0364\n",
      "     17      144.7376      135.5424  0.0372\n",
      "     18      \u001b[36m134.2645\u001b[0m      \u001b[32m128.7229\u001b[0m  0.0364\n",
      "     18      120.4195      119.5345  0.0355\n",
      "     18      150.5150      143.3982  0.0370\n",
      "     18      \u001b[36m141.5471\u001b[0m      132.0212  0.0365\n",
      "     18      144.4726      135.8544  0.0372\n",
      "     19      134.3461      \u001b[32m127.8498\u001b[0m  0.0356\n",
      "     19      120.3055      120.3504  0.0353\n",
      "     19      150.5694      143.8552  0.0361\n",
      "     19      142.0020      132.3380  0.0362\n",
      "     19      144.7182      \u001b[32m135.1875\u001b[0m  0.0375\n",
      "     20      135.2047      128.0139  0.0357\n",
      "     20      120.9831      119.0762  0.0368\n",
      "     20      151.1296      142.8809  0.0361\n",
      "     20      141.7197      132.7488  0.0365\n",
      "     21      134.6845      128.0439  0.0364\n",
      "     21      120.1859      119.9762  0.0361\n",
      "     20      144.5927      135.6081  0.0396\n",
      "     21      151.0717      142.4113  0.0376\n",
      "     21      141.6593      132.4288  0.0365\n",
      "     22      135.0172      127.9812  0.0366\n",
      "     22      120.7556      119.6636  0.0364\n",
      "     21      144.7703      135.6272  0.0376\n",
      "     22      150.5659      142.4190  0.0372\n",
      "     22      \u001b[36m141.1787\u001b[0m      131.8366  0.0367\n",
      "     23      134.8180      \u001b[32m127.8027\u001b[0m  0.0363\n",
      "     23      120.5561      118.4239  0.0364\n",
      "     22      145.1103      135.6720  0.0375\n",
      "     23      150.2167      142.3271  0.0371\n",
      "     23      141.7628      131.8025  0.0358\n",
      "     24      134.8137      127.8385  0.0363\n",
      "     24      120.6381      118.4870  0.0366\n",
      "     23      145.0353      135.6871  0.0374\n",
      "     24      150.6895      \u001b[32m142.1046\u001b[0m  0.0357\n",
      "     24      141.5504      131.6369  0.0362\n",
      "     25      134.4886      \u001b[32m127.7623\u001b[0m  0.0367\n",
      "     25      120.4114      117.9859  0.0360\n",
      "     24      144.5350      135.6097  0.0368\n",
      "     25      150.8523      142.1076  0.0360\n",
      "     25      141.8980      131.6680  0.0355\n",
      "     26      120.4147      118.2614  0.0362\n",
      "     26      \u001b[36m134.0672\u001b[0m      \u001b[32m127.7591\u001b[0m  0.0363\n",
      "     25      144.9438      135.7046  0.0368\n",
      "     26      150.9352      \u001b[32m142.0200\u001b[0m  0.0361\n",
      "     26      141.6834      132.7532  0.0353\n",
      "     27      120.3944      117.7294  0.0365\n",
      "     27      134.5755      128.1262  0.0366\n",
      "     26      144.5150      135.6556  0.0371\n",
      "     27      150.3836      142.0678  0.0368\n",
      "     27      142.0768      132.9669  0.0368\n",
      "     28      \u001b[36m119.9914\u001b[0m      117.8731  0.0359\n",
      "     28      134.4904      129.2116  0.0369\n",
      "     27      144.5171      135.3088  0.0375\n",
      "     28      150.5697      142.1690  0.0363\n",
      "     28      141.8114      131.5882  0.0361\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 9.\n",
      "     29      134.3827      129.2302  0.0367\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     28      144.4354      \u001b[32m135.1520\u001b[0m  0.0376\n",
      "     29      150.4039      142.1801  0.0358\n",
      "     29      141.6306      131.9466  0.0359\n",
      "     30      134.3448      129.1735  0.0366\n",
      "     30      150.2440      142.3457  0.0348\n",
      "     29      144.4340      \u001b[32m135.0559\u001b[0m  0.0368\n",
      "     30      141.2035      131.8252  0.0365\n",
      "     31      134.1870      128.8383  0.0359\n",
      "     31      150.7066      142.1836  0.0375\n",
      "     30      144.6786      135.1949  0.0380\n",
      "     31      \u001b[36m141.1175\u001b[0m      131.8347  0.0365\n",
      "     32      134.1927      128.8429  0.0372\n",
      "     32      150.3192      142.6743  0.0366\n",
      "     32      141.3243      131.4916  0.0375\n",
      "     31      \u001b[36m144.0961\u001b[0m      135.3779  0.0382\n",
      "     33      134.2975      128.7957  0.0362\n",
      "     33      150.6813      142.7327  0.0362\n",
      "     33      141.2701      131.3014  0.0359\n",
      "     32      144.5069      135.1829  0.0363\n",
      "     34      134.5963      128.6916  0.0360\n",
      "     34      \u001b[36m150.0363\u001b[0m      142.8195  0.0362\n",
      "     34      141.5253      132.3902  0.0356\n",
      "     33      144.7403      135.1392  0.0370\n",
      "     35      134.1986      128.7883  0.0364\n",
      "     35      150.2118      143.4586  0.0364\n",
      "     35      141.4973      132.4787  0.0355\n",
      "     34      144.8281      135.6573  0.0364\n",
      "     36      134.2575      128.5049  0.0361\n",
      "     36      150.7141      142.5915  0.0363\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 16.\n",
      "     35      144.2039      135.5624  0.0376\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     37      134.4931      129.0716  0.0356\n",
      "     37      150.3364      142.6001  0.0355\n",
      "     36      144.3998      135.7503  0.0372\n",
      "     38      134.0912      128.9352  0.0350\n",
      "     38      150.2202      142.7818  0.0355\n",
      "     37      144.9700      135.9262  0.0362\n",
      "     39      134.6701      128.8719  0.0351\n",
      "     39      150.6804      142.7779  0.0353\n",
      "     38      144.8002      135.4177  0.0366\n",
      "     40      134.2138      128.4408  0.0357\n",
      "     40      150.7040      142.7673  0.0364\n",
      "     39      144.5567      135.4902  0.0360\n",
      "     41      134.3612      128.3601  0.0347\n",
      "     41      150.0683      142.8478  0.0352\n",
      "     40      145.0065      135.8282  0.0366\n",
      "     42      \u001b[36m133.9805\u001b[0m      128.7409  0.0365\n",
      "     42      150.5669      143.1924  0.0358\n",
      "     41      144.9635      136.1916  0.0366\n",
      "     43      134.1726      128.4112  0.0361\n",
      "     43      150.4976      143.7109  0.0348\n",
      "     42      145.3480      135.6606  0.0357\n",
      "     44      134.4854      127.9526  0.0353\n",
      "     44      150.2191      143.3774  0.0360\n",
      "     43      145.1942      135.5711  0.0361\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 25.\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     45      150.1213      142.9356  0.0353\n",
      "     44      144.9918      135.7542  0.0368\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 26.\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     45      144.8466      135.7587  0.0353\n",
      "     46      144.5129      135.8488  0.0354\n",
      "     47      144.4006      135.9177  0.0352\n",
      "     48      144.5278      135.8468  0.0347\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 29.\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m169.3706\u001b[0m      \u001b[32m142.5087\u001b[0m  0.0483\n",
      "      2      \u001b[36m147.8621\u001b[0m      \u001b[32m132.9513\u001b[0m  0.0485\n",
      "      3      \u001b[36m142.5244\u001b[0m      \u001b[32m128.5468\u001b[0m  0.0460\n",
      "      4      \u001b[36m141.6908\u001b[0m      \u001b[32m127.3777\u001b[0m  0.0581\n",
      "      5      \u001b[36m140.4503\u001b[0m      127.5049  0.0493\n",
      "      6      \u001b[36m139.9588\u001b[0m      \u001b[32m125.0975\u001b[0m  0.0463\n",
      "      7      \u001b[36m139.8889\u001b[0m      125.4436  0.0488\n",
      "      8      140.9013      \u001b[32m124.3958\u001b[0m  0.0460\n",
      "      9      \u001b[36m139.7114\u001b[0m      124.4994  0.0508\n",
      "     10      \u001b[36m139.1482\u001b[0m      \u001b[32m124.2223\u001b[0m  0.0494\n",
      "     11      \u001b[36m138.8326\u001b[0m      125.8668  0.0787\n",
      "     12      139.1028      124.5153  0.0478\n",
      "     13      138.9926      \u001b[32m124.2141\u001b[0m  0.0482\n",
      "     14      138.8582      \u001b[32m124.0335\u001b[0m  0.0468\n",
      "     15      138.8847      \u001b[32m123.8351\u001b[0m  0.0482\n",
      "     16      138.8334      \u001b[32m123.3532\u001b[0m  0.0474\n",
      "     17      \u001b[36m138.5991\u001b[0m      123.4456  0.0459\n",
      "     18      138.8925      123.6643  0.0475\n",
      "     19      138.9744      123.6764  0.0471\n",
      "     20      \u001b[36m138.5574\u001b[0m      123.6729  0.0515\n",
      "     21      138.7633      123.6743  0.0475\n",
      "     22      138.7008      123.7430  0.0461\n",
      "     23      138.9265      123.9255  0.0482\n",
      "     24      138.7168      123.9213  0.0475\n",
      "     25      138.7205      123.8881  0.0494\n",
      "     26      138.7044      123.8822  0.0588\n",
      "     27      138.6568      123.7395  0.0477\n",
      "     28      \u001b[36m138.5104\u001b[0m      123.7096  0.0460\n",
      "     29      \u001b[36m138.4813\u001b[0m      123.9453  0.0473\n",
      "     30      138.6444      124.5244  0.0466\n",
      "     31      139.0667      124.9980  0.0447\n",
      "     32      138.5568      124.2466  0.0490\n",
      "     33      138.7416      123.8093  0.0461\n",
      "     34      138.8816      124.5660  0.0482\n",
      "     35      138.8230      124.7764  0.0482\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 16.\n",
      "durations 1.2333333 355.2\n",
      "dtype X_train MKI67                float32\n",
      "EGFR                 float32\n",
      "PGR                  float32\n",
      "ERBB2                float32\n",
      "hormone_treatment    float32\n",
      "radiotherapy         float32\n",
      "chemotherapy         float32\n",
      "ER_positive          float32\n",
      "age                  float32\n",
      "dtype: object\n",
      "dtype y_train float64\n",
      "dtype X_test MKI67                float32\n",
      "EGFR                 float32\n",
      "PGR                  float32\n",
      "ERBB2                float32\n",
      "hormone_treatment    float32\n",
      "radiotherapy         float32\n",
      "chemotherapy         float32\n",
      "ER_positive          float32\n",
      "age                  float32\n",
      "dtype: object\n",
      "dtype y_test float32\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m325.5805\u001b[0m  0.0030\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2      325.5805  0.0027\n",
      "      3      325.5805  0.0025\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m358.5262\u001b[0m  0.0029\n",
      "      4      325.5805  0.0025\n",
      "      2      358.5262  0.0026\n",
      "      3      358.5262  0.0025\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m145.8143\u001b[0m      \u001b[32m127.9777\u001b[0m  0.0385\n",
      "      5      325.5805  0.0025\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      4      358.5262  0.0026\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      6      325.5805  0.0044\n",
      "      5      358.5262  0.0027\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m393.6083\u001b[0m  0.0041\n",
      "      7      325.5805  0.0025\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m166.5064\u001b[0m      \u001b[32m148.2607\u001b[0m  0.0416\n",
      "      6      358.5262  0.0025\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m177.3079\u001b[0m      \u001b[32m160.0403\u001b[0m  0.0385\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m380.4451\u001b[0m  0.0029\n",
      "      7      358.5262  0.0025\n",
      "      8      325.5805  0.0041\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      2      393.6083  0.0049\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      8      358.5262  0.0025\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m188.1053\u001b[0m      \u001b[32m154.6953\u001b[0m  0.0386\n",
      "      3      393.6083  0.0026\n",
      "      9      358.5262  0.0025\n",
      "      2      380.4451  0.0059\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m400.2012\u001b[0m  0.0031\n",
      "      4      393.6083  0.0025\n",
      "      3      380.4451  0.0026\n",
      "      2      400.2012  0.0026\n",
      "      9      325.5805  0.0077\n",
      "      5      393.6083  0.0026\n",
      "      4      380.4451  0.0027\n",
      "     10      358.5262  0.0054\n",
      "      3      400.2012  0.0025\n",
      "     10      325.5805  0.0028\n",
      "      6      393.6083  0.0025\n",
      "      5      380.4451  0.0025\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m191.3620\u001b[0m      \u001b[32m160.2140\u001b[0m  0.0428\n",
      "      4      400.2012  0.0025\n",
      "     11      325.5805  0.0026\n",
      "      7      393.6083  0.0025\n",
      "      6      380.4451  0.0025\n",
      "     12      325.5805  0.0025\n",
      "      8      393.6083  0.0025\n",
      "     11      358.5262  0.0069\n",
      "      5      400.2012  0.0056\n",
      "      7      380.4451  0.0039\n",
      "     13      325.5805  0.0031\n",
      "     12      358.5262  0.0028\n",
      "      9      393.6083  0.0047\n",
      "      6      400.2012  0.0028\n",
      "      8      380.4451  0.0026\n",
      "     13      358.5262  0.0025\n",
      "     14      325.5805  0.0026\n",
      "      9      380.4451  0.0025\n",
      "      7      400.2012  0.0027\n",
      "     10      393.6083  0.0028\n",
      "     14      358.5262  0.0025\n",
      "     15      325.5805  0.0025\n",
      "     10      380.4451  0.0025\n",
      "      8      400.2012  0.0025\n",
      "     11      393.6083  0.0026\n",
      "      2      \u001b[36m128.6421\u001b[0m      \u001b[32m120.5449\u001b[0m  0.0395\n",
      "     15      358.5262  0.0035\n",
      "      9      400.2012  0.0025\n",
      "     11      380.4451  0.0027\n",
      "     16      325.5805  0.0038\n",
      "     16      358.5262  0.0027\n",
      "     12      393.6083  0.0050\n",
      "     12      380.4451  0.0025\n",
      "     10      400.2012  0.0032\n",
      "     17      358.5262  0.0025\n",
      "     13      380.4451  0.0025\n",
      "     13      393.6083  0.0027\n",
      "     11      400.2012  0.0026\n",
      "      2      \u001b[36m156.9209\u001b[0m      \u001b[32m147.2864\u001b[0m  0.0391\n",
      "     18      358.5262  0.0028\n",
      "     14      393.6083  0.0025\n",
      "     12      400.2012  0.0025\n",
      "     17      325.5805  0.0080\n",
      "     14      380.4451  0.0036\n",
      "     19      358.5262  0.0025\n",
      "     13      400.2012  0.0025\n",
      "     18      325.5805  0.0028\n",
      "     20      358.5262  0.0025\n",
      "     15      393.6083  0.0055\n",
      "     14      400.2012  0.0025\n",
      "     19      325.5805  0.0026\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 1.\n",
      "     16      393.6083  0.0026\n",
      "     15      400.2012  0.0025\n",
      "     15      380.4451  0.0069\n",
      "     20      325.5805  0.0025\n",
      "      2      \u001b[36m145.9691\u001b[0m      \u001b[32m136.1272\u001b[0m  0.0500\n",
      "      2      \u001b[36m159.0911\u001b[0m      \u001b[32m138.1213\u001b[0m  0.0456\n",
      "     16      400.2012  0.0025\n",
      "     17      393.6083  0.0025\n",
      "     16      380.4451  0.0027\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 1.\n",
      "     18      393.6083  0.0025\n",
      "     17      400.2012  0.0029\n",
      "     19      393.6083  0.0025\n",
      "     17      380.4451  0.0052\n",
      "     18      400.2012  0.0026\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     20      393.6083  0.0025\n",
      "     18      380.4451  0.0027\n",
      "     19      400.2012  0.0025\n",
      "      2      \u001b[36m164.6281\u001b[0m      \u001b[32m147.5415\u001b[0m  0.0462\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 1.\n",
      "     20      400.2012  0.0025\n",
      "     19      380.4451  0.0029\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 1.\n",
      "     20      380.4451  0.0025\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "      3      \u001b[36m124.5622\u001b[0m      \u001b[32m119.3063\u001b[0m  0.0359\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 1.\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "      3      \u001b[36m151.4542\u001b[0m      \u001b[32m144.0881\u001b[0m  0.0447\n",
      "      3      \u001b[36m142.4240\u001b[0m      \u001b[32m131.9035\u001b[0m  0.0380\n",
      "      3      \u001b[36m152.7760\u001b[0m      \u001b[32m135.3165\u001b[0m  0.0392\n",
      "      3      \u001b[36m157.9520\u001b[0m      \u001b[32m141.9687\u001b[0m  0.0377\n",
      "      4      \u001b[36m123.5479\u001b[0m      \u001b[32m118.8552\u001b[0m  0.0377\n",
      "      4      \u001b[36m147.6862\u001b[0m      \u001b[32m139.4749\u001b[0m  0.0367\n",
      "      4      \u001b[36m139.3943\u001b[0m      \u001b[32m129.7262\u001b[0m  0.0373\n",
      "      4      \u001b[36m150.6683\u001b[0m      136.6358  0.0385\n",
      "      4      \u001b[36m154.3861\u001b[0m      \u001b[32m139.3694\u001b[0m  0.0369\n",
      "      5      \u001b[36m122.6188\u001b[0m      \u001b[32m118.5905\u001b[0m  0.0369\n",
      "      5      \u001b[36m146.5262\u001b[0m      \u001b[32m139.1009\u001b[0m  0.0362\n",
      "      5      \u001b[36m136.8871\u001b[0m      \u001b[32m129.0160\u001b[0m  0.0359\n",
      "      5      151.2282      135.4396  0.0378\n",
      "      5      \u001b[36m151.8872\u001b[0m      \u001b[32m139.1353\u001b[0m  0.0371\n",
      "      6      122.9659      118.7829  0.0370\n",
      "      6      \u001b[36m145.2465\u001b[0m      \u001b[32m137.1202\u001b[0m  0.0369\n",
      "      6      136.9908      \u001b[32m128.5418\u001b[0m  0.0366\n",
      "      6      \u001b[36m149.4556\u001b[0m      \u001b[32m133.3164\u001b[0m  0.0375\n",
      "      6      \u001b[36m150.7623\u001b[0m      139.7072  0.0364\n",
      "      7      122.6676      \u001b[32m118.5531\u001b[0m  0.0367\n",
      "      7      \u001b[36m145.0568\u001b[0m      137.5949  0.0370\n",
      "      7      \u001b[36m136.7742\u001b[0m      \u001b[32m128.1598\u001b[0m  0.0369\n",
      "      7      \u001b[36m147.8880\u001b[0m      \u001b[32m133.2749\u001b[0m  0.0379\n",
      "      7      150.9525      140.2171  0.0369\n",
      "      8      \u001b[36m122.2716\u001b[0m      \u001b[32m118.4233\u001b[0m  0.0358\n",
      "      8      \u001b[36m144.7700\u001b[0m      137.7692  0.0361\n",
      "      8      \u001b[36m136.5396\u001b[0m      \u001b[32m127.8966\u001b[0m  0.0366\n",
      "      8      149.8271      \u001b[32m132.9440\u001b[0m  0.0377\n",
      "      8      150.8524      140.4347  0.0357\n",
      "      9      \u001b[36m121.8105\u001b[0m      \u001b[32m118.0167\u001b[0m  0.0370\n",
      "      9      \u001b[36m144.2530\u001b[0m      137.8466  0.0362\n",
      "      9      \u001b[36m135.9334\u001b[0m      \u001b[32m127.7784\u001b[0m  0.0362\n",
      "      9      150.8987      139.8593  0.0359\n",
      "      9      148.9701      \u001b[32m132.8760\u001b[0m  0.0378\n",
      "     10      121.8970      118.3393  0.0354\n",
      "     10      \u001b[36m143.9814\u001b[0m      137.4335  0.0389\n",
      "     10      135.9385      127.8460  0.0392\n",
      "     10      151.0394      139.4468  0.0394\n",
      "     10      149.1505      133.0368  0.0404\n",
      "     11      122.0374      118.4717  0.0375\n",
      "     11      \u001b[36m143.8640\u001b[0m      \u001b[32m137.1191\u001b[0m  0.0397\n",
      "     11      \u001b[36m135.1800\u001b[0m      128.1609  0.0378\n",
      "     11      \u001b[36m150.7605\u001b[0m      139.2806  0.0360\n",
      "     11      148.0691      133.6440  0.0378\n",
      "     12      \u001b[36m121.7739\u001b[0m      118.2071  0.0367\n",
      "     12      \u001b[36m143.1371\u001b[0m      137.1299  0.0362\n",
      "     12      135.3435      128.0640  0.0366\n",
      "     12      \u001b[36m150.7016\u001b[0m      \u001b[32m139.1194\u001b[0m  0.0363\n",
      "     12      147.9021      133.6223  0.0375\n",
      "     13      \u001b[36m121.4608\u001b[0m      118.9944  0.0365\n",
      "     13      143.5735      137.1874  0.0357\n",
      "     13      \u001b[36m135.1399\u001b[0m      127.8471  0.0365\n",
      "     13      \u001b[36m150.5173\u001b[0m      139.1213  0.0363\n",
      "     14      121.6239      118.9291  0.0358\n",
      "     13      148.1505      133.1615  0.0373\n",
      "     14      143.2233      \u001b[32m137.0039\u001b[0m  0.0368\n",
      "     14      135.6230      128.0554  0.0361\n",
      "     14      150.7432      139.5830  0.0361\n",
      "     15      \u001b[36m120.8892\u001b[0m      118.9350  0.0366\n",
      "     14      \u001b[36m147.6999\u001b[0m      133.3404  0.0373\n",
      "     15      \u001b[36m142.7521\u001b[0m      \u001b[32m136.9991\u001b[0m  0.0364\n",
      "     15      \u001b[36m135.1302\u001b[0m      128.1991  0.0358\n",
      "     15      150.9763      139.4802  0.0355\n",
      "     16      121.4467      118.9978  0.0373\n",
      "     15      148.0167      133.3449  0.0379\n",
      "     16      143.2976      \u001b[32m136.9974\u001b[0m  0.0362\n",
      "     16      135.2993      128.5750  0.0370\n",
      "     16      150.6500      139.2159  0.0359\n",
      "     17      121.0450      \u001b[32m117.4598\u001b[0m  0.0360\n",
      "     16      148.0859      133.2399  0.0379\n",
      "     17      143.4112      137.3245  0.0360\n",
      "     17      135.3731      128.5800  0.0364\n",
      "     17      150.9495      139.5836  0.0367\n",
      "     18      121.0508      \u001b[32m116.8828\u001b[0m  0.0365\n",
      "     17      148.1380      133.0044  0.0373\n",
      "     18      142.7556      137.6293  0.0367\n",
      "     18      135.1364      128.1173  0.0356\n",
      "     18      \u001b[36m150.3176\u001b[0m      139.4862  0.0365\n",
      "     19      \u001b[36m120.5799\u001b[0m      116.9820  0.0366\n",
      "     18      147.7126      133.1161  0.0366\n",
      "     19      \u001b[36m142.6732\u001b[0m      137.6296  0.0367\n",
      "     19      135.3423      128.1942  0.0362\n",
      "     19      150.6117      139.5180  0.0362\n",
      "     20      120.8160      116.9195  0.0365\n",
      "     19      148.2786      133.2311  0.0381\n",
      "     20      142.9467      137.4471  0.0359\n",
      "     20      135.8263      128.2486  0.0357\n",
      "     20      150.6677      139.5220  0.0358\n",
      "     21      \u001b[36m120.5156\u001b[0m      117.3424  0.0361\n",
      "     20      \u001b[36m147.5513\u001b[0m      133.7310  0.0370\n",
      "     21      143.2995      137.9340  0.0365\n",
      "     21      135.5608      \u001b[32m127.4120\u001b[0m  0.0360\n",
      "     21      150.3588      139.5261  0.0362\n",
      "     22      120.9546      117.5419  0.0374\n",
      "     21      \u001b[36m147.5354\u001b[0m      134.0796  0.0374\n",
      "     22      143.2084      138.3952  0.0363\n",
      "     22      \u001b[36m135.1223\u001b[0m      127.4873  0.0365\n",
      "     22      \u001b[36m150.1007\u001b[0m      139.5280  0.0364\n",
      "     23      121.0550      118.1945  0.0371\n",
      "     22      \u001b[36m147.4386\u001b[0m      134.0845  0.0365\n",
      "     23      142.9287      138.3964  0.0360\n",
      "     23      \u001b[36m134.8987\u001b[0m      \u001b[32m127.4051\u001b[0m  0.0360\n",
      "     23      150.2009      139.2179  0.0363\n",
      "     24      120.9348      117.9616  0.0369\n",
      "     23      147.8444      134.1028  0.0381\n",
      "     24      143.2923      138.4202  0.0366\n",
      "     24      135.2474      127.4160  0.0360\n",
      "     24      151.0226      139.5970  0.0359\n",
      "     25      121.3475      117.5360  0.0363\n",
      "     24      147.6078      134.1274  0.0370\n",
      "     25      143.7514      138.5595  0.0358\n",
      "     25      135.5738      127.4610  0.0363\n",
      "     25      150.4659      139.5987  0.0360\n",
      "     26      121.1415      117.5362  0.0358\n",
      "     25      148.0844      133.7567  0.0380\n",
      "     26      142.8241      138.4371  0.0362\n",
      "     26      135.1799      127.6212  0.0362\n",
      "     26      150.3713      139.4502  0.0366\n",
      "     27      121.4746      117.5349  0.0364\n",
      "     26      147.6806      133.5422  0.0374\n",
      "     27      143.1341      138.6580  0.0365\n",
      "     27      135.2449      127.5651  0.0357\n",
      "     27      150.4205      139.5052  0.0363\n",
      "     28      120.9012      117.5340  0.0365\n",
      "     27      147.5905      133.6562  0.0372\n",
      "     28      143.6990      138.6738  0.0359\n",
      "     28      135.2060      127.9572  0.0353\n",
      "     28      150.3858      139.2240  0.0358\n",
      "     29      120.7098      117.5345  0.0362\n",
      "     28      147.7439      133.7930  0.0377\n",
      "     29      142.9439      138.6731  0.0360\n",
      "     29      135.3608      127.8209  0.0361\n",
      "     29      150.3031      139.1758  0.0373\n",
      "     30      120.7956      117.5344  0.0370\n",
      "     30      143.1666      138.5138  0.0362\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 9.\n",
      "     30      135.7973      \u001b[32m126.6687\u001b[0m  0.0354\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     30      \u001b[36m149.8267\u001b[0m      139.2667  0.0361\n",
      "     31      120.9551      117.4433  0.0358\n",
      "     31      143.4302      138.8211  0.0368\n",
      "     31      134.9712      127.0839  0.0361\n",
      "     31      \u001b[36m149.8213\u001b[0m      139.2502  0.0360\n",
      "     32      120.8055      117.4423  0.0363\n",
      "     32      143.1557      139.4887  0.0356\n",
      "     32      135.4483      127.4981  0.0365\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 12.\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     33      120.6748      117.4511  0.0362\n",
      "     33      143.3639      139.4884  0.0357\n",
      "     33      135.5454      127.2069  0.0364\n",
      "     34      120.6620      117.2616  0.0362\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 14.\n",
      "     34      \u001b[36m134.7696\u001b[0m      127.4393  0.0354\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     35      120.8363      117.1929  0.0354\n",
      "     35      135.0219      127.1660  0.0342\n",
      "     36      120.9489      117.1925  0.0350\n",
      "     36      135.3194      127.0315  0.0360\n",
      "     37      120.8735      \u001b[32m116.8671\u001b[0m  0.0348\n",
      "     37      135.2802      127.8118  0.0353\n",
      "     38      120.8424      \u001b[32m116.7644\u001b[0m  0.0352\n",
      "     38      \u001b[36m134.7587\u001b[0m      127.4111  0.0354\n",
      "     39      \u001b[36m120.4416\u001b[0m      117.8755  0.0353\n",
      "     39      135.2230      127.7130  0.0358\n",
      "     40      121.2425      117.2189  0.0350\n",
      "     40      135.6349      127.4518  0.0355\n",
      "     41      121.0646      117.5215  0.0356\n",
      "     41      135.0568      127.0974  0.0360\n",
      "     42      120.8959      117.0581  0.0354\n",
      "     42      135.0081      127.2628  0.0355\n",
      "     43      \u001b[36m120.2874\u001b[0m      \u001b[32m116.7419\u001b[0m  0.0353\n",
      "     43      135.1280      127.4306  0.0353\n",
      "     44      120.8541      116.8957  0.0365\n",
      "     44      135.1853      127.4283  0.0355\n",
      "     45      120.5954      116.9216  0.0351\n",
      "     45      135.3402      127.4460  0.0351\n",
      "     46      120.9324      117.3054  0.0353\n",
      "     46      135.1822      127.5480  0.0351\n",
      "     47      121.1171      116.9490  0.0364\n",
      "     47      135.0920      129.1040  0.0365\n",
      "     48      121.3328      117.2866  0.0386\n",
      "     48      135.8811      130.5044  0.0375\n",
      "     49      121.0834      117.4404  0.0350\n",
      "     49      136.0132      130.4347  0.0352\n",
      "     50      122.0017      117.9241  0.0378\n",
      "Restoring best model from epoch 43.\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 30.\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m167.0762\u001b[0m      \u001b[32m143.2557\u001b[0m  0.0500\n",
      "      2      \u001b[36m146.3960\u001b[0m      \u001b[32m129.8315\u001b[0m  0.0515\n",
      "      3      \u001b[36m141.9365\u001b[0m      \u001b[32m128.0827\u001b[0m  0.0475\n",
      "      4      \u001b[36m140.5189\u001b[0m      128.6025  0.0505\n",
      "      5      \u001b[36m140.0141\u001b[0m      \u001b[32m126.5991\u001b[0m  0.0542\n",
      "      6      140.1459      \u001b[32m126.4305\u001b[0m  0.0474\n",
      "      7      \u001b[36m139.4697\u001b[0m      126.5463  0.0498\n",
      "      8      139.9643      126.6375  0.0467\n",
      "      9      139.6104      126.9687  0.0483\n",
      "     10      139.6689      127.0182  0.0478\n",
      "     11      139.6113      127.2208  0.0490\n",
      "     12      \u001b[36m139.4442\u001b[0m      126.8135  0.0497\n",
      "     13      139.4761      \u001b[32m126.4114\u001b[0m  0.0477\n",
      "     14      \u001b[36m139.2652\u001b[0m      \u001b[32m126.0231\u001b[0m  0.0481\n",
      "     15      139.3485      \u001b[32m125.8502\u001b[0m  0.0804\n",
      "     16      139.9037      126.4416  0.0548\n",
      "     17      139.7777      125.9627  0.0493\n",
      "     18      139.6094      125.9438  0.0511\n",
      "     19      139.7019      126.2755  0.0480\n",
      "     20      139.8239      125.9688  0.0501\n",
      "     21      139.5197      126.1217  0.0484\n",
      "     22      \u001b[36m139.0962\u001b[0m      126.4675  0.0571\n",
      "     23      139.1396      126.3195  0.0601\n",
      "     24      139.2656      125.9308  0.0565\n",
      "     25      139.3939      126.1368  0.0595\n",
      "     26      139.4731      127.3629  0.0585\n",
      "     27      139.1812      126.8420  0.0583\n",
      "     28      139.1277      125.9293  0.0532\n",
      "     29      \u001b[36m138.9944\u001b[0m      126.1832  0.0496\n",
      "     30      139.2207      126.3059  0.0547\n",
      "     31      139.4685      126.1458  0.0681\n",
      "     32      139.1747      126.0018  0.0563\n",
      "     33      139.3302      126.2914  0.0529\n",
      "     34      139.0417      126.4668  0.0554\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 15.\n",
      "durations 1.2666667 351.0\n",
      "split age            float32\n",
      "sex           category\n",
      "sample_yr      float32\n",
      "kappa          float32\n",
      "lambda         float32\n",
      "flc_grp        float32\n",
      "creatinine     float32\n",
      "mgus_0.0         uint8\n",
      "mgus_1.0         uint8\n",
      "dtype: object\n",
      "dtype X_train age            float32\n",
      "sex           category\n",
      "sample_yr      float32\n",
      "kappa          float32\n",
      "lambda         float32\n",
      "flc_grp        float32\n",
      "creatinine     float32\n",
      "mgus_0.0         uint8\n",
      "mgus_1.0         uint8\n",
      "dtype: object\n",
      "dtype y_train float64\n",
      "dtype X_test age            float32\n",
      "sex           category\n",
      "sample_yr      float32\n",
      "kappa          float32\n",
      "lambda         float32\n",
      "flc_grp        float32\n",
      "creatinine     float32\n",
      "mgus_0.0         uint8\n",
      "mgus_1.0         uint8\n",
      "dtype: object\n",
      "dtype y_test float32\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m526.5511\u001b[0m      \u001b[32m522.0874\u001b[0m  0.0934\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m679.9148\u001b[0m      \u001b[32m699.2835\u001b[0m  0.1003\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m845.9871\u001b[0m      \u001b[32m884.2510\u001b[0m  0.1036\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m895.7284\u001b[0m      \u001b[32m928.3268\u001b[0m  0.1067\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m907.6710\u001b[0m      \u001b[32m942.7028\u001b[0m  0.0962\n",
      "      2      \u001b[36m490.2662\u001b[0m      \u001b[32m493.8632\u001b[0m  0.0989\n",
      "      2      \u001b[36m756.2311\u001b[0m      \u001b[32m782.8297\u001b[0m  0.1145\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m97.8858\u001b[0m       \u001b[32m99.8789\u001b[0m  0.2655\n",
      "      2      \u001b[36m620.3419\u001b[0m      \u001b[32m645.6327\u001b[0m  0.1389\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m61.9159\u001b[0m       \u001b[32m60.2986\u001b[0m  0.2732\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m93.5437\u001b[0m       \u001b[32m96.5882\u001b[0m  0.2817\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m77.4485\u001b[0m       \u001b[32m80.7855\u001b[0m  0.2905\n",
      "      2      \u001b[36m793.4327\u001b[0m      \u001b[32m812.6647\u001b[0m  0.1231\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m99.1959\u001b[0m      \u001b[32m101.5082\u001b[0m  0.2883\n",
      "      2      \u001b[36m802.5826\u001b[0m      \u001b[32m821.2056\u001b[0m  0.1430\n",
      "      3      \u001b[36m485.8435\u001b[0m      \u001b[32m487.7545\u001b[0m  0.1086\n",
      "      3      \u001b[36m617.5740\u001b[0m      \u001b[32m641.4166\u001b[0m  0.0799\n",
      "      3      \u001b[36m748.2181\u001b[0m      \u001b[32m767.3334\u001b[0m  0.1046\n",
      "      3      \u001b[36m781.2360\u001b[0m      \u001b[32m793.1172\u001b[0m  0.0804\n",
      "      3      \u001b[36m789.2055\u001b[0m      \u001b[32m804.7623\u001b[0m  0.0899\n",
      "      4      \u001b[36m483.5873\u001b[0m      \u001b[32m486.7030\u001b[0m  0.0917\n",
      "      4      \u001b[36m614.6688\u001b[0m      \u001b[32m640.5253\u001b[0m  0.0833\n",
      "      4      \u001b[36m745.7816\u001b[0m      \u001b[32m766.7078\u001b[0m  0.0722\n",
      "      4      \u001b[36m777.5167\u001b[0m      \u001b[32m792.1235\u001b[0m  0.0874\n",
      "      4      \u001b[36m783.1941\u001b[0m      806.2740  0.0710\n",
      "      2       \u001b[36m60.7056\u001b[0m       61.3433  0.2153\n",
      "      5      \u001b[36m744.6619\u001b[0m      \u001b[32m766.3919\u001b[0m  0.0563\n",
      "      2       \u001b[36m75.5393\u001b[0m       \u001b[32m79.7367\u001b[0m  0.2130\n",
      "      5      \u001b[36m482.4985\u001b[0m      \u001b[32m486.5040\u001b[0m  0.0812\n",
      "      2       \u001b[36m95.6766\u001b[0m       \u001b[32m99.5352\u001b[0m  0.2362\n",
      "      5      \u001b[36m612.5482\u001b[0m      \u001b[32m639.6703\u001b[0m  0.0767\n",
      "      2       \u001b[36m91.2501\u001b[0m       96.9208  0.2452\n",
      "      2       \u001b[36m96.0389\u001b[0m      \u001b[32m101.0579\u001b[0m  0.2220\n",
      "      5      \u001b[36m777.0074\u001b[0m      792.4998  0.0797\n",
      "      5      \u001b[36m781.5701\u001b[0m      805.6028  0.0763\n",
      "      6      \u001b[36m480.9257\u001b[0m      \u001b[32m485.7667\u001b[0m  0.0621\n",
      "      6      \u001b[36m743.1806\u001b[0m      \u001b[32m765.3802\u001b[0m  0.0675\n",
      "      6      \u001b[36m610.8650\u001b[0m      \u001b[32m638.9264\u001b[0m  0.0782\n",
      "      6      \u001b[36m776.3436\u001b[0m      \u001b[32m792.1066\u001b[0m  0.0803\n",
      "      6      781.7032      \u001b[32m804.1818\u001b[0m  0.0687\n",
      "      7      \u001b[36m479.2930\u001b[0m      \u001b[32m485.6988\u001b[0m  0.0602\n",
      "      7      \u001b[36m742.2081\u001b[0m      \u001b[32m765.1636\u001b[0m  0.0858\n",
      "      7      \u001b[36m610.3871\u001b[0m      638.9279  0.0701\n",
      "      3       60.7155       60.3163  0.1857\n",
      "      7      \u001b[36m780.9337\u001b[0m      \u001b[32m803.7031\u001b[0m  0.0750\n",
      "      8      \u001b[36m479.0890\u001b[0m      \u001b[32m483.6914\u001b[0m  0.0733\n",
      "      3       75.9020       80.4752  0.2125\n",
      "      7      \u001b[36m775.0236\u001b[0m      \u001b[32m791.6819\u001b[0m  0.0903\n",
      "      3       \u001b[36m91.0677\u001b[0m       \u001b[32m95.8339\u001b[0m  0.1887\n",
      "      3       \u001b[36m95.8939\u001b[0m      101.3096  0.1902\n",
      "      3       \u001b[36m94.8390\u001b[0m       \u001b[32m99.1994\u001b[0m  0.2207\n",
      "      8      \u001b[36m610.1030\u001b[0m      639.1142  0.0675\n",
      "      8      \u001b[36m742.0020\u001b[0m      \u001b[32m764.3115\u001b[0m  0.0835\n",
      "      8      \u001b[36m779.8129\u001b[0m      \u001b[32m802.6079\u001b[0m  0.0887\n",
      "      8      \u001b[36m774.0588\u001b[0m      \u001b[32m790.9246\u001b[0m  0.0929\n",
      "      9      \u001b[36m478.2226\u001b[0m      \u001b[32m483.0350\u001b[0m  0.1066\n",
      "      9      \u001b[36m609.2065\u001b[0m      639.1882  0.0908\n",
      "      9      \u001b[36m740.3083\u001b[0m      \u001b[32m764.0773\u001b[0m  0.0971\n",
      "      9      \u001b[36m773.3441\u001b[0m      \u001b[32m790.6650\u001b[0m  0.0663\n",
      "     10      \u001b[36m477.1654\u001b[0m      485.2113  0.1008\n",
      "      9      \u001b[36m778.9819\u001b[0m      \u001b[32m802.2091\u001b[0m  0.1301\n",
      "     10      \u001b[36m608.9649\u001b[0m      \u001b[32m638.2074\u001b[0m  0.0983\n",
      "     10      \u001b[36m739.8946\u001b[0m      764.4039  0.0977\n",
      "      4       75.8588       81.3076  0.2413\n",
      "      4       \u001b[36m90.7651\u001b[0m       \u001b[32m95.6934\u001b[0m  0.2509\n",
      "     10      773.7162      791.4486  0.0977\n",
      "      4       \u001b[36m60.3342\u001b[0m       61.0293  0.2887\n",
      "      4       \u001b[36m95.2190\u001b[0m      \u001b[32m100.2746\u001b[0m  0.2665\n",
      "     10      \u001b[36m778.8077\u001b[0m      802.4886  0.0778\n",
      "      4       95.1556       99.3828  0.2691\n",
      "     11      478.0881      485.0445  0.0959\n",
      "     11      \u001b[36m738.4217\u001b[0m      \u001b[32m763.8394\u001b[0m  0.0802\n",
      "     11      \u001b[36m608.3852\u001b[0m      \u001b[32m637.4896\u001b[0m  0.0929\n",
      "     11      \u001b[36m771.8009\u001b[0m      792.4398  0.0886\n",
      "     11      \u001b[36m778.1693\u001b[0m      802.4841  0.0753\n",
      "     12      \u001b[36m475.6419\u001b[0m      485.0966  0.0690\n",
      "     12      \u001b[36m608.1246\u001b[0m      \u001b[32m637.4885\u001b[0m  0.0687\n",
      "     12      738.6212      \u001b[32m763.2410\u001b[0m  0.0815\n",
      "     12      772.5742      791.9414  0.0611\n",
      "     12      \u001b[36m777.1599\u001b[0m      \u001b[32m801.8329\u001b[0m  0.0580\n",
      "     13      \u001b[36m474.9855\u001b[0m      484.8570  0.0711\n",
      "      5       75.9826       80.5225  0.2060\n",
      "     13      \u001b[36m737.0614\u001b[0m      763.5399  0.0638\n",
      "     13      \u001b[36m607.6808\u001b[0m      637.6227  0.0724\n",
      "      5       90.7741       \u001b[32m95.5538\u001b[0m  0.2086\n",
      "     13      \u001b[36m771.7818\u001b[0m      791.2479  0.0633\n",
      "      5       96.0373      101.1241  0.2066\n",
      "      5       60.5386       60.6519  0.2238\n",
      "     13      777.8895      \u001b[32m801.6854\u001b[0m  0.0608\n",
      "     14      \u001b[36m473.7727\u001b[0m      485.2471  0.0740\n",
      "     14      \u001b[36m736.6781\u001b[0m      764.0691  0.0777\n",
      "     14      607.7377      638.0413  0.0781\n",
      "      5       \u001b[36m94.6770\u001b[0m       \u001b[32m99.0541\u001b[0m  0.2623\n",
      "     14      \u001b[36m771.5795\u001b[0m      790.7203  0.0840\n",
      "     14      777.4608      \u001b[32m801.6534\u001b[0m  0.0745\n",
      "     15      475.0916      484.6672  0.0700\n",
      "     15      \u001b[36m736.1725\u001b[0m      763.6453  0.0672\n",
      "     15      \u001b[36m606.7400\u001b[0m      638.1560  0.0718\n",
      "     15      \u001b[36m770.5869\u001b[0m      \u001b[32m790.3354\u001b[0m  0.0662\n",
      "      6       \u001b[36m75.2624\u001b[0m       80.1700  0.1918\n",
      "     15      \u001b[36m775.6670\u001b[0m      \u001b[32m800.9784\u001b[0m  0.0853\n",
      "     16      \u001b[36m473.6140\u001b[0m      483.9054  0.0714\n",
      "     16      606.7841      637.9960  0.0604\n",
      "     16      \u001b[36m735.4538\u001b[0m      764.7676  0.0757\n",
      "      6       96.0208      101.3217  0.1925\n",
      "      6       60.4602       60.8506  0.1938\n",
      "      6       \u001b[36m90.4348\u001b[0m       96.0476  0.2119\n",
      "     16      \u001b[36m770.3008\u001b[0m      790.5138  0.0668\n",
      "     16      776.8763      801.3572  0.0614\n",
      "     17      \u001b[36m472.3854\u001b[0m      485.2677  0.0675\n",
      "     17      607.1668      638.4734  0.0641\n",
      "     17      \u001b[36m734.9779\u001b[0m      765.3288  0.0761\n",
      "     17      770.3750      \u001b[32m790.1823\u001b[0m  0.0675\n",
      "     17      775.7011      802.1046  0.0632\n",
      "     18      \u001b[36m472.3624\u001b[0m      484.8792  0.0585\n",
      "     18      \u001b[36m606.7014\u001b[0m      638.7576  0.0638\n",
      "      6       94.9719       99.0636  0.2414\n",
      "     18      \u001b[36m734.3106\u001b[0m      764.5945  0.0701\n",
      "     18      \u001b[36m769.1739\u001b[0m      791.0554  0.0631\n",
      "     19      \u001b[36m471.9934\u001b[0m      484.0809  0.0563\n",
      "     19      \u001b[36m605.7200\u001b[0m      638.9863  0.0581\n",
      "      7       \u001b[36m75.1617\u001b[0m       80.0745  0.2086\n",
      "     18      776.0411      803.2000  0.0845\n",
      "      7       61.0315       \u001b[32m59.9380\u001b[0m  0.1946\n",
      "      7       95.9034      100.8572  0.2061\n",
      "      7       \u001b[36m90.3488\u001b[0m       95.7188  0.2017\n",
      "     19      769.9871      790.4394  0.0664\n",
      "     20      472.1332      485.4864  0.0670\n",
      "     19      734.6048      765.0254  0.0934\n",
      "     20      607.1476      637.9458  0.0648\n",
      "     19      775.8709      802.8983  0.0712\n",
      "     20      \u001b[36m768.9411\u001b[0m      790.9447  0.0647\n",
      "     21      472.6049      485.4980  0.0645\n",
      "     21      606.0181      637.7291  0.0662\n",
      "     20      775.9586      802.3707  0.0751\n",
      "      7       94.8735       99.3949  0.2136\n",
      "     20      \u001b[36m733.2918\u001b[0m      763.5958  0.1014\n",
      "     21      769.1588      791.7325  0.0854\n",
      "     22      \u001b[36m470.8148\u001b[0m      487.2972  0.0785\n",
      "     22      605.7942      638.9371  0.0736\n",
      "      8       75.5192       80.0328  0.2181\n",
      "      8       \u001b[36m60.1925\u001b[0m       60.7784  0.2165\n",
      "     21      733.6579      \u001b[32m763.0526\u001b[0m  0.0805\n",
      "      8       \u001b[36m90.2716\u001b[0m       95.5761  0.2182\n",
      "      8       95.3368      100.3334  0.2202\n",
      "     21      776.4266      \u001b[32m800.9474\u001b[0m  0.0993\n",
      "     23      471.2654      486.4724  0.0750\n",
      "     23      \u001b[36m605.5310\u001b[0m      638.2587  0.0735\n",
      "     22      \u001b[36m768.8499\u001b[0m      791.7303  0.0873\n",
      "     24      472.1882      487.0836  0.0571\n",
      "     24      605.8267      \u001b[32m635.6165\u001b[0m  0.0610\n",
      "     22      \u001b[36m774.8846\u001b[0m      801.4396  0.0964\n",
      "     23      \u001b[36m768.4558\u001b[0m      791.9769  0.0898\n",
      "     22      733.9309      763.1153  0.1313\n",
      "     25      606.1689      637.1373  0.0639\n",
      "     25      471.6267      488.5123  0.0734\n",
      "      8       94.7905       \u001b[32m99.0359\u001b[0m  0.2632\n",
      "     23      \u001b[36m774.8510\u001b[0m      802.0541  0.0836\n",
      "     24      768.8688      791.2009  0.0796\n",
      "     23      \u001b[36m733.2633\u001b[0m      763.4929  0.0957\n",
      "      9       60.7091       61.4743  0.2502\n",
      "     26      471.3510      485.3040  0.0870\n",
      "      9       75.4948       79.7444  0.2778\n",
      "     26      \u001b[36m605.3839\u001b[0m      638.8345  0.0913\n",
      "      9       \u001b[36m90.2465\u001b[0m       \u001b[32m95.4337\u001b[0m  0.2458\n",
      "      9       95.8564      100.7635  0.2642\n",
      "     25      \u001b[36m767.7587\u001b[0m      792.6257  0.1127\n",
      "     24      775.0010      801.6206  0.1276\n",
      "     27      606.6890      638.0616  0.1108\n",
      "     24      733.5185      765.4540  0.1483\n",
      "     27      \u001b[36m470.6073\u001b[0m      484.7535  0.1421\n",
      "     26      769.3804      791.4618  0.0974\n",
      "     25      \u001b[36m774.7676\u001b[0m      \u001b[32m800.7057\u001b[0m  0.1030\n",
      "     28      607.2067      639.5740  0.0874\n",
      "      9       95.4845       99.5962  0.2809\n",
      "     25      \u001b[36m733.0787\u001b[0m      \u001b[32m762.2961\u001b[0m  0.0877\n",
      "     28      472.5500      486.2253  0.1029\n",
      "     27      768.2927      792.8177  0.0775\n",
      "     10       61.3720       61.3970  0.2618\n",
      "     26      774.9770      801.3943  0.0899\n",
      "     29      \u001b[36m604.9321\u001b[0m      639.2075  0.0855\n",
      "     10       95.8012      101.8534  0.2673\n",
      "     10       75.5437       80.2749  0.2876\n",
      "     10       90.2515       96.3219  0.2914\n",
      "     26      733.9139      \u001b[32m761.2448\u001b[0m  0.0837\n",
      "     28      768.9156      791.2120  0.0768\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 9.\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     27      \u001b[36m773.5882\u001b[0m      802.5058  0.0836\n",
      "     30      607.0022      636.9006  0.0671\n",
      "     27      \u001b[36m731.8064\u001b[0m      \u001b[32m760.1810\u001b[0m  0.0713\n",
      "     29      769.6220      791.3207  0.0685\n",
      "     28      774.0839      802.0280  0.0706\n",
      "     31      607.3215      636.7585  0.0818\n",
      "     28      732.7445      \u001b[32m759.4968\u001b[0m  0.0646\n",
      "     10       94.8985       99.5315  0.2367\n",
      "     30      769.1509      792.4604  0.0758\n",
      "     11       61.2147       60.0970  0.2287\n",
      "     11       \u001b[36m90.2144\u001b[0m       \u001b[32m95.3266\u001b[0m  0.1971\n",
      "     32      607.5648      637.0751  0.0668\n",
      "     11       75.5258       \u001b[32m79.7031\u001b[0m  0.2174\n",
      "     11       95.6250      100.4603  0.2197\n",
      "     29      \u001b[36m731.1277\u001b[0m      759.8339  0.0730\n",
      "     29      773.8685      801.5850  0.0916\n",
      "     31      768.7951      793.3871  0.0835\n",
      "     33      607.8643      638.0836  0.0792\n",
      "     30      \u001b[36m730.2254\u001b[0m      759.5758  0.0748\n",
      "     30      775.0846      800.7364  0.0841\n",
      "     32      \u001b[36m767.6485\u001b[0m      792.6537  0.0722\n",
      "     34      605.2313      637.7552  0.0707\n",
      "     31      730.3343      764.3660  0.0753\n",
      "     31      773.7582      800.7734  0.0771\n",
      "     11       \u001b[36m94.6484\u001b[0m       \u001b[32m98.8266\u001b[0m  0.2376\n",
      "     12       90.6002       95.4465  0.2089\n",
      "     33      \u001b[36m767.1664\u001b[0m      792.4409  0.0795\n",
      "     12       60.9384       60.3597  0.2192\n",
      "     35      605.2779      636.5720  0.0641\n",
      "     32      \u001b[36m729.8528\u001b[0m      763.7870  0.0669\n",
      "     12       75.5248       79.8898  0.2264\n",
      "     12       95.5578      \u001b[32m100.2301\u001b[0m  0.2333\n",
      "     32      \u001b[36m773.1576\u001b[0m      \u001b[32m800.1812\u001b[0m  0.0699\n",
      "     34      767.6689      791.7930  0.0658\n",
      "     36      606.0421      639.1262  0.0662\n",
      "     33      \u001b[36m729.6879\u001b[0m      763.3344  0.0744\n",
      "     33      \u001b[36m772.9038\u001b[0m      801.0237  0.0707\n",
      "     35      768.3133      791.8342  0.0634\n",
      "     37      605.8087      640.8134  0.0702\n",
      "     12       94.8375       99.0377  0.1914\n",
      "     34      773.9359      801.6389  0.0685\n",
      "     13       61.0518       \u001b[32m59.9146\u001b[0m  0.1776\n",
      "     34      730.1181      760.1460  0.0811\n",
      "     13       90.2877       95.9069  0.1868\n",
      "     36      768.1035      790.4777  0.0635\n",
      "     38      607.6704      639.6554  0.0613\n",
      "     13       75.4819       80.8198  0.1859\n",
      "     13       95.3597      100.3235  0.1954\n",
      "     35      729.7908      762.4133  0.0663\n",
      "     35      774.0230      801.1401  0.0706\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 17.\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     39      606.7503      638.4374  0.0754\n",
      "     36      730.1166      760.3777  0.0641\n",
      "     36      773.0520      801.1744  0.0753\n",
      "     40      606.5618      637.2715  0.0622\n",
      "     14       60.7090       \u001b[32m59.8833\u001b[0m  0.1745\n",
      "     13       94.7787       98.9987  0.1874\n",
      "     14       \u001b[36m90.0235\u001b[0m       96.4196  0.1768\n",
      "     37      \u001b[36m729.0438\u001b[0m      \u001b[32m758.8597\u001b[0m  0.0677\n",
      "     14       75.5517       79.7804  0.1765\n",
      "     37      \u001b[36m772.7081\u001b[0m      802.9630  0.0680\n",
      "     14       95.4733      100.7347  0.1699\n",
      "     41      605.8475      637.6049  0.0588\n",
      "     38      \u001b[36m728.7898\u001b[0m      761.6721  0.0571\n",
      "     38      774.7945      803.2569  0.0568\n",
      "     42      \u001b[36m604.8017\u001b[0m      636.2211  0.0570\n",
      "     39      729.6094      762.7653  0.0609\n",
      "     39      775.5085      805.5851  0.0579\n",
      "     15       \u001b[36m60.1584\u001b[0m       \u001b[32m59.6245\u001b[0m  0.1576\n",
      "     14       \u001b[36m94.4121\u001b[0m       99.0970  0.1561\n",
      "     43      605.4031      636.6310  0.0561\n",
      "     15       90.6934       95.9684  0.1651\n",
      "     15       75.1650       80.0611  0.1597\n",
      "     40      729.3613      762.3297  0.0585\n",
      "     15       95.6277      100.3208  0.1609\n",
      "     40      776.4131      803.1635  0.0603\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 24.\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     41      \u001b[36m727.7492\u001b[0m      759.3730  0.0600\n",
      "     41      774.9134      804.1085  0.0582\n",
      "     16       \u001b[36m60.1452\u001b[0m       60.1591  0.1576\n",
      "     15       94.9026       98.9878  0.1591\n",
      "     42      \u001b[36m727.1876\u001b[0m      759.3636  0.0582\n",
      "     16       90.0922       95.3712  0.1584\n",
      "     42      775.4291      802.9763  0.0572\n",
      "     16       75.2005       79.8097  0.1634\n",
      "     16       95.7745       \u001b[32m99.7617\u001b[0m  0.1581\n",
      "     43      728.0031      760.8224  0.0567\n",
      "     43      773.7163      801.5730  0.0574\n",
      "     44      727.9708      763.0396  0.0566\n",
      "     44      774.8363      803.2755  0.0573\n",
      "     17       \u001b[36m60.1298\u001b[0m       60.6487  0.1581\n",
      "     16       95.1677       99.9522  0.1584\n",
      "     17       90.2867       \u001b[32m95.2541\u001b[0m  0.1581\n",
      "     45      728.1178      762.0074  0.0563\n",
      "     45      773.2866      802.5328  0.0577\n",
      "     17       75.2827       80.0661  0.1588\n",
      "     17       95.8862      100.8584  0.1589\n",
      "     46      \u001b[36m725.1906\u001b[0m      761.0692  0.0584\n",
      "     46      773.4408      \u001b[32m799.5573\u001b[0m  0.0574\n",
      "     47      726.4848      761.7042  0.0569\n",
      "     47      772.8566      \u001b[32m798.8306\u001b[0m  0.0575\n",
      "     18       60.1590       59.9151  0.1596\n",
      "     17       94.9282       99.1605  0.1600\n",
      "     18       90.2896       \u001b[32m94.5602\u001b[0m  0.1575\n",
      "     48      725.7350      760.6538  0.0571\n",
      "     18       75.4091       \u001b[32m79.6336\u001b[0m  0.1574\n",
      "     48      773.0080      \u001b[32m798.5215\u001b[0m  0.0577\n",
      "     18       \u001b[36m95.1933\u001b[0m      100.2774  0.1586\n",
      "     49      725.6807      761.3395  0.0560\n",
      "     49      773.5831      800.7771  0.0579\n",
      "     50      725.9400      761.2764  0.0570\n",
      "Restoring best model from epoch 37.\n",
      "     19       \u001b[36m60.0157\u001b[0m       60.4193  0.1567\n",
      "     50      772.7471      800.0765  0.0575\n",
      "Restoring best model from epoch 48.\n",
      "     18       94.8598       99.3941  0.1591\n",
      "     19       \u001b[36m89.9009\u001b[0m       94.8960  0.1581\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     19       75.4306       80.1583  0.1604\n",
      "     19       95.5762      100.6707  0.1612\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     20       60.2634       60.4016  0.1572\n",
      "     19       94.6729       99.4132  0.1575\n",
      "     20       90.2622       95.1609  0.1574\n",
      "     20       75.3649       80.2364  0.1584\n",
      "     20       95.8786      100.5916  0.1587\n",
      "     21       60.2753       60.2956  0.1562\n",
      "     20       94.6425       99.1754  0.1562\n",
      "     21       \u001b[36m89.7843\u001b[0m       95.4441  0.1580\n",
      "     21       75.2396       80.0099  0.1570\n",
      "     21       95.2366      100.4477  0.1579\n",
      "     22       60.0926       60.3765  0.1566\n",
      "     21       94.8027      100.1611  0.1574\n",
      "     22       90.1432       95.1842  0.1579\n",
      "     22       75.5289       \u001b[32m79.4815\u001b[0m  0.1565\n",
      "     22       \u001b[36m95.1745\u001b[0m      100.2683  0.1579\n",
      "     23       60.1365       60.1253  0.1576\n",
      "     22       94.9075       99.0735  0.1555\n",
      "     23       90.1782       95.7653  0.1561\n",
      "     23       \u001b[36m75.1360\u001b[0m       80.4383  0.1580\n",
      "     23       95.6187      101.0031  0.1575\n",
      "     24       60.2932       60.7014  0.1571\n",
      "     23       94.7904       99.3960  0.1569\n",
      "     24       90.8930       96.3556  0.1564\n",
      "     24       75.2540       79.5824  0.1564\n",
      "     24       95.9991      100.8712  0.1572\n",
      "     25       60.3424       60.7112  0.1571\n",
      "     24       94.7002       99.4859  0.1577\n",
      "     25       90.3413       95.5996  0.1566\n",
      "     25       75.1923       79.5842  0.1569\n",
      "     25       95.3657      100.3743  0.1562\n",
      "     26       60.4497       60.3582  0.1571\n",
      "     25       94.8140       99.5110  0.1564\n",
      "     26       90.0198       94.9051  0.1556\n",
      "     26       \u001b[36m74.9724\u001b[0m       80.7681  0.1553\n",
      "     26       95.2933      100.2119  0.1575\n",
      "     27       60.1936       59.8711  0.1576\n",
      "     26       95.3638       99.0945  0.1578\n",
      "     27       90.3465       95.4542  0.1566\n",
      "     27       75.4870       79.9213  0.1580\n",
      "     27       95.2706      100.3984  0.1570\n",
      "     28       60.1083       60.3629  0.1561\n",
      "     27       94.9243       99.0761  0.1572\n",
      "     28       89.8273       95.3911  0.1567\n",
      "     28       75.3234       79.8157  0.1566\n",
      "     28       95.3721      100.7758  0.1571\n",
      "     29       60.3872       60.1315  0.1567\n",
      "     28       94.7889       99.1258  0.1574\n",
      "     29       \u001b[36m89.7404\u001b[0m       95.4642  0.1564\n",
      "     29       75.0867       79.9066  0.1576\n",
      "     29       \u001b[36m94.9958\u001b[0m      100.3390  0.1585\n",
      "     30       60.6830       59.9744  0.1582\n",
      "     29       94.9666       99.4543  0.1575\n",
      "     30       89.8735       95.7712  0.1584\n",
      "     30       75.3034       80.2567  0.1589\n",
      "     30       95.3119      100.6410  0.1585\n",
      "     31       60.3102       60.6466  0.1548\n",
      "     30       94.7390       99.1887  0.1568\n",
      "     31       90.1830       95.3259  0.1555\n",
      "     31       75.2586       79.9474  0.1564\n",
      "     31       95.3412      100.3998  0.1571\n",
      "     32       60.1068       60.1999  0.1583\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 11.\n",
      "     32       90.0208       95.3850  0.1606\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     32       75.2713       80.4640  0.1633\n",
      "     32       95.7531      100.6458  0.1625\n",
      "     33       60.2559       59.6968  0.1616\n",
      "     33       90.3715       95.2911  0.1609\n",
      "     33       75.2849       79.6983  0.1581\n",
      "     33       95.3752      100.3306  0.1592\n",
      "     34       60.2485       61.1922  0.1587\n",
      "     34       89.7406       95.9525  0.1593\n",
      "     34       75.2390       79.4974  0.1594\n",
      "     34       95.3858      100.7538  0.1598\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 15.\n",
      "     35       90.1217       95.2224  0.1603\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     35       75.4394       79.8507  0.1597\n",
      "     35       95.2263      100.2387  0.1608\n",
      "     36       89.9663       95.4207  0.1589\n",
      "     36       75.2281       80.1440  0.1572\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 16.\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     37       89.8248       96.6171  0.1537\n",
      "     37       75.4344       80.5386  0.1542\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 18.\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     38       75.3142       79.7756  0.1617\n",
      "     39       \u001b[36m74.8780\u001b[0m       79.6667  0.1630\n",
      "     40       75.0377       79.8899  0.1544\n",
      "     41       75.3686       80.2528  0.1673\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 22.\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m85.9575\u001b[0m       \u001b[32m86.8959\u001b[0m  0.2228\n",
      "      2       \u001b[36m85.2721\u001b[0m       87.6118  0.2078\n",
      "      3       \u001b[36m85.0134\u001b[0m       \u001b[32m86.8562\u001b[0m  0.2025\n",
      "      4       \u001b[36m84.3910\u001b[0m       \u001b[32m86.6790\u001b[0m  0.2038\n",
      "      5       84.5477       88.8155  0.2045\n",
      "      6       84.6164       88.2492  0.2243\n",
      "      7       84.6148       86.9694  0.2228\n",
      "      8       \u001b[36m84.0163\u001b[0m       87.1136  0.1984\n",
      "      9       84.3487       87.4448  0.1972\n",
      "     10       84.0865       86.8348  0.1979\n",
      "     11       \u001b[36m84.0038\u001b[0m       87.6160  0.1975\n",
      "     12       84.3021       87.3678  0.2069\n",
      "     13       84.2094       89.1566  0.2712\n",
      "     14       84.6362       87.5287  0.2250\n",
      "     15       \u001b[36m83.9880\u001b[0m       88.0462  0.2709\n",
      "     16       84.2458       87.7296  0.2239\n",
      "     17       84.2133       87.6759  0.2094\n",
      "     18       84.8898       88.4510  0.2237\n",
      "     19       84.7647       87.3133  0.2144\n",
      "     20       84.4233       87.6410  0.2231\n",
      "     21       84.2761       87.3520  0.2506\n",
      "     22       84.2576       87.4957  0.2092\n",
      "     23       \u001b[36m83.9085\u001b[0m       87.0753  0.2071\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 4.\n",
      "durations 1.0 5215.0\n",
      "dtype X_train age            float32\n",
      "sex           category\n",
      "sample_yr      float32\n",
      "kappa          float32\n",
      "lambda         float32\n",
      "flc_grp        float32\n",
      "creatinine     float32\n",
      "mgus_0.0         uint8\n",
      "mgus_1.0         uint8\n",
      "dtype: object\n",
      "dtype y_train float64\n",
      "dtype X_test age            float32\n",
      "sex           category\n",
      "sample_yr      float32\n",
      "kappa          float32\n",
      "lambda         float32\n",
      "flc_grp        float32\n",
      "creatinine     float32\n",
      "mgus_0.0         uint8\n",
      "mgus_1.0         uint8\n",
      "dtype: object\n",
      "dtype y_test float32\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m39.5947\u001b[0m       \u001b[32m39.1411\u001b[0m  0.3002\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m31.3201\u001b[0m       \u001b[32m30.8513\u001b[0m  0.3193\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m50.0391\u001b[0m       \u001b[32m48.0937\u001b[0m  0.3145\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m47.0493\u001b[0m       \u001b[32m45.7236\u001b[0m  0.3259\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m49.4241\u001b[0m       \u001b[32m47.5661\u001b[0m  0.3515\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m123.5796\u001b[0m      \u001b[32m123.5862\u001b[0m  0.3654\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m199.1525\u001b[0m      \u001b[32m191.8299\u001b[0m  0.3762\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m157.4370\u001b[0m      \u001b[32m156.9087\u001b[0m  0.4112\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m189.3718\u001b[0m      \u001b[32m181.1584\u001b[0m  0.4027\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m202.0487\u001b[0m      \u001b[32m192.6549\u001b[0m  0.3992\n",
      "      2       \u001b[36m49.0610\u001b[0m       48.1023  0.2471\n",
      "      2       \u001b[36m38.8837\u001b[0m       \u001b[32m39.0620\u001b[0m  0.2670\n",
      "      2       \u001b[36m45.9284\u001b[0m       45.9537  0.2484\n",
      "      2       \u001b[36m30.5270\u001b[0m       31.0809  0.2563\n",
      "      2       \u001b[36m48.2070\u001b[0m       47.8022  0.2433\n",
      "      2      \u001b[36m120.7341\u001b[0m      \u001b[32m122.5148\u001b[0m  0.3502\n",
      "      2      \u001b[36m153.4838\u001b[0m      157.8851  0.3159\n",
      "      2      \u001b[36m183.7633\u001b[0m      184.3733  0.3136\n",
      "      2      \u001b[36m193.5887\u001b[0m      \u001b[32m189.6911\u001b[0m  0.3360\n",
      "      3       \u001b[36m38.6170\u001b[0m       39.1458  0.2208\n",
      "      3       \u001b[36m30.1468\u001b[0m       30.9138  0.2194\n",
      "      3       \u001b[36m48.8244\u001b[0m       48.2640  0.2493\n",
      "      3       45.9361       \u001b[32m45.4895\u001b[0m  0.2442\n",
      "      2      \u001b[36m195.2619\u001b[0m      \u001b[32m192.6376\u001b[0m  0.3599\n",
      "      3       48.2668       47.7198  0.2491\n",
      "      4       \u001b[36m38.5381\u001b[0m       39.2957  0.2315\n",
      "      4       30.6479       31.0870  0.2675\n",
      "      3      120.7962      124.3565  0.3369\n",
      "      4       45.9783       45.7831  0.2479\n",
      "      3      153.8955      159.8972  0.3202\n",
      "      4       49.1620       48.5111  0.2609\n",
      "      3      \u001b[36m192.4689\u001b[0m      190.2845  0.3240\n",
      "      4       \u001b[36m48.1666\u001b[0m       48.2369  0.2530\n",
      "      3      \u001b[36m182.4445\u001b[0m      181.4848  0.3496\n",
      "      3      \u001b[36m194.8723\u001b[0m      \u001b[32m192.5620\u001b[0m  0.3109\n",
      "      5       38.5532       39.1229  0.2466\n",
      "      5       30.3831       31.0425  0.2937\n",
      "      5       49.2593       48.2556  0.2900\n",
      "      5       48.3317       48.3295  0.2938\n",
      "      5       46.0855       45.6753  0.3332\n",
      "      4      155.0490      157.9601  0.3521\n",
      "      4      120.7871      124.9040  0.3615\n",
      "      4      192.7777      \u001b[32m189.6423\u001b[0m  0.3480\n",
      "      4      182.7141      182.5919  0.3656\n",
      "      4      195.2032      193.1089  0.3541\n",
      "      6       \u001b[36m38.5168\u001b[0m       \u001b[32m38.8148\u001b[0m  0.2973\n",
      "      6       30.4920       \u001b[32m30.7692\u001b[0m  0.2555\n",
      "      6       48.9286       \u001b[32m48.0082\u001b[0m  0.2591\n",
      "      6       48.5674       48.2406  0.2712\n",
      "      6       \u001b[36m45.9144\u001b[0m       45.5814  0.2710\n",
      "      5      \u001b[36m120.0865\u001b[0m      124.9474  0.2935\n",
      "      5      154.0503      159.1720  0.3303\n",
      "      5      183.6718      184.4666  0.2950\n",
      "      5      192.7841      191.2673  0.3565\n",
      "      7       \u001b[36m38.4716\u001b[0m       39.0430  0.2605\n",
      "      5      195.8139      193.1516  0.3456\n",
      "      7       30.1579       30.8073  0.2326\n",
      "      7       \u001b[36m48.7707\u001b[0m       48.2287  0.2322\n",
      "      7       \u001b[36m45.6676\u001b[0m       45.5385  0.2582\n",
      "      7       48.4793       47.9693  0.2685\n",
      "      6      120.4127      125.0507  0.3173\n",
      "      8       38.5131       39.0018  0.2359\n",
      "      6      154.7748      157.9084  0.3154\n",
      "      8       30.3857       30.9049  0.2469\n",
      "      6      193.3489      191.4189  0.3064\n",
      "      6      184.2217      184.1250  0.3394\n",
      "      8       \u001b[36m48.7298\u001b[0m       48.1895  0.2447\n",
      "      8       \u001b[36m45.5585\u001b[0m       45.5033  0.2494\n",
      "      6      195.5590      193.9027  0.3502\n",
      "      8       \u001b[36m48.1430\u001b[0m       47.8327  0.2512\n",
      "      9       \u001b[36m38.4278\u001b[0m       38.9688  0.2416\n",
      "      9       48.8730       48.4375  0.2361\n",
      "      9       30.5157       31.0346  0.2570\n",
      "      7      153.5804      \u001b[32m156.3761\u001b[0m  0.2850\n",
      "      7      \u001b[36m119.2120\u001b[0m      124.0642  0.3667\n",
      "      7      192.8891      190.9875  0.3123\n",
      "      7      184.3851      184.1911  0.3099\n",
      "      9       48.1436       48.0483  0.2497\n",
      "      9       45.9514       45.7653  0.2713\n",
      "      7      194.9703      \u001b[32m192.1370\u001b[0m  0.3102\n",
      "     10       38.6932       39.4685  0.2416\n",
      "     10       48.8031       48.2395  0.2535\n",
      "     10       30.2558       30.9535  0.2479\n",
      "     10       \u001b[36m48.0771\u001b[0m       47.8232  0.2407\n",
      "     10       45.6288       45.6140  0.2338\n",
      "      8      154.0662      157.4231  0.3396\n",
      "      8      119.6812      124.8243  0.3114\n",
      "      8      192.6448      191.4897  0.3452\n",
      "      8      182.9421      181.3990  0.3418\n",
      "      8      194.9225      192.4134  0.3168\n",
      "     11       38.6224       39.0525  0.2678\n",
      "     11       48.7725       48.0465  0.2447\n",
      "     11       30.4035       31.0232  0.2587\n",
      "     11       45.8233       45.8528  0.2560\n",
      "     11       48.2392       48.0099  0.2726\n",
      "      9      153.8679      \u001b[32m156.2718\u001b[0m  0.3179\n",
      "      9      119.2921      124.5745  0.3203\n",
      "     12       38.6785       39.0262  0.2369\n",
      "      9      192.7044      190.5145  0.3375\n",
      "      9      \u001b[36m181.9334\u001b[0m      182.7705  0.3355\n",
      "     12       30.3214       31.0312  0.2348\n",
      "     12       48.8701       48.2719  0.2740\n",
      "      9      194.9321      192.2325  0.3098\n",
      "     12       48.1455       47.7725  0.2367\n",
      "     12       45.6590       45.9660  0.2475\n",
      "     13       38.5792       39.0647  0.2558\n",
      "     13       30.2859       31.2696  0.2354\n",
      "     10      \u001b[36m153.0710\u001b[0m      156.7944  0.3180\n",
      "     13       48.8797       48.1184  0.2397\n",
      "     10      120.3992      124.5664  0.3542\n",
      "     10      182.8890      183.2401  0.3109\n",
      "     10      193.6345      191.0019  0.3262\n",
      "     13       45.7622       46.1347  0.2442\n",
      "     13       48.2288       48.0025  0.2524\n",
      "     10      194.8958      \u001b[32m192.0235\u001b[0m  0.3271\n",
      "     14       \u001b[36m30.0573\u001b[0m       30.9511  0.2392\n",
      "     14       38.4910       39.0618  0.2607\n",
      "     14       48.8714       48.1902  0.2438\n",
      "     11      154.0995      156.6727  0.3109\n",
      "     14       45.7537       45.7866  0.2531\n",
      "     14       48.2258       47.8570  0.2683\n",
      "     11      119.3436      124.6686  0.3197\n",
      "     11      182.6462      182.6446  0.3371\n",
      "     11      193.2638      190.4200  0.3351\n",
      "     11      \u001b[36m194.8257\u001b[0m      \u001b[32m191.9746\u001b[0m  0.3269\n",
      "     15       30.3083       30.9453  0.2429\n",
      "     15       38.4660       39.1405  0.2423\n",
      "     15       48.8458       48.1120  0.2569\n",
      "     15       45.7906       45.9004  0.2238\n",
      "     15       48.2611       47.8578  0.2601\n",
      "     12      153.2423      157.1743  0.3321\n",
      "     12      119.6693      122.7665  0.3310\n",
      "     12      182.3611      \u001b[32m180.8933\u001b[0m  0.3256\n",
      "     16       30.1134       30.7817  0.2665\n",
      "     12      \u001b[36m193.9734\u001b[0m      \u001b[32m191.9337\u001b[0m  0.2991\n",
      "     16       \u001b[36m38.2535\u001b[0m       39.1029  0.2845\n",
      "     12      193.2620      190.3670  0.3823\n",
      "     16       48.9913       48.1384  0.2890\n",
      "     16       45.8488       45.6851  0.2845\n",
      "     16       48.3501       47.8629  0.2869\n",
      "     13      154.1369      \u001b[32m155.4910\u001b[0m  0.3043\n",
      "     17       30.2329       30.7916  0.2596\n",
      "     17       38.6516       39.0852  0.2604\n",
      "     13      119.7432      123.3313  0.3310\n",
      "     13      182.3870      \u001b[32m180.6372\u001b[0m  0.3169\n",
      "     13      194.4684      192.3642  0.3067\n",
      "     17       48.8006       48.2521  0.2839\n",
      "     17       45.6429       45.8332  0.2384\n",
      "     13      \u001b[36m192.3887\u001b[0m      190.7952  0.3139\n",
      "     17       48.2909       48.1621  0.2558\n",
      "     14      153.4425      \u001b[32m155.3814\u001b[0m  0.3181\n",
      "     18       38.6324       38.9074  0.2568\n",
      "     18       30.3984       30.9103  0.2768\n",
      "     18       48.8345       48.1938  0.2346\n",
      "     18       46.0040       45.9479  0.2489\n",
      "     14      \u001b[36m181.8495\u001b[0m      182.0509  0.3071\n",
      "     14      194.9958      192.5070  0.3144\n",
      "     14      119.6381      125.2530  0.3236\n",
      "     14      192.9072      191.3525  0.2967\n",
      "     18       48.2364       47.9992  0.2578\n",
      "     19       30.1518       31.1241  0.2266\n",
      "     19       38.7200       38.9666  0.2637\n",
      "     15      153.5704      155.3948  0.3001\n",
      "     19       48.8112       48.2189  0.2414\n",
      "     19       45.8706       45.9193  0.2797\n",
      "     15      119.7228      125.1261  0.3057\n",
      "     15      183.3108      184.7649  0.3169\n",
      "     19       48.4114       47.9147  0.2492\n",
      "     15      194.7964      192.3631  0.3324\n",
      "     15      193.3670      191.9097  0.2987\n",
      "     20       30.2556       31.0574  0.2395\n",
      "     20       38.5678       39.0853  0.2354\n",
      "     20       49.2690       48.1305  0.2431\n",
      "     20       46.0275       46.0669  0.2537\n",
      "     16      153.1422      156.1788  0.3312\n",
      "     20       48.3846       47.9623  0.2447\n",
      "     16      183.5978      184.4486  0.2845\n",
      "     16      194.6923      \u001b[32m191.7048\u001b[0m  0.3092\n",
      "     16      120.1192      124.1730  0.3459\n",
      "     21       30.4544       30.8317  0.2711\n",
      "     16      192.9898      191.4278  0.3170\n",
      "     21       38.7792       38.8360  0.2532\n",
      "     21       48.9248       48.0546  0.2391\n",
      "     21       45.9683       46.0712  0.2758\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 1.\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     17      153.2014      156.2698  0.3612\n",
      "     22       30.0859       30.7868  0.2441\n",
      "     22       38.6316       39.0956  0.2356\n",
      "     17      184.0531      184.5604  0.3108\n",
      "     22       48.7650       \u001b[32m47.9484\u001b[0m  0.2220\n",
      "     17      194.9953      193.2149  0.2919\n",
      "     17      192.6483      191.6971  0.2784\n",
      "     17      119.8401      123.5435  0.3057\n",
      "     22       45.8388       45.7264  0.2495\n",
      "     23       30.2300       30.8013  0.2196\n",
      "     23       \u001b[36m48.5828\u001b[0m       47.9743  0.2296\n",
      "     23       38.5907       38.8719  0.2458\n",
      "     18      153.2964      158.2386  0.2774\n",
      "     18      182.5426      182.6509  0.2985\n",
      "     18      192.8010      191.3589  0.2931\n",
      "     18      194.3980      191.8565  0.3007\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "     18      120.3915      123.4620  0.3142\n",
      "Restoring best model from epoch 3.\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     24       30.2076       30.7743  0.2152\n",
      "     24       48.8335       48.4148  0.2116\n",
      "     24       38.4390       39.0929  0.2212\n",
      "     19      153.0731      156.5157  0.2839\n",
      "     19      182.3335      181.8655  0.2736\n",
      "     19      \u001b[36m191.9549\u001b[0m      190.6338  0.2789\n",
      "     19      194.8152      191.8772  0.2761\n",
      "     19      \u001b[36m119.1591\u001b[0m      125.1331  0.2659\n",
      "     25       30.1099       30.7790  0.2088\n",
      "     25       48.9331       47.9933  0.2033\n",
      "     25       38.3525       39.0747  0.2097\n",
      "     20      153.1157      156.3661  0.2780\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 6.\n",
      "     20      182.4337      182.1849  0.2728\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     26       48.6353       48.0930  0.2088\n",
      "     20      194.4633      191.8854  0.2649\n",
      "     20      192.2838      191.6675  0.2724\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 6.\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     20      119.3349      124.8527  0.2717\n",
      "     27       48.8981       48.3949  0.2088\n",
      "     21      153.6036      156.4780  0.2669\n",
      "     21      182.4890      182.8612  0.2666\n",
      "     21      194.9636      192.4816  0.2676\n",
      "     21      192.4786      191.1902  0.2688\n",
      "     21      119.9604      124.5346  0.2734\n",
      "     28       48.9538       48.2952  0.2063\n",
      "     22      153.2742      156.4064  0.2666\n",
      "     22      182.5919      182.4486  0.2672\n",
      "     22      195.2127      193.4090  0.2667\n",
      "     22      192.2722      190.8812  0.2684\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 2.\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     29       48.8667       47.9804  0.2122\n",
      "     23      \u001b[36m153.0201\u001b[0m      156.3681  0.2690\n",
      "     23      182.8986      183.8720  0.2694\n",
      "     23      195.3573      194.0037  0.2665\n",
      "     23      192.1382      190.2300  0.2675\n",
      "     30       48.8399       48.2029  0.2073\n",
      "     24      \u001b[36m152.7179\u001b[0m      156.6526  0.2660\n",
      "     31       48.7896       48.0587  0.2089\n",
      "     24      183.0797      182.9522  0.2675\n",
      "     24      195.2569      192.1867  0.2662\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 4.\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     32       48.7925       48.0739  0.2062\n",
      "     25      153.1561      157.0853  0.2661\n",
      "     25      182.3554      182.5052  0.2650\n",
      "     25      195.5265      \u001b[32m191.0676\u001b[0m  0.2654\n",
      "     33       48.9813       48.1520  0.2062\n",
      "     26      154.1328      157.1207  0.2646\n",
      "     26      181.9013      182.6829  0.2655\n",
      "     26      194.7614      191.3531  0.2653\n",
      "     34       48.8974       48.3448  0.2070\n",
      "     27      153.1407      159.5391  0.2643\n",
      "     27      182.0951      \u001b[32m180.1936\u001b[0m  0.2660\n",
      "     35       48.8430       48.2295  0.2090\n",
      "     27      194.8260      192.7742  0.2656\n",
      "     36       48.8978       48.1329  0.2080\n",
      "     28      153.1719      157.5321  0.2641\n",
      "     28      \u001b[36m181.7364\u001b[0m      182.1982  0.2661\n",
      "     28      195.3209      193.2800  0.2664\n",
      "     37       49.2921       48.2699  0.2072\n",
      "     29      153.9042      155.9011  0.2654\n",
      "     29      181.8954      182.4688  0.2675\n",
      "     29      195.2706      191.8525  0.2681\n",
      "     38       49.1517       48.3603  0.2083\n",
      "     30      152.9369      158.1212  0.2662\n",
      "     30      181.8312      182.4012  0.2650\n",
      "     30      195.5985      192.9029  0.2645\n",
      "     39       48.9493       48.0942  0.2061\n",
      "     31      153.9488      159.0474  0.2645\n",
      "     40       48.7761       48.2060  0.2066\n",
      "     31      182.0926      182.5972  0.2655\n",
      "     31      195.1949      191.8250  0.2658\n",
      "     41       48.8556       48.0080  0.2078\n",
      "     32      154.1223      157.3013  0.2693\n",
      "     32      182.4160      182.1138  0.2721\n",
      "     32      194.4104      191.8226  0.2686\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 22.\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     33      153.2350      156.6589  0.2643\n",
      "     33      182.4324      183.4954  0.2640\n",
      "     33      194.6007      192.4037  0.2663\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 14.\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     34      182.7776      184.3408  0.2634\n",
      "     34      194.6196      192.4392  0.2645\n",
      "     35      182.7146      183.9502  0.2598\n",
      "     35      194.9542      192.1938  0.2560\n",
      "     36      182.0124      182.5274  0.2572\n",
      "     36      195.1050      193.2816  0.2566\n",
      "     37      182.5394      182.2242  0.2556\n",
      "     37      195.5192      192.3930  0.2581\n",
      "     38      182.3149      183.3717  0.2594\n",
      "     38      194.7265      192.7691  0.2586\n",
      "     39      182.1799      182.7286  0.2575\n",
      "     39      194.9396      194.0065  0.2540\n",
      "     40      182.8010      182.9217  0.2564\n",
      "     40      195.7788      192.3778  0.2574\n",
      "     41      182.6833      182.2772  0.2555\n",
      "     41      195.1335      192.0389  0.2596\n",
      "     42      182.5181      181.8651  0.2561\n",
      "     42      195.0608      191.4801  0.2590\n",
      "     43      182.1128      182.7164  0.2546\n",
      "     43      195.0252      191.9835  0.2575\n",
      "     44      182.9992      182.2240  0.2619\n",
      "     44      195.7132      193.0642  0.2626\n",
      "     45      181.9430      182.7569  0.2601\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 25.\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     46      \u001b[36m181.5975\u001b[0m      182.6387  0.2567\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 27.\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m43.7897\u001b[0m       \u001b[32m41.0187\u001b[0m  0.2997\n",
      "      2       \u001b[36m43.0591\u001b[0m       41.2633  0.2865\n",
      "      3       43.1744       \u001b[32m40.9817\u001b[0m  0.2864\n",
      "      4       43.1455       41.1850  0.2847\n",
      "      5       43.4053       41.7516  0.2870\n",
      "      6       43.3187       41.2338  0.2919\n",
      "      7       43.2938       41.6376  0.2731\n",
      "      8       43.2715       41.6939  0.2997\n",
      "      9       43.1119       41.4815  0.2887\n",
      "     10       43.3161       41.9008  0.2919\n",
      "     11       43.2933       41.5156  0.3120\n",
      "     12       \u001b[36m42.9247\u001b[0m       41.5237  0.3153\n",
      "     13       42.9603       41.2111  0.3328\n",
      "     14       43.0175       41.0708  0.3067\n",
      "     15       \u001b[36m42.8912\u001b[0m       41.1588  0.2805\n",
      "     16       43.0168       41.2378  0.2896\n",
      "     17       43.1899       41.1966  0.2919\n",
      "     18       42.9690       41.4347  0.2866\n",
      "     19       42.9055       41.4294  0.2805\n",
      "     20       43.0967       41.3733  0.2999\n",
      "     21       43.0358       \u001b[32m40.9801\u001b[0m  0.2873\n",
      "     22       43.1591       41.2196  0.2849\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 3.\n",
      "durations 1.0 5123.0\n",
      "dtype X_train age            float32\n",
      "sex           category\n",
      "sample_yr      float32\n",
      "kappa          float32\n",
      "lambda         float32\n",
      "flc_grp        float32\n",
      "creatinine     float32\n",
      "mgus_0.0         uint8\n",
      "mgus_1.0         uint8\n",
      "dtype: object\n",
      "dtype y_train float64\n",
      "dtype X_test age            float32\n",
      "sex           category\n",
      "sample_yr      float32\n",
      "kappa          float32\n",
      "lambda         float32\n",
      "flc_grp        float32\n",
      "creatinine     float32\n",
      "mgus_0.0         uint8\n",
      "mgus_1.0         uint8\n",
      "dtype: object\n",
      "dtype y_test float32\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m49.1995\u001b[0m       \u001b[32m47.8558\u001b[0m  0.2205\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m30.9839\u001b[0m       \u001b[32m31.2924\u001b[0m  0.2507\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m46.9062\u001b[0m       \u001b[32m45.3850\u001b[0m  0.2492\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m40.0177\u001b[0m       \u001b[32m37.6629\u001b[0m  0.2694\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m49.7308\u001b[0m       \u001b[32m48.6052\u001b[0m  0.2679\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m124.1032\u001b[0m      \u001b[32m121.6090\u001b[0m  0.3031\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m159.9938\u001b[0m      \u001b[32m150.8512\u001b[0m  0.3214\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m199.4181\u001b[0m      \u001b[32m194.8959\u001b[0m  0.3118\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m188.1240\u001b[0m      \u001b[32m181.8411\u001b[0m  0.3513\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m198.0265\u001b[0m      \u001b[32m191.2923\u001b[0m  0.3458\n",
      "      2       \u001b[36m47.8215\u001b[0m       48.1677  0.2607\n",
      "      2       \u001b[36m39.0945\u001b[0m       37.7462  0.2387\n",
      "      2       \u001b[36m45.7121\u001b[0m       45.4725  0.2559\n",
      "      2       \u001b[36m30.5890\u001b[0m       \u001b[32m30.3286\u001b[0m  0.2839\n",
      "      2       \u001b[36m48.5558\u001b[0m       48.9702  0.2606\n",
      "      2      \u001b[36m122.4658\u001b[0m      \u001b[32m121.3007\u001b[0m  0.3155\n",
      "      2      \u001b[36m154.6664\u001b[0m      \u001b[32m150.4548\u001b[0m  0.3164\n",
      "      2      \u001b[36m190.4305\u001b[0m      \u001b[32m190.8871\u001b[0m  0.3049\n",
      "      2      \u001b[36m193.4409\u001b[0m      \u001b[32m194.2654\u001b[0m  0.3273\n",
      "      2      \u001b[36m182.2614\u001b[0m      \u001b[32m181.1966\u001b[0m  0.3217\n",
      "      3       47.8914       \u001b[32m47.7801\u001b[0m  0.2399\n",
      "      3       \u001b[36m38.9768\u001b[0m       \u001b[32m37.5587\u001b[0m  0.2377\n",
      "      3       45.7262       45.6769  0.2671\n",
      "      3       \u001b[36m48.4557\u001b[0m       48.7422  0.2535\n",
      "      3       \u001b[36m30.3106\u001b[0m       30.4724  0.2680\n",
      "      3      \u001b[36m120.5542\u001b[0m      121.8718  0.3310\n",
      "      3      154.7280      \u001b[32m150.3352\u001b[0m  0.3152\n",
      "      4       39.0685       37.6796  0.2428\n",
      "      4       47.9655       48.2453  0.2636\n",
      "      4       \u001b[36m45.7053\u001b[0m       45.4656  0.2367\n",
      "      3      182.5123      181.2015  0.3068\n",
      "      3      \u001b[36m192.2397\u001b[0m      194.5385  0.3203\n",
      "      4       \u001b[36m48.3598\u001b[0m       \u001b[32m48.5842\u001b[0m  0.2260\n",
      "      3      \u001b[36m190.3474\u001b[0m      191.0679  0.3312\n",
      "      4       \u001b[36m30.1595\u001b[0m       30.9586  0.2407\n",
      "      5       39.0892       38.1884  0.2294\n",
      "      5       47.9571       \u001b[32m47.6581\u001b[0m  0.2577\n",
      "      5       48.3940       \u001b[32m48.2444\u001b[0m  0.2492\n",
      "      4      \u001b[36m120.4448\u001b[0m      121.4961  0.3055\n",
      "      5       \u001b[36m45.6088\u001b[0m       45.4359  0.2762\n",
      "      5       30.4438       30.4381  0.2603\n",
      "      4      \u001b[36m154.0018\u001b[0m      150.4242  0.3195\n",
      "      4      \u001b[36m182.0350\u001b[0m      181.2944  0.3204\n",
      "      4      191.0457      191.1785  0.3161\n",
      "      4      193.6211      \u001b[32m193.0933\u001b[0m  0.3593\n",
      "      6       38.9831       37.6543  0.2769\n",
      "      6       47.8911       47.8507  0.2563\n",
      "      6       \u001b[36m45.4336\u001b[0m       \u001b[32m45.3523\u001b[0m  0.2463\n",
      "      6       \u001b[36m30.0924\u001b[0m       30.4440  0.2458\n",
      "      6       48.4906       48.5478  0.2892\n",
      "      5      121.2640      122.9762  0.3140\n",
      "      5      154.7778      151.5834  0.3165\n",
      "      5      182.1532      \u001b[32m181.0855\u001b[0m  0.3017\n",
      "      5      194.7686      196.5019  0.2822\n",
      "      5      190.3825      192.1422  0.3346\n",
      "      7       \u001b[36m38.7921\u001b[0m       \u001b[32m37.5181\u001b[0m  0.2549\n",
      "      7       45.5239       45.5377  0.2246\n",
      "      7       \u001b[36m47.6517\u001b[0m       47.9918  0.2519\n",
      "      7       48.4586       48.3538  0.2384\n",
      "      7       30.2355       30.5596  0.2662\n",
      "      6      121.7683      122.1182  0.3026\n",
      "      6      154.6591      152.1708  0.3236\n",
      "      6      \u001b[36m181.9547\u001b[0m      181.3627  0.3212\n",
      "      6      190.5236      192.0766  0.2951\n",
      "      8       \u001b[36m38.7334\u001b[0m       37.8388  0.2345\n",
      "      6      194.9303      195.5150  0.3282\n",
      "      8       45.7150       45.5762  0.2340\n",
      "      8       47.8082       48.1736  0.2545\n",
      "      8       \u001b[36m48.3359\u001b[0m       48.4367  0.2685\n",
      "      8       30.1750       30.4907  0.2611\n",
      "      7      \u001b[36m120.3450\u001b[0m      121.7905  0.3141\n",
      "      9       38.9829       37.5747  0.2343\n",
      "      7      \u001b[36m181.8155\u001b[0m      181.2935  0.2994\n",
      "      9       47.9553       47.7048  0.2468\n",
      "      7      154.9202      \u001b[32m150.1133\u001b[0m  0.3258\n",
      "      9       45.6314       45.4572  0.2737\n",
      "      7      190.5141      191.7381  0.3087\n",
      "      9       \u001b[36m48.2725\u001b[0m       48.2701  0.2422\n",
      "      7      194.1467      196.1173  0.3344\n",
      "      9       30.1325       30.5809  0.2586\n",
      "     10       38.8455       38.1449  0.2546\n",
      "     10       45.5919       45.5882  0.2358\n",
      "     10       47.7388       47.7851  0.2636\n",
      "      8      \u001b[36m119.7518\u001b[0m      122.5133  0.3196\n",
      "      8      \u001b[36m181.4437\u001b[0m      182.7464  0.2967\n",
      "     10       \u001b[36m30.0879\u001b[0m       30.5556  0.2316\n",
      "      8      \u001b[36m153.8872\u001b[0m      \u001b[32m149.4743\u001b[0m  0.3222\n",
      "     10       48.3306       48.3965  0.2818\n",
      "      8      \u001b[36m190.3156\u001b[0m      191.1560  0.3206\n",
      "      8      193.0982      194.4634  0.2994\n",
      "     11       39.4677       38.0550  0.2276\n",
      "     11       45.5585       45.6103  0.2465\n",
      "     11       47.7693       47.8596  0.2813\n",
      "      9      119.9376      122.0542  0.2830\n",
      "     11       30.3270       30.3839  0.2541\n",
      "     11       48.4231       48.4021  0.2538\n",
      "      9      154.0715      150.6100  0.3007\n",
      "      9      183.0161      182.9623  0.3298\n",
      "     12       39.2994       37.6905  0.2300\n",
      "      9      \u001b[36m189.7743\u001b[0m      191.1647  0.3567\n",
      "      9      192.5290      193.7040  0.3562\n",
      "     12       45.5896       45.4267  0.2442\n",
      "     12       47.7520       48.0438  0.2378\n",
      "     12       30.2264       30.4701  0.2352\n",
      "     12       48.3989       48.4379  0.2509\n",
      "     10      120.9494      \u001b[32m121.1614\u001b[0m  0.3058\n",
      "     10      154.9821      150.7438  0.2847\n",
      "     10      182.2320      181.3447  0.2996\n",
      "     13       39.4913       37.8003  0.2597\n",
      "     13       45.5510       45.5075  0.2639\n",
      "     10      \u001b[36m192.1697\u001b[0m      193.2981  0.2947\n",
      "     10      190.4126      191.4069  0.3406\n",
      "     13       \u001b[36m30.0779\u001b[0m       30.3923  0.2499\n",
      "     13       48.2831       48.4540  0.2184\n",
      "     13       47.8747       47.8533  0.3032\n",
      "     11      119.9862      \u001b[32m120.6870\u001b[0m  0.2856\n",
      "     14       39.1452       37.6891  0.2355\n",
      "     11      154.5801      151.7967  0.3044\n",
      "     14       45.5032       45.3705  0.2379\n",
      "     11      182.3410      181.4072  0.3391\n",
      "     11      \u001b[36m191.8451\u001b[0m      193.7191  0.2995\n",
      "     14       \u001b[36m47.6248\u001b[0m       47.8345  0.2216\n",
      "     14       30.1220       30.3862  0.2756\n",
      "     14       \u001b[36m48.1573\u001b[0m       48.3942  0.2630\n",
      "     11      190.2925      191.9980  0.2991\n",
      "     15       38.8930       37.7168  0.2510\n",
      "     12      120.1042      121.7553  0.3223\n",
      "     15       45.8948       45.6859  0.2462\n",
      "     12      155.1653      151.7257  0.3289\n",
      "     15       47.8993       47.9919  0.2419\n",
      "     15       48.4432       48.6542  0.2404\n",
      "     12      181.4990      181.6773  0.2991\n",
      "     15       30.2142       30.5273  0.2555\n",
      "     12      190.0931      191.4310  0.3217\n",
      "     12      192.0778      193.3536  0.3598\n",
      "     16       38.8665       37.8725  0.2717\n",
      "     16       46.1513       45.6843  0.2603\n",
      "     16       48.3679       48.8696  0.2139\n",
      "     13      120.2086      122.3305  0.3315\n",
      "     16       47.7173       \u001b[32m47.6548\u001b[0m  0.2478\n",
      "     16       \u001b[36m30.0530\u001b[0m       30.4620  0.2571\n",
      "     13      154.6673      151.5007  0.3111\n",
      "     13      181.8496      181.9418  0.3291\n",
      "     17       38.7979       38.0269  0.2572\n",
      "     13      190.7097      191.1623  0.3215\n",
      "     13      191.9331      195.4809  0.3217\n",
      "     17       48.7585       48.6904  0.2232\n",
      "     17       45.9695       45.4238  0.2580\n",
      "     17       47.7865       47.7082  0.2533\n",
      "     17       \u001b[36m29.9345\u001b[0m       30.4895  0.2525\n",
      "     14      \u001b[36m119.4497\u001b[0m      121.8002  0.3189\n",
      "     14      155.4657      150.3697  0.3183\n",
      "     18       39.0064       37.7035  0.2364\n",
      "     14      181.8948      181.8239  0.3083\n",
      "     18       48.4949       48.7995  0.2248\n",
      "     18       45.5456       45.6605  0.2438\n",
      "     18       47.7103       47.7813  0.2373\n",
      "     14      190.0081      191.6475  0.3457\n",
      "     18       29.9986       30.5038  0.2457\n",
      "     14      192.9044      194.5075  0.3428\n",
      "     15      120.5850      122.7161  0.2925\n",
      "     19       38.8491       37.8369  0.2365\n",
      "     19       48.3887       48.7427  0.2439\n",
      "     15      154.6386      149.8340  0.3066\n",
      "     19       45.5677       45.7179  0.2525\n",
      "     19       47.8485       48.0931  0.2132\n",
      "     15      \u001b[36m181.2752\u001b[0m      181.9861  0.3263\n",
      "     19       30.1189       30.4765  0.2638\n",
      "     15      \u001b[36m189.5736\u001b[0m      193.0433  0.3138\n",
      "     15      193.1273      194.9407  0.3144\n",
      "     20       38.7711       37.5214  0.2352\n",
      "     20       48.3945       48.4379  0.2534\n",
      "     16      120.4490      122.6707  0.3135\n",
      "     20       47.7841       47.7200  0.2327\n",
      "     20       45.6063       \u001b[32m45.3024\u001b[0m  0.2468\n",
      "     16      154.1074      149.7514  0.3103\n",
      "     20       30.1331       30.6046  0.2644\n",
      "     16      181.7964      181.1883  0.3454\n",
      "     21       38.7967       37.9205  0.2547\n",
      "     21       47.9219       48.0047  0.2127\n",
      "     16      190.1626      191.2953  0.3223\n",
      "     16      192.5856      \u001b[32m193.0845\u001b[0m  0.3141\n",
      "     21       48.4629       \u001b[32m48.2408\u001b[0m  0.2470\n",
      "     17      122.1009      121.7390  0.3128\n",
      "     21       45.7360       45.6919  0.3250\n",
      "     17      154.4614      150.2822  0.2946\n",
      "     21       30.0964       30.4577  0.2403\n",
      "     22       39.0178       37.5854  0.2315\n",
      "     17      181.6267      \u001b[32m181.0535\u001b[0m  0.2791\n",
      "     22       47.8300       47.8455  0.2444\n",
      "     22       48.4371       48.8076  0.2531\n",
      "     17      189.8624      \u001b[32m190.5993\u001b[0m  0.2879\n",
      "     17      192.6081      \u001b[32m193.0716\u001b[0m  0.3231\n",
      "     22       46.0393       45.9448  0.2564\n",
      "     18      121.0662      121.9002  0.3057\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 2.\n",
      "     18      154.5764      150.2869  0.2879\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     23       38.9307       37.6373  0.2733\n",
      "     23       47.8656       48.0259  0.2356\n",
      "     23       48.6832       48.6504  0.2369\n",
      "     18      181.9991      181.6743  0.3280\n",
      "     23       45.9320       45.3649  0.2157\n",
      "     18      189.7233      \u001b[32m190.5156\u001b[0m  0.3029\n",
      "     18      192.7456      195.2389  0.2909\n",
      "     24       38.7683       37.6777  0.2245\n",
      "     19      120.6666      121.2024  0.3048\n",
      "     24       47.7042       48.1765  0.2343\n",
      "     24       48.5779       48.3621  0.2226\n",
      "     19      \u001b[36m153.1442\u001b[0m      \u001b[32m149.4315\u001b[0m  0.3232\n",
      "     19      181.5021      181.2952  0.2780\n",
      "     24       45.6287       45.4166  0.2278\n",
      "     19      \u001b[36m189.5236\u001b[0m      190.9992  0.3112\n",
      "     19      195.0371      196.3701  0.2887\n",
      "     25       38.8979       37.7034  0.2285\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 5.\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 5.\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     20      120.1798      122.0801  0.3120\n",
      "     20      153.2003      150.4612  0.2738\n",
      "     25       45.6999       45.4646  0.2110\n",
      "     20      181.3734      \u001b[32m180.8925\u001b[0m  0.2795\n",
      "     26       \u001b[36m38.6378\u001b[0m       \u001b[32m37.4888\u001b[0m  0.2145\n",
      "     20      190.5020      192.7109  0.2767\n",
      "     20      193.4502      \u001b[32m192.7402\u001b[0m  0.2776\n",
      "     26       45.6807       45.3644  0.2123\n",
      "     21      120.8728      123.9864  0.2711\n",
      "     21      153.8471      150.1760  0.2696\n",
      "     27       38.8761       37.5841  0.2135\n",
      "     21      181.5747      181.5562  0.2696\n",
      "     27       45.5940       45.4124  0.2098\n",
      "     21      190.6410      192.6434  0.2689\n",
      "     21      192.4494      193.5310  0.2676\n",
      "     22      121.0419      122.4786  0.2825\n",
      "     28       38.7166       37.8563  0.2168\n",
      "     22      154.6538      \u001b[32m149.3391\u001b[0m  0.2745\n",
      "     22      \u001b[36m180.7697\u001b[0m      181.0610  0.2770\n",
      "     28       45.7277       45.4393  0.2224\n",
      "     22      190.6317      191.4719  0.2787\n",
      "     22      192.3484      193.8523  0.2806\n",
      "     29       38.7923       37.6031  0.2105\n",
      "     23      154.7098      151.2089  0.2675\n",
      "     23      120.3747      121.6850  0.2845\n",
      "     29       45.5420       45.3688  0.2183\n",
      "     23      181.4137      182.0400  0.2678\n",
      "     30       38.9276       37.6446  0.2168\n",
      "     23      191.9973      193.3160  0.2686\n",
      "     23      190.3793      192.4582  0.2735\n",
      "     24      154.5205      151.5049  0.2691\n",
      "     30       45.6741       45.4041  0.2116\n",
      "     24      119.9459      120.9225  0.2715\n",
      "     24      181.2719      181.8117  0.2653\n",
      "     31       38.9941       37.6965  0.2108\n",
      "     24      190.8645      192.7490  0.2665\n",
      "     24      192.2356      193.7351  0.2687\n",
      "     31       45.4721       \u001b[32m45.2916\u001b[0m  0.2101\n",
      "     25      155.9484      154.0668  0.2697\n",
      "     25      119.5049      121.9452  0.2716\n",
      "     32       39.1190       37.7250  0.2102\n",
      "     25      181.9629      \u001b[32m180.8899\u001b[0m  0.2686\n",
      "     32       45.8674       45.3784  0.2111\n",
      "     25      191.6111      194.0804  0.2686\n",
      "     25      192.4927      194.7498  0.2686\n",
      "     33       39.3811       37.7205  0.2109\n",
      "     26      155.9026      152.4772  0.2673\n",
      "     26      119.9800      121.7515  0.2721\n",
      "     26      181.6005      180.8915  0.2683\n",
      "     33       45.7025       45.4155  0.2122\n",
      "     26      190.0016      191.1395  0.2691\n",
      "     26      192.6463      194.6270  0.2691\n",
      "     34       39.4496       \u001b[32m37.3541\u001b[0m  0.2107\n",
      "     27      155.5489      151.0509  0.2689\n",
      "     27      120.6285      121.6583  0.2716\n",
      "     34       45.6761       45.4306  0.2112\n",
      "     27      181.1606      182.1731  0.2721\n",
      "     35       39.0034       38.0707  0.2151\n",
      "     27      190.8055      192.6818  0.2717\n",
      "     27      192.6364      194.4174  0.2707\n",
      "     35       45.6026       45.3164  0.2189\n",
      "     28      155.6403      151.5990  0.2779\n",
      "     28      121.1608      122.1791  0.2803\n",
      "     28      181.6617      181.7682  0.2704\n",
      "     36       38.7875       37.6964  0.2132\n",
      "     28      191.5133      192.8766  0.2686\n",
      "     28      191.8511      192.9493  0.2698\n",
      "     36       45.5447       45.3406  0.2115\n",
      "     29      155.1661      151.2759  0.2688\n",
      "     37       38.8479       37.4160  0.2106\n",
      "     29      119.6265      122.2791  0.2715\n",
      "     29      181.0128      182.0383  0.2677\n",
      "     37       45.6227       45.3756  0.2114\n",
      "     29      191.5942      191.2281  0.2687\n",
      "     29      \u001b[36m191.7882\u001b[0m      \u001b[32m192.4098\u001b[0m  0.2675\n",
      "     38       38.8298       37.5537  0.2130\n",
      "     30      155.0622      150.6820  0.2665\n",
      "     30      120.7834      122.0811  0.2694\n",
      "     30      180.7733      181.3584  0.2674\n",
      "     38       45.5943       45.4647  0.2105\n",
      "     30      191.3217      \u001b[32m190.2516\u001b[0m  0.2674\n",
      "     30      192.2162      193.3726  0.2691\n",
      "     39       38.8067       37.5624  0.2124\n",
      "     31      154.0855      149.3567  0.2681\n",
      "     39       45.6584       45.5558  0.2124\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 11.\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     31      \u001b[36m180.5055\u001b[0m      181.8735  0.2674\n",
      "     40       38.9200       38.2132  0.2130\n",
      "     31      190.4388      190.9924  0.2689\n",
      "     31      192.0565      195.2808  0.2685\n",
      "     40       45.5814       45.3670  0.2108\n",
      "     32      154.7400      150.4613  0.2677\n",
      "     32      181.0759      181.7862  0.2668\n",
      "     41       39.2748       37.6855  0.2107\n",
      "     32      190.5515      191.4349  0.2692\n",
      "     32      193.0076      195.9385  0.2676\n",
      "     41       45.6523       45.4679  0.2130\n",
      "     33      154.3227      151.7479  0.2718\n",
      "     42       39.0693       37.7257  0.2179\n",
      "     33      181.4891      181.0496  0.2735\n",
      "     42       45.5895       45.4308  0.2157\n",
      "     33      192.8833      195.2207  0.2719\n",
      "     33      190.8537      191.3444  0.2727\n",
      "     43       38.7366       37.5188  0.2171\n",
      "     34      154.8574      152.1260  0.2730\n",
      "     34      181.4028      180.9103  0.2735\n",
      "     43       45.5096       45.4802  0.2182\n",
      "     34      193.2568      194.4237  0.2691\n",
      "     34      191.4924      193.0089  0.2696\n",
      "     44       38.8743       37.5528  0.2146\n",
      "     35      155.4770      150.1577  0.2688\n",
      "     44       45.7405       45.5947  0.2091\n",
      "     35      181.5762      181.3282  0.2704\n",
      "     45       38.8760       37.4609  0.2141\n",
      "     35      192.8462      194.6176  0.2665\n",
      "     35      192.7952      194.2493  0.2667\n",
      "     45       \u001b[36m45.4209\u001b[0m       45.3628  0.2110\n",
      "     36      155.2572      \u001b[32m149.3037\u001b[0m  0.2663\n",
      "     46       38.8242       37.9618  0.2086\n",
      "     36      180.6820      182.0318  0.2665\n",
      "     46       45.6406       45.5808  0.2089\n",
      "     36      192.2187      193.1466  0.2665\n",
      "     36      191.9926      194.1347  0.2670\n",
      "     37      154.8938      149.8611  0.2664\n",
      "     47       38.8116       37.6846  0.2088\n",
      "     37      182.2053      181.6034  0.2675\n",
      "     47       45.6161       45.5358  0.2106\n",
      "     37      191.8851      193.7025  0.2646\n",
      "     37      192.3788      193.5367  0.2664\n",
      "     48       38.6532       37.7935  0.2118\n",
      "     38      155.0104      151.0092  0.2668\n",
      "     48       45.4416       45.4603  0.2102\n",
      "     38      181.0519      181.2077  0.2682\n",
      "     49       38.6543       37.9783  0.2086\n",
      "     38      191.6704      193.1741  0.2661\n",
      "     38      192.4469      194.0899  0.2667\n",
      "     39      154.9976      149.7257  0.2649\n",
      "     49       45.4839       45.4520  0.2098\n",
      "     39      \u001b[36m180.3683\u001b[0m      181.3990  0.2676\n",
      "     50       38.8893       38.0898  0.2145\n",
      "Restoring best model from epoch 34.\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     39      191.5387      192.1643  0.2671\n",
      "     39      192.7665      192.9168  0.2663\n",
      "     50       45.4263       45.3991  0.2091\n",
      "Restoring best model from epoch 31.\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     40      154.6838      151.3245  0.2676\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 20.\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     40      190.7140      192.0286  0.2650\n",
      "     40      191.8160      193.2606  0.2669\n",
      "     41      154.7852      152.6687  0.2642\n",
      "     41      190.5713      190.3737  0.2629\n",
      "     41      192.2412      193.4496  0.2632\n",
      "     42      154.9865      151.8856  0.2664\n",
      "     42      190.2752      190.5553  0.2628\n",
      "     42      192.2843      193.6727  0.2646\n",
      "     43      154.8384      150.7593  0.2647\n",
      "     43      190.2687      190.9468  0.2663\n",
      "     43      192.3481      193.8078  0.2626\n",
      "     44      154.5543      150.5637  0.2654\n",
      "     44      190.3421      191.0547  0.2623\n",
      "     44      191.9114      193.7476  0.2630\n",
      "     45      154.6767      150.8308  0.2643\n",
      "     45      189.8665      191.1834  0.2639\n",
      "     45      192.7871      194.4121  0.2645\n",
      "     46      154.8429      151.0840  0.2646\n",
      "     46      193.2402      193.9378  0.2630\n",
      "     46      190.2099      \u001b[32m190.0175\u001b[0m  0.2640\n",
      "     47      154.9602      152.9554  0.2635\n",
      "     47      192.6624      193.2477  0.2653\n",
      "     47      190.0780      191.3619  0.2653\n",
      "     48      154.2607      152.5016  0.2649\n",
      "     48      190.3469      191.1322  0.2636\n",
      "     48      191.8120      193.2033  0.2644\n",
      "     49      153.9546      153.0274  0.2654\n",
      "     49      190.4339      190.3746  0.2639\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 29.\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     50      154.7426      151.5398  0.2636\n",
      "Restoring best model from epoch 36.\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     50      190.1671      190.7148  0.2589\n",
      "Restoring best model from epoch 46.\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m43.3662\u001b[0m       \u001b[32m42.7188\u001b[0m  0.3891\n",
      "      2       \u001b[36m42.3799\u001b[0m       \u001b[32m42.6505\u001b[0m  0.4086\n",
      "      3       42.3911       42.9208  0.3029\n",
      "      4       \u001b[36m42.3758\u001b[0m       43.2174  0.2696\n",
      "      5       42.5579       42.7276  0.2613\n",
      "      6       \u001b[36m42.3476\u001b[0m       42.8374  0.2915\n",
      "      7       \u001b[36m42.3392\u001b[0m       \u001b[32m42.6101\u001b[0m  0.2721\n",
      "      8       42.4159       42.9149  0.2654\n",
      "      9       42.3482       \u001b[32m42.5382\u001b[0m  0.2712\n",
      "     10       42.4608       42.7481  0.2661\n",
      "     11       \u001b[36m42.3353\u001b[0m       42.8394  0.2640\n",
      "     12       42.6592       43.0281  0.2711\n",
      "     13       42.3777       42.6539  0.2841\n",
      "     14       \u001b[36m42.2778\u001b[0m       42.7384  0.2647\n",
      "     15       \u001b[36m42.2761\u001b[0m       42.7708  0.2721\n",
      "     16       42.7959       42.9040  0.2743\n",
      "     17       42.4434       42.8017  0.2672\n",
      "     18       42.3757       42.7536  0.2687\n",
      "     19       42.4108       42.9139  0.2678\n",
      "     20       42.3038       43.0066  0.2715\n",
      "     21       42.5542       42.7266  0.2653\n",
      "     22       42.6805       42.7185  0.2639\n",
      "     23       42.4689       42.9041  0.2718\n",
      "     24       42.4903       42.9505  0.2657\n",
      "     25       42.3786       42.6809  0.2816\n",
      "     26       42.3146       42.8863  0.2660\n",
      "     27       42.5460       42.9378  0.2699\n",
      "     28       42.3641       42.7013  0.2654\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 9.\n",
      "durations 1.0 5166.0\n",
      "dtype X_train age            float32\n",
      "sex           category\n",
      "sample_yr      float32\n",
      "kappa          float32\n",
      "lambda         float32\n",
      "flc_grp        float32\n",
      "creatinine     float32\n",
      "mgus_0.0         uint8\n",
      "mgus_1.0         uint8\n",
      "dtype: object\n",
      "dtype y_train float64\n",
      "dtype X_test age            float32\n",
      "sex           category\n",
      "sample_yr      float32\n",
      "kappa          float32\n",
      "lambda         float32\n",
      "flc_grp        float32\n",
      "creatinine     float32\n",
      "mgus_0.0         uint8\n",
      "mgus_1.0         uint8\n",
      "dtype: object\n",
      "dtype y_test float32\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m30.6393\u001b[0m       \u001b[32m31.6273\u001b[0m  0.2327\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m49.2055\u001b[0m       \u001b[32m47.7717\u001b[0m  0.2652\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m39.2239\u001b[0m       \u001b[32m40.3197\u001b[0m  0.2867\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m46.6115\u001b[0m       \u001b[32m45.7970\u001b[0m  0.2834\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m49.5276\u001b[0m       \u001b[32m47.8638\u001b[0m  0.2961\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m156.1158\u001b[0m      \u001b[32m160.4814\u001b[0m  0.3329\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m120.7646\u001b[0m      \u001b[32m126.9459\u001b[0m  0.3873\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m187.4621\u001b[0m      \u001b[32m180.9134\u001b[0m  0.3589\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m196.8917\u001b[0m      \u001b[32m189.3972\u001b[0m  0.3344\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m200.2730\u001b[0m      \u001b[32m194.1735\u001b[0m  0.3248\n",
      "      2       \u001b[36m29.9740\u001b[0m       31.9516  0.2656\n",
      "      2       \u001b[36m47.6707\u001b[0m       \u001b[32m47.1966\u001b[0m  0.2433\n",
      "      2       \u001b[36m38.0583\u001b[0m       \u001b[32m39.9223\u001b[0m  0.2477\n",
      "      2       \u001b[36m45.8265\u001b[0m       \u001b[32m45.2444\u001b[0m  0.2550\n",
      "      2       \u001b[36m48.4543\u001b[0m       \u001b[32m47.6998\u001b[0m  0.2670\n",
      "      2      \u001b[36m151.6149\u001b[0m      \u001b[32m159.5885\u001b[0m  0.3167\n",
      "      2      \u001b[36m190.0386\u001b[0m      191.2424  0.3001\n",
      "      3       47.7149       47.4796  0.2297\n",
      "      2      \u001b[36m118.7999\u001b[0m      128.8212  0.3284\n",
      "      2      \u001b[36m181.8289\u001b[0m      \u001b[32m180.7902\u001b[0m  0.3210\n",
      "      3       \u001b[36m29.9632\u001b[0m       31.9017  0.2671\n",
      "      3       \u001b[36m38.0129\u001b[0m       \u001b[32m39.9016\u001b[0m  0.2493\n",
      "      2      \u001b[36m195.3503\u001b[0m      \u001b[32m191.2419\u001b[0m  0.3152\n",
      "      3       \u001b[36m45.7708\u001b[0m       46.3871  0.2439\n",
      "      3       \u001b[36m48.2074\u001b[0m       47.9976  0.2654\n",
      "      4       47.6783       48.0520  0.2632\n",
      "      3      151.6597      159.9814  0.3270\n",
      "      4       45.9528       45.8659  0.2665\n",
      "      4       \u001b[36m37.9573\u001b[0m       40.0665  0.2889\n",
      "      3      \u001b[36m189.9705\u001b[0m      189.7643  0.3285\n",
      "      4       \u001b[36m29.7597\u001b[0m       32.3329  0.3182\n",
      "      3      119.1096      132.5037  0.3292\n",
      "      3      \u001b[36m181.3786\u001b[0m      182.2825  0.3487\n",
      "      4       \u001b[36m48.1299\u001b[0m       47.7324  0.2778\n",
      "      3      \u001b[36m193.7416\u001b[0m      193.8996  0.3710\n",
      "      5       48.0434       47.6046  0.2419\n",
      "      5       \u001b[36m45.6687\u001b[0m       \u001b[32m45.1915\u001b[0m  0.2239\n",
      "      5       38.1962       40.4287  0.2394\n",
      "      5       \u001b[36m29.6940\u001b[0m       31.9945  0.2666\n",
      "      4      151.7090      160.2022  0.3108\n",
      "      5       48.6072       48.4545  0.2359\n",
      "      4      181.6670      181.5612  0.2931\n",
      "      4      191.0731      191.2211  0.3337\n",
      "      4      118.8249      127.9673  0.3259\n",
      "      4      \u001b[36m192.0617\u001b[0m      \u001b[32m191.1185\u001b[0m  0.3257\n",
      "      6       \u001b[36m47.5927\u001b[0m       47.4191  0.2239\n",
      "      6       \u001b[36m45.6159\u001b[0m       45.4120  0.2467\n",
      "      6       38.1072       40.6389  0.2417\n",
      "      6       29.7264       31.7171  0.2344\n",
      "      6       48.5076       47.9263  0.2616\n",
      "      5      \u001b[36m150.4803\u001b[0m      160.2477  0.3387\n",
      "      5      181.4354      180.9530  0.3066\n",
      "      5      119.1786      127.4447  0.2970\n",
      "      5      190.0938      190.4468  0.3106\n",
      "      7       \u001b[36m47.5636\u001b[0m       47.8647  0.2410\n",
      "      5      192.3761      191.4862  0.3004\n",
      "      7       \u001b[36m37.9438\u001b[0m       40.0121  0.2304\n",
      "      7       \u001b[36m45.5401\u001b[0m       45.5328  0.2480\n",
      "      7       29.8363       31.7446  0.2457\n",
      "      7       48.4849       47.7601  0.2359\n",
      "      8       47.8949       47.3313  0.2580\n",
      "      6      \u001b[36m181.0784\u001b[0m      182.3174  0.2919\n",
      "      8       \u001b[36m37.8319\u001b[0m       40.2435  0.2361\n",
      "      6      \u001b[36m189.5768\u001b[0m      \u001b[32m188.3524\u001b[0m  0.2973\n",
      "      6      151.7661      160.3672  0.3264\n",
      "      8       45.5611       45.9880  0.2487\n",
      "      6      120.3967      \u001b[32m126.3726\u001b[0m  0.3422\n",
      "      8       29.9461       31.8801  0.2573\n",
      "      6      192.6290      192.1255  0.3062\n",
      "      8       48.4784       47.9261  0.2314\n",
      "      9       \u001b[36m47.4995\u001b[0m       \u001b[32m46.9692\u001b[0m  0.2393\n",
      "      9       37.9348       \u001b[32m39.7982\u001b[0m  0.2329\n",
      "      9       45.5813       \u001b[32m45.1172\u001b[0m  0.2538\n",
      "      9       29.9786       31.7634  0.2362\n",
      "      7      152.3555      160.2503  0.3008\n",
      "      7      181.3001      181.9894  0.3249\n",
      "      9       48.4122       48.0576  0.2468\n",
      "      7      \u001b[36m189.2056\u001b[0m      188.7232  0.3294\n",
      "      7      118.8370      \u001b[32m126.1552\u001b[0m  0.3090\n",
      "      7      \u001b[36m191.5955\u001b[0m      191.4226  0.3173\n",
      "     10       38.2340       40.0501  0.2321\n",
      "     10       47.7501       48.2439  0.2706\n",
      "     10       \u001b[36m29.5444\u001b[0m       31.8762  0.2184\n",
      "     10       \u001b[36m45.5131\u001b[0m       \u001b[32m45.0269\u001b[0m  0.2627\n",
      "     10       48.4556       \u001b[32m47.3965\u001b[0m  0.2500\n",
      "      8      \u001b[36m180.9277\u001b[0m      180.9097  0.3093\n",
      "      8      189.6435      190.1649  0.2972\n",
      "      8      151.5121      160.4981  0.3351\n",
      "      8      \u001b[36m118.2804\u001b[0m      126.8494  0.3119\n",
      "      8      192.1678      191.4437  0.3050\n",
      "     11       38.1569       39.9514  0.2507\n",
      "     11       47.8520       47.3554  0.2638\n",
      "     11       29.6320       31.7958  0.2559\n",
      "     11       45.7491       45.8231  0.2628\n",
      "     11       48.5013       48.4459  0.2619\n",
      "      9      181.1548      182.3630  0.3059\n",
      "      9      189.4815      190.1142  0.3255\n",
      "      9      152.1963      159.5974  0.3214\n",
      "     12       \u001b[36m47.4307\u001b[0m       47.2953  0.2251\n",
      "     12       37.9500       40.0076  0.2608\n",
      "      9      118.5871      127.6667  0.3319\n",
      "      9      192.0756      191.5554  0.3165\n",
      "     12       29.7844       31.7682  0.2640\n",
      "     12       45.5427       45.1291  0.2511\n",
      "     12       48.3090       47.7414  0.2469\n",
      "     10      181.7921      181.4300  0.2720\n",
      "     13       47.7624       47.4702  0.2573\n",
      "     13       37.9411       39.8897  0.2599\n",
      "     13       29.6328       31.8258  0.2415\n",
      "     10      152.1867      \u001b[32m159.4766\u001b[0m  0.3226\n",
      "     10      189.9989      189.1965  0.3328\n",
      "     13       45.5290       45.1443  0.2575\n",
      "     10      118.7453      129.4446  0.3211\n",
      "     13       48.2254       47.6189  0.2296\n",
      "     10      192.9156      191.3398  0.3149\n",
      "     14       \u001b[36m37.7127\u001b[0m       40.0001  0.2244\n",
      "     14       47.5277       47.2447  0.2386\n",
      "     11      \u001b[36m180.5303\u001b[0m      \u001b[32m179.6413\u001b[0m  0.3022\n",
      "     14       29.8212       31.9116  0.2512\n",
      "     14       48.2405       47.6182  0.2418\n",
      "     14       45.6513       45.5923  0.2601\n",
      "     11      \u001b[36m118.1230\u001b[0m      127.8245  0.2816\n",
      "     11      189.6473      189.5217  0.3301\n",
      "     11      150.8991      \u001b[32m158.8037\u001b[0m  0.3380\n",
      "     11      192.1356      191.3474  0.3309\n",
      "     15       38.0365       40.0344  0.2339\n",
      "     15       47.5684       47.2490  0.2452\n",
      "     15       29.6415       32.7235  0.2348\n",
      "     15       48.1655       47.7143  0.2176\n",
      "     15       45.8831       45.6667  0.2402\n",
      "     12      \u001b[36m180.4639\u001b[0m      180.4801  0.3173\n",
      "     12      \u001b[36m117.5510\u001b[0m      \u001b[32m126.1271\u001b[0m  0.3001\n",
      "     16       47.6556       47.4507  0.2168\n",
      "     16       38.2335       40.0868  0.2394\n",
      "     12      191.7985      192.5720  0.2879\n",
      "     12      190.3368      192.6979  0.3296\n",
      "     12      150.8046      160.2582  0.3397\n",
      "     16       48.2739       47.9868  0.2331\n",
      "     16       29.7512       31.9390  0.2457\n",
      "     16       45.6974       45.1732  0.2541\n",
      "     13      180.7449      180.4341  0.3226\n",
      "     17       47.4592       47.4702  0.2494\n",
      "     17       37.7639       40.4798  0.2429\n",
      "     13      118.7085      127.6461  0.3128\n",
      "     13      191.0372      192.2015  0.2954\n",
      "     17       48.4924       47.9876  0.2407\n",
      "     13      \u001b[36m191.1391\u001b[0m      191.6639  0.3119\n",
      "     13      152.1143      160.1727  0.2935\n",
      "     17       29.5683       31.7738  0.2433\n",
      "     17       \u001b[36m45.4502\u001b[0m       45.4506  0.2575\n",
      "     18       38.0161       39.8524  0.2301\n",
      "     18       47.6869       47.3801  0.2501\n",
      "     14      180.8431      180.3147  0.3181\n",
      "     18       29.7795       31.8167  0.2205\n",
      "     18       48.3450       47.8513  0.2331\n",
      "     14      119.4082      126.8935  0.3175\n",
      "     14      192.3165      192.6793  0.2985\n",
      "     18       \u001b[36m45.4337\u001b[0m       45.3953  0.2433\n",
      "     14      190.0934      \u001b[32m188.2598\u001b[0m  0.3232\n",
      "     14      151.8613      159.1657  0.3136\n",
      "     19       38.1217       40.0021  0.2324\n",
      "     19       47.7646       47.6197  0.2273\n",
      "     19       29.7211       31.8374  0.2404\n",
      "     19       48.3157       48.4702  0.2600\n",
      "     15      180.6487      181.1427  0.3188\n",
      "     19       45.8364       45.1424  0.2312\n",
      "     15      118.8284      131.0393  0.3092\n",
      "     15      189.8978      188.2617  0.2996\n",
      "     20       38.0943       39.9239  0.2272\n",
      "     15      151.8510      159.9769  0.3054\n",
      "     20       \u001b[36m47.4017\u001b[0m       47.0928  0.2487\n",
      "     15      194.5712      196.8151  0.3807\n",
      "     20       29.6589       31.7341  0.2518\n",
      "     20       48.3407       47.9694  0.2505\n",
      "     20       45.4782       45.2608  0.2391\n",
      "     16      180.9473      181.5639  0.2789\n",
      "     21       38.0360       40.1160  0.2608\n",
      "     16      118.6409      130.7404  0.3056\n",
      "     21       47.8388       47.1269  0.2474\n",
      "     16      189.3238      189.6175  0.3020\n",
      "     16      152.3570      158.9644  0.3024\n",
      "     16      194.7593      195.4618  0.2893\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 1.\n",
      "     21       48.3089       47.7056  0.2377\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     21       45.6285       45.0406  0.2637\n",
      "     17      180.5381      180.9189  0.2925\n",
      "     22       38.5214       39.8378  0.2383\n",
      "     22       47.8281       47.7340  0.2236\n",
      "     17      119.6753      130.6711  0.3165\n",
      "     17      151.1293      159.2906  0.2887\n",
      "     17      189.4999      188.8642  0.2992\n",
      "     22       48.4709       47.5808  0.2450\n",
      "     17      195.1815      194.7308  0.2842\n",
      "     22       \u001b[36m45.3921\u001b[0m       45.7237  0.2322\n",
      "     23       38.0520       39.9054  0.2144\n",
      "     23       48.0839       47.2869  0.2141\n",
      "     18      \u001b[36m180.4512\u001b[0m      \u001b[32m179.5258\u001b[0m  0.2935\n",
      "     23       48.2145       47.7442  0.2317\n",
      "     18      120.0040      126.9296  0.2898\n",
      "     18      189.8077      188.6767  0.2887\n",
      "     23       45.4130       45.4508  0.2355\n",
      "     18      151.9460      160.0005  0.3233\n",
      "     18      194.8358      194.3526  0.3101\n",
      "     24       47.7547       47.3922  0.2391\n",
      "     24       38.2579       40.4296  0.2495\n",
      "     24       48.4324       48.1040  0.2142\n",
      "     19      180.7307      179.7159  0.2875\n",
      "     24       45.5017       45.3517  0.2170\n",
      "     19      190.2650      189.7605  0.2706\n",
      "     19      119.4356      127.6449  0.2871\n",
      "     25       47.8590       47.1452  0.2222\n",
      "     25       38.4325       40.5203  0.2380\n",
      "     19      151.0801      159.2360  0.2902\n",
      "     19      193.2319      192.0415  0.2873\n",
      "     25       48.5030       47.9702  0.2401\n",
      "     25       45.6165       45.2794  0.2267\n",
      "     20      180.7810      180.4304  0.2814\n",
      "     26       38.0488       39.8545  0.2155\n",
      "     26       47.6705       47.1833  0.2572\n",
      "     20      118.8789      127.1540  0.2801\n",
      "     20      190.6535      189.2038  0.2927\n",
      "     20      150.9532      161.0222  0.2862\n",
      "     26       48.2159       48.2929  0.2076\n",
      "     20      193.0348      191.6699  0.2741\n",
      "     26       45.5595       45.3344  0.2438\n",
      "     27       38.3089       39.8986  0.2199\n",
      "     21      180.8250      180.9902  0.3034\n",
      "     27       47.6578       47.3145  0.2309\n",
      "     21      118.6668      127.8983  0.2815\n",
      "     27       48.3330       47.5862  0.2266\n",
      "     21      189.9746      189.8799  0.2847\n",
      "     21      152.2811      159.9944  0.2916\n",
      "     21      192.4646      \u001b[32m190.7753\u001b[0m  0.2805\n",
      "     27       45.4516       45.2908  0.2362\n",
      "     28       38.1248       40.4398  0.2462\n",
      "     28       47.6379       47.2715  0.2205\n",
      "     22      \u001b[36m180.0290\u001b[0m      180.6957  0.2998\n",
      "     28       48.3637       47.9133  0.2262\n",
      "     22      189.2906      190.1229  0.2784\n",
      "     22      119.0782      127.9371  0.2954\n",
      "     28       45.6932       45.1956  0.2194\n",
      "     22      191.9925      191.6478  0.2918\n",
      "     22      152.2211      159.0851  0.2986\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 9.\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 9.\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     29       48.2419       48.4778  0.2343\n",
      "     23      180.5910      180.0964  0.2722\n",
      "     29       45.4520       45.3269  0.2125\n",
      "     23      119.4272      127.0465  0.2673\n",
      "     23      189.3160      189.1833  0.2708\n",
      "     23      150.7291      159.9949  0.2665\n",
      "     23      193.4854      194.1252  0.2719\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 10.\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 10.\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     24      180.4649      181.7231  0.2630\n",
      "     24      190.5943      190.5447  0.2646\n",
      "     24      119.3278      126.9723  0.2662\n",
      "     24      193.4354      191.7417  0.2635\n",
      "     24      151.1073      160.4542  0.2642\n",
      "     25      181.0267      182.3492  0.2647\n",
      "     25      190.2113      189.4313  0.2558\n",
      "     25      117.9677      126.8232  0.2634\n",
      "     25      151.4566      159.6432  0.2649\n",
      "     25      193.1912      192.8805  0.2674\n",
      "     26      182.2823      184.1367  0.2627\n",
      "     26      189.3447      189.1080  0.2620\n",
      "     26      118.7592      126.2721  0.2651\n",
      "     26      150.9023      \u001b[32m158.6740\u001b[0m  0.2617\n",
      "     26      194.9407      191.9896  0.2619\n",
      "     27      181.9248      183.0373  0.2594\n",
      "     27      189.6939      189.2176  0.2640\n",
      "     27      118.6378      126.7084  0.2610\n",
      "     27      151.9496      158.7484  0.2634\n",
      "     27      193.4624      191.5695  0.2642\n",
      "     28      181.1793      181.3355  0.2632\n",
      "     28      \u001b[36m189.2030\u001b[0m      190.6246  0.2599\n",
      "     28      118.3623      127.8580  0.2637\n",
      "     28      153.1064      160.0752  0.2562\n",
      "     28      191.9120      191.5930  0.2617\n",
      "     29      181.5795      181.0248  0.2632\n",
      "     29      190.5072      190.4194  0.2637\n",
      "     29      117.9841      126.7534  0.2630\n",
      "     29      153.1502      160.1516  0.2631\n",
      "     29      191.9935      191.8609  0.2584\n",
      "     30      181.6925      181.4892  0.2641\n",
      "     30      189.3046      188.5988  0.2635\n",
      "     30      118.2721      127.8464  0.2622\n",
      "     30      152.9521      159.3483  0.2632\n",
      "     30      192.1488      191.0689  0.2605\n",
      "     31      181.4611      181.6908  0.2610\n",
      "     31      190.4174      \u001b[32m187.7677\u001b[0m  0.2593\n",
      "     31      118.2417      129.2909  0.2638\n",
      "     31      151.6572      158.9003  0.2632\n",
      "     31      192.6471      193.5298  0.2656\n",
      "     32      181.4097      181.2718  0.2645\n",
      "     32      189.9579      189.1158  0.2627\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 12.\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     32      151.8821      \u001b[32m158.5127\u001b[0m  0.2603\n",
      "     32      195.5957      193.3559  0.2658\n",
      "     33      180.6114      180.2172  0.2643\n",
      "     33      189.2924      188.8974  0.2649\n",
      "     33      151.5878      159.5417  0.2636\n",
      "     33      194.9394      192.1522  0.2650\n",
      "     34      180.9558      180.3208  0.2662\n",
      "     34      190.0390      \u001b[32m186.9269\u001b[0m  0.2650\n",
      "     34      151.5076      \u001b[32m158.4839\u001b[0m  0.2634\n",
      "     34      193.3452      192.2719  0.2663\n",
      "     35      180.3919      180.6947  0.2632\n",
      "     35      189.4790      \u001b[32m186.2004\u001b[0m  0.2627\n",
      "     35      150.9209      158.7131  0.2646\n",
      "     35      192.8919      191.7129  0.2655\n",
      "     36      180.7122      180.6703  0.2644\n",
      "     36      190.1470      189.1386  0.2673\n",
      "     36      152.0542      160.2317  0.2636\n",
      "     36      192.3636      192.2323  0.2647\n",
      "     37      \u001b[36m179.9868\u001b[0m      180.2139  0.2646\n",
      "     37      190.4229      188.4844  0.2651\n",
      "     37      152.8379      160.4597  0.2633\n",
      "     37      193.4006      191.2526  0.2681\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 18.\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     38      190.0688      188.0810  0.2659\n",
      "     38      153.2342      161.4325  0.2657\n",
      "     38      192.8139      191.9593  0.2625\n",
      "     39      189.7116      188.4762  0.2619\n",
      "     39      153.4606      161.4759  0.2615\n",
      "     39      192.4576      191.4994  0.2633\n",
      "     40      189.5215      188.9687  0.2633\n",
      "     40      153.2214      159.4601  0.2638\n",
      "     40      192.9488      191.8117  0.2630\n",
      "     41      190.0689      188.2760  0.2625\n",
      "     41      152.2203      159.8062  0.2628\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 21.\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     42      189.6847      189.0451  0.2572\n",
      "     42      152.2839      159.0595  0.2542\n",
      "     43      189.5688      189.9912  0.2538\n",
      "     43      151.3021      159.1654  0.2547\n",
      "     44      189.8389      188.4529  0.2533\n",
      "     44      150.4878      159.2315  0.2533\n",
      "     45      189.6288      188.4210  0.2537\n",
      "     45      151.0668      159.9269  0.2549\n",
      "     46      190.0536      188.0516  0.2545\n",
      "     46      151.2014      159.9072  0.2556\n",
      "     47      189.6338      187.6057  0.2555\n",
      "     47      151.1503      159.7254  0.2564\n",
      "     48      189.5422      188.4390  0.2529\n",
      "     48      150.5664      \u001b[32m158.4271\u001b[0m  0.2529\n",
      "     49      \u001b[36m188.9156\u001b[0m      188.0657  0.2531\n",
      "     49      151.6364      \u001b[32m158.3635\u001b[0m  0.2549\n",
      "     50      189.0349      187.8818  0.2532\n",
      "Restoring best model from epoch 35.\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     50      151.4826      158.7277  0.2529\n",
      "Restoring best model from epoch 49.\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m42.9803\u001b[0m       \u001b[32m43.0027\u001b[0m  0.2748\n",
      "      2       \u001b[36m42.1719\u001b[0m       43.1058  0.2698\n",
      "      3       \u001b[36m41.9773\u001b[0m       \u001b[32m42.6751\u001b[0m  0.2731\n",
      "      4       42.3612       43.2728  0.2670\n",
      "      5       42.0823       42.7959  0.2579\n",
      "      6       42.2153       42.7755  0.2737\n",
      "      7       42.0679       42.7643  0.2682\n",
      "      8       42.1907       42.9906  0.2748\n",
      "      9       42.1710       43.1233  0.2722\n",
      "     10       42.1935       42.8092  0.2621\n",
      "     11       42.2061       42.9338  0.2650\n",
      "     12       42.1453       43.1585  0.2734\n",
      "     13       42.1837       42.9323  0.2539\n",
      "     14       42.3141       42.8330  0.2675\n",
      "     15       42.7159       43.7850  0.2729\n",
      "     16       43.0002       43.3434  0.2695\n",
      "     17       42.4502       43.0160  0.2643\n",
      "     18       42.1658       \u001b[32m42.6335\u001b[0m  0.3235\n",
      "     19       42.1234       42.7941  0.2725\n",
      "     20       42.1436       \u001b[32m42.5177\u001b[0m  0.2626\n",
      "     21       42.1692       42.8705  0.2667\n",
      "     22       42.1404       42.6390  0.2659\n",
      "     23       41.9905       42.7102  0.2645\n",
      "     24       42.0938       42.6726  0.2698\n",
      "     25       42.0324       42.6842  0.2635\n",
      "     26       \u001b[36m41.8756\u001b[0m       42.6353  0.2639\n",
      "     27       41.9337       42.7507  0.2805\n",
      "     28       42.2677       42.8557  0.2654\n",
      "     29       42.1155       42.8655  0.2705\n",
      "     30       41.9359       42.8392  0.2650\n",
      "     31       42.1575       43.1574  0.2630\n",
      "     32       42.0152       42.8710  0.2666\n",
      "     33       \u001b[36m41.8534\u001b[0m       42.7894  0.2665\n",
      "     34       41.9943       42.6085  0.2628\n",
      "     35       42.0033       42.6828  0.2659\n",
      "     36       42.1440       42.7819  0.2737\n",
      "     37       42.1528       42.7064  0.2618\n",
      "     38       42.1763       43.1070  0.2644\n",
      "     39       42.1779       42.7428  0.2686\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 20.\n",
      "durations 1.0 5171.0\n",
      "dtype X_train age            float32\n",
      "sex           category\n",
      "sample_yr      float32\n",
      "kappa          float32\n",
      "lambda         float32\n",
      "flc_grp        float32\n",
      "creatinine     float32\n",
      "mgus_0.0         uint8\n",
      "mgus_1.0         uint8\n",
      "dtype: object\n",
      "dtype y_train float64\n",
      "dtype X_test age            float32\n",
      "sex           category\n",
      "sample_yr      float32\n",
      "kappa          float32\n",
      "lambda         float32\n",
      "flc_grp        float32\n",
      "creatinine     float32\n",
      "mgus_0.0         uint8\n",
      "mgus_1.0         uint8\n",
      "dtype: object\n",
      "dtype y_test float32\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m39.7443\u001b[0m       \u001b[32m40.2362\u001b[0m  0.2600\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m30.7891\u001b[0m       \u001b[32m30.6145\u001b[0m  0.2580\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m49.4405\u001b[0m       \u001b[32m48.8802\u001b[0m  0.2497\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m49.3075\u001b[0m       \u001b[32m48.9076\u001b[0m  0.2652\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m46.7681\u001b[0m       \u001b[32m46.0976\u001b[0m  0.2728\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m122.1951\u001b[0m      \u001b[32m121.5735\u001b[0m  0.3161\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m189.5718\u001b[0m      \u001b[32m184.7876\u001b[0m  0.3337\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m198.3885\u001b[0m      \u001b[32m197.4945\u001b[0m  0.3157\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m158.3488\u001b[0m      \u001b[32m160.3050\u001b[0m  0.3655\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m200.6518\u001b[0m      \u001b[32m195.7357\u001b[0m  0.3265\n",
      "      2       \u001b[36m48.1536\u001b[0m       \u001b[32m48.6505\u001b[0m  0.2197\n",
      "      2       \u001b[36m38.7776\u001b[0m       40.4379  0.2547\n",
      "      2       \u001b[36m48.5530\u001b[0m       \u001b[32m48.6810\u001b[0m  0.2431\n",
      "      2       \u001b[36m45.5519\u001b[0m       46.1963  0.2383\n",
      "      2       \u001b[36m29.9985\u001b[0m       30.7472  0.2569\n",
      "      2      \u001b[36m119.0740\u001b[0m      122.3145  0.3264\n",
      "      2      \u001b[36m192.8380\u001b[0m      \u001b[32m196.3755\u001b[0m  0.2986\n",
      "      2      \u001b[36m184.0078\u001b[0m      \u001b[32m183.9283\u001b[0m  0.3202\n",
      "      2      \u001b[36m154.5264\u001b[0m      163.3457  0.3073\n",
      "      2      \u001b[36m195.2599\u001b[0m      195.8808  0.3175\n",
      "      3       45.9842       46.5276  0.2284\n",
      "      3       \u001b[36m38.6388\u001b[0m       \u001b[32m39.9180\u001b[0m  0.2341\n",
      "      3       \u001b[36m47.9437\u001b[0m       48.7666  0.2579\n",
      "      3       \u001b[36m48.4788\u001b[0m       48.8813  0.2461\n",
      "      3       \u001b[36m29.9818\u001b[0m       \u001b[32m30.2962\u001b[0m  0.2538\n",
      "      3      \u001b[36m118.4462\u001b[0m      122.8065  0.2917\n",
      "      4       \u001b[36m38.5202\u001b[0m       40.6714  0.2354\n",
      "      4       46.0245       46.6501  0.2422\n",
      "      4       48.4848       48.8473  0.2338\n",
      "      4       48.2685       49.0132  0.2406\n",
      "      3      \u001b[36m183.1517\u001b[0m      \u001b[32m183.7830\u001b[0m  0.3077\n",
      "      3      \u001b[36m192.7087\u001b[0m      \u001b[32m196.2440\u001b[0m  0.3309\n",
      "      4       \u001b[36m29.5422\u001b[0m       30.5030  0.2421\n",
      "      3      \u001b[36m153.8052\u001b[0m      \u001b[32m159.1063\u001b[0m  0.3261\n",
      "      3      \u001b[36m194.8830\u001b[0m      197.7380  0.3432\n",
      "      5       38.7827       40.1768  0.2302\n",
      "      5       48.5102       48.7897  0.2420\n",
      "      5       46.0596       47.0920  0.2530\n",
      "      5       47.9504       \u001b[32m48.6064\u001b[0m  0.2580\n",
      "      5       30.1570       30.6026  0.2468\n",
      "      4      119.3962      122.4431  0.3201\n",
      "      4      \u001b[36m191.9390\u001b[0m      \u001b[32m194.0514\u001b[0m  0.2879\n",
      "      4      183.4507      184.5412  0.2963\n",
      "      4      154.9627      161.7826  0.3110\n",
      "      4      \u001b[36m193.8769\u001b[0m      \u001b[32m194.2241\u001b[0m  0.2968\n",
      "      6       \u001b[36m48.4470\u001b[0m       49.4141  0.2372\n",
      "      6       38.7851       40.1297  0.2600\n",
      "      6       46.0254       46.5310  0.2376\n",
      "      6       48.0012       48.9321  0.2382\n",
      "      6       29.8497       \u001b[32m30.2187\u001b[0m  0.2528\n",
      "      5      184.2540      185.7574  0.2978\n",
      "      5      119.9523      123.0122  0.3259\n",
      "      5      \u001b[36m191.7937\u001b[0m      194.2378  0.3221\n",
      "      5      \u001b[36m192.8965\u001b[0m      195.6903  0.2963\n",
      "      5      \u001b[36m153.1600\u001b[0m      162.1487  0.3481\n",
      "      7       48.6740       49.2279  0.2372\n",
      "      7       38.9397       40.1251  0.2390\n",
      "      7       45.8479       46.2428  0.2357\n",
      "      7       \u001b[36m47.8781\u001b[0m       48.7897  0.2525\n",
      "      7       29.9625       30.5957  0.2372\n",
      "      6      119.0232      \u001b[32m121.1229\u001b[0m  0.3010\n",
      "      6      184.0362      184.7985  0.3132\n",
      "      6      191.8573      194.2577  0.3046\n",
      "      8       45.7927       46.1108  0.2379\n",
      "      8       38.6730       40.4342  0.2439\n",
      "      8       \u001b[36m48.3803\u001b[0m       49.2175  0.2541\n",
      "      6      193.0943      194.5513  0.3147\n",
      "      8       \u001b[36m47.8658\u001b[0m       48.7290  0.2388\n",
      "      6      153.6128      160.6246  0.3070\n",
      "      8       29.8808       31.1434  0.2419\n",
      "      9       45.9589       46.3593  0.2481\n",
      "      9       48.4081       48.7829  0.2352\n",
      "      9       38.6011       40.1506  0.2444\n",
      "      7      \u001b[36m118.0271\u001b[0m      121.5407  0.3078\n",
      "      7      191.8069      195.1830  0.3094\n",
      "      7      183.2292      186.1590  0.3262\n",
      "      9       \u001b[36m47.8456\u001b[0m       48.7206  0.2514\n",
      "      9       29.7922       30.7378  0.2525\n",
      "      7      193.1091      195.9334  0.3080\n",
      "      7      153.4315      \u001b[32m158.2972\u001b[0m  0.3090\n",
      "     10       \u001b[36m48.2734\u001b[0m       48.8724  0.2365\n",
      "     10       \u001b[36m38.4976\u001b[0m       \u001b[32m39.9097\u001b[0m  0.2389\n",
      "     10       45.8156       46.4136  0.2621\n",
      "     10       47.8535       48.6626  0.2268\n",
      "     10       29.9792       30.5022  0.2592\n",
      "      8      \u001b[36m182.5472\u001b[0m      184.1821  0.2976\n",
      "      8      118.0356      122.8964  0.3375\n",
      "      8      \u001b[36m191.7866\u001b[0m      195.4854  0.3327\n",
      "      8      193.8634      195.7563  0.3045\n",
      "      8      153.8151      162.7558  0.3123\n",
      "     11       48.3717       48.8404  0.2498\n",
      "     11       \u001b[36m38.4683\u001b[0m       39.9181  0.2540\n",
      "     11       46.0562       46.1383  0.2448\n",
      "     11       47.8871       48.7686  0.2450\n",
      "     11       29.7302       30.5572  0.2307\n",
      "      9      119.3907      124.5625  0.2928\n",
      "      9      182.7657      185.7643  0.3180\n",
      "      9      192.5304      196.1227  0.3211\n",
      "      9      193.8690      \u001b[32m194.1389\u001b[0m  0.3043\n",
      "      9      154.2731      162.6491  0.3104\n",
      "     12       48.5043       48.9746  0.2434\n",
      "     12       38.5425       40.0331  0.2367\n",
      "     12       47.9446       \u001b[32m48.4052\u001b[0m  0.2353\n",
      "     12       45.9532       46.6104  0.2584\n",
      "     12       29.5800       30.4300  0.2530\n",
      "     10      119.5478      121.8418  0.2900\n",
      "     13       \u001b[36m47.7795\u001b[0m       48.7854  0.2144\n",
      "     13       38.6876       40.0138  0.2276\n",
      "     13       \u001b[36m48.1037\u001b[0m       \u001b[32m48.6423\u001b[0m  0.2554\n",
      "     10      191.9763      196.0188  0.3063\n",
      "     13       45.6866       46.4413  0.2334\n",
      "     10      194.0649      194.6683  0.3036\n",
      "     10      183.3309      184.5956  0.3692\n",
      "     10      154.6875      159.2335  0.2967\n",
      "     13       29.5700       30.8328  0.2383\n",
      "     14       38.8649       40.7871  0.2279\n",
      "     14       47.9822       48.6182  0.2440\n",
      "     14       48.4072       48.9320  0.2462\n",
      "     14       46.2382       47.1002  0.2413\n",
      "     14       29.6161       30.4812  0.2339\n",
      "     11      119.3605      124.7680  0.3302\n",
      "     11      \u001b[36m191.2730\u001b[0m      195.6148  0.3093\n",
      "     11      153.4349      160.6447  0.2936\n",
      "     11      193.6431      195.9028  0.3208\n",
      "     11      183.6579      185.5778  0.3224\n",
      "     15       38.6716       40.5571  0.2331\n",
      "     15       47.9225       48.8514  0.2203\n",
      "     15       46.0708       46.3964  0.2353\n",
      "     15       29.7053       30.3791  0.2421\n",
      "     15       48.4093       48.9702  0.2791\n",
      "     12      119.8873      122.2058  0.3121\n",
      "     12      191.4548      195.8731  0.3159\n",
      "     12      \u001b[36m153.0364\u001b[0m      159.1225  0.3030\n",
      "     12      193.9060      195.1494  0.3180\n",
      "     12      184.5827      186.4506  0.3056\n",
      "     16       38.5065       40.0142  0.2378\n",
      "     16       47.9106       48.5596  0.2513\n",
      "     16       46.0809       46.9950  0.2376\n",
      "     16       29.8007       30.4984  0.2515\n",
      "     16       48.3039       49.1777  0.2629\n",
      "     13      118.3887      122.3379  0.3146\n",
      "     13      191.4991      195.3068  0.2882\n",
      "     17       38.5409       39.9215  0.2472\n",
      "     17       48.1081       48.8033  0.2436\n",
      "     13      153.0566      160.1741  0.3099\n",
      "     17       46.1623       46.4550  0.2284\n",
      "     13      193.0489      195.8147  0.3036\n",
      "     13      183.2946      185.2404  0.3180\n",
      "     17       29.7669       30.2964  0.2382\n",
      "     17       48.5181       48.8278  0.2904\n",
      "     18       38.7093       40.2434  0.2321\n",
      "     18       48.3186       48.8863  0.2350\n",
      "     18       45.7853       46.2285  0.2431\n",
      "     14      119.1652      125.0291  0.3091\n",
      "     14      191.7489      195.1955  0.3048\n",
      "     18       29.8121       30.3433  0.2324\n",
      "     14      194.0337      194.8708  0.3045\n",
      "     14      \u001b[36m152.8717\u001b[0m      161.9553  0.3180\n",
      "     14      182.8934      186.1230  0.3220\n",
      "     18       48.3272       48.8676  0.2612\n",
      "     19       38.4741       40.5394  0.2435\n",
      "     19       47.7914       48.4321  0.2290\n",
      "     19       45.9862       46.4606  0.2362\n",
      "     19       29.8127       31.0897  0.2305\n",
      "     15      119.7155      122.2909  0.3149\n",
      "     15      192.0322      194.6730  0.3131\n",
      "     15      \u001b[36m192.7765\u001b[0m      195.4087  0.2910\n",
      "     19       48.2542       48.7134  0.2395\n",
      "     15      153.9445      160.4701  0.3423\n",
      "     20       47.8059       48.7784  0.2407\n",
      "     20       38.5715       40.1239  0.2477\n",
      "     20       45.9204       46.1178  0.2133\n",
      "     15      183.0377      184.3283  0.3595\n",
      "     20       30.0608       31.4514  0.2558\n",
      "     20       48.1666       48.8627  0.2427\n",
      "     16      191.3664      195.8289  0.3051\n",
      "     16      118.1451      \u001b[32m120.3844\u001b[0m  0.3160\n",
      "     16      194.1333      195.8639  0.3175\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 1.\n",
      "     21       \u001b[36m47.7556\u001b[0m       48.6417  0.2368\n",
      "     21       38.5428       40.0779  0.2430\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     16      154.1129      160.5119  0.3031\n",
      "     21       30.0433       30.2843  0.2471\n",
      "     16      182.7681      184.7454  0.3193\n",
      "     21       48.3372       49.1206  0.2554\n",
      "     22       47.7664       48.8109  0.2155\n",
      "     17      191.6818      195.4858  0.2667\n",
      "     22       38.8532       40.3183  0.2452\n",
      "     17      118.1576      121.9419  0.2910\n",
      "     17      193.0366      194.6662  0.2965\n",
      "     22       29.6870       30.3844  0.2100\n",
      "     17      153.7998      162.2420  0.2911\n",
      "     17      182.6119      185.1485  0.2910\n",
      "     23       47.8616       48.8303  0.2134\n",
      "     22       48.3938       48.9451  0.2415\n",
      "     23       38.7555       39.9734  0.2202\n",
      "     23       29.8468       30.4893  0.2049\n",
      "     18      192.8544      195.9351  0.2941\n",
      "     18      118.0745      122.1990  0.2865\n",
      "     18      193.9027      195.6182  0.2876\n",
      "     18      154.5278      162.4686  0.2933\n",
      "     24       48.0293       49.0475  0.2341\n",
      "     23       48.3266       48.6920  0.2315\n",
      "     18      182.5782      185.6291  0.2925\n",
      "     24       38.5919       40.0562  0.2331\n",
      "     24       \u001b[36m29.5411\u001b[0m       30.4434  0.2315\n",
      "     19      192.5926      195.1587  0.2917\n",
      "     19      118.2304      122.4476  0.3010\n",
      "     19      194.1388      194.1946  0.2835\n",
      "     25       48.0836       48.6444  0.2217\n",
      "     24       48.1469       48.9429  0.2227\n",
      "     19      154.3254      162.4861  0.2879\n",
      "     25       \u001b[36m38.3401\u001b[0m       40.5553  0.2323\n",
      "     25       29.7212       30.7412  0.2326\n",
      "     19      \u001b[36m182.2106\u001b[0m      185.8773  0.3007\n",
      "     20      192.1722      194.2377  0.2826\n",
      "     26       47.9528       48.8428  0.2370\n",
      "     25       \u001b[36m48.1011\u001b[0m       48.9172  0.2202\n",
      "     20      194.3220      194.5935  0.2795\n",
      "     20      119.2012      123.2149  0.2938\n",
      "     26       38.3577       40.1443  0.2366\n",
      "     20      153.6623      159.6417  0.2818\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 6.\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     20      182.5968      186.8470  0.2814\n",
      "     27       47.8497       48.7984  0.2029\n",
      "     26       48.3115       49.0497  0.2295\n",
      "     27       38.5887       40.1109  0.2056\n",
      "     21      191.3425      194.0942  0.2882\n",
      "     21      193.3255      194.2656  0.2698\n",
      "     21      119.8789      124.5109  0.2707\n",
      "     21      153.4898      158.9548  0.2715\n",
      "     28       47.7809       48.5558  0.2034\n",
      "     21      183.0621      187.2460  0.2629\n",
      "     27       48.3531       48.9387  0.2112\n",
      "     28       38.6670       40.1320  0.2062\n",
      "     22      191.5219      194.0545  0.2650\n",
      "     22      194.2723      195.9290  0.2651\n",
      "     22      119.8299      121.7734  0.2659\n",
      "     22      153.6695      161.0996  0.2634\n",
      "     29       47.8774       48.6780  0.2047\n",
      "     28       48.4787       49.3663  0.2082\n",
      "     22      182.7606      \u001b[32m183.6232\u001b[0m  0.2623\n",
      "     29       38.6281       40.1997  0.2069\n",
      "     23      \u001b[36m191.1709\u001b[0m      194.6801  0.2650\n",
      "     23      195.5686      196.0509  0.2617\n",
      "     30       48.0118       48.7103  0.2052\n",
      "     23      118.3971      123.2836  0.2685\n",
      "     29       48.7457       49.5093  0.2113\n",
      "     23      155.0266      161.7045  0.2659\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 10.\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     23      182.6474      184.6821  0.2639\n",
      "     31       47.9961       48.7181  0.2063\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 4.\n",
      "     30       48.7527       49.4960  0.2109\n",
      "     24      194.1449      194.8373  0.2647\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     24      \u001b[36m117.6426\u001b[0m      121.1798  0.2669\n",
      "     24      153.1094      160.2637  0.2658\n",
      "     24      183.3707      185.8371  0.2690\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 12.\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     31       48.7361       49.2290  0.2166\n",
      "     25      193.5342      195.8176  0.2687\n",
      "     25      118.2610      122.2741  0.2715\n",
      "     25      153.3865      159.6931  0.2685\n",
      "     25      182.7589      184.1878  0.2650\n",
      "     32       48.4982       49.1780  0.2080\n",
      "     26      195.2475      198.5627  0.2626\n",
      "     26      118.2632      124.4327  0.2667\n",
      "     26      153.6699      159.9904  0.2641\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 13.\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     26      182.6070      184.3027  0.2630\n",
      "     27      195.7199      197.6014  0.2637\n",
      "     27      118.3927      122.0027  0.2673\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 7.\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     27      182.9956      185.2620  0.2660\n",
      "     28      195.1875      195.3867  0.2621\n",
      "     28      118.5804      122.0114  0.2647\n",
      "     28      183.8255      185.6022  0.2622\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 9.\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     29      118.1988      122.4269  0.2655\n",
      "     29      184.6452      185.9829  0.2586\n",
      "     30      118.3758      124.1631  0.2577\n",
      "     30      183.7494      184.3096  0.2526\n",
      "     31      120.3211      124.7157  0.2562\n",
      "     31      183.4209      185.7452  0.2531\n",
      "     32      120.3016      124.3515  0.2600\n",
      "     32      183.3204      184.7743  0.2565\n",
      "     33      119.3849      123.0339  0.2581\n",
      "     33      183.1188      184.0423  0.2540\n",
      "     34      119.1971      124.4663  0.2591\n",
      "     34      182.9484      183.8004  0.2630\n",
      "     35      120.6001      124.3821  0.2597\n",
      "     35      183.0357      183.9548  0.2534\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 16.\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     36      182.5531      184.0539  0.2549\n",
      "     37      182.9486      185.6372  0.2512\n",
      "     38      183.0629      184.8195  0.2501\n",
      "     39      182.5606      184.5834  0.2482\n",
      "     40      183.4307      184.2197  0.2537\n",
      "     41      182.7222      184.1654  0.2487\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 22.\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m172.2671\u001b[0m      \u001b[32m176.7715\u001b[0m  0.2247\n",
      "      2      \u001b[36m168.1383\u001b[0m      \u001b[32m175.6142\u001b[0m  0.2200\n",
      "      3      168.7563      \u001b[32m174.9177\u001b[0m  0.3383\n",
      "      4      168.4735      \u001b[32m173.6527\u001b[0m  0.2560\n",
      "      5      168.5178      175.8561  0.2499\n",
      "      6      168.7609      175.8393  0.2533\n",
      "      7      \u001b[36m167.8182\u001b[0m      175.6863  0.2383\n",
      "      8      \u001b[36m167.6271\u001b[0m      175.4868  0.2889\n",
      "      9      \u001b[36m167.6157\u001b[0m      177.5450  0.2580\n",
      "     10      168.4789      175.7143  0.2432\n",
      "     11      \u001b[36m167.2679\u001b[0m      174.5708  0.2479\n",
      "     12      167.8035      175.0134  0.2543\n",
      "     13      168.2224      174.7695  0.2396\n",
      "     14      167.6481      180.6720  0.2556\n",
      "     15      167.7594      173.6910  0.2594\n",
      "     16      167.7067      175.7033  0.2577\n",
      "     17      168.2328      176.9548  0.2546\n",
      "     18      169.1727      177.0901  0.2644\n",
      "     19      168.9485      176.1701  0.2612\n",
      "     20      168.3354      174.7502  0.2445\n",
      "     21      168.3408      173.9968  0.2501\n",
      "     22      168.4890      \u001b[32m173.5605\u001b[0m  0.2732\n",
      "     23      168.3130      174.2764  0.2725\n",
      "     24      167.4598      174.9581  0.2605\n",
      "     25      167.9192      174.0204  0.2500\n",
      "     26      \u001b[36m167.2423\u001b[0m      175.6393  0.2616\n",
      "     27      167.7704      175.0224  0.2608\n",
      "     28      168.6742      176.1626  0.2428\n",
      "     29      168.1523      174.2876  0.2438\n",
      "     30      168.1456      175.8707  0.2623\n",
      "     31      167.8992      176.0738  0.2386\n",
      "     32      168.9195      176.0387  0.2464\n",
      "     33      169.6727      177.1918  0.2623\n",
      "     34      170.3026      177.0461  0.2443\n",
      "     35      170.4470      177.7724  0.2396\n",
      "     36      169.9566      176.5299  0.2541\n",
      "     37      170.0770      175.8217  0.2436\n",
      "     38      169.2952      174.1230  0.2588\n",
      "     39      169.6853      176.7944  0.2397\n",
      "     40      169.4110      175.8256  0.2315\n",
      "     41      168.6326      174.3857  0.2425\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 22.\n",
      "durations 1.0 5187.0\n",
      "split horm_treatment      category\n",
      "menopause           category\n",
      "age                  float32\n",
      "n_positive_nodes     float32\n",
      "progesterone         float32\n",
      "estrogene            float32\n",
      "grade_0.0              uint8\n",
      "grade_1.0              uint8\n",
      "grade_2.0              uint8\n",
      "dtype: object\n",
      "dtype X_train horm_treatment      category\n",
      "menopause           category\n",
      "age                  float32\n",
      "n_positive_nodes     float32\n",
      "progesterone         float32\n",
      "estrogene            float32\n",
      "grade_0.0              uint8\n",
      "grade_1.0              uint8\n",
      "grade_2.0              uint8\n",
      "dtype: object\n",
      "dtype y_train float64\n",
      "dtype X_test horm_treatment      category\n",
      "menopause           category\n",
      "age                  float32\n",
      "n_positive_nodes     float32\n",
      "progesterone         float32\n",
      "estrogene            float32\n",
      "grade_0.0              uint8\n",
      "grade_1.0              uint8\n",
      "grade_2.0              uint8\n",
      "dtype: object\n",
      "dtype y_test float32\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m87.7015\u001b[0m       \u001b[32m68.0439\u001b[0m  0.0675\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m71.5346\u001b[0m       \u001b[32m59.6118\u001b[0m  0.0799\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m74.4803\u001b[0m       \u001b[32m60.0106\u001b[0m  0.0888\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m79.3835\u001b[0m       \u001b[32m62.3937\u001b[0m  0.0850\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m97.0031\u001b[0m       \u001b[32m79.8219\u001b[0m  0.1003\n",
      "      2       \u001b[36m75.5815\u001b[0m       \u001b[32m65.5370\u001b[0m  0.0688\n",
      "      2       \u001b[36m63.0499\u001b[0m       \u001b[32m54.9699\u001b[0m  0.0608\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m292.0739\u001b[0m      \u001b[32m248.0801\u001b[0m  0.1148\n",
      "      2       \u001b[36m69.1257\u001b[0m       \u001b[32m60.0428\u001b[0m  0.0654\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m332.3180\u001b[0m      \u001b[32m253.8372\u001b[0m  0.1003\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m410.3931\u001b[0m      \u001b[32m329.1879\u001b[0m  0.0895\n",
      "      2       \u001b[36m65.5417\u001b[0m       \u001b[32m57.2129\u001b[0m  0.0870\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m309.7884\u001b[0m      \u001b[32m242.8903\u001b[0m  0.1378\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m367.2609\u001b[0m      \u001b[32m285.3615\u001b[0m  0.1163\n",
      "      2       \u001b[36m83.8108\u001b[0m       \u001b[32m75.4156\u001b[0m  0.0728\n",
      "      3       \u001b[36m61.1798\u001b[0m       \u001b[32m54.8595\u001b[0m  0.0602\n",
      "      3       \u001b[36m73.9896\u001b[0m       \u001b[32m65.2666\u001b[0m  0.0695\n",
      "      3       \u001b[36m68.6267\u001b[0m       60.0835  0.0795\n",
      "      2      \u001b[36m261.0725\u001b[0m      \u001b[32m231.9799\u001b[0m  0.0881\n",
      "      2      \u001b[36m292.3769\u001b[0m      \u001b[32m241.2280\u001b[0m  0.0901\n",
      "      3       \u001b[36m65.2515\u001b[0m       \u001b[32m56.8032\u001b[0m  0.0795\n",
      "      2      \u001b[36m350.2561\u001b[0m      \u001b[32m296.6722\u001b[0m  0.0997\n",
      "      4       \u001b[36m60.6208\u001b[0m       \u001b[32m54.8442\u001b[0m  0.0589\n",
      "      3       \u001b[36m82.5214\u001b[0m       \u001b[32m74.2553\u001b[0m  0.0650\n",
      "      2      \u001b[36m272.0551\u001b[0m      \u001b[32m225.1999\u001b[0m  0.0922\n",
      "      2      \u001b[36m319.2683\u001b[0m      \u001b[32m258.1428\u001b[0m  0.1014\n",
      "      4       \u001b[36m68.1071\u001b[0m       \u001b[32m59.5487\u001b[0m  0.0624\n",
      "      4       \u001b[36m73.6697\u001b[0m       \u001b[32m64.8776\u001b[0m  0.0832\n",
      "      4       \u001b[36m64.4988\u001b[0m       \u001b[32m56.7204\u001b[0m  0.0667\n",
      "      5       \u001b[36m60.4783\u001b[0m       55.3112  0.0576\n",
      "      3      \u001b[36m282.1016\u001b[0m      \u001b[32m236.5235\u001b[0m  0.0835\n",
      "      4       \u001b[36m81.3968\u001b[0m       \u001b[32m73.2069\u001b[0m  0.0687\n",
      "      3      \u001b[36m262.3365\u001b[0m      \u001b[32m221.8103\u001b[0m  0.0844\n",
      "      3      \u001b[36m251.7620\u001b[0m      \u001b[32m219.3538\u001b[0m  0.1177\n",
      "      3      \u001b[36m336.7707\u001b[0m      \u001b[32m280.9042\u001b[0m  0.0970\n",
      "      5       \u001b[36m73.4499\u001b[0m       \u001b[32m64.7017\u001b[0m  0.0692\n",
      "      5       \u001b[36m67.9960\u001b[0m       \u001b[32m59.1518\u001b[0m  0.0749\n",
      "      3      \u001b[36m306.8472\u001b[0m      \u001b[32m251.4313\u001b[0m  0.0963\n",
      "      6       \u001b[36m60.4222\u001b[0m       55.3129  0.0608\n",
      "      5       \u001b[36m64.2101\u001b[0m       \u001b[32m56.3736\u001b[0m  0.0769\n",
      "      5       \u001b[36m80.6317\u001b[0m       \u001b[32m72.8151\u001b[0m  0.0650\n",
      "      6       \u001b[36m73.1414\u001b[0m       \u001b[32m64.4932\u001b[0m  0.0671\n",
      "      4      \u001b[36m277.2141\u001b[0m      \u001b[32m235.3818\u001b[0m  0.1014\n",
      "      6       68.5130       \u001b[32m59.0271\u001b[0m  0.0661\n",
      "      4      \u001b[36m259.2114\u001b[0m      \u001b[32m219.2141\u001b[0m  0.0883\n",
      "      4      \u001b[36m245.1579\u001b[0m      \u001b[32m217.4086\u001b[0m  0.0903\n",
      "      7       \u001b[36m60.3124\u001b[0m       \u001b[32m54.4632\u001b[0m  0.0670\n",
      "      4      \u001b[36m326.9553\u001b[0m      282.8839  0.0950\n",
      "      4      \u001b[36m299.6606\u001b[0m      252.5358  0.0920\n",
      "      6       \u001b[36m64.0108\u001b[0m       \u001b[32m56.1871\u001b[0m  0.0741\n",
      "      6       \u001b[36m79.4526\u001b[0m       \u001b[32m72.6848\u001b[0m  0.0728\n",
      "      7       \u001b[36m72.8339\u001b[0m       \u001b[32m64.0393\u001b[0m  0.0604\n",
      "      8       60.4133       \u001b[32m54.3625\u001b[0m  0.0610\n",
      "      7       \u001b[36m67.7097\u001b[0m       59.0407  0.0784\n",
      "      5      \u001b[36m256.5104\u001b[0m      \u001b[32m216.9514\u001b[0m  0.0808\n",
      "      5      \u001b[36m274.2542\u001b[0m      \u001b[32m231.4294\u001b[0m  0.0902\n",
      "      5      \u001b[36m324.0461\u001b[0m      282.8458  0.0780\n",
      "      7       \u001b[36m63.9297\u001b[0m       \u001b[32m55.9607\u001b[0m  0.0629\n",
      "      5      \u001b[36m244.1160\u001b[0m      \u001b[32m215.8920\u001b[0m  0.1096\n",
      "      7       \u001b[36m79.2430\u001b[0m       \u001b[32m72.4818\u001b[0m  0.0873\n",
      "      5      \u001b[36m296.2961\u001b[0m      \u001b[32m250.5173\u001b[0m  0.0961\n",
      "      8       \u001b[36m72.6168\u001b[0m       64.1173  0.0691\n",
      "      8       \u001b[36m67.5515\u001b[0m       \u001b[32m58.7948\u001b[0m  0.0684\n",
      "      9       60.4818       54.4665  0.0793\n",
      "      8       64.1326       \u001b[32m55.6295\u001b[0m  0.0665\n",
      "      6      \u001b[36m271.6232\u001b[0m      \u001b[32m230.0805\u001b[0m  0.0829\n",
      "      6      \u001b[36m255.2986\u001b[0m      217.0193  0.0889\n",
      "      6      \u001b[36m322.1896\u001b[0m      \u001b[32m280.4082\u001b[0m  0.0907\n",
      "      8       \u001b[36m78.6373\u001b[0m       \u001b[32m72.4811\u001b[0m  0.0677\n",
      "      9       \u001b[36m72.3163\u001b[0m       64.2155  0.0689\n",
      "      6      \u001b[36m242.7915\u001b[0m      \u001b[32m213.3271\u001b[0m  0.0815\n",
      "     10       60.3512       54.4988  0.0581\n",
      "      6      \u001b[36m293.3455\u001b[0m      \u001b[32m249.8903\u001b[0m  0.0987\n",
      "      9       \u001b[36m67.5368\u001b[0m       \u001b[32m58.5555\u001b[0m  0.0819\n",
      "      9       \u001b[36m63.8314\u001b[0m       \u001b[32m55.5863\u001b[0m  0.0759\n",
      "      7      \u001b[36m271.2090\u001b[0m      230.0894  0.0888\n",
      "      9       \u001b[36m78.4406\u001b[0m       \u001b[32m72.2513\u001b[0m  0.0665\n",
      "      7      \u001b[36m254.8232\u001b[0m      217.4470  0.0950\n",
      "     10       \u001b[36m72.2706\u001b[0m       64.2950  0.0751\n",
      "      7      \u001b[36m320.5321\u001b[0m      \u001b[32m278.9363\u001b[0m  0.0945\n",
      "     11       60.3346       54.4750  0.0841\n",
      "     10       \u001b[36m67.1344\u001b[0m       \u001b[32m58.3008\u001b[0m  0.0698\n",
      "      7      \u001b[36m290.7820\u001b[0m      \u001b[32m249.1048\u001b[0m  0.0832\n",
      "      7      \u001b[36m241.5443\u001b[0m      \u001b[32m211.8796\u001b[0m  0.1119\n",
      "     10       \u001b[36m78.3583\u001b[0m       \u001b[32m71.9672\u001b[0m  0.0744\n",
      "     10       \u001b[36m63.4008\u001b[0m       55.6581  0.1034\n",
      "     11       \u001b[36m72.1181\u001b[0m       64.1897  0.0784\n",
      "      8      \u001b[36m268.9566\u001b[0m      \u001b[32m229.9855\u001b[0m  0.0910\n",
      "      8      \u001b[36m254.5361\u001b[0m      \u001b[32m216.6543\u001b[0m  0.0938\n",
      "     12       \u001b[36m60.2095\u001b[0m       54.4776  0.0727\n",
      "     11       67.2556       58.3036  0.0697\n",
      "      8      \u001b[36m317.4084\u001b[0m      \u001b[32m276.8703\u001b[0m  0.0907\n",
      "      8      290.8563      249.1817  0.0752\n",
      "     11       \u001b[36m78.2993\u001b[0m       72.0325  0.0626\n",
      "      8      \u001b[36m239.8423\u001b[0m      212.5770  0.0930\n",
      "     11       \u001b[36m63.2954\u001b[0m       \u001b[32m55.5853\u001b[0m  0.0733\n",
      "     12       \u001b[36m71.7880\u001b[0m       64.2835  0.0773\n",
      "      9      269.2313      \u001b[32m229.7685\u001b[0m  0.0848\n",
      "     12       \u001b[36m67.1001\u001b[0m       58.5632  0.0647\n",
      "     13       \u001b[36m59.9359\u001b[0m       54.5512  0.0829\n",
      "      9      \u001b[36m253.3955\u001b[0m      217.7462  0.0940\n",
      "      9      317.8378      277.2014  0.0973\n",
      "      9      \u001b[36m289.5974\u001b[0m      249.2226  0.0880\n",
      "     12       \u001b[36m78.2810\u001b[0m       \u001b[32m71.9671\u001b[0m  0.0723\n",
      "     13       72.0666       64.3667  0.0596\n",
      "      9      \u001b[36m238.6647\u001b[0m      212.6790  0.0844\n",
      "     12       63.7018       55.8749  0.0742\n",
      "     13       \u001b[36m66.2463\u001b[0m       58.5722  0.0738\n",
      "     14       \u001b[36m59.3471\u001b[0m       54.5875  0.0728\n",
      "     10      \u001b[36m268.1095\u001b[0m      \u001b[32m228.0185\u001b[0m  0.0891\n",
      "     13       78.4068       \u001b[32m71.9671\u001b[0m  0.0646\n",
      "     10      \u001b[36m316.2746\u001b[0m      277.6433  0.0783\n",
      "     14       72.5011       64.4438  0.0677\n",
      "     10      \u001b[36m252.0899\u001b[0m      217.4356  0.0970\n",
      "     13       63.6029       55.9949  0.0634\n",
      "     10      290.0090      249.3242  0.0950\n",
      "     10      \u001b[36m237.5259\u001b[0m      212.2982  0.0826\n",
      "     15       \u001b[36m59.1412\u001b[0m       54.3691  0.0649\n",
      "     14       66.2642       58.3874  0.0780\n",
      "     14       78.3000       \u001b[32m71.9671\u001b[0m  0.0619\n",
      "     15       \u001b[36m71.6264\u001b[0m       64.1223  0.0612\n",
      "     11      316.2787      \u001b[32m276.8372\u001b[0m  0.0853\n",
      "     11      268.9865      \u001b[32m225.1549\u001b[0m  0.1109\n",
      "     14       63.4436       56.0823  0.0914\n",
      "     16       59.1518       54.4074  0.0656\n",
      "     11      289.6695      249.3082  0.0895\n",
      "     11      252.4504      217.8102  0.1050\n",
      "     11      \u001b[36m236.6625\u001b[0m      212.5855  0.0885\n",
      "     15       \u001b[36m66.1963\u001b[0m       58.3873  0.0740\n",
      "     15       78.3554       71.9671  0.0689\n",
      "     16       \u001b[36m71.6158\u001b[0m       64.1223  0.0780\n",
      "     15       \u001b[36m63.0572\u001b[0m       55.7184  0.0695\n",
      "     17       59.1455       54.4072  0.0680\n",
      "     12      \u001b[36m315.6912\u001b[0m      \u001b[32m275.8575\u001b[0m  0.0865\n",
      "     12      \u001b[36m267.3757\u001b[0m      \u001b[32m224.9209\u001b[0m  0.0887\n",
      "     16       78.2992       72.0621  0.0700\n",
      "     16       \u001b[36m66.1908\u001b[0m       58.3873  0.0747\n",
      "     12      289.9439      \u001b[32m248.4870\u001b[0m  0.0828\n",
      "     12      252.2606      217.7853  0.0878\n",
      "     17       \u001b[36m71.6106\u001b[0m       64.1223  0.0672\n",
      "     12      236.8222      \u001b[32m210.8442\u001b[0m  0.1074\n",
      "     16       63.2559       55.9538  0.0702\n",
      "     18       59.1667       54.4086  0.0758\n",
      "     17       78.2989       \u001b[32m71.9671\u001b[0m  0.0625\n",
      "     17       \u001b[36m66.1879\u001b[0m       58.3873  0.0669\n",
      "     13      \u001b[36m267.2283\u001b[0m      224.9274  0.0803\n",
      "     18       \u001b[36m71.6084\u001b[0m       64.1223  0.0630\n",
      "     13      \u001b[36m288.2613\u001b[0m      \u001b[32m247.6442\u001b[0m  0.0825\n",
      "     13      252.4705      217.8402  0.0859\n",
      "     13      \u001b[36m314.1911\u001b[0m      275.9016  0.1112\n",
      "     18       78.2986       \u001b[32m71.9671\u001b[0m  0.0614\n",
      "     13      \u001b[36m236.2669\u001b[0m      \u001b[32m210.8275\u001b[0m  0.0943\n",
      "     17       63.9341       55.9541  0.0768\n",
      "     18       \u001b[36m66.1850\u001b[0m       58.3873  0.0747\n",
      "     19       \u001b[36m59.1052\u001b[0m       54.4076  0.0842\n",
      "     19       \u001b[36m71.6079\u001b[0m       64.1223  0.0680\n",
      "     14      \u001b[36m267.1315\u001b[0m      224.9853  0.0923\n",
      "     14      \u001b[36m287.8029\u001b[0m      247.6458  0.0763\n",
      "     14      \u001b[36m313.5427\u001b[0m      \u001b[32m275.8077\u001b[0m  0.0776\n",
      "     14      253.0749      218.0736  0.0867\n",
      "     19       78.2988       \u001b[32m71.9671\u001b[0m  0.0800\n",
      "     19       66.1864       58.3873  0.0629\n",
      "     18       63.5322       55.9774  0.0824\n",
      "     20       59.1053       54.4072  0.0745\n",
      "     14      236.4395      \u001b[32m210.8117\u001b[0m  0.0936\n",
      "     20       71.6356       64.1223  0.0810\n",
      "     15      267.4785      224.9227  0.0851\n",
      "     15      \u001b[36m287.4625\u001b[0m      247.7919  0.0915\n",
      "     15      313.5526      \u001b[32m275.7888\u001b[0m  0.0883\n",
      "     20       78.2985       71.9671  0.0620\n",
      "     19       63.5411       56.0067  0.0662\n",
      "     20       66.1862       58.3873  0.0738\n",
      "     21       59.1053       54.4074  0.0656\n",
      "     21       71.6140       64.1223  0.0616\n",
      "     15      \u001b[36m251.9278\u001b[0m      218.0527  0.1284\n",
      "     15      \u001b[36m235.5522\u001b[0m      \u001b[32m210.6665\u001b[0m  0.0998\n",
      "     16      \u001b[36m266.9424\u001b[0m      224.9740  0.0836\n",
      "     21       78.2987       71.9671  0.0679\n",
      "     16      287.5820      247.7910  0.0804\n",
      "     21       66.1858       58.3873  0.0608\n",
      "     20       63.8129       55.6161  0.0671\n",
      "     16      313.5817      275.9365  0.0843\n",
      "     22       59.1069       54.4072  0.0713\n",
      "     22       71.6081       64.1223  0.0633\n",
      "     22       78.2986       72.2679  0.0653\n",
      "     22       66.1859       58.3873  0.0623\n",
      "     16      236.2302      \u001b[32m210.5144\u001b[0m  0.0854\n",
      "     16      252.2466      218.0095  0.0998\n",
      "     21       63.9338       55.9098  0.0754\n",
      "     17      \u001b[36m266.7354\u001b[0m      \u001b[32m224.8864\u001b[0m  0.0926\n",
      "     17      287.6820      247.7910  0.0883\n",
      "     23       59.1054       54.4347  0.0705\n",
      "     23       71.6081       64.1223  0.0698\n",
      "     17      313.6058      \u001b[32m275.7841\u001b[0m  0.0871\n",
      "     23       66.1856       58.3873  0.0789\n",
      "     17      235.5790      212.1100  0.0758\n",
      "     22       63.8591       56.0169  0.0649\n",
      "     23       78.2986       \u001b[32m71.9671\u001b[0m  0.0896\n",
      "     24       59.1054       54.4426  0.0674\n",
      "     24       71.6080       64.1223  0.0670\n",
      "     17      252.1321      218.0606  0.0927\n",
      "     18      266.9066      \u001b[32m224.8834\u001b[0m  0.0807\n",
      "     18      313.6106      \u001b[32m275.7828\u001b[0m  0.0805\n",
      "     18      288.0027      \u001b[32m247.6285\u001b[0m  0.0959\n",
      "     23       63.3212       55.7146  0.0696\n",
      "     24       66.1851       58.3873  0.0756\n",
      "     24       78.2984       \u001b[32m71.9670\u001b[0m  0.0696\n",
      "     18      235.9021      212.2285  0.0814\n",
      "     25       71.6080       64.1223  0.0695\n",
      "     25       59.1054       54.4463  0.0797\n",
      "     18      251.9806      217.6189  0.0856\n",
      "     19      \u001b[36m266.5231\u001b[0m      \u001b[32m224.8753\u001b[0m  0.1001\n",
      "     19      \u001b[36m286.9248\u001b[0m      \u001b[32m247.6264\u001b[0m  0.0852\n",
      "     19      \u001b[36m313.1236\u001b[0m      \u001b[32m275.7822\u001b[0m  0.0982\n",
      "     25       78.2987       72.0699  0.0635\n",
      "     25       66.1857       58.3873  0.0669\n",
      "     26       71.6079       64.1223  0.0688\n",
      "     24       63.4201       55.6671  0.0817\n",
      "     26       59.1056       54.4427  0.0659\n",
      "     19      \u001b[36m235.4951\u001b[0m      212.2422  0.0847\n",
      "     19      252.3179      217.4914  0.0782\n",
      "     20      287.8129      \u001b[32m247.4701\u001b[0m  0.0849\n",
      "     20      \u001b[36m313.0805\u001b[0m      \u001b[32m275.7820\u001b[0m  0.0794\n",
      "     26       78.2987       72.0367  0.0734\n",
      "     25       63.5120       55.6243  0.0634\n",
      "     20      \u001b[36m266.4649\u001b[0m      \u001b[32m224.8752\u001b[0m  0.0999\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 7.\n",
      "     26       66.1854       58.3873  0.0814\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     27       59.1054       54.4443  0.0796\n",
      "     20      \u001b[36m235.3437\u001b[0m      212.2598  0.0926\n",
      "     20      252.3915      218.2313  0.0879\n",
      "     27       66.1854       58.3873  0.0608\n",
      "     21      287.2854      \u001b[32m247.3150\u001b[0m  0.0765\n",
      "     27       78.2985       71.9670  0.0708\n",
      "     26       63.2965       55.9494  0.0774\n",
      "     21      266.6264      \u001b[32m224.8751\u001b[0m  0.0741\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 8.\n",
      "     21      313.3753      \u001b[32m275.7820\u001b[0m  0.0851\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     21      236.0470      212.2451  0.0806\n",
      "     28       66.1852       58.3873  0.0590\n",
      "     28       78.2989       71.9670  0.0660\n",
      "     27       63.3508       55.6684  0.0606\n",
      "     21      252.0137      217.6034  0.0871\n",
      "     22      287.4614      \u001b[32m247.3144\u001b[0m  0.0750\n",
      "     22      \u001b[36m313.0803\u001b[0m      \u001b[32m275.7819\u001b[0m  0.0741\n",
      "     22      266.6977      224.8754  0.0801\n",
      "     29       66.1853       58.3873  0.0607\n",
      "     29       78.2984       72.0716  0.0586\n",
      "     22      235.7292      212.2951  0.0752\n",
      "     28       63.5785       55.6941  0.0607\n",
      "     22      252.6434      216.7635  0.0755\n",
      "     23      287.4907      \u001b[32m247.2943\u001b[0m  0.0753\n",
      "     23      313.1243      275.7854  0.0741\n",
      "     23      267.6342      225.1898  0.0731\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 10.\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 9.\n",
      "Restoring best model from epoch 10.\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     23      236.1985      212.2948  0.0824\n",
      "     24      287.0532      247.3119  0.0749\n",
      "     23      \u001b[36m251.1339\u001b[0m      216.7774  0.0770\n",
      "     24      313.0808      \u001b[32m275.7818\u001b[0m  0.0763\n",
      "     24      \u001b[36m265.9625\u001b[0m      225.0333  0.0757\n",
      "     24      236.4486      212.2948  0.0757\n",
      "     25      287.1737      247.4432  0.0753\n",
      "     24      251.4367      \u001b[32m215.9648\u001b[0m  0.0743\n",
      "     25      313.1678      275.7821  0.0741\n",
      "     25      \u001b[36m265.8952\u001b[0m      225.6594  0.0727\n",
      "     25      236.5309      212.2947  0.0734\n",
      "     26      287.6712      \u001b[32m247.2890\u001b[0m  0.0745\n",
      "     25      251.8146      216.1437  0.0748\n",
      "     26      313.3639      275.9383  0.0734\n",
      "     26      \u001b[36m265.5303\u001b[0m      225.8220  0.0733\n",
      "     26      236.6898      212.2947  0.0749\n",
      "     27      287.2336      247.3103  0.0741\n",
      "     26      251.6132      \u001b[32m215.8720\u001b[0m  0.0729\n",
      "     27      313.1679      275.9384  0.0724\n",
      "     27      \u001b[36m265.0460\u001b[0m      225.5064  0.0726\n",
      "     27      235.9369      212.2946  0.0733\n",
      "     27      251.4576      216.6217  0.0721\n",
      "     28      287.8301      247.3098  0.0736\n",
      "     28      313.1675      275.9383  0.0717\n",
      "     28      265.1794      225.0846  0.0726\n",
      "     28      235.6553      212.2786  0.0726\n",
      "     28      \u001b[36m251.0161\u001b[0m      216.7340  0.0734\n",
      "     29      287.1624      \u001b[32m247.1613\u001b[0m  0.0738\n",
      "     29      313.1674      275.9383  0.0731\n",
      "     29      265.2186      225.9941  0.0725\n",
      "     29      236.0572      212.1960  0.0731\n",
      "     29      251.7305      216.3488  0.0737\n",
      "     30      287.4317      \u001b[32m247.1535\u001b[0m  0.0736\n",
      "     30      313.1677      275.9383  0.0723\n",
      "     30      \u001b[36m264.9280\u001b[0m      225.2055  0.0729\n",
      "     30      235.7748      210.8174  0.0724\n",
      "     30      251.7624      216.4291  0.0731\n",
      "     31      313.2238      275.9383  0.0723\n",
      "     31      288.0665      \u001b[32m247.1534\u001b[0m  0.0738\n",
      "     31      264.9934      225.3887  0.0730\n",
      "     31      235.6432      210.9745  0.0728\n",
      "     31      251.3720      216.6904  0.0737\n",
      "     32      313.1695      275.9382  0.0726\n",
      "     32      287.8182      \u001b[32m247.1534\u001b[0m  0.0737\n",
      "     32      265.2325      225.5458  0.0734\n",
      "     32      235.4512      211.0061  0.0730\n",
      "     33      313.1735      275.9382  0.0724\n",
      "     32      251.2528      216.7765  0.0733\n",
      "     33      287.8208      \u001b[32m247.1296\u001b[0m  0.0737\n",
      "     33      265.1547      225.4965  0.0723\n",
      "     33      235.6138      210.9807  0.0727\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 14.\n",
      "     33      251.4940      216.7311  0.0729\n",
      "     34      287.9340      247.1534  0.0729\n",
      "     34      \u001b[36m264.6971\u001b[0m      225.2248  0.0725\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     34      \u001b[36m235.0288\u001b[0m      210.9750  0.0728\n",
      "     34      251.2178      216.4836  0.0738\n",
      "     35      264.7434      225.0757  0.0727\n",
      "     35      287.6665      247.1535  0.0738\n",
      "     35      235.5484      210.9748  0.0731\n",
      "     35      251.7207      217.7674  0.0730\n",
      "     36      \u001b[36m264.3275\u001b[0m      225.0749  0.0720\n",
      "     36      287.9050      247.1534  0.0727\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 16.\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     36      252.1818      216.6104  0.0729\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 17.\n",
      "     37      287.9393      247.1534  0.0726\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     37      \u001b[36m250.5755\u001b[0m      216.0498  0.0737\n",
      "     38      287.8742      247.1775  0.0715\n",
      "     38      251.2955      \u001b[32m215.3030\u001b[0m  0.0716\n",
      "     39      287.4139      247.4665  0.0718\n",
      "     39      250.9328      \u001b[32m215.2811\u001b[0m  0.0718\n",
      "     40      287.6565      \u001b[32m246.9507\u001b[0m  0.0722\n",
      "     41      287.4616      \u001b[32m246.5782\u001b[0m  0.0713\n",
      "     40      250.8841      \u001b[32m215.0409\u001b[0m  0.0724\n",
      "     41      \u001b[36m250.0895\u001b[0m      \u001b[32m214.7623\u001b[0m  0.0717\n",
      "     42      288.2313      246.8869  0.0732\n",
      "     43      287.3319      247.0416  0.0708\n",
      "     42      250.6301      \u001b[32m214.1937\u001b[0m  0.0728\n",
      "     44      287.3443      247.0414  0.0717\n",
      "     43      250.3277      \u001b[32m214.0348\u001b[0m  0.0720\n",
      "     45      287.6383      247.0417  0.0719\n",
      "     44      250.5298      \u001b[32m214.0138\u001b[0m  0.0708\n",
      "     46      287.8374      247.0419  0.0717\n",
      "     45      251.0536      214.0553  0.0717\n",
      "     47      287.4490      247.6421  0.0716\n",
      "     46      250.6664      214.3512  0.0716\n",
      "     48      287.4530      247.3130  0.0727\n",
      "     47      250.7119      215.1446  0.0719\n",
      "     49      287.0506      246.9974  0.0719\n",
      "     48      250.6515      215.1496  0.0715\n",
      "     50      288.4737      247.0154  0.0717\n",
      "Restoring best model from epoch 41.\n",
      "     49      250.7600      215.0983  0.0719\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     50      251.3164      214.8408  0.0729\n",
      "Restoring best model from epoch 43.\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m334.7129\u001b[0m      \u001b[32m237.5576\u001b[0m  0.0656\n",
      "      2      \u001b[36m290.3408\u001b[0m      \u001b[32m221.7136\u001b[0m  0.0714\n",
      "      3      \u001b[36m282.7591\u001b[0m      \u001b[32m217.6663\u001b[0m  0.0686\n",
      "      4      \u001b[36m279.4863\u001b[0m      \u001b[32m216.1551\u001b[0m  0.0644\n",
      "      5      \u001b[36m278.5078\u001b[0m      \u001b[32m213.4619\u001b[0m  0.0710\n",
      "      6      \u001b[36m276.6213\u001b[0m      \u001b[32m211.8672\u001b[0m  0.0699\n",
      "      7      \u001b[36m274.7887\u001b[0m      212.4330  0.0652\n",
      "      8      \u001b[36m273.9545\u001b[0m      \u001b[32m210.5008\u001b[0m  0.0770\n",
      "      9      \u001b[36m271.8233\u001b[0m      \u001b[32m209.5978\u001b[0m  0.0706\n",
      "     10      \u001b[36m271.1092\u001b[0m      209.8444  0.0677\n",
      "     11      271.1403      209.8139  0.0723\n",
      "     12      272.1244      \u001b[32m209.5514\u001b[0m  0.0710\n",
      "     13      271.9604      210.1423  0.0651\n",
      "     14      271.6385      209.9228  0.0678\n",
      "     15      271.2809      209.6476  0.0733\n",
      "     16      \u001b[36m271.0161\u001b[0m      209.6426  0.0713\n",
      "     17      271.0248      209.6747  0.0809\n",
      "     18      \u001b[36m270.4729\u001b[0m      \u001b[32m209.5495\u001b[0m  0.0586\n",
      "     19      \u001b[36m270.2027\u001b[0m      \u001b[32m209.5452\u001b[0m  0.0665\n",
      "     20      271.1448      212.3203  0.0696\n",
      "     21      271.3545      212.0854  0.0719\n",
      "     22      271.9938      214.3784  0.0722\n",
      "     23      272.1218      214.4610  0.0661\n",
      "     24      273.0081      214.9650  0.0670\n",
      "     25      273.6668      215.7905  0.0833\n",
      "     26      273.1275      213.4915  0.0678\n",
      "     27      271.2663      209.7611  0.0642\n",
      "     28      272.2331      209.8581  0.0678\n",
      "     29      271.8268      212.1855  0.0654\n",
      "     30      271.3574      211.4457  0.0636\n",
      "     31      271.3692      211.4153  0.0686\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 12.\n",
      "durations 0.52566737 87.359344\n",
      "dtype X_train horm_treatment      category\n",
      "menopause           category\n",
      "age                  float32\n",
      "n_positive_nodes     float32\n",
      "progesterone         float32\n",
      "estrogene            float32\n",
      "grade_0.0              uint8\n",
      "grade_1.0              uint8\n",
      "grade_2.0              uint8\n",
      "dtype: object\n",
      "dtype y_train float64\n",
      "dtype X_test horm_treatment      category\n",
      "menopause           category\n",
      "age                  float32\n",
      "n_positive_nodes     float32\n",
      "progesterone         float32\n",
      "estrogene            float32\n",
      "grade_0.0              uint8\n",
      "grade_1.0              uint8\n",
      "grade_2.0              uint8\n",
      "dtype: object\n",
      "dtype y_test float32\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m77.8595\u001b[0m       \u001b[32m66.3331\u001b[0m  0.0274\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m82.4056\u001b[0m       \u001b[32m67.8825\u001b[0m  0.0263\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m92.1468\u001b[0m       \u001b[32m74.4541\u001b[0m  0.0264\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m74.5691\u001b[0m       \u001b[32m61.6600\u001b[0m  0.0290\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m104.1331\u001b[0m       \u001b[32m88.4039\u001b[0m  0.0262\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      2       \u001b[36m67.9982\u001b[0m       \u001b[32m61.7857\u001b[0m  0.0250\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2       \u001b[36m70.4353\u001b[0m       \u001b[32m62.4588\u001b[0m  0.0283\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m153.2199\u001b[0m      \u001b[32m130.3840\u001b[0m  0.0291\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m147.2541\u001b[0m      \u001b[32m125.1497\u001b[0m  0.0325\n",
      "      2       \u001b[36m78.1036\u001b[0m       \u001b[32m69.4292\u001b[0m  0.0329\n",
      "      2       \u001b[36m88.3260\u001b[0m       \u001b[32m80.1357\u001b[0m  0.0251\n",
      "      2       \u001b[36m64.2349\u001b[0m       \u001b[32m57.2477\u001b[0m  0.0362\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m163.2561\u001b[0m      \u001b[32m133.0130\u001b[0m  0.0423\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m180.8738\u001b[0m      \u001b[32m149.1492\u001b[0m  0.0339\n",
      "      3       \u001b[36m65.4738\u001b[0m       \u001b[32m60.6032\u001b[0m  0.0339\n",
      "      3       \u001b[36m67.8018\u001b[0m       \u001b[32m61.2454\u001b[0m  0.0322\n",
      "      2      \u001b[36m135.5127\u001b[0m      \u001b[32m119.5326\u001b[0m  0.0290\n",
      "      3       \u001b[36m82.8081\u001b[0m       \u001b[32m77.7858\u001b[0m  0.0270\n",
      "      2      \u001b[36m132.2120\u001b[0m      \u001b[32m115.8498\u001b[0m  0.0316\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m209.0644\u001b[0m      \u001b[32m178.5398\u001b[0m  0.0375\n",
      "      3       \u001b[36m61.7448\u001b[0m       \u001b[32m56.5148\u001b[0m  0.0296\n",
      "      3       \u001b[36m74.8255\u001b[0m       \u001b[32m67.4208\u001b[0m  0.0375\n",
      "      2      \u001b[36m155.8841\u001b[0m      \u001b[32m138.3017\u001b[0m  0.0288\n",
      "      4       \u001b[36m64.6957\u001b[0m       \u001b[32m60.3304\u001b[0m  0.0298\n",
      "      2      \u001b[36m142.2709\u001b[0m      \u001b[32m121.9068\u001b[0m  0.0319\n",
      "      2      \u001b[36m179.5246\u001b[0m      \u001b[32m156.2480\u001b[0m  0.0289\n",
      "      4       \u001b[36m67.4504\u001b[0m       \u001b[32m61.0844\u001b[0m  0.0370\n",
      "      3      \u001b[36m129.0583\u001b[0m      \u001b[32m111.1008\u001b[0m  0.0309\n",
      "      4       \u001b[36m81.8673\u001b[0m       \u001b[32m76.9028\u001b[0m  0.0313\n",
      "      3      \u001b[36m132.4401\u001b[0m      \u001b[32m117.5730\u001b[0m  0.0412\n",
      "      4       \u001b[36m60.8594\u001b[0m       \u001b[32m55.7237\u001b[0m  0.0342\n",
      "      4       \u001b[36m74.0093\u001b[0m       \u001b[32m66.9238\u001b[0m  0.0330\n",
      "      5       \u001b[36m64.3393\u001b[0m       \u001b[32m59.9844\u001b[0m  0.0254\n",
      "      3      \u001b[36m150.4262\u001b[0m      \u001b[32m134.1774\u001b[0m  0.0314\n",
      "      3      \u001b[36m138.5407\u001b[0m      \u001b[32m120.7676\u001b[0m  0.0309\n",
      "      3      \u001b[36m168.4570\u001b[0m      \u001b[32m151.4915\u001b[0m  0.0311\n",
      "      5       \u001b[36m67.2623\u001b[0m       \u001b[32m60.9632\u001b[0m  0.0311\n",
      "      5       \u001b[36m60.3453\u001b[0m       \u001b[32m55.5756\u001b[0m  0.0331\n",
      "      4      \u001b[36m125.5170\u001b[0m      \u001b[32m110.5053\u001b[0m  0.0394\n",
      "      5       \u001b[36m73.6271\u001b[0m       \u001b[32m66.7908\u001b[0m  0.0300\n",
      "      4      \u001b[36m129.2777\u001b[0m      \u001b[32m116.7769\u001b[0m  0.0385\n",
      "      6       \u001b[36m64.1534\u001b[0m       \u001b[32m59.8596\u001b[0m  0.0298\n",
      "      5       \u001b[36m81.2098\u001b[0m       \u001b[32m76.4105\u001b[0m  0.0442\n",
      "      4      \u001b[36m136.6791\u001b[0m      120.8455  0.0290\n",
      "      4      \u001b[36m148.2634\u001b[0m      \u001b[32m133.2513\u001b[0m  0.0369\n",
      "      6       \u001b[36m67.1384\u001b[0m       \u001b[32m60.9494\u001b[0m  0.0259\n",
      "      4      \u001b[36m164.3351\u001b[0m      \u001b[32m150.5223\u001b[0m  0.0312\n",
      "      6       \u001b[36m60.3121\u001b[0m       \u001b[32m55.3635\u001b[0m  0.0241\n",
      "      6       \u001b[36m73.3706\u001b[0m       66.8635  0.0270\n",
      "      6       \u001b[36m80.2257\u001b[0m       \u001b[32m75.4766\u001b[0m  0.0296\n",
      "      5      \u001b[36m123.3203\u001b[0m      \u001b[32m108.3857\u001b[0m  0.0388\n",
      "      5      \u001b[36m136.0097\u001b[0m      \u001b[32m119.8273\u001b[0m  0.0327\n",
      "      5      \u001b[36m128.8623\u001b[0m      \u001b[32m116.0172\u001b[0m  0.0386\n",
      "      7       \u001b[36m63.6402\u001b[0m       \u001b[32m59.3956\u001b[0m  0.0392\n",
      "      7       \u001b[36m60.0496\u001b[0m       55.4379  0.0253\n",
      "      5      \u001b[36m163.0986\u001b[0m      \u001b[32m150.3013\u001b[0m  0.0274\n",
      "      7       67.1456       \u001b[32m60.7782\u001b[0m  0.0379\n",
      "      7       \u001b[36m72.7721\u001b[0m       \u001b[32m66.6583\u001b[0m  0.0293\n",
      "      5      \u001b[36m147.4040\u001b[0m      \u001b[32m133.2127\u001b[0m  0.0423\n",
      "      7       \u001b[36m79.7389\u001b[0m       \u001b[32m75.1050\u001b[0m  0.0311\n",
      "      6      \u001b[36m121.6916\u001b[0m      108.4229  0.0278\n",
      "      8       \u001b[36m63.5748\u001b[0m       59.4342  0.0288\n",
      "      6      \u001b[36m127.3220\u001b[0m      116.2592  0.0317\n",
      "      6      \u001b[36m135.5662\u001b[0m      120.2723  0.0330\n",
      "      8       \u001b[36m59.9053\u001b[0m       \u001b[32m55.1697\u001b[0m  0.0316\n",
      "      8       \u001b[36m66.8792\u001b[0m       60.9872  0.0336\n",
      "      6      \u001b[36m146.6429\u001b[0m      \u001b[32m132.3886\u001b[0m  0.0317\n",
      "      8       \u001b[36m78.9199\u001b[0m       \u001b[32m75.0803\u001b[0m  0.0262\n",
      "      6      \u001b[36m162.0958\u001b[0m      \u001b[32m148.2319\u001b[0m  0.0477\n",
      "      7      \u001b[36m121.3072\u001b[0m      108.7225  0.0347\n",
      "      7      \u001b[36m127.3046\u001b[0m      116.2458  0.0291\n",
      "      8       \u001b[36m72.4201\u001b[0m       66.6785  0.0474\n",
      "      9       \u001b[36m59.6966\u001b[0m       \u001b[32m55.1261\u001b[0m  0.0254\n",
      "      7      \u001b[36m135.0028\u001b[0m      \u001b[32m119.3569\u001b[0m  0.0322\n",
      "      9       \u001b[36m63.5572\u001b[0m       59.4630  0.0371\n",
      "      7      \u001b[36m145.4540\u001b[0m      \u001b[32m131.2888\u001b[0m  0.0268\n",
      "      9       \u001b[36m78.8727\u001b[0m       \u001b[32m75.0500\u001b[0m  0.0280\n",
      "      8      \u001b[36m120.1523\u001b[0m      108.8229  0.0272\n",
      "      7      \u001b[36m161.3166\u001b[0m      \u001b[32m148.2297\u001b[0m  0.0329\n",
      "     10       \u001b[36m59.6937\u001b[0m       55.1532  0.0239\n",
      "      9       \u001b[36m66.3377\u001b[0m       \u001b[32m60.6478\u001b[0m  0.0438\n",
      "      9       \u001b[36m72.3625\u001b[0m       \u001b[32m66.3577\u001b[0m  0.0266\n",
      "     10       \u001b[36m63.4438\u001b[0m       59.5757  0.0245\n",
      "      8      135.0148      119.7718  0.0368\n",
      "      8      \u001b[36m144.8275\u001b[0m      \u001b[32m130.6335\u001b[0m  0.0289\n",
      "      8      \u001b[36m126.8074\u001b[0m      \u001b[32m115.9144\u001b[0m  0.0489\n",
      "     10       \u001b[36m78.8659\u001b[0m       75.0542  0.0328\n",
      "      9      120.1602      \u001b[32m108.0687\u001b[0m  0.0286\n",
      "      8      \u001b[36m161.1958\u001b[0m      \u001b[32m148.1876\u001b[0m  0.0311\n",
      "     10       \u001b[36m66.1651\u001b[0m       60.7091  0.0305\n",
      "     11       \u001b[36m63.3770\u001b[0m       \u001b[32m59.3310\u001b[0m  0.0310\n",
      "     11       \u001b[36m59.6469\u001b[0m       55.1785  0.0350\n",
      "     10       \u001b[36m72.2879\u001b[0m       \u001b[32m66.3494\u001b[0m  0.0372\n",
      "      9      \u001b[36m134.8607\u001b[0m      120.5196  0.0401\n",
      "      9      \u001b[36m144.6156\u001b[0m      \u001b[32m130.4884\u001b[0m  0.0356\n",
      "      9      126.8247      \u001b[32m115.9119\u001b[0m  0.0350\n",
      "     10      \u001b[36m119.7445\u001b[0m      \u001b[32m108.0665\u001b[0m  0.0301\n",
      "      9      \u001b[36m160.8907\u001b[0m      148.5926  0.0279\n",
      "     12       \u001b[36m63.3632\u001b[0m       \u001b[32m59.1611\u001b[0m  0.0240\n",
      "     12       \u001b[36m59.6339\u001b[0m       55.1784  0.0274\n",
      "     11       72.2913       \u001b[32m66.3441\u001b[0m  0.0249\n",
      "     11       \u001b[36m78.8609\u001b[0m       75.0513  0.0417\n",
      "     11       \u001b[36m66.0300\u001b[0m       60.7023  0.0419\n",
      "     10      \u001b[36m160.6055\u001b[0m      148.6435  0.0257\n",
      "     10      134.8806      120.0536  0.0313\n",
      "     10      \u001b[36m144.5515\u001b[0m      \u001b[32m130.4766\u001b[0m  0.0313\n",
      "     13       59.6699       55.1766  0.0252\n",
      "     11      119.9151      108.6952  0.0315\n",
      "     10      \u001b[36m126.7382\u001b[0m      \u001b[32m115.9082\u001b[0m  0.0327\n",
      "     12       72.3121       66.3471  0.0295\n",
      "     12       \u001b[36m78.8582\u001b[0m       75.0705  0.0294\n",
      "     13       \u001b[36m63.2550\u001b[0m       59.2853  0.0390\n",
      "     12       66.0456       60.7048  0.0363\n",
      "     14       59.6501       55.1800  0.0258\n",
      "     11      160.7211      148.2859  0.0291\n",
      "     11      \u001b[36m144.1170\u001b[0m      130.5732  0.0319\n",
      "     11      \u001b[36m134.6433\u001b[0m      \u001b[32m118.2762\u001b[0m  0.0354\n",
      "     14       63.2657       59.2040  0.0242\n",
      "     13       72.3048       \u001b[32m66.3033\u001b[0m  0.0303\n",
      "     11      \u001b[36m126.7040\u001b[0m      115.9797  0.0341\n",
      "     12      \u001b[36m119.2391\u001b[0m      108.9016  0.0372\n",
      "     13       78.9058       75.0695  0.0353\n",
      "     13       66.0614       60.7032  0.0271\n",
      "     12      144.4270      130.5836  0.0258\n",
      "     12      \u001b[36m134.2494\u001b[0m      118.4919  0.0270\n",
      "     14       \u001b[36m72.1897\u001b[0m       \u001b[32m66.1695\u001b[0m  0.0259\n",
      "     13      119.4517      108.5311  0.0260\n",
      "     15       \u001b[36m59.5787\u001b[0m       55.1799  0.0563\n",
      "     12      \u001b[36m160.4300\u001b[0m      149.4634  0.0715\n",
      "     12      \u001b[36m126.6823\u001b[0m      115.9353  0.0776\n",
      "     14       78.9018       75.0638  0.0745\n",
      "     15       \u001b[36m63.1189\u001b[0m       59.4239  0.0915\n",
      "     13      144.2489      130.9714  0.0724\n",
      "     13      \u001b[36m134.0131\u001b[0m      \u001b[32m117.7715\u001b[0m  0.0781\n",
      "     14       66.0932       60.7695  0.0905\n",
      "     16       \u001b[36m59.5526\u001b[0m       55.1809  0.0591\n",
      "     14      119.6500      108.5573  0.0767\n",
      "     13      \u001b[36m159.9090\u001b[0m      148.7134  0.0530\n",
      "     15       \u001b[36m78.8500\u001b[0m       75.0676  0.0389\n",
      "     16       63.1366       59.2014  0.0331\n",
      "     14      144.3467      130.8919  0.0354\n",
      "     15       \u001b[36m71.9991\u001b[0m       66.2019  0.1020\n",
      "     13      126.8722      116.1521  0.0558\n",
      "     14      \u001b[36m133.6243\u001b[0m      117.7762  0.0334\n",
      "     17       \u001b[36m59.5379\u001b[0m       55.1802  0.0337\n",
      "     15       \u001b[36m65.9515\u001b[0m       60.8782  0.0417\n",
      "     14      \u001b[36m159.2356\u001b[0m      148.7140  0.0382\n",
      "     15      119.8093      108.5460  0.0496\n",
      "     17       63.2923       \u001b[32m59.1327\u001b[0m  0.0342\n",
      "     16       \u001b[36m71.9303\u001b[0m       66.2016  0.0350\n",
      "     14      \u001b[36m126.6779\u001b[0m      \u001b[32m115.7957\u001b[0m  0.0329\n",
      "     16       78.8903       75.0584  0.0487\n",
      "     15      144.1343      130.5044  0.0460\n",
      "     16       66.0036       \u001b[32m60.5972\u001b[0m  0.0284\n",
      "     15      \u001b[36m133.5043\u001b[0m      117.8864  0.0379\n",
      "     18       59.5932       55.1743  0.0385\n",
      "     18       \u001b[36m63.1152\u001b[0m       \u001b[32m59.0475\u001b[0m  0.0241\n",
      "     16      119.3718      108.5743  0.0331\n",
      "     17       \u001b[36m71.8936\u001b[0m       66.2015  0.0261\n",
      "     15      159.8896      148.6841  0.0413\n",
      "     15      \u001b[36m126.5708\u001b[0m      115.8353  0.0280\n",
      "     17       \u001b[36m78.8425\u001b[0m       75.0553  0.0273\n",
      "     19       59.5711       55.1776  0.0244\n",
      "     17       66.0161       60.6248  0.0295\n",
      "     19       \u001b[36m63.1047\u001b[0m       59.1395  0.0236\n",
      "     16      144.3062      130.7610  0.0425\n",
      "     18       \u001b[36m71.8934\u001b[0m       66.2008  0.0278\n",
      "     16      133.8098      118.3910  0.0487\n",
      "     16      159.5535      \u001b[32m147.3297\u001b[0m  0.0299\n",
      "     17      119.4881      108.5713  0.0369\n",
      "     18       78.9033       75.0531  0.0307\n",
      "     16      126.8281      115.9770  0.0326\n",
      "     20       59.5636       55.1774  0.0296\n",
      "     18       65.9992       60.7978  0.0336\n",
      "     20       63.1268       \u001b[32m59.0323\u001b[0m  0.0304\n",
      "     17      144.1567      130.5274  0.0285\n",
      "     19       71.8938       66.2008  0.0279\n",
      "     17      \u001b[36m133.4619\u001b[0m      \u001b[32m117.5347\u001b[0m  0.0321\n",
      "     17      \u001b[36m126.5388\u001b[0m      116.0737  0.0263\n",
      "     21       59.5968       55.1703  0.0249\n",
      "     18      119.6598      \u001b[32m107.7767\u001b[0m  0.0362\n",
      "     19       78.9019       \u001b[32m75.0486\u001b[0m  0.0337\n",
      "     19       66.1355       \u001b[32m60.4811\u001b[0m  0.0249\n",
      "     21       \u001b[36m63.0799\u001b[0m       \u001b[32m59.0192\u001b[0m  0.0247\n",
      "     17      \u001b[36m158.8296\u001b[0m      \u001b[32m147.3115\u001b[0m  0.0395\n",
      "     18      \u001b[36m133.2393\u001b[0m      \u001b[32m117.4465\u001b[0m  0.0261\n",
      "     20       \u001b[36m71.8916\u001b[0m       66.2006  0.0359\n",
      "     18      \u001b[36m126.3706\u001b[0m      116.0357  0.0272\n",
      "     18      \u001b[36m143.8758\u001b[0m      130.5317  0.0432\n",
      "     22       59.5994       55.1788  0.0320\n",
      "     22       63.1836       59.1439  0.0263\n",
      "     19      119.7024      107.8646  0.0292\n",
      "     20       65.9990       \u001b[32m60.3479\u001b[0m  0.0358\n",
      "     18      159.0432      \u001b[32m147.2308\u001b[0m  0.0337\n",
      "     20       78.8674       \u001b[32m75.0484\u001b[0m  0.0395\n",
      "     21       71.8930       66.2008  0.0320\n",
      "     19      \u001b[36m126.3581\u001b[0m      \u001b[32m115.7663\u001b[0m  0.0333\n",
      "     23       59.5982       55.1794  0.0282\n",
      "     23       63.0968       59.1457  0.0289\n",
      "     19      133.2996      \u001b[32m117.4298\u001b[0m  0.0387\n",
      "     19      143.9286      130.4950  0.0375\n",
      "     20      119.3390      108.2054  0.0339\n",
      "     21       78.8615       75.0486  0.0242\n",
      "     19      \u001b[36m158.4201\u001b[0m      147.6200  0.0269\n",
      "     21       66.1146       60.5116  0.0386\n",
      "     22       71.8922       66.2005  0.0259\n",
      "     24       59.5744       55.1781  0.0245\n",
      "     20      126.5876      \u001b[32m115.4844\u001b[0m  0.0266\n",
      "     24       \u001b[36m63.0522\u001b[0m       59.1476  0.0249\n",
      "     20      143.9289      130.5073  0.0280\n",
      "     21      119.3889      107.8765  0.0319\n",
      "     22       78.8594       75.0491  0.0340\n",
      "     22       66.0569       60.6897  0.0288\n",
      "     25       59.5978       55.1795  0.0264\n",
      "     20      133.5755      117.4514  0.0459\n",
      "     20      \u001b[36m158.2738\u001b[0m      147.3401  0.0436\n",
      "     23       71.8937       66.2006  0.0324\n",
      "     25       63.0723       59.1134  0.0311\n",
      "     21      \u001b[36m126.3272\u001b[0m      115.6568  0.0319\n",
      "     21      144.4796      130.5102  0.0308\n",
      "     23       78.8762       75.0491  0.0262\n",
      "     23       \u001b[36m65.9393\u001b[0m       60.3781  0.0266\n",
      "     26       59.5680       55.1800  0.0270\n",
      "     24       71.8951       66.2004  0.0237\n",
      "     21      158.6814      147.3211  0.0273\n",
      "     22      119.3806      \u001b[32m107.7750\u001b[0m  0.0427\n",
      "     26       63.0763       59.1335  0.0294\n",
      "     21      133.2443      117.5203  0.0407\n",
      "     22      144.1426      130.5183  0.0256\n",
      "     24       78.8836       \u001b[32m75.0480\u001b[0m  0.0255\n",
      "     24       66.0083       \u001b[32m60.2895\u001b[0m  0.0258\n",
      "     27       59.5535       55.1822  0.0239\n",
      "     22      158.6553      147.3320  0.0273\n",
      "     22      \u001b[36m126.1301\u001b[0m      115.5272  0.0507\n",
      "     27       \u001b[36m63.0338\u001b[0m       59.0847  0.0286\n",
      "     25       71.8928       66.2003  0.0379\n",
      "     22      133.5831      117.5698  0.0275\n",
      "     23      119.6086      108.2110  0.0369\n",
      "     25       78.8634       \u001b[32m75.0474\u001b[0m  0.0322\n",
      "     23      144.4995      130.6221  0.0343\n",
      "     25       66.0043       60.4831  0.0320\n",
      "     28       59.5718       55.1825  0.0350\n",
      "     23      158.3103      147.3360  0.0328\n",
      "     28       63.0899       59.0749  0.0247\n",
      "     23      126.5625      115.9732  0.0340\n",
      "     26       71.8928       66.2003  0.0335\n",
      "     26       78.8640       \u001b[32m75.0473\u001b[0m  0.0246\n",
      "     24      119.5317      107.9853  0.0347\n",
      "     23      133.2710      117.7735  0.0355\n",
      "     24      144.2438      130.6046  0.0347\n",
      "     26       66.0144       60.6721  0.0299\n",
      "     24      158.6405      147.3253  0.0257\n",
      "     29       \u001b[36m62.9887\u001b[0m       59.0485  0.0269\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 9.\n",
      "score functions types     27       78.8647       75.0474  0.0258\n",
      " <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     24      126.5218      115.6830  0.0348\n",
      "     27       71.8926       66.2003  0.0283\n",
      "     25      119.3416      108.1141  0.0317\n",
      "     25      144.1710      130.7326  0.0310\n",
      "     24      133.4438      117.7595  0.0383\n",
      "     25      \u001b[36m158.0320\u001b[0m      147.3322  0.0331\n",
      "     27       65.9960       60.6743  0.0353\n",
      "     30       63.0582       \u001b[32m58.9519\u001b[0m  0.0332\n",
      "     28       71.8922       66.2004  0.0245\n",
      "     28       78.8626       75.0476  0.0262\n",
      "     25      \u001b[36m126.0788\u001b[0m      115.7829  0.0300\n",
      "     26      119.2856      107.8187  0.0273\n",
      "     25      133.2410      117.8479  0.0314\n",
      "     26      158.1564      147.3209  0.0276\n",
      "     31       \u001b[36m62.9545\u001b[0m       59.0901  0.0268\n",
      "     28       65.9413       60.6598  0.0291\n",
      "     29       \u001b[36m71.8876\u001b[0m       66.2004  0.0258\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 9.\n",
      "     26      126.4730      115.7985  0.0270\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     27      119.4215      107.8462  0.0308\n",
      "     26      144.3267      130.5302  0.0528\n",
      "     32       63.0245       59.0700  0.0310\n",
      "     27      158.3642      147.3149  0.0344\n",
      "     26      133.6827      117.8838  0.0391\n",
      "     30       \u001b[36m71.8841\u001b[0m       66.2003  0.0389\n",
      "     28      119.5353      107.8264  0.0258\n",
      "     27      144.4810      130.4850  0.0265\n",
      "     29       65.9758       60.7991  0.0466\n",
      "     27      126.3756      115.7108  0.0459\n",
      "     33       63.0309       59.0569  0.0282\n",
      "     28      \u001b[36m157.9893\u001b[0m      \u001b[32m146.4468\u001b[0m  0.0283\n",
      "     31       71.9350       66.2008  0.0249\n",
      "     29      119.5340      108.1650  0.0275\n",
      "     27      133.2737      117.8645  0.0339\n",
      "     30       66.0715       60.4878  0.0260\n",
      "     28      144.2581      \u001b[32m130.0411\u001b[0m  0.0293\n",
      "     28      126.6599      115.8137  0.0277\n",
      "     34       62.9595       59.0886  0.0236\n",
      "     32       71.8922       66.2011  0.0239\n",
      "     30      119.6896      108.1429  0.0271\n",
      "     29      \u001b[36m157.7998\u001b[0m      146.4479  0.0354\n",
      "     28      \u001b[36m133.2170\u001b[0m      117.9389  0.0303\n",
      "     31       65.9879       60.6582  0.0280\n",
      "     29      143.9339      130.5637  0.0302\n",
      "     29      126.6201      115.6483  0.0294\n",
      "     33       71.8930       66.2019  0.0267\n",
      "     35       63.0437       59.0048  0.0338\n",
      "     31      119.2807      108.2000  0.0287\n",
      "     29      \u001b[36m133.2130\u001b[0m      117.8574  0.0263\n",
      "     30      \u001b[36m157.6808\u001b[0m      146.5159  0.0299\n",
      "     32       65.9781       60.4806  0.0285\n",
      "     30      144.2693      130.5730  0.0281\n",
      "     30      126.1469      115.7925  0.0263\n",
      "     36       63.0061       59.0652  0.0249\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 14.\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     31      \u001b[36m157.6750\u001b[0m      146.5403  0.0258\n",
      "     32      119.3611      107.8086  0.0295\n",
      "     30      133.3191      119.3486  0.0306\n",
      "     31      144.0042      130.5928  0.0262\n",
      "     33       65.9951       60.5323  0.0285\n",
      "     31      126.2121      116.2506  0.0280\n",
      "     37       \u001b[36m62.9522\u001b[0m       59.0775  0.0245\n",
      "     32      \u001b[36m157.6127\u001b[0m      146.5175  0.0270\n",
      "     33      \u001b[36m119.2033\u001b[0m      107.8561  0.0267\n",
      "     34       \u001b[36m65.9347\u001b[0m       60.4010  0.0251\n",
      "     31      133.3631      118.9909  0.0281\n",
      "     32      144.1047      130.6907  0.0275\n",
      "     38       63.0275       59.0148  0.0237\n",
      "     32      126.1417      116.1881  0.0284\n",
      "     33      157.6225      \u001b[32m146.4329\u001b[0m  0.0266\n",
      "     35       \u001b[36m65.9094\u001b[0m       60.3092  0.0257\n",
      "     34      119.4513      \u001b[32m107.5714\u001b[0m  0.0273\n",
      "     32      \u001b[36m133.1347\u001b[0m      118.0680  0.0272\n",
      "     39       62.9885       59.1758  0.0247\n",
      "     33      143.9153      130.5667  0.0300\n",
      "     33      126.5662      116.7703  0.0269\n",
      "     34      157.7874      \u001b[32m146.3550\u001b[0m  0.0273\n",
      "     36       65.9133       \u001b[32m60.2380\u001b[0m  0.0255\n",
      "     35      119.4096      108.0065  0.0267\n",
      "     40       63.1422       \u001b[32m58.9516\u001b[0m  0.0236\n",
      "     33      133.2609      118.2081  0.0271\n",
      "     34      144.1824      130.5730  0.0285\n",
      "     34      126.4480      116.0402  0.0269\n",
      "     37       \u001b[36m65.8866\u001b[0m       60.2930  0.0256\n",
      "     35      \u001b[36m157.6017\u001b[0m      146.5442  0.0267\n",
      "     36      119.3620      108.1426  0.0266\n",
      "     41       63.1946       59.0828  0.0244\n",
      "     34      133.2596      118.5432  0.0277\n",
      "     35      126.6568      116.1186  0.0269\n",
      "     35      \u001b[36m143.7490\u001b[0m      130.6738  0.0309\n",
      "     38       \u001b[36m65.8843\u001b[0m       \u001b[32m60.2376\u001b[0m  0.0250\n",
      "     42       63.0205       59.2033  0.0236\n",
      "     36      \u001b[36m157.5594\u001b[0m      146.5864  0.0264\n",
      "     37      119.2903      108.1390  0.0270\n",
      "     35      \u001b[36m133.0056\u001b[0m      118.6502  0.0278\n",
      "     36      126.4031      115.9455  0.0269\n",
      "     36      143.9473      130.5560  0.0280\n",
      "     43       63.0850       59.0552  0.0244\n",
      "     39       65.9250       \u001b[32m60.1911\u001b[0m  0.0260\n",
      "     37      157.8669      146.5863  0.0269\n",
      "     38      119.2787      108.2129  0.0259\n",
      "     36      133.2880      118.6016  0.0265\n",
      "     37      126.0999      116.6435  0.0259\n",
      "     37      143.9725      130.5209  0.0267\n",
      "     44       63.0372       59.0797  0.0252\n",
      "     40       65.9386       60.1987  0.0275\n",
      "     38      157.6314      146.5093  0.0266\n",
      "     39      119.4281      108.3980  0.0281\n",
      "     37      133.2670      118.9821  0.0290\n",
      "     38      126.6905      115.6312  0.0279\n",
      "     38      144.0352      130.6152  0.0358\n",
      "     45       62.9964       59.0761  0.0299\n",
      "     41       65.9233       60.4315  0.0335\n",
      "     40      119.3657      108.0604  0.0301\n",
      "     39      157.7374      146.5113  0.0330\n",
      "     38      133.3267      118.4752  0.0345\n",
      "     39      126.8148      115.5386  0.0387\n",
      "     39      144.0804      130.6391  0.0290\n",
      "     46       62.9868       \u001b[32m58.9073\u001b[0m  0.0291\n",
      "     40      157.7855      146.5032  0.0264\n",
      "     42       66.0017       60.2851  0.0290\n",
      "     41      \u001b[36m119.0736\u001b[0m      107.8732  0.0302\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 19.\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 20.\n",
      "     40      144.1985      130.5260  0.0285\n",
      "     47       \u001b[36m62.9416\u001b[0m       59.0440  0.0258\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     41      157.6391      146.5101  0.0266\n",
      "     43       65.9439       60.4200  0.0259\n",
      "     42      119.2299      \u001b[32m107.4807\u001b[0m  0.0269\n",
      "     48       62.9605       59.1569  0.0255\n",
      "     41      143.8175      130.7476  0.0270\n",
      "     42      157.6277      146.5131  0.0264\n",
      "     44       65.9521       60.3514  0.0267\n",
      "     43      \u001b[36m118.8486\u001b[0m      \u001b[32m107.4801\u001b[0m  0.0261\n",
      "     49       \u001b[36m62.9255\u001b[0m       59.1114  0.0246\n",
      "     42      144.1817      130.5385  0.0263\n",
      "     43      157.6172      146.7427  0.0262\n",
      "     45       \u001b[36m65.8788\u001b[0m       60.2682  0.0263\n",
      "     44      119.0624      107.4834  0.0282\n",
      "     50       \u001b[36m62.8884\u001b[0m       59.0813  0.0249\n",
      "Restoring best model from epoch 46.\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     43      144.0712      130.5176  0.0268\n",
      "     44      157.6611      146.7427  0.0262\n",
      "     46       65.9483       \u001b[32m60.1781\u001b[0m  0.0257\n",
      "     45      118.9986      \u001b[32m107.4786\u001b[0m  0.0271\n",
      "     44      143.9650      130.5181  0.0266\n",
      "     45      157.6609      146.7427  0.0264\n",
      "     47       65.9038       60.3744  0.0268\n",
      "     46      119.1182      107.8470  0.0270\n",
      "     45      144.0798      130.3501  0.0272\n",
      "     46      157.6813      146.7427  0.0259\n",
      "     48       65.9111       \u001b[32m60.0758\u001b[0m  0.0256\n",
      "     47      119.2802      107.7685  0.0265\n",
      "     46      143.8030      \u001b[32m129.8442\u001b[0m  0.0265\n",
      "     47      157.6837      146.7427  0.0260\n",
      "     49       \u001b[36m65.8647\u001b[0m       60.2512  0.0257\n",
      "     48      119.0270      107.7149  0.0267\n",
      "     47      143.9153      130.3004  0.0264\n",
      "     50       \u001b[36m65.8391\u001b[0m       60.4864  0.0260\n",
      "Restoring best model from epoch 48.\n",
      "     48      157.6824      146.7427  0.0276\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     49      118.9448      107.7681  0.0258\n",
      "     48      143.7496      130.3005  0.0269\n",
      "     49      157.6616      146.7427  0.0251\n",
      "     50      119.0315      \u001b[32m107.3697\u001b[0m  0.0259\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     50      157.6606      146.7427  0.0251\n",
      "Restoring best model from epoch 34.\n",
      "     49      143.9528      129.9743  0.0259\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     50      143.7727      130.3071  0.0254\n",
      "Restoring best model from epoch 46.\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m85.3571\u001b[0m       \u001b[32m70.4187\u001b[0m  0.0355\n",
      "      2       \u001b[36m73.2440\u001b[0m       \u001b[32m66.2559\u001b[0m  0.0418\n",
      "      3       \u001b[36m69.1376\u001b[0m       \u001b[32m64.5872\u001b[0m  0.0361\n",
      "      4       \u001b[36m68.4416\u001b[0m       \u001b[32m63.9675\u001b[0m  0.0440\n",
      "      5       \u001b[36m68.0081\u001b[0m       \u001b[32m63.7214\u001b[0m  0.0362\n",
      "      6       \u001b[36m67.8967\u001b[0m       \u001b[32m63.6330\u001b[0m  0.0326\n",
      "      7       \u001b[36m67.6471\u001b[0m       \u001b[32m63.5548\u001b[0m  0.0369\n",
      "      8       67.7031       63.5584  0.0388\n",
      "      9       67.6681       63.5695  0.0361\n",
      "     10       67.6695       63.5570  0.0336\n",
      "     11       67.6641       63.5568  0.0350\n",
      "     12       67.6491       63.5622  0.0445\n",
      "     13       \u001b[36m67.6470\u001b[0m       63.6309  0.0363\n",
      "     14       67.6883       63.6881  0.0357\n",
      "     15       67.7088       63.5890  0.0337\n",
      "     16       67.6963       63.5688  0.0362\n",
      "     17       \u001b[36m67.6027\u001b[0m       63.5623  0.0330\n",
      "     18       67.6803       63.5744  0.0353\n",
      "     19       67.6342       63.5772  0.0442\n",
      "     20       67.6504       63.5587  0.0344\n",
      "     21       67.6976       63.5565  0.0342\n",
      "     22       67.6683       63.5563  0.0338\n",
      "     23       67.6222       63.5649  0.0360\n",
      "     24       67.6935       63.6915  0.0364\n",
      "     25       67.6774       63.5553  0.0332\n",
      "     26       67.6616       63.5551  0.0438\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 7.\n",
      "durations 1.87269 84.0\n",
      "dtype X_train horm_treatment      category\n",
      "menopause           category\n",
      "age                  float32\n",
      "n_positive_nodes     float32\n",
      "progesterone         float32\n",
      "estrogene            float32\n",
      "grade_0.0              uint8\n",
      "grade_1.0              uint8\n",
      "grade_2.0              uint8\n",
      "dtype: object\n",
      "dtype y_train float64\n",
      "dtype X_test horm_treatment      category\n",
      "menopause           category\n",
      "age                  float32\n",
      "n_positive_nodes     float32\n",
      "progesterone         float32\n",
      "estrogene            float32\n",
      "grade_0.0              uint8\n",
      "grade_1.0              uint8\n",
      "grade_2.0              uint8\n",
      "dtype: object\n",
      "dtype y_test float32\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m74.6363\u001b[0m       \u001b[32m62.2204\u001b[0m  0.0258\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m78.4195\u001b[0m       \u001b[32m65.2886\u001b[0m  0.0258\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m82.3046\u001b[0m       \u001b[32m67.5606\u001b[0m  0.0279\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m90.1723\u001b[0m       \u001b[32m74.6064\u001b[0m  0.0256\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m103.4496\u001b[0m       \u001b[32m86.1623\u001b[0m  0.0266\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2       \u001b[36m65.1611\u001b[0m       \u001b[32m57.3646\u001b[0m  0.0244\n",
      "      2       \u001b[36m68.1465\u001b[0m       \u001b[32m59.9841\u001b[0m  0.0261\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m146.3632\u001b[0m      \u001b[32m120.9330\u001b[0m  0.0324\n",
      "      2       \u001b[36m70.4532\u001b[0m       \u001b[32m63.7343\u001b[0m  0.0266\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2       \u001b[36m76.7286\u001b[0m       \u001b[32m70.5112\u001b[0m  0.0297\n",
      "      2       \u001b[36m87.8028\u001b[0m       \u001b[32m77.9038\u001b[0m  0.0269\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m155.3712\u001b[0m      \u001b[32m131.0516\u001b[0m  0.0307\n",
      "      3       \u001b[36m61.9375\u001b[0m       \u001b[32m56.0260\u001b[0m  0.0260\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m163.1893\u001b[0m      \u001b[32m133.9181\u001b[0m  0.0298\n",
      "      3       \u001b[36m65.5032\u001b[0m       \u001b[32m58.4438\u001b[0m  0.0255\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2      \u001b[36m131.1974\u001b[0m      \u001b[32m115.1957\u001b[0m  0.0283\n",
      "      3       \u001b[36m74.0609\u001b[0m       \u001b[32m68.6715\u001b[0m  0.0271\n",
      "      3       \u001b[36m68.1356\u001b[0m       \u001b[32m62.2972\u001b[0m  0.0408\n",
      "      2      \u001b[36m136.5482\u001b[0m      \u001b[32m116.8497\u001b[0m  0.0287\n",
      "      3       \u001b[36m83.1931\u001b[0m       \u001b[32m75.1289\u001b[0m  0.0321\n",
      "      4       \u001b[36m64.1408\u001b[0m       \u001b[32m57.7796\u001b[0m  0.0269\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m177.8723\u001b[0m      \u001b[32m147.7124\u001b[0m  0.0400\n",
      "      4       \u001b[36m60.8877\u001b[0m       \u001b[32m55.6926\u001b[0m  0.0350\n",
      "      2      \u001b[36m141.7330\u001b[0m      \u001b[32m123.1351\u001b[0m  0.0349\n",
      "      3      \u001b[36m129.1434\u001b[0m      \u001b[32m114.8739\u001b[0m  0.0291\n",
      "      4       \u001b[36m72.6400\u001b[0m       \u001b[32m67.8142\u001b[0m  0.0263\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m206.3881\u001b[0m      \u001b[32m169.3680\u001b[0m  0.0485\n",
      "      4       \u001b[36m67.2156\u001b[0m       \u001b[32m61.5824\u001b[0m  0.0283\n",
      "      4       \u001b[36m81.2099\u001b[0m       \u001b[32m74.9609\u001b[0m  0.0287\n",
      "      5       \u001b[36m63.6558\u001b[0m       \u001b[32m57.7056\u001b[0m  0.0260\n",
      "      3      \u001b[36m131.4728\u001b[0m      \u001b[32m114.7184\u001b[0m  0.0312\n",
      "      5       \u001b[36m60.3379\u001b[0m       55.8731  0.0285\n",
      "      2      \u001b[36m155.6012\u001b[0m      \u001b[32m138.0152\u001b[0m  0.0313\n",
      "      5       \u001b[36m72.0660\u001b[0m       \u001b[32m67.7175\u001b[0m  0.0237\n",
      "      3      \u001b[36m138.4259\u001b[0m      \u001b[32m120.9912\u001b[0m  0.0405\n",
      "      4      \u001b[36m127.3944\u001b[0m      \u001b[32m113.2040\u001b[0m  0.0440\n",
      "      5       \u001b[36m66.6671\u001b[0m       \u001b[32m61.1625\u001b[0m  0.0279\n",
      "      4      \u001b[36m129.0900\u001b[0m      \u001b[32m113.7937\u001b[0m  0.0264\n",
      "      6       \u001b[36m63.2565\u001b[0m       \u001b[32m57.6864\u001b[0m  0.0301\n",
      "      6       \u001b[36m59.4059\u001b[0m       55.8183  0.0275\n",
      "      5       \u001b[36m80.0448\u001b[0m       \u001b[32m74.4152\u001b[0m  0.0371\n",
      "      6       \u001b[36m71.7984\u001b[0m       67.7326  0.0257\n",
      "      2      \u001b[36m177.1387\u001b[0m      \u001b[32m150.1051\u001b[0m  0.0570\n",
      "      4      \u001b[36m137.1409\u001b[0m      \u001b[32m120.0344\u001b[0m  0.0335\n",
      "      3      \u001b[36m149.4818\u001b[0m      \u001b[32m134.5221\u001b[0m  0.0507\n",
      "      5      \u001b[36m129.0259\u001b[0m      \u001b[32m113.6079\u001b[0m  0.0301\n",
      "      5      \u001b[36m124.3330\u001b[0m      \u001b[32m109.5630\u001b[0m  0.0428\n",
      "      7       \u001b[36m63.1806\u001b[0m       57.7303  0.0390\n",
      "      6       \u001b[36m66.4262\u001b[0m       61.2167  0.0452\n",
      "      7       59.4369       55.7960  0.0473\n",
      "      6       \u001b[36m79.2333\u001b[0m       \u001b[32m74.1097\u001b[0m  0.0479\n",
      "      7       71.8642       \u001b[32m67.6679\u001b[0m  0.0544\n",
      "      4      \u001b[36m146.7649\u001b[0m      \u001b[32m132.7235\u001b[0m  0.0426\n",
      "      8       \u001b[36m62.7236\u001b[0m       57.6892  0.0325\n",
      "      3      \u001b[36m169.7157\u001b[0m      \u001b[32m145.8317\u001b[0m  0.0484\n",
      "      8       59.4518       55.7219  0.0291\n",
      "      7       \u001b[36m78.9370\u001b[0m       \u001b[32m73.7097\u001b[0m  0.0274\n",
      "      6      \u001b[36m122.3299\u001b[0m      \u001b[32m107.8638\u001b[0m  0.0437\n",
      "      5      \u001b[36m135.3698\u001b[0m      120.1234  0.0586\n",
      "      7       \u001b[36m66.1545\u001b[0m       61.2876  0.0444\n",
      "      6      \u001b[36m127.8403\u001b[0m      \u001b[32m112.7520\u001b[0m  0.0589\n",
      "      8       \u001b[36m71.6438\u001b[0m       \u001b[32m67.5723\u001b[0m  0.0317\n",
      "      9       59.4683       55.7880  0.0306\n",
      "      9       \u001b[36m62.5281\u001b[0m       57.7847  0.0374\n",
      "      4      \u001b[36m165.2621\u001b[0m      \u001b[32m145.6924\u001b[0m  0.0375\n",
      "      5      \u001b[36m145.4679\u001b[0m      133.0424  0.0416\n",
      "      8       \u001b[36m65.9980\u001b[0m       61.3706  0.0280\n",
      "      8       \u001b[36m78.4960\u001b[0m       \u001b[32m73.4862\u001b[0m  0.0334\n",
      "      6      \u001b[36m133.4240\u001b[0m      \u001b[32m119.9316\u001b[0m  0.0333\n",
      "      7      \u001b[36m126.8756\u001b[0m      \u001b[32m112.0811\u001b[0m  0.0321\n",
      "      9       \u001b[36m71.3844\u001b[0m       \u001b[32m67.3169\u001b[0m  0.0277\n",
      "      7      \u001b[36m120.7447\u001b[0m      108.0151  0.0433\n",
      "     10       59.5099       55.7476  0.0246\n",
      "      5      \u001b[36m163.0678\u001b[0m      145.7227  0.0289\n",
      "     10       62.6796       57.8041  0.0323\n",
      "      9       78.5190       73.4865  0.0254\n",
      "      9       \u001b[36m65.5814\u001b[0m       61.5425  0.0289\n",
      "      6      \u001b[36m144.2803\u001b[0m      133.4533  0.0309\n",
      "      8      \u001b[36m126.3056\u001b[0m      \u001b[32m111.7521\u001b[0m  0.0344\n",
      "     10       \u001b[36m70.8883\u001b[0m       \u001b[32m67.1532\u001b[0m  0.0381\n",
      "      7      \u001b[36m132.3271\u001b[0m      \u001b[32m119.8793\u001b[0m  0.0446\n",
      "     11       \u001b[36m59.3229\u001b[0m       55.8239  0.0282\n",
      "      8      \u001b[36m119.7669\u001b[0m      107.9216  0.0403\n",
      "      6      \u001b[36m160.4985\u001b[0m      \u001b[32m145.5144\u001b[0m  0.0297\n",
      "     11       62.6853       57.8849  0.0292\n",
      "     10       78.5462       \u001b[32m73.4786\u001b[0m  0.0313\n",
      "     10       65.7294       61.4823  0.0355\n",
      "      7      \u001b[36m143.5592\u001b[0m      \u001b[32m132.4164\u001b[0m  0.0396\n",
      "      9      \u001b[36m125.9135\u001b[0m      111.9914  0.0275\n",
      "     11       70.9449       67.3363  0.0283\n",
      "      8      \u001b[36m131.9980\u001b[0m      120.3289  0.0287\n",
      "     12       \u001b[36m59.2480\u001b[0m       55.8451  0.0322\n",
      "      7      \u001b[36m159.9189\u001b[0m      \u001b[32m144.9449\u001b[0m  0.0272\n",
      "     11       78.5058       73.4845  0.0273\n",
      "      9      120.2104      107.8903  0.0399\n",
      "     12       62.6364       57.7976  0.0372\n",
      "     10      126.5756      112.1295  0.0281\n",
      "     11       65.6218       61.5444  0.0343\n",
      "     12       70.9446       67.3507  0.0276\n",
      "      8      \u001b[36m143.2690\u001b[0m      \u001b[32m132.1815\u001b[0m  0.0374\n",
      "      9      \u001b[36m131.5310\u001b[0m      \u001b[32m119.5549\u001b[0m  0.0311\n",
      "      8      \u001b[36m159.5822\u001b[0m      \u001b[32m144.5622\u001b[0m  0.0288\n",
      "     13       59.4790       \u001b[32m55.5894\u001b[0m  0.0332\n",
      "     12       78.4993       73.4842  0.0398\n",
      "     13       \u001b[36m62.5124\u001b[0m       58.0157  0.0334\n",
      "     11      126.6314      112.7964  0.0322\n",
      "     10      \u001b[36m119.4786\u001b[0m      107.9047  0.0383\n",
      "     12       65.5851       61.6436  0.0328\n",
      "     13       70.9534       \u001b[32m67.1322\u001b[0m  0.0297\n",
      "      9      \u001b[36m142.9336\u001b[0m      \u001b[32m131.9979\u001b[0m  0.0297\n",
      "     10      \u001b[36m131.5297\u001b[0m      \u001b[32m119.2562\u001b[0m  0.0358\n",
      "      9      \u001b[36m159.0624\u001b[0m      144.6525  0.0328\n",
      "     14       62.6119       \u001b[32m57.6658\u001b[0m  0.0261\n",
      "     13       78.5015       73.4854  0.0333\n",
      "     14       70.9450       \u001b[32m67.1322\u001b[0m  0.0271\n",
      "     14       59.4363       \u001b[32m55.3732\u001b[0m  0.0515\n",
      "     12      \u001b[36m125.4596\u001b[0m      113.1866  0.0373\n",
      "     13       65.5917       61.4728  0.0411\n",
      "     14       78.5024       73.4844  0.0237\n",
      "     11      \u001b[36m131.2046\u001b[0m      \u001b[32m118.8323\u001b[0m  0.0395\n",
      "     10      \u001b[36m142.7577\u001b[0m      132.0853  0.0531\n",
      "     10      \u001b[36m158.7670\u001b[0m      \u001b[32m144.2282\u001b[0m  0.0454\n",
      "     11      \u001b[36m119.1805\u001b[0m      \u001b[32m107.6774\u001b[0m  0.0608\n",
      "     13      125.8472      113.2566  0.0282\n",
      "     15       59.2605       \u001b[32m55.3297\u001b[0m  0.0298\n",
      "     15       70.9170       \u001b[32m67.1299\u001b[0m  0.0378\n",
      "     15       62.7241       57.7396  0.0580\n",
      "     11      \u001b[36m142.3826\u001b[0m      132.1867  0.0272\n",
      "     12      \u001b[36m118.8957\u001b[0m      107.8976  0.0276\n",
      "     12      131.6732      118.9471  0.0362\n",
      "     14       65.6357       61.4086  0.0502\n",
      "     14      \u001b[36m125.0929\u001b[0m      113.2490  0.0344\n",
      "     15       78.5031       73.4841  0.0463\n",
      "     11      \u001b[36m158.6839\u001b[0m      \u001b[32m144.1189\u001b[0m  0.0387\n",
      "     16       59.2919       55.3658  0.0348\n",
      "     16       62.7248       57.8543  0.0248\n",
      "     16       \u001b[36m70.8875\u001b[0m       \u001b[32m67.1286\u001b[0m  0.0463\n",
      "     13      119.0834      107.7792  0.0268\n",
      "     13      \u001b[36m131.0050\u001b[0m      120.5806  0.0269\n",
      "     12      \u001b[36m142.3653\u001b[0m      132.0719  0.0355\n",
      "     16       78.4989       73.4836  0.0260\n",
      "     15       65.6051       61.4941  0.0385\n",
      "     15      125.4528      113.2486  0.0328\n",
      "     17       62.6225       57.7503  0.0323\n",
      "     12      \u001b[36m158.4953\u001b[0m      \u001b[32m143.8863\u001b[0m  0.0380\n",
      "     17       59.3882       55.5922  0.0387\n",
      "     14      119.6105      107.8332  0.0267\n",
      "     17       70.9275       67.1309  0.0287\n",
      "     14      \u001b[36m130.9726\u001b[0m      121.0618  0.0294\n",
      "     17       78.5024       73.4830  0.0249\n",
      "     13      \u001b[36m142.2655\u001b[0m      \u001b[32m131.7531\u001b[0m  0.0312\n",
      "     16      \u001b[36m125.0040\u001b[0m      113.2652  0.0267\n",
      "     16       \u001b[36m65.4730\u001b[0m       61.4492  0.0287\n",
      "     18       59.4402       \u001b[32m55.3020\u001b[0m  0.0287\n",
      "     18       62.7103       57.8716  0.0345\n",
      "     15      131.4889      120.8949  0.0266\n",
      "     13      \u001b[36m158.1744\u001b[0m      \u001b[32m143.6994\u001b[0m  0.0370\n",
      "     15      120.2078      107.8949  0.0346\n",
      "     18       70.9130       67.1311  0.0357\n",
      "     18       78.5048       73.4828  0.0328\n",
      "     14      142.3243      \u001b[32m131.7434\u001b[0m  0.0337\n",
      "     17      125.5907      113.2280  0.0343\n",
      "     19       59.2977       55.3415  0.0301\n",
      "     16      \u001b[36m130.6418\u001b[0m      120.7640  0.0272\n",
      "     17       \u001b[36m65.4621\u001b[0m       61.4371  0.0418\n",
      "     19       62.5964       57.8894  0.0342\n",
      "     16      120.1578      108.0860  0.0314\n",
      "     14      158.2684      \u001b[32m143.6923\u001b[0m  0.0343\n",
      "     19       78.5014       73.4824  0.0302\n",
      "     19       71.0137       67.1320  0.0357\n",
      "     15      142.5083      131.7518  0.0337\n",
      "     18      125.3235      112.7259  0.0315\n",
      "     18       \u001b[36m65.3600\u001b[0m       61.5333  0.0268\n",
      "     17      131.1840      120.0849  0.0333\n",
      "     20       62.6001       57.7958  0.0282\n",
      "     17      119.3323      107.9158  0.0284\n",
      "     20       \u001b[36m78.4950\u001b[0m       73.4819  0.0249\n",
      "     20       59.3234       \u001b[32m55.2957\u001b[0m  0.0439\n",
      "     15      \u001b[36m157.9387\u001b[0m      \u001b[32m143.2701\u001b[0m  0.0359\n",
      "     20       70.9653       67.1448  0.0326\n",
      "     16      142.5390      131.7545  0.0263\n",
      "     19      125.3865      112.8735  0.0325\n",
      "     19       65.5023       61.5342  0.0296\n",
      "     21       \u001b[36m62.4357\u001b[0m       57.8429  0.0248\n",
      "     21       70.9420       67.1378  0.0238\n",
      "     16      158.2894      143.6992  0.0261\n",
      "     21       78.5021       73.4835  0.0352\n",
      "     21       59.3265       55.3392  0.0330\n",
      "     17      142.2873      131.7661  0.0332\n",
      "     18      131.3710      119.5033  0.0478\n",
      "     18      119.1596      107.7783  0.0455\n",
      "     20       65.4683       61.4043  0.0261\n",
      "     20      125.1860      113.0715  0.0292\n",
      "     22       70.9499       67.1889  0.0275\n",
      "     22       62.7188       57.9225  0.0404\n",
      "     17      158.3079      144.0357  0.0311\n",
      "     22       59.3808       \u001b[32m55.1991\u001b[0m  0.0334\n",
      "     21      \u001b[36m124.8499\u001b[0m      112.6119  0.0358\n",
      "     19      131.0179      120.0112  0.0483\n",
      "     21       65.5242       61.1806  0.0476\n",
      "     22       78.5051       73.4821  0.0648\n",
      "     18      \u001b[36m142.1644\u001b[0m      131.8924  0.0739\n",
      "     23       62.5031       58.1215  0.0633\n",
      "     19      119.1140      108.3544  0.0829\n",
      "     23       70.9454       67.1491  0.0772\n",
      "     22      \u001b[36m124.7044\u001b[0m      112.5983  0.0576\n",
      "     23       59.3080       55.3739  0.0760\n",
      "     18      \u001b[36m157.9228\u001b[0m      143.3148  0.0841\n",
      "     19      142.2346      131.9575  0.0332\n",
      "     20      131.3365      120.6838  0.0651\n",
      "     22       65.5513       61.3875  0.0664\n",
      "     23       78.5014       73.4803  0.0650\n",
      "     20      119.1059      108.2894  0.0450\n",
      "     24       62.7239       58.0429  0.0487\n",
      "     24       59.2493       55.3772  0.0269\n",
      "     19      157.9448      \u001b[32m143.2596\u001b[0m  0.0286\n",
      "     20      \u001b[36m142.1100\u001b[0m      131.9239  0.0317\n",
      "     24       70.9257       67.1875  0.0446\n",
      "     23      124.7689      112.8505  0.0471\n",
      "     23       65.6018       61.1906  0.0401\n",
      "     25       59.2822       55.2560  0.0270\n",
      "     21      131.3428      119.9376  0.0486\n",
      "     24       78.5037       73.4819  0.0411\n",
      "     21      119.2954      108.5940  0.0413\n",
      "     25       70.9588       67.3169  0.0392\n",
      "     24      124.7770      112.5785  0.0313\n",
      "     25       62.6568       57.8503  0.0492\n",
      "     26       59.3028       55.4406  0.0242\n",
      "     20      \u001b[36m157.8359\u001b[0m      143.2661  0.0479\n",
      "     21      142.3727      131.8885  0.0487\n",
      "     24       65.4451       61.2745  0.0308\n",
      "     25       78.5009       73.4797  0.0334\n",
      "     22      119.1973      108.2172  0.0271\n",
      "     26       62.5935       57.9228  0.0240\n",
      "     26       70.9716       67.1718  0.0262\n",
      "     22      130.7764      120.4581  0.0473\n",
      "     21      \u001b[36m157.7825\u001b[0m      143.7084  0.0304     22      142.2477      131.9504  0.0273\n",
      "\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 5.\n",
      "     26       78.5017       73.4815  0.0268\n",
      "     27       59.3587       55.6656  0.0411\n",
      "     25      124.7121      112.6147  0.0469\n",
      "     27       62.5564       57.8294  0.0238\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     27       70.9088       67.1363  0.0300\n",
      "     23      118.9412      108.1681  0.0380\n",
      "     23      130.7288      120.1125  0.0307\n",
      "     23      142.3565      131.9163  0.0279\n",
      "     22      157.8605      143.7081  0.0290\n",
      "     27       78.4999       73.4793  0.0263\n",
      "     28       59.3786       55.4399  0.0277\n",
      "     26      124.7592      112.6419  0.0276\n",
      "     28       70.8997       \u001b[32m67.0580\u001b[0m  0.0249\n",
      "     28       62.5294       57.9241  0.0337\n",
      "     24      119.1004      108.1161  0.0324\n",
      "     24      142.4339      131.9675  0.0260\n",
      "     23      158.0277      143.7080  0.0259\n",
      "     24      130.7426      120.4669  0.0321\n",
      "     28       78.5031       73.4791  0.0239\n",
      "     27      124.8456      112.6247  0.0282\n",
      "     29       70.9537       67.1607  0.0237\n",
      "     29       62.4869       58.1902  0.0289\n",
      "     24      157.7834      143.2604  0.0359\n",
      "     25      \u001b[36m118.7093\u001b[0m      108.1102  0.0378\n",
      "     29       78.4984       73.4789  0.0330\n",
      "     29       59.3071       55.2071  0.0585\n",
      "     25      142.2536      132.0418  0.0418\n",
      "     25      \u001b[36m130.4512\u001b[0m      120.3932  0.0438\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 8.\n",
      "     30       71.0374       67.3398  0.0410\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     26      142.2683      131.9166  0.0262\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 10.\n",
      "     30       62.6396       57.9064  0.0480\n",
      "     26      119.1689      107.8459  0.0330\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     30       \u001b[36m59.2474\u001b[0m       55.4330  0.0340\n",
      "     25      \u001b[36m157.6492\u001b[0m      143.2605  0.0388\n",
      "     26      130.8460      120.2174  0.0328\n",
      "     31       70.9844       67.1525  0.0266\n",
      "     31       62.5501       57.7854  0.0241\n",
      "     27      142.2524      \u001b[32m131.2785\u001b[0m  0.0272\n",
      "     27      118.8310      108.3281  0.0265\n",
      "     31       59.3270       55.3413  0.0251\n",
      "     26      158.0658      143.2606  0.0270\n",
      "     27      \u001b[36m130.3755\u001b[0m      120.3504  0.0268\n",
      "     32       70.9461       67.2265  0.0246\n",
      "     32       62.5894       57.8595  0.0247\n",
      "     28      \u001b[36m142.0174\u001b[0m      131.3236  0.0268\n",
      "     28      \u001b[36m118.6590\u001b[0m      108.3060  0.0262\n",
      "     32       \u001b[36m59.2402\u001b[0m       55.3144  0.0237\n",
      "     27      158.0860      143.2812  0.0263\n",
      "     33       70.8880       67.2807  0.0243\n",
      "     28      130.9205      120.2462  0.0271\n",
      "     33       62.6068       57.8587  0.0241\n",
      "     33       59.3732       55.5189  0.0244\n",
      "     29      142.2874      131.3221  0.0263\n",
      "     29      118.7436      108.3000  0.0273\n",
      "     28      \u001b[36m157.5567\u001b[0m      143.2606  0.0266\n",
      "     34       70.9829       67.2712  0.0249\n",
      "     29      130.5775      119.8871  0.0260\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 14.\n",
      "     34       \u001b[36m59.2011\u001b[0m       55.5462  0.0256\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     30      142.1691      131.6089  0.0281\n",
      "     30      \u001b[36m118.6147\u001b[0m      108.2108  0.0277\n",
      "     35       71.0262       67.2750  0.0252\n",
      "     29      \u001b[36m157.4255\u001b[0m      143.2604  0.0269\n",
      "     30      131.1434      120.3502  0.0281\n",
      "     35       59.2453       55.6775  0.0241\n",
      "     31      142.0980      131.6322  0.0265\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 11.\n",
      "     36       70.9558       67.2610  0.0249\n",
      "     30      \u001b[36m157.2426\u001b[0m      143.2602  0.0274\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 11.\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     36       59.2229       55.6200  0.0254\n",
      "     32      142.0528      131.4064  0.0272\n",
      "     37       70.9453       67.1328  0.0252\n",
      "     31      157.4102      143.2601  0.0268\n",
      "     37       59.3004       55.3816  0.0246\n",
      "     38       70.9001       67.1374  0.0242\n",
      "     33      \u001b[36m141.9522\u001b[0m      131.4083  0.0268\n",
      "     32      157.3879      143.2600  0.0270\n",
      "     38       59.2869       55.5385  0.0256\n",
      "     39       70.9590       67.1341  0.0251\n",
      "     34      141.9616      131.4062  0.0268\n",
      "     33      \u001b[36m157.2310\u001b[0m      143.2600  0.0264\n",
      "     39       59.2876       55.3589  0.0239\n",
      "     40       70.9311       67.1346  0.0244\n",
      "     35      \u001b[36m141.9072\u001b[0m      131.4049  0.0265\n",
      "     34      \u001b[36m157.1429\u001b[0m      143.2599  0.0270\n",
      "     40       59.2219       55.4217  0.0237\n",
      "     41       70.9035       67.1612  0.0237\n",
      "     36      141.9511      131.4004  0.0258\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 15.\n",
      "     41       59.3718       55.3940  0.0244\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     42       70.9025       67.1593  0.0240\n",
      "     37      142.0247      131.4072  0.0263\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 22.\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     43       70.9022       67.1399  0.0248\n",
      "     38      \u001b[36m141.7842\u001b[0m      131.4279  0.0267\n",
      "     44       \u001b[36m70.8713\u001b[0m       67.1994  0.0232\n",
      "     39      \u001b[36m141.7788\u001b[0m      131.4269  0.0254\n",
      "     45       70.9810       67.1478  0.0239\n",
      "     40      141.9667      131.4196  0.0257\n",
      "     46       70.9047       67.1544  0.0237\n",
      "     41      141.9259      131.4315  0.0268\n",
      "     47       \u001b[36m70.8581\u001b[0m       67.1311  0.0256\n",
      "     42      142.1504      131.4305  0.0270\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 28.\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     43      \u001b[36m141.7722\u001b[0m      131.4275  0.0264\n",
      "     44      141.7902      131.4302  0.0253\n",
      "     45      141.9275      131.4289  0.0254\n",
      "     46      \u001b[36m141.7139\u001b[0m      131.4285  0.0252\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 27.\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m84.3683\u001b[0m       \u001b[32m69.5887\u001b[0m  0.0357\n",
      "      2       \u001b[36m72.6707\u001b[0m       \u001b[32m66.0166\u001b[0m  0.0448\n",
      "      3       \u001b[36m70.0221\u001b[0m       \u001b[32m64.5390\u001b[0m  0.0432\n",
      "      4       \u001b[36m68.5891\u001b[0m       \u001b[32m63.4644\u001b[0m  0.0360\n",
      "      5       \u001b[36m67.8866\u001b[0m       \u001b[32m63.2589\u001b[0m  0.0346\n",
      "      6       \u001b[36m67.7338\u001b[0m       \u001b[32m63.2574\u001b[0m  0.0360\n",
      "      7       \u001b[36m67.6811\u001b[0m       63.2579  0.0369\n",
      "      8       \u001b[36m67.6368\u001b[0m       63.2869  0.0350\n",
      "      9       \u001b[36m67.6344\u001b[0m       63.2866  0.0439\n",
      "     10       67.6573       63.2860  0.0354\n",
      "     11       \u001b[36m67.6322\u001b[0m       63.2858  0.0344\n",
      "     12       \u001b[36m67.6316\u001b[0m       63.2859  0.0342\n",
      "     13       \u001b[36m67.6308\u001b[0m       63.2576  0.0351\n",
      "     14       67.6323       63.2861  0.0352\n",
      "     15       67.6312       63.2725  0.0429\n",
      "     16       67.6312       63.2730  0.0351\n",
      "     17       \u001b[36m67.6261\u001b[0m       63.2862  0.0318\n",
      "     18       67.6313       \u001b[32m63.2538\u001b[0m  0.0340\n",
      "     19       67.6316       63.2592  0.0351\n",
      "     20       67.6316       63.2856  0.0355\n",
      "     21       67.6319       63.2564  0.0436\n",
      "     22       67.6306       \u001b[32m63.2340\u001b[0m  0.0359\n",
      "     23       67.6303       63.2657  0.0352\n",
      "     24       67.6372       63.2866  0.0344\n",
      "     25       67.6321       63.2870  0.0366\n",
      "     26       67.6333       63.2706  0.0349\n",
      "     27       67.6319       63.2864  0.0449\n",
      "     28       67.6316       63.2681  0.0372\n",
      "     29       67.6313       63.2863  0.0363\n",
      "     30       67.6311       63.2747  0.0339\n",
      "     31       67.6310       63.2862  0.0444\n",
      "     32       67.6312       63.2590  0.0481\n",
      "     33       67.6311       63.2861  0.0432\n",
      "     34       67.6305       63.2861  0.0370\n",
      "     35       67.6307       63.2861  0.0349\n",
      "     36       67.6310       63.2862  0.0357\n",
      "     37       67.6308       63.2854  0.0347\n",
      "     38       67.6306       63.2861  0.0340\n",
      "     39       67.6303       63.2862  0.0436\n",
      "     40       67.6310       63.2856  0.0354\n",
      "     41       67.6282       63.2831  0.0352\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 22.\n",
      "durations 0.49281314 85.81519\n",
      "dtype X_train horm_treatment      category\n",
      "menopause           category\n",
      "age                  float32\n",
      "n_positive_nodes     float32\n",
      "progesterone         float32\n",
      "estrogene            float32\n",
      "grade_0.0              uint8\n",
      "grade_1.0              uint8\n",
      "grade_2.0              uint8\n",
      "dtype: object\n",
      "dtype y_train float64\n",
      "dtype X_test horm_treatment      category\n",
      "menopause           category\n",
      "age                  float32\n",
      "n_positive_nodes     float32\n",
      "progesterone         float32\n",
      "estrogene            float32\n",
      "grade_0.0              uint8\n",
      "grade_1.0              uint8\n",
      "grade_2.0              uint8\n",
      "dtype: object\n",
      "dtype y_test float32\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m74.5122\u001b[0m       \u001b[32m64.2260\u001b[0m  0.0266\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m79.4866\u001b[0m       \u001b[32m64.5272\u001b[0m  0.0259\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m84.5418\u001b[0m       \u001b[32m67.8643\u001b[0m  0.0270\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m93.5969\u001b[0m       \u001b[32m76.7480\u001b[0m  0.0275\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2       \u001b[36m64.6931\u001b[0m       \u001b[32m59.2407\u001b[0m  0.0254\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m106.5616\u001b[0m       \u001b[32m88.6601\u001b[0m  0.0339\n",
      "      2       \u001b[36m68.5868\u001b[0m       \u001b[32m58.8320\u001b[0m  0.0283\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m146.2734\u001b[0m      \u001b[32m123.1382\u001b[0m  0.0348\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m154.5306\u001b[0m      \u001b[32m125.2393\u001b[0m  0.0315\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m166.2478\u001b[0m      \u001b[32m131.0597\u001b[0m  0.0306\n",
      "      2       \u001b[36m72.4476\u001b[0m       \u001b[32m62.6824\u001b[0m  0.0327\n",
      "      2       \u001b[36m79.5790\u001b[0m       \u001b[32m69.5078\u001b[0m  0.0330\n",
      "      3       \u001b[36m62.3534\u001b[0m       \u001b[32m57.8758\u001b[0m  0.0277\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m183.6337\u001b[0m      \u001b[32m150.0316\u001b[0m  0.0335\n",
      "      2       \u001b[36m89.5488\u001b[0m       \u001b[32m75.5308\u001b[0m  0.0343\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m204.7307\u001b[0m      \u001b[32m167.3782\u001b[0m  0.0281\n",
      "      2      \u001b[36m129.7987\u001b[0m      \u001b[32m113.2772\u001b[0m  0.0300\n",
      "      3       \u001b[36m65.4881\u001b[0m       \u001b[32m58.0018\u001b[0m  0.0390\n",
      "      2      \u001b[36m136.7476\u001b[0m      \u001b[32m115.9195\u001b[0m  0.0393\n",
      "      2      \u001b[36m144.7697\u001b[0m      \u001b[32m121.4577\u001b[0m  0.0380\n",
      "      3       \u001b[36m69.3648\u001b[0m       \u001b[32m61.6823\u001b[0m  0.0391\n",
      "      2      \u001b[36m159.2179\u001b[0m      \u001b[32m135.8837\u001b[0m  0.0287\n",
      "      4       \u001b[36m61.2149\u001b[0m       \u001b[32m56.3604\u001b[0m  0.0377\n",
      "      3       \u001b[36m81.1476\u001b[0m       \u001b[32m73.1457\u001b[0m  0.0327\n",
      "      3       \u001b[36m75.6932\u001b[0m       \u001b[32m68.1843\u001b[0m  0.0494\n",
      "      3      \u001b[36m124.5407\u001b[0m      \u001b[32m108.0419\u001b[0m  0.0355\n",
      "      4       \u001b[36m64.6541\u001b[0m       \u001b[32m57.2366\u001b[0m  0.0381\n",
      "      4       \u001b[36m68.1224\u001b[0m       \u001b[32m60.5787\u001b[0m  0.0331\n",
      "      2      \u001b[36m179.4864\u001b[0m      \u001b[32m154.8769\u001b[0m  0.0541\n",
      "      4       \u001b[36m74.2499\u001b[0m       \u001b[32m66.5653\u001b[0m  0.0259\n",
      "      3      \u001b[36m141.0531\u001b[0m      \u001b[32m119.2410\u001b[0m  0.0474\n",
      "      5       \u001b[36m60.4000\u001b[0m       \u001b[32m56.3174\u001b[0m  0.0352\n",
      "      3      \u001b[36m133.4960\u001b[0m      \u001b[32m114.5719\u001b[0m  0.0507\n",
      "      3      \u001b[36m154.3069\u001b[0m      \u001b[32m133.0956\u001b[0m  0.0507\n",
      "      4       \u001b[36m79.7181\u001b[0m       \u001b[32m73.0996\u001b[0m  0.0413\n",
      "      4      \u001b[36m122.0135\u001b[0m      108.5924  0.0403\n",
      "      5       \u001b[36m67.6685\u001b[0m       \u001b[32m59.6126\u001b[0m  0.0337\n",
      "      3      \u001b[36m173.3825\u001b[0m      \u001b[32m150.6082\u001b[0m  0.0343\n",
      "      5       \u001b[36m64.1771\u001b[0m       57.2427  0.0404\n",
      "      6       \u001b[36m60.0617\u001b[0m       \u001b[32m55.6576\u001b[0m  0.0299\n",
      "      4      \u001b[36m132.3398\u001b[0m      \u001b[32m114.5332\u001b[0m  0.0302\n",
      "      4      \u001b[36m151.9476\u001b[0m      \u001b[32m131.9135\u001b[0m  0.0316\n",
      "      5       \u001b[36m79.4990\u001b[0m       73.1943  0.0297\n",
      "      4      \u001b[36m139.6654\u001b[0m      \u001b[32m118.0918\u001b[0m  0.0435\n",
      "      5       \u001b[36m73.6432\u001b[0m       \u001b[32m66.4340\u001b[0m  0.0453\n",
      "      4      \u001b[36m168.9074\u001b[0m      \u001b[32m145.5919\u001b[0m  0.0258\n",
      "      6       \u001b[36m63.9876\u001b[0m       \u001b[32m57.2062\u001b[0m  0.0283\n",
      "      6       \u001b[36m67.3291\u001b[0m       \u001b[32m59.5643\u001b[0m  0.0343\n",
      "      7       \u001b[36m59.9442\u001b[0m       \u001b[32m54.9572\u001b[0m  0.0281\n",
      "      5      \u001b[36m120.4624\u001b[0m      108.9971  0.0442\n",
      "      5      \u001b[36m129.7742\u001b[0m      \u001b[32m112.9777\u001b[0m  0.0335\n",
      "      6       \u001b[36m79.3824\u001b[0m       \u001b[32m73.0281\u001b[0m  0.0238\n",
      "      5      \u001b[36m148.2120\u001b[0m      \u001b[32m129.8960\u001b[0m  0.0317\n",
      "      5      \u001b[36m136.9081\u001b[0m      \u001b[32m117.1603\u001b[0m  0.0360\n",
      "      5      \u001b[36m163.4099\u001b[0m      \u001b[32m143.1543\u001b[0m  0.0268\n",
      "      6       \u001b[36m73.6347\u001b[0m       66.4900  0.0395\n",
      "      6      120.5012      108.9541  0.0300\n",
      "      7       \u001b[36m63.7643\u001b[0m       \u001b[32m57.1923\u001b[0m  0.0328\n",
      "      6      \u001b[36m129.2082\u001b[0m      \u001b[32m111.6450\u001b[0m  0.0268\n",
      "      7       \u001b[36m79.1816\u001b[0m       \u001b[32m72.7588\u001b[0m  0.0278\n",
      "      8       \u001b[36m59.6372\u001b[0m       \u001b[32m54.9100\u001b[0m  0.0373\n",
      "      6      \u001b[36m146.6709\u001b[0m      \u001b[32m129.4399\u001b[0m  0.0294\n",
      "      6      \u001b[36m136.1704\u001b[0m      \u001b[32m116.3313\u001b[0m  0.0271\n",
      "      6      \u001b[36m160.4402\u001b[0m      \u001b[32m142.7088\u001b[0m  0.0284\n",
      "      7       \u001b[36m67.0948\u001b[0m       \u001b[32m59.5022\u001b[0m  0.0521\n",
      "      8       63.7894       57.2381  0.0239\n",
      "      7      \u001b[36m120.0145\u001b[0m      \u001b[32m107.4690\u001b[0m  0.0276\n",
      "      9       \u001b[36m59.5371\u001b[0m       54.9107  0.0257\n",
      "      8       \u001b[36m79.1459\u001b[0m       \u001b[32m72.7385\u001b[0m  0.0297\n",
      "      7      \u001b[36m128.9521\u001b[0m      111.9109  0.0336\n",
      "      7      \u001b[36m146.5147\u001b[0m      129.6031  0.0285\n",
      "      7       \u001b[36m73.5460\u001b[0m       66.4878  0.0480\n",
      "      7      \u001b[36m135.0139\u001b[0m      117.0703  0.0282\n",
      "      7      160.7063      \u001b[32m142.1393\u001b[0m  0.0354\n",
      "     10       \u001b[36m59.5291\u001b[0m       \u001b[32m54.8920\u001b[0m  0.0251\n",
      "      8      \u001b[36m119.2251\u001b[0m      107.4708  0.0303\n",
      "      9       79.1518       72.7402  0.0270\n",
      "      8      \u001b[36m128.6204\u001b[0m      111.9131  0.0274\n",
      "      8       \u001b[36m67.0123\u001b[0m       59.5026  0.0437\n",
      "      9       \u001b[36m63.6611\u001b[0m       57.2914  0.0430\n",
      "      8      \u001b[36m146.3462\u001b[0m      129.8008  0.0278\n",
      "      8      135.0336      117.0519  0.0266\n",
      "      8      \u001b[36m159.5964\u001b[0m      \u001b[32m142.0669\u001b[0m  0.0308\n",
      "     11       \u001b[36m59.4022\u001b[0m       54.9234  0.0305\n",
      "     10       79.1493       72.7551  0.0280\n",
      "      9      \u001b[36m119.0479\u001b[0m      \u001b[32m107.4684\u001b[0m  0.0338\n",
      "      9      \u001b[36m128.3564\u001b[0m      111.9834  0.0290\n",
      "     10       \u001b[36m63.4794\u001b[0m       57.2372  0.0273\n",
      "      9      \u001b[36m146.1764\u001b[0m      129.6834  0.0284\n",
      "      9      \u001b[36m135.0061\u001b[0m      117.1583  0.0311\n",
      "      9      \u001b[36m159.1608\u001b[0m      \u001b[32m141.9142\u001b[0m  0.0280\n",
      "     10      \u001b[36m118.5416\u001b[0m      \u001b[32m107.4293\u001b[0m  0.0302\n",
      "     11       \u001b[36m63.4419\u001b[0m       57.2397  0.0329\n",
      "     11       \u001b[36m79.1348\u001b[0m       72.7964  0.0399\n",
      "      8       \u001b[36m73.5267\u001b[0m       66.4673  0.0897\n",
      "     10      \u001b[36m127.7268\u001b[0m      112.0008  0.0415\n",
      "     12       59.4347       \u001b[32m54.7633\u001b[0m  0.0563\n",
      "     10      \u001b[36m146.1059\u001b[0m      129.6573  0.0494\n",
      "     10      \u001b[36m134.8378\u001b[0m      117.1619  0.0452\n",
      "      9       \u001b[36m66.7322\u001b[0m       \u001b[32m59.4530\u001b[0m  0.0847\n",
      "     10      \u001b[36m158.8505\u001b[0m      142.0654  0.0405\n",
      "     12       \u001b[36m79.1344\u001b[0m       \u001b[32m72.6427\u001b[0m  0.0271\n",
      "     12       63.5425       57.2855  0.0346\n",
      "     11      118.7953      \u001b[32m107.3742\u001b[0m  0.0432\n",
      "     11      127.7703      112.0085  0.0335\n",
      "     13       59.4630       54.7650  0.0263\n",
      "     11      \u001b[36m134.2305\u001b[0m      116.6862  0.0313\n",
      "     11      \u001b[36m146.0650\u001b[0m      129.6994  0.0362\n",
      "     11      158.9152      142.9212  0.0287\n",
      "     13       \u001b[36m79.0897\u001b[0m       72.6692  0.0307\n",
      "      9       \u001b[36m73.4840\u001b[0m       66.5242  0.0599\n",
      "     12      119.0738      107.4080  0.0262\n",
      "     10       \u001b[36m66.5168\u001b[0m       \u001b[32m59.3532\u001b[0m  0.0436\n",
      "     14       59.4067       \u001b[32m54.7602\u001b[0m  0.0237\n",
      "     13       63.5520       57.2857  0.0364\n",
      "     12      127.9235      112.0087  0.0367\n",
      "     12      \u001b[36m145.7745\u001b[0m      129.6608  0.0291\n",
      "     14       79.1162       72.6433  0.0244\n",
      "     12      159.1398      142.4806  0.0310\n",
      "     15       \u001b[36m59.3980\u001b[0m       54.7831  0.0235\n",
      "     12      \u001b[36m134.0894\u001b[0m      116.5805  0.0394\n",
      "     14       63.4595       57.2857  0.0298\n",
      "     13      118.9231      107.4169  0.0378\n",
      "     13      127.9368      111.9988  0.0275\n",
      "     11       66.5197       59.7309  0.0383\n",
      "     10       \u001b[36m73.3184\u001b[0m       \u001b[32m66.2716\u001b[0m  0.0420\n",
      "     13      145.8270      129.6733  0.0291\n",
      "     15       79.1336       \u001b[32m72.6254\u001b[0m  0.0310\n",
      "     16       59.4261       \u001b[32m54.7597\u001b[0m  0.0265\n",
      "     13      \u001b[36m158.7970\u001b[0m      142.4207  0.0332\n",
      "     15       63.4561       57.2560  0.0291\n",
      "     13      \u001b[36m133.5515\u001b[0m      116.3615  0.0404\n",
      "     14      127.8752      111.9900  0.0312\n",
      "     14      \u001b[36m118.4853\u001b[0m      107.4684  0.0341\n",
      "     14      145.9298      129.6307  0.0277\n",
      "     16       79.1394       \u001b[32m72.6241\u001b[0m  0.0338\n",
      "     12       \u001b[36m66.4845\u001b[0m       59.7414  0.0452\n",
      "     17       59.3990       \u001b[32m54.7153\u001b[0m  0.0331\n",
      "     11       \u001b[36m72.9827\u001b[0m       \u001b[32m66.2119\u001b[0m  0.0471\n",
      "     14      158.8767      142.2858  0.0350\n",
      "     14      133.9197      \u001b[32m116.0301\u001b[0m  0.0272\n",
      "     16       63.4566       57.2211  0.0313\n",
      "     15      127.7818      111.9953  0.0306\n",
      "     15      118.4929      107.4666  0.0313\n",
      "     15      146.1800      129.6549  0.0296\n",
      "     13       \u001b[36m66.4398\u001b[0m       59.8135  0.0306\n",
      "     18       \u001b[36m59.3493\u001b[0m       54.8807  0.0294\n",
      "     17       79.1622       72.6243  0.0356\n",
      "     15      159.3329      142.0871  0.0311\n",
      "     17       \u001b[36m63.3666\u001b[0m       \u001b[32m57.1782\u001b[0m  0.0299\n",
      "     15      133.9794      116.1973  0.0306\n",
      "     16      \u001b[36m127.6602\u001b[0m      111.9979  0.0298\n",
      "     12       \u001b[36m72.7884\u001b[0m       \u001b[32m66.0760\u001b[0m  0.0453\n",
      "     16      \u001b[36m145.7010\u001b[0m      129.5478  0.0330\n",
      "     16      118.7628      107.4368  0.0352\n",
      "     16      \u001b[36m158.7555\u001b[0m      142.0151  0.0258\n",
      "     14       66.4906       59.5157  0.0341\n",
      "     19       \u001b[36m59.2909\u001b[0m       54.9036  0.0333\n",
      "     18       79.1480       \u001b[32m72.6237\u001b[0m  0.0340\n",
      "     18       63.4668       57.2031  0.0307\n",
      "     13       72.8106       \u001b[32m66.0030\u001b[0m  0.0276\n",
      "     17      \u001b[36m145.4578\u001b[0m      \u001b[32m129.1143\u001b[0m  0.0280\n",
      "     16      134.1549      116.4901  0.0415\n",
      "     17      118.5034      107.4039  0.0312\n",
      "     17      127.7028      112.0030  0.0430\n",
      "     15       \u001b[36m66.3355\u001b[0m       59.8774  0.0250\n",
      "     20       59.3511       54.7397  0.0273\n",
      "     17      159.4057      142.1097  0.0340\n",
      "     19       79.1508       \u001b[32m72.6236\u001b[0m  0.0339\n",
      "     19       63.4250       57.1846  0.0336\n",
      "     14       \u001b[36m72.7198\u001b[0m       66.0230  0.0314\n",
      "     18      145.6247      \u001b[32m129.1054\u001b[0m  0.0282\n",
      "     17      134.5082      116.1717  0.0296\n",
      "     16       \u001b[36m66.3289\u001b[0m       59.9671  0.0240\n",
      "     18      118.7343      107.3843  0.0359\n",
      "     21       59.3173       54.8766  0.0294\n",
      "     18      \u001b[36m158.3076\u001b[0m      142.4520  0.0284\n",
      "     20       79.1444       72.6237  0.0244\n",
      "     15       \u001b[36m72.6996\u001b[0m       \u001b[32m65.9079\u001b[0m  0.0268\n",
      "     17       66.4896       59.8478  0.0248\n",
      "     19      145.4892      129.1291  0.0313\n",
      "     20       63.4029       \u001b[32m57.1456\u001b[0m  0.0395\n",
      "     18      133.9128      116.8452  0.0341\n",
      "     22       59.4127       54.7246  0.0252\n",
      "     21       79.1464       72.6243  0.0264\n",
      "     19      158.5909      142.4483  0.0316\n",
      "     19      118.7042      \u001b[32m106.9453\u001b[0m  0.0355\n",
      "     18       66.3367       59.8394  0.0256\n",
      "     16       72.7127       \u001b[32m65.9064\u001b[0m  0.0356\n",
      "     21       63.4768       57.1496  0.0276\n",
      "     19      \u001b[36m133.1261\u001b[0m      116.4846  0.0290\n",
      "     20      \u001b[36m145.1995\u001b[0m      129.1292  0.0350\n",
      "     22       79.1368       72.6279  0.0296\n",
      "     23       59.3336       54.8903  0.0356\n",
      "     20      \u001b[36m118.3024\u001b[0m      \u001b[32m106.5365\u001b[0m  0.0325\n",
      "     19       \u001b[36m66.2969\u001b[0m       59.9268  0.0301\n",
      "     20      158.4789      142.4481  0.0415\n",
      "     21      145.5809      129.1102  0.0290\n",
      "     20      133.5309      116.4126  0.0315\n",
      "     17       72.7105       66.0393  0.0351\n",
      "     24       59.3038       54.7677  0.0251\n",
      "     23       79.1361       72.6488  0.0267\n",
      "     22       63.3703       57.1507  0.0457\n",
      "     18      \u001b[36m127.5381\u001b[0m      112.0167  0.1329\n",
      "     21      \u001b[36m118.1857\u001b[0m      106.7037  0.0329\n",
      "     20       66.3426       59.8713  0.0331\n",
      "     21      158.6418      142.4526  0.0319\n",
      "     25       \u001b[36m59.2771\u001b[0m       54.8494  0.0248\n",
      "     21      \u001b[36m133.0296\u001b[0m      116.4113  0.0299\n",
      "     22      145.8066      129.1175  0.0316\n",
      "     24       79.1465       72.6541  0.0275\n",
      "     18       72.7346       66.0588  0.0376\n",
      "     23       63.4167       57.1560  0.0319\n",
      "     22      \u001b[36m118.1204\u001b[0m      106.7995  0.0265\n",
      "     25       79.1548       72.7229  0.0241\n",
      "     19      \u001b[36m127.5283\u001b[0m      112.1052  0.0409\n",
      "     21       66.3269       59.8143  0.0367\n",
      "     22      158.6099      142.4499  0.0349\n",
      "     22      133.2690      116.4094  0.0328\n",
      "     23      145.6512      \u001b[32m129.1020\u001b[0m  0.0341\n",
      "     26       59.3777       54.8741  0.0429\n",
      "     19       \u001b[36m72.6812\u001b[0m       66.2854  0.0377\n",
      "     23      118.2400      106.8507  0.0333\n",
      "     24       63.4306       57.1712  0.0366\n",
      "     26       79.1408       72.6668  0.0265\n",
      "     22       66.4206       59.7627  0.0262\n",
      "     20      \u001b[36m127.5235\u001b[0m      111.8586  0.0371\n",
      "     23      133.2790      116.4109  0.0316\n",
      "     23      158.4842      142.0050  0.0361\n",
      "     24      145.4501      \u001b[32m129.0855\u001b[0m  0.0323\n",
      "     27       59.3263       54.7393  0.0346\n",
      "     24      118.3640      106.9080  0.0285\n",
      "     25       63.4566       57.1598  0.0282\n",
      "     20       72.9085       66.3498  0.0344\n",
      "     23       66.4707       59.7269  0.0354\n",
      "     24      133.1948      \u001b[32m115.9839\u001b[0m  0.0266\n",
      "     24      158.7555      141.9957  0.0266\n",
      "     27       79.1238       72.6890  0.0409\n",
      "     25      \u001b[36m144.9531\u001b[0m      129.1245  0.0274\n",
      "     28       59.3829       54.7584  0.0238\n",
      "     21      \u001b[36m127.1088\u001b[0m      112.7106  0.0434\n",
      "     25      118.2314      106.9099  0.0340\n",
      "     26       63.4043       57.1767  0.0333\n",
      "     24       66.4039       59.8065  0.0247\n",
      "     25      133.2986      115.9874  0.0264\n",
      "     28       79.1300       72.7665  0.0270\n",
      "     21       72.7502       \u001b[32m65.8699\u001b[0m  0.0395\n",
      "     29       59.3848       54.7580  0.0251\n",
      "     26      145.4768      129.1293  0.0333\n",
      "     25      158.9564      141.9966  0.0376\n",
      "     22      127.7985      112.6285  0.0327\n",
      "     26      118.6040      106.5822  0.0314\n",
      "     26      133.4503      116.0521  0.0267\n",
      "     30       59.4144       54.8709  0.0254\n",
      "     22       72.6915       66.0332  0.0312\n",
      "     27       63.3818       57.2190  0.0314\n",
      "     25       66.3563       59.7929  0.0392\n",
      "     27      144.9822      \u001b[32m128.6838\u001b[0m  0.0311\n",
      "     29       79.0988       72.7209  0.0454\n",
      "     26      158.5441      \u001b[32m141.6364\u001b[0m  0.0385\n",
      "     28       63.3784       57.2222  0.0258\n",
      "     23       72.7481       65.8911  0.0316\n",
      "     31       59.2778       54.7672  0.0365\n",
      "     27      133.5891      116.3859  0.0427\n",
      "     27      158.5237      \u001b[32m141.6358\u001b[0m  0.0258\n",
      "     27      118.1447      107.1314  0.0567\n",
      "     26       \u001b[36m66.2940\u001b[0m       59.8164  0.0472\n",
      "     30       \u001b[36m79.0885\u001b[0m       72.7853  0.0331\n",
      "     23      127.5033      112.7239  0.0634\n",
      "     29       63.3873       57.2316  0.0266\n",
      "     28      145.0912      \u001b[32m128.6820\u001b[0m  0.0527\n",
      "     24       72.7663       \u001b[32m65.8319\u001b[0m  0.0304\n",
      "     32       59.3193       54.8789  0.0360\n",
      "     28      133.4755      116.3075  0.0379\n",
      "     28      158.5326      \u001b[32m141.6358\u001b[0m  0.0337\n",
      "     28      118.8577      107.4225  0.0358\n",
      "     24      127.6366      112.3822  0.0318\n",
      "     27       66.4143       59.7152  0.0343\n",
      "     30       63.3865       57.1541  0.0340\n",
      "     31       79.1796       72.7472  0.0371\n",
      "     33       59.3479       54.7539  0.0240\n",
      "     29      145.0083      128.6842  0.0335\n",
      "     25       72.7111       65.9156  0.0346\n",
      "     29      133.0660      116.0337  0.0275\n",
      "     29      \u001b[36m158.1274\u001b[0m      \u001b[32m141.6357\u001b[0m  0.0305\n",
      "     28       66.4102       59.7663  0.0264\n",
      "     31       63.4219       57.2047  0.0241\n",
      "     25      127.4869      111.7819  0.0361\n",
      "     29      118.6049      106.9948  0.0382\n",
      "     30      145.0620      \u001b[32m128.6653\u001b[0m  0.0276\n",
      "     34       59.3538       54.7506  0.0307\n",
      "     32       79.1412       72.6379  0.0378\n",
      "     26       72.7454       66.0457  0.0279\n",
      "     29       66.4442       59.8379  0.0267\n",
      "     30      \u001b[36m158.0542\u001b[0m      \u001b[32m141.6357\u001b[0m  0.0298\n",
      "     32       \u001b[36m63.2845\u001b[0m       57.1603  0.0264\n",
      "     30      133.2924      116.3102  0.0415\n",
      "     35       59.3119       54.7481  0.0250\n",
      "     30      118.6258      106.9637  0.0321\n",
      "     31      145.1685      \u001b[32m128.6466\u001b[0m  0.0332\n",
      "     27       72.7309       66.0500  0.0315\n",
      "     33       79.1276       72.6504  0.0326\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 10.\n",
      "     31      \u001b[36m158.0288\u001b[0m      141.6357  0.0271\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 6.\n",
      "     31      133.8183      116.6494  0.0265\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     36       59.3293       54.7650  0.0292\n",
      "     31      \u001b[36m117.9757\u001b[0m      106.9618  0.0296\n",
      "     33       \u001b[36m63.2257\u001b[0m       \u001b[32m57.1327\u001b[0m  0.0449\n",
      "     28       72.7088       66.0520  0.0262\n",
      "     32      145.4643      \u001b[32m128.6238\u001b[0m  0.0267\n",
      "     34       79.0999       72.7322  0.0279\n",
      "     32      \u001b[36m158.0053\u001b[0m      142.1495  0.0287\n",
      "     32      133.4186      116.8590  0.0283\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 17.\n",
      "     34       63.3539       57.1719  0.0243\n",
      "     29       72.7023       66.0798  0.0243\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     32      118.3591      106.7659  0.0276\n",
      "     33      145.0276      129.0709  0.0282\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 15.\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     33      158.1927      141.6428  0.0263\n",
      "     33      133.1714      116.7696  0.0295\n",
      "     35       63.3801       57.1590  0.0248\n",
      "     30       \u001b[36m72.6392\u001b[0m       66.1147  0.0245\n",
      "     33      118.1977      106.9008  0.0271\n",
      "     34      145.4122      129.1315  0.0275\n",
      "     34      158.1259      \u001b[32m141.6357\u001b[0m  0.0270\n",
      "     34      133.1931      116.7672  0.0285\n",
      "     36       63.4363       57.1577  0.0257\n",
      "     31       72.6806       66.0484  0.0258\n",
      "     34      118.4867      106.9010  0.0270\n",
      "     35      145.2264      129.1307  0.0281\n",
      "     35      158.0677      141.6357  0.0298\n",
      "     37       63.3410       57.2193  0.0253\n",
      "     32       72.6523       66.0475  0.0264\n",
      "     35      133.0529      \u001b[32m115.9737\u001b[0m  0.0304\n",
      "     35      118.4660      106.8917  0.0284\n",
      "     36      145.1875      129.1765  0.0284\n",
      "     36      158.0202      \u001b[32m141.6357\u001b[0m  0.0282\n",
      "     38       63.3419       57.1690  0.0253\n",
      "     33       \u001b[36m72.6287\u001b[0m       66.0515  0.0298\n",
      "     36      \u001b[36m133.0073\u001b[0m      115.9835  0.0297\n",
      "     36      118.2952      106.7286  0.0265\n",
      "     37      145.4935      129.5575  0.0277\n",
      "     37      158.0452      141.6357  0.0267\n",
      "     39       63.2485       57.2937  0.0249\n",
      "     34       72.7370       66.0279  0.0260\n",
      "     37      133.1292      115.9834  0.0281\n",
      "     37      118.1432      106.9012  0.0273\n",
      "     38      145.3528      129.4832  0.0270\n",
      "     38      158.0432      141.6357  0.0254\n",
      "     40       63.3643       57.1543  0.0250\n",
      "     35       72.6533       66.0423  0.0252\n",
      "     38      133.1097      115.9909  0.0276\n",
      "     38      118.0548      106.9010  0.0270\n",
      "     39      145.5467      129.1133  0.0267\n",
      "     39      158.0127      142.0832  0.0254\n",
      "     41       63.3057       57.1753  0.0248\n",
      "     36       \u001b[36m72.6241\u001b[0m       66.0458  0.0252\n",
      "     39      133.2388      115.9833  0.0277\n",
      "     39      118.2591      106.9011  0.0276\n",
      "     40      145.5637      129.1316  0.0267\n",
      "     42       63.3730       57.1715  0.0248\n",
      "     40      \u001b[36m157.9744\u001b[0m      141.8793  0.0277\n",
      "     37       72.7143       66.0536  0.0243\n",
      "     40      133.1151      115.9833  0.0275\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "     41      145.6618      129.1316  0.0264\n",
      "Restoring best model from epoch 20.\n",
      "     43       63.2741       57.1810  0.0249\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     41      158.1452      141.7741  0.0319\n",
      "     38       72.6582       65.8414  0.0263\n",
      "     41      133.1160      115.9833  0.0308\n",
      "     42      145.3002      129.1315  0.0285\n",
      "     44       \u001b[36m63.1905\u001b[0m       57.2292  0.0270\n",
      "     42      157.9982      142.0822  0.0296\n",
      "     39       \u001b[36m72.6053\u001b[0m       66.1547  0.0331\n",
      "     45       63.2355       \u001b[32m57.1319\u001b[0m  0.0271\n",
      "     42      133.1132      115.9833  0.0314\n",
      "     43      145.4352      129.1316  0.0306\n",
      "     43      157.9758      142.0832  0.0312\n",
      "     40       72.7177       66.2529  0.0289\n",
      "     46       63.3065       \u001b[32m57.0565\u001b[0m  0.0255\n",
      "     43      133.1144      115.9833  0.0316\n",
      "     44      145.2619      129.1272  0.0372\n",
      "     41       72.6431       65.8566  0.0266\n",
      "     44      158.2237      141.6982  0.0312\n",
      "     47       63.3371       57.0610  0.0261\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 24.\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     45      145.3056      128.6860  0.0289\n",
      "     42       72.6946       65.8464  0.0259\n",
      "     45      158.1665      141.6471  0.0288\n",
      "     48       63.3549       57.1486  0.0267\n",
      "     43       72.6538       66.0573  0.0258\n",
      "     46      145.7380      128.6844  0.0283\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 26.\n",
      "     49       63.2478       57.1560  0.0250\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 24.\n",
      "     47      145.3629      129.1213  0.0259\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     50       63.3799       \u001b[32m57.0496\u001b[0m  0.0248\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     48      145.3287      129.1317  0.0259\n",
      "     49      145.2341      129.1316  0.0256\n",
      "     50      145.1296      129.1308  0.0256\n",
      "Restoring best model from epoch 32.\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m85.2102\u001b[0m       \u001b[32m69.2107\u001b[0m  0.0385\n",
      "      2       \u001b[36m72.6490\u001b[0m       \u001b[32m64.7857\u001b[0m  0.0417\n",
      "      3       \u001b[36m69.7975\u001b[0m       \u001b[32m64.2484\u001b[0m  0.0394\n",
      "      4       \u001b[36m69.0504\u001b[0m       \u001b[32m63.8160\u001b[0m  0.0341\n",
      "      5       \u001b[36m68.4662\u001b[0m       \u001b[32m63.4317\u001b[0m  0.0346\n",
      "      6       \u001b[36m68.1049\u001b[0m       \u001b[32m63.3557\u001b[0m  0.0347\n",
      "      7       \u001b[36m67.9917\u001b[0m       \u001b[32m63.1189\u001b[0m  0.0375\n",
      "      8       \u001b[36m67.9509\u001b[0m       \u001b[32m63.1080\u001b[0m  0.0427\n",
      "      9       67.9728       63.1185  0.0371\n",
      "     10       67.9700       63.1184  0.0343\n",
      "     11       67.9721       63.1181  0.0360\n",
      "     12       \u001b[36m67.9199\u001b[0m       63.1154  0.0356\n",
      "     13       68.0679       63.3860  0.0381\n",
      "     14       68.0320       63.1178  0.0422\n",
      "     15       67.9601       \u001b[32m63.0922\u001b[0m  0.0350\n",
      "     16       67.9909       63.0988  0.0338\n",
      "     17       67.9229       63.1904  0.0333\n",
      "     18       67.9234       63.1203  0.0343\n",
      "     19       \u001b[36m67.9070\u001b[0m       63.1567  0.0336\n",
      "     20       67.9669       63.1115  0.0433\n",
      "     21       67.9175       63.1172  0.0357\n",
      "     22       67.9622       63.1154  0.0360\n",
      "     23       67.9359       63.1159  0.0342\n",
      "     24       67.9298       63.1030  0.0344\n",
      "     25       \u001b[36m67.9062\u001b[0m       63.1039  0.0342\n",
      "     26       67.9332       63.2300  0.0433\n",
      "     27       67.9243       63.2861  0.0354\n",
      "     28       \u001b[36m67.8991\u001b[0m       63.1142  0.0340\n",
      "     29       67.9125       63.2272  0.0340\n",
      "     30       67.9420       63.2942  0.0371\n",
      "     31       68.0147       63.1053  0.0365\n",
      "     32       67.9337       63.1029  0.0437\n",
      "     33       67.9397       63.0929  0.0378\n",
      "     34       67.9144       63.2173  0.0338\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 15.\n",
      "durations 1.4784395 84.0\n",
      "dtype X_train horm_treatment      category\n",
      "menopause           category\n",
      "age                  float32\n",
      "n_positive_nodes     float32\n",
      "progesterone         float32\n",
      "estrogene            float32\n",
      "grade_0.0              uint8\n",
      "grade_1.0              uint8\n",
      "grade_2.0              uint8\n",
      "dtype: object\n",
      "dtype y_train float64\n",
      "dtype X_test horm_treatment      category\n",
      "menopause           category\n",
      "age                  float32\n",
      "n_positive_nodes     float32\n",
      "progesterone         float32\n",
      "estrogene            float32\n",
      "grade_0.0              uint8\n",
      "grade_1.0              uint8\n",
      "grade_2.0              uint8\n",
      "dtype: object\n",
      "dtype y_test float32\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m73.3584\u001b[0m       \u001b[32m62.3266\u001b[0m  0.0285\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m79.2002\u001b[0m       \u001b[32m64.5384\u001b[0m  0.0263\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m82.8503\u001b[0m       \u001b[32m67.9102\u001b[0m  0.0261\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m91.3819\u001b[0m       \u001b[32m78.1274\u001b[0m  0.0253\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m104.3266\u001b[0m       \u001b[32m91.3759\u001b[0m  0.0315\n",
      "      2       \u001b[36m68.2463\u001b[0m       \u001b[32m59.2186\u001b[0m  0.0255\n",
      "      2       \u001b[36m65.7052\u001b[0m       \u001b[32m59.0121\u001b[0m  0.0305\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m146.3325\u001b[0m      \u001b[32m122.8134\u001b[0m  0.0364\n",
      "      2       \u001b[36m77.3926\u001b[0m       \u001b[32m71.8999\u001b[0m  0.0241\n",
      "      2       \u001b[36m70.8704\u001b[0m       \u001b[32m64.4359\u001b[0m  0.0333\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m179.0176\u001b[0m      \u001b[32m152.7658\u001b[0m  0.0316\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m155.2939\u001b[0m      \u001b[32m131.9401\u001b[0m  0.0416\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m162.9222\u001b[0m      \u001b[32m131.9997\u001b[0m  0.0411\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m206.8361\u001b[0m      \u001b[32m172.1744\u001b[0m  0.0382\n",
      "      2       \u001b[36m88.8241\u001b[0m       \u001b[32m80.0474\u001b[0m  0.0356\n",
      "      3       \u001b[36m62.5987\u001b[0m       \u001b[32m55.5769\u001b[0m  0.0303\n",
      "      2      \u001b[36m132.1252\u001b[0m      \u001b[32m115.7973\u001b[0m  0.0320\n",
      "      3       \u001b[36m69.2050\u001b[0m       \u001b[32m62.7465\u001b[0m  0.0245\n",
      "      3       \u001b[36m65.7320\u001b[0m       \u001b[32m58.5286\u001b[0m  0.0381\n",
      "      2      \u001b[36m156.4357\u001b[0m      \u001b[32m138.8942\u001b[0m  0.0289\n",
      "      2      \u001b[36m136.7972\u001b[0m      \u001b[32m118.2789\u001b[0m  0.0296\n",
      "      3       \u001b[36m74.5143\u001b[0m       \u001b[32m69.5741\u001b[0m  0.0388\n",
      "      4       \u001b[36m64.9270\u001b[0m       58.7210  0.0263\n",
      "      2      \u001b[36m141.5120\u001b[0m      \u001b[32m125.5746\u001b[0m  0.0455\n",
      "      3      \u001b[36m128.5958\u001b[0m      \u001b[32m113.5469\u001b[0m  0.0332\n",
      "      3       \u001b[36m82.7025\u001b[0m       \u001b[32m77.6293\u001b[0m  0.0401\n",
      "      3      \u001b[36m131.9702\u001b[0m      \u001b[32m116.4121\u001b[0m  0.0283\n",
      "      2      \u001b[36m174.2255\u001b[0m      \u001b[32m154.9260\u001b[0m  0.0501\n",
      "      4       \u001b[36m68.3172\u001b[0m       \u001b[32m61.5503\u001b[0m  0.0411\n",
      "      4       \u001b[36m72.9247\u001b[0m       \u001b[32m69.3225\u001b[0m  0.0323\n",
      "      3      \u001b[36m150.1548\u001b[0m      \u001b[32m136.5254\u001b[0m  0.0373\n",
      "      4       \u001b[36m60.2441\u001b[0m       \u001b[32m55.3156\u001b[0m  0.0538\n",
      "      3      \u001b[36m137.9331\u001b[0m      \u001b[32m123.0944\u001b[0m  0.0398\n",
      "      5       \u001b[36m67.7894\u001b[0m       \u001b[32m61.1819\u001b[0m  0.0262\n",
      "      4      \u001b[36m124.5061\u001b[0m      \u001b[32m109.9964\u001b[0m  0.0374\n",
      "      5       \u001b[36m59.8920\u001b[0m       \u001b[32m54.6499\u001b[0m  0.0241\n",
      "      4      \u001b[36m147.7313\u001b[0m      \u001b[32m136.1374\u001b[0m  0.0302\n",
      "      4      \u001b[36m129.8129\u001b[0m      117.0114  0.0393\n",
      "      5       \u001b[36m64.5613\u001b[0m       \u001b[32m58.2540\u001b[0m  0.0485\n",
      "      4       \u001b[36m80.8730\u001b[0m       \u001b[32m75.7971\u001b[0m  0.0428\n",
      "      3      \u001b[36m166.4941\u001b[0m      \u001b[32m154.0001\u001b[0m  0.0401\n",
      "      5       \u001b[36m72.3221\u001b[0m       \u001b[32m69.1803\u001b[0m  0.0421\n",
      "      6       \u001b[36m67.4993\u001b[0m       \u001b[32m60.8313\u001b[0m  0.0301\n",
      "      5      \u001b[36m122.8218\u001b[0m      \u001b[32m107.5564\u001b[0m  0.0280\n",
      "      6       \u001b[36m59.4650\u001b[0m       \u001b[32m54.3956\u001b[0m  0.0258\n",
      "      4      \u001b[36m136.4365\u001b[0m      \u001b[32m120.7413\u001b[0m  0.0359\n",
      "      4      \u001b[36m164.1593\u001b[0m      \u001b[32m150.5341\u001b[0m  0.0282\n",
      "      6       \u001b[36m64.5580\u001b[0m       \u001b[32m58.2448\u001b[0m  0.0346\n",
      "      5       \u001b[36m79.5969\u001b[0m       \u001b[32m75.1875\u001b[0m  0.0348\n",
      "      5      \u001b[36m129.1031\u001b[0m      117.0174  0.0382\n",
      "      5      \u001b[36m145.3686\u001b[0m      \u001b[32m135.7526\u001b[0m  0.0425\n",
      "      6       \u001b[36m72.2116\u001b[0m       \u001b[32m68.8184\u001b[0m  0.0337\n",
      "      6      \u001b[36m121.4128\u001b[0m      \u001b[32m106.5498\u001b[0m  0.0272\n",
      "      7       \u001b[36m66.9093\u001b[0m       \u001b[32m60.8211\u001b[0m  0.0298\n",
      "      5      \u001b[36m134.9903\u001b[0m      \u001b[32m120.3972\u001b[0m  0.0265\n",
      "      7       \u001b[36m59.0217\u001b[0m       \u001b[32m54.3672\u001b[0m  0.0339\n",
      "      5      \u001b[36m162.5693\u001b[0m      \u001b[32m149.6417\u001b[0m  0.0307\n",
      "      7       \u001b[36m64.1131\u001b[0m       58.4755  0.0335\n",
      "      6       \u001b[36m79.1414\u001b[0m       \u001b[32m75.1820\u001b[0m  0.0342\n",
      "      7       \u001b[36m72.1538\u001b[0m       \u001b[32m68.8133\u001b[0m  0.0305\n",
      "      6      \u001b[36m128.8610\u001b[0m      117.0332  0.0376\n",
      "      8       \u001b[36m66.7733\u001b[0m       \u001b[32m60.6240\u001b[0m  0.0242\n",
      "      7      \u001b[36m120.1062\u001b[0m      \u001b[32m105.9780\u001b[0m  0.0273\n",
      "      6      \u001b[36m145.0036\u001b[0m      \u001b[32m135.5963\u001b[0m  0.0391\n",
      "      8       59.0847       54.3816  0.0315\n",
      "      6      \u001b[36m134.6848\u001b[0m      121.1976  0.0381\n",
      "      7       \u001b[36m79.0731\u001b[0m       75.1882  0.0288\n",
      "      8       \u001b[36m72.0368\u001b[0m       68.8612  0.0291\n",
      "      9       \u001b[36m66.4768\u001b[0m       60.7299  0.0262\n",
      "      8       \u001b[36m63.9980\u001b[0m       58.4686  0.0353\n",
      "      8      \u001b[36m119.4999\u001b[0m      106.0634  0.0270\n",
      "      7      \u001b[36m128.8544\u001b[0m      116.8852  0.0302\n",
      "      7      \u001b[36m144.7615\u001b[0m      \u001b[32m135.3212\u001b[0m  0.0266\n",
      "      6      \u001b[36m160.8406\u001b[0m      \u001b[32m148.0935\u001b[0m  0.0443\n",
      "      9       59.1115       54.4112  0.0299\n",
      "      8       \u001b[36m78.7421\u001b[0m       75.2010  0.0283\n",
      "      9       72.0766       \u001b[32m68.4663\u001b[0m  0.0265\n",
      "      9       \u001b[36m63.6473\u001b[0m       \u001b[32m57.8396\u001b[0m  0.0256\n",
      "     10       \u001b[36m66.4171\u001b[0m       \u001b[32m60.5480\u001b[0m  0.0268\n",
      "      7      \u001b[36m134.0887\u001b[0m      \u001b[32m120.2769\u001b[0m  0.0408\n",
      "      7      \u001b[36m159.3552\u001b[0m      \u001b[32m147.2726\u001b[0m  0.0317\n",
      "      9      \u001b[36m119.3807\u001b[0m      \u001b[32m105.6698\u001b[0m  0.0369\n",
      "      8      \u001b[36m144.2218\u001b[0m      135.6016  0.0352\n",
      "      8      \u001b[36m128.5561\u001b[0m      \u001b[32m116.2329\u001b[0m  0.0364\n",
      "     10       \u001b[36m58.9932\u001b[0m       54.4146  0.0239\n",
      "     11       \u001b[36m66.3685\u001b[0m       60.6051  0.0238\n",
      "      9       \u001b[36m78.6438\u001b[0m       75.2395  0.0268\n",
      "     10       \u001b[36m71.9230\u001b[0m       \u001b[32m68.1627\u001b[0m  0.0279\n",
      "     10       \u001b[36m63.6407\u001b[0m       \u001b[32m57.8073\u001b[0m  0.0307\n",
      "      8      \u001b[36m132.9313\u001b[0m      120.7330  0.0277\n",
      "     11       59.0051       54.4237  0.0269\n",
      "      9      144.4339      \u001b[32m135.2723\u001b[0m  0.0316\n",
      "     10      119.5850      \u001b[32m105.6338\u001b[0m  0.0356\n",
      "      8      \u001b[36m158.8684\u001b[0m      \u001b[32m146.8713\u001b[0m  0.0373\n",
      "     11       \u001b[36m71.7773\u001b[0m       68.1799  0.0294\n",
      "      9      128.7996      \u001b[32m115.6600\u001b[0m  0.0437\n",
      "     10       \u001b[36m78.6210\u001b[0m       75.2420  0.0353\n",
      "     12       66.3951       60.6060  0.0499\n",
      "     11      119.4944      \u001b[32m105.6260\u001b[0m  0.0265\n",
      "     11       \u001b[36m63.6169\u001b[0m       57.8119  0.0490\n",
      "      9      133.2297      120.6297  0.0491\n",
      "     12       59.0001       54.5580  0.0536\n",
      "     10      \u001b[36m144.0755\u001b[0m      \u001b[32m134.6597\u001b[0m  0.0514\n",
      "     12      119.4837      105.7537  0.0267\n",
      "     12       \u001b[36m63.5665\u001b[0m       57.8147  0.0267      9      \u001b[36m158.6654\u001b[0m      \u001b[32m146.8198\u001b[0m  0.0576\n",
      "\n",
      "     12       \u001b[36m71.6456\u001b[0m       \u001b[32m68.0320\u001b[0m  0.0499\n",
      "     13       \u001b[36m66.2576\u001b[0m       \u001b[32m60.4757\u001b[0m  0.0328\n",
      "     10      \u001b[36m128.2474\u001b[0m      115.7408  0.0563\n",
      "     11       78.6511       75.2226  0.0573\n",
      "     10      \u001b[36m132.8794\u001b[0m      120.7501  0.0451\n",
      "     13      \u001b[36m119.2766\u001b[0m      105.6468  0.0305\n",
      "     11      \u001b[36m143.8562\u001b[0m      \u001b[32m134.0568\u001b[0m  0.0349\n",
      "     10      \u001b[36m158.1568\u001b[0m      146.9022  0.0323\n",
      "     13       \u001b[36m63.5633\u001b[0m       57.8139  0.0330\n",
      "     13       59.0305       54.4868  0.0460\n",
      "     14       66.2714       60.5268  0.0355\n",
      "     12       78.7317       \u001b[32m75.0413\u001b[0m  0.0272\n",
      "     11      \u001b[36m127.9696\u001b[0m      \u001b[32m114.4484\u001b[0m  0.0424\n",
      "     11      \u001b[36m132.1846\u001b[0m      \u001b[32m118.0436\u001b[0m  0.0306\n",
      "     14      119.3987      105.6630  0.0260\n",
      "     12      \u001b[36m143.5840\u001b[0m      \u001b[32m133.6751\u001b[0m  0.0278\n",
      "     14       59.0304       54.6197  0.0257\n",
      "     13       \u001b[36m71.5456\u001b[0m       \u001b[32m68.0183\u001b[0m  0.0598\n",
      "     15       \u001b[36m66.2202\u001b[0m       60.5810  0.0344\n",
      "     15      \u001b[36m119.1266\u001b[0m      \u001b[32m105.6169\u001b[0m  0.0262\n",
      "     13       \u001b[36m78.6207\u001b[0m       75.0435  0.0421\n",
      "     14       \u001b[36m63.5629\u001b[0m       \u001b[32m57.8056\u001b[0m  0.0510\n",
      "     14       \u001b[36m71.5339\u001b[0m       \u001b[32m68.0150\u001b[0m  0.0279\n",
      "     11      158.2122      146.9023  0.0570\n",
      "     12      \u001b[36m127.6659\u001b[0m      114.8348  0.0412\n",
      "     12      132.5201      \u001b[32m117.9624\u001b[0m  0.0433\n",
      "     15       59.0264       54.6245  0.0490\n",
      "     16      \u001b[36m118.9909\u001b[0m      105.6207  0.0332\n",
      "     16       \u001b[36m66.2053\u001b[0m       60.5432  0.0453\n",
      "     14       \u001b[36m78.6004\u001b[0m       75.0894  0.0356\n",
      "     15       63.5642       \u001b[32m57.8008\u001b[0m  0.0367\n",
      "     13      \u001b[36m143.1394\u001b[0m      133.7023  0.0656\n",
      "     13      127.7131      \u001b[32m114.1817\u001b[0m  0.0329\n",
      "     12      \u001b[36m157.8177\u001b[0m      146.9028  0.0418\n",
      "     13      132.4125      118.3089  0.0440\n",
      "     15       \u001b[36m71.4687\u001b[0m       68.0155  0.0551\n",
      "     15       78.6397       75.1769  0.0275\n",
      "     16       59.0098       54.5829  0.0387\n",
      "     17       66.2540       60.6787  0.0331\n",
      "     17      119.5480      105.6600  0.0386\n",
      "     16       \u001b[36m63.5506\u001b[0m       \u001b[32m57.7923\u001b[0m  0.0316\n",
      "     14      \u001b[36m142.7698\u001b[0m      133.8503  0.0330\n",
      "     14      \u001b[36m127.6124\u001b[0m      114.3039  0.0319\n",
      "     13      \u001b[36m157.6673\u001b[0m      146.8959  0.0292\n",
      "     14      132.6481      119.4656  0.0334\n",
      "     16       \u001b[36m78.5820\u001b[0m       75.1143  0.0255\n",
      "     18       \u001b[36m66.2011\u001b[0m       60.5347  0.0259\n",
      "     18      119.3087      105.7026  0.0291\n",
      "     16       \u001b[36m71.4086\u001b[0m       68.0340  0.0392\n",
      "     17       \u001b[36m58.9819\u001b[0m       54.5992  0.0349\n",
      "     17       \u001b[36m63.5389\u001b[0m       57.8529  0.0333\n",
      "     14      157.9317      146.8977  0.0278\n",
      "     15      \u001b[36m127.5252\u001b[0m      114.4596  0.0357\n",
      "     15      132.5830      118.2203  0.0284\n",
      "     19       66.2654       60.6146  0.0288\n",
      "     19      119.2757      105.7093  0.0259\n",
      "     15      \u001b[36m142.5691\u001b[0m      133.8631  0.0522\n",
      "     17       \u001b[36m71.3827\u001b[0m       68.0704  0.0292\n",
      "     18       \u001b[36m58.9619\u001b[0m       54.6214  0.0316\n",
      "     17       78.5823       75.0646  0.0439\n",
      "     15      158.1044      146.8978  0.0315\n",
      "     18       \u001b[36m63.4653\u001b[0m       57.9154  0.0392\n",
      "     20      119.2893      105.7605  0.0275\n",
      "     16      \u001b[36m132.0438\u001b[0m      118.4069  0.0409\n",
      "     20       \u001b[36m66.1791\u001b[0m       60.7866  0.0364\n",
      "     16      \u001b[36m127.3228\u001b[0m      114.8651  0.0409\n",
      "     19       59.0101       54.6208  0.0341\n",
      "     16      142.8556      133.9024  0.0475\n",
      "     18       71.3921       68.0698  0.0447\n",
      "     19       63.5101       57.9419  0.0410\n",
      "     18       78.6039       75.0847  0.0509\n",
      "     21      119.4375      105.8489  0.0356\n",
      "     16      158.2166      146.8974  0.0606\n",
      "     17      \u001b[36m131.8819\u001b[0m      118.5627  0.0401\n",
      "     17      142.6622      133.8856  0.0306\n",
      "     17      \u001b[36m127.0398\u001b[0m      114.8163  0.0390\n",
      "     19       71.3891       68.0689  0.0306\n",
      "     20       58.9978       54.6067  0.0396\n",
      "     21       \u001b[36m66.1720\u001b[0m       60.7562  0.0494\n",
      "     19       78.5899       75.0797  0.0273\n",
      "     22      119.2506      105.8641  0.0260\n",
      "     17      157.8574      146.8968  0.0274\n",
      "     18      132.1089      118.3918  0.0274\n",
      "     18      142.5709      133.8915  0.0256\n",
      "     20       63.4776       57.9684  0.0434\n",
      "     18      127.1873      114.8261  0.0268\n",
      "     21       \u001b[36m58.9394\u001b[0m       54.4920  0.0265\n",
      "     20       \u001b[36m71.3805\u001b[0m       68.0681  0.0284\n",
      "     23      119.1180      106.0207  0.0268\n",
      "     22       \u001b[36m66.1614\u001b[0m       60.7031  0.0332\n",
      "     20       78.6141       75.0823  0.0337\n",
      "     19      132.3260      118.5493  0.0261\n",
      "     19      127.2012      114.7638  0.0268\n",
      "     19      \u001b[36m142.5267\u001b[0m      133.8660  0.0293\n",
      "     21       \u001b[36m71.3798\u001b[0m       68.0493  0.0255\n",
      "     21       \u001b[36m63.4335\u001b[0m       57.9185  0.0335\n",
      "     22       59.1632       54.5646  0.0360\n",
      "     21       78.6186       \u001b[32m75.0172\u001b[0m  0.0254\n",
      "     23       66.2438       60.8038  0.0311\n",
      "     24      119.5702      105.9955  0.0324\n",
      "     20      \u001b[36m142.3549\u001b[0m      133.8416  0.0259\n",
      "     20      132.1776      118.8534  0.0324\n",
      "     20      \u001b[36m126.9369\u001b[0m      114.5528  0.0279\n",
      "     22       71.4017       68.0188  0.0269\n",
      "     22       63.5383       57.9684  0.0263\n",
      "     22       \u001b[36m78.5609\u001b[0m       75.1070  0.0257\n",
      "     23       59.0334       54.6173  0.0325\n",
      "     18      158.0861      146.8966  0.0767\n",
      "     24       66.2576       60.6519  0.0341\n",
      "     25      119.2635      106.5283  0.0371\n",
      "     21      \u001b[36m126.5850\u001b[0m      114.4930  0.0268\n",
      "     21      142.6375      133.9210  0.0332\n",
      "     23       63.5659       57.9956  0.0282\n",
      "     23       71.3972       68.0431  0.0332\n",
      "     21      132.1009      118.8307  0.0362\n",
      "     24       58.9964       54.6027  0.0249\n",
      "     23       \u001b[36m78.4995\u001b[0m       75.3446  0.0319\n",
      "     24       63.6044       58.0053  0.0257\n",
      "     22      142.4753      134.6813  0.0265\n",
      "     22      126.9440      114.7398  0.0322\n",
      "     25       \u001b[36m66.1445\u001b[0m       60.6238  0.0383\n",
      "     26      119.5278      106.2161  0.0353\n",
      "     22      132.0524      119.2408  0.0281\n",
      "     25       59.0124       54.6068  0.0265\n",
      "     24       71.3934       \u001b[32m68.0137\u001b[0m  0.0345\n",
      "     19      157.7396      146.8964  0.0557\n",
      "     24       78.5952       75.2653  0.0361\n",
      "     26       66.1795       \u001b[32m60.4697\u001b[0m  0.0280\n",
      "     23      142.5409      134.6762  0.0305     25       63.5395       57.9564  0.0315\n",
      "\n",
      "     23      132.3729      119.2829  0.0274\n",
      "     23      126.8200      114.4269  0.0339\n",
      "     25       \u001b[36m71.3167\u001b[0m       \u001b[32m68.0098\u001b[0m  0.0249\n",
      "     26       58.9843       54.6079  0.0282\n",
      "     27      119.3272      106.2699  0.0395\n",
      "     20      \u001b[36m157.6313\u001b[0m      146.8828  0.0362\n",
      "     25       78.5635       75.0801  0.0268\n",
      "     26       63.4837       57.9431  0.0278\n",
      "     26       71.4225       68.0690  0.0237\n",
      "     24      \u001b[36m142.3372\u001b[0m      134.3368  0.0296\n",
      "     24      126.9748      114.2244  0.0293\n",
      "     27       \u001b[36m66.1187\u001b[0m       60.5735  0.0358\n",
      "     24      132.0907      \u001b[32m117.4537\u001b[0m  0.0341\n",
      "     28      119.1583      105.8227  0.0325\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 7.\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     26       78.6136       75.0928  0.0298\n",
      "     21      \u001b[36m157.6136\u001b[0m      146.8973  0.0336\n",
      "     27       63.4689       57.9419  0.0264\n",
      "     25      142.5478      134.1919  0.0282\n",
      "     27       71.3788       68.0474  0.0314\n",
      "     28       66.1818       60.6020  0.0258\n",
      "     25      127.1386      114.1923  0.0331\n",
      "     29      119.1265      105.8582  0.0269\n",
      "     25      132.2795      \u001b[32m117.4226\u001b[0m  0.0344\n",
      "     27       78.6061       75.0961  0.0256\n",
      "     22      157.6329      146.8960  0.0272\n",
      "     28       \u001b[36m63.4192\u001b[0m       57.9985  0.0281\n",
      "     29       66.1220       60.6040  0.0271\n",
      "     26      \u001b[36m142.3372\u001b[0m      134.4254  0.0340\n",
      "     26      127.4639      \u001b[32m113.3319\u001b[0m  0.0274\n",
      "     30      \u001b[36m118.8324\u001b[0m      105.8704  0.0280\n",
      "     26      \u001b[36m131.8761\u001b[0m      118.1605  0.0269\n",
      "     28       78.5728       75.2770  0.0284\n",
      "     28       71.3877       68.0301  0.0451\n",
      "     29       63.4872       57.9990  0.0305\n",
      "     30       \u001b[36m66.0793\u001b[0m       60.6313  0.0551\n",
      "     27      131.8780      119.1202  0.0472\n",
      "     23      157.8948      146.8959  0.0660\n",
      "     29       71.3505       68.0111  0.0451\n",
      "     29       78.5850       75.3082  0.0534\n",
      "     31      119.0969      105.8571  0.0720     27      142.3811      134.1149  0.0774\n",
      "\n",
      "     27      126.9663      113.4737  0.0834\n",
      "     30       63.4879       57.9575  0.0703\n",
      "     30       78.5780       75.2578  0.0327\n",
      "     28      \u001b[36m131.8659\u001b[0m      119.2443  0.0470\n",
      "     31       66.1393       60.5561  0.0553\n",
      "     32      119.0189      106.2098  0.0335\n",
      "     28      142.4817      133.8654  0.0377\n",
      "     30       71.3551       68.0353  0.0570\n",
      "     28      126.9417      113.6226  0.0324\n",
      "     31       63.4510       57.9269  0.0354\n",
      "     29      131.9611      119.1352  0.0328\n",
      "     24      157.6353      146.8957  0.0807\n",
      "     29      142.3387      133.8655  0.0265\n",
      "     29      126.6918      113.5303  0.0263\n",
      "     33      119.6282      106.1629  0.0327\n",
      "     31       78.5613       75.3334  0.0459\n",
      "     32       \u001b[36m66.0613\u001b[0m       60.6109  0.0435\n",
      "     32       63.4732       57.9272  0.0238\n",
      "     31       71.3625       68.0506  0.0397\n",
      "     30      132.2824      119.1122  0.0255\n",
      "     30      142.3619      133.8655  0.0260\n",
      "     30      126.6161      113.5278  0.0255\n",
      "     34      119.4298      106.1543  0.0260\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 13.\n",
      "     32       78.5991       75.1789  0.0300\n",
      "     33       63.4989       57.9441  0.0272\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     32       71.3337       68.0124  0.0252\n",
      "     25      157.9646      146.8955  0.0479\n",
      "     31      \u001b[36m131.6820\u001b[0m      119.0143  0.0260\n",
      "     31      \u001b[36m142.3175\u001b[0m      133.8836  0.0262\n",
      "     31      \u001b[36m126.3410\u001b[0m      113.5463  0.0261\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 15.\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     33       78.5855       75.2616  0.0266\n",
      "     34       63.4536       57.9424  0.0253\n",
      "     33       71.4096       68.0691  0.0239\n",
      "     32      \u001b[36m131.5993\u001b[0m      119.1993  0.0279\n",
      "     26      157.7373      146.8955  0.0353\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 12.\n",
      "     32      126.5968      113.5723  0.0276\n",
      "     34       78.5705       75.5341  0.0242\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     35       \u001b[36m63.4174\u001b[0m       57.9330  0.0246\n",
      "     34       71.3854       68.0691  0.0245\n",
      "     33      132.3565      119.3158  0.0271\n",
      "     27      157.9853      146.8955  0.0291\n",
      "     33      126.4671      113.4983  0.0268\n",
      "     35       78.5598       75.1689  0.0247\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 16.\n",
      "     35       71.3816       68.0695  0.0247\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     34      132.0510      119.4069  0.0268\n",
      "     28      158.0386      146.8955  0.0264\n",
      "     34      126.4522      113.4636  0.0265\n",
      "     36       78.5695       75.0693  0.0253\n",
      "     36       71.3794       68.0698  0.0245\n",
      "     35      132.5402      119.4688  0.0269\n",
      "     37       78.5728       75.1895  0.0246\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 9.\n",
      "     35      126.5258      113.5561  0.0268\n",
      "     37       71.4152       68.0641  0.0250\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     36      132.4814      119.8754  0.0266\n",
      "     38       71.3746       68.0700  0.0247\n",
      "     38       78.5370       75.4463  0.0287\n",
      "     36      126.3779      113.4915  0.0270\n",
      "     37      132.0375      119.8993  0.0265\n",
      "     39       71.3601       68.0693  0.0240\n",
      "     39       78.5774       75.4325  0.0238\n",
      "     37      126.6703      \u001b[32m113.1139\u001b[0m  0.0261\n",
      "     38      132.2748      119.5289  0.0257\n",
      "     40       71.3602       68.0537  0.0238\n",
      "     40       78.5078       75.2930  0.0240\n",
      "     38      126.5323      113.3860  0.0255\n",
      "     39      132.5055      119.0265  0.0265\n",
      "     41       71.3515       68.0679  0.0238\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 21.\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     39      126.5108      \u001b[32m113.0534\u001b[0m  0.0263\n",
      "     42       71.4039       68.0690  0.0254\n",
      "     40      131.7446      117.7440  0.0264\n",
      "     40      126.6495      113.5539  0.0254\n",
      "     43       71.3809       68.0684  0.0243\n",
      "     41      132.1189      117.9995  0.0266\n",
      "     41      126.5834      113.4687  0.0262\n",
      "     44       71.3693       68.0561  0.0245\n",
      "     42      132.3853      118.0913  0.0261\n",
      "     42      126.5148      113.5370  0.0267\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 25.\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     43      132.1752      118.0897  0.0253\n",
      "     43      126.4020      113.5579  0.0267\n",
      "     44      132.8332      118.0915  0.0271\n",
      "     44      \u001b[36m126.3226\u001b[0m      113.6677  0.0275\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 25.\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     45      \u001b[36m126.3115\u001b[0m      113.6797  0.0254\n",
      "     46      126.4621      113.6251  0.0257\n",
      "     47      \u001b[36m126.2249\u001b[0m      113.6397  0.0250\n",
      "     48      \u001b[36m126.0806\u001b[0m      113.5937  0.0254\n",
      "     49      126.2747      113.5831  0.0258\n",
      "     50      126.3712      113.6636  0.0261\n",
      "Restoring best model from epoch 39.\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m83.6049\u001b[0m       \u001b[32m72.6861\u001b[0m  0.0366\n",
      "      2       \u001b[36m71.6988\u001b[0m       \u001b[32m68.1753\u001b[0m  0.0405\n",
      "      3       \u001b[36m69.0969\u001b[0m       \u001b[32m66.7200\u001b[0m  0.0348\n",
      "      4       \u001b[36m68.6379\u001b[0m       \u001b[32m65.9870\u001b[0m  0.0431\n",
      "      5       \u001b[36m67.9901\u001b[0m       \u001b[32m65.5598\u001b[0m  0.0389\n",
      "      6       \u001b[36m67.5191\u001b[0m       \u001b[32m65.5121\u001b[0m  0.0353\n",
      "      7       67.6005       \u001b[32m65.2453\u001b[0m  0.0340\n",
      "      8       \u001b[36m67.4546\u001b[0m       \u001b[32m65.1456\u001b[0m  0.0333\n",
      "      9       \u001b[36m67.3659\u001b[0m       \u001b[32m65.0473\u001b[0m  0.0424\n",
      "     10       \u001b[36m67.3544\u001b[0m       65.0490  0.0349\n",
      "     11       67.3644       65.0491  0.0355\n",
      "     12       67.3571       65.0498  0.0357\n",
      "     13       \u001b[36m67.3290\u001b[0m       65.0478  0.0348\n",
      "     14       67.3893       65.0687  0.0487\n",
      "     15       67.3387       65.1959  0.0347\n",
      "     16       67.3309       65.1899  0.0347\n",
      "     17       \u001b[36m67.3226\u001b[0m       65.3377  0.0355\n",
      "     18       67.3260       65.1686  0.0341\n",
      "     19       67.3789       65.2168  0.0343\n",
      "     20       67.3249       65.2234  0.0347\n",
      "     21       \u001b[36m67.3042\u001b[0m       65.2063  0.0339\n",
      "     22       67.3715       65.2198  0.0462\n",
      "     23       67.3221       65.0480  0.0353\n",
      "     24       67.3272       \u001b[32m65.0439\u001b[0m  0.0340\n",
      "     25       67.3300       65.0527  0.0340\n",
      "     26       \u001b[36m67.2992\u001b[0m       65.2467  0.0347\n",
      "     27       67.3380       65.2220  0.0350\n",
      "     28       67.3030       65.0461  0.0465\n",
      "     29       67.3525       \u001b[32m65.0368\u001b[0m  0.0349\n",
      "     30       67.3523       65.0421  0.0364\n",
      "     31       67.3764       65.0388  0.0353\n",
      "     32       67.3530       65.0434  0.0534\n",
      "     33       67.3296       \u001b[32m65.0354\u001b[0m  0.0433\n",
      "     34       67.3577       65.0365  0.0417\n",
      "     35       67.3506       65.0745  0.0382\n",
      "     36       67.3239       65.0829  0.0334\n",
      "     37       67.3301       65.0503  0.0357\n",
      "     38       67.3453       65.0487  0.0348\n",
      "     39       67.3453       65.0404  0.0426\n",
      "     40       67.3647       \u001b[32m65.0348\u001b[0m  0.0347\n",
      "     41       67.3162       65.0389  0.0366\n",
      "     42       \u001b[36m67.2930\u001b[0m       65.0747  0.0405\n",
      "     43       67.3509       65.1205  0.0338\n",
      "     44       67.3202       65.0558  0.0419\n",
      "     45       67.3209       65.0445  0.0352\n",
      "     46       67.3380       65.0858  0.0382\n",
      "     47       67.3174       65.1367  0.0352\n",
      "     48       67.3789       65.1049  0.0332\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 29.\n",
      "durations 0.26283368 84.20534\n",
      "split age                  float32\n",
      "sex                 category\n",
      "race                 float32\n",
      "n_comorbidities      float32\n",
      "diabetes            category\n",
      "dementia            category\n",
      "blood_pressure       float32\n",
      "heart_rate           float32\n",
      "respiration_rate     float32\n",
      "temperature          float32\n",
      "white_blood_cell     float32\n",
      "serum_sodium         float32\n",
      "serum_creatinine     float32\n",
      "cancer_0.0             uint8\n",
      "cancer_1.0             uint8\n",
      "cancer_2.0             uint8\n",
      "dtype: object\n",
      "dtype X_train age                  float32\n",
      "sex                 category\n",
      "race                 float32\n",
      "n_comorbidities      float32\n",
      "diabetes            category\n",
      "dementia            category\n",
      "blood_pressure       float32\n",
      "heart_rate           float32\n",
      "respiration_rate     float32\n",
      "temperature          float32\n",
      "white_blood_cell     float32\n",
      "serum_sodium         float32\n",
      "serum_creatinine     float32\n",
      "cancer_0.0             uint8\n",
      "cancer_1.0             uint8\n",
      "cancer_2.0             uint8\n",
      "dtype: object\n",
      "dtype y_train float64\n",
      "dtype X_test age                  float32\n",
      "sex                 category\n",
      "race                 float32\n",
      "n_comorbidities      float32\n",
      "diabetes            category\n",
      "dementia            category\n",
      "blood_pressure       float32\n",
      "heart_rate           float32\n",
      "respiration_rate     float32\n",
      "temperature          float32\n",
      "white_blood_cell     float32\n",
      "serum_sodium         float32\n",
      "serum_creatinine     float32\n",
      "cancer_0.0             uint8\n",
      "cancer_1.0             uint8\n",
      "cancer_2.0             uint8\n",
      "dtype: object\n",
      "dtype y_test float32\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m76.4367\u001b[0m       \u001b[32m68.1263\u001b[0m  0.1207\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m79.8537\u001b[0m       \u001b[32m70.7358\u001b[0m  0.1199\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m76.3818\u001b[0m       \u001b[32m68.3997\u001b[0m  0.1372\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m93.5973\u001b[0m       \u001b[32m79.6704\u001b[0m  0.1129\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m96.4438\u001b[0m       \u001b[32m84.0106\u001b[0m  0.1323\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m151.7867\u001b[0m      \u001b[32m139.7149\u001b[0m  0.1413\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m151.8081\u001b[0m      \u001b[32m139.6255\u001b[0m  0.1681\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m192.2651\u001b[0m      \u001b[32m173.1536\u001b[0m  0.1380\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m159.4273\u001b[0m      \u001b[32m146.1800\u001b[0m  0.1738\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m187.5384\u001b[0m      \u001b[32m165.5563\u001b[0m  0.1616\n",
      "      2       \u001b[36m70.5662\u001b[0m       70.8433  0.1305\n",
      "      2       \u001b[36m68.0382\u001b[0m       \u001b[32m68.1699\u001b[0m  0.1200\n",
      "      2       \u001b[36m80.8069\u001b[0m       \u001b[32m79.6696\u001b[0m  0.1256\n",
      "      2       \u001b[36m68.0592\u001b[0m       68.1828  0.1500\n",
      "      2       \u001b[36m83.4465\u001b[0m       \u001b[32m83.2965\u001b[0m  0.1423\n",
      "      2      \u001b[36m138.1739\u001b[0m      \u001b[32m136.6938\u001b[0m  0.1371\n",
      "      2      \u001b[36m138.1537\u001b[0m      \u001b[32m136.8926\u001b[0m  0.1160\n",
      "      2      \u001b[36m144.4470\u001b[0m      \u001b[32m142.6627\u001b[0m  0.1217\n",
      "      2      \u001b[36m172.0063\u001b[0m      \u001b[32m168.6636\u001b[0m  0.1434\n",
      "      2      \u001b[36m166.0433\u001b[0m      \u001b[32m161.9182\u001b[0m  0.1345\n",
      "      3       \u001b[36m80.7985\u001b[0m       79.6700  0.1128\n",
      "      3       \u001b[36m67.9919\u001b[0m       68.1883  0.1260\n",
      "      3       \u001b[36m67.9708\u001b[0m       68.1859  0.1204\n",
      "      3       \u001b[36m83.3477\u001b[0m       83.3006  0.1124\n",
      "      3       \u001b[36m70.5183\u001b[0m       70.8796  0.1693\n",
      "      3      \u001b[36m136.7071\u001b[0m      137.0364  0.1215\n",
      "      3      \u001b[36m136.6851\u001b[0m      \u001b[32m136.3786\u001b[0m  0.1414\n",
      "      3      \u001b[36m142.3177\u001b[0m      \u001b[32m141.9651\u001b[0m  0.1396\n",
      "      3      \u001b[36m164.0853\u001b[0m      \u001b[32m160.5403\u001b[0m  0.1279\n",
      "      3      \u001b[36m168.8122\u001b[0m      \u001b[32m168.1909\u001b[0m  0.1523\n",
      "      4       80.7991       79.6703  0.1194\n",
      "      4       \u001b[36m67.9702\u001b[0m       68.1861  0.1301\n",
      "      4       \u001b[36m67.9691\u001b[0m       68.1871  0.1524\n",
      "      4       83.3486       83.2969  0.1388\n",
      "      4      136.8108      \u001b[32m136.5720\u001b[0m  0.1568\n",
      "      4       70.5216       70.8350  0.1718\n",
      "      4      \u001b[36m162.8703\u001b[0m      \u001b[32m160.3690\u001b[0m  0.1307\n",
      "      4      \u001b[36m141.8614\u001b[0m      \u001b[32m141.8075\u001b[0m  0.1359\n",
      "      4      \u001b[36m136.3119\u001b[0m      136.6836  0.1581\n",
      "      5       80.7994       79.6708  0.1124\n",
      "      4      \u001b[36m168.7585\u001b[0m      \u001b[32m167.8172\u001b[0m  0.1367\n",
      "      5       \u001b[36m67.9614\u001b[0m       68.1795  0.1218\n",
      "      5       83.3482       83.2970  0.1057\n",
      "      5       \u001b[36m67.9563\u001b[0m       68.2242  0.1208\n",
      "      5      136.7836      \u001b[32m136.1985\u001b[0m  0.1302\n",
      "      6       80.8004       79.6709  0.1130\n",
      "      5       70.5194       70.8356  0.1355\n",
      "      5      \u001b[36m162.8346\u001b[0m      \u001b[32m159.5491\u001b[0m  0.1341\n",
      "      5      \u001b[36m167.8500\u001b[0m      \u001b[32m166.4869\u001b[0m  0.1223\n",
      "      5      136.5249      136.4732  0.1461\n",
      "      5      \u001b[36m141.4051\u001b[0m      \u001b[32m141.5660\u001b[0m  0.1447\n",
      "      6       67.9869       68.1835  0.1259\n",
      "      6       67.9733       68.1878  0.1244\n",
      "      6       83.3502       83.2971  0.1299\n",
      "      7       80.7995       79.6711  0.1068\n",
      "      6       70.5196       70.8352  0.1196\n",
      "      6      \u001b[36m136.5676\u001b[0m      136.2921  0.1407\n",
      "      6      \u001b[36m161.8739\u001b[0m      \u001b[32m159.2379\u001b[0m  0.1278\n",
      "      6      \u001b[36m136.2699\u001b[0m      136.5020  0.1198\n",
      "      7       83.3484       83.2966  0.1007\n",
      "      6      \u001b[36m141.0773\u001b[0m      \u001b[32m141.4073\u001b[0m  0.1438\n",
      "      7       \u001b[36m67.9574\u001b[0m       68.1864  0.1351\n",
      "      7       67.9706       68.1879  0.1372\n",
      "      6      \u001b[36m166.9473\u001b[0m      166.5964  0.1727\n",
      "      8       80.8000       79.6711  0.1297\n",
      "      7       70.5217       70.8288  0.1190\n",
      "      7      \u001b[36m161.6637\u001b[0m      \u001b[32m159.2337\u001b[0m  0.1203\n",
      "      8       83.3483       83.2968  0.1098\n",
      "      7      \u001b[36m136.3196\u001b[0m      136.2509  0.1471\n",
      "      7      136.2928      136.5328  0.1340\n",
      "      8       67.9622       68.1691  0.1179\n",
      "      7      141.0789      141.5323  0.1468\n",
      "      7      \u001b[36m166.8571\u001b[0m      166.5980  0.1264\n",
      "      9       80.8021       79.6712  0.1248\n",
      "      8       \u001b[36m70.5166\u001b[0m       70.7879  0.1288\n",
      "      9       83.3478       83.2968  0.1236\n",
      "      8      \u001b[36m161.6474\u001b[0m      159.2374  0.1330\n",
      "      8      \u001b[36m136.1787\u001b[0m      \u001b[32m136.1143\u001b[0m  0.1268\n",
      "      8       67.9697       68.1891  0.2193\n",
      "      8      \u001b[36m136.2470\u001b[0m      \u001b[32m136.1953\u001b[0m  0.1400\n",
      "      9       67.9903       68.1835  0.1397\n",
      "      8      \u001b[36m166.7944\u001b[0m      166.4972  0.1239\n",
      "      8      141.4488      141.4683  0.1538\n",
      "     10       80.7995       79.6711  0.1195\n",
      "      9       \u001b[36m70.4944\u001b[0m       70.7901  0.1221\n",
      "     10       83.3480       83.2969  0.1116\n",
      "      9      \u001b[36m161.6218\u001b[0m      159.2382  0.1343\n",
      "     10       67.9783       68.1866  0.1118\n",
      "      9       67.9691       68.1927  0.1374\n",
      "      9      \u001b[36m136.1469\u001b[0m      136.3273  0.1287\n",
      "      9      \u001b[36m136.0647\u001b[0m      \u001b[32m136.0808\u001b[0m  0.1551\n",
      "     11       80.7996       79.6709  0.1081\n",
      "     11       83.3491       83.2966  0.1113\n",
      "      9      166.9061      166.4883  0.1647\n",
      "      9      141.6689      141.8189  0.1442\n",
      "     10       70.5214       70.7886  0.1454\n",
      "     11       \u001b[36m67.9489\u001b[0m       68.1793  0.1142\n",
      "     10       67.9748       68.1944  0.1254\n",
      "     10      161.6269      159.2374  0.1454\n",
      "     10      \u001b[36m136.1293\u001b[0m      136.2001  0.1294\n",
      "     12       80.7994       79.6711  0.1119\n",
      "     10      136.1553      136.1119  0.1469\n",
      "     12       83.3480       83.2967  0.1313\n",
      "     10      141.4625      141.8329  0.1273\n",
      "     10      \u001b[36m166.7152\u001b[0m      166.4882  0.1362\n",
      "     11       70.5139       70.7899  0.1158\n",
      "     12       67.9811       68.1913  0.1112\n",
      "     11       67.9680       68.1877  0.1208\n",
      "     11      \u001b[36m161.5935\u001b[0m      159.2349  0.1433\n",
      "     11      \u001b[36m135.9684\u001b[0m      \u001b[32m136.1050\u001b[0m  0.1406\n",
      "     13       80.7994       79.6710  0.1235\n",
      "     11      136.1738      136.2753  0.1303\n",
      "     13       \u001b[36m83.3477\u001b[0m       83.2966  0.1212\n",
      "     12       70.5097       70.7937  0.1203\n",
      "     11      141.1851      \u001b[32m141.3825\u001b[0m  0.1443\n",
      "     11      166.7204      166.6006  0.1411\n",
      "     13       67.9565       68.2088  0.1468\n",
      "     12       67.9732       68.1876  0.1609\n",
      "     14       80.7994       79.6710  0.1697\n",
      "     12      \u001b[36m161.5934\u001b[0m      159.2373  0.1863\n",
      "     12      136.0748      136.1289  0.1787\n",
      "     12      \u001b[36m135.9516\u001b[0m      136.1244  0.2031\n",
      "     13       70.5125       70.8048  0.1522\n",
      "     14       \u001b[36m83.3476\u001b[0m       83.2965  0.1708\n",
      "     12      \u001b[36m166.6902\u001b[0m      166.4882  0.1667\n",
      "     14       67.9548       68.1707  0.1583\n",
      "     13       67.9709       68.1877  0.1372\n",
      "     12      141.0833      \u001b[32m141.2169\u001b[0m  0.2106\n",
      "     15       80.7991       79.6709  0.1282\n",
      "     13      \u001b[36m161.5882\u001b[0m      \u001b[32m159.1247\u001b[0m  0.1258\n",
      "     13      \u001b[36m136.0331\u001b[0m      136.1942  0.1335\n",
      "     13      135.9595      136.1244  0.1359\n",
      "     14       70.4971       \u001b[32m70.7321\u001b[0m  0.1299\n",
      "     15       67.9511       68.1902  0.1011\n",
      "     13      \u001b[36m166.6851\u001b[0m      166.4882  0.1321\n",
      "     15       83.3476       \u001b[32m83.2964\u001b[0m  0.1660\n",
      "     14       67.9710       68.1879  0.1245\n",
      "     16       80.7988       79.6708  0.1169\n",
      "     13      \u001b[36m141.0386\u001b[0m      141.2169  0.1415\n",
      "     14      161.5930      159.2343  0.1289\n",
      "     15       70.5167       70.7322  0.1183\n",
      "     14      136.0680      136.0853  0.1483\n",
      "     16       67.9633       68.2314  0.1309\n",
      "     14      \u001b[36m135.9377\u001b[0m      136.1244  0.1493\n",
      "     14      \u001b[36m166.6848\u001b[0m      166.5639  0.1498\n",
      "     16       \u001b[36m83.3474\u001b[0m       \u001b[32m83.2963\u001b[0m  0.1486\n",
      "     17       80.7988       79.6708  0.1203\n",
      "     15       67.9707       68.1886  0.1558\n",
      "     14      \u001b[36m141.0357\u001b[0m      141.2169  0.1520\n",
      "     15      161.5930      159.2338  0.1488\n",
      "     16       70.5262       70.7325  0.1423\n",
      "     15      136.1710      \u001b[32m136.0567\u001b[0m  0.1234\n",
      "     15      135.9432      136.1244  0.1223\n",
      "     17       67.9655       68.1825  0.1313\n",
      "     15      166.7170      166.4881  0.1339\n",
      "     17       \u001b[36m83.3470\u001b[0m       \u001b[32m83.2962\u001b[0m  0.1278\n",
      "     18       80.7987       79.6707  0.1329\n",
      "     16       67.9701       68.1883  0.1263\n",
      "     15      \u001b[36m141.0272\u001b[0m      141.2169  0.1324\n",
      "     17       70.5131       \u001b[32m70.7305\u001b[0m  0.1082\n",
      "     18       67.9628       68.1880  0.1192\n",
      "     16      \u001b[36m161.5879\u001b[0m      159.2373  0.1494\n",
      "     16      \u001b[36m135.9377\u001b[0m      136.1244  0.1336\n",
      "     16      136.0674      136.1244  0.1383\n",
      "     18       \u001b[36m83.3470\u001b[0m       \u001b[32m83.2961\u001b[0m  0.1218\n",
      "     19       80.7986       79.6706  0.1133\n",
      "     16      166.6897      166.4890  0.1348\n",
      "     17       67.9682       68.1877  0.1403\n",
      "     18       70.5056       \u001b[32m70.7287\u001b[0m  0.1261\n",
      "     16      141.0717      141.3370  0.1469\n",
      "     19       67.9699       68.1851  0.1224\n",
      "     17      161.6213      159.2373  0.1375\n",
      "     17      \u001b[36m135.9360\u001b[0m      136.1244  0.1405\n",
      "     17      135.9377      136.1244  0.1411\n",
      "     17      166.6944      166.7007  0.1201\n",
      "     19       \u001b[36m83.3469\u001b[0m       \u001b[32m83.2961\u001b[0m  0.1321\n",
      "     18       67.9719       68.2415  0.1193\n",
      "     20       \u001b[36m80.7984\u001b[0m       79.6704  0.1590\n",
      "     19       70.5408       70.7324  0.1185\n",
      "     17      141.0975      \u001b[32m141.2156\u001b[0m  0.1212\n",
      "     20       67.9667       68.1820  0.1158\n",
      "     18      135.9476      136.2371  0.1189\n",
      "     18      \u001b[36m135.9377\u001b[0m      136.1244  0.1375\n",
      "     18      \u001b[36m161.5878\u001b[0m      159.2373  0.1529\n",
      "     18      166.7100      166.4881  0.1299\n",
      "     20       \u001b[36m83.3469\u001b[0m       \u001b[32m83.2959\u001b[0m  0.1269\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 1.\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     20       70.5204       70.7343  0.1161\n",
      "     19       67.9898       68.1874  0.1360\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 1.\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     18      141.0477      141.2169  0.1660\n",
      "     19      135.9364      136.1244  0.1390\n",
      "     21       \u001b[36m83.3467\u001b[0m       83.2959  0.1189\n",
      "     19      161.5927      \u001b[32m159.1055\u001b[0m  0.1377\n",
      "     19      166.6911      166.4881  0.1306\n",
      "     19      135.9377      136.1244  0.1502\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 1.\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     20       67.9621       \u001b[32m68.1685\u001b[0m  0.1163\n",
      "     19      141.0344      141.2169  0.1147\n",
      "     20      135.9866      136.1244  0.1135\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 2.\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     20      161.5927      159.2176  0.1152\n",
      "     20      166.6896      166.4881  0.1161\n",
      "     20      135.9377      136.1244  0.1154\n",
      "     21       67.9628       68.1873  0.1059\n",
      "     20      141.0346      141.2169  0.1157\n",
      "     21      \u001b[36m135.9275\u001b[0m      136.1244  0.1152\n",
      "     21      161.5926      159.2176  0.1152\n",
      "     21      166.6897      \u001b[32m166.4845\u001b[0m  0.1159\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 2.\n",
      "     21      135.9377      136.1244  0.1153\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     21      141.0297      141.2169  0.1139\n",
      "     22      135.9496      136.1244  0.1144\n",
      "     22      161.5926      159.2373  0.1140\n",
      "     22      166.6895      166.4881  0.1154\n",
      "     22      \u001b[36m135.9376\u001b[0m      136.1244  0.1141\n",
      "     22      141.0369      141.2169  0.1138\n",
      "     23      135.9288      136.1143  0.1127\n",
      "     23      161.5926      159.2373  0.1147\n",
      "     23      166.6895      166.4881  0.1138\n",
      "     23      135.9376      136.1244  0.1146\n",
      "     23      141.0297      141.2169  0.1136\n",
      "     24      136.0223      136.0654  0.1137\n",
      "     24      161.6200      159.2373  0.1132\n",
      "     24      166.6894      166.4881  0.1150\n",
      "     24      135.9376      136.1244  0.1137\n",
      "     24      141.0345      141.2169  0.1141\n",
      "     25      135.9505      136.0862  0.1151\n",
      "     25      161.6127      159.1246  0.1146\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 5.\n",
      "     25      135.9377      136.1244  0.1135\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     25      141.0294      141.2169  0.1134\n",
      "     26      136.0893      \u001b[32m135.9906\u001b[0m  0.1131\n",
      "     26      161.5927      159.1246  0.1134\n",
      "     26      135.9376      136.1244  0.1153\n",
      "     26      141.0294      141.2169  0.1155\n",
      "     27      136.0657      136.0651  0.1154\n",
      "     27      161.5926      159.1246  0.1149\n",
      "     27      135.9377      136.1244  0.1128\n",
      "     28      135.9754      136.0674  0.1121\n",
      "     27      \u001b[36m141.0244\u001b[0m      141.2169  0.1137\n",
      "     28      161.5926      159.1246  0.1123\n",
      "     28      135.9377      136.1244  0.1144\n",
      "     28      141.0295      141.2169  0.1143\n",
      "     29      135.9650      136.1244  0.1152\n",
      "     29      161.5926      159.1246  0.1140\n",
      "     29      \u001b[36m135.9376\u001b[0m      136.1244  0.1170\n",
      "     29      141.0580      141.2169  0.1139\n",
      "     30      \u001b[36m135.9182\u001b[0m      136.1047  0.1162\n",
      "     30      161.5926      159.1246  0.1129\n",
      "     30      135.9376      136.1244  0.1137\n",
      "     30      141.0294      141.2169  0.1128\n",
      "     31      \u001b[36m135.9068\u001b[0m      136.1244  0.1126\n",
      "     31      161.5926      159.1246  0.1144\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 11.\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     31      141.0292      \u001b[32m141.1972\u001b[0m  0.1127\n",
      "     32      135.9862      136.1244  0.1138\n",
      "     32      161.5926      159.1246  0.1105\n",
      "     32      \u001b[36m141.0189\u001b[0m      141.1972  0.1117\n",
      "     33      135.9987      136.1047  0.1126\n",
      "     33      161.5925      159.1246  0.1106\n",
      "     33      141.0560      \u001b[32m141.1972\u001b[0m  0.1119\n",
      "     34      135.9798      136.1244  0.1127\n",
      "     34      161.5926      159.1246  0.1120\n",
      "     34      141.1297      \u001b[32m141.1381\u001b[0m  0.1138\n",
      "     35      \u001b[36m135.8878\u001b[0m      136.1047  0.1130\n",
      "     35      161.5925      159.1246  0.1100\n",
      "     35      \u001b[36m141.0096\u001b[0m      141.1971  0.1133\n",
      "     36      136.0777      136.1048  0.1114\n",
      "     36      161.5925      159.1246  0.1111\n",
      "     36      141.0342      141.1972  0.1119\n",
      "     37      135.9459      136.1047  0.1130\n",
      "     37      161.5927      159.1246  0.1121\n",
      "     37      141.0245      141.1972  0.1130\n",
      "     38      136.0053      136.1244  0.1130\n",
      "     38      161.5925      159.1246  0.1118\n",
      "     38      141.0513      141.1975  0.1114\n",
      "     39      135.9376      136.1244  0.1111\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 19.\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     39      141.0344      141.1990  0.1103\n",
      "     40      135.9326      136.1244  0.1100\n",
      "     40      141.0344      141.2029  0.1111\n",
      "     41      135.9376      136.1244  0.1114\n",
      "     41      141.0344      141.1973  0.1073\n",
      "     42      135.9376      136.1244  0.1090\n",
      "     42      141.0337      141.1972  0.1100\n",
      "     43      135.9612      136.1244  0.1097\n",
      "     43      141.0344      141.1972  0.1112\n",
      "     44      135.9292      136.1244  0.1096\n",
      "     44      141.0687      141.2169  0.1089\n",
      "     45      135.9376      136.1244  0.1092\n",
      "     45      141.0344      141.2169  0.1108\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 26.\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     46      141.0344      141.2169  0.1081\n",
      "     47      141.0344      141.2169  0.1091\n",
      "     48      141.0349      141.2169  0.1067\n",
      "     49      141.0344      141.2169  0.1086\n",
      "     50      141.0344      141.2169  0.1070\n",
      "Restoring best model from epoch 34.\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m164.1428\u001b[0m      \u001b[32m153.6509\u001b[0m  0.1818\n",
      "      2      \u001b[36m151.3374\u001b[0m      \u001b[32m151.8045\u001b[0m  0.1522\n",
      "      3      \u001b[36m149.8080\u001b[0m      \u001b[32m150.0576\u001b[0m  0.1519\n",
      "      4      \u001b[36m148.8070\u001b[0m      \u001b[32m149.9410\u001b[0m  0.1531\n",
      "      5      \u001b[36m148.1803\u001b[0m      149.9882  0.1435\n",
      "      6      148.2720      150.1603  0.1615\n",
      "      7      148.4267      \u001b[32m149.8674\u001b[0m  0.1484\n",
      "      8      148.4212      \u001b[32m149.6000\u001b[0m  0.1492\n",
      "      9      \u001b[36m148.0493\u001b[0m      \u001b[32m149.4532\u001b[0m  0.1501\n",
      "     10      \u001b[36m147.9239\u001b[0m      \u001b[32m149.4377\u001b[0m  0.1556\n",
      "     11      147.9629      149.7920  0.1588\n",
      "     12      147.9622      149.5433  0.1511\n",
      "     13      \u001b[36m147.8796\u001b[0m      149.5417  0.1504\n",
      "     14      \u001b[36m147.8666\u001b[0m      149.4532  0.1472\n",
      "     15      \u001b[36m147.8519\u001b[0m      149.4532  0.1570\n",
      "     16      147.8520      149.4532  0.1503\n",
      "     17      \u001b[36m147.8516\u001b[0m      149.4532  0.1507\n",
      "     18      147.8516      149.4532  0.1593\n",
      "     19      147.8517      149.4532  0.1504\n",
      "     20      147.8516      149.4532  0.1520\n",
      "     21      147.8516      149.4532  0.1462\n",
      "     22      147.8520      149.4532  0.1456\n",
      "     23      \u001b[36m147.8516\u001b[0m      149.4532  0.1539\n",
      "     24      \u001b[36m147.8516\u001b[0m      149.4532  0.1477\n",
      "     25      147.8516      149.4532  0.1545\n",
      "     26      147.8516      149.4532  0.1471\n",
      "     27      \u001b[36m147.8514\u001b[0m      149.4532  0.1475\n",
      "     28      147.8516      149.4532  0.1470\n",
      "     29      147.8516      149.4532  0.1436\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 10.\n",
      "durations 3.0 2024.0\n",
      "dtype X_train age                  float32\n",
      "sex                 category\n",
      "race                 float32\n",
      "n_comorbidities      float32\n",
      "diabetes            category\n",
      "dementia            category\n",
      "blood_pressure       float32\n",
      "heart_rate           float32\n",
      "respiration_rate     float32\n",
      "temperature          float32\n",
      "white_blood_cell     float32\n",
      "serum_sodium         float32\n",
      "serum_creatinine     float32\n",
      "cancer_0.0             uint8\n",
      "cancer_1.0             uint8\n",
      "cancer_2.0             uint8\n",
      "dtype: object\n",
      "dtype y_train float64\n",
      "dtype X_test age                  float32\n",
      "sex                 category\n",
      "race                 float32\n",
      "n_comorbidities      float32\n",
      "diabetes            category\n",
      "dementia            category\n",
      "blood_pressure       float32\n",
      "heart_rate           float32\n",
      "respiration_rate     float32\n",
      "temperature          float32\n",
      "white_blood_cell     float32\n",
      "serum_sodium         float32\n",
      "serum_creatinine     float32\n",
      "cancer_0.0             uint8\n",
      "cancer_1.0             uint8\n",
      "cancer_2.0             uint8\n",
      "dtype: object\n",
      "dtype y_test float32\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m753.2039\u001b[0m      \u001b[32m692.5614\u001b[0m  0.0668\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m755.2593\u001b[0m      \u001b[32m699.6686\u001b[0m  0.0640\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m935.0362\u001b[0m      \u001b[32m841.5062\u001b[0m  0.0663\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m789.9379\u001b[0m      \u001b[32m729.7421\u001b[0m  0.0861\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m975.0938\u001b[0m      \u001b[32m867.0303\u001b[0m  0.0653\n",
      "      2      \u001b[36m659.8097\u001b[0m      \u001b[32m582.8135\u001b[0m  0.0671\n",
      "      2      \u001b[36m666.8303\u001b[0m      \u001b[32m586.7865\u001b[0m  0.0782\n",
      "      2      \u001b[36m827.5639\u001b[0m      \u001b[32m713.4568\u001b[0m  0.0636\n",
      "      2      \u001b[36m697.7647\u001b[0m      \u001b[32m613.5551\u001b[0m  0.0530\n",
      "      2      \u001b[36m864.7066\u001b[0m      \u001b[32m741.6440\u001b[0m  0.0687\n",
      "      3      \u001b[36m594.6520\u001b[0m      \u001b[32m542.9276\u001b[0m  0.0652\n",
      "      3      \u001b[36m594.0654\u001b[0m      \u001b[32m543.0655\u001b[0m  0.0545\n",
      "      3      \u001b[36m731.7020\u001b[0m      \u001b[32m642.7981\u001b[0m  0.0667\n",
      "      3      \u001b[36m618.0547\u001b[0m      \u001b[32m564.4613\u001b[0m  0.0681\n",
      "      3      \u001b[36m767.4256\u001b[0m      \u001b[32m667.2691\u001b[0m  0.0638\n",
      "      4      \u001b[36m567.9087\u001b[0m      \u001b[32m526.8172\u001b[0m  0.0552\n",
      "      4      \u001b[36m565.0583\u001b[0m      \u001b[32m524.1769\u001b[0m  0.0682\n",
      "      4      \u001b[36m683.0746\u001b[0m      \u001b[32m614.5461\u001b[0m  0.0582\n",
      "      4      \u001b[36m584.8735\u001b[0m      \u001b[32m543.9909\u001b[0m  0.0561\n",
      "      4      \u001b[36m710.9109\u001b[0m      \u001b[32m629.1044\u001b[0m  0.0618\n",
      "      5      \u001b[36m556.5831\u001b[0m      \u001b[32m516.1664\u001b[0m  0.0531\n",
      "      5      \u001b[36m554.1027\u001b[0m      \u001b[32m516.3094\u001b[0m  0.0638\n",
      "      5      \u001b[36m661.6675\u001b[0m      \u001b[32m605.8278\u001b[0m  0.0610\n",
      "      5      \u001b[36m683.2725\u001b[0m      \u001b[32m621.8807\u001b[0m  0.0540\n",
      "      5      \u001b[36m573.5339\u001b[0m      \u001b[32m535.8872\u001b[0m  0.0853\n",
      "      6      \u001b[36m549.7569\u001b[0m      \u001b[32m513.5629\u001b[0m  0.0729\n",
      "      6      \u001b[36m547.2413\u001b[0m      \u001b[32m513.7745\u001b[0m  0.0542\n",
      "      6      \u001b[36m651.0530\u001b[0m      \u001b[32m603.5458\u001b[0m  0.0725\n",
      "      6      \u001b[36m672.5380\u001b[0m      \u001b[32m621.8651\u001b[0m  0.0641\n",
      "      6      \u001b[36m566.8409\u001b[0m      \u001b[32m533.1956\u001b[0m  0.0569\n",
      "      7      \u001b[36m546.0997\u001b[0m      \u001b[32m512.8985\u001b[0m  0.0777\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m148.7244\u001b[0m      \u001b[32m140.3089\u001b[0m  0.4945\n",
      "      7      \u001b[36m545.6120\u001b[0m      \u001b[32m513.4865\u001b[0m  0.0802\n",
      "      7      \u001b[36m669.2540\u001b[0m      621.8683  0.0578\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m147.7969\u001b[0m      \u001b[32m139.4620\u001b[0m  0.5046\n",
      "      7      \u001b[36m647.7760\u001b[0m      \u001b[32m603.3460\u001b[0m  0.0736\n",
      "      7      \u001b[36m564.1542\u001b[0m      \u001b[32m532.6158\u001b[0m  0.0659\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m154.8008\u001b[0m      \u001b[32m144.8979\u001b[0m  0.5402\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m180.5094\u001b[0m      \u001b[32m166.8624\u001b[0m  0.5341\n",
      "      8      \u001b[36m544.1715\u001b[0m      \u001b[32m513.4290\u001b[0m  0.0639\n",
      "      8      \u001b[36m668.9398\u001b[0m      621.8698  0.0611\n",
      "      8      \u001b[36m544.0804\u001b[0m      513.4082  0.0808\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m188.3602\u001b[0m      \u001b[32m171.2319\u001b[0m  0.5520\n",
      "      8      \u001b[36m563.7111\u001b[0m      532.6584  0.0696\n",
      "      8      \u001b[36m646.8314\u001b[0m      603.3504  0.0748\n",
      "      9      \u001b[36m668.4158\u001b[0m      621.8701  0.0610\n",
      "      9      \u001b[36m543.4214\u001b[0m      513.6884  0.0636\n",
      "      9      \u001b[36m543.3955\u001b[0m      \u001b[32m513.3706\u001b[0m  0.0709\n",
      "      9      \u001b[36m563.3377\u001b[0m      532.6838  0.0546\n",
      "      9      \u001b[36m646.2263\u001b[0m      603.3514  0.0589\n",
      "     10      \u001b[36m543.0630\u001b[0m      513.4595  0.0577\n",
      "     10      \u001b[36m668.3866\u001b[0m      621.8717  0.0702\n",
      "     10      543.5827      513.7437  0.0641\n",
      "     10      \u001b[36m563.0577\u001b[0m      532.7012  0.0669\n",
      "     10      \u001b[36m645.9003\u001b[0m      603.3541  0.0732\n",
      "     11      543.4016      513.4975  0.0525\n",
      "     11      \u001b[36m668.1899\u001b[0m      621.8743  0.0661\n",
      "     11      544.1639      513.6419  0.0651\n",
      "     11      563.5136      532.7289  0.0578\n",
      "     11      \u001b[36m645.8557\u001b[0m      603.3565  0.0681\n",
      "     12      \u001b[36m542.8279\u001b[0m      513.5172  0.0683\n",
      "     12      \u001b[36m563.0558\u001b[0m      532.7432  0.0585\n",
      "     12      \u001b[36m668.1882\u001b[0m      621.8792  0.0637\n",
      "     12      \u001b[36m543.2329\u001b[0m      513.6788  0.0645\n",
      "     12      \u001b[36m645.7521\u001b[0m      603.3600  0.0656\n",
      "     13      542.8380      513.5216  0.0609\n",
      "     13      \u001b[36m543.0281\u001b[0m      513.7574  0.0586\n",
      "     13      \u001b[36m668.1654\u001b[0m      621.8837  0.0635\n",
      "     13      \u001b[36m562.7908\u001b[0m      532.7684  0.0663\n",
      "     13      645.7826      603.3627  0.0577\n",
      "     14      542.9041      513.5255  0.0517\n",
      "     14      \u001b[36m542.5213\u001b[0m      514.0556  0.0616\n",
      "     14      668.4123      621.8891  0.0628\n",
      "     14      562.8316      532.7876  0.0710\n",
      "     14      \u001b[36m645.5570\u001b[0m      603.3668  0.0655\n",
      "     15      542.9892      513.5073  0.0605\n",
      "     15      542.9507      514.1472  0.0580\n",
      "     15      668.2057      621.8945  0.0646\n",
      "      2      \u001b[36m137.4303\u001b[0m      \u001b[32m137.6479\u001b[0m  0.5236\n",
      "     15      \u001b[36m562.7274\u001b[0m      532.8097  0.0804\n",
      "      2      \u001b[36m137.9840\u001b[0m      \u001b[32m138.5608\u001b[0m  0.5457\n",
      "      2      \u001b[36m142.5916\u001b[0m      \u001b[32m143.4664\u001b[0m  0.4974\n",
      "     16      543.0695      513.5008  0.0628\n",
      "     15      645.5805      603.3711  0.0804\n",
      "      2      \u001b[36m165.0564\u001b[0m      \u001b[32m162.1941\u001b[0m  0.5153\n",
      "     16      542.6942      513.8496  0.0666\n",
      "     16      668.1966      621.9009  0.0600\n",
      "     16      562.9018      532.8223  0.0565\n",
      "      2      \u001b[36m171.7657\u001b[0m      \u001b[32m167.7143\u001b[0m  0.5372\n",
      "     17      543.0327      513.5068  0.0750\n",
      "     16      645.6332      603.3778  0.0748\n",
      "     17      542.5463      513.4829  0.0776\n",
      "     17      668.2279      621.9077  0.0797\n",
      "     17      562.7453      532.8296  0.0798\n",
      "     18      542.9326      513.5106  0.0600\n",
      "     17      645.6676      603.3849  0.0885\n",
      "     18      668.2500      621.9148  0.0674\n",
      "     18      \u001b[36m542.4758\u001b[0m      513.3142  0.0796\n",
      "     19      542.9859      513.5175  0.0671\n",
      "     18      562.7535      532.8367  0.0838\n",
      "     19      542.8349      513.3360  0.0613\n",
      "     19      668.2485      621.9222  0.0789\n",
      "     18      645.6638      603.3904  0.0969\n",
      "     20      542.9477      513.5170  0.0779\n",
      "     19      562.8049      532.8419  0.0737\n",
      "     20      668.2973      621.9311  0.0728\n",
      "     20      543.2367      513.3404  0.1012\n",
      "     19      645.6508      603.3984  0.0845\n",
      "     21      542.8777      513.5215  0.0837\n",
      "     20      563.2831      532.8543  0.0871\n",
      "     21      668.2796      621.9385  0.0780\n",
      "     21      542.6710      513.2239  0.0678\n",
      "     20      645.6593      603.4068  0.0732\n",
      "     21      562.7916      532.8593  0.0634\n",
      "     22      543.0019      513.5260  0.0808\n",
      "     22      668.2448      621.9425  0.0646\n",
      "     21      645.7294      603.4115  0.0571\n",
      "     22      \u001b[36m542.0351\u001b[0m      513.2111  0.0771\n",
      "     23      542.9898      513.5254  0.0588\n",
      "      3      \u001b[36m136.1201\u001b[0m      \u001b[32m137.0165\u001b[0m  0.5383\n",
      "     22      562.8911      532.8624  0.0783\n",
      "     23      668.2934      621.9461  0.0691\n",
      "     22      645.6976      603.4214  0.0606\n",
      "     23      542.8452      513.2953  0.0673\n",
      "     24      542.9620      513.5289  0.0531\n",
      "      3      \u001b[36m136.6445\u001b[0m      \u001b[32m137.5770\u001b[0m  0.5727\n",
      "      3      \u001b[36m141.2176\u001b[0m      \u001b[32m142.5308\u001b[0m  0.5711\n",
      "     23      562.8715      532.8650  0.0620\n",
      "      3      \u001b[36m162.1800\u001b[0m      \u001b[32m161.2225\u001b[0m  0.5627\n",
      "     24      668.2588      621.9490  0.0590\n",
      "     23      645.7105      603.4282  0.0678\n",
      "      3      \u001b[36m168.5189\u001b[0m      \u001b[32m166.6049\u001b[0m  0.5565\n",
      "     24      542.8068      513.3725  0.0625\n",
      "     24      562.9290      532.8693  0.0563\n",
      "     25      542.9929      513.5298  0.0803\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 5.\n",
      "     25      542.5444      513.2971  0.0560\n",
      "     24      645.6331      603.4339  0.0642\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     25      562.7701      532.8725  0.0626\n",
      "     26      542.9384      513.5391  0.0630\n",
      "     26      542.9850      513.1749  0.0626\n",
      "     25      645.6691      603.4362  0.0710\n",
      "     27      542.9537      513.5621  0.0531\n",
      "     26      562.8612      532.8753  0.0615\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 7.\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     26      645.6890      603.4419  0.0614\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 7.\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     28      542.9749      513.5759  0.0668\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 7.\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 9.\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "      4      \u001b[36m135.8679\u001b[0m      \u001b[32m136.8982\u001b[0m  0.4794\n",
      "      4      \u001b[36m140.8440\u001b[0m      142.7433  0.4555\n",
      "      4      \u001b[36m161.1289\u001b[0m      \u001b[32m160.8059\u001b[0m  0.4580\n",
      "      4      \u001b[36m167.4414\u001b[0m      \u001b[32m166.5147\u001b[0m  0.4583\n",
      "      4      \u001b[36m136.2549\u001b[0m      \u001b[32m137.2237\u001b[0m  0.5312\n",
      "      5      \u001b[36m135.6824\u001b[0m      137.1218  0.4412\n",
      "      5      141.1750      143.0126  0.4430\n",
      "      5      \u001b[36m161.0563\u001b[0m      160.8070  0.4399\n",
      "      5      \u001b[36m167.2390\u001b[0m      \u001b[32m166.2222\u001b[0m  0.4392\n",
      "      5      \u001b[36m136.0610\u001b[0m      \u001b[32m137.0735\u001b[0m  0.4430\n",
      "      6      135.7629      137.1715  0.4365\n",
      "      6      \u001b[36m140.7462\u001b[0m      \u001b[32m142.4223\u001b[0m  0.4368\n",
      "      6      161.0579      160.9222  0.4361\n",
      "      6      \u001b[36m167.1460\u001b[0m      166.4178  0.4345\n",
      "      6      \u001b[36m135.5318\u001b[0m      137.1581  0.4311\n",
      "      7      \u001b[36m135.5623\u001b[0m      137.2866  0.4375\n",
      "      7      \u001b[36m140.5203\u001b[0m      \u001b[32m142.3525\u001b[0m  0.4379\n",
      "      7      161.0611      160.8111  0.4374\n",
      "      7      167.1481      166.4204  0.4398\n",
      "      7      \u001b[36m135.5060\u001b[0m      137.1620  0.4392\n",
      "      8      \u001b[36m135.5408\u001b[0m      137.2950  0.4371\n",
      "      8      140.5341      142.3537  0.4383\n",
      "      8      161.0620      160.9278  0.4368\n",
      "      8      167.1497      166.4232  0.4359\n",
      "      8      135.5082      137.1629  0.4373\n",
      "      9      \u001b[36m135.4868\u001b[0m      137.2498  0.4327\n",
      "      9      140.5350      142.3560  0.4302\n",
      "      9      161.0620      160.9280  0.4319\n",
      "      9      167.1498      166.4153  0.4318\n",
      "      9      135.5086      137.1640  0.4325\n",
      "     10      \u001b[36m135.4672\u001b[0m      137.1143  0.4323\n",
      "     10      140.5349      142.3566  0.4332\n",
      "     10      161.0615      160.8454  0.4353\n",
      "     10      167.1502      166.3075  0.4338\n",
      "     10      135.5076      137.1647  0.4370\n",
      "     11      135.5008      137.1698  0.4365\n",
      "     11      140.5348      142.3570  0.4349\n",
      "     11      161.0605      160.8772  0.4366\n",
      "     11      167.1477      166.2960  0.4356\n",
      "     11      135.5072      137.1644  0.4325\n",
      "     12      135.5030      137.1628  0.4367\n",
      "     12      140.5283      \u001b[32m142.3521\u001b[0m  0.4370\n",
      "     12      161.0598      160.8186  0.4336\n",
      "     12      167.1474      166.2975  0.4359\n",
      "     12      135.5061      137.1639  0.4350\n",
      "     13      135.5039      137.1636  0.4314\n",
      "     13      140.5214      \u001b[32m142.3126\u001b[0m  0.4313\n",
      "     13      161.0590      160.8169  0.4315\n",
      "     13      167.1464      166.2943  0.4306\n",
      "     13      \u001b[36m135.5053\u001b[0m      137.1634  0.4323\n",
      "     14      135.5061      137.1625  0.4334\n",
      "     14      140.5360      142.3491  0.4347\n",
      "     14      161.0579      160.8165  0.4345\n",
      "     14      \u001b[36m167.1455\u001b[0m      \u001b[32m166.1750\u001b[0m  0.4345\n",
      "     14      \u001b[36m135.5047\u001b[0m      137.1640  0.4327\n",
      "     15      135.5031      137.1649  0.4313\n",
      "     15      140.5316      142.3550  0.4306\n",
      "     15      161.0572      160.8155  0.4305\n",
      "     15      \u001b[36m167.1447\u001b[0m      \u001b[32m166.1092\u001b[0m  0.4322\n",
      "     15      \u001b[36m135.5039\u001b[0m      137.1629  0.4339\n",
      "     16      135.5028      137.1637  0.4321\n",
      "     16      140.5306      142.3568  0.4291\n",
      "     16      161.0566      160.8142  0.4285\n",
      "     16      \u001b[36m167.1440\u001b[0m      \u001b[32m165.9160\u001b[0m  0.4328\n",
      "     16      \u001b[36m135.5031\u001b[0m      137.1621  0.4317\n",
      "     17      135.5037      137.1639  0.4293\n",
      "     17      140.5417      142.3630  0.4275\n",
      "     17      \u001b[36m161.0558\u001b[0m      160.8116  0.4276\n",
      "     17      \u001b[36m167.1433\u001b[0m      \u001b[32m165.8619\u001b[0m  0.4304\n",
      "     17      \u001b[36m135.5025\u001b[0m      137.1619  0.4297\n",
      "     18      135.5038      137.1624  0.4319\n",
      "     18      140.5268      142.3544  0.4304\n",
      "     18      \u001b[36m161.0553\u001b[0m      \u001b[32m160.8039\u001b[0m  0.4287\n",
      "     18      \u001b[36m167.1428\u001b[0m      \u001b[32m165.8611\u001b[0m  0.4288\n",
      "     18      \u001b[36m135.5019\u001b[0m      137.1616  0.4289\n",
      "     19      135.4964      137.1636  0.4303\n",
      "     19      140.5306      142.3556  0.4274\n",
      "     19      \u001b[36m161.0548\u001b[0m      \u001b[32m160.7241\u001b[0m  0.4284\n",
      "     19      \u001b[36m167.1423\u001b[0m      \u001b[32m165.8605\u001b[0m  0.4295\n",
      "     19      \u001b[36m135.5017\u001b[0m      137.1622  0.4303\n",
      "     20      135.6880      137.2420  0.4313\n",
      "     20      140.5300      142.3550  0.4310\n",
      "     20      \u001b[36m161.0542\u001b[0m      \u001b[32m160.7021\u001b[0m  0.4318\n",
      "     20      \u001b[36m167.1418\u001b[0m      \u001b[32m165.8602\u001b[0m  0.4335\n",
      "     20      \u001b[36m135.5010\u001b[0m      137.1610  0.4316\n",
      "     21      135.4935      137.1539  0.4312\n",
      "     21      140.5297      142.3547  0.4300\n",
      "     21      \u001b[36m161.0538\u001b[0m      \u001b[32m160.7009\u001b[0m  0.4293\n",
      "     21      \u001b[36m167.1414\u001b[0m      \u001b[32m165.8597\u001b[0m  0.4282\n",
      "     21      \u001b[36m135.5005\u001b[0m      137.1604  0.4306\n",
      "     22      135.4968      137.1612  0.4314\n",
      "     22      140.5293      142.3544  0.4326\n",
      "     22      \u001b[36m161.0534\u001b[0m      \u001b[32m160.7008\u001b[0m  0.4328\n",
      "     22      \u001b[36m167.1410\u001b[0m      \u001b[32m165.8594\u001b[0m  0.4326\n",
      "     22      \u001b[36m135.5002\u001b[0m      137.1600  0.4342\n",
      "     23      135.5027      137.1615  0.4336\n",
      "     23      140.5289      142.3541  0.4321\n",
      "     23      \u001b[36m161.0530\u001b[0m      \u001b[32m160.7002\u001b[0m  0.4329\n",
      "     23      \u001b[36m167.1407\u001b[0m      \u001b[32m165.8591\u001b[0m  0.4324\n",
      "     23      \u001b[36m135.5000\u001b[0m      137.1604  0.4335\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 4.\n",
      "     24      140.5285      142.3539  0.4306\n",
      "     24      \u001b[36m161.0526\u001b[0m      \u001b[32m160.6999\u001b[0m  0.4301\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     24      \u001b[36m167.1404\u001b[0m      \u001b[32m165.8589\u001b[0m  0.4309\n",
      "     24      \u001b[36m135.4995\u001b[0m      137.1597  0.4301\n",
      "     25      140.5283      142.3536  0.4310\n",
      "     25      \u001b[36m161.0523\u001b[0m      160.6999  0.4319\n",
      "     25      \u001b[36m167.1401\u001b[0m      \u001b[32m165.8586\u001b[0m  0.4309\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 5.\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     26      140.5280      142.3534  0.4267\n",
      "     26      \u001b[36m161.0520\u001b[0m      \u001b[32m160.6996\u001b[0m  0.4267\n",
      "     26      \u001b[36m167.1398\u001b[0m      \u001b[32m165.8583\u001b[0m  0.4296\n",
      "     27      140.5277      142.3531  0.4289\n",
      "     27      \u001b[36m161.0517\u001b[0m      \u001b[32m160.6993\u001b[0m  0.4280\n",
      "     27      \u001b[36m167.1396\u001b[0m      \u001b[32m165.8582\u001b[0m  0.4268\n",
      "     28      140.5275      142.3527  0.4272\n",
      "     28      \u001b[36m161.0515\u001b[0m      \u001b[32m160.6989\u001b[0m  0.4290\n",
      "     28      \u001b[36m167.1394\u001b[0m      \u001b[32m165.8580\u001b[0m  0.4264\n",
      "     29      140.5272      142.3526  0.4289\n",
      "     29      \u001b[36m161.0512\u001b[0m      \u001b[32m160.6988\u001b[0m  0.4262\n",
      "     29      \u001b[36m167.1392\u001b[0m      \u001b[32m165.8579\u001b[0m  0.4288\n",
      "     30      140.5270      142.3524  0.4263\n",
      "     30      \u001b[36m161.0510\u001b[0m      \u001b[32m160.6984\u001b[0m  0.4261\n",
      "     30      \u001b[36m167.1390\u001b[0m      \u001b[32m165.8577\u001b[0m  0.4251\n",
      "     31      140.5268      142.3522  0.4261\n",
      "     31      \u001b[36m161.0508\u001b[0m      \u001b[32m160.6984\u001b[0m  0.4262\n",
      "     31      \u001b[36m167.1389\u001b[0m      \u001b[32m165.8576\u001b[0m  0.4252\n",
      "     32      140.5266      142.3519  0.4269\n",
      "     32      \u001b[36m161.0506\u001b[0m      \u001b[32m160.6982\u001b[0m  0.4246\n",
      "     32      \u001b[36m167.1387\u001b[0m      165.8576  0.4255\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 13.\n",
      "     33      \u001b[36m161.0504\u001b[0m      \u001b[32m160.6981\u001b[0m  0.4281\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     33      \u001b[36m167.1386\u001b[0m      165.8576  0.4273\n",
      "     34      \u001b[36m161.0502\u001b[0m      \u001b[32m160.6981\u001b[0m  0.4164\n",
      "     34      \u001b[36m167.1384\u001b[0m      \u001b[32m165.8571\u001b[0m  0.4139\n",
      "     35      \u001b[36m161.0501\u001b[0m      \u001b[32m160.6977\u001b[0m  0.4148\n",
      "     35      \u001b[36m167.1383\u001b[0m      \u001b[32m165.8571\u001b[0m  0.4116\n",
      "     36      \u001b[36m161.0499\u001b[0m      \u001b[32m160.6975\u001b[0m  0.4188\n",
      "     36      \u001b[36m167.1382\u001b[0m      \u001b[32m165.8570\u001b[0m  0.4243\n",
      "     37      \u001b[36m161.0498\u001b[0m      \u001b[32m160.6974\u001b[0m  0.4163\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 17.\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     38      \u001b[36m161.0497\u001b[0m      160.6975  0.4099\n",
      "     39      \u001b[36m161.0495\u001b[0m      \u001b[32m160.6972\u001b[0m  0.4122\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 20.\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m160.9670\u001b[0m      \u001b[32m149.6308\u001b[0m  0.3612\n",
      "      2      \u001b[36m149.9772\u001b[0m      \u001b[32m148.6508\u001b[0m  0.3621\n",
      "      3      \u001b[36m148.6008\u001b[0m      \u001b[32m148.5707\u001b[0m  0.4871\n",
      "      4      \u001b[36m148.2788\u001b[0m      \u001b[32m148.4759\u001b[0m  0.3721\n",
      "      5      \u001b[36m148.2464\u001b[0m      148.4797  0.3570\n",
      "      6      148.2701      148.4812  0.3530\n",
      "      7      148.2491      148.4817  0.3502\n",
      "      8      \u001b[36m148.2450\u001b[0m      148.4816  0.3719\n",
      "      9      \u001b[36m148.2447\u001b[0m      \u001b[32m148.3957\u001b[0m  0.3703\n",
      "     10      148.2460      \u001b[32m148.3957\u001b[0m  0.3512\n",
      "     11      \u001b[36m148.2446\u001b[0m      \u001b[32m148.3950\u001b[0m  0.3629\n",
      "     12      \u001b[36m148.2437\u001b[0m      \u001b[32m148.3940\u001b[0m  0.3683\n",
      "     13      \u001b[36m148.2427\u001b[0m      \u001b[32m148.3934\u001b[0m  0.3530\n",
      "     14      \u001b[36m148.2419\u001b[0m      \u001b[32m148.3931\u001b[0m  0.3661\n",
      "     15      \u001b[36m148.2411\u001b[0m      \u001b[32m148.3923\u001b[0m  0.3699\n",
      "     16      \u001b[36m148.2406\u001b[0m      \u001b[32m148.3917\u001b[0m  0.3420\n",
      "     17      \u001b[36m148.2400\u001b[0m      \u001b[32m148.3912\u001b[0m  0.3582\n",
      "     18      \u001b[36m148.2395\u001b[0m      \u001b[32m148.3908\u001b[0m  0.3653\n",
      "     19      \u001b[36m148.2391\u001b[0m      \u001b[32m148.3904\u001b[0m  0.3727\n",
      "     20      \u001b[36m148.2387\u001b[0m      \u001b[32m148.3903\u001b[0m  0.3776\n",
      "     21      \u001b[36m148.2384\u001b[0m      \u001b[32m148.3898\u001b[0m  0.3598\n",
      "     22      \u001b[36m148.2381\u001b[0m      \u001b[32m148.3895\u001b[0m  0.3641\n",
      "     23      \u001b[36m148.2379\u001b[0m      \u001b[32m148.3893\u001b[0m  0.3690\n",
      "     24      \u001b[36m148.2376\u001b[0m      \u001b[32m148.3892\u001b[0m  0.3774\n",
      "     25      \u001b[36m148.2374\u001b[0m      \u001b[32m148.3889\u001b[0m  0.3570\n",
      "     26      \u001b[36m148.2372\u001b[0m      \u001b[32m148.3886\u001b[0m  0.3569\n",
      "     27      \u001b[36m148.2371\u001b[0m      \u001b[32m148.3884\u001b[0m  0.3547\n",
      "     28      \u001b[36m148.2369\u001b[0m      148.3886  0.3623\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 9.\n",
      "durations 3.0 2026.0\n",
      "dtype X_train age                  float32\n",
      "sex                 category\n",
      "race                 float32\n",
      "n_comorbidities      float32\n",
      "diabetes            category\n",
      "dementia            category\n",
      "blood_pressure       float32\n",
      "heart_rate           float32\n",
      "respiration_rate     float32\n",
      "temperature          float32\n",
      "white_blood_cell     float32\n",
      "serum_sodium         float32\n",
      "serum_creatinine     float32\n",
      "cancer_0.0             uint8\n",
      "cancer_1.0             uint8\n",
      "cancer_2.0             uint8\n",
      "dtype: object\n",
      "dtype y_train float64\n",
      "dtype X_test age                  float32\n",
      "sex                 category\n",
      "race                 float32\n",
      "n_comorbidities      float32\n",
      "diabetes            category\n",
      "dementia            category\n",
      "blood_pressure       float32\n",
      "heart_rate           float32\n",
      "respiration_rate     float32\n",
      "temperature          float32\n",
      "white_blood_cell     float32\n",
      "serum_sodium         float32\n",
      "serum_creatinine     float32\n",
      "cancer_0.0             uint8\n",
      "cancer_1.0             uint8\n",
      "cancer_2.0             uint8\n",
      "dtype: object\n",
      "dtype y_test float32\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m757.0509\u001b[0m      \u001b[32m701.1893\u001b[0m  0.0566\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m758.7554\u001b[0m      \u001b[32m700.6528\u001b[0m  0.0622\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m933.3286\u001b[0m      \u001b[32m835.4859\u001b[0m  0.0569\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m788.5569\u001b[0m      \u001b[32m720.6309\u001b[0m  0.0743\n",
      "      2      \u001b[36m668.7028\u001b[0m      \u001b[32m584.8542\u001b[0m  0.0526\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m978.0901\u001b[0m      \u001b[32m865.0229\u001b[0m  0.0709\n",
      "      2      \u001b[36m826.9199\u001b[0m      \u001b[32m707.9031\u001b[0m  0.0524\n",
      "      2      \u001b[36m673.7267\u001b[0m      \u001b[32m594.3578\u001b[0m  0.0711\n",
      "      2      \u001b[36m701.2815\u001b[0m      \u001b[32m607.5810\u001b[0m  0.0574\n",
      "      2      \u001b[36m869.9417\u001b[0m      \u001b[32m735.9984\u001b[0m  0.0590\n",
      "      3      \u001b[36m597.0068\u001b[0m      \u001b[32m537.8180\u001b[0m  0.0761\n",
      "      3      \u001b[36m737.9079\u001b[0m      \u001b[32m645.8281\u001b[0m  0.0669\n",
      "      3      \u001b[36m606.6524\u001b[0m      \u001b[32m544.0280\u001b[0m  0.0810\n",
      "      3      \u001b[36m627.5081\u001b[0m      \u001b[32m559.5797\u001b[0m  0.0803\n",
      "      3      \u001b[36m774.5080\u001b[0m      \u001b[32m674.9629\u001b[0m  0.0679\n",
      "      4      \u001b[36m567.3316\u001b[0m      \u001b[32m523.3020\u001b[0m  0.0641\n",
      "      4      \u001b[36m691.2378\u001b[0m      \u001b[32m620.7974\u001b[0m  0.0641\n",
      "      4      \u001b[36m574.7905\u001b[0m      \u001b[32m527.0996\u001b[0m  0.0604\n",
      "      4      \u001b[36m592.8058\u001b[0m      \u001b[32m542.9803\u001b[0m  0.0669\n",
      "      5      \u001b[36m555.2769\u001b[0m      \u001b[32m517.7921\u001b[0m  0.0539\n",
      "      4      \u001b[36m721.9386\u001b[0m      \u001b[32m641.3378\u001b[0m  0.0717\n",
      "      5      \u001b[36m664.0298\u001b[0m      \u001b[32m607.8496\u001b[0m  0.0605\n",
      "      5      \u001b[36m560.3278\u001b[0m      \u001b[32m519.7702\u001b[0m  0.0549\n",
      "      6      \u001b[36m550.1056\u001b[0m      \u001b[32m516.8731\u001b[0m  0.0616\n",
      "      5      \u001b[36m692.7987\u001b[0m      \u001b[32m626.7284\u001b[0m  0.0587\n",
      "      5      \u001b[36m576.6366\u001b[0m      \u001b[32m537.1183\u001b[0m  0.0810\n",
      "      6      \u001b[36m650.9082\u001b[0m      \u001b[32m605.1014\u001b[0m  0.0641\n",
      "      6      \u001b[36m551.0373\u001b[0m      \u001b[32m517.1625\u001b[0m  0.0603\n",
      "      7      \u001b[36m546.5566\u001b[0m      \u001b[32m516.0929\u001b[0m  0.0611\n",
      "      6      \u001b[36m677.4796\u001b[0m      \u001b[32m624.6988\u001b[0m  0.0679\n",
      "      6      \u001b[36m568.9210\u001b[0m      \u001b[32m533.3076\u001b[0m  0.0712\n",
      "      7      \u001b[36m646.9500\u001b[0m      \u001b[32m605.0861\u001b[0m  0.0630\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m148.3066\u001b[0m      \u001b[32m139.5129\u001b[0m  0.5075\n",
      "      7      \u001b[36m548.8502\u001b[0m      \u001b[32m516.5149\u001b[0m  0.0661\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m186.4442\u001b[0m      \u001b[32m170.9863\u001b[0m  0.4931\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m148.6718\u001b[0m      \u001b[32m140.2145\u001b[0m  0.5235\n",
      "      8      \u001b[36m546.1945\u001b[0m      \u001b[32m515.9818\u001b[0m  0.0530\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m178.6056\u001b[0m      \u001b[32m165.3888\u001b[0m  0.5155\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m154.3518\u001b[0m      \u001b[32m143.5741\u001b[0m  0.5375\n",
      "      7      \u001b[36m671.9747\u001b[0m      624.7123  0.0625\n",
      "      8      \u001b[36m645.2552\u001b[0m      605.0878  0.0540\n",
      "      8      \u001b[36m546.4959\u001b[0m      \u001b[32m515.8878\u001b[0m  0.0601\n",
      "      7      \u001b[36m564.8920\u001b[0m      \u001b[32m532.6823\u001b[0m  0.0831\n",
      "      9      546.6075      \u001b[32m515.8539\u001b[0m  0.0518\n",
      "      8      \u001b[36m670.4696\u001b[0m      624.6988  0.0536\n",
      "      9      \u001b[36m645.0033\u001b[0m      605.0891  0.0562\n",
      "     10      \u001b[36m545.9598\u001b[0m      \u001b[32m515.6993\u001b[0m  0.0606\n",
      "      9      \u001b[36m546.0416\u001b[0m      515.9590  0.0709\n",
      "      9      \u001b[36m670.0956\u001b[0m      624.6997  0.0525\n",
      "      8      \u001b[36m563.6209\u001b[0m      532.7333  0.0789\n",
      "     10      \u001b[36m644.9992\u001b[0m      605.0905  0.0554\n",
      "     11      \u001b[36m545.4143\u001b[0m      \u001b[32m515.6780\u001b[0m  0.0552\n",
      "     10      \u001b[36m545.4205\u001b[0m      515.9640  0.0649\n",
      "     10      \u001b[36m669.7253\u001b[0m      624.7046  0.0615\n",
      "      9      \u001b[36m563.1317\u001b[0m      532.7413  0.0646\n",
      "     11      \u001b[36m644.9367\u001b[0m      605.0933  0.0644\n",
      "     12      545.4580      \u001b[32m515.6502\u001b[0m  0.0636\n",
      "     11      545.6529      515.9656  0.0650\n",
      "     10      \u001b[36m562.7712\u001b[0m      532.7436  0.0526\n",
      "     11      669.9994      624.7096  0.0814\n",
      "     12      \u001b[36m644.8877\u001b[0m      605.0974  0.0675\n",
      "     13      545.5572      \u001b[32m515.6324\u001b[0m  0.0627\n",
      "     11      562.8251      532.7452  0.0533\n",
      "     12      \u001b[36m545.2547\u001b[0m      515.9674  0.0622\n",
      "     12      \u001b[36m669.5522\u001b[0m      624.7150  0.0655\n",
      "     13      645.0531      605.1017  0.0601\n",
      "     13      \u001b[36m545.2416\u001b[0m      515.9697  0.0675\n",
      "     14      545.6016      515.6548  0.0863\n",
      "     12      562.9851      532.7476  0.1019\n",
      "     14      645.0008      605.1069  0.0697\n",
      "     13      669.6405      624.7178  0.0727\n",
      "     15      \u001b[36m545.2872\u001b[0m      515.8211  0.0521\n",
      "     15      645.2545      605.1118  0.0516\n",
      "     13      562.8130      532.7497  0.0581\n",
      "     14      669.6088      624.7221  0.0525\n",
      "      2      \u001b[36m137.7205\u001b[0m      \u001b[32m137.8978\u001b[0m  0.4892\n",
      "     14      545.2529      515.9724  0.1173\n",
      "     16      \u001b[36m545.0505\u001b[0m      515.8649  0.0593\n",
      "      2      \u001b[36m137.6167\u001b[0m      \u001b[32m138.0080\u001b[0m  0.5146\n",
      "      2      \u001b[36m172.0955\u001b[0m      \u001b[32m167.8842\u001b[0m  0.5222\n",
      "     16      644.9752      605.1169  0.0611\n",
      "     14      \u001b[36m562.7518\u001b[0m      532.7527  0.0702\n",
      "     15      \u001b[36m669.5344\u001b[0m      624.7271  0.0707\n",
      "      2      \u001b[36m142.1994\u001b[0m      \u001b[32m142.2647\u001b[0m  0.5377\n",
      "     17      545.4757      515.8924  0.0679\n",
      "     15      545.3484      515.9768  0.0721\n",
      "      2      \u001b[36m164.2321\u001b[0m      \u001b[32m161.8956\u001b[0m  0.5621\n",
      "     16      669.5511      624.7323  0.0620\n",
      "     17      644.9881      605.1251  0.0765\n",
      "     15      562.8631      532.7568  0.0774\n",
      "     16      \u001b[36m545.2223\u001b[0m      515.9816  0.0674\n",
      "     18      545.2173      515.9274  0.0858\n",
      "     18      645.0622      605.1333  0.0621\n",
      "     17      669.5624      624.7393  0.0825\n",
      "     16      562.7901      532.7606  0.0778\n",
      "     17      \u001b[36m545.2158\u001b[0m      515.9867  0.0832\n",
      "     19      \u001b[36m544.8865\u001b[0m      515.9529  0.0756\n",
      "     19      645.0446      605.1411  0.0726\n",
      "     18      669.5723      624.7463  0.0687\n",
      "     17      562.7999      532.7656  0.1114\n",
      "     18      545.3042      515.9930  0.1022\n",
      "     20      545.0240      515.9711  0.0955\n",
      "     20      645.0238      605.1495  0.0995\n",
      "     19      669.6814      624.7544  0.1060\n",
      "     18      562.8694      532.7731  0.0765\n",
      "     19      \u001b[36m545.2024\u001b[0m      515.9996  0.0633\n",
      "     21      545.1323      516.0447  0.0649\n",
      "     21      645.0636      605.1563  0.0524\n",
      "     20      669.5988      624.7604  0.0542\n",
      "     22      545.1811      516.1444  0.0586\n",
      "     22      645.0445      605.1608  0.0602\n",
      "     19      562.8009      532.7797  0.0693\n",
      "     20      545.2452      516.0048  0.0677\n",
      "     21      669.6112      624.7667  0.0570\n",
      "     23      645.0812      605.1658  0.0588\n",
      "     20      562.8335      532.7866  0.0642\n",
      "     21      \u001b[36m545.1791\u001b[0m      516.0099  0.0622\n",
      "     23      545.2894      516.0845  0.0675\n",
      "     22      669.6295      624.7707  0.0684\n",
      "      3      \u001b[36m136.7353\u001b[0m      \u001b[32m137.4614\u001b[0m  0.5604\n",
      "     24      645.0784      605.1735  0.0584\n",
      "     21      562.8531      532.7927  0.0548\n",
      "     24      545.0845      516.0318  0.0618\n",
      "     22      545.2731      516.0157  0.0623\n",
      "     23      669.6559      624.7761  0.0618\n",
      "      3      \u001b[36m136.4590\u001b[0m      138.0049  0.6114\n",
      "      3      \u001b[36m169.3926\u001b[0m      \u001b[32m167.1791\u001b[0m  0.5846\n",
      "     25      645.0622      605.1782  0.0612\n",
      "     22      562.9652      532.7989  0.0626\n",
      "      3      \u001b[36m141.2920\u001b[0m      142.2815  0.5780\n",
      "     25      545.2254      516.0215  0.0617\n",
      "     23      545.2478      516.0197  0.0622\n",
      "     24      669.5969      624.7804  0.0610\n",
      "      3      \u001b[36m162.2620\u001b[0m      \u001b[32m161.1780\u001b[0m  0.6002\n",
      "     24      \u001b[36m545.1775\u001b[0m      516.0256  0.0565\n",
      "     23      562.8533      532.8030  0.0647\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 6.\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     26      545.2193      516.0221  0.0860\n",
      "     25      669.6196      624.7795  0.0727\n",
      "     24      562.8688      532.8086  0.0546\n",
      "     25      \u001b[36m545.1659\u001b[0m      516.0295  0.0681\n",
      "     27      545.2329      516.0184  0.0618\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 6.\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     26      545.2597      516.0349  0.0528\n",
      "     25      562.7885      532.8122  0.0665\n",
      "     28      544.9719      516.0351  0.0543\n",
      "     26      562.9428      532.8172  0.0523\n",
      "     27      545.2530      516.0385  0.0553\n",
      "     29      545.3720      516.0019  0.0537\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 7.\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 8.\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     30      545.1165      515.9948  0.0551\n",
      "     31      545.4449      516.0068  0.0538\n",
      "      4      \u001b[36m136.3713\u001b[0m      \u001b[32m137.4152\u001b[0m  0.4582\n",
      "      4      \u001b[36m167.4974\u001b[0m      \u001b[32m166.6073\u001b[0m  0.4451\n",
      "     32      545.0265      515.9957  0.0538\n",
      "      4      \u001b[36m136.0695\u001b[0m      \u001b[32m137.7908\u001b[0m  0.4707\n",
      "      4      \u001b[36m141.1582\u001b[0m      142.5824  0.4505\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 13.\n",
      "      4      \u001b[36m161.2177\u001b[0m      161.2176  0.4445\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "      5      \u001b[36m136.1289\u001b[0m      137.4921  0.4325\n",
      "      5      \u001b[36m167.3510\u001b[0m      166.6114  0.4306\n",
      "      5      \u001b[36m135.8051\u001b[0m      \u001b[32m137.2555\u001b[0m  0.4334\n",
      "      5      \u001b[36m140.6821\u001b[0m      \u001b[32m141.9835\u001b[0m  0.4377\n",
      "      5      \u001b[36m161.1186\u001b[0m      161.2203  0.4283\n",
      "      6      \u001b[36m135.9213\u001b[0m      \u001b[32m137.3106\u001b[0m  0.4309\n",
      "      6      167.3750      166.6494  0.4300\n",
      "      6      \u001b[36m135.7053\u001b[0m      \u001b[32m137.2550\u001b[0m  0.4304\n",
      "      6      \u001b[36m140.1900\u001b[0m      \u001b[32m141.8945\u001b[0m  0.4361\n",
      "      6      \u001b[36m161.0957\u001b[0m      161.2243  0.4319\n",
      "      7      \u001b[36m135.9030\u001b[0m      137.3120  0.4298\n",
      "      7      167.3542      166.6513  0.4324\n",
      "      7      135.7063      137.2567  0.4316\n",
      "      7      \u001b[36m140.1819\u001b[0m      141.8986  0.4350\n",
      "      7      161.1237      161.2260  0.4311\n",
      "      8      \u001b[36m135.7271\u001b[0m      \u001b[32m137.2357\u001b[0m  0.4311\n",
      "      8      167.3545      166.6544  0.4290\n",
      "      8      \u001b[36m135.7001\u001b[0m      137.2600  0.4295\n",
      "      8      \u001b[36m140.1569\u001b[0m      141.9006  0.4375\n",
      "      8      161.0991      161.2284  0.4286\n",
      "      9      \u001b[36m135.7091\u001b[0m      \u001b[32m137.2280\u001b[0m  0.4319\n",
      "      9      167.3561      166.6559  0.4313\n",
      "      9      135.7063      137.4447  0.4325\n",
      "      9      140.1592      141.9012  0.4333\n",
      "      9      \u001b[36m161.0953\u001b[0m      161.2285  0.4308\n",
      "     10      135.7274      137.2515  0.4344\n",
      "     10      167.3589      166.6575  0.4289\n",
      "     10      135.7854      137.2582  0.4291\n",
      "     10      140.1590      141.9020  0.4368\n",
      "     10      161.0959      161.2289  0.4312\n",
      "     11      \u001b[36m135.6970\u001b[0m      137.2574  0.4325\n",
      "     11      167.3530      166.6554  0.4286\n",
      "     11      135.7070      137.2626  0.4305\n",
      "     11      \u001b[36m161.0950\u001b[0m      161.2286  0.4291\n",
      "     11      140.1586      141.9023  0.4326\n",
      "     12      135.7041      137.2627  0.4319\n",
      "     12      167.3529      166.6559  0.4307\n",
      "     12      135.7094      137.2630  0.4293\n",
      "     12      \u001b[36m161.0940\u001b[0m      161.2283  0.4293\n",
      "     12      140.1578      141.9018  0.4342\n",
      "     13      135.7104      137.2647  0.4293\n",
      "     13      167.3524      166.6562  0.4314\n",
      "     13      135.7091      137.2637  0.4296\n",
      "     13      \u001b[36m161.0931\u001b[0m      161.2278  0.4300\n",
      "     13      140.1570      141.9015  0.4337\n",
      "     14      135.7099      137.2639  0.4315\n",
      "     14      167.3514      166.6557  0.4306\n",
      "     14      135.7089      137.2638  0.4320\n",
      "     14      \u001b[36m161.0923\u001b[0m      161.2273  0.4344\n",
      "     14      \u001b[36m140.1562\u001b[0m      141.9015  0.4341\n",
      "     15      135.7090      137.2639  0.4297\n",
      "     15      \u001b[36m167.3507\u001b[0m      166.6553  0.4304\n",
      "     15      135.7087      137.2639  0.4314\n",
      "     15      \u001b[36m161.0914\u001b[0m      161.2265  0.4294\n",
      "     15      \u001b[36m140.1555\u001b[0m      141.9008  0.4354\n",
      "     16      135.7086      137.2635  0.4326\n",
      "     16      \u001b[36m167.3501\u001b[0m      166.6550  0.4320\n",
      "     16      135.7079      137.2633  0.4296\n",
      "     16      \u001b[36m161.0906\u001b[0m      161.2259  0.4292\n",
      "     16      \u001b[36m140.1551\u001b[0m      141.9005  0.4320\n",
      "     17      135.7082      137.2632  0.4299\n",
      "     17      \u001b[36m167.3494\u001b[0m      166.6546  0.4294\n",
      "     17      135.7073      137.2631  0.4312\n",
      "     17      \u001b[36m161.0899\u001b[0m      161.2254  0.4298\n",
      "     17      \u001b[36m140.1543\u001b[0m      141.8998  0.4338\n",
      "     18      135.7075      137.2626  0.4315\n",
      "     18      \u001b[36m167.3488\u001b[0m      166.6542  0.4306\n",
      "     18      135.7073      137.2630  0.4298\n",
      "     18      \u001b[36m161.0893\u001b[0m      161.2249  0.4303\n",
      "     18      \u001b[36m140.1538\u001b[0m      141.8996  0.4363\n",
      "     19      135.7070      137.2624  0.4342\n",
      "     19      \u001b[36m167.3483\u001b[0m      166.6538  0.4320\n",
      "     19      135.7064      137.2617  0.4307\n",
      "     19      \u001b[36m161.0887\u001b[0m      161.2243  0.4299\n",
      "     19      \u001b[36m140.1534\u001b[0m      141.8994  0.4380\n",
      "     20      135.7065      137.2617  0.4301\n",
      "     20      \u001b[36m167.3479\u001b[0m      166.6535  0.4300\n",
      "     20      135.7060      137.2618  0.4319\n",
      "     20      \u001b[36m161.0882\u001b[0m      161.2237  0.4303\n",
      "     20      \u001b[36m140.1528\u001b[0m      141.8989  0.4326\n",
      "     21      135.7060      137.2615  0.4327\n",
      "     21      \u001b[36m167.3474\u001b[0m      166.6529  0.4262\n",
      "     21      135.7056      137.2613  0.4335\n",
      "     21      \u001b[36m161.0877\u001b[0m      161.2235  0.4291\n",
      "     21      \u001b[36m140.1525\u001b[0m      141.8988  0.4358\n",
      "     22      135.7057      137.2616  0.4297\n",
      "     22      \u001b[36m167.3470\u001b[0m      166.6525  0.4303\n",
      "     22      135.7052      137.2612  0.4324\n",
      "     22      \u001b[36m161.0873\u001b[0m      161.2230  0.4311\n",
      "     22      \u001b[36m140.1521\u001b[0m      141.8985  0.4338\n",
      "     23      135.7053      137.2609  0.4308\n",
      "     23      \u001b[36m167.3467\u001b[0m      166.6523  0.4296\n",
      "     23      135.7049      137.2605  0.4320\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 3.\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     23      \u001b[36m140.1517\u001b[0m      141.8976  0.4334\n",
      "     24      135.7050      137.2608  0.4306\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 4.\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     24      135.7045      137.2605  0.4331\n",
      "     24      \u001b[36m140.1515\u001b[0m      141.8977  0.4376\n",
      "     25      135.7047      137.2608  0.4303\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 5.\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     25      \u001b[36m140.1512\u001b[0m      141.8972  0.4289\n",
      "     26      135.7044      137.2606  0.4250\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 6.\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     27      135.7041      137.2603  0.4137\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 8.\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m162.6321\u001b[0m      \u001b[32m151.2286\u001b[0m  0.3891\n",
      "      2      \u001b[36m151.2473\u001b[0m      \u001b[32m149.2693\u001b[0m  0.3700\n",
      "      3      \u001b[36m149.5537\u001b[0m      \u001b[32m149.1422\u001b[0m  0.3598\n",
      "      4      \u001b[36m148.5654\u001b[0m      \u001b[32m148.5462\u001b[0m  0.3731\n",
      "      5      \u001b[36m148.2446\u001b[0m      148.5487  0.3622\n",
      "      6      148.2463      148.5538  0.3587\n",
      "      7      148.2475      148.5906  0.3581\n",
      "      8      148.2470      148.5667  0.3740\n",
      "      9      148.2455      148.5528  0.3654\n",
      "     10      148.2447      148.5483  0.4380\n",
      "     11      \u001b[36m148.2437\u001b[0m      \u001b[32m148.4851\u001b[0m  0.3878\n",
      "     12      \u001b[36m148.2427\u001b[0m      \u001b[32m148.4822\u001b[0m  0.3694\n",
      "     13      \u001b[36m148.2420\u001b[0m      \u001b[32m148.4801\u001b[0m  0.3628\n",
      "     14      \u001b[36m148.2413\u001b[0m      \u001b[32m148.4796\u001b[0m  0.3738\n",
      "     15      \u001b[36m148.2407\u001b[0m      \u001b[32m148.4792\u001b[0m  0.3771\n",
      "     16      \u001b[36m148.2402\u001b[0m      \u001b[32m148.4785\u001b[0m  0.4026\n",
      "     17      \u001b[36m148.2397\u001b[0m      \u001b[32m148.4783\u001b[0m  0.3694\n",
      "     18      \u001b[36m148.2393\u001b[0m      \u001b[32m148.4780\u001b[0m  0.3748\n",
      "     19      \u001b[36m148.2390\u001b[0m      \u001b[32m148.4777\u001b[0m  0.3838\n",
      "     20      \u001b[36m148.2386\u001b[0m      \u001b[32m148.4775\u001b[0m  0.3686\n",
      "     21      \u001b[36m148.2383\u001b[0m      \u001b[32m148.4773\u001b[0m  0.4032\n",
      "     22      \u001b[36m148.2381\u001b[0m      \u001b[32m148.4769\u001b[0m  0.3564\n",
      "     23      \u001b[36m148.2378\u001b[0m      148.4773  0.3926\n",
      "     24      \u001b[36m148.2376\u001b[0m      \u001b[32m148.4767\u001b[0m  0.3610\n",
      "     25      \u001b[36m148.2374\u001b[0m      \u001b[32m148.4764\u001b[0m  0.3842\n",
      "     26      \u001b[36m148.2372\u001b[0m      \u001b[32m148.4761\u001b[0m  0.3846\n",
      "     27      \u001b[36m148.2370\u001b[0m      148.4763  0.3619\n",
      "     28      \u001b[36m148.2369\u001b[0m      \u001b[32m148.4759\u001b[0m  0.3597\n",
      "     29      \u001b[36m148.2367\u001b[0m      \u001b[32m148.4756\u001b[0m  0.3679\n",
      "     30      \u001b[36m148.2366\u001b[0m      148.4758  0.3800\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 11.\n",
      "durations 3.0 2029.0\n",
      "dtype X_train age                  float32\n",
      "sex                 category\n",
      "race                 float32\n",
      "n_comorbidities      float32\n",
      "diabetes            category\n",
      "dementia            category\n",
      "blood_pressure       float32\n",
      "heart_rate           float32\n",
      "respiration_rate     float32\n",
      "temperature          float32\n",
      "white_blood_cell     float32\n",
      "serum_sodium         float32\n",
      "serum_creatinine     float32\n",
      "cancer_0.0             uint8\n",
      "cancer_1.0             uint8\n",
      "cancer_2.0             uint8\n",
      "dtype: object\n",
      "dtype y_train float64\n",
      "dtype X_test age                  float32\n",
      "sex                 category\n",
      "race                 float32\n",
      "n_comorbidities      float32\n",
      "diabetes            category\n",
      "dementia            category\n",
      "blood_pressure       float32\n",
      "heart_rate           float32\n",
      "respiration_rate     float32\n",
      "temperature          float32\n",
      "white_blood_cell     float32\n",
      "serum_sodium         float32\n",
      "serum_creatinine     float32\n",
      "cancer_0.0             uint8\n",
      "cancer_1.0             uint8\n",
      "cancer_2.0             uint8\n",
      "dtype: object\n",
      "dtype y_test float32\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m760.2865\u001b[0m      \u001b[32m685.3799\u001b[0m  0.0690\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m760.3561\u001b[0m      \u001b[32m687.1991\u001b[0m  0.0631\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m795.3199\u001b[0m      \u001b[32m718.2000\u001b[0m  0.0763\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m939.1712\u001b[0m      \u001b[32m829.9855\u001b[0m  0.0784\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m974.8591\u001b[0m      \u001b[32m879.4798\u001b[0m  0.1004\n",
      "      2      \u001b[36m671.3650\u001b[0m      \u001b[32m586.7135\u001b[0m  0.0873\n",
      "      2      \u001b[36m666.0640\u001b[0m      \u001b[32m579.7268\u001b[0m  0.0964\n",
      "      2      \u001b[36m708.4793\u001b[0m      \u001b[32m608.0705\u001b[0m  0.0803\n",
      "      2      \u001b[36m830.5470\u001b[0m      \u001b[32m694.7661\u001b[0m  0.0910\n",
      "      2      \u001b[36m868.1352\u001b[0m      \u001b[32m752.0238\u001b[0m  0.0886\n",
      "      3      \u001b[36m602.5595\u001b[0m      \u001b[32m542.1372\u001b[0m  0.0899\n",
      "      3      \u001b[36m593.5221\u001b[0m      \u001b[32m541.2712\u001b[0m  0.0897\n",
      "      3      \u001b[36m626.3518\u001b[0m      \u001b[32m554.5878\u001b[0m  0.0872\n",
      "      3      \u001b[36m734.3158\u001b[0m      \u001b[32m634.0345\u001b[0m  0.0754\n",
      "      4      \u001b[36m575.6678\u001b[0m      \u001b[32m525.4003\u001b[0m  0.0678\n",
      "      4      \u001b[36m569.8423\u001b[0m      \u001b[32m523.2332\u001b[0m  0.0654\n",
      "      3      \u001b[36m772.6499\u001b[0m      \u001b[32m673.3150\u001b[0m  0.0838\n",
      "      4      \u001b[36m592.3585\u001b[0m      \u001b[32m537.3174\u001b[0m  0.0707\n",
      "      4      \u001b[36m685.4632\u001b[0m      \u001b[32m608.6078\u001b[0m  0.0819\n",
      "      5      \u001b[36m557.8497\u001b[0m      \u001b[32m513.7551\u001b[0m  0.0605\n",
      "      5      \u001b[36m561.3984\u001b[0m      \u001b[32m514.4049\u001b[0m  0.0725\n",
      "      5      \u001b[36m576.9722\u001b[0m      \u001b[32m530.2874\u001b[0m  0.0672\n",
      "      4      \u001b[36m712.3648\u001b[0m      \u001b[32m635.1148\u001b[0m  0.0720\n",
      "      5      \u001b[36m663.6557\u001b[0m      \u001b[32m600.0984\u001b[0m  0.0701\n",
      "      6      \u001b[36m552.5505\u001b[0m      \u001b[32m510.3980\u001b[0m  0.0698\n",
      "      5      \u001b[36m681.4904\u001b[0m      \u001b[32m629.1960\u001b[0m  0.0650\n",
      "      6      \u001b[36m553.7635\u001b[0m      \u001b[32m510.2797\u001b[0m  0.0715\n",
      "      6      \u001b[36m570.1016\u001b[0m      \u001b[32m528.2736\u001b[0m  0.0719\n",
      "      6      \u001b[36m653.4345\u001b[0m      \u001b[32m598.4098\u001b[0m  0.0826\n",
      "      7      \u001b[36m549.4536\u001b[0m      \u001b[32m509.5325\u001b[0m  0.0705\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m146.6624\u001b[0m      \u001b[32m137.8988\u001b[0m  0.5554\n",
      "      6      \u001b[36m670.4394\u001b[0m      \u001b[32m628.7698\u001b[0m  0.0698\n",
      "      7      \u001b[36m550.8598\u001b[0m      \u001b[32m508.9772\u001b[0m  0.0743\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m148.7115\u001b[0m      \u001b[32m139.9053\u001b[0m  0.5613\n",
      "      7      \u001b[36m568.0272\u001b[0m      \u001b[32m527.9249\u001b[0m  0.0739\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m156.2972\u001b[0m      \u001b[32m145.4218\u001b[0m  0.5744\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m180.2833\u001b[0m      \u001b[32m163.9546\u001b[0m  0.5737\n",
      "      7      \u001b[36m649.6631\u001b[0m      \u001b[32m598.3099\u001b[0m  0.0783\n",
      "      7      \u001b[36m668.6856\u001b[0m      \u001b[32m628.7641\u001b[0m  0.0719\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m186.9790\u001b[0m      \u001b[32m172.7083\u001b[0m  0.6134\n",
      "      8      \u001b[36m548.2584\u001b[0m      \u001b[32m508.4435\u001b[0m  0.0705\n",
      "      8      \u001b[36m547.0993\u001b[0m      \u001b[32m509.2741\u001b[0m  0.0887\n",
      "      8      \u001b[36m567.1842\u001b[0m      527.9428  0.0759\n",
      "      8      \u001b[36m648.7949\u001b[0m      \u001b[32m598.3069\u001b[0m  0.0557\n",
      "      8      \u001b[36m668.3369\u001b[0m      \u001b[32m628.7622\u001b[0m  0.0696\n",
      "      9      548.3195      \u001b[32m508.2056\u001b[0m  0.0767\n",
      "      9      548.3061      \u001b[32m509.0974\u001b[0m  0.0861\n",
      "      9      \u001b[36m566.6045\u001b[0m      527.9412  0.0805\n",
      "      9      \u001b[36m647.8766\u001b[0m      598.3079  0.0712\n",
      "      9      \u001b[36m668.2697\u001b[0m      628.7626  0.0621\n",
      "     10      \u001b[36m547.2679\u001b[0m      508.2103  0.0577\n",
      "     10      547.2171      \u001b[32m508.9901\u001b[0m  0.0585\n",
      "     10      566.7180      527.9425  0.0698\n",
      "     10      647.9757      598.3102  0.0673\n",
      "     10      668.4937      628.7640  0.0758\n",
      "     11      \u001b[36m547.0626\u001b[0m      509.2159  0.0594\n",
      "     11      \u001b[36m547.1486\u001b[0m      \u001b[32m508.1941\u001b[0m  0.0882\n",
      "     11      \u001b[36m566.4158\u001b[0m      527.9448  0.0647\n",
      "     11      \u001b[36m647.8489\u001b[0m      598.3129  0.0744\n",
      "     11      \u001b[36m668.1361\u001b[0m      628.7661  0.0598\n",
      "     12      \u001b[36m546.7212\u001b[0m      509.1382  0.0533\n",
      "     12      566.6014      527.9484  0.0586\n",
      "     12      \u001b[36m546.8890\u001b[0m      \u001b[32m508.1856\u001b[0m  0.0766\n",
      "     12      647.8820      598.3170  0.0724\n",
      "     12      668.1529      628.7696  0.0660\n",
      "     13      547.4666      509.0206  0.0723\n",
      "     13      566.5430      527.9512  0.0773\n",
      "     13      546.8921      508.1874  0.0710\n",
      "     13      647.8809      598.3217  0.0596\n",
      "     13      668.2355      628.7735  0.0615\n",
      "     14      547.4322      509.1299  0.0559\n",
      "     14      566.4710      527.9543  0.0550\n",
      "     14      \u001b[36m647.7910\u001b[0m      598.3272  0.0577\n",
      "     14      \u001b[36m546.8226\u001b[0m      508.1917  0.0773\n",
      "      2      \u001b[36m165.0191\u001b[0m      \u001b[32m160.5479\u001b[0m  0.5040\n",
      "      2      \u001b[36m143.7719\u001b[0m      \u001b[32m141.9766\u001b[0m  0.5127\n",
      "     14      668.2763      628.7775  0.0734\n",
      "     15      \u001b[36m546.4852\u001b[0m      509.1846  0.0798\n",
      "      2      \u001b[36m136.7679\u001b[0m      \u001b[32m136.5164\u001b[0m  0.5791\n",
      "     15      \u001b[36m566.3794\u001b[0m      527.9581  0.0823\n",
      "     15      647.8377      598.3333  0.0619\n",
      "     15      668.1679      628.7826  0.0537\n",
      "     15      546.8319      508.2129  0.0840\n",
      "     16      546.7679      509.0996  0.0653\n",
      "      2      \u001b[36m170.9004\u001b[0m      \u001b[32m168.9409\u001b[0m  0.5547\n",
      "     16      566.3996      527.9648  0.0655\n",
      "     16      648.0981      598.3417  0.0633\n",
      "     17      546.9100      \u001b[32m508.7546\u001b[0m  0.0630\n",
      "     16      668.1990      628.7903  0.0826\n",
      "     16      \u001b[36m546.6247\u001b[0m      508.2067  0.0694\n",
      "     17      566.4547      527.9713  0.0633\n",
      "      2      \u001b[36m137.7631\u001b[0m      \u001b[32m137.3434\u001b[0m  0.7038\n",
      "     17      647.8636      598.3528  0.0846\n",
      "     17      668.1811      628.7950  0.0663\n",
      "     17      546.7336      508.6132  0.0673\n",
      "     18      \u001b[36m546.1507\u001b[0m      \u001b[32m508.4311\u001b[0m  0.0704\n",
      "     18      566.5319      527.9774  0.0595\n",
      "     18      668.2435      628.8018  0.0534\n",
      "     18      647.8995      598.3598  0.0712\n",
      "     18      547.0445      508.8527  0.0566\n",
      "     19      546.2274      508.4367  0.0676\n",
      "     19      566.5808      527.9844  0.0661\n",
      "     19      647.9785      598.3842  0.0646\n",
      "     19      668.2791      628.8089  0.0724\n",
      "     20      546.5140      508.4983  0.0534\n",
      "     20      566.4014      527.9926  0.0607\n",
      "     19      547.1287      508.7027  0.0985\n",
      "     21      546.5355      \u001b[32m508.4210\u001b[0m  0.0527\n",
      "     20      668.2590      628.8165  0.0625\n",
      "     20      647.8773      598.3801  0.0786\n",
      "     21      566.5586      527.9975  0.0802\n",
      "     20      547.3110      508.2425  0.0768\n",
      "     22      546.2177      \u001b[32m508.3631\u001b[0m  0.0616\n",
      "     21      668.2434      628.8231  0.0579\n",
      "     21      647.9548      599.2291  0.0537\n",
      "      3      \u001b[36m141.4436\u001b[0m      \u001b[32m141.0347\u001b[0m  0.4819\n",
      "     21      546.8398      508.2383  0.0591\n",
      "     23      546.6038      \u001b[32m508.3306\u001b[0m  0.0569\n",
      "     22      566.4832      528.0012  0.0812\n",
      "     22      668.2245      628.8302  0.0659\n",
      "     22      647.9999      598.3917  0.0570\n",
      "      3      \u001b[36m162.8261\u001b[0m      \u001b[32m159.9192\u001b[0m  0.5464\n",
      "      3      \u001b[36m136.2117\u001b[0m      \u001b[32m136.2960\u001b[0m  0.5294\n",
      "     22      546.7174      508.2480  0.0623\n",
      "     24      546.6643      \u001b[32m508.3167\u001b[0m  0.0673\n",
      "     23      668.2383      628.8357  0.0569\n",
      "     23      647.9320      598.3979  0.0597\n",
      "     23      566.4511      528.0066  0.0762\n",
      "      3      \u001b[36m167.9979\u001b[0m      \u001b[32m167.6732\u001b[0m  0.5147\n",
      "     23      547.0476      508.2507  0.0767\n",
      "     24      647.9166      598.4047  0.0621\n",
      "     25      546.4860      508.3422  0.0832\n",
      "     24      668.2550      628.8420  0.0950\n",
      "     24      566.4805      528.0120  0.0808\n",
      "      3      \u001b[36m137.2212\u001b[0m      \u001b[32m136.3121\u001b[0m  0.5339\n",
      "     24      546.9153      508.2538  0.0734\n",
      "     25      647.9136      598.4070  0.0772\n",
      "     26      546.6788      508.3171  0.0723\n",
      "     25      668.3638      628.8465  0.0606\n",
      "     25      566.4802      528.0156  0.0613\n",
      "     25      546.7826      508.2569  0.0698\n",
      "     26      647.9649      598.4104  0.0662\n",
      "     27      546.7892      508.3476  0.0580\n",
      "     26      566.4546      528.0207  0.0538\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 6.\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 7.\n",
      "     28      546.2195      508.3760  0.0570\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 7.\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     26      547.0255      508.2477  0.0728\n",
      "     29      546.7729      508.3942  0.0552\n",
      "     27      546.7698      508.2497  0.0557\n",
      "     30      546.3432      508.3950  0.0541\n",
      "     28      546.9316      508.2591  0.0599\n",
      "      4      \u001b[36m141.1597\u001b[0m      141.1380  0.5199\n",
      "     31      546.4643      508.3452  0.0553\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 9.\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "      4      \u001b[36m161.6756\u001b[0m      \u001b[32m159.6982\u001b[0m  0.5037\n",
      "     32      \u001b[36m546.1383\u001b[0m      508.3212  0.0552\n",
      "      4      \u001b[36m135.7978\u001b[0m      \u001b[32m136.0137\u001b[0m  0.5031\n",
      "      4      \u001b[36m166.8346\u001b[0m      \u001b[32m167.6009\u001b[0m  0.4989\n",
      "     33      \u001b[36m546.0924\u001b[0m      \u001b[32m508.1135\u001b[0m  0.0553\n",
      "      4      \u001b[36m136.4304\u001b[0m      136.5474  0.4575\n",
      "     34      546.5104      508.1938  0.0534\n",
      "     35      546.3180      508.2306  0.0544\n",
      "     36      546.1901      508.2618  0.0553\n",
      "     37      546.3366      508.2695  0.0547\n",
      "     38      546.3009      508.2618  0.0539\n",
      "      5      \u001b[36m140.9017\u001b[0m      141.1143  0.4356\n",
      "     39      \u001b[36m545.8731\u001b[0m      508.2249  0.0566\n",
      "      5      \u001b[36m161.3937\u001b[0m      159.6985  0.4375\n",
      "      5      \u001b[36m135.7687\u001b[0m      \u001b[32m135.9259\u001b[0m  0.4364\n",
      "     40      546.2059      508.2263  0.0545\n",
      "      5      \u001b[36m166.7674\u001b[0m      \u001b[32m167.5976\u001b[0m  0.4377\n",
      "     41      545.8887      508.2734  0.0582\n",
      "      5      \u001b[36m136.0972\u001b[0m      136.3225  0.4421\n",
      "     42      546.2147      508.3379  0.0638\n",
      "     43      546.3761      508.3896  0.0546\n",
      "     44      546.7383      508.4400  0.0538\n",
      "     45      546.6583      508.4343  0.0543\n",
      "     46      546.1500      508.3564  0.0532\n",
      "      6      \u001b[36m140.6880\u001b[0m      141.1335  0.4464\n",
      "     47      546.0344      508.3780  0.0524\n",
      "      6      161.3955      159.7002  0.4490\n",
      "      6      \u001b[36m135.7575\u001b[0m      \u001b[32m135.9251\u001b[0m  0.4418\n",
      "     48      545.9188      508.3801  0.0553\n",
      "      6      166.7711      167.5993  0.4519\n",
      "     49      546.2721      508.3461  0.0641\n",
      "      6      \u001b[36m136.0728\u001b[0m      136.4997  0.4437\n",
      "     50      546.5288      508.4407  0.0537\n",
      "Restoring best model from epoch 33.\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "      7      140.7015      141.1338  0.4338\n",
      "      7      161.3992      159.7033  0.4423\n",
      "      7      \u001b[36m135.7422\u001b[0m      135.9271  0.4354\n",
      "      7      166.7730      167.6032  0.4354\n",
      "      7      136.2919      \u001b[32m136.1178\u001b[0m  0.4345\n",
      "      8      140.6916      141.1384  0.4354\n",
      "      8      161.4025      159.7047  0.4354\n",
      "      8      135.7447      135.9294  0.4313\n",
      "      8      166.7722      167.6053  0.4334\n",
      "      8      \u001b[36m135.9852\u001b[0m      \u001b[32m136.0288\u001b[0m  0.4338\n",
      "      9      140.7034      141.1375  0.4350\n",
      "      9      161.4156      159.7098  0.4354\n",
      "      9      135.7448      135.9304  0.4332\n",
      "      9      166.7869      \u001b[32m167.5005\u001b[0m  0.4345\n",
      "      9      \u001b[36m135.7378\u001b[0m      \u001b[32m136.0214\u001b[0m  0.4340\n",
      "     10      140.7032      141.1401  0.4343\n",
      "     10      161.4015      159.7065  0.4336\n",
      "     10      135.7442      135.9313  0.4322\n",
      "     10      166.7703      \u001b[32m167.4992\u001b[0m  0.4306\n",
      "     10      135.7396      \u001b[32m135.9330\u001b[0m  0.4338\n",
      "     11      140.7053      141.1416  0.4387\n",
      "     11      135.7680      136.0617  0.4374\n",
      "     11      161.4004      159.7076  0.4425\n",
      "     11      166.7731      167.4996  0.4398\n",
      "     11      135.7442      \u001b[32m135.9303\u001b[0m  0.4362\n",
      "     12      140.7047      141.1424  0.4341\n",
      "     12      135.7587      136.1661  0.4343\n",
      "     12      161.4007      159.7086  0.4328\n",
      "     12      166.7723      167.4996  0.4332\n",
      "     12      135.7441      135.9311  0.4333\n",
      "     13      140.7037      141.1421  0.4385\n",
      "     13      135.7557      135.9299  0.4398\n",
      "     13      161.4003      159.7085  0.4373\n",
      "     13      166.7714      167.4995  0.4366\n",
      "     13      135.7436      135.9316  0.4406\n",
      "     14      140.7208      141.1391  0.4379\n",
      "     14      135.7429      135.9334  0.4372\n",
      "     14      161.3996      159.7084  0.4372\n",
      "     14      166.7707      167.4992  0.4317\n",
      "     14      135.7436      135.9324  0.4344\n",
      "     15      140.6970      141.1399  0.4341\n",
      "     15      161.3989      159.7081  0.4354\n",
      "     15      135.7430      135.9320  0.4362\n",
      "     15      166.7699      \u001b[32m167.4988\u001b[0m  0.4341\n",
      "     15      135.7430      135.9325  0.4334\n",
      "     16      140.7027      141.1424  0.4368\n",
      "     16      \u001b[36m135.7415\u001b[0m      135.9329  0.4333\n",
      "     16      161.3983      159.7075  0.4347\n",
      "     16      166.7692      \u001b[32m167.4984\u001b[0m  0.4349\n",
      "     16      135.7426      136.7304  0.4358\n",
      "     17      140.7023      141.1418  0.4498\n",
      "     17      161.3977      159.7071  0.4525\n",
      "     17      \u001b[36m135.7340\u001b[0m      135.9588  0.4538\n",
      "     17      166.7685      \u001b[32m167.4981\u001b[0m  0.4523\n",
      "     17      135.7421      135.9437  0.4557\n",
      "     18      140.7019      141.1416  0.4303\n",
      "     18      161.3971      159.7068  0.4217\n",
      "     18      135.7851      136.1694  0.4300\n",
      "     18      166.7679      \u001b[32m167.4977\u001b[0m  0.4302\n",
      "     18      135.7418      135.9310  0.4298\n",
      "     19      140.7016      141.1412  0.4298\n",
      "     19      161.3966      159.7064  0.4265\n",
      "     19      135.7613      \u001b[32m135.9230\u001b[0m  0.4300\n",
      "     19      \u001b[36m166.7673\u001b[0m      \u001b[32m167.4974\u001b[0m  0.4237\n",
      "     19      135.7412      135.9304  0.4213\n",
      "     20      140.7012      141.1409  0.4280\n",
      "     20      161.3961      159.7060  0.4255\n",
      "     20      135.7350      135.9290  0.4262\n",
      "     20      \u001b[36m166.7668\u001b[0m      \u001b[32m167.4969\u001b[0m  0.4163\n",
      "     20      135.7406      \u001b[32m135.9298\u001b[0m  0.4286\n",
      "     21      140.7007      141.1405  0.4344\n",
      "     21      161.3957      159.7056  0.4353\n",
      "     21      135.7414      135.9316  0.4334\n",
      "     21      \u001b[36m166.7664\u001b[0m      \u001b[32m167.4967\u001b[0m  0.4285\n",
      "     21      135.7403      \u001b[32m135.9294\u001b[0m  0.4302\n",
      "     22      140.7004      141.1403  0.4251\n",
      "     22      161.3953      159.7052  0.4251\n",
      "     22      135.7413      135.9310  0.4231\n",
      "     22      \u001b[36m166.7659\u001b[0m      \u001b[32m167.4962\u001b[0m  0.4110\n",
      "     22      135.7400      135.9296  0.4240\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 3.\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     23      161.3949      159.7051  0.4230\n",
      "     23      135.7408      135.9305  0.4245\n",
      "     23      \u001b[36m166.7655\u001b[0m      \u001b[32m167.4958\u001b[0m  0.4251\n",
      "     23      135.7394      \u001b[32m135.9292\u001b[0m  0.4131\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 4.\n",
      "     24      135.7404      135.9305  0.4323\n",
      "     24      \u001b[36m166.7652\u001b[0m      \u001b[32m167.4956\u001b[0m  0.4306\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     24      135.7392      \u001b[32m135.9284\u001b[0m  0.4285\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 5.\n",
      "     25      \u001b[36m166.7649\u001b[0m      \u001b[32m167.4954\u001b[0m  0.4251\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     25      135.7390      135.9284  0.4220\n",
      "     26      \u001b[36m166.7646\u001b[0m      \u001b[32m167.4951\u001b[0m  0.4113\n",
      "     26      135.7382      \u001b[32m135.9284\u001b[0m  0.4096\n",
      "     27      \u001b[36m166.7643\u001b[0m      \u001b[32m167.4949\u001b[0m  0.4056\n",
      "     27      135.7392      \u001b[32m135.9280\u001b[0m  0.4067\n",
      "     28      \u001b[36m166.7640\u001b[0m      \u001b[32m167.4946\u001b[0m  0.4062\n",
      "     28      \u001b[36m135.7378\u001b[0m      \u001b[32m135.9276\u001b[0m  0.4118\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 9.\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     29      135.7379      \u001b[32m135.9273\u001b[0m  0.4805\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 10.\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m161.6404\u001b[0m      \u001b[32m148.7608\u001b[0m  0.3791\n",
      "      2      \u001b[36m150.3166\u001b[0m      \u001b[32m147.4261\u001b[0m  0.3704\n",
      "      3      \u001b[36m149.1009\u001b[0m      \u001b[32m147.4260\u001b[0m  0.3746\n",
      "      4      \u001b[36m148.5917\u001b[0m      \u001b[32m147.3538\u001b[0m  0.3565\n",
      "      5      \u001b[36m148.5225\u001b[0m      147.4001  0.3706\n",
      "      6      148.5338      147.4061  0.3642\n",
      "      7      \u001b[36m148.4924\u001b[0m      147.4143  0.3584\n",
      "      8      \u001b[36m148.4922\u001b[0m      147.4129  0.3818\n",
      "      9      \u001b[36m148.4912\u001b[0m      147.4190  0.3714\n",
      "     10      \u001b[36m148.4899\u001b[0m      147.4148  0.3664\n",
      "     11      \u001b[36m148.4892\u001b[0m      147.4135  0.3491\n",
      "     12      \u001b[36m148.4880\u001b[0m      147.4211  0.3770\n",
      "     13      \u001b[36m148.4873\u001b[0m      147.4216  0.4313\n",
      "     14      \u001b[36m148.4866\u001b[0m      147.4212  0.3633\n",
      "     15      \u001b[36m148.4859\u001b[0m      147.4206  0.3566\n",
      "     16      \u001b[36m148.4854\u001b[0m      147.4200  0.3677\n",
      "     17      \u001b[36m148.4849\u001b[0m      147.4198  0.3504\n",
      "     18      \u001b[36m148.4845\u001b[0m      147.4193  0.3942\n",
      "     19      \u001b[36m148.4841\u001b[0m      147.4192  0.4472\n",
      "     20      \u001b[36m148.4837\u001b[0m      147.4187  0.4027\n",
      "     21      \u001b[36m148.4834\u001b[0m      147.4183  0.3785\n",
      "     22      \u001b[36m148.4832\u001b[0m      147.4182  0.4267\n",
      "     23      \u001b[36m148.4829\u001b[0m      147.4179  0.3787\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 4.\n",
      "durations 3.0 2029.0\n",
      "dtype X_train age                  float32\n",
      "sex                 category\n",
      "race                 float32\n",
      "n_comorbidities      float32\n",
      "diabetes            category\n",
      "dementia            category\n",
      "blood_pressure       float32\n",
      "heart_rate           float32\n",
      "respiration_rate     float32\n",
      "temperature          float32\n",
      "white_blood_cell     float32\n",
      "serum_sodium         float32\n",
      "serum_creatinine     float32\n",
      "cancer_0.0             uint8\n",
      "cancer_1.0             uint8\n",
      "cancer_2.0             uint8\n",
      "dtype: object\n",
      "dtype y_train float64\n",
      "dtype X_test age                  float32\n",
      "sex                 category\n",
      "race                 float32\n",
      "n_comorbidities      float32\n",
      "diabetes            category\n",
      "dementia            category\n",
      "blood_pressure       float32\n",
      "heart_rate           float32\n",
      "respiration_rate     float32\n",
      "temperature          float32\n",
      "white_blood_cell     float32\n",
      "serum_sodium         float32\n",
      "serum_creatinine     float32\n",
      "cancer_0.0             uint8\n",
      "cancer_1.0             uint8\n",
      "cancer_2.0             uint8\n",
      "dtype: object\n",
      "dtype y_test float32\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m785.5495\u001b[0m      \u001b[32m712.3334\u001b[0m  0.1337\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m753.5111\u001b[0m      \u001b[32m692.5420\u001b[0m  0.1507\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m934.2435\u001b[0m      \u001b[32m835.7843\u001b[0m  0.1141\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m753.6471\u001b[0m      \u001b[32m688.1384\u001b[0m  0.1372\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m978.5268\u001b[0m      \u001b[32m877.5625\u001b[0m  0.1779\n",
      "      2      \u001b[36m674.5309\u001b[0m      \u001b[32m587.2658\u001b[0m  0.1165\n",
      "      2      \u001b[36m702.6300\u001b[0m      \u001b[32m606.6099\u001b[0m  0.1523\n",
      "      2      \u001b[36m663.8846\u001b[0m      \u001b[32m583.4102\u001b[0m  0.1575\n",
      "      2      \u001b[36m828.0903\u001b[0m      \u001b[32m707.9265\u001b[0m  0.1475\n",
      "      2      \u001b[36m866.4235\u001b[0m      \u001b[32m746.7733\u001b[0m  0.0725\n",
      "      3      \u001b[36m602.6134\u001b[0m      \u001b[32m535.9763\u001b[0m  0.0817\n",
      "      3      \u001b[36m770.7957\u001b[0m      \u001b[32m682.6173\u001b[0m  0.0643\n",
      "      3      \u001b[36m590.4320\u001b[0m      \u001b[32m537.8818\u001b[0m  0.0787\n",
      "      3      \u001b[36m735.8308\u001b[0m      \u001b[32m644.2463\u001b[0m  0.0759\n",
      "      3      \u001b[36m631.3627\u001b[0m      \u001b[32m557.3395\u001b[0m  0.1205\n",
      "      4      \u001b[36m568.6348\u001b[0m      \u001b[32m518.8526\u001b[0m  0.0668\n",
      "      4      \u001b[36m721.7905\u001b[0m      \u001b[32m643.9609\u001b[0m  0.0717\n",
      "      4      \u001b[36m566.9216\u001b[0m      \u001b[32m523.9125\u001b[0m  0.0787\n",
      "      4      \u001b[36m687.5374\u001b[0m      \u001b[32m614.5163\u001b[0m  0.0800\n",
      "      4      \u001b[36m594.0394\u001b[0m      \u001b[32m538.4825\u001b[0m  0.0846\n",
      "      5      \u001b[36m554.1570\u001b[0m      \u001b[32m511.4129\u001b[0m  0.0724\n",
      "      5      \u001b[36m555.8612\u001b[0m      \u001b[32m515.4222\u001b[0m  0.0602\n",
      "      5      \u001b[36m689.9271\u001b[0m      \u001b[32m632.7874\u001b[0m  0.0740\n",
      "      5      \u001b[36m661.7018\u001b[0m      \u001b[32m606.1924\u001b[0m  0.0729\n",
      "      5      \u001b[36m575.7785\u001b[0m      \u001b[32m530.5563\u001b[0m  0.0718\n",
      "      6      \u001b[36m546.9817\u001b[0m      \u001b[32m509.3682\u001b[0m  0.0651\n",
      "      6      \u001b[36m550.8255\u001b[0m      \u001b[32m511.6375\u001b[0m  0.0657\n",
      "      6      \u001b[36m675.7293\u001b[0m      \u001b[32m631.5877\u001b[0m  0.0686\n",
      "      6      \u001b[36m650.3143\u001b[0m      \u001b[32m605.1674\u001b[0m  0.0707\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m154.4422\u001b[0m      \u001b[32m143.5719\u001b[0m  0.6819\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m187.6475\u001b[0m      \u001b[32m172.8027\u001b[0m  0.6827\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m149.0844\u001b[0m      \u001b[32m138.8369\u001b[0m  0.6934\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m180.3289\u001b[0m      \u001b[32m164.9195\u001b[0m  0.6879\n",
      "      7      \u001b[36m545.8159\u001b[0m      \u001b[32m510.3021\u001b[0m  0.0614\n",
      "      6      \u001b[36m565.2734\u001b[0m      \u001b[32m528.2543\u001b[0m  0.0789\n",
      "      7      \u001b[36m543.6305\u001b[0m      509.4534  0.0799\n",
      "      7      \u001b[36m672.1192\u001b[0m      \u001b[32m631.3971\u001b[0m  0.0682\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m148.9425\u001b[0m      \u001b[32m138.5255\u001b[0m  0.7242\n",
      "      7      \u001b[36m647.3470\u001b[0m      \u001b[32m605.1582\u001b[0m  0.0779\n",
      "      7      \u001b[36m562.5122\u001b[0m      \u001b[32m528.0054\u001b[0m  0.0697\n",
      "      8      \u001b[36m544.6318\u001b[0m      \u001b[32m509.3703\u001b[0m  0.0746\n",
      "      8      \u001b[36m543.5043\u001b[0m      509.5540  0.0680\n",
      "      8      \u001b[36m671.4897\u001b[0m      \u001b[32m631.3891\u001b[0m  0.0742\n",
      "      8      \u001b[36m645.8130\u001b[0m      605.1587  0.0546\n",
      "      9      543.5090      509.5973  0.0546\n",
      "      9      \u001b[36m543.5228\u001b[0m      509.4368  0.0644\n",
      "      9      \u001b[36m670.9581\u001b[0m      \u001b[32m631.3890\u001b[0m  0.0632\n",
      "      9      645.8910      605.1597  0.0586\n",
      "      8      \u001b[36m561.4963\u001b[0m      528.0563  0.1070\n",
      "     10      \u001b[36m543.4546\u001b[0m      509.5378  0.0559\n",
      "     10      \u001b[36m670.7188\u001b[0m      631.4596  0.0660\n",
      "     10      \u001b[36m543.0321\u001b[0m      509.6078  0.0845\n",
      "     10      \u001b[36m645.4726\u001b[0m      605.1599  0.0764\n",
      "      9      562.1471      528.0799  0.0542\n",
      "     11      \u001b[36m542.7330\u001b[0m      509.7007  0.0746\n",
      "     11      \u001b[36m670.3758\u001b[0m      631.4664  0.0609\n",
      "     11      645.6700      605.1607  0.0585\n",
      "     11      \u001b[36m542.9995\u001b[0m      509.6132  0.0962\n",
      "     10      561.5185      528.0668  0.0825\n",
      "     12      542.9166      509.5843  0.0609\n",
      "     12      \u001b[36m670.2361\u001b[0m      631.4702  0.0658\n",
      "     12      645.5984      605.1633  0.0586\n",
      "     12      \u001b[36m542.8346\u001b[0m      509.6216  0.0706\n",
      "     11      \u001b[36m561.3390\u001b[0m      528.0698  0.0654\n",
      "     13      542.9492      509.7047  0.0627\n",
      "     13      645.6371      605.1663  0.0675\n",
      "     13      670.3168      631.4753  0.0836\n",
      "     12      561.4628      528.0838  0.0701\n",
      "     14      \u001b[36m542.7024\u001b[0m      509.8566  0.0601\n",
      "     13      \u001b[36m542.7648\u001b[0m      509.6303  0.0801\n",
      "     14      645.5434      605.1713  0.0669\n",
      "      2      \u001b[36m164.5608\u001b[0m      \u001b[32m161.8097\u001b[0m  0.4996\n",
      "      2      \u001b[36m142.5113\u001b[0m      \u001b[32m141.7455\u001b[0m  0.5214\n",
      "      2      \u001b[36m170.2259\u001b[0m      \u001b[32m169.5033\u001b[0m  0.5180\n",
      "     14      670.3474      631.4788  0.0697\n",
      "      2      \u001b[36m138.7987\u001b[0m      \u001b[32m137.6991\u001b[0m  0.4885\n",
      "     15      \u001b[36m542.6669\u001b[0m      510.0980  0.0586\n",
      "      2      \u001b[36m138.3150\u001b[0m      \u001b[32m136.3930\u001b[0m  0.5415\n",
      "     14      542.9168      509.6264  0.0552\n",
      "     15      645.5860      605.1767  0.0654\n",
      "     13      \u001b[36m561.2268\u001b[0m      528.0863  0.0976\n",
      "     16      \u001b[36m542.4112\u001b[0m      510.0182  0.0578\n",
      "     15      542.9895      509.6268  0.0559\n",
      "     15      670.3915      631.4834  0.0806\n",
      "     14      \u001b[36m561.1942\u001b[0m      528.0895  0.0627\n",
      "     16      645.6140      605.1820  0.0789\n",
      "     16      542.8452      509.6279  0.0615\n",
      "     16      670.3260      631.4883  0.0609\n",
      "     17      542.5022      510.2573  0.0702\n",
      "     15      561.4473      528.0923  0.0646\n",
      "     17      645.6279      605.1891  0.0625\n",
      "     17      670.2650      631.4948  0.0581\n",
      "     17      542.8880      509.6312  0.0610\n",
      "     18      542.4119      510.7931  0.0645\n",
      "     16      \u001b[36m561.1675\u001b[0m      528.0962  0.0685\n",
      "     18      670.3129      631.5012  0.0588\n",
      "     18      645.6358      605.1962  0.0687\n",
      "     19      542.5456      510.6579  0.0582\n",
      "     18      543.0707      509.6346  0.0683\n",
      "     17      561.2571      528.1006  0.0633\n",
      "     19      670.2860      631.5086  0.0621\n",
      "     20      542.4688      510.3344  0.0684\n",
      "     19      645.6785      605.2046  0.0807\n",
      "     19      543.1169      509.6327  0.0923\n",
      "     20      670.3113      631.5162  0.0675\n",
      "     21      542.7095      510.1953  0.0561\n",
      "     18      \u001b[36m560.9928\u001b[0m      528.1042  0.0815\n",
      "     20      645.6862      605.2135  0.0773\n",
      "     20      542.8783      509.6306  0.0601\n",
      "     21      670.3283      631.5251  0.0556\n",
      "     19      561.1254      528.1089  0.0520\n",
      "     21      645.6292      605.2208  0.0575\n",
      "      3      \u001b[36m141.3939\u001b[0m      \u001b[32m141.1093\u001b[0m  0.4849\n",
      "     22      542.8148      510.3288  0.1015\n",
      "     21      542.8730      509.6273  0.0746\n",
      "     20      561.2187      528.1130  0.0575\n",
      "      3      \u001b[36m137.4588\u001b[0m      \u001b[32m136.5704\u001b[0m  0.5066\n",
      "     22      670.3461      631.5315  0.0731\n",
      "      3      \u001b[36m167.8783\u001b[0m      \u001b[32m168.4367\u001b[0m  0.5320\n",
      "     22      645.6270      605.2279  0.0684\n",
      "     22      542.9404      509.6327  0.0625\n",
      "     21      561.1507      528.1175  0.0543\n",
      "      3      \u001b[36m136.4569\u001b[0m      \u001b[32m136.1656\u001b[0m  0.5447\n",
      "      3      \u001b[36m162.0860\u001b[0m      \u001b[32m161.1212\u001b[0m  0.5826\n",
      "     23      670.4667      631.5368  0.0629\n",
      "     23      542.5180      510.7761  0.1059\n",
      "     23      542.8376      509.6399  0.0571\n",
      "     22      561.3613      528.1241  0.0630\n",
      "     23      645.6542      605.2333  0.0935\n",
      "     24      670.3567      631.5436  0.0667\n",
      "     24      542.6569      510.6255  0.0610\n",
      "     24      542.8789      509.6444  0.0659\n",
      "     23      561.3485      528.1284  0.0595\n",
      "     24      645.6554      605.2396  0.0668\n",
      "     25      670.3373      631.5488  0.0704\n",
      "     25      542.7295      510.1907  0.0625\n",
      "     25      542.8232      509.6386  0.0632\n",
      "     24      561.1805      528.1323  0.0725\n",
      "     25      645.6541      605.2430  0.0654\n",
      "     26      670.3803      631.5540  0.0684\n",
      "     26      542.7726      510.5408  0.0623\n",
      "     25      561.2100      528.1365  0.0568\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 6.\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 6.\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     27      542.7382      510.3475  0.0564\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 7.\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     26      561.2205      528.1391  0.0656\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 8.\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 7.\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "      4      \u001b[36m140.8366\u001b[0m      \u001b[32m140.6454\u001b[0m  0.5021\n",
      "      4      \u001b[36m167.2970\u001b[0m      \u001b[32m168.1473\u001b[0m  0.4785\n",
      "      4      \u001b[36m136.8189\u001b[0m      \u001b[32m136.4628\u001b[0m  0.4944\n",
      "      4      \u001b[36m136.0997\u001b[0m      \u001b[32m135.8586\u001b[0m  0.4846\n",
      "      4      \u001b[36m161.3612\u001b[0m      \u001b[32m160.8819\u001b[0m  0.4922\n",
      "      5      141.0154      141.4603  0.4381\n",
      "      5      \u001b[36m167.1313\u001b[0m      \u001b[32m168.1463\u001b[0m  0.4394\n",
      "      5      \u001b[36m136.8082\u001b[0m      \u001b[32m135.7336\u001b[0m  0.4385\n",
      "      5      \u001b[36m135.8563\u001b[0m      \u001b[32m135.7899\u001b[0m  0.4406\n",
      "      5      \u001b[36m161.1943\u001b[0m      160.8841  0.4402\n",
      "      6      \u001b[36m140.4425\u001b[0m      \u001b[32m140.4661\u001b[0m  0.4765\n",
      "      6      \u001b[36m167.0071\u001b[0m      168.1477  0.4752\n",
      "      6      \u001b[36m135.9018\u001b[0m      135.7554  0.4785\n",
      "      6      135.9295      136.2999  0.4748\n",
      "      6      161.1986      160.8883  0.4738\n",
      "      7      \u001b[36m140.3628\u001b[0m      \u001b[32m140.4634\u001b[0m  0.4449\n",
      "      7      167.0115      168.1514  0.4483\n",
      "      7      \u001b[36m135.7863\u001b[0m      135.7369  0.4480\n",
      "      7      136.1030      135.8937  0.4480\n",
      "      7      161.2006      160.8911  0.4505\n",
      "      8      140.3859      140.4668  0.4601\n",
      "      8      167.0146      168.1542  0.4556\n",
      "      8      135.8180      135.7574  0.4588\n",
      "      8      135.9241      135.8381  0.4540\n",
      "      8      161.2009      160.8926  0.4531\n",
      "      9      140.3641      140.4677  0.4380\n",
      "      9      167.0125      168.1550  0.4370\n",
      "      9      135.8064      135.7607  0.4390\n",
      "      9      \u001b[36m135.7687\u001b[0m      136.1839  0.4407\n",
      "      9      161.2003      160.8922  0.4409\n",
      "     10      140.3642      140.4692  0.4685\n",
      "     10      167.0136      168.1562  0.4657\n",
      "     10      135.8128      135.9969  0.4679\n",
      "     10      135.8072      \u001b[32m135.7564\u001b[0m  0.4662\n",
      "     10      161.1987      160.8919  0.4710\n",
      "     11      140.3643      140.4694  0.4669\n",
      "     11      167.0129      168.1570  0.4654\n",
      "     11      \u001b[36m135.7546\u001b[0m      135.7575  0.4700\n",
      "     11      135.8028      135.7625  0.4655\n",
      "     11      161.1978      160.8913  0.4628\n",
      "     12      140.3641      140.4695  0.4412\n",
      "     12      167.0115      168.1562  0.4395\n",
      "     12      135.8079      135.7626  0.4439\n",
      "     12      135.8199      135.7633  0.4398\n",
      "     12      161.1964      160.8907  0.4411\n",
      "     13      140.3637      140.4691  0.4450\n",
      "     13      167.0105      168.1560  0.4465\n",
      "     13      135.8136      135.7642  0.4484\n",
      "     13      135.8122      135.7653  0.4529\n",
      "     13      161.1955      160.8898  0.4516\n",
      "     14      \u001b[36m140.3624\u001b[0m      140.4689  0.4602\n",
      "     14      167.0098      168.1556  0.4624\n",
      "     14      135.8132      135.7638  0.4587\n",
      "     14      135.8129      135.8112  0.4582\n",
      "     14      161.1945      160.8891  0.4555\n",
      "     15      \u001b[36m140.3620\u001b[0m      140.4684  0.4398\n",
      "     15      167.0090      168.1555  0.4429\n",
      "     15      135.8127      135.7638  0.4482\n",
      "     15      135.8036      135.8035  0.4506\n",
      "     15      \u001b[36m161.1937\u001b[0m      160.8889  0.4464\n",
      "     16      \u001b[36m140.3615\u001b[0m      140.4679  0.4649\n",
      "     16      167.0082      168.1559  0.4620\n",
      "     16      135.8119      135.7638  0.4605\n",
      "     16      \u001b[36m161.1929\u001b[0m      160.8883  0.4544\n",
      "     16      135.8110      135.7621  0.4600\n",
      "     17      \u001b[36m140.3602\u001b[0m      140.4676  0.4399\n",
      "     17      167.0075      168.1627  0.4404\n",
      "     17      135.8118      135.7633  0.4390\n",
      "     17      135.8075      135.7702  0.4393\n",
      "     17      \u001b[36m161.1923\u001b[0m      160.8879  0.4402\n",
      "     18      \u001b[36m140.3600\u001b[0m      140.4670  0.4622\n",
      "     18      \u001b[36m167.0069\u001b[0m      168.1712  0.4557\n",
      "     18      135.8112      135.7629  0.4547\n",
      "     18      \u001b[36m161.1917\u001b[0m      160.8873  0.4548\n",
      "     18      135.8106      135.7635  0.4603\n",
      "     19      \u001b[36m140.3594\u001b[0m      140.4667  0.4374\n",
      "     19      \u001b[36m167.0063\u001b[0m      168.1715  0.4370\n",
      "     19      135.8106      135.7626  0.4383\n",
      "     19      \u001b[36m161.1912\u001b[0m      160.8868  0.4380\n",
      "     19      135.8249      \u001b[32m135.7151\u001b[0m  0.4400\n",
      "     20      \u001b[36m140.3590\u001b[0m      140.4663  0.4495\n",
      "     20      \u001b[36m167.0058\u001b[0m      168.1712  0.4411\n",
      "     20      135.8102      135.7623  0.4529\n",
      "     20      \u001b[36m161.1907\u001b[0m      160.8869  0.4478\n",
      "     20      135.8085      135.7574  0.4492\n",
      "     21      \u001b[36m140.3584\u001b[0m      140.4660  0.4662\n",
      "     21      \u001b[36m167.0053\u001b[0m      168.1708  0.4671\n",
      "     21      135.8097      135.7619  0.4664\n",
      "     21      \u001b[36m161.1902\u001b[0m      160.8862  0.4608\n",
      "     21      135.8086      135.7632  0.4636\n",
      "     22      \u001b[36m140.3580\u001b[0m      140.4654  0.4367\n",
      "     22      \u001b[36m167.0049\u001b[0m      168.1705  0.4387\n",
      "     22      135.8093      135.7615  0.4397\n",
      "     22      \u001b[36m161.1898\u001b[0m      160.8858  0.4384\n",
      "     22      135.8104      135.7624  0.4387\n",
      "     23      \u001b[36m140.3577\u001b[0m      140.4653  0.4352\n",
      "     23      \u001b[36m167.0046\u001b[0m      168.1701  0.4370\n",
      "     23      135.8089      135.7612  0.4350\n",
      "     23      \u001b[36m161.1895\u001b[0m      160.8853  0.4332\n",
      "     23      135.8097      135.7620  0.4330\n",
      "     24      \u001b[36m140.3573\u001b[0m      140.4650  0.4340\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 4.\n",
      "     24      135.8086      135.7612  0.4336\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 4.\n",
      "     24      135.8093      135.7617  0.4380\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     25      \u001b[36m140.3571\u001b[0m      140.4650  0.4288\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 5.\n",
      "     25      135.8089      135.7613  0.4236\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 6.\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "     26      135.8085      135.7610  0.4195\n",
      "     27      135.8081      135.7606  0.4378\n",
      "     28      135.8078      135.7601  0.4554\n",
      "     29      135.8074      135.7599  0.4593\n",
      "     30      135.8071      135.7597  0.4530\n",
      "     31      135.8068      135.7592  0.4309\n",
      "     32      135.8065      135.7593  0.4345\n",
      "     33      135.8063      135.7585  0.4483\n",
      "     34      135.8060      135.7584  0.4159\n",
      "     35      135.8058      135.7587  0.4451\n",
      "     36      135.8056      135.7582  0.4603\n",
      "     37      135.8055      135.7578  0.4536\n",
      "     38      135.8053      135.7578  0.4259\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 19.\n",
      "score functions types <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m161.3777\u001b[0m      \u001b[32m148.9589\u001b[0m  0.3686\n",
      "      2      \u001b[36m150.7480\u001b[0m      \u001b[32m146.9645\u001b[0m  0.3653\n",
      "      3      \u001b[36m149.1210\u001b[0m      \u001b[32m146.6136\u001b[0m  0.3588\n",
      "      4      \u001b[36m148.6359\u001b[0m      \u001b[32m146.5216\u001b[0m  0.3671\n",
      "      5      148.6545      \u001b[32m146.5184\u001b[0m  0.3529\n",
      "      6      148.6556      \u001b[32m146.4295\u001b[0m  0.3674\n",
      "      7      148.6615      146.4432  0.3596\n",
      "      8      148.6526      146.4428  0.3700\n",
      "      9      148.6562      146.4586  0.3711\n",
      "     10      148.6526      146.4606  0.3670\n",
      "     11      148.6524      146.4613  0.3597\n",
      "     12      148.6515      146.4608  0.3430\n",
      "     13      148.6509      146.4603  0.3749\n",
      "     14      148.6501      146.4599  0.3673\n",
      "     15      148.6494      146.4593  0.3565\n",
      "     16      148.6488      146.4589  0.3463\n",
      "     17      148.6483      146.4586  0.3639\n",
      "     18      148.6478      146.4583  0.3753\n",
      "     19      148.6474      146.4577  0.3530\n",
      "     20      148.6470      146.4581  0.3458\n",
      "     21      148.6467      146.4577  0.3695\n",
      "     22      148.6464      146.4575  0.3668\n",
      "     23      148.6460      146.4569  0.3529\n",
      "     24      148.6458      146.4566  0.3599\n",
      "     25      148.6456      146.4563  0.3678\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Restoring best model from epoch 6.\n",
      "durations 3.0 2029.0\n"
     ]
    }
   ],
   "source": [
    "data_set_fns = [load_metabric,  load_flchain, load_rgbsg, load_support] #, load_flchain, load_rgbsg, load_support, load_tcga]\n",
    "data_set_fns_str = ['load_metabric', 'load_flchain', 'load_rgbsg', 'load_support'] \n",
    "one_hot_dict = {'load_flchain': ['mgus'], 'load_support':['cancer'], 'load_rgbsg':['grade']}\n",
    "\n",
    "#n_iter = 10\n",
    "n_cuts = 10\n",
    "for idx, dataset in enumerate(data_set_fns):\n",
    "\n",
    "        model = '_deephit_'\n",
    "        num_durations = 10\n",
    "        data = dataset(path=\"/Users/JUSC/Documents/xgbsurv/xgbsurv/datasets/data/\", as_frame=True)\n",
    "        X  = data.data\n",
    "        y = data.target\n",
    "        if data_set_fns_str[idx] in one_hot_dict.keys():\n",
    "                X = pd.get_dummies(X, columns=one_hot_dict[data_set_fns_str[idx]])\n",
    "        filename = data.filename\n",
    "        dataset_name = filename.split('_')[0]\n",
    "        # add IBS later\n",
    "        outer_scores = {'cindex_train_'+dataset_name:[], 'cindex_test_'+dataset_name:[],\n",
    "                        'ibs_train_'+dataset_name:[], 'ibs_test_'+dataset_name:[]}\n",
    "        best_params = {'best_params_'+dataset_name:[]}\n",
    "        best_model = {'best_model_'+dataset_name:[]}\n",
    "        \n",
    "        ct = make_column_transformer(\n",
    "                (StandardScaler(), make_column_selector(dtype_include=['float32'])),\n",
    "                #(OneHotEncoder(sparse_output=False), make_column_selector(dtype_include=['category', 'object']))\n",
    "                remainder='passthrough')\n",
    "\n",
    "        # Deephit transformation\n",
    "        labtrans = DeepHitSingle.label_transform(num_durations)\n",
    "\n",
    "        for i, (train_index, test_index) in enumerate(outer_custom_cv.split(X, y)):\n",
    "                # Split data into training and testing sets for outer fold\n",
    "                X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "                y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "                # transform training data as in deephit implementation by Kvamme\n",
    "                time, event = transform_back(y_train.values)\n",
    "                y_train_inter = labtrans.fit_transform(time, event)\n",
    "                y_train_final = pd.Series(transform(y_train_inter[0],y_train_inter[1]))\n",
    "                cuts = torch.from_numpy(labtrans.cuts)\n",
    "                X_train, y_train = sort_X_y_pandas(X_train, y_train_final)\n",
    "                X_test, y_test = sort_X_y_pandas(X_test, y_test)\n",
    "\n",
    "                print('dtype X_train', X_train.dtypes)\n",
    "                print('dtype y_train', y_train.dtypes)\n",
    "                print('dtype X_test', X_test.dtypes)\n",
    "                print('dtype y_test', y_test.dtypes)\n",
    "                # save splits and data\n",
    "                savetxt('splits/train_index_'+str(i)+'_'+filename, train_index, delimiter=',')\n",
    "                savetxt('splits/test_index_'+str(i)+'_'+filename, test_index, delimiter=',')\n",
    "                \n",
    "                savetxt('splits/X_train_'+str(i)+'_'+filename, X_train, delimiter=',')\n",
    "                savetxt('splits/X_test_'+str(i)+'_'+filename, X_test, delimiter=',')\n",
    "\n",
    "                savetxt('splits/y_train_'+str(i)+'_'+filename, y_train, delimiter=',')\n",
    "                savetxt('splits/y_test_'+str(i)+'_'+filename, y_test, delimiter=',')\n",
    "                net = NeuralNet(\n",
    "                SurvivalModel, \n",
    "                module__n_layers = 1,\n",
    "                module__input_units = X.shape[1],\n",
    "                #module__idx_durations = \n",
    "                #module__num_nodes = 32,\n",
    "                #module__dropout = 0.1, # these could also be removed\n",
    "                module__out_features = labtrans.out_features,\n",
    "                # for split sizes when result size = 1\n",
    "                iterator_train__drop_last=True,\n",
    "                #iterator_valid__drop_last=True,\n",
    "                criterion=DeephitLoss(duration_bins=cuts),\n",
    "                optimizer=torch.optim.AdamW,\n",
    "                optimizer__weight_decay = 0.4,\n",
    "                batch_size=32, # separate train and valid->iterator_train__batch_size=128 and iterator_valid__batch_size=128 ?\n",
    "                callbacks=[\n",
    "                (\n",
    "                        \"sched\",\n",
    "                        LRScheduler(\n",
    "                        torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
    "                        monitor=\"valid_loss\",\n",
    "                        patience=5,\n",
    "                        ),\n",
    "                ),\n",
    "                (\n",
    "                        \"es\",\n",
    "                        EarlyStopping(\n",
    "                        monitor=\"valid_loss\",\n",
    "                        patience=20, # increase for deephit\n",
    "                        load_best=True,\n",
    "                        ),\n",
    "                ),\n",
    "                (\"seed\", FixSeed(seed=42)),\n",
    "                (\"Input Shape Setter\",InputShapeSetter())\n",
    "                ],\n",
    "                \n",
    "                #[EarlyStopping(patience=10)],\n",
    "                # add extensive callback, and random number seed\n",
    "                #TODO: enable stratification, verify\n",
    "                train_split=ValidSplit(0.2), # might cause lower performance in metrics, explain in thesis\n",
    "                #lr=0.001,\n",
    "                max_epochs=50, #0,#100\n",
    "                #train_split=None,\n",
    "                verbose=1\n",
    "                )\n",
    "                #strat = np.sign(y_train.values)\n",
    "                #valid_split = ValidSplit(cv=0.1, stratified=strat, random_state=42)\n",
    "                \n",
    "                pipe = Pipeline([('scaler',ct),\n",
    "                        ('estimator', net)])\n",
    "                \n",
    "                custom_scoring = partial(custom_scoring_function,duration_bins=cuts)\n",
    "                \n",
    "                rs = RandomizedSearchCV(pipe, param_grid_breslow, scoring = make_scorer(custom_scoring, greater_is_better=False), n_jobs=-1, \n",
    "                                    n_iter=2, refit=True)\n",
    "\n",
    "\n",
    "                rs.fit(X_train, y_train)\n",
    "                best_preds_train = rs.best_estimator_.predict(X_train)\n",
    "                best_preds_test = rs.best_estimator_.predict(X_test)\n",
    "                # save hyperparameter settings\n",
    "                params = rs.best_estimator_.get_params\n",
    "                best_params['best_params_'+dataset_name] += [rs.best_params_]\n",
    "                best_model['best_model_'+dataset_name] += [params]\n",
    "                surv_func_train = 1-np.cumsum(best_preds_train)\n",
    "\n",
    "                try:\n",
    "                    #use survival func formula here von kvamme paper\n",
    "\n",
    "                    df_survival_train = 1-np.cumsum(best_preds_train)\n",
    "                    durations_train, events_train = transform_back(y_train.values)\n",
    "                    time_grid_train = np.linspace(durations_train.min(), durations_train.max(), 100)\n",
    "                    ev = EvalSurv(df_survival_train, durations_train, events_train, censor_surv='km')\n",
    "                    print('Concordance Index',ev.concordance_td('antolini'))\n",
    "                    print('Integrated Brier Score:',ev.integrated_brier_score(time_grid_train))\n",
    "                    cindex_score_train = ev.concordance_td('antolini')\n",
    "                    ibs_score_train = ev.integrated_brier_score(time_grid_train)\n",
    "\n",
    "                    outer_scores['cindex_train_'+dataset_name] += [cindex_score_train]\n",
    "                    outer_scores['ibs_train_'+dataset_name] += [ibs_score_train]\n",
    "\n",
    "                except:\n",
    "                    outer_scores['cindex_train_'+dataset_name] += [np.nan]\n",
    "                    outer_scores['ibs_train_'+dataset_name] += [np.nan]\n",
    "                    \n",
    "                try:\n",
    "                    df_survival_test = 1-np.cumsum(best_preds_test)\n",
    "                    durations_test, events_test = transform_back(y_test.values)\n",
    "                    print('durations',durations_test.min(), durations_test.max())\n",
    "                    time_grid_test = np.linspace(durations_test.min(), durations_test.max(), 100)\n",
    "                    ev = EvalSurv(df_survival_test, durations_test, events_test, censor_surv='km')\n",
    "                    print('Concordance Index',ev.concordance_td('antolini'))\n",
    "                    print('Integrated Brier Score:',ev.integrated_brier_score(time_grid_test))\n",
    "                    cindex_score_test = ev.concordance_td('antolini')\n",
    "                    ibs_score_test = ev.integrated_brier_score(time_grid_test)\n",
    "\n",
    "                    outer_scores['cindex_test_'+dataset_name] += [cindex_score_test]\n",
    "                    outer_scores['ibs_test_'+dataset_name] += [ibs_score_test]\n",
    "                except: \n",
    "                    outer_scores['cindex_test_'+dataset_name] += [np.nan]\n",
    "                    outer_scores['ibs_test_'+dataset_name] += [np.nan]\n",
    "            \n",
    "        df_best_params = pd.DataFrame(best_params)\n",
    "        df_best_model = pd.DataFrame(best_model)\n",
    "        df_outer_scores = pd.DataFrame(outer_scores)\n",
    "        df_metrics = pd.concat([df_best_params,df_best_model,df_outer_scores], axis=1)\n",
    "        df_metrics.to_csv('metrics/metric_summary'+model+str(i)+'_'+filename, index=False)\n",
    "\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>8.940697e-07</td>\n",
       "      <td>4.768372e-07</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.999989</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>1.960993e-05</td>\n",
       "      <td>1.001358e-05</td>\n",
       "      <td>-1.192093e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.999996</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>6.377697e-06</td>\n",
       "      <td>3.397465e-06</td>\n",
       "      <td>1.192093e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.999984</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>2.932549e-05</td>\n",
       "      <td>1.525879e-05</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>2.026558e-06</td>\n",
       "      <td>1.072884e-06</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7094</th>\n",
       "      <td>0.999985</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>2.831221e-05</td>\n",
       "      <td>1.448393e-05</td>\n",
       "      <td>1.192093e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7095</th>\n",
       "      <td>0.999977</td>\n",
       "      <td>0.000158</td>\n",
       "      <td>0.000137</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>4.404783e-05</td>\n",
       "      <td>2.235174e-05</td>\n",
       "      <td>-2.384186e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7096</th>\n",
       "      <td>0.999977</td>\n",
       "      <td>0.000160</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>0.000119</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>4.446507e-05</td>\n",
       "      <td>2.259016e-05</td>\n",
       "      <td>5.960464e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7097</th>\n",
       "      <td>0.999977</td>\n",
       "      <td>0.000157</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>4.392862e-05</td>\n",
       "      <td>2.211332e-05</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7098</th>\n",
       "      <td>0.999973</td>\n",
       "      <td>0.000188</td>\n",
       "      <td>0.000162</td>\n",
       "      <td>0.000140</td>\n",
       "      <td>0.000117</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>5.072355e-05</td>\n",
       "      <td>2.622604e-05</td>\n",
       "      <td>-3.576279e-07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7099 rows  10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6  \\\n",
       "0     0.999999  0.000003  0.000003  0.000002  0.000002  0.000001  0.000001   \n",
       "1     0.999989  0.000070  0.000060  0.000051  0.000042  0.000034  0.000027   \n",
       "2     0.999996  0.000022  0.000020  0.000017  0.000014  0.000011  0.000009   \n",
       "3     0.999984  0.000104  0.000090  0.000077  0.000064  0.000052  0.000042   \n",
       "4     0.999999  0.000007  0.000006  0.000005  0.000004  0.000003  0.000003   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "7094  0.999985  0.000101  0.000087  0.000075  0.000062  0.000050  0.000040   \n",
       "7095  0.999977  0.000158  0.000137  0.000118  0.000098  0.000079  0.000063   \n",
       "7096  0.999977  0.000160  0.000139  0.000119  0.000099  0.000080  0.000063   \n",
       "7097  0.999977  0.000157  0.000136  0.000116  0.000097  0.000078  0.000062   \n",
       "7098  0.999973  0.000188  0.000162  0.000140  0.000117  0.000095  0.000074   \n",
       "\n",
       "                 7             8             9  \n",
       "0     8.940697e-07  4.768372e-07  0.000000e+00  \n",
       "1     1.960993e-05  1.001358e-05 -1.192093e-07  \n",
       "2     6.377697e-06  3.397465e-06  1.192093e-07  \n",
       "3     2.932549e-05  1.525879e-05  0.000000e+00  \n",
       "4     2.026558e-06  1.072884e-06  0.000000e+00  \n",
       "...            ...           ...           ...  \n",
       "7094  2.831221e-05  1.448393e-05  1.192093e-07  \n",
       "7095  4.404783e-05  2.235174e-05 -2.384186e-07  \n",
       "7096  4.446507e-05  2.259016e-05  5.960464e-08  \n",
       "7097  4.392862e-05  2.211332e-05  0.000000e+00  \n",
       "7098  5.072355e-05  2.622604e-05 -3.576279e-07  \n",
       "\n",
       "[7099 rows x 10 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "surv_func_train = 1-np.cumsum(best_preds_train, axis=1)\n",
    "pd.DataFrame(surv_func_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00000058, 0.999997  , 0.99999744, 0.99999785, 0.9999982 ,\n",
       "       0.9999985 , 0.9999988 , 0.9999991 , 0.9999995 , 1.        ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.set_printoptions(suppress=True)\n",
    "np.cumsum(best_preds_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00000134, 0.99999243, 0.9999933 , 0.9999943 , 0.99999523,\n",
       "       0.9999961 , 0.9999969 , 0.9999978 , 0.99999875, 0.9999999 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.cumsum(best_preds_test[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pass time classification bins in the loss class, add the bins to the loss init and add it to self, then in the forward pass it to the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_metabric\n",
      "(1903, 9) (1903,)\n",
      "split MKI67                float32\n",
      "EGFR                 float32\n",
      "PGR                  float32\n",
      "ERBB2                float32\n",
      "hormone_treatment    float32\n",
      "radiotherapy         float32\n",
      "chemotherapy         float32\n",
      "ER_positive          float32\n",
      "age                  float32\n",
      "dtype: object\n",
      "(1522, 9) <class 'pandas.core.frame.DataFrame'>\n",
      "(1522,) <class 'pandas.core.series.Series'>\n",
      "(381, 9) <class 'pandas.core.frame.DataFrame'>\n",
      "(381,) <class 'pandas.core.series.Series'>\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m395.7535\u001b[0m  0.0086\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m397.0852\u001b[0m  0.0092\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m351.2593\u001b[0m  0.0093\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m396.6680\u001b[0m  0.0130\n",
      "      2      395.7535  0.0074\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m397.2546\u001b[0m  0.0118\n",
      "      2      396.6680  0.0058\n",
      "      2      397.0852  0.0096\n",
      "      2      351.2593  0.0087\n",
      "      3      395.7535  0.0068\n",
      "      2      397.2546  0.0060\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m182.2325\u001b[0m      \u001b[32m172.0494\u001b[0m  0.0260\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m215.0037\u001b[0m      \u001b[32m175.8924\u001b[0m  0.0258\n",
      "      3      396.6680  0.0065\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m215.3274\u001b[0m      \u001b[32m171.8849\u001b[0m  0.0301\n",
      "      3      351.2593  0.0075\n",
      "      4      395.7535  0.0057\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m222.4758\u001b[0m      \u001b[32m180.4924\u001b[0m  0.0292\n",
      "      3      397.0852  0.0097\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m203.9800\u001b[0m      \u001b[32m188.1370\u001b[0m  0.0336\n",
      "      4      396.6680  0.0066\n",
      "      4      351.2593  0.0062\n",
      "      3      397.2546  0.0115\n",
      "      4      397.0852  0.0055\n",
      "      5      396.6680  0.0056\n",
      "      5      397.0852  0.0053\n",
      "      5      351.2593  0.0069\n",
      "      5      395.7535  0.0138\n",
      "      6      396.6680  0.0053\n",
      "      4      397.2546  0.0090\n",
      "      2      \u001b[36m190.0561\u001b[0m      \u001b[32m174.1224\u001b[0m  0.0206\n",
      "      6      397.0852  0.0053\n",
      "      6      351.2593  0.0052\n",
      "      2      \u001b[36m192.6442\u001b[0m      181.5122  0.0194\n",
      "      6      395.7535  0.0063\n",
      "      2      \u001b[36m158.0276\u001b[0m      \u001b[32m166.1558\u001b[0m  0.0266\n",
      "      7      397.0852  0.0051\n",
      "      2      \u001b[36m177.8027\u001b[0m      \u001b[32m187.8467\u001b[0m  0.0185\n",
      "      2      \u001b[36m190.2470\u001b[0m      172.8833  0.0252\n",
      "      7      396.6680  0.0084\n",
      "      5      397.2546  0.0084\n",
      "      7      395.7535  0.0054\n",
      "      7      351.2593  0.0091\n",
      "      8      396.6680  0.0054\n",
      "      8      395.7535  0.0057\n",
      "      6      397.2546  0.0075\n",
      "      8      351.2593  0.0056\n",
      "      8      397.0852  0.0096\n",
      "      7      397.2546  0.0053\n",
      "      3      \u001b[36m169.3818\u001b[0m      \u001b[32m169.5180\u001b[0m  0.0207\n",
      "      9      395.7535  0.0066\n",
      "      9      351.2593  0.0054\n",
      "      9      396.6680  0.0099\n",
      "      3      \u001b[36m148.8350\u001b[0m      \u001b[32m164.9565\u001b[0m  0.0192\n",
      "      3      \u001b[36m179.4172\u001b[0m      \u001b[32m178.2658\u001b[0m  0.0218\n",
      "      3      \u001b[36m163.9528\u001b[0m      \u001b[32m186.3672\u001b[0m  0.0193\n",
      "      8      397.2546  0.0052\n",
      "     10      395.7535  0.0054\n",
      "Restoring best model from epoch 1.\n",
      "     10      396.6680  0.0054\n",
      "      3      \u001b[36m173.4214\u001b[0m      \u001b[32m170.3292\u001b[0m  0.0215\n",
      "Restoring best model from epoch 1.\n",
      "      9      397.0852  0.0165\n",
      "      9      397.2546  0.0079\n",
      "     10      351.2593  0.0139\n",
      "Restoring best model from epoch 1.\n",
      "     10      397.2546  0.0058\n",
      "Restoring best model from epoch 1.\n",
      "     10      397.0852  0.0083\n",
      "Restoring best model from epoch 1.\n",
      "      4      \u001b[36m163.2407\u001b[0m      172.3501  0.0254\n",
      "      4      \u001b[36m143.2031\u001b[0m      165.2570  0.0256\n",
      "      4      \u001b[36m171.4196\u001b[0m      180.8890  0.0308\n",
      "      4      \u001b[36m159.0241\u001b[0m      \u001b[32m184.7348\u001b[0m  0.0310\n",
      "      4      \u001b[36m165.9876\u001b[0m      171.3337  0.0336\n",
      "      5      \u001b[36m156.0628\u001b[0m      176.2767  0.0283\n",
      "      5      \u001b[36m163.0911\u001b[0m      185.7525  0.0225\n",
      "      5      \u001b[36m137.7916\u001b[0m      171.2280  0.0288\n",
      "      5      \u001b[36m152.1663\u001b[0m      190.1453  0.0243\n",
      "      5      \u001b[36m159.5276\u001b[0m      174.8556  0.0256\n",
      "      6      \u001b[36m135.7838\u001b[0m      172.3343  0.0201\n",
      "      6      \u001b[36m147.8464\u001b[0m      192.5362  0.0211\n",
      "      6      \u001b[36m161.4202\u001b[0m      186.5061  0.0298\n",
      "      6      \u001b[36m154.3121\u001b[0m      175.5926  0.0352\n",
      "      6      \u001b[36m157.3638\u001b[0m      174.4598  0.0238\n",
      "      7      \u001b[36m132.4323\u001b[0m      176.5979  0.0209\n",
      "      7      \u001b[36m144.5970\u001b[0m      193.8677  0.0189\n",
      "      7      \u001b[36m156.6923\u001b[0m      188.7915  0.0191\n",
      "      7      \u001b[36m151.5220\u001b[0m      176.3137  0.0199\n",
      "      7      \u001b[36m154.3827\u001b[0m      176.0352  0.0195\n",
      "      8      \u001b[36m129.5305\u001b[0m      178.8751  0.0180\n",
      "      8      \u001b[36m139.2257\u001b[0m      196.9444  0.0189\n",
      "      8      \u001b[36m155.0135\u001b[0m      191.8247  0.0191\n",
      "      8      \u001b[36m147.8879\u001b[0m      177.7718  0.0189\n",
      "      8      \u001b[36m150.1857\u001b[0m      175.7091  0.0190\n",
      "      9      \u001b[36m125.3022\u001b[0m      184.9000  0.0192\n",
      "      9      \u001b[36m136.0371\u001b[0m      199.4112  0.0184\n",
      "      9      \u001b[36m149.8313\u001b[0m      194.8625  0.0184\n",
      "      9      \u001b[36m145.3147\u001b[0m      179.1975  0.0186\n",
      "      9      \u001b[36m147.5271\u001b[0m      177.0160  0.0187\n",
      "     10      \u001b[36m120.8972\u001b[0m      185.6802  0.0187\n",
      "Restoring best model from epoch 3.\n",
      "     10      136.4185      201.6519  0.0187\n",
      "Restoring best model from epoch 4.\n",
      "     10      \u001b[36m143.5342\u001b[0m      195.8945  0.0189\n",
      "Restoring best model from epoch 3.\n",
      "     10      \u001b[36m137.8136\u001b[0m      179.8485  0.0183\n",
      "Restoring best model from epoch 3.\n",
      "     10      \u001b[36m141.5959\u001b[0m      177.4762  0.0204\n",
      "Restoring best model from epoch 3.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m209.7029\u001b[0m      \u001b[32m183.7764\u001b[0m  0.0432\n",
      "      2      \u001b[36m186.6529\u001b[0m      183.8264  0.0267\n",
      "      3      \u001b[36m174.4472\u001b[0m      \u001b[32m179.6679\u001b[0m  0.0292\n",
      "      4      \u001b[36m166.2866\u001b[0m      182.5522  0.0287\n",
      "      5      \u001b[36m165.4196\u001b[0m      184.1559  0.0263\n",
      "      6      \u001b[36m160.7177\u001b[0m      184.6225  0.0251\n",
      "      7      \u001b[36m157.3839\u001b[0m      186.5138  0.0271\n",
      "      8      \u001b[36m155.6423\u001b[0m      190.0094  0.0287\n",
      "      9      \u001b[36m152.7815\u001b[0m      191.1123  0.0270\n",
      "     10      \u001b[36m148.7985\u001b[0m      192.1721  0.0257\n",
      "Restoring best model from epoch 3.\n",
      "(1522, 9) <class 'pandas.core.frame.DataFrame'>\n",
      "(1522,) <class 'pandas.core.series.Series'>\n",
      "(381, 9) <class 'pandas.core.frame.DataFrame'>\n",
      "(381,) <class 'pandas.core.series.Series'>\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m329.4160\u001b[0m  0.0029\n",
      "      2      329.4160  0.0026\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m392.1397\u001b[0m  0.0029\n",
      "      3      329.4160  0.0033\n",
      "      2      392.1397  0.0026\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m444.4271\u001b[0m  0.0031\n",
      "      4      329.4160  0.0027\n",
      "      3      392.1397  0.0025\n",
      "      5      329.4160  0.0026\n",
      "      4      392.1397  0.0025\n",
      "      2      444.4271  0.0033\n",
      "      6      329.4160  0.0026\n",
      "      5      392.1397  0.0025\n",
      "      3      444.4271  0.0026\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m207.9053\u001b[0m      \u001b[32m186.2364\u001b[0m  0.0385\n",
      "      7      329.4160  0.0025\n",
      "      6      392.1397  0.0025\n",
      "      4      444.4271  0.0025\n",
      "      7      392.1397  0.0025\n",
      "      8      329.4160  0.0025\n",
      "      5      444.4271  0.0028\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m447.9965\u001b[0m  0.0028\n",
      "      8      392.1397  0.0025\n",
      "      9      329.4160  0.0025\n",
      "      6      444.4271  0.0027\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m216.3753\u001b[0m      \u001b[32m210.1829\u001b[0m  0.0415\n",
      "      9      392.1397  0.0024\n",
      "     10      329.4160  0.0030\n",
      "Restoring best model from epoch 1.\n",
      "      7      444.4271  0.0026\n",
      "      2      447.9965  0.0035\n",
      "     10      392.1397  0.0024\n",
      "Restoring best model from epoch 1.\n",
      "      8      444.4271  0.0026\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m209.9245\u001b[0m      \u001b[32m206.5792\u001b[0m  0.0375\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m208.5203\u001b[0m      \u001b[32m200.9454\u001b[0m  0.0441\n",
      "      9      444.4271  0.0025\n",
      "      3      447.9965  0.0051\n",
      "      4      447.9965  0.0031\n",
      "     10      444.4271  0.0035\n",
      "Restoring best model from epoch 1.\n",
      "      5      447.9965  0.0026\n",
      "      6      447.9965  0.0025\n",
      "      7      447.9965  0.0025\n",
      "      8      447.9965  0.0026\n",
      "      9      447.9965  0.0029\n",
      "     10      447.9965  0.0029\n",
      "Restoring best model from epoch 1.\n",
      "      2      \u001b[36m194.6239\u001b[0m      \u001b[32m172.5185\u001b[0m  0.0397\n",
      "      2      \u001b[36m201.0578\u001b[0m      \u001b[32m193.7195\u001b[0m  0.0390\n",
      "      2      \u001b[36m192.3067\u001b[0m      \u001b[32m187.7439\u001b[0m  0.0382\n",
      "      2      \u001b[36m196.1358\u001b[0m      \u001b[32m195.1937\u001b[0m  0.0415\n",
      "      3      \u001b[36m186.1962\u001b[0m      175.2569  0.0396\n",
      "      3      \u001b[36m188.0626\u001b[0m      200.5593  0.0385\n",
      "      3      \u001b[36m182.3071\u001b[0m      196.8634  0.0380\n",
      "      3      \u001b[36m179.5698\u001b[0m      196.1772  0.0392\n",
      "      4      \u001b[36m179.0694\u001b[0m      179.1453  0.0380\n",
      "      4      \u001b[36m183.4675\u001b[0m      \u001b[32m192.7678\u001b[0m  0.0385\n",
      "      4      \u001b[36m175.5055\u001b[0m      191.1173  0.0388\n",
      "      4      \u001b[36m175.8959\u001b[0m      195.2839  0.0386\n",
      "      5      179.4140      179.2963  0.0381\n",
      "      5      \u001b[36m181.5170\u001b[0m      197.0452  0.0368\n",
      "      5      175.7364      193.9770  0.0371\n",
      "      5      177.3268      \u001b[32m193.7176\u001b[0m  0.0376\n",
      "      6      \u001b[36m177.3072\u001b[0m      178.5050  0.0371\n",
      "      6      \u001b[36m178.3038\u001b[0m      197.3905  0.0362\n",
      "      6      \u001b[36m175.3248\u001b[0m      188.5282  0.0372\n",
      "      6      \u001b[36m174.3499\u001b[0m      194.3041  0.0380\n",
      "      7      \u001b[36m174.9375\u001b[0m      179.4420  0.0383\n",
      "      7      179.3488      196.3506  0.0365\n",
      "      7      \u001b[36m171.6320\u001b[0m      188.1209  0.0382\n",
      "      7      \u001b[36m172.2349\u001b[0m      \u001b[32m190.8621\u001b[0m  0.0376\n",
      "      8      \u001b[36m173.1131\u001b[0m      181.4701  0.0385\n",
      "      8      \u001b[36m177.2073\u001b[0m      193.8199  0.0374\n",
      "      8      \u001b[36m169.2533\u001b[0m      \u001b[32m183.4970\u001b[0m  0.0377\n",
      "      8      \u001b[36m168.5089\u001b[0m      \u001b[32m187.6146\u001b[0m  0.0380\n",
      "      9      \u001b[36m169.7655\u001b[0m      180.7111  0.0371\n",
      "      9      \u001b[36m173.1445\u001b[0m      195.5822  0.0373\n",
      "      9      \u001b[36m165.0419\u001b[0m      185.8671  0.0370\n",
      "      9      \u001b[36m166.7682\u001b[0m      187.7887  0.0373\n",
      "     10      \u001b[36m169.6494\u001b[0m      180.1568  0.0379\n",
      "Restoring best model from epoch 2.\n",
      "     10      \u001b[36m170.6507\u001b[0m      196.5549  0.0370\n",
      "Restoring best model from epoch 4.\n",
      "     10      165.4874      187.3291  0.0388\n",
      "Restoring best model from epoch 8.\n",
      "     10      \u001b[36m166.6755\u001b[0m      190.4001  0.0379\n",
      "Restoring best model from epoch 8.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m443.7227\u001b[0m  0.0045\n",
      "      2      443.7227  0.0027\n",
      "      3      443.7227  0.0026\n",
      "      4      443.7227  0.0029\n",
      "      5      443.7227  0.0025\n",
      "      6      443.7227  0.0024\n",
      "      7      443.7227  0.0026\n",
      "      8      443.7227  0.0026\n",
      "      9      443.7227  0.0024\n",
      "     10      443.7227  0.0025\n",
      "Restoring best model from epoch 1.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m196.2156\u001b[0m      \u001b[32m156.5546\u001b[0m  0.0393\n",
      "      2      \u001b[36m178.0164\u001b[0m      \u001b[32m151.2487\u001b[0m  0.0373\n",
      "      3      \u001b[36m170.5913\u001b[0m      152.5802  0.0352\n",
      "      4      171.5452      152.1424  0.0347\n",
      "      5      \u001b[36m167.4776\u001b[0m      151.3359  0.0346\n",
      "      6      \u001b[36m163.6673\u001b[0m      152.1200  0.0350\n",
      "      7      \u001b[36m162.6043\u001b[0m      \u001b[32m149.9203\u001b[0m  0.0350\n",
      "      8      \u001b[36m161.7621\u001b[0m      \u001b[32m149.8566\u001b[0m  0.0345\n",
      "      9      \u001b[36m158.1050\u001b[0m      154.0798  0.0346\n",
      "     10      162.0815      152.5385  0.0343\n",
      "Restoring best model from epoch 8.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m209.7142\u001b[0m      \u001b[32m181.3312\u001b[0m  0.0543\n",
      "      2      \u001b[36m204.6959\u001b[0m      \u001b[32m176.6630\u001b[0m  0.0496\n",
      "      3      \u001b[36m190.4122\u001b[0m      \u001b[32m173.2234\u001b[0m  0.0513\n",
      "      4      \u001b[36m188.6366\u001b[0m      \u001b[32m173.1458\u001b[0m  0.0539\n",
      "      5      \u001b[36m187.7841\u001b[0m      174.4621  0.0490\n",
      "      6      \u001b[36m182.3226\u001b[0m      \u001b[32m173.0896\u001b[0m  0.0489\n",
      "      7      182.7245      176.6864  0.0481\n",
      "      8      \u001b[36m179.2689\u001b[0m      176.5540  0.0482\n",
      "      9      179.9014      175.6104  0.0510\n",
      "     10      \u001b[36m176.6401\u001b[0m      173.2858  0.0501\n",
      "Restoring best model from epoch 6.\n",
      "(1522, 9) <class 'pandas.core.frame.DataFrame'>\n",
      "(1522,) <class 'pandas.core.series.Series'>\n",
      "(381, 9) <class 'pandas.core.frame.DataFrame'>\n",
      "(381,) <class 'pandas.core.series.Series'>\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m338.0912\u001b[0m  0.0028\n",
      "      2      338.0912  0.0037\n",
      "      3      338.0912  0.0027\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m393.7112\u001b[0m  0.0031\n",
      "      4      338.0912  0.0026\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m188.9447\u001b[0m      \u001b[32m161.6644\u001b[0m  0.0390\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m438.3473\u001b[0m  0.0029\n",
      "      2      393.7112  0.0026\n",
      "      5      338.0912  0.0034\n",
      "      3      393.7112  0.0025\n",
      "      6      338.0912  0.0026\n",
      "      4      393.7112  0.0025\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m202.8561\u001b[0m      \u001b[32m186.1566\u001b[0m  0.0419\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m216.3638\u001b[0m      \u001b[32m204.5750\u001b[0m  0.0386\n",
      "      7      338.0912  0.0025\n",
      "      2      438.3473  0.0075\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m428.7114\u001b[0m  0.0029\n",
      "      3      438.3473  0.0025\n",
      "      5      393.7112  0.0057\n",
      "      8      338.0912  0.0035\n",
      "      4      438.3473  0.0025\n",
      "      6      393.7112  0.0026\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m210.5506\u001b[0m      \u001b[32m196.1124\u001b[0m  0.0409\n",
      "      2      428.7114  0.0072\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m459.7006\u001b[0m  0.0032\n",
      "      7      393.7112  0.0025\n",
      "      5      438.3473  0.0038\n",
      "      2      459.7006  0.0028\n",
      "      9      338.0912  0.0080\n",
      "      6      438.3473  0.0027\n",
      "      8      393.7112  0.0039\n",
      "      3      428.7114  0.0052\n",
      "      3      459.7006  0.0025\n",
      "      7      438.3473  0.0025\n",
      "      9      393.7112  0.0026\n",
      "      4      428.7114  0.0026\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m210.8362\u001b[0m      \u001b[32m209.7936\u001b[0m  0.0425\n",
      "      8      438.3473  0.0031\n",
      "     10      393.7112  0.0025\n",
      "      5      428.7114  0.0026\n",
      "Restoring best model from epoch 1.\n",
      "      4      459.7006  0.0040\n",
      "     10      338.0912  0.0070\n",
      "Restoring best model from epoch 1.\n",
      "      9      438.3473  0.0027\n",
      "      6      428.7114  0.0026\n",
      "      5      459.7006  0.0029\n",
      "     10      438.3473  0.0026\n",
      "Restoring best model from epoch 1.\n",
      "      7      428.7114  0.0025\n",
      "      6      459.7006  0.0026\n",
      "      8      428.7114  0.0025\n",
      "      7      459.7006  0.0028\n",
      "      9      428.7114  0.0025\n",
      "      8      459.7006  0.0027\n",
      "      2      \u001b[36m177.1327\u001b[0m      \u001b[32m156.7940\u001b[0m  0.0393\n",
      "     10      428.7114  0.0025\n",
      "Restoring best model from epoch 1.\n",
      "      9      459.7006  0.0025\n",
      "     10      459.7006  0.0025\n",
      "Restoring best model from epoch 1.\n",
      "      2      \u001b[36m190.9930\u001b[0m      \u001b[32m182.1425\u001b[0m  0.0391\n",
      "      2      \u001b[36m196.3215\u001b[0m      \u001b[32m195.4101\u001b[0m  0.0453\n",
      "      2      \u001b[36m190.7597\u001b[0m      \u001b[32m191.2909\u001b[0m  0.0400\n",
      "      2      \u001b[36m189.8050\u001b[0m      \u001b[32m200.6074\u001b[0m  0.0387\n",
      "      3      \u001b[36m167.8736\u001b[0m      \u001b[32m156.2443\u001b[0m  0.0393\n",
      "      3      \u001b[36m184.5309\u001b[0m      \u001b[32m175.5286\u001b[0m  0.0399\n",
      "      3      \u001b[36m188.0725\u001b[0m      \u001b[32m192.7828\u001b[0m  0.0394\n",
      "      3      \u001b[36m185.3034\u001b[0m      \u001b[32m179.5244\u001b[0m  0.0407\n",
      "      3      \u001b[36m183.7074\u001b[0m      \u001b[32m197.8966\u001b[0m  0.0395\n",
      "      4      \u001b[36m163.4744\u001b[0m      159.0793  0.0402\n",
      "      4      \u001b[36m175.6807\u001b[0m      177.5917  0.0405\n",
      "      4      \u001b[36m184.3118\u001b[0m      \u001b[32m190.0219\u001b[0m  0.0394\n",
      "      4      \u001b[36m178.2458\u001b[0m      182.3931  0.0397\n",
      "      4      \u001b[36m175.0330\u001b[0m      \u001b[32m195.2926\u001b[0m  0.0389\n",
      "      5      163.9865      160.9310  0.0381\n",
      "      5      177.9489      176.1551  0.0372\n",
      "      5      \u001b[36m180.9074\u001b[0m      192.1166  0.0369\n",
      "      5      \u001b[36m171.7469\u001b[0m      \u001b[32m179.2142\u001b[0m  0.0371\n",
      "      5      \u001b[36m173.0186\u001b[0m      \u001b[32m192.2954\u001b[0m  0.0368\n",
      "      6      \u001b[36m159.8746\u001b[0m      159.3902  0.0373\n",
      "      6      \u001b[36m172.1584\u001b[0m      \u001b[32m175.1040\u001b[0m  0.0369\n",
      "      6      181.3274      192.7905  0.0375\n",
      "      6      173.7954      181.1819  0.0380\n",
      "      6      \u001b[36m170.0936\u001b[0m      \u001b[32m190.6170\u001b[0m  0.0403\n",
      "      7      \u001b[36m156.1901\u001b[0m      158.7202  0.0354\n",
      "      7      \u001b[36m171.6114\u001b[0m      177.6985  0.0376\n",
      "      7      \u001b[36m176.6210\u001b[0m      193.4692  0.0368\n",
      "      7      \u001b[36m167.9350\u001b[0m      183.5993  0.0369\n",
      "      7      \u001b[36m167.3895\u001b[0m      193.9633  0.0365\n",
      "      8      156.7194      158.2644  0.0357\n",
      "      8      \u001b[36m169.9070\u001b[0m      177.3602  0.0373\n",
      "      8      \u001b[36m175.1405\u001b[0m      191.8768  0.0370\n",
      "      8      \u001b[36m166.3073\u001b[0m      182.3830  0.0376\n",
      "      8      \u001b[36m163.6969\u001b[0m      196.4294  0.0362\n",
      "      9      \u001b[36m155.1185\u001b[0m      157.0691  0.0360\n",
      "      9      \u001b[36m169.8147\u001b[0m      175.9988  0.0366\n",
      "      9      \u001b[36m171.7228\u001b[0m      191.1002  0.0367\n",
      "      9      \u001b[36m165.9090\u001b[0m      179.6163  0.0364\n",
      "      9      165.0031      196.0685  0.0370\n",
      "     10      \u001b[36m150.6232\u001b[0m      156.7840  0.0366\n",
      "Restoring best model from epoch 3.\n",
      "     10      \u001b[36m167.2674\u001b[0m      178.3711  0.0371\n",
      "Restoring best model from epoch 6.\n",
      "     10      172.1841      191.4933  0.0366\n",
      "Restoring best model from epoch 4.\n",
      "     10      \u001b[36m164.4993\u001b[0m      \u001b[32m178.3370\u001b[0m  0.0376\n",
      "     10      164.6806      194.8795  0.0375\n",
      "Restoring best model from epoch 6.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m213.1587\u001b[0m      \u001b[32m174.0612\u001b[0m  0.0502\n",
      "      2      \u001b[36m200.3133\u001b[0m      \u001b[32m169.0424\u001b[0m  0.0503\n",
      "      3      \u001b[36m187.6337\u001b[0m      173.0593  0.0488\n",
      "      4      188.9193      \u001b[32m168.4600\u001b[0m  0.0494\n",
      "      5      \u001b[36m186.5069\u001b[0m      169.6748  0.0465\n",
      "      6      \u001b[36m184.4309\u001b[0m      \u001b[32m167.4841\u001b[0m  0.0472\n",
      "      7      \u001b[36m180.3782\u001b[0m      168.4911  0.0483\n",
      "      8      \u001b[36m179.1805\u001b[0m      170.1799  0.0479\n",
      "      9      179.4075      169.1494  0.0496\n",
      "     10      \u001b[36m178.7040\u001b[0m      170.4560  0.0475\n",
      "Restoring best model from epoch 6.\n",
      "(1523, 9) <class 'pandas.core.frame.DataFrame'>\n",
      "(1523,) <class 'pandas.core.series.Series'>\n",
      "(380, 9) <class 'pandas.core.frame.DataFrame'>\n",
      "(380,) <class 'pandas.core.series.Series'>\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m360.9350\u001b[0m  0.0032\n",
      "      2      360.9350  0.0028\n",
      "      3      360.9350  0.0026\n",
      "      4      360.9350  0.0025\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m399.4638\u001b[0m  0.0030\n",
      "      5      360.9350  0.0038\n",
      "      2      399.4638  0.0036\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m406.9466\u001b[0m  0.0029\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m182.6559\u001b[0m      \u001b[32m172.0761\u001b[0m  0.0433\n",
      "      3      399.4638  0.0027\n",
      "      2      406.9466  0.0026\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m204.6330\u001b[0m      \u001b[32m191.6336\u001b[0m  0.0406\n",
      "      4      399.4638  0.0026\n",
      "      6      360.9350  0.0069\n",
      "      3      406.9466  0.0038\n",
      "      5      399.4638  0.0025\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m409.0932\u001b[0m  0.0028\n",
      "      4      406.9466  0.0026\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m217.7549\u001b[0m      \u001b[32m192.2383\u001b[0m  0.0419\n",
      "      6      399.4638  0.0025\n",
      "      2      409.0932  0.0027\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m423.5490\u001b[0m  0.0028\n",
      "      5      406.9466  0.0025\n",
      "      7      399.4638  0.0025\n",
      "      7      360.9350  0.0084\n",
      "      3      409.0932  0.0046\n",
      "      2      423.5490  0.0050\n",
      "      6      406.9466  0.0040\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m219.5474\u001b[0m      \u001b[32m195.8659\u001b[0m  0.0377\n",
      "      8      360.9350  0.0027\n",
      "      4      409.0932  0.0026\n",
      "      3      423.5490  0.0026\n",
      "      7      406.9466  0.0026\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m210.6865\u001b[0m      \u001b[32m189.0066\u001b[0m  0.0440\n",
      "      9      360.9350  0.0025\n",
      "      5      409.0932  0.0025\n",
      "      8      399.4638  0.0025\n",
      "      4      423.5490  0.0025\n",
      "      8      406.9466  0.0025\n",
      "     10      360.9350  0.0031\n",
      "Restoring best model from epoch 1.\n",
      "      6      409.0932  0.0026\n",
      "      9      399.4638  0.0027\n",
      "      9      406.9466  0.0029\n",
      "      7      409.0932  0.0025\n",
      "     10      399.4638  0.0026\n",
      "Restoring best model from epoch 1.\n",
      "     10      406.9466  0.0025\n",
      "Restoring best model from epoch 1.\n",
      "      5      423.5490  0.0061\n",
      "      8      409.0932  0.0025\n",
      "      6      423.5490  0.0026\n",
      "      9      409.0932  0.0030\n",
      "      7      423.5490  0.0031\n",
      "     10      409.0932  0.0027\n",
      "Restoring best model from epoch 1.\n",
      "      8      423.5490  0.0031\n",
      "      9      423.5490  0.0026\n",
      "     10      423.5490  0.0026\n",
      "Restoring best model from epoch 1.\n",
      "      2      \u001b[36m187.3077\u001b[0m      192.4752  0.0431\n",
      "      2      \u001b[36m173.2808\u001b[0m      173.1341  0.0466\n",
      "      2      \u001b[36m198.6503\u001b[0m      \u001b[32m190.4339\u001b[0m  0.0411\n",
      "      2      \u001b[36m191.3383\u001b[0m      \u001b[32m192.4195\u001b[0m  0.0441\n",
      "      2      \u001b[36m188.2331\u001b[0m      \u001b[32m178.5508\u001b[0m  0.0423\n",
      "      3      \u001b[36m178.9104\u001b[0m      199.5681  0.0378\n",
      "      3      \u001b[36m163.6798\u001b[0m      \u001b[32m170.9013\u001b[0m  0.0386\n",
      "      3      \u001b[36m192.5226\u001b[0m      191.0461  0.0384\n",
      "      3      \u001b[36m184.5575\u001b[0m      195.3374  0.0386\n",
      "      3      \u001b[36m183.6668\u001b[0m      179.9277  0.0386\n",
      "      4      \u001b[36m178.0269\u001b[0m      196.1013  0.0373\n",
      "      4      \u001b[36m161.0141\u001b[0m      172.0152  0.0381\n",
      "      4      \u001b[36m187.7070\u001b[0m      \u001b[32m187.0372\u001b[0m  0.0382\n",
      "      4      \u001b[36m179.5390\u001b[0m      \u001b[32m186.8593\u001b[0m  0.0388\n",
      "      4      \u001b[36m179.2244\u001b[0m      178.9310  0.0385\n",
      "      5      \u001b[36m174.1203\u001b[0m      192.0746  0.0373\n",
      "      5      \u001b[36m159.0408\u001b[0m      174.2166  0.0378\n",
      "      5      \u001b[36m183.8719\u001b[0m      188.1767  0.0372\n",
      "      5      \u001b[36m177.6590\u001b[0m      187.5503  0.0364\n",
      "      5      \u001b[36m176.4972\u001b[0m      \u001b[32m178.3390\u001b[0m  0.0365\n",
      "      6      \u001b[36m169.7830\u001b[0m      195.6343  0.0374\n",
      "      6      \u001b[36m157.6065\u001b[0m      175.7348  0.0367\n",
      "      6      \u001b[36m182.0896\u001b[0m      \u001b[32m187.0371\u001b[0m  0.0357\n",
      "      6      \u001b[36m177.0671\u001b[0m      \u001b[32m185.8835\u001b[0m  0.0368\n",
      "      6      176.6530      \u001b[32m177.8917\u001b[0m  0.0366\n",
      "      7      171.7504      195.9663  0.0379\n",
      "      7      \u001b[36m155.5329\u001b[0m      177.3787  0.0373\n",
      "      7      \u001b[36m180.0821\u001b[0m      187.3072  0.0369\n",
      "      7      \u001b[36m175.9141\u001b[0m      186.9843  0.0367\n",
      "      7      \u001b[36m176.2342\u001b[0m      178.8624  0.0360\n",
      "      8      \u001b[36m165.1794\u001b[0m      195.3149  0.0366\n",
      "      8      \u001b[36m155.3239\u001b[0m      176.0199  0.0373\n",
      "      8      \u001b[36m176.9697\u001b[0m      187.3357  0.0362\n",
      "      8      \u001b[36m172.3530\u001b[0m      \u001b[32m185.0020\u001b[0m  0.0369\n",
      "      8      \u001b[36m170.9404\u001b[0m      180.6153  0.0374\n",
      "      9      \u001b[36m164.9077\u001b[0m      195.8215  0.0364\n",
      "      9      \u001b[36m150.3192\u001b[0m      177.5112  0.0367\n",
      "      9      \u001b[36m175.4074\u001b[0m      190.3542  0.0367\n",
      "      9      \u001b[36m169.2174\u001b[0m      186.1346  0.0354\n",
      "      9      \u001b[36m165.5485\u001b[0m      181.0513  0.0364\n",
      "     10      165.2407      196.2389  0.0375\n",
      "Restoring best model from epoch 1.\n",
      "     10      \u001b[36m150.0961\u001b[0m      176.4209  0.0361\n",
      "Restoring best model from epoch 3.\n",
      "     10      175.9051      188.5406  0.0362\n",
      "Restoring best model from epoch 4.\n",
      "     10      172.7626      187.2122  0.0372\n",
      "Restoring best model from epoch 8.\n",
      "     10      167.2171      181.8021  0.0379\n",
      "Restoring best model from epoch 6.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m210.3090\u001b[0m      \u001b[32m180.9442\u001b[0m  0.0530\n",
      "      2      \u001b[36m197.3117\u001b[0m      181.8197  0.0511\n",
      "      3      \u001b[36m192.0940\u001b[0m      183.4528  0.0496\n",
      "      4      \u001b[36m185.4313\u001b[0m      \u001b[32m180.7330\u001b[0m  0.0474\n",
      "      5      \u001b[36m184.4475\u001b[0m      \u001b[32m180.7086\u001b[0m  0.0563\n",
      "      6      \u001b[36m181.4502\u001b[0m      \u001b[32m179.7770\u001b[0m  0.0509\n",
      "      7      183.5706      181.5156  0.0490\n",
      "      8      \u001b[36m179.9662\u001b[0m      181.5988  0.0486\n",
      "      9      \u001b[36m177.4051\u001b[0m      181.5006  0.0775\n",
      "     10      177.8211      182.8256  0.0486\n",
      "Restoring best model from epoch 6.\n",
      "(1523, 9) <class 'pandas.core.frame.DataFrame'>\n",
      "(1523,) <class 'pandas.core.series.Series'>\n",
      "(380, 9) <class 'pandas.core.frame.DataFrame'>\n",
      "(380,) <class 'pandas.core.series.Series'>\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m357.6868\u001b[0m  0.0030\n",
      "      2      357.6868  0.0034\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m405.7148\u001b[0m  0.0031\n",
      "      3      357.6868  0.0027\n",
      "      2      405.7148  0.0028\n",
      "      4      357.6868  0.0026\n",
      "      3      405.7148  0.0025\n",
      "      5      357.6868  0.0025\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m432.2415\u001b[0m  0.0031\n",
      "      4      405.7148  0.0036\n",
      "      6      357.6868  0.0026\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m182.2308\u001b[0m      \u001b[32m166.7411\u001b[0m  0.0405\n",
      "      2      432.2415  0.0027\n",
      "      5      405.7148  0.0026\n",
      "      7      357.6868  0.0025\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m205.3985\u001b[0m      \u001b[32m190.7804\u001b[0m  0.0390\n",
      "      6      405.7148  0.0025\n",
      "      3      432.2415  0.0035\n",
      "      8      357.6868  0.0025\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m214.7277\u001b[0m      \u001b[32m199.3876\u001b[0m  0.0381\n",
      "      7      405.7148  0.0030\n",
      "      9      357.6868  0.0025\n",
      "      4      432.2415  0.0050\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m406.2276\u001b[0m  0.0031\n",
      "      8      405.7148  0.0025\n",
      "     10      357.6868  0.0025\n",
      "Restoring best model from epoch 1.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m416.0956\u001b[0m  0.0028\n",
      "      5      432.2415  0.0026\n",
      "      9      405.7148  0.0025\n",
      "      2      406.2276  0.0028\n",
      "     10      405.7148  0.0025\n",
      "Restoring best model from epoch 1.\n",
      "      2      416.0956  0.0040\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m214.5832\u001b[0m      \u001b[32m187.8452\u001b[0m  0.0388\n",
      "      3      406.2276  0.0037\n",
      "      6      432.2415  0.0045\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m218.7271\u001b[0m      \u001b[32m183.9210\u001b[0m  0.0442\n",
      "      3      416.0956  0.0026\n",
      "      7      432.2415  0.0025\n",
      "      4      406.2276  0.0031\n",
      "      4      416.0956  0.0025\n",
      "      8      432.2415  0.0025\n",
      "      5      406.2276  0.0026\n",
      "      5      416.0956  0.0031\n",
      "      9      432.2415  0.0026\n",
      "      6      406.2276  0.0025\n",
      "      6      416.0956  0.0026\n",
      "     10      432.2415  0.0025\n",
      "Restoring best model from epoch 1.\n",
      "      7      406.2276  0.0026\n",
      "      7      416.0956  0.0025\n",
      "      8      406.2276  0.0025\n",
      "      8      416.0956  0.0025\n",
      "      9      406.2276  0.0025\n",
      "      9      416.0956  0.0025\n",
      "     10      406.2276  0.0025\n",
      "Restoring best model from epoch 1.\n",
      "     10      416.0956  0.0025\n",
      "Restoring best model from epoch 1.\n",
      "      2      \u001b[36m200.9982\u001b[0m      \u001b[32m194.3264\u001b[0m  0.0390\n",
      "      2      \u001b[36m192.1695\u001b[0m      \u001b[32m183.8446\u001b[0m  0.0434\n",
      "      2      \u001b[36m176.5276\u001b[0m      \u001b[32m159.6159\u001b[0m  0.0510\n",
      "      2      \u001b[36m198.4657\u001b[0m      \u001b[32m187.1711\u001b[0m  0.0388\n",
      "      2      \u001b[36m196.0498\u001b[0m      \u001b[32m174.0531\u001b[0m  0.0397\n",
      "      3      \u001b[36m187.9804\u001b[0m      \u001b[32m192.3779\u001b[0m  0.0376\n",
      "      3      \u001b[36m181.1267\u001b[0m      \u001b[32m182.6788\u001b[0m  0.0380\n",
      "      3      \u001b[36m164.8093\u001b[0m      \u001b[32m157.8024\u001b[0m  0.0382\n",
      "      3      \u001b[36m187.7646\u001b[0m      \u001b[32m178.1160\u001b[0m  0.0381\n",
      "      3      \u001b[36m185.0405\u001b[0m      \u001b[32m173.6771\u001b[0m  0.0366\n",
      "      4      \u001b[36m187.0974\u001b[0m      \u001b[32m189.8664\u001b[0m  0.0377\n",
      "      4      \u001b[36m176.3577\u001b[0m      \u001b[32m180.5222\u001b[0m  0.0369\n",
      "      4      165.0995      \u001b[32m157.3349\u001b[0m  0.0377\n",
      "      4      \u001b[36m178.8191\u001b[0m      \u001b[32m176.3739\u001b[0m  0.0377\n",
      "      4      \u001b[36m183.6019\u001b[0m      \u001b[32m172.4729\u001b[0m  0.0374\n",
      "      5      178.4128      181.5503  0.0370\n",
      "      5      \u001b[36m185.6326\u001b[0m      \u001b[32m189.1776\u001b[0m  0.0374\n",
      "      5      \u001b[36m162.5460\u001b[0m      \u001b[32m156.3764\u001b[0m  0.0370\n",
      "      5      \u001b[36m177.5939\u001b[0m      176.9541  0.0376\n",
      "      5      \u001b[36m177.1227\u001b[0m      172.8995  0.0373\n",
      "      6      \u001b[36m176.3498\u001b[0m      180.5867  0.0370\n",
      "      6      \u001b[36m179.1753\u001b[0m      \u001b[32m188.1198\u001b[0m  0.0366\n",
      "      6      \u001b[36m160.7396\u001b[0m      159.3609  0.0370\n",
      "      6      \u001b[36m174.3028\u001b[0m      176.6626  0.0370\n",
      "      6      \u001b[36m175.1810\u001b[0m      172.4966  0.0370\n",
      "      7      \u001b[36m170.7340\u001b[0m      181.8113  0.0361\n",
      "      7      \u001b[36m175.8636\u001b[0m      189.0511  0.0373\n",
      "      7      \u001b[36m158.8368\u001b[0m      161.1473  0.0368\n",
      "      7      \u001b[36m173.8092\u001b[0m      \u001b[32m174.4802\u001b[0m  0.0365\n",
      "      7      \u001b[36m170.6952\u001b[0m      \u001b[32m169.6402\u001b[0m  0.0366\n",
      "      8      172.0439      \u001b[32m178.8823\u001b[0m  0.0365\n",
      "      8      177.0638      \u001b[32m187.3003\u001b[0m  0.0367\n",
      "      8      \u001b[36m157.8002\u001b[0m      159.5712  0.0371\n",
      "      8      \u001b[36m169.9797\u001b[0m      \u001b[32m169.0590\u001b[0m  0.0367\n",
      "      8      \u001b[36m172.5854\u001b[0m      \u001b[32m172.5744\u001b[0m  0.0375\n",
      "      9      \u001b[36m169.9369\u001b[0m      \u001b[32m178.3779\u001b[0m  0.0373\n",
      "      9      \u001b[36m174.9034\u001b[0m      190.7290  0.0367\n",
      "      9      157.8036      160.5785  0.0369\n",
      "      9      \u001b[36m170.4085\u001b[0m      174.1692  0.0358\n",
      "      9      170.4530      170.7777  0.0371\n",
      "     10      \u001b[36m169.7972\u001b[0m      180.7457  0.0371\n",
      "Restoring best model from epoch 9.\n",
      "     10      176.6589      190.4016  0.0373\n",
      "Restoring best model from epoch 8.\n",
      "     10      \u001b[36m155.8259\u001b[0m      161.3433  0.0369\n",
      "Restoring best model from epoch 5.\n",
      "     10      \u001b[36m169.5782\u001b[0m      177.0680  0.0365\n",
      "Restoring best model from epoch 8.\n",
      "     10      \u001b[36m167.7676\u001b[0m      173.4528  0.0370\n",
      "Restoring best model from epoch 8.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m213.0166\u001b[0m      \u001b[32m181.4493\u001b[0m  0.0507\n",
      "      2      \u001b[36m196.4637\u001b[0m      185.4528  0.0498\n",
      "      3      \u001b[36m190.4209\u001b[0m      \u001b[32m177.6633\u001b[0m  0.0485\n",
      "      4      \u001b[36m185.2077\u001b[0m      \u001b[32m177.4383\u001b[0m  0.1937\n",
      "      5      \u001b[36m184.7025\u001b[0m      \u001b[32m177.3431\u001b[0m  0.0632\n",
      "      6      \u001b[36m181.2402\u001b[0m      \u001b[32m176.4497\u001b[0m  0.0494\n",
      "      7      181.9632      \u001b[32m174.4172\u001b[0m  0.0476\n",
      "      8      \u001b[36m179.4701\u001b[0m      174.8774  0.0504\n",
      "      9      \u001b[36m178.0746\u001b[0m      176.7135  0.0494\n",
      "     10      \u001b[36m175.8357\u001b[0m      177.7229  0.0467\n",
      "Restoring best model from epoch 7.\n",
      "load_flchain\n",
      "(7871, 9) (7871,)\n",
      "split age            float32\n",
      "sex           category\n",
      "sample_yr      float32\n",
      "kappa          float32\n",
      "lambda         float32\n",
      "flc_grp        float32\n",
      "creatinine     float32\n",
      "mgus_0.0         uint8\n",
      "mgus_1.0         uint8\n",
      "dtype: object\n",
      "(6296, 9) <class 'pandas.core.frame.DataFrame'>\n",
      "(6296,) <class 'pandas.core.series.Series'>\n",
      "(1575, 9) <class 'pandas.core.frame.DataFrame'>\n",
      "(1575,) <class 'pandas.core.series.Series'>\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m766.0597\u001b[0m      \u001b[32m698.7053\u001b[0m  0.0663\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1     \u001b[36m1216.4105\u001b[0m     \u001b[32m1173.8993\u001b[0m  0.0686\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1     \u001b[36m1521.6481\u001b[0m     \u001b[32m1412.0698\u001b[0m  0.0761\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1     \u001b[36m1515.4929\u001b[0m     \u001b[32m1404.0526\u001b[0m  0.0748\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1     \u001b[36m1536.2428\u001b[0m     \u001b[32m1433.9532\u001b[0m  0.0657\n",
      "      2      \u001b[36m626.1289\u001b[0m      \u001b[32m652.7825\u001b[0m  0.0697\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m84.6702\u001b[0m       \u001b[32m72.5034\u001b[0m  0.1830\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m116.9417\u001b[0m       \u001b[32m99.8174\u001b[0m  0.1788\n",
      "      2     \u001b[36m1148.6772\u001b[0m     \u001b[32m1238.3168\u001b[0m  0.0603\n",
      "      2      \u001b[36m883.0587\u001b[0m      \u001b[32m979.1885\u001b[0m  0.0824\n",
      "      2     \u001b[36m1236.6897\u001b[0m     \u001b[32m1353.2991\u001b[0m  0.0665\n",
      "      2     \u001b[36m1217.5094\u001b[0m     \u001b[32m1307.4551\u001b[0m  0.0706\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m150.8396\u001b[0m      \u001b[32m136.8997\u001b[0m  0.1930\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m160.8661\u001b[0m      \u001b[32m146.3403\u001b[0m  0.1912\n",
      "      3      \u001b[36m595.1493\u001b[0m      \u001b[32m619.7230\u001b[0m  0.0668\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m161.3594\u001b[0m      \u001b[32m139.8466\u001b[0m  0.2178\n",
      "      3     \u001b[36m1177.8276\u001b[0m     \u001b[32m1228.0787\u001b[0m  0.0566\n",
      "      3      \u001b[36m837.8018\u001b[0m      \u001b[32m878.5527\u001b[0m  0.0700\n",
      "      3     \u001b[36m1099.2381\u001b[0m     \u001b[32m1139.6620\u001b[0m  0.0822\n",
      "      3     \u001b[36m1178.3973\u001b[0m     \u001b[32m1241.7499\u001b[0m  0.0708\n",
      "      4      \u001b[36m583.4152\u001b[0m      \u001b[32m595.8721\u001b[0m  0.0575\n",
      "      4      \u001b[36m812.1325\u001b[0m      \u001b[32m835.5326\u001b[0m  0.0671\n",
      "      4     \u001b[36m1068.6327\u001b[0m     \u001b[32m1106.3442\u001b[0m  0.0666\n",
      "      4     \u001b[36m1153.7856\u001b[0m     \u001b[32m1163.9419\u001b[0m  0.0759\n",
      "      4     \u001b[36m1148.1483\u001b[0m     \u001b[32m1181.4498\u001b[0m  0.0828\n",
      "      5      \u001b[36m572.3185\u001b[0m      \u001b[32m579.8968\u001b[0m  0.0725\n",
      "      2       \u001b[36m77.5477\u001b[0m       72.5171  0.1858\n",
      "      2      \u001b[36m104.9077\u001b[0m       \u001b[32m97.5341\u001b[0m  0.1894\n",
      "      5      \u001b[36m797.4932\u001b[0m      \u001b[32m800.5925\u001b[0m  0.0695\n",
      "      2      \u001b[36m147.0729\u001b[0m      \u001b[32m145.4244\u001b[0m  0.1882\n",
      "      2      \u001b[36m138.5823\u001b[0m      \u001b[32m132.8372\u001b[0m  0.2055\n",
      "      5     \u001b[36m1064.0288\u001b[0m     \u001b[32m1073.8062\u001b[0m  0.0772\n",
      "      2      \u001b[36m148.2435\u001b[0m      \u001b[32m138.5172\u001b[0m  0.1821\n",
      "      5     \u001b[36m1146.2944\u001b[0m     \u001b[32m1137.7965\u001b[0m  0.0822\n",
      "      5     \u001b[36m1136.6949\u001b[0m     \u001b[32m1159.8138\u001b[0m  0.0690\n",
      "      6      \u001b[36m564.2665\u001b[0m      582.7234  0.0681\n",
      "      6      \u001b[36m779.2885\u001b[0m      \u001b[32m786.4662\u001b[0m  0.0604\n",
      "      6     \u001b[36m1124.6168\u001b[0m     1168.9113  0.0562\n",
      "      6     \u001b[36m1138.5282\u001b[0m     1143.9260  0.0640\n",
      "      6     \u001b[36m1051.8122\u001b[0m     \u001b[32m1061.2714\u001b[0m  0.0727\n",
      "      7      \u001b[36m559.8835\u001b[0m      \u001b[32m579.4997\u001b[0m  0.0601\n",
      "      7      \u001b[36m774.4016\u001b[0m      \u001b[32m772.4856\u001b[0m  0.0643\n",
      "      7     \u001b[36m1125.6783\u001b[0m     1143.1077  0.0605\n",
      "      7     \u001b[36m1040.1340\u001b[0m     \u001b[32m1059.3264\u001b[0m  0.0625\n",
      "      7     \u001b[36m1112.2639\u001b[0m     1164.9591  0.0766\n",
      "      8      \u001b[36m549.4900\u001b[0m      \u001b[32m571.4275\u001b[0m  0.0697\n",
      "      3      \u001b[36m101.9863\u001b[0m       97.9726  0.1937\n",
      "      3       \u001b[36m76.1863\u001b[0m       \u001b[32m71.3579\u001b[0m  0.2013\n",
      "      8      \u001b[36m765.8423\u001b[0m      \u001b[32m770.9312\u001b[0m  0.0728\n",
      "      8     \u001b[36m1114.9530\u001b[0m     \u001b[32m1133.4526\u001b[0m  0.0584\n",
      "      8     \u001b[36m1099.4041\u001b[0m     \u001b[32m1159.6256\u001b[0m  0.0573\n",
      "      3      \u001b[36m145.7435\u001b[0m      145.6260  0.2130\n",
      "      3      \u001b[36m135.7379\u001b[0m      \u001b[32m132.4207\u001b[0m  0.2065\n",
      "      9      \u001b[36m546.1718\u001b[0m      572.7266  0.0620\n",
      "      8     \u001b[36m1028.9949\u001b[0m     \u001b[32m1055.6017\u001b[0m  0.0765\n",
      "      3      \u001b[36m146.2179\u001b[0m      140.5295  0.2367\n",
      "      9      \u001b[36m754.9865\u001b[0m      774.6938  0.0576\n",
      "      9     \u001b[36m1111.3037\u001b[0m     \u001b[32m1130.8994\u001b[0m  0.0747\n",
      "     10      \u001b[36m539.5799\u001b[0m      575.5032  0.0625\n",
      "Restoring best model from epoch 8.\n",
      "      9     \u001b[36m1093.2476\u001b[0m     1161.7850  0.0746\n",
      "      9     \u001b[36m1023.8252\u001b[0m     \u001b[32m1052.3666\u001b[0m  0.0691\n",
      "     10      \u001b[36m747.4812\u001b[0m      778.4025  0.0797\n",
      "Restoring best model from epoch 8.\n",
      "      4      \u001b[36m101.5106\u001b[0m       \u001b[32m96.7149\u001b[0m  0.1776\n",
      "     10     \u001b[36m1094.3837\u001b[0m     1136.3984  0.0656\n",
      "Restoring best model from epoch 9.\n",
      "      4       \u001b[36m75.8053\u001b[0m       71.9778  0.1766\n",
      "     10     \u001b[36m1079.9083\u001b[0m     1166.8810  0.0561\n",
      "Restoring best model from epoch 8.\n",
      "     10     \u001b[36m1012.2734\u001b[0m     \u001b[32m1047.8853\u001b[0m  0.0645\n",
      "      4      \u001b[36m144.3813\u001b[0m      \u001b[32m143.7994\u001b[0m  0.1655\n",
      "      4      \u001b[36m135.5574\u001b[0m      \u001b[32m131.1764\u001b[0m  0.1763\n",
      "      4      146.4917      139.0180  0.1882\n",
      "      5       \u001b[36m75.1166\u001b[0m       71.7842  0.1599\n",
      "      5      \u001b[36m101.0987\u001b[0m       97.8998  0.1636\n",
      "      5      \u001b[36m142.4482\u001b[0m      \u001b[32m143.4091\u001b[0m  0.1591\n",
      "      5      \u001b[36m134.7328\u001b[0m      132.4853  0.1587\n",
      "      5      \u001b[36m144.3944\u001b[0m      139.2435  0.1596\n",
      "      6       75.1526       72.6747  0.1581\n",
      "      6      101.6125       \u001b[32m95.5681\u001b[0m  0.1602\n",
      "      6      142.5706      144.2120  0.1596\n",
      "      6      \u001b[36m133.5798\u001b[0m      \u001b[32m131.1471\u001b[0m  0.1598\n",
      "      6      \u001b[36m143.8194\u001b[0m      \u001b[32m137.5461\u001b[0m  0.1601\n",
      "      7       \u001b[36m74.5855\u001b[0m       71.5240  0.1585\n",
      "      7       \u001b[36m99.6532\u001b[0m       \u001b[32m95.4471\u001b[0m  0.1592\n",
      "      7      142.6098      \u001b[32m142.9168\u001b[0m  0.1575\n",
      "      7      \u001b[36m133.1498\u001b[0m      \u001b[32m130.6179\u001b[0m  0.1594\n",
      "      7      144.1692      138.3419  0.1597\n",
      "      8       \u001b[36m74.2656\u001b[0m       72.2164  0.1585\n",
      "      8       \u001b[36m99.2736\u001b[0m       \u001b[32m95.1885\u001b[0m  0.1582\n",
      "      8      \u001b[36m140.7981\u001b[0m      144.3842  0.1587\n",
      "      8      \u001b[36m132.3881\u001b[0m      \u001b[32m129.4975\u001b[0m  0.1582\n",
      "      8      \u001b[36m142.7071\u001b[0m      137.5566  0.1603\n",
      "      9       74.6068       71.4022  0.1595\n",
      "      9       \u001b[36m99.2375\u001b[0m       95.9943  0.1594\n",
      "      9      141.3064      143.4565  0.1589\n",
      "      9      \u001b[36m131.5360\u001b[0m      131.2567  0.1590\n",
      "      9      \u001b[36m142.2408\u001b[0m      139.4429  0.1586\n",
      "     10       \u001b[36m72.6830\u001b[0m       \u001b[32m70.6135\u001b[0m  0.1591\n",
      "     10       \u001b[36m99.1508\u001b[0m       95.5852  0.1597\n",
      "Restoring best model from epoch 8.\n",
      "     10      141.1411      144.2176  0.1591\n",
      "Restoring best model from epoch 7.\n",
      "     10      131.7659      129.9136  0.1583\n",
      "Restoring best model from epoch 8.\n",
      "     10      142.8068      138.7550  0.1593\n",
      "Restoring best model from epoch 6.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1     \u001b[36m1379.7915\u001b[0m     \u001b[32m1191.9966\u001b[0m  0.0716\n",
      "      2     \u001b[36m1067.3381\u001b[0m     \u001b[32m1008.9790\u001b[0m  0.0656\n",
      "      3     \u001b[36m1020.2950\u001b[0m      \u001b[32m935.1744\u001b[0m  0.0664\n",
      "      4     \u001b[36m1004.4086\u001b[0m      \u001b[32m905.3726\u001b[0m  0.0686\n",
      "      5      \u001b[36m992.2858\u001b[0m      908.3728  0.0687\n",
      "      6      \u001b[36m985.5495\u001b[0m      907.4719  0.0677\n",
      "      7      \u001b[36m973.5047\u001b[0m      \u001b[32m903.0492\u001b[0m  0.0650\n",
      "      8      \u001b[36m968.5153\u001b[0m      904.6940  0.0664\n",
      "      9      \u001b[36m961.6645\u001b[0m      903.3258  0.0687\n",
      "     10      \u001b[36m954.4862\u001b[0m      906.3793  0.0662\n",
      "Restoring best model from epoch 7.\n",
      "(6297, 9) <class 'pandas.core.frame.DataFrame'>\n",
      "(6297,) <class 'pandas.core.series.Series'>\n",
      "(1574, 9) <class 'pandas.core.frame.DataFrame'>\n",
      "(1574,) <class 'pandas.core.series.Series'>\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m43.3657\u001b[0m       \u001b[32m37.9538\u001b[0m  0.2426\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m82.5997\u001b[0m       \u001b[32m72.5330\u001b[0m  0.2248\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m82.4051\u001b[0m       \u001b[32m74.0117\u001b[0m  0.2426\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m59.1175\u001b[0m       \u001b[32m50.5734\u001b[0m  0.2706\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m76.6580\u001b[0m       \u001b[32m67.8921\u001b[0m  0.2641\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m326.8594\u001b[0m      \u001b[32m335.2891\u001b[0m  0.3158\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m175.4568\u001b[0m      \u001b[32m198.0136\u001b[0m  0.3486\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m238.6410\u001b[0m      \u001b[32m235.3220\u001b[0m  0.3387\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m302.8279\u001b[0m      \u001b[32m314.8185\u001b[0m  0.3348\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m330.2023\u001b[0m      \u001b[32m334.9220\u001b[0m  0.3165\n",
      "      2       \u001b[36m40.1137\u001b[0m       \u001b[32m37.6806\u001b[0m  0.2555\n",
      "      2       \u001b[36m76.2304\u001b[0m       73.7679  0.2645\n",
      "      2       \u001b[36m75.0362\u001b[0m       \u001b[32m72.4741\u001b[0m  0.2568\n",
      "      2       \u001b[36m69.7456\u001b[0m       \u001b[32m66.3790\u001b[0m  0.2500\n",
      "      2       \u001b[36m53.4070\u001b[0m       \u001b[32m49.2529\u001b[0m  0.2556\n",
      "      2      \u001b[36m202.6639\u001b[0m      \u001b[32m203.7723\u001b[0m  0.3039\n",
      "      2      \u001b[36m297.6066\u001b[0m      \u001b[32m319.6893\u001b[0m  0.3032\n",
      "      2      \u001b[36m271.3734\u001b[0m      \u001b[32m289.6365\u001b[0m  0.3283\n",
      "      2      \u001b[36m294.6184\u001b[0m      \u001b[32m317.6977\u001b[0m  0.3362\n",
      "      2      \u001b[36m155.6181\u001b[0m      \u001b[32m162.0054\u001b[0m  0.3349\n",
      "      3       \u001b[36m39.9588\u001b[0m       \u001b[32m36.8896\u001b[0m  0.2626\n",
      "      3       \u001b[36m74.8007\u001b[0m       \u001b[32m71.6712\u001b[0m  0.2507\n",
      "      3       \u001b[36m68.4560\u001b[0m       66.8052  0.2536\n",
      "      3       \u001b[36m52.0757\u001b[0m       \u001b[32m48.9629\u001b[0m  0.2608\n",
      "      3       \u001b[36m73.4090\u001b[0m       73.9942  0.2939\n",
      "      4       \u001b[36m68.2536\u001b[0m       67.2354  0.2303\n",
      "      3      \u001b[36m198.1320\u001b[0m      \u001b[32m200.6168\u001b[0m  0.3277\n",
      "      4       \u001b[36m39.1491\u001b[0m       \u001b[32m36.7476\u001b[0m  0.2589\n",
      "      3      \u001b[36m289.2683\u001b[0m      \u001b[32m307.7124\u001b[0m  0.3125\n",
      "      3      \u001b[36m290.3026\u001b[0m      \u001b[32m307.1021\u001b[0m  0.3212\n",
      "      4       \u001b[36m73.6480\u001b[0m       72.3357  0.2683\n",
      "      4       73.8324       74.7769  0.2484\n",
      "      3      \u001b[36m150.6157\u001b[0m      \u001b[32m160.3164\u001b[0m  0.3475\n",
      "      4       \u001b[36m51.9122\u001b[0m       49.2017  0.2804\n",
      "      3      \u001b[36m267.0046\u001b[0m      \u001b[32m277.1548\u001b[0m  0.3651\n",
      "      5       \u001b[36m67.7421\u001b[0m       \u001b[32m66.2453\u001b[0m  0.2607\n",
      "      5       \u001b[36m38.8528\u001b[0m       \u001b[32m36.4088\u001b[0m  0.2544\n",
      "      5       73.9474       72.9895  0.2541\n",
      "      5       \u001b[36m72.9791\u001b[0m       75.2668  0.2405\n",
      "      5       \u001b[36m51.5254\u001b[0m       49.4865  0.2661\n",
      "      4      \u001b[36m195.4369\u001b[0m      \u001b[32m196.5865\u001b[0m  0.3317\n",
      "      4      \u001b[36m283.2758\u001b[0m      \u001b[32m295.4223\u001b[0m  0.3388\n",
      "      4      \u001b[36m149.3286\u001b[0m      \u001b[32m156.3922\u001b[0m  0.3195\n",
      "      4      \u001b[36m285.0089\u001b[0m      \u001b[32m296.3620\u001b[0m  0.3965\n",
      "      4      \u001b[36m261.3604\u001b[0m      \u001b[32m273.9660\u001b[0m  0.3660\n",
      "      6       67.8663       66.7124  0.2294\n",
      "      6       \u001b[36m38.2160\u001b[0m       45.0937  0.2592\n",
      "      6       \u001b[36m73.3452\u001b[0m       73.1176  0.2551\n",
      "      6       73.0039       74.5156  0.2507\n",
      "      6       \u001b[36m50.6476\u001b[0m       \u001b[32m48.6421\u001b[0m  0.2379\n",
      "      5      \u001b[36m193.7422\u001b[0m      \u001b[32m193.5669\u001b[0m  0.3375\n",
      "      5      \u001b[36m280.6015\u001b[0m      296.9824  0.3487\n",
      "      5      \u001b[36m148.8700\u001b[0m      \u001b[32m154.7651\u001b[0m  0.3334\n",
      "      5      \u001b[36m282.8461\u001b[0m      298.1891  0.3337\n",
      "      5      263.8943      \u001b[32m268.8097\u001b[0m  0.3432\n",
      "      7       \u001b[36m72.4080\u001b[0m       75.7247  0.2599\n",
      "      7       \u001b[36m66.9442\u001b[0m       67.6486  0.3030\n",
      "      7       38.2377       44.0968  0.2811\n",
      "      7       \u001b[36m72.1798\u001b[0m       74.9120  0.2882\n",
      "      7       50.8201       48.8182  0.2852\n",
      "      6      194.1179      195.6723  0.3459\n",
      "      6      \u001b[36m278.1051\u001b[0m      \u001b[32m291.6358\u001b[0m  0.3369\n",
      "      8       72.4561       77.4619  0.2496\n",
      "      6      \u001b[36m146.0941\u001b[0m      156.6929  0.3420\n",
      "      8       38.3457       36.7189  0.2539\n",
      "      8       72.3091       76.3122  0.2356\n",
      "      6      \u001b[36m280.7130\u001b[0m      \u001b[32m290.5084\u001b[0m  0.3453\n",
      "      8       \u001b[36m50.6024\u001b[0m       50.1160  0.2387\n",
      "      8       \u001b[36m66.8628\u001b[0m       67.1230  0.3058\n",
      "      6      \u001b[36m256.8818\u001b[0m      271.8952  0.3432\n",
      "      9       \u001b[36m71.9953\u001b[0m       74.2604  0.2432\n",
      "      9       \u001b[36m38.0325\u001b[0m       36.7940  0.2477\n",
      "      9       \u001b[36m71.1261\u001b[0m       74.5425  0.2507\n",
      "      9       50.8092       49.1932  0.2310\n",
      "      7      \u001b[36m143.7835\u001b[0m      163.6913  0.3052\n",
      "      7      \u001b[36m192.6513\u001b[0m      198.8564  0.3341\n",
      "      7      \u001b[36m276.8549\u001b[0m      293.9626  0.3295\n",
      "      7      \u001b[36m277.8918\u001b[0m      300.3006  0.3211\n",
      "      9       66.9970       67.3521  0.3212\n",
      "      7      \u001b[36m253.8899\u001b[0m      274.1631  0.3271\n",
      "     10       \u001b[36m71.5295\u001b[0m       74.1035  0.2205\n",
      "Restoring best model from epoch 3.\n",
      "     10       71.1860       74.3088  0.2296\n",
      "Restoring best model from epoch 2.\n",
      "     10       \u001b[36m37.7332\u001b[0m       38.1016  0.2415\n",
      "Restoring best model from epoch 5.\n",
      "     10       \u001b[36m50.3543\u001b[0m       49.5468  0.2587\n",
      "Restoring best model from epoch 6.\n",
      "      8      \u001b[36m142.3742\u001b[0m      \u001b[32m150.2178\u001b[0m  0.2839\n",
      "      8      278.3315      294.7618  0.2873\n",
      "     10       \u001b[36m65.9877\u001b[0m       66.6471  0.2483\n",
      "Restoring best model from epoch 5.\n",
      "      8      \u001b[36m191.3256\u001b[0m      199.7293  0.3218\n",
      "      8      278.0352      297.7282  0.2888\n",
      "      8      257.7468      273.6769  0.2752\n",
      "      9      \u001b[36m141.3210\u001b[0m      164.7483  0.2711\n",
      "      9      \u001b[36m273.4839\u001b[0m      291.9379  0.2736\n",
      "      9      191.4270      199.1440  0.2685\n",
      "      9      \u001b[36m275.7669\u001b[0m      295.0558  0.2717\n",
      "      9      255.2484      272.0638  0.2689\n",
      "     10      141.5671      152.6947  0.2716\n",
      "Restoring best model from epoch 8.\n",
      "     10      \u001b[36m271.3009\u001b[0m      294.1992  0.2727\n",
      "Restoring best model from epoch 6.\n",
      "     10      \u001b[36m187.1603\u001b[0m      197.3845  0.2710\n",
      "Restoring best model from epoch 5.\n",
      "     10      \u001b[36m273.1736\u001b[0m      299.4019  0.2724\n",
      "Restoring best model from epoch 6.\n",
      "     10      \u001b[36m252.2536\u001b[0m      270.2798  0.2699\n",
      "Restoring best model from epoch 5.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m73.4910\u001b[0m       \u001b[32m60.3468\u001b[0m  0.2986\n",
      "      2       \u001b[36m66.7017\u001b[0m       \u001b[32m60.0324\u001b[0m  0.2981\n",
      "      3       \u001b[36m66.1227\u001b[0m       60.9494  0.3130\n",
      "      4       \u001b[36m65.4821\u001b[0m       60.7286  0.3159\n",
      "      5       \u001b[36m65.1346\u001b[0m       60.9347  0.2918\n",
      "      6       \u001b[36m64.2185\u001b[0m       60.4637  0.2880\n",
      "      7       64.3539       60.5826  0.3322\n",
      "      8       \u001b[36m64.1881\u001b[0m       60.4241  0.2981\n",
      "      9       \u001b[36m63.3714\u001b[0m       60.1376  0.2821\n",
      "     10       \u001b[36m62.9724\u001b[0m       60.0480  0.2878\n",
      "Restoring best model from epoch 2.\n",
      "(6297, 9) <class 'pandas.core.frame.DataFrame'>\n",
      "(6297,) <class 'pandas.core.series.Series'>\n",
      "(1574, 9) <class 'pandas.core.frame.DataFrame'>\n",
      "(1574,) <class 'pandas.core.series.Series'>\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m83.2482\u001b[0m       \u001b[32m72.4250\u001b[0m  0.2254\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m42.5147\u001b[0m       \u001b[32m39.4831\u001b[0m  0.2560\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m60.2751\u001b[0m       \u001b[32m46.2982\u001b[0m  0.2571\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m78.0196\u001b[0m       \u001b[32m65.7406\u001b[0m  0.2681\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m82.1835\u001b[0m       \u001b[32m70.9368\u001b[0m  0.2641\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m308.0879\u001b[0m      \u001b[32m263.8424\u001b[0m  0.3087\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m240.2831\u001b[0m      \u001b[32m196.5675\u001b[0m  0.3154\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m324.1746\u001b[0m      \u001b[32m309.0904\u001b[0m  0.3222\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m175.2161\u001b[0m      \u001b[32m168.8457\u001b[0m  0.3469\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m328.7374\u001b[0m      \u001b[32m302.6160\u001b[0m  0.3590\n",
      "      2       \u001b[36m75.6808\u001b[0m       \u001b[32m71.4847\u001b[0m  0.2891\n",
      "      2       \u001b[36m54.0579\u001b[0m       \u001b[32m46.2680\u001b[0m  0.2808\n",
      "      2       \u001b[36m69.9374\u001b[0m       \u001b[32m64.8626\u001b[0m  0.2736\n",
      "      2       \u001b[36m39.0429\u001b[0m       \u001b[32m39.3354\u001b[0m  0.3037\n",
      "      2       \u001b[36m74.8216\u001b[0m       \u001b[32m70.6738\u001b[0m  0.2933\n",
      "      2      \u001b[36m276.5281\u001b[0m      \u001b[32m262.1766\u001b[0m  0.3384\n",
      "      2      \u001b[36m208.2652\u001b[0m      \u001b[32m185.3129\u001b[0m  0.3610\n",
      "      2      \u001b[36m157.6144\u001b[0m      \u001b[32m165.7084\u001b[0m  0.3509\n",
      "      2      \u001b[36m297.6939\u001b[0m      \u001b[32m293.1700\u001b[0m  0.3537\n",
      "      2      \u001b[36m298.7282\u001b[0m      \u001b[32m290.1705\u001b[0m  0.3282\n",
      "      3       \u001b[36m54.0525\u001b[0m       \u001b[32m45.6254\u001b[0m  0.2768\n",
      "      3       \u001b[36m69.5211\u001b[0m       \u001b[32m63.7844\u001b[0m  0.2766\n",
      "      3       \u001b[36m73.4758\u001b[0m       \u001b[32m70.4691\u001b[0m  0.2603\n",
      "      3       \u001b[36m74.2661\u001b[0m       \u001b[32m70.9426\u001b[0m  0.2996\n",
      "      3       \u001b[36m38.5728\u001b[0m       \u001b[32m38.6755\u001b[0m  0.2744\n",
      "      3      \u001b[36m268.5947\u001b[0m      \u001b[32m253.4463\u001b[0m  0.3318\n",
      "      3      \u001b[36m286.1459\u001b[0m      \u001b[32m283.2616\u001b[0m  0.3029\n",
      "      3      \u001b[36m202.0169\u001b[0m      \u001b[32m180.4795\u001b[0m  0.3514\n",
      "      3      \u001b[36m288.9857\u001b[0m      \u001b[32m281.6016\u001b[0m  0.3353\n",
      "      3      \u001b[36m152.8256\u001b[0m      \u001b[32m157.0509\u001b[0m  0.3375\n",
      "      4       \u001b[36m73.6866\u001b[0m       \u001b[32m70.6133\u001b[0m  0.2290\n",
      "      4       \u001b[36m53.1764\u001b[0m       45.8949  0.2543\n",
      "      4       \u001b[36m69.0036\u001b[0m       \u001b[32m63.3973\u001b[0m  0.2540\n",
      "      4       \u001b[36m38.2339\u001b[0m       \u001b[32m38.5844\u001b[0m  0.2495\n",
      "      4       \u001b[36m72.7188\u001b[0m       \u001b[32m70.0397\u001b[0m  0.2570\n",
      "      4      \u001b[36m264.9874\u001b[0m      \u001b[32m251.8062\u001b[0m  0.3066\n",
      "      5       \u001b[36m72.6848\u001b[0m       70.3151  0.2483\n",
      "      5       \u001b[36m72.9025\u001b[0m       \u001b[32m70.3178\u001b[0m  0.2769\n",
      "      5       \u001b[36m52.8028\u001b[0m       \u001b[32m45.2302\u001b[0m  0.2710\n",
      "      5       \u001b[36m68.5860\u001b[0m       63.5514  0.2743\n",
      "      5       \u001b[36m38.1243\u001b[0m       \u001b[32m38.1182\u001b[0m  0.2887\n",
      "      4      \u001b[36m199.2786\u001b[0m      \u001b[32m179.5638\u001b[0m  0.3268\n",
      "      4      \u001b[36m282.1605\u001b[0m      283.6393  0.3404\n",
      "      4      \u001b[36m149.7564\u001b[0m      \u001b[32m153.9567\u001b[0m  0.3370\n",
      "      4      \u001b[36m286.7855\u001b[0m      \u001b[32m278.5368\u001b[0m  0.3556\n",
      "      6       \u001b[36m51.9224\u001b[0m       45.8665  0.2623\n",
      "      6       \u001b[36m72.7182\u001b[0m       70.4059  0.2750\n",
      "      6       \u001b[36m72.5220\u001b[0m       70.2875  0.2781\n",
      "      6       \u001b[36m67.7871\u001b[0m       63.6016  0.2601\n",
      "      6       \u001b[36m37.4843\u001b[0m       38.3412  0.2468\n",
      "      5      \u001b[36m262.7343\u001b[0m      256.6548  0.3314\n",
      "      5      \u001b[36m280.2861\u001b[0m      283.2936  0.3523\n",
      "      5      \u001b[36m197.4879\u001b[0m      181.1056  0.3637\n",
      "      5      \u001b[36m283.9641\u001b[0m      282.7600  0.3335\n",
      "      5      \u001b[36m147.1380\u001b[0m      \u001b[32m149.4969\u001b[0m  0.3551\n",
      "      7       52.2851       45.2763  0.2375\n",
      "      7       \u001b[36m37.1651\u001b[0m       38.1275  0.2380\n",
      "      7       \u001b[36m72.6372\u001b[0m       70.5111  0.2718\n",
      "      7       \u001b[36m67.4055\u001b[0m       \u001b[32m63.1262\u001b[0m  0.2736\n",
      "      7       \u001b[36m71.7868\u001b[0m       \u001b[32m69.8229\u001b[0m  0.2747\n",
      "      6      \u001b[36m258.6975\u001b[0m      257.9866  0.3133\n",
      "      6      \u001b[36m275.3254\u001b[0m      285.4855  0.3074\n",
      "      6      \u001b[36m278.2548\u001b[0m      283.3094  0.3121\n",
      "      6      \u001b[36m143.3075\u001b[0m      154.4958  0.3288\n",
      "      6      \u001b[36m195.1076\u001b[0m      \u001b[32m178.9672\u001b[0m  0.3417\n",
      "      8       \u001b[36m51.5615\u001b[0m       45.5824  0.2529\n",
      "      8       37.4267       38.2674  0.2491\n",
      "      8       \u001b[36m67.2109\u001b[0m       63.3577  0.2344\n",
      "      8       \u001b[36m71.4032\u001b[0m       70.0760  0.2413\n",
      "      8       \u001b[36m72.0754\u001b[0m       70.3224  0.2478\n",
      "      7      \u001b[36m255.9850\u001b[0m      257.5483  0.3238\n",
      "      9       51.9667       \u001b[32m45.1402\u001b[0m  0.2507\n",
      "      7      \u001b[36m272.2825\u001b[0m      \u001b[32m281.5011\u001b[0m  0.3062\n",
      "      7      \u001b[36m276.9665\u001b[0m      280.1568  0.3046\n",
      "      9       \u001b[36m37.0147\u001b[0m       38.1621  0.2600\n",
      "      9       \u001b[36m71.9029\u001b[0m       70.3684  0.2477\n",
      "      7      195.4606      179.4154  0.2991\n",
      "      7      144.0912      152.5644  0.3169\n",
      "      9       67.4490       63.2404  0.2781\n",
      "      9       71.9349       70.0363  0.2708\n",
      "     10       51.7077       45.3850  0.2434\n",
      "Restoring best model from epoch 9.\n",
      "      8      \u001b[36m255.2367\u001b[0m      255.8137  0.3034\n",
      "     10       \u001b[36m71.6284\u001b[0m       \u001b[32m70.1072\u001b[0m  0.2248\n",
      "     10       37.0238       38.2921  0.2446\n",
      "Restoring best model from epoch 5.\n",
      "      8      \u001b[36m274.7463\u001b[0m      278.7982  0.2899\n",
      "     10       \u001b[36m67.0612\u001b[0m       63.5560  0.2536\n",
      "Restoring best model from epoch 7.\n",
      "     10       \u001b[36m71.1197\u001b[0m       70.0515  0.2630\n",
      "Restoring best model from epoch 7.\n",
      "      8      273.0808      283.3109  0.3251\n",
      "      8      \u001b[36m194.1148\u001b[0m      180.0913  0.3071\n",
      "      8      \u001b[36m141.6548\u001b[0m      154.2457  0.3093\n",
      "      9      \u001b[36m252.2177\u001b[0m      257.0483  0.2797\n",
      "      9      \u001b[36m274.1510\u001b[0m      279.9525  0.2716\n",
      "      9      \u001b[36m270.3350\u001b[0m      \u001b[32m281.4666\u001b[0m  0.2711\n",
      "      9      \u001b[36m193.7052\u001b[0m      \u001b[32m178.7593\u001b[0m  0.2706\n",
      "      9      142.1249      152.5247  0.2717\n",
      "     10      \u001b[36m250.0034\u001b[0m      257.8997  0.2714\n",
      "Restoring best model from epoch 4.\n",
      "     10      \u001b[36m273.8717\u001b[0m      280.8023  0.2685\n",
      "Restoring best model from epoch 4.\n",
      "     10      \u001b[36m270.1928\u001b[0m      282.4062  0.2696\n",
      "Restoring best model from epoch 9.\n",
      "     10      \u001b[36m192.5164\u001b[0m      180.4079  0.2710\n",
      "Restoring best model from epoch 9.\n",
      "     10      141.8302      153.9517  0.2730\n",
      "Restoring best model from epoch 5.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m73.1658\u001b[0m       \u001b[32m63.5081\u001b[0m  0.3352\n",
      "      2       \u001b[36m66.1860\u001b[0m       \u001b[32m62.6056\u001b[0m  0.2912\n",
      "      3       \u001b[36m65.2514\u001b[0m       \u001b[32m61.6608\u001b[0m  0.2853\n",
      "      4       \u001b[36m64.9773\u001b[0m       \u001b[32m61.2255\u001b[0m  0.3106\n",
      "      5       \u001b[36m63.9095\u001b[0m       61.2755  0.2858\n",
      "      6       64.2050       61.5432  0.2813\n",
      "      7       \u001b[36m63.8127\u001b[0m       \u001b[32m61.0947\u001b[0m  0.2863\n",
      "      8       \u001b[36m63.6723\u001b[0m       61.2003  0.2798\n",
      "      9       \u001b[36m63.4921\u001b[0m       61.4623  0.2803\n",
      "     10       \u001b[36m63.3139\u001b[0m       61.1031  0.2885\n",
      "Restoring best model from epoch 7.\n",
      "(6297, 9) <class 'pandas.core.frame.DataFrame'>\n",
      "(6297,) <class 'pandas.core.series.Series'>\n",
      "(1574, 9) <class 'pandas.core.frame.DataFrame'>\n",
      "(1574,) <class 'pandas.core.series.Series'>\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m41.1519\u001b[0m       \u001b[32m42.5168\u001b[0m  0.2469\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m81.5460\u001b[0m       \u001b[32m72.7189\u001b[0m  0.2275\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m57.9983\u001b[0m       \u001b[32m49.8177\u001b[0m  0.2456\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m76.7338\u001b[0m       \u001b[32m64.3417\u001b[0m  0.2578\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m81.2430\u001b[0m       \u001b[32m72.1385\u001b[0m  0.2611\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m305.0305\u001b[0m      \u001b[32m272.0260\u001b[0m  0.3116\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m229.0993\u001b[0m      \u001b[32m217.5875\u001b[0m  0.3220\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m322.2752\u001b[0m      \u001b[32m305.6334\u001b[0m  0.3190\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m171.6039\u001b[0m      \u001b[32m177.5410\u001b[0m  0.3529\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m325.8471\u001b[0m      \u001b[32m313.1347\u001b[0m  0.3278\n",
      "      2       \u001b[36m52.5179\u001b[0m       50.0364  0.2232\n",
      "      2       \u001b[36m38.4448\u001b[0m       42.8173  0.2501\n",
      "      2       \u001b[36m75.7362\u001b[0m       \u001b[32m71.7691\u001b[0m  0.2538\n",
      "      2       \u001b[36m74.8819\u001b[0m       \u001b[32m69.9320\u001b[0m  0.2583\n",
      "      2       \u001b[36m69.8631\u001b[0m       \u001b[32m63.6080\u001b[0m  0.2727\n",
      "      2      \u001b[36m268.4312\u001b[0m      \u001b[32m264.7360\u001b[0m  0.2952\n",
      "      2      \u001b[36m200.9473\u001b[0m      \u001b[32m204.8941\u001b[0m  0.3240\n",
      "      3       \u001b[36m51.3657\u001b[0m       50.6378  0.2272\n",
      "      2      \u001b[36m147.9491\u001b[0m      \u001b[32m167.5216\u001b[0m  0.3195\n",
      "      2      \u001b[36m292.0108\u001b[0m      \u001b[32m294.2030\u001b[0m  0.3234\n",
      "      2      \u001b[36m294.1001\u001b[0m      \u001b[32m299.5713\u001b[0m  0.3532\n",
      "      3       \u001b[36m37.6211\u001b[0m       \u001b[32m41.8076\u001b[0m  0.2597\n",
      "      3       \u001b[36m73.9211\u001b[0m       \u001b[32m71.2215\u001b[0m  0.2510\n",
      "      3       \u001b[36m69.1536\u001b[0m       64.0133  0.2308\n",
      "      3       \u001b[36m73.2507\u001b[0m       70.9869  0.2620\n",
      "      4       \u001b[36m50.6545\u001b[0m       49.9287  0.2240\n",
      "      3      \u001b[36m264.3313\u001b[0m      \u001b[32m260.9346\u001b[0m  0.3200\n",
      "      4       \u001b[36m73.3351\u001b[0m       71.2975  0.2393\n",
      "      3      \u001b[36m144.0573\u001b[0m      \u001b[32m165.3892\u001b[0m  0.2965\n",
      "      3      \u001b[36m194.8640\u001b[0m      \u001b[32m201.9713\u001b[0m  0.3171\n",
      "      4       \u001b[36m37.3867\u001b[0m       \u001b[32m41.4439\u001b[0m  0.2501\n",
      "      3      \u001b[36m284.8565\u001b[0m      \u001b[32m288.1034\u001b[0m  0.3187\n",
      "      4       \u001b[36m68.2719\u001b[0m       64.0884  0.2679\n",
      "      4       \u001b[36m73.1682\u001b[0m       70.5653  0.2649\n",
      "      3      \u001b[36m286.7570\u001b[0m      \u001b[32m291.3862\u001b[0m  0.3735\n",
      "      5       50.7553       \u001b[32m49.7304\u001b[0m  0.2412\n",
      "      5       \u001b[36m37.1585\u001b[0m       41.7731  0.2175\n",
      "      4      \u001b[36m261.4358\u001b[0m      \u001b[32m260.2093\u001b[0m  0.3050\n",
      "      5       73.3451       \u001b[32m70.7395\u001b[0m  0.2792\n",
      "      5       68.4879       64.4360  0.2660\n",
      "      5       \u001b[36m72.8660\u001b[0m       70.6976  0.2434\n",
      "      4      \u001b[36m191.6885\u001b[0m      \u001b[32m196.7822\u001b[0m  0.3261\n",
      "      4      \u001b[36m143.2337\u001b[0m      \u001b[32m163.6115\u001b[0m  0.3316\n",
      "      4      \u001b[36m282.7215\u001b[0m      291.8713  0.3258\n",
      "      6       \u001b[36m49.5343\u001b[0m       50.1099  0.2406\n",
      "      4      \u001b[36m284.6669\u001b[0m      \u001b[32m289.7582\u001b[0m  0.3213\n",
      "      6       \u001b[36m36.5005\u001b[0m       41.4763  0.2585\n",
      "      6       72.9788       71.9151  0.2276\n",
      "      6       \u001b[36m72.4677\u001b[0m       72.7192  0.2900\n",
      "      6       \u001b[36m67.8534\u001b[0m       63.6744  0.2770\n",
      "      5      \u001b[36m258.1576\u001b[0m      \u001b[32m256.3243\u001b[0m  0.3311\n",
      "      5      \u001b[36m189.9398\u001b[0m      200.3687  0.2980\n",
      "      7       49.9025       50.5986  0.2367\n",
      "      5      \u001b[36m140.3045\u001b[0m      164.8781  0.3277\n",
      "      5      \u001b[36m281.4225\u001b[0m      \u001b[32m287.9680\u001b[0m  0.3367\n",
      "      7       36.8906       \u001b[32m41.3194\u001b[0m  0.2451\n",
      "      5      \u001b[36m280.6553\u001b[0m      291.4607  0.3352\n",
      "      7       \u001b[36m72.3532\u001b[0m       70.8839  0.2399\n",
      "      7       \u001b[36m71.8138\u001b[0m       71.2505  0.2683\n",
      "      7       \u001b[36m67.3841\u001b[0m       64.0220  0.2507\n",
      "      8       49.7510       50.7467  0.2423\n",
      "      6      \u001b[36m257.0184\u001b[0m      260.4074  0.3224\n",
      "      6      \u001b[36m189.4448\u001b[0m      196.8835  0.3376\n",
      "      8       36.7150       41.3212  0.2431\n",
      "      6      \u001b[36m278.8301\u001b[0m      \u001b[32m282.8930\u001b[0m  0.3057\n",
      "      6      \u001b[36m139.6673\u001b[0m      164.3040  0.3451\n",
      "      8       \u001b[36m71.7261\u001b[0m       71.5405  0.2431\n",
      "      6      \u001b[36m279.7136\u001b[0m      \u001b[32m284.8167\u001b[0m  0.3001\n",
      "      8       \u001b[36m67.3639\u001b[0m       65.8151  0.2361\n",
      "      8       \u001b[36m71.6136\u001b[0m       71.0706  0.2643\n",
      "      9       \u001b[36m49.2279\u001b[0m       50.1537  0.2585\n",
      "      7      \u001b[36m255.9049\u001b[0m      262.3018  0.2940\n",
      "      9       \u001b[36m36.1595\u001b[0m       \u001b[32m41.0803\u001b[0m  0.2594\n",
      "      9       \u001b[36m71.3430\u001b[0m       71.6527  0.2587\n",
      "      7      \u001b[36m186.0623\u001b[0m      198.2840  0.3316\n",
      "      7      \u001b[36m278.7526\u001b[0m      285.9453  0.3171\n",
      "      7      \u001b[36m137.3776\u001b[0m      165.3516  0.3118\n",
      "      9       \u001b[36m66.3617\u001b[0m       64.1333  0.2504\n",
      "      9       71.8108       \u001b[32m70.6944\u001b[0m  0.2451\n",
      "      7      \u001b[36m279.0117\u001b[0m      \u001b[32m279.8515\u001b[0m  0.3315\n",
      "     10       49.3859       50.2008  0.2620\n",
      "Restoring best model from epoch 5.\n",
      "     10       \u001b[36m35.9822\u001b[0m       41.2221  0.2356\n",
      "Restoring best model from epoch 9.\n",
      "     10       \u001b[36m70.9496\u001b[0m       71.8295  0.2412\n",
      "Restoring best model from epoch 2.\n",
      "      8      \u001b[36m254.8662\u001b[0m      \u001b[32m253.1719\u001b[0m  0.3129\n",
      "     10       \u001b[36m66.1662\u001b[0m       65.6509  0.2240\n",
      "Restoring best model from epoch 2.\n",
      "     10       \u001b[36m71.4518\u001b[0m       70.8402  0.2307\n",
      "Restoring best model from epoch 9.\n",
      "      8      \u001b[36m274.9107\u001b[0m      284.8772  0.2777\n",
      "      8      138.3238      167.7117  0.2818\n",
      "      8      186.5948      198.6014  0.3032\n",
      "      8      \u001b[36m275.4075\u001b[0m      280.3671  0.2856\n",
      "      9      \u001b[36m251.8067\u001b[0m      255.6859  0.2720\n",
      "      9      274.9558      285.8064  0.2714\n",
      "      9      \u001b[36m134.8723\u001b[0m      168.2607  0.2722\n",
      "      9      \u001b[36m184.5748\u001b[0m      203.1680  0.2727\n",
      "      9      276.6893      280.5186  0.2728\n",
      "     10      \u001b[36m249.9119\u001b[0m      259.0474  0.2702\n",
      "Restoring best model from epoch 8.\n",
      "     10      \u001b[36m271.0046\u001b[0m      285.9563  0.2714\n",
      "Restoring best model from epoch 6.\n",
      "     10      \u001b[36m134.0238\u001b[0m      169.8057  0.2720\n",
      "Restoring best model from epoch 4.\n",
      "     10      185.1305      198.7819  0.2715\n",
      "Restoring best model from epoch 4.\n",
      "     10      \u001b[36m273.1393\u001b[0m      280.6968  0.2702\n",
      "Restoring best model from epoch 7.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m72.2020\u001b[0m       \u001b[32m61.8591\u001b[0m  0.2933\n",
      "      2       \u001b[36m65.4832\u001b[0m       \u001b[32m61.3113\u001b[0m  0.2990\n",
      "      3       \u001b[36m64.6672\u001b[0m       61.5736  0.2868\n",
      "      4       \u001b[36m64.2577\u001b[0m       \u001b[32m60.8016\u001b[0m  0.3049\n",
      "      5       \u001b[36m63.5561\u001b[0m       61.0738  0.2858\n",
      "      6       \u001b[36m63.2359\u001b[0m       \u001b[32m60.3867\u001b[0m  0.2811\n",
      "      7       63.2538       61.1910  0.3292\n",
      "      8       \u001b[36m62.9135\u001b[0m       60.8583  0.2790\n",
      "      9       63.2181       60.7083  0.2977\n",
      "     10       62.9364       60.9501  0.2960\n",
      "Restoring best model from epoch 6.\n",
      "(6297, 9) <class 'pandas.core.frame.DataFrame'>\n",
      "(6297,) <class 'pandas.core.series.Series'>\n",
      "(1574, 9) <class 'pandas.core.frame.DataFrame'>\n",
      "(1574,) <class 'pandas.core.series.Series'>\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m43.1478\u001b[0m       \u001b[32m40.1195\u001b[0m  0.2638\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m58.5294\u001b[0m       \u001b[32m50.8753\u001b[0m  0.2773\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m81.9966\u001b[0m       \u001b[32m71.7475\u001b[0m  0.2694\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m77.3390\u001b[0m       \u001b[32m67.5823\u001b[0m  0.2774\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m83.1561\u001b[0m       \u001b[32m71.8519\u001b[0m  0.2749\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m178.0353\u001b[0m      \u001b[32m195.4310\u001b[0m  0.3269\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m234.0286\u001b[0m      \u001b[32m236.5997\u001b[0m  0.3389\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m305.0928\u001b[0m      \u001b[32m320.4761\u001b[0m  0.3518\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m327.5420\u001b[0m      \u001b[32m342.0709\u001b[0m  0.3499\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m328.3009\u001b[0m      \u001b[32m333.7111\u001b[0m  0.3378\n",
      "      2       \u001b[36m39.4458\u001b[0m       \u001b[32m38.4766\u001b[0m  0.2576\n",
      "      2       \u001b[36m76.0187\u001b[0m       \u001b[32m70.7090\u001b[0m  0.2494\n",
      "      2       \u001b[36m52.9648\u001b[0m       \u001b[32m50.4022\u001b[0m  0.2668\n",
      "      2       \u001b[36m70.6688\u001b[0m       \u001b[32m66.4538\u001b[0m  0.2708\n",
      "      2       \u001b[36m75.8105\u001b[0m       \u001b[32m70.7395\u001b[0m  0.2781\n",
      "      2      \u001b[36m203.0132\u001b[0m      \u001b[32m213.2767\u001b[0m  0.3130\n",
      "      2      \u001b[36m156.1236\u001b[0m      \u001b[32m162.8839\u001b[0m  0.3610\n",
      "      2      \u001b[36m293.4089\u001b[0m      \u001b[32m287.0971\u001b[0m  0.3229\n",
      "      2      \u001b[36m294.6937\u001b[0m      \u001b[32m298.0292\u001b[0m  0.3298\n",
      "      2      \u001b[36m272.9173\u001b[0m      \u001b[32m280.8740\u001b[0m  0.3546\n",
      "      3       \u001b[36m73.9040\u001b[0m       70.8174  0.2290\n",
      "      3       \u001b[36m39.1011\u001b[0m       \u001b[32m38.2855\u001b[0m  0.2725\n",
      "      3       \u001b[36m52.1689\u001b[0m       \u001b[32m50.1413\u001b[0m  0.2506\n",
      "      3       \u001b[36m73.7686\u001b[0m       71.6144  0.2441\n",
      "      3       \u001b[36m68.9600\u001b[0m       67.1739  0.2681\n",
      "      4       \u001b[36m51.9691\u001b[0m       50.2138  0.2362\n",
      "      4       \u001b[36m73.6052\u001b[0m       \u001b[32m70.3859\u001b[0m  0.2651\n",
      "      3      \u001b[36m198.4722\u001b[0m      \u001b[32m203.5173\u001b[0m  0.3534\n",
      "      3      \u001b[36m151.9274\u001b[0m      \u001b[32m153.1532\u001b[0m  0.3303\n",
      "      3      \u001b[36m285.1058\u001b[0m      \u001b[32m282.4216\u001b[0m  0.3332\n",
      "      3      \u001b[36m289.2683\u001b[0m      \u001b[32m290.4294\u001b[0m  0.3372\n",
      "      4       \u001b[36m73.7292\u001b[0m       \u001b[32m70.6951\u001b[0m  0.2632\n",
      "      4       \u001b[36m38.4147\u001b[0m       \u001b[32m37.9603\u001b[0m  0.2761\n",
      "      3      \u001b[36m268.9234\u001b[0m      \u001b[32m276.8445\u001b[0m  0.3378\n",
      "      4       \u001b[36m68.4350\u001b[0m       66.8992  0.2843\n",
      "      5       \u001b[36m73.0925\u001b[0m       \u001b[32m70.2819\u001b[0m  0.2502\n",
      "      5       \u001b[36m51.8443\u001b[0m       50.4603  0.2672\n",
      "      5       38.6262       39.2968  0.2406\n",
      "      5       \u001b[36m73.4471\u001b[0m       71.4009  0.2573\n",
      "      4      \u001b[36m149.8249\u001b[0m      157.1912  0.3000\n",
      "      4      \u001b[36m196.1397\u001b[0m      \u001b[32m199.8088\u001b[0m  0.3050\n",
      "      4      \u001b[36m283.7140\u001b[0m      \u001b[32m282.2056\u001b[0m  0.3267\n",
      "      4      \u001b[36m288.2918\u001b[0m      \u001b[32m288.3149\u001b[0m  0.3332\n",
      "      5       \u001b[36m67.8845\u001b[0m       \u001b[32m66.4319\u001b[0m  0.2873\n",
      "      4      \u001b[36m266.4946\u001b[0m      \u001b[32m270.1014\u001b[0m  0.3410\n",
      "      6       \u001b[36m72.5608\u001b[0m       70.3881  0.2606\n",
      "      6       \u001b[36m50.7149\u001b[0m       50.7845  0.2465\n",
      "      6       \u001b[36m38.0266\u001b[0m       38.6121  0.2570\n",
      "      6       \u001b[36m73.3436\u001b[0m       70.7816  0.2470\n",
      "      5      \u001b[36m146.2335\u001b[0m      157.4646  0.3024\n",
      "      5      \u001b[36m193.8656\u001b[0m      202.3690  0.3060\n",
      "      6       \u001b[36m67.4136\u001b[0m       \u001b[32m65.8378\u001b[0m  0.2783\n",
      "      5      \u001b[36m280.2996\u001b[0m      282.2671  0.3245\n",
      "      5      \u001b[36m281.8025\u001b[0m      \u001b[32m286.3317\u001b[0m  0.3298\n",
      "      5      267.5710      \u001b[32m264.7096\u001b[0m  0.3135\n",
      "      7       \u001b[36m72.3162\u001b[0m       \u001b[32m70.1627\u001b[0m  0.2391\n",
      "      7       50.9375       \u001b[32m50.0420\u001b[0m  0.2383\n",
      "      7       \u001b[36m37.8557\u001b[0m       38.5199  0.2523\n",
      "      7       \u001b[36m72.8213\u001b[0m       70.7542  0.2545\n",
      "      7       \u001b[36m67.3735\u001b[0m       66.2002  0.2619\n",
      "      6      \u001b[36m192.9232\u001b[0m      204.5251  0.3259\n",
      "      6      \u001b[36m144.5428\u001b[0m      155.6921  0.3480\n",
      "      8       \u001b[36m71.6943\u001b[0m       \u001b[32m69.4286\u001b[0m  0.2459\n",
      "      6      \u001b[36m279.2493\u001b[0m      \u001b[32m285.3072\u001b[0m  0.3149\n",
      "      8       \u001b[36m50.6889\u001b[0m       50.2976  0.2585\n",
      "      6      \u001b[36m273.7063\u001b[0m      283.2128  0.3434\n",
      "      6      \u001b[36m259.9334\u001b[0m      \u001b[32m263.5632\u001b[0m  0.3078\n",
      "      8       \u001b[36m71.8091\u001b[0m       71.2114  0.2464\n",
      "      8       \u001b[36m37.5947\u001b[0m       38.3548  0.2544\n",
      "      8       \u001b[36m67.3507\u001b[0m       \u001b[32m65.8152\u001b[0m  0.2440\n",
      "      9       \u001b[36m71.6299\u001b[0m       70.0454  0.2367\n",
      "      9       \u001b[36m50.4228\u001b[0m       \u001b[32m49.9938\u001b[0m  0.2315\n",
      "      7      \u001b[36m190.7581\u001b[0m      202.2131  0.3249\n",
      "      7      \u001b[36m142.8253\u001b[0m      154.2547  0.3178\n",
      "      9       72.3924       70.8833  0.2497\n",
      "      9       \u001b[36m37.4153\u001b[0m       38.1128  0.2533\n",
      "      7      \u001b[36m277.3271\u001b[0m      286.3710  0.3027\n",
      "      7      \u001b[36m255.5838\u001b[0m      271.6965  0.3285\n",
      "      7      \u001b[36m272.7147\u001b[0m      \u001b[32m282.0771\u001b[0m  0.3322\n",
      "      9       \u001b[36m66.7492\u001b[0m       \u001b[32m65.7948\u001b[0m  0.2489\n",
      "     10       71.6790       70.0437  0.2248\n",
      "Restoring best model from epoch 8.\n",
      "     10       \u001b[36m50.2926\u001b[0m       50.1399  0.2538\n",
      "Restoring best model from epoch 9.\n",
      "     10       \u001b[36m37.2628\u001b[0m       38.2969  0.2207\n",
      "Restoring best model from epoch 4.\n",
      "     10       72.0890       71.3273  0.2506\n",
      "Restoring best model from epoch 4.\n",
      "      8      143.8259      153.1635  0.3119\n",
      "      8      \u001b[36m188.9142\u001b[0m      207.1046  0.3324\n",
      "      8      \u001b[36m276.2768\u001b[0m      288.1657  0.3100\n",
      "      8      256.2227      267.2917  0.2823\n",
      "     10       66.7823       66.6516  0.2326\n",
      "Restoring best model from epoch 9.\n",
      "      8      \u001b[36m272.1345\u001b[0m      283.6078  0.3015\n",
      "      9      \u001b[36m141.5444\u001b[0m      153.4484  0.2702\n",
      "      9      \u001b[36m187.6716\u001b[0m      209.8913  0.2705\n",
      "      9      \u001b[36m272.7867\u001b[0m      291.0410  0.2707\n",
      "      9      \u001b[36m254.1274\u001b[0m      269.2815  0.2708\n",
      "      9      272.5216      286.1303  0.2716\n",
      "     10      \u001b[36m137.3971\u001b[0m      \u001b[32m152.6201\u001b[0m  0.2717\n",
      "     10      188.4835      218.9588  0.2707\n",
      "Restoring best model from epoch 4.\n",
      "     10      275.4073      291.8586  0.2713\n",
      "Restoring best model from epoch 6.\n",
      "     10      \u001b[36m253.8383\u001b[0m      268.2475  0.2700\n",
      "Restoring best model from epoch 6.\n",
      "     10      272.3159      285.5669  0.2693\n",
      "Restoring best model from epoch 7.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m71.5905\u001b[0m       \u001b[32m66.2023\u001b[0m  0.3014\n",
      "      2       \u001b[36m65.2603\u001b[0m       67.1363  0.2853\n",
      "      3       \u001b[36m63.8727\u001b[0m       \u001b[32m65.3572\u001b[0m  0.2840\n",
      "      4       \u001b[36m63.8579\u001b[0m       65.5304  0.2853\n",
      "      5       \u001b[36m63.3510\u001b[0m       65.3600  0.2798\n",
      "      6       \u001b[36m63.0959\u001b[0m       \u001b[32m65.2243\u001b[0m  0.2930\n",
      "      7       \u001b[36m62.7608\u001b[0m       65.3504  0.2818\n",
      "      8       \u001b[36m62.6335\u001b[0m       66.1131  0.2826\n",
      "      9       62.7068       65.7249  0.2864\n",
      "     10       \u001b[36m62.1707\u001b[0m       66.2086  0.2992\n",
      "Restoring best model from epoch 6.\n",
      "load_rgbsg\n",
      "(2232, 9) (2232,)\n",
      "split horm_treatment      category\n",
      "menopause           category\n",
      "age                  float32\n",
      "n_positive_nodes     float32\n",
      "progesterone         float32\n",
      "estrogene            float32\n",
      "grade_0.0              uint8\n",
      "grade_1.0              uint8\n",
      "grade_2.0              uint8\n",
      "dtype: object\n",
      "(1785, 9) <class 'pandas.core.frame.DataFrame'>\n",
      "(1785,) <class 'pandas.core.series.Series'>\n",
      "(447, 9) <class 'pandas.core.frame.DataFrame'>\n",
      "(447,) <class 'pandas.core.series.Series'>\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m108.8526\u001b[0m       \u001b[32m91.8840\u001b[0m  0.0656\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m111.0556\u001b[0m       \u001b[32m89.7221\u001b[0m  0.0608\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m110.6445\u001b[0m       \u001b[32m91.3429\u001b[0m  0.0724\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m127.0623\u001b[0m      \u001b[32m107.8717\u001b[0m  0.0676\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m118.2279\u001b[0m       \u001b[32m97.1071\u001b[0m  0.0764\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m456.9252\u001b[0m      \u001b[32m364.7076\u001b[0m  0.0936\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m457.0542\u001b[0m      \u001b[32m362.3557\u001b[0m  0.0906\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m478.1340\u001b[0m      \u001b[32m377.8270\u001b[0m  0.0917\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m495.4048\u001b[0m      \u001b[32m398.7436\u001b[0m  0.0834\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m524.9656\u001b[0m      \u001b[32m437.3743\u001b[0m  0.0783\n",
      "      2      \u001b[36m103.8273\u001b[0m       92.6879  0.0673\n",
      "      2       \u001b[36m99.7207\u001b[0m       \u001b[32m89.5449\u001b[0m  0.0778\n",
      "      2      \u001b[36m122.4871\u001b[0m      108.4463  0.0641\n",
      "      2      \u001b[36m102.5909\u001b[0m       \u001b[32m89.5117\u001b[0m  0.0865\n",
      "      2      \u001b[36m109.0694\u001b[0m       \u001b[32m96.3274\u001b[0m  0.0704\n",
      "      2      \u001b[36m421.2127\u001b[0m      \u001b[32m370.2648\u001b[0m  0.0762\n",
      "      3      \u001b[36m101.2156\u001b[0m       \u001b[32m90.2767\u001b[0m  0.0637\n",
      "      3      \u001b[36m117.5548\u001b[0m      108.3021  0.0595\n",
      "      2      \u001b[36m423.7906\u001b[0m      377.0587  0.0976\n",
      "      2      \u001b[36m480.9211\u001b[0m      441.2793  0.0868\n",
      "      2      \u001b[36m414.7122\u001b[0m      \u001b[32m355.3669\u001b[0m  0.0999\n",
      "      2      \u001b[36m441.0537\u001b[0m      \u001b[32m396.5189\u001b[0m  0.0954\n",
      "      3      \u001b[36m100.2609\u001b[0m       91.0402  0.0752\n",
      "      3       \u001b[36m96.1834\u001b[0m       \u001b[32m89.2036\u001b[0m  0.0897\n",
      "      3      \u001b[36m104.4002\u001b[0m       \u001b[32m93.4257\u001b[0m  0.0771\n",
      "      4       \u001b[36m99.8884\u001b[0m       \u001b[32m89.5388\u001b[0m  0.0649\n",
      "      4      \u001b[36m115.9996\u001b[0m      \u001b[32m106.6858\u001b[0m  0.0636\n",
      "      4       \u001b[36m99.4253\u001b[0m       91.0775  0.0615\n",
      "      3      \u001b[36m391.5642\u001b[0m      \u001b[32m369.3691\u001b[0m  0.0943\n",
      "      3      \u001b[36m385.4635\u001b[0m      366.2520  0.0832\n",
      "      3      \u001b[36m409.6434\u001b[0m      \u001b[32m390.3647\u001b[0m  0.0805\n",
      "      3      \u001b[36m449.4533\u001b[0m      \u001b[32m422.1886\u001b[0m  0.0917\n",
      "      4      \u001b[36m103.3756\u001b[0m       93.6385  0.0694\n",
      "      3      \u001b[36m393.6639\u001b[0m      \u001b[32m361.9183\u001b[0m  0.1014\n",
      "      4       \u001b[36m95.6907\u001b[0m       90.6426  0.0893\n",
      "      5      \u001b[36m115.3278\u001b[0m      106.7600  0.0714\n",
      "      5       \u001b[36m98.7399\u001b[0m       89.8124  0.0725\n",
      "      5       \u001b[36m98.8461\u001b[0m       89.6489  0.0639\n",
      "      5      \u001b[36m102.2457\u001b[0m       94.3466  0.0659\n",
      "      4      \u001b[36m390.8881\u001b[0m      398.8314  0.0742\n",
      "      4      \u001b[36m366.8616\u001b[0m      \u001b[32m364.4772\u001b[0m  0.0857\n",
      "      4      \u001b[36m370.0917\u001b[0m      359.6511  0.0837\n",
      "      5       \u001b[36m93.6715\u001b[0m       89.9440  0.0629\n",
      "      6      \u001b[36m115.1566\u001b[0m      \u001b[32m105.8591\u001b[0m  0.0575\n",
      "      4      \u001b[36m382.0562\u001b[0m      \u001b[32m356.6837\u001b[0m  0.0883\n",
      "      4      \u001b[36m429.1528\u001b[0m      \u001b[32m420.9128\u001b[0m  0.0978\n",
      "      6       \u001b[36m98.4562\u001b[0m       \u001b[32m89.4402\u001b[0m  0.0643\n",
      "      6       \u001b[36m97.9008\u001b[0m       \u001b[32m89.2426\u001b[0m  0.0819\n",
      "      6      103.1383       \u001b[32m93.2035\u001b[0m  0.0685\n",
      "      6       93.7821       90.2192  0.0633\n",
      "      5      \u001b[36m388.3241\u001b[0m      \u001b[32m390.3254\u001b[0m  0.0899\n",
      "      7      \u001b[36m114.1612\u001b[0m      106.4562  0.0728\n",
      "      7       \u001b[36m97.2976\u001b[0m       90.3259  0.0626\n",
      "      5      \u001b[36m427.7679\u001b[0m      422.1282  0.0740\n",
      "      5      \u001b[36m376.5893\u001b[0m      \u001b[32m355.8789\u001b[0m  0.0828\n",
      "      7       \u001b[36m96.7541\u001b[0m       89.6409  0.0633\n",
      "      5      \u001b[36m362.7814\u001b[0m      362.0802  0.1107\n",
      "      5      \u001b[36m356.5324\u001b[0m      \u001b[32m357.2871\u001b[0m  0.1106\n",
      "      7      \u001b[36m102.1193\u001b[0m       93.5951  0.0686\n",
      "      7       94.7545       90.4392  0.0726\n",
      "      8      \u001b[36m113.9576\u001b[0m      \u001b[32m104.3524\u001b[0m  0.0638\n",
      "      8       97.5451       90.4473  0.0598\n",
      "      8       \u001b[36m96.0110\u001b[0m       90.1737  0.0606\n",
      "      6      \u001b[36m376.4033\u001b[0m      \u001b[32m381.1139\u001b[0m  0.0811\n",
      "      6      \u001b[36m360.7570\u001b[0m      357.5574  0.0775\n",
      "      8      102.2012       93.5582  0.0658      6      \u001b[36m417.8688\u001b[0m      424.3262  0.0993\n",
      "\n",
      "      6      \u001b[36m365.7144\u001b[0m      357.1965  0.0943\n",
      "      8       \u001b[36m93.2342\u001b[0m       90.5764  0.0598\n",
      "      6      \u001b[36m347.9788\u001b[0m      358.1768  0.1064\n",
      "      9       \u001b[36m96.6550\u001b[0m       90.7297  0.0701\n",
      "      9      \u001b[36m113.7636\u001b[0m      106.4945  0.0837\n",
      "      9       96.6759       89.3048  0.0766\n",
      "      7      \u001b[36m373.9083\u001b[0m      \u001b[32m380.2953\u001b[0m  0.0810\n",
      "      9      \u001b[36m101.5613\u001b[0m       93.5130  0.0595\n",
      "      9       \u001b[36m93.0429\u001b[0m       90.1126  0.0660\n",
      "      7      \u001b[36m353.9740\u001b[0m      361.6469  0.0884\n",
      "      7      366.1573      364.3299  0.0848\n",
      "      7      \u001b[36m415.5644\u001b[0m      426.8313  0.0897\n",
      "     10      \u001b[36m112.9929\u001b[0m      106.4857  0.0599\n",
      "Restoring best model from epoch 8.\n",
      "     10       96.5098       \u001b[32m88.8755\u001b[0m  0.0636\n",
      "      7      \u001b[36m344.9310\u001b[0m      362.5561  0.0900\n",
      "     10       96.7629       90.1904  0.0908\n",
      "Restoring best model from epoch 6.\n",
      "     10       \u001b[36m91.4394\u001b[0m       89.6854  0.0609\n",
      "     10      \u001b[36m101.4760\u001b[0m       93.2992  0.0707\n",
      "Restoring best model from epoch 3.\n",
      "Restoring best model from epoch 6.\n",
      "      8      \u001b[36m367.8855\u001b[0m      384.5724  0.0848\n",
      "      8      \u001b[36m345.0034\u001b[0m      364.1434  0.0825\n",
      "      8      \u001b[36m356.4388\u001b[0m      361.4316  0.0772\n",
      "      8      \u001b[36m406.4864\u001b[0m      425.7790  0.0769\n",
      "      8      \u001b[36m338.1819\u001b[0m      362.9871  0.0755\n",
      "      9      \u001b[36m358.5037\u001b[0m      389.9695  0.0745\n",
      "      9      \u001b[36m337.0033\u001b[0m      361.8930  0.0733\n",
      "      9      \u001b[36m343.7081\u001b[0m      366.2455  0.0732\n",
      "      9      \u001b[36m400.1884\u001b[0m      \u001b[32m418.2112\u001b[0m  0.0735\n",
      "      9      \u001b[36m330.4257\u001b[0m      363.5969  0.0739\n",
      "     10      361.6703      394.4641  0.0735\n",
      "Restoring best model from epoch 7.\n",
      "     10      342.5279      359.4315  0.0734\n",
      "Restoring best model from epoch 2.\n",
      "     10      \u001b[36m343.5295\u001b[0m      369.9402  0.0737\n",
      "Restoring best model from epoch 5.\n",
      "     10      \u001b[36m396.7499\u001b[0m      430.3822  0.0742\n",
      "Restoring best model from epoch 9.\n",
      "     10      \u001b[36m326.8338\u001b[0m      371.8725  0.0743\n",
      "Restoring best model from epoch 5.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m121.3995\u001b[0m      \u001b[32m102.4728\u001b[0m  0.0864\n",
      "      2      \u001b[36m114.4507\u001b[0m      \u001b[32m102.3276\u001b[0m  0.0822\n",
      "      3      \u001b[36m111.6730\u001b[0m       \u001b[32m99.5386\u001b[0m  0.0964\n",
      "      4      \u001b[36m110.8121\u001b[0m       99.9766  0.0840\n",
      "      5      \u001b[36m109.6643\u001b[0m      100.5743  0.0813\n",
      "      6      \u001b[36m108.9265\u001b[0m       \u001b[32m99.4891\u001b[0m  0.0819\n",
      "      7      \u001b[36m107.8537\u001b[0m       99.9749  0.0869\n",
      "      8      107.9495       \u001b[32m99.1864\u001b[0m  0.0833\n",
      "      9      \u001b[36m107.2098\u001b[0m       99.3752  0.0869\n",
      "     10      107.8500       99.6050  0.0828\n",
      "Restoring best model from epoch 8.\n",
      "(1785, 9) <class 'pandas.core.frame.DataFrame'>\n",
      "(1785,) <class 'pandas.core.series.Series'>\n",
      "(447, 9) <class 'pandas.core.frame.DataFrame'>\n",
      "(447,) <class 'pandas.core.series.Series'>\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m108.2069\u001b[0m       \u001b[32m92.7772\u001b[0m  0.0257\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m105.5002\u001b[0m       \u001b[32m95.0092\u001b[0m  0.0258\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m105.9729\u001b[0m       \u001b[32m90.9797\u001b[0m  0.0248\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m113.9379\u001b[0m      \u001b[32m100.3971\u001b[0m  0.0257\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m123.8985\u001b[0m      \u001b[32m113.0774\u001b[0m  0.0251\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m220.9468\u001b[0m      \u001b[32m183.7448\u001b[0m  0.0287\n",
      "      2       \u001b[36m96.2714\u001b[0m       \u001b[32m93.4579\u001b[0m  0.0250\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m217.3744\u001b[0m      \u001b[32m191.1776\u001b[0m  0.0290\n",
      "      2       \u001b[36m94.4623\u001b[0m       \u001b[32m88.5222\u001b[0m  0.0241\n",
      "      2      \u001b[36m102.8316\u001b[0m       \u001b[32m96.9035\u001b[0m  0.0265\n",
      "      2      \u001b[36m112.7862\u001b[0m      \u001b[32m110.8853\u001b[0m  0.0241\n",
      "      2       \u001b[36m98.3743\u001b[0m       \u001b[32m91.5734\u001b[0m  0.0452\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m222.7849\u001b[0m      \u001b[32m190.3337\u001b[0m  0.0397\n",
      "      3       \u001b[36m94.1573\u001b[0m       93.9257  0.0246\n",
      "      2      \u001b[36m199.0099\u001b[0m      \u001b[32m186.0501\u001b[0m  0.0274\n",
      "      3       \u001b[36m90.7143\u001b[0m       88.6638  0.0272\n",
      "      2      \u001b[36m202.1774\u001b[0m      \u001b[32m180.0776\u001b[0m  0.0336\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m258.5459\u001b[0m      \u001b[32m222.6375\u001b[0m  0.0379\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m238.2250\u001b[0m      \u001b[32m200.5234\u001b[0m  0.0469\n",
      "      3      \u001b[36m109.8903\u001b[0m      110.9247  0.0258\n",
      "      3       \u001b[36m98.8915\u001b[0m       \u001b[32m96.3105\u001b[0m  0.0292\n",
      "      3       \u001b[36m96.2790\u001b[0m       92.4124  0.0329\n",
      "      2      \u001b[36m198.1380\u001b[0m      \u001b[32m177.9028\u001b[0m  0.0318\n",
      "      4       \u001b[36m92.9549\u001b[0m       \u001b[32m93.2620\u001b[0m  0.0269\n",
      "      3      \u001b[36m193.8596\u001b[0m      181.2822  0.0280\n",
      "      2      \u001b[36m210.8204\u001b[0m      \u001b[32m192.9272\u001b[0m  0.0268\n",
      "      4       \u001b[36m90.3265\u001b[0m       89.4471  0.0351\n",
      "      3      \u001b[36m187.0262\u001b[0m      \u001b[32m183.8256\u001b[0m  0.0366\n",
      "      2      \u001b[36m229.8850\u001b[0m      \u001b[32m217.5080\u001b[0m  0.0360\n",
      "      4      \u001b[36m108.7952\u001b[0m      \u001b[32m110.6058\u001b[0m  0.0302\n",
      "      4       98.9556       \u001b[32m95.7878\u001b[0m  0.0326\n",
      "      3      \u001b[36m184.0471\u001b[0m      \u001b[32m172.3222\u001b[0m  0.0266\n",
      "      5       \u001b[36m92.8009\u001b[0m       94.6207  0.0253\n",
      "      4       \u001b[36m95.1116\u001b[0m       92.9628  0.0320\n",
      "      5       \u001b[36m88.9818\u001b[0m       89.3782  0.0273\n",
      "      4      \u001b[36m192.0967\u001b[0m      180.9894  0.0357\n",
      "      4      187.0918      184.4191  0.0342\n",
      "      5      \u001b[36m107.8601\u001b[0m      111.4522  0.0322\n",
      "      3      \u001b[36m218.8405\u001b[0m      \u001b[32m214.7288\u001b[0m  0.0330\n",
      "      3      \u001b[36m200.3076\u001b[0m      \u001b[32m189.1426\u001b[0m  0.0421\n",
      "      4      \u001b[36m180.5555\u001b[0m      172.8830  0.0293\n",
      "      6       \u001b[36m92.2829\u001b[0m       95.0258  0.0298\n",
      "      5       \u001b[36m94.2913\u001b[0m       96.0451  0.0276\n",
      "      5       \u001b[36m97.7908\u001b[0m       96.3288  0.0387\n",
      "      6       \u001b[36m88.7512\u001b[0m       89.8889  0.0293\n",
      "      5      \u001b[36m187.0151\u001b[0m      181.8007  0.0285\n",
      "      6      \u001b[36m107.2114\u001b[0m      111.6872  0.0242\n",
      "      5      \u001b[36m178.6332\u001b[0m      185.6099  0.0308\n",
      "      4      \u001b[36m217.1519\u001b[0m      215.3663  0.0323\n",
      "      6       \u001b[36m93.7190\u001b[0m       94.6414  0.0270\n",
      "      5      \u001b[36m178.3469\u001b[0m      \u001b[32m172.2700\u001b[0m  0.0290\n",
      "      7       \u001b[36m91.3402\u001b[0m       94.1688  0.0298\n",
      "      6       \u001b[36m96.9770\u001b[0m       96.6234  0.0349\n",
      "      4      \u001b[36m197.5375\u001b[0m      190.2437  0.0438\n",
      "      7       \u001b[36m87.6441\u001b[0m       89.7872  0.0306\n",
      "      7      \u001b[36m106.2997\u001b[0m      111.7055  0.0292\n",
      "      5      \u001b[36m215.2746\u001b[0m      216.6508  0.0280\n",
      "      6      187.8123      184.0391  0.0406\n",
      "      8       \u001b[36m91.2748\u001b[0m       94.4202  0.0263\n",
      "      6      178.5991      173.8659  0.0289\n",
      "      6      181.1857      186.5340  0.0377\n",
      "      7       \u001b[36m93.0873\u001b[0m       95.8757  0.0330\n",
      "      7       \u001b[36m96.2275\u001b[0m       95.8504  0.0279\n",
      "      5      \u001b[36m195.4984\u001b[0m      \u001b[32m188.6042\u001b[0m  0.0327\n",
      "      8       88.5600       89.7553  0.0308\n",
      "      8      \u001b[36m105.6892\u001b[0m      112.5102  0.0348\n",
      "      7      \u001b[36m183.6220\u001b[0m      184.6704  0.0296\n",
      "      8       \u001b[36m92.9558\u001b[0m       95.3543  0.0253\n",
      "      6      \u001b[36m212.0094\u001b[0m      217.5902  0.0336\n",
      "      9       \u001b[36m90.3328\u001b[0m       95.5400  0.0326\n",
      "      7      \u001b[36m172.3213\u001b[0m      172.6112  0.0332\n",
      "      7      \u001b[36m176.3816\u001b[0m      188.3826  0.0319\n",
      "      8       \u001b[36m96.0269\u001b[0m       96.7298  0.0260\n",
      "      9       \u001b[36m85.6534\u001b[0m       89.5559  0.0278\n",
      "      6      \u001b[36m193.8215\u001b[0m      191.9719  0.0297\n",
      "      9      106.3284      114.1447  0.0301\n",
      "      9       \u001b[36m90.0717\u001b[0m       95.8455  0.0264\n",
      "      8      \u001b[36m182.9555\u001b[0m      186.7129  0.0330\n",
      "      8      173.6901      174.0988  0.0299\n",
      "      8      \u001b[36m175.7120\u001b[0m      188.2873  0.0283\n",
      "     10       \u001b[36m89.2047\u001b[0m       95.3033  0.0398\n",
      "Restoring best model from epoch 4.\n",
      "      9       \u001b[36m95.5969\u001b[0m       98.0865  0.0350\n",
      "      7      \u001b[36m190.3996\u001b[0m      189.9741  0.0341\n",
      "      7      \u001b[36m210.4958\u001b[0m      217.1741  0.0584\n",
      "     10      \u001b[36m105.1940\u001b[0m      112.5907  0.0359\n",
      "Restoring best model from epoch 4.\n",
      "     10       \u001b[36m90.0547\u001b[0m       96.4243  0.0361\n",
      "Restoring best model from epoch 2.\n",
      "     10       \u001b[36m85.2868\u001b[0m       89.5256  0.0502\n",
      "Restoring best model from epoch 2.\n",
      "      9      \u001b[36m179.8823\u001b[0m      187.5696  0.0355\n",
      "      9      \u001b[36m173.1010\u001b[0m      188.0850  0.0374\n",
      "      9      \u001b[36m170.1931\u001b[0m      175.6929  0.0397\n",
      "      8      190.8874      190.0318  0.0354\n",
      "     10       \u001b[36m94.3389\u001b[0m       97.5654  0.0420\n",
      "Restoring best model from epoch 4.\n",
      "      8      211.1021      216.6331  0.0359\n",
      "     10      \u001b[36m179.3585\u001b[0m      187.8470  0.0308\n",
      "Restoring best model from epoch 2.\n",
      "     10      173.4297      188.0930  0.0286\n",
      "Restoring best model from epoch 3.\n",
      "     10      170.6292      177.9466  0.0291\n",
      "Restoring best model from epoch 5.\n",
      "      9      190.9224      190.5417  0.0268\n",
      "      9      \u001b[36m208.5515\u001b[0m      217.0167  0.0264\n",
      "     10      \u001b[36m188.2487\u001b[0m      189.7507  0.0275\n",
      "Restoring best model from epoch 5.\n",
      "     10      \u001b[36m207.1322\u001b[0m      217.4723  0.0263\n",
      "Restoring best model from epoch 3.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m117.2570\u001b[0m      \u001b[32m104.7207\u001b[0m  0.0339\n",
      "      2      \u001b[36m106.9894\u001b[0m      \u001b[32m102.5021\u001b[0m  0.0354\n",
      "      3      \u001b[36m104.3756\u001b[0m      103.5524  0.0372\n",
      "      4      \u001b[36m103.2342\u001b[0m      102.6973  0.0356\n",
      "      5      \u001b[36m102.1126\u001b[0m      103.2468  0.0358\n",
      "      6      \u001b[36m102.1124\u001b[0m      102.8604  0.0361\n",
      "      7      \u001b[36m101.3116\u001b[0m      102.8973  0.0327\n",
      "      8      101.5363      \u001b[32m102.4825\u001b[0m  0.0375\n",
      "      9      102.0335      103.0548  0.0345\n",
      "     10      \u001b[36m101.0219\u001b[0m      103.1364  0.0351\n",
      "Restoring best model from epoch 8.\n",
      "(1786, 9) <class 'pandas.core.frame.DataFrame'>\n",
      "(1786,) <class 'pandas.core.series.Series'>\n",
      "(446, 9) <class 'pandas.core.frame.DataFrame'>\n",
      "(446,) <class 'pandas.core.series.Series'>\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m104.3477\u001b[0m       \u001b[32m93.2958\u001b[0m  0.0265\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m105.0190\u001b[0m       \u001b[32m93.6387\u001b[0m  0.0271\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m107.1350\u001b[0m       \u001b[32m91.5042\u001b[0m  0.0263\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m112.4684\u001b[0m      \u001b[32m100.2710\u001b[0m  0.0301\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m124.5889\u001b[0m      \u001b[32m109.1968\u001b[0m  0.0265\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m216.4459\u001b[0m      \u001b[32m184.9532\u001b[0m  0.0314\n",
      "      2       \u001b[36m95.9585\u001b[0m       \u001b[32m92.3850\u001b[0m  0.0241\n",
      "      2       \u001b[36m96.4781\u001b[0m       \u001b[32m91.8542\u001b[0m  0.0285\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m214.3074\u001b[0m      \u001b[32m187.4456\u001b[0m  0.0308\n",
      "      2       \u001b[36m93.8431\u001b[0m       \u001b[32m88.4803\u001b[0m  0.0345\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m219.5523\u001b[0m      \u001b[32m191.1433\u001b[0m  0.0378\n",
      "      2      \u001b[36m103.4043\u001b[0m       \u001b[32m97.0701\u001b[0m  0.0324\n",
      "      2      \u001b[36m114.1461\u001b[0m      \u001b[32m106.9787\u001b[0m  0.0316\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m254.6457\u001b[0m      \u001b[32m219.2713\u001b[0m  0.0302\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m231.2740\u001b[0m      \u001b[32m202.4797\u001b[0m  0.0392\n",
      "      3       \u001b[36m94.1786\u001b[0m       92.3265  0.0299\n",
      "      3       \u001b[36m93.6216\u001b[0m       92.8176  0.0306\n",
      "      2      \u001b[36m193.6484\u001b[0m      \u001b[32m183.7268\u001b[0m  0.0341\n",
      "      2      \u001b[36m195.6359\u001b[0m      \u001b[32m180.2892\u001b[0m  0.0388\n",
      "      3       \u001b[36m99.1185\u001b[0m       \u001b[32m96.6286\u001b[0m  0.0254\n",
      "      3       \u001b[36m90.9264\u001b[0m       \u001b[32m88.3973\u001b[0m  0.0319\n",
      "      2      \u001b[36m194.2710\u001b[0m      \u001b[32m179.4847\u001b[0m  0.0321\n",
      "      2      \u001b[36m228.7465\u001b[0m      \u001b[32m211.6977\u001b[0m  0.0306\n",
      "      2      \u001b[36m206.2277\u001b[0m      \u001b[32m194.4238\u001b[0m  0.0283\n",
      "      4       \u001b[36m91.9454\u001b[0m       \u001b[32m92.2576\u001b[0m  0.0285\n",
      "      4       \u001b[36m91.9735\u001b[0m       93.4010  0.0310\n",
      "      3      \u001b[36m189.4379\u001b[0m      181.3117  0.0295\n",
      "      3      \u001b[36m109.9150\u001b[0m      106.9797  0.0457\n",
      "      4       \u001b[36m88.8699\u001b[0m       88.5543  0.0309\n",
      "      4       \u001b[36m97.6977\u001b[0m       \u001b[32m96.3220\u001b[0m  0.0533\n",
      "      3      \u001b[36m181.1606\u001b[0m      \u001b[32m175.0440\u001b[0m  0.0517\n",
      "      3      \u001b[36m185.5962\u001b[0m      \u001b[32m182.6316\u001b[0m  0.0631\n",
      "      5       93.0209       93.6250  0.0432\n",
      "      3      \u001b[36m194.0844\u001b[0m      \u001b[32m191.8948\u001b[0m  0.0492\n",
      "      5       92.3461       92.8647  0.0523\n",
      "      4      \u001b[36m186.9935\u001b[0m      181.8520  0.0488\n",
      "      3      \u001b[36m216.3719\u001b[0m      \u001b[32m209.2761\u001b[0m  0.0612\n",
      "      4      \u001b[36m109.0137\u001b[0m      107.8148  0.0517\n",
      "      5       89.0412       89.2649  0.0426\n",
      "      5       \u001b[36m97.6627\u001b[0m       97.0463  0.0335\n",
      "      4      \u001b[36m176.1273\u001b[0m      \u001b[32m173.1298\u001b[0m  0.0360\n",
      "      5      \u001b[36m108.0831\u001b[0m      108.5464  0.0240\n",
      "      6       \u001b[36m88.1620\u001b[0m       88.9724  0.0238\n",
      "      5      \u001b[36m184.4889\u001b[0m      182.4580  0.0299\n",
      "      4      217.1140      211.1494  0.0289\n",
      "      4      \u001b[36m181.7488\u001b[0m      \u001b[32m180.7539\u001b[0m  0.0448\n",
      "      4      \u001b[36m191.4918\u001b[0m      \u001b[32m191.2521\u001b[0m  0.0389\n",
      "      6       \u001b[36m90.8207\u001b[0m       \u001b[32m92.2082\u001b[0m  0.0396\n",
      "      6       \u001b[36m91.9317\u001b[0m       93.4332  0.0485\n",
      "      6       \u001b[36m96.4560\u001b[0m       96.9645  0.0292\n",
      "      6      \u001b[36m108.0721\u001b[0m      108.1926  0.0237\n",
      "      7       \u001b[36m86.8258\u001b[0m       89.5289  0.0234\n",
      "      6      \u001b[36m183.4984\u001b[0m      181.0969  0.0266\n",
      "      5      \u001b[36m188.9078\u001b[0m      192.7640  0.0281\n",
      "      5      \u001b[36m174.1121\u001b[0m      174.4758  0.0380\n",
      "      7       \u001b[36m89.7991\u001b[0m       93.2279  0.0282\n",
      "      7       \u001b[36m90.6503\u001b[0m       93.6944  0.0296\n",
      "      5      \u001b[36m211.7933\u001b[0m      213.2007  0.0394\n",
      "      5      \u001b[36m177.7579\u001b[0m      180.8001  0.0406\n",
      "      7       \u001b[36m95.9366\u001b[0m       97.6023  0.0267\n",
      "      7      \u001b[36m106.9289\u001b[0m      109.6101  0.0233\n",
      "      8       87.1930       90.7090  0.0334\n",
      "      6      \u001b[36m173.6197\u001b[0m      176.3436  0.0263\n",
      "      7      \u001b[36m179.0128\u001b[0m      183.6296  0.0299\n",
      "      8       91.5533       93.7742  0.0271\n",
      "      8       \u001b[36m89.4494\u001b[0m       93.4040  0.0312\n",
      "      8       96.2042       98.1958  0.0239\n",
      "      6      212.1918      215.0693  0.0272\n",
      "      6      \u001b[36m187.5108\u001b[0m      193.9535  0.0397\n",
      "      8      107.4078      109.5544  0.0348\n",
      "      7      \u001b[36m169.2309\u001b[0m      177.1828  0.0261\n",
      "      6      179.3113      182.0371  0.0419\n",
      "      8      180.1498      184.1869  0.0293\n",
      "      9       87.0489       91.7998  0.0319\n",
      "      9       90.3051       94.1506  0.0259\n",
      "      9       \u001b[36m95.0016\u001b[0m       98.1375  0.0255\n",
      "      7      \u001b[36m186.0081\u001b[0m      195.5990  0.0271\n",
      "      9       \u001b[36m89.5728\u001b[0m       93.6464  0.0454\n",
      "      7      \u001b[36m176.3726\u001b[0m      182.1027  0.0280\n",
      "     10       \u001b[36m84.3461\u001b[0m       91.6200  0.0253\n",
      "Restoring best model from epoch 3.\n",
      "      9      \u001b[36m104.0271\u001b[0m      109.4612  0.0342\n",
      "      8      \u001b[36m168.7227\u001b[0m      177.8464  0.0325\n",
      "     10       \u001b[36m88.2629\u001b[0m       94.5295  0.0251\n",
      "Restoring best model from epoch 6.\n",
      "      9      \u001b[36m176.9777\u001b[0m      184.3088  0.0301\n",
      "     10       \u001b[36m94.7289\u001b[0m       98.3130  0.0269\n",
      "Restoring best model from epoch 4.\n",
      "      7      \u001b[36m207.8356\u001b[0m      215.9227  0.0538\n",
      "      8      \u001b[36m184.5188\u001b[0m      197.4705  0.0310\n",
      "     10       \u001b[36m88.9184\u001b[0m       93.9852  0.0243\n",
      "Restoring best model from epoch 2.\n",
      "     10      \u001b[36m104.0118\u001b[0m      108.9861  0.0241\n",
      "Restoring best model from epoch 2.\n",
      "      9      170.4507      178.6625  0.0269\n",
      "      8      \u001b[36m174.3878\u001b[0m      180.8535  0.0320\n",
      "     10      \u001b[36m176.3320\u001b[0m      184.4124  0.0308\n",
      "Restoring best model from epoch 2.\n",
      "      8      \u001b[36m207.7715\u001b[0m      217.3640  0.0276\n",
      "      9      \u001b[36m181.4975\u001b[0m      197.3789  0.0270\n",
      "     10      \u001b[36m167.3489\u001b[0m      176.3881  0.0279\n",
      "      9      \u001b[36m173.1636\u001b[0m      180.9389  0.0267\n",
      "Restoring best model from epoch 4.\n",
      "      9      \u001b[36m201.8939\u001b[0m      218.5697  0.0275\n",
      "     10      183.1751      200.3907  0.0279\n",
      "Restoring best model from epoch 4.\n",
      "     10      173.2830      184.2097  0.0267\n",
      "Restoring best model from epoch 4.\n",
      "     10      203.1026      218.5480  0.0269\n",
      "Restoring best model from epoch 3.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m118.7310\u001b[0m      \u001b[32m102.9173\u001b[0m  0.0352\n",
      "      2      \u001b[36m108.2351\u001b[0m      \u001b[32m101.8913\u001b[0m  0.0344\n",
      "      3      \u001b[36m105.5057\u001b[0m      101.9794  0.0333\n",
      "      4      \u001b[36m103.5805\u001b[0m      \u001b[32m101.8710\u001b[0m  0.0331\n",
      "      5      104.1443      103.0987  0.0335\n",
      "      6      103.7080      102.0420  0.0336\n",
      "      7      \u001b[36m102.1810\u001b[0m      102.3974  0.0343\n",
      "      8      102.3959      102.1248  0.0341\n",
      "      9      102.2207      102.0079  0.0337\n",
      "     10      \u001b[36m101.2607\u001b[0m      102.4247  0.0326\n",
      "Restoring best model from epoch 4.\n",
      "(1786, 9) <class 'pandas.core.frame.DataFrame'>\n",
      "(1786,) <class 'pandas.core.series.Series'>\n",
      "(446, 9) <class 'pandas.core.frame.DataFrame'>\n",
      "(446,) <class 'pandas.core.series.Series'>\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m106.5156\u001b[0m       \u001b[32m92.6747\u001b[0m  0.0270\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m104.5371\u001b[0m       \u001b[32m91.9062\u001b[0m  0.0267\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m105.2698\u001b[0m       \u001b[32m88.9416\u001b[0m  0.0265\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m114.8886\u001b[0m       \u001b[32m97.7902\u001b[0m  0.0267\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m122.5567\u001b[0m      \u001b[32m107.9647\u001b[0m  0.0250\n",
      "      2       \u001b[36m96.6581\u001b[0m       \u001b[32m92.0218\u001b[0m  0.0245\n",
      "      2       \u001b[36m95.2001\u001b[0m       \u001b[32m90.0128\u001b[0m  0.0257\n",
      "      2       \u001b[36m91.6049\u001b[0m       \u001b[32m86.9950\u001b[0m  0.0250\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m217.8213\u001b[0m      \u001b[32m184.0819\u001b[0m  0.0268\n",
      "      2      \u001b[36m101.6913\u001b[0m       \u001b[32m94.2594\u001b[0m  0.0246\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m218.5139\u001b[0m      \u001b[32m182.7601\u001b[0m  0.0381\n",
      "      2      \u001b[36m110.0893\u001b[0m      \u001b[32m106.8693\u001b[0m  0.0278\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m218.7708\u001b[0m      \u001b[32m183.4652\u001b[0m  0.0338\n",
      "      3       \u001b[36m93.8329\u001b[0m       92.3969  0.0257\n",
      "      3       \u001b[36m92.5350\u001b[0m       \u001b[32m89.4751\u001b[0m  0.0338\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m251.8578\u001b[0m      \u001b[32m214.0528\u001b[0m  0.0309\n",
      "      3       \u001b[36m99.8110\u001b[0m       \u001b[32m93.7394\u001b[0m  0.0283\n",
      "      3       \u001b[36m89.6037\u001b[0m       \u001b[32m85.9090\u001b[0m  0.0332\n",
      "      3      \u001b[36m108.2331\u001b[0m      \u001b[32m106.7447\u001b[0m  0.0269\n",
      "      2      \u001b[36m191.3684\u001b[0m      \u001b[32m176.8258\u001b[0m  0.0390\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m232.7531\u001b[0m      \u001b[32m196.6012\u001b[0m  0.0410\n",
      "      2      \u001b[36m197.2352\u001b[0m      \u001b[32m178.7713\u001b[0m  0.0326\n",
      "      2      \u001b[36m190.2698\u001b[0m      \u001b[32m171.8956\u001b[0m  0.0345\n",
      "      4       \u001b[36m93.3374\u001b[0m       \u001b[32m91.8494\u001b[0m  0.0310\n",
      "      4       \u001b[36m91.2056\u001b[0m       90.3533  0.0312\n",
      "      4       \u001b[36m98.1331\u001b[0m       94.3234  0.0293\n",
      "      2      \u001b[36m224.6389\u001b[0m      \u001b[32m206.7565\u001b[0m  0.0310\n",
      "      3      \u001b[36m186.5450\u001b[0m      178.0923  0.0269\n",
      "      4       \u001b[36m87.4099\u001b[0m       86.5543  0.0341\n",
      "      3      \u001b[36m186.3365\u001b[0m      181.7127  0.0303\n",
      "      2      \u001b[36m209.6631\u001b[0m      \u001b[32m186.4907\u001b[0m  0.0333\n",
      "      5       \u001b[36m92.1312\u001b[0m       \u001b[32m91.6005\u001b[0m  0.0291\n",
      "      3      \u001b[36m180.2172\u001b[0m      \u001b[32m171.5832\u001b[0m  0.0301\n",
      "      4      \u001b[36m106.6902\u001b[0m      106.9031  0.0425\n",
      "      5       \u001b[36m90.7217\u001b[0m       92.1929  0.0241\n",
      "      5       \u001b[36m97.5154\u001b[0m       94.0550  0.0287\n",
      "      3      \u001b[36m216.5387\u001b[0m      210.3771  0.0294\n",
      "      4      \u001b[36m180.2314\u001b[0m      177.5175  0.0337\n",
      "      5      \u001b[36m105.9287\u001b[0m      107.6866  0.0239\n",
      "      5       \u001b[36m87.0953\u001b[0m       87.0155  0.0375\n",
      "      4      \u001b[36m175.3001\u001b[0m      \u001b[32m169.2955\u001b[0m  0.0263\n",
      "      3      \u001b[36m197.3731\u001b[0m      \u001b[32m185.8221\u001b[0m  0.0338\n",
      "      6       \u001b[36m91.9665\u001b[0m       91.7394  0.0301\n",
      "      4      \u001b[36m183.9609\u001b[0m      181.0419  0.0418\n",
      "      6       \u001b[36m90.1109\u001b[0m       92.7809  0.0366\n",
      "      4      \u001b[36m212.6699\u001b[0m      210.9808  0.0355\n",
      "      5      \u001b[36m171.3267\u001b[0m      171.6783  0.0275\n",
      "      6      \u001b[36m105.4575\u001b[0m      107.5415  0.0293\n",
      "      7       \u001b[36m89.8012\u001b[0m       91.9900  0.0269\n",
      "      5      \u001b[36m178.0103\u001b[0m      179.4828  0.0370\n",
      "      6       \u001b[36m86.8458\u001b[0m       88.3598  0.0320\n",
      "      6       \u001b[36m96.6908\u001b[0m       94.3970  0.0484\n",
      "      4      \u001b[36m193.3307\u001b[0m      185.8372  0.0325\n",
      "      5      \u001b[36m182.2223\u001b[0m      183.3049  0.0292\n",
      "      7       \u001b[36m88.4374\u001b[0m       92.1295  0.0356\n",
      "      7       \u001b[36m86.3356\u001b[0m       89.1984  0.0240\n",
      "      8       91.6880       91.9261  0.0285\n",
      "      7      \u001b[36m104.3124\u001b[0m      109.5162  0.0311\n",
      "      7       \u001b[36m96.4860\u001b[0m       95.7764  0.0252\n",
      "      6      \u001b[36m176.1728\u001b[0m      181.2394  0.0276\n",
      "      5      \u001b[36m208.8876\u001b[0m      209.9310  0.0370\n",
      "      6      \u001b[36m171.0111\u001b[0m      172.5405  0.0370\n",
      "      5      \u001b[36m190.0142\u001b[0m      186.9924  0.0330\n",
      "      6      \u001b[36m181.9675\u001b[0m      183.5305  0.0374\n",
      "      8       \u001b[36m85.4979\u001b[0m       90.3211  0.0256\n",
      "      8       \u001b[36m95.7040\u001b[0m       96.2863  0.0238\n",
      "      9       \u001b[36m88.7150\u001b[0m       92.8355  0.0264\n",
      "      7      \u001b[36m173.0112\u001b[0m      180.8264  0.0283\n",
      "      8      \u001b[36m103.7509\u001b[0m      109.3330  0.0311\n",
      "      6      210.3123      211.1515  0.0350\n",
      "      7      \u001b[36m167.1939\u001b[0m      173.6176  0.0335\n",
      "      8       88.4973       93.5790  0.0452\n",
      "      6      190.5363      187.3129  0.0318\n",
      "      7      \u001b[36m177.8816\u001b[0m      182.9093  0.0295\n",
      "      9       \u001b[36m85.0963\u001b[0m       89.3235  0.0310\n",
      "      9       \u001b[36m94.8866\u001b[0m       97.2830  0.0294\n",
      "     10       89.9833       92.8994  0.0379\n",
      "Restoring best model from epoch 5.\n",
      "      9       \u001b[36m88.2383\u001b[0m       93.2367  0.0257\n",
      "      8      174.6727      180.3719  0.0366\n",
      "      9      \u001b[36m103.6090\u001b[0m      109.9318  0.0373\n",
      "      7      \u001b[36m187.5999\u001b[0m      188.1627  0.0297\n",
      "      8      \u001b[36m176.5340\u001b[0m      183.0105  0.0290\n",
      "      7      \u001b[36m206.5361\u001b[0m      212.4340  0.0398\n",
      "      8      167.8054      176.4796  0.0387\n",
      "     10       \u001b[36m93.2127\u001b[0m       97.0395  0.0290\n",
      "Restoring best model from epoch 3.\n",
      "     10       \u001b[36m82.3410\u001b[0m       89.6398  0.0318\n",
      "Restoring best model from epoch 3.\n",
      "     10       \u001b[36m86.1735\u001b[0m       93.1838  0.0268\n",
      "Restoring best model from epoch 3.\n",
      "     10      \u001b[36m101.0192\u001b[0m      110.1865  0.0253\n",
      "Restoring best model from epoch 3.\n",
      "      9      \u001b[36m170.0664\u001b[0m      181.3066  0.0326\n",
      "      9      \u001b[36m172.0524\u001b[0m      183.3067  0.0272\n",
      "      8      \u001b[36m186.6476\u001b[0m      189.5840  0.0327\n",
      "      9      \u001b[36m166.5143\u001b[0m      178.0275  0.0269\n",
      "      8      \u001b[36m204.8197\u001b[0m      211.5117  0.0317\n",
      "     10      171.7692      182.0320  0.0280\n",
      "Restoring best model from epoch 2.\n",
      "     10      175.2584      182.8529  0.0271\n",
      "Restoring best model from epoch 2.\n",
      "      9      \u001b[36m184.5506\u001b[0m      191.8436  0.0271\n",
      "     10      168.9460      181.8754  0.0271\n",
      "Restoring best model from epoch 4.\n",
      "      9      \u001b[36m199.1241\u001b[0m      211.8824  0.0269\n",
      "     10      186.3032      191.4300  0.0260\n",
      "Restoring best model from epoch 3.\n",
      "     10      202.3553      211.9598  0.0261\n",
      "Restoring best model from epoch 2.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m117.5141\u001b[0m      \u001b[32m100.7488\u001b[0m  0.0352\n",
      "      2      \u001b[36m106.5419\u001b[0m       \u001b[32m99.7909\u001b[0m  0.0359\n",
      "      3      \u001b[36m104.0093\u001b[0m       99.7934  0.0348\n",
      "      4      \u001b[36m103.0386\u001b[0m       99.9547  0.0461\n",
      "      5      \u001b[36m101.9348\u001b[0m       \u001b[32m99.0950\u001b[0m  0.0363\n",
      "      6      \u001b[36m101.7134\u001b[0m       99.3964  0.0332\n",
      "      7      102.0545       99.9674  0.0350\n",
      "      8      \u001b[36m101.6457\u001b[0m      101.4346  0.0343\n",
      "      9      \u001b[36m100.9085\u001b[0m      101.2324  0.0325\n",
      "     10      \u001b[36m100.3561\u001b[0m      100.5581  0.0332\n",
      "Restoring best model from epoch 5.\n",
      "(1786, 9) <class 'pandas.core.frame.DataFrame'>\n",
      "(1786,) <class 'pandas.core.series.Series'>\n",
      "(446, 9) <class 'pandas.core.frame.DataFrame'>\n",
      "(446,) <class 'pandas.core.series.Series'>\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m107.3019\u001b[0m       \u001b[32m92.4093\u001b[0m  0.0285\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m104.3435\u001b[0m       \u001b[32m91.8908\u001b[0m  0.0358\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m106.1339\u001b[0m       \u001b[32m87.4887\u001b[0m  0.0303\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m113.2666\u001b[0m       \u001b[32m99.3417\u001b[0m  0.0278\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m124.2080\u001b[0m      \u001b[32m109.2250\u001b[0m  0.0283\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m217.1216\u001b[0m      \u001b[32m182.5272\u001b[0m  0.0294\n",
      "      2       \u001b[36m97.4369\u001b[0m       \u001b[32m89.5822\u001b[0m  0.0303\n",
      "      2       \u001b[36m94.6402\u001b[0m       \u001b[32m84.4253\u001b[0m  0.0300\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m218.3231\u001b[0m      \u001b[32m183.2816\u001b[0m  0.0338\n",
      "      2      \u001b[36m102.5536\u001b[0m       \u001b[32m96.9497\u001b[0m  0.0304\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m220.9682\u001b[0m      \u001b[32m181.7378\u001b[0m  0.0333\n",
      "      2       \u001b[36m96.7134\u001b[0m       \u001b[32m91.0729\u001b[0m  0.0382\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m231.4088\u001b[0m      \u001b[32m197.0338\u001b[0m  0.0336\n",
      "      2      \u001b[36m113.4746\u001b[0m      \u001b[32m107.5230\u001b[0m  0.0313\n",
      "      2      \u001b[36m196.9109\u001b[0m      \u001b[32m178.8743\u001b[0m  0.0298\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m257.8542\u001b[0m      \u001b[32m215.7476\u001b[0m  0.0393\n",
      "      3       \u001b[36m95.5541\u001b[0m       89.7793  0.0331\n",
      "      3       \u001b[36m91.3740\u001b[0m       84.9145  0.0279\n",
      "      3       \u001b[36m93.8969\u001b[0m       91.5222  0.0254\n",
      "      2      \u001b[36m203.0333\u001b[0m      \u001b[32m177.3921\u001b[0m  0.0349\n",
      "      2      \u001b[36m198.0349\u001b[0m      \u001b[32m170.8504\u001b[0m  0.0344\n",
      "      3       \u001b[36m98.9185\u001b[0m       \u001b[32m96.6374\u001b[0m  0.0435\n",
      "      2      \u001b[36m209.4360\u001b[0m      \u001b[32m191.0159\u001b[0m  0.0353\n",
      "      3      \u001b[36m191.2663\u001b[0m      \u001b[32m178.8055\u001b[0m  0.0298\n",
      "      3      \u001b[36m110.3738\u001b[0m      \u001b[32m107.4434\u001b[0m  0.0310\n",
      "      4       \u001b[36m92.7175\u001b[0m       93.5081  0.0314\n",
      "      4       \u001b[36m90.5442\u001b[0m       85.1011  0.0350\n",
      "      2      \u001b[36m228.3799\u001b[0m      \u001b[32m212.5331\u001b[0m  0.0376\n",
      "      4       \u001b[36m94.1575\u001b[0m       \u001b[32m89.4763\u001b[0m  0.0361\n",
      "      3      \u001b[36m189.6626\u001b[0m      178.8094  0.0407\n",
      "      4      \u001b[36m109.8480\u001b[0m      107.7890  0.0269\n",
      "      3      \u001b[36m184.8136\u001b[0m      \u001b[32m167.5821\u001b[0m  0.0370\n",
      "      4       98.9902       97.3599  0.0328\n",
      "      4      \u001b[36m188.0467\u001b[0m      \u001b[32m178.4711\u001b[0m  0.0316\n",
      "      3      \u001b[36m197.7521\u001b[0m      \u001b[32m190.8327\u001b[0m  0.0339\n",
      "      5       92.7644       93.3041  0.0280\n",
      "      3      \u001b[36m221.5274\u001b[0m      213.0509  0.0311\n",
      "      5       \u001b[36m89.7299\u001b[0m       85.6250  0.0372\n",
      "      5       \u001b[36m93.8058\u001b[0m       89.7369  0.0396\n",
      "      4      \u001b[36m185.2645\u001b[0m      \u001b[32m177.2102\u001b[0m  0.0305\n",
      "      4      \u001b[36m181.7380\u001b[0m      \u001b[32m165.3534\u001b[0m  0.0308\n",
      "      5       \u001b[36m97.6426\u001b[0m       98.7315  0.0316\n",
      "      6       \u001b[36m92.0976\u001b[0m       93.1710  0.0248\n",
      "      5      \u001b[36m185.3603\u001b[0m      \u001b[32m177.9343\u001b[0m  0.0350\n",
      "      5      \u001b[36m109.5941\u001b[0m      109.0567  0.0426\n",
      "      4      \u001b[36m218.0443\u001b[0m      \u001b[32m212.4083\u001b[0m  0.0266\n",
      "      4      \u001b[36m194.5808\u001b[0m      \u001b[32m190.6434\u001b[0m  0.0452\n",
      "      6       93.9233       \u001b[32m89.2483\u001b[0m  0.0243\n",
      "      6       90.1015       86.5398  0.0365\n",
      "      5      \u001b[36m185.0023\u001b[0m      \u001b[32m177.1521\u001b[0m  0.0297\n",
      "      5      \u001b[36m176.9972\u001b[0m      165.6652  0.0298\n",
      "      7       \u001b[36m91.3425\u001b[0m       93.0275  0.0273\n",
      "      6      \u001b[36m184.7406\u001b[0m      179.5115  0.0264\n",
      "      6      \u001b[36m108.5498\u001b[0m      109.2908  0.0243\n",
      "      5      \u001b[36m211.9174\u001b[0m      216.6967  0.0317\n",
      "      7       \u001b[36m91.8556\u001b[0m       \u001b[32m89.1429\u001b[0m  0.0272\n",
      "      6       97.7578       97.7735  0.0456\n",
      "      5      \u001b[36m192.1905\u001b[0m      193.4388  0.0336\n",
      "      6      177.3499      165.3902  0.0256\n",
      "      6      185.4208      \u001b[32m177.0941\u001b[0m  0.0274\n",
      "      7      \u001b[36m107.1836\u001b[0m      108.8098  0.0235\n",
      "      7      \u001b[36m179.7323\u001b[0m      179.5627  0.0261\n",
      "      7       \u001b[36m88.3959\u001b[0m       85.9710  0.0338\n",
      "      8       91.4736       93.2640  0.0396\n",
      "      8       \u001b[36m91.8308\u001b[0m       89.8493  0.0269\n",
      "      6      212.9109      216.3114  0.0305\n",
      "      8      107.2511      109.7469  0.0236\n",
      "      8       \u001b[36m87.8798\u001b[0m       86.4616  0.0241\n",
      "      7      \u001b[36m180.6127\u001b[0m      \u001b[32m176.5178\u001b[0m  0.0300\n",
      "      7      \u001b[36m172.0381\u001b[0m      166.1932  0.0323\n",
      "      7       \u001b[36m95.7417\u001b[0m       98.7554  0.0397\n",
      "      6      193.4088      193.2594  0.0391\n",
      "      8      \u001b[36m179.3555\u001b[0m      179.2015  0.0394\n",
      "      9       \u001b[36m89.8171\u001b[0m       94.6453  0.0297\n",
      "      7      \u001b[36m208.9356\u001b[0m      216.3474  0.0273\n",
      "      9      107.8645      109.2938  0.0234\n",
      "      9       \u001b[36m91.6424\u001b[0m       90.2508  0.0343\n",
      "      9       \u001b[36m86.7860\u001b[0m       86.2581  0.0276\n",
      "      8       96.7228       98.3467  0.0254\n",
      "      8      173.6276      167.9856  0.0321\n",
      "      8      \u001b[36m179.5950\u001b[0m      179.4593  0.0328\n",
      "     10      \u001b[36m105.4000\u001b[0m      109.3084  0.0234\n",
      "Restoring best model from epoch 3.\n",
      "     10       \u001b[36m89.0575\u001b[0m       96.0445  0.0280\n",
      "Restoring best model from epoch 2.\n",
      "      8      \u001b[36m208.8487\u001b[0m      218.5876  0.0308\n",
      "      9      180.5621      180.3682  0.0348\n",
      "     10       \u001b[36m86.0796\u001b[0m       86.1139  0.0242\n",
      "Restoring best model from epoch 2.\n",
      "      9       96.4477       98.2834  0.0260\n",
      "      9      \u001b[36m177.5560\u001b[0m      179.7850  0.0266\n",
      "      9      \u001b[36m169.2781\u001b[0m      167.4225  0.0268\n",
      "     10       92.1318       89.8139  0.0384\n",
      "Restoring best model from epoch 7.\n",
      "      7      \u001b[36m186.2948\u001b[0m      194.0965  0.0563\n",
      "     10      181.3637      179.7765  0.0265\n",
      "Restoring best model from epoch 5.\n",
      "      9      \u001b[36m205.8025\u001b[0m      218.1852  0.0286\n",
      "     10       \u001b[36m94.0300\u001b[0m       98.2663  0.0271\n",
      "Restoring best model from epoch 3.\n",
      "     10      177.7918      178.9137  0.0264\n",
      "     10      \u001b[36m167.5473\u001b[0m      169.3518  0.0265\n",
      "Restoring best model from epoch 7.\n",
      "Restoring best model from epoch 4.\n",
      "      8      188.0432      194.3798  0.0281\n",
      "     10      \u001b[36m203.4738\u001b[0m      216.0162  0.0325\n",
      "Restoring best model from epoch 4.\n",
      "      9      \u001b[36m183.5517\u001b[0m      193.8742  0.0359\n",
      "     10      \u001b[36m181.9945\u001b[0m      194.4064  0.0269\n",
      "Restoring best model from epoch 4.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m117.9084\u001b[0m      \u001b[32m106.5363\u001b[0m  0.0358\n",
      "      2      \u001b[36m108.7699\u001b[0m      \u001b[32m104.9329\u001b[0m  0.0369\n",
      "      3      \u001b[36m104.6771\u001b[0m      \u001b[32m104.4264\u001b[0m  0.0343\n",
      "      4      \u001b[36m104.4622\u001b[0m      104.7953  0.0329\n",
      "      5      \u001b[36m103.3268\u001b[0m      104.4618  0.0339\n",
      "      6      103.5100      104.7179  0.0345\n",
      "      7      \u001b[36m101.9644\u001b[0m      105.7396  0.0329\n",
      "      8      102.8481      105.1260  0.0347\n",
      "      9      102.2538      105.2392  0.0333\n",
      "     10      \u001b[36m100.1911\u001b[0m      104.9280  0.0327\n",
      "Restoring best model from epoch 3.\n",
      "load_support\n",
      "(8873, 16) (8873,)\n",
      "split age                  float32\n",
      "sex                 category\n",
      "race                 float32\n",
      "n_comorbidities      float32\n",
      "diabetes            category\n",
      "dementia            category\n",
      "blood_pressure       float32\n",
      "heart_rate           float32\n",
      "respiration_rate     float32\n",
      "temperature          float32\n",
      "white_blood_cell     float32\n",
      "serum_sodium         float32\n",
      "serum_creatinine     float32\n",
      "cancer_0.0             uint8\n",
      "cancer_1.0             uint8\n",
      "cancer_2.0             uint8\n",
      "dtype: object\n",
      "(7098, 16) <class 'pandas.core.frame.DataFrame'>\n",
      "(7098,) <class 'pandas.core.series.Series'>\n",
      "(1775, 16) <class 'pandas.core.frame.DataFrame'>\n",
      "(1775,) <class 'pandas.core.series.Series'>\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m92.7508\u001b[0m       \u001b[32m81.2191\u001b[0m  0.1111\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m80.6598\u001b[0m       \u001b[32m59.5438\u001b[0m  0.1024\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m92.8549\u001b[0m       \u001b[32m80.8610\u001b[0m  0.1145\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m83.1311\u001b[0m       \u001b[32m65.3429\u001b[0m  0.1114\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m82.4339\u001b[0m       \u001b[32m66.2405\u001b[0m  0.1073\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m194.1606\u001b[0m      \u001b[32m163.1141\u001b[0m  0.1213\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m179.0354\u001b[0m      \u001b[32m132.2903\u001b[0m  0.1206\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m192.6601\u001b[0m      \u001b[32m160.6081\u001b[0m  0.1329\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m178.1101\u001b[0m      \u001b[32m123.5234\u001b[0m  0.1287\n",
      "      2       \u001b[36m72.6597\u001b[0m       \u001b[32m65.3098\u001b[0m  0.1123\n",
      "      2       \u001b[36m85.2924\u001b[0m       \u001b[32m80.7046\u001b[0m  0.1148\n",
      "      2       \u001b[36m65.1912\u001b[0m       59.7723  0.1165\n",
      "      2       \u001b[36m85.1178\u001b[0m       \u001b[32m81.1547\u001b[0m  0.1252\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m182.2259\u001b[0m      \u001b[32m134.5004\u001b[0m  0.1520\n",
      "      2       \u001b[36m67.3568\u001b[0m       \u001b[32m65.6755\u001b[0m  0.1245\n",
      "      2      \u001b[36m148.8823\u001b[0m      \u001b[32m131.8270\u001b[0m  0.1189\n",
      "      2      \u001b[36m172.3684\u001b[0m      \u001b[32m161.1556\u001b[0m  0.1624\n",
      "      2      \u001b[36m172.6071\u001b[0m      160.6991  0.1345\n",
      "      3       \u001b[36m64.2919\u001b[0m       \u001b[32m58.7331\u001b[0m  0.1540\n",
      "      2      \u001b[36m132.6502\u001b[0m      \u001b[32m117.6998\u001b[0m  0.1639\n",
      "      3       \u001b[36m71.9080\u001b[0m       \u001b[32m64.4513\u001b[0m  0.1914\n",
      "      3       \u001b[36m83.9903\u001b[0m       \u001b[32m80.2739\u001b[0m  0.2216\n",
      "      3       \u001b[36m84.3707\u001b[0m       \u001b[32m79.9288\u001b[0m  0.2307\n",
      "      3       \u001b[36m66.4990\u001b[0m       \u001b[32m64.9998\u001b[0m  0.2517\n",
      "      2      \u001b[36m137.8408\u001b[0m      \u001b[32m132.0999\u001b[0m  0.2545\n",
      "      3      \u001b[36m144.0321\u001b[0m      \u001b[32m129.1138\u001b[0m  0.2782\n",
      "      3      \u001b[36m169.5775\u001b[0m      \u001b[32m160.3948\u001b[0m  0.3197\n",
      "      4       \u001b[36m63.5278\u001b[0m       58.8761  0.2530\n",
      "      3      \u001b[36m169.8951\u001b[0m      \u001b[32m159.0041\u001b[0m  0.3353\n",
      "      4       \u001b[36m83.2470\u001b[0m       80.3389  0.2144\n",
      "      4       \u001b[36m83.5512\u001b[0m       80.2629  0.2140\n",
      "      4       \u001b[36m71.1039\u001b[0m       \u001b[32m64.4123\u001b[0m  0.2554\n",
      "      3      \u001b[36m128.7635\u001b[0m      117.8138  0.2940\n",
      "      4       \u001b[36m65.8056\u001b[0m       \u001b[32m64.8098\u001b[0m  0.1898\n",
      "      3      \u001b[36m132.9953\u001b[0m      \u001b[32m130.3197\u001b[0m  0.2140\n",
      "      4      \u001b[36m142.5950\u001b[0m      130.6466  0.1808\n",
      "      5       \u001b[36m62.7671\u001b[0m       59.0144  0.1341\n",
      "      4      \u001b[36m166.4773\u001b[0m      160.5734  0.1600\n",
      "      5       \u001b[36m65.2316\u001b[0m       65.2690  0.1161\n",
      "      5       \u001b[36m82.4685\u001b[0m       \u001b[32m80.2276\u001b[0m  0.1410\n",
      "      5       \u001b[36m82.6916\u001b[0m       79.9299  0.1467\n",
      "      4      \u001b[36m127.4272\u001b[0m      \u001b[32m117.1112\u001b[0m  0.1473\n",
      "      4      \u001b[36m167.7125\u001b[0m      159.8705  0.1825\n",
      "      5       \u001b[36m70.3527\u001b[0m       64.4937  0.1520\n",
      "      4      \u001b[36m131.7930\u001b[0m      130.6345  0.1338\n",
      "      6       62.8617       59.0413  0.1357\n",
      "      5      \u001b[36m139.8160\u001b[0m      130.6043  0.1580\n",
      "      6       \u001b[36m65.1684\u001b[0m       65.0178  0.1037\n",
      "      5      \u001b[36m165.7094\u001b[0m      \u001b[32m160.3646\u001b[0m  0.1284\n",
      "      6       \u001b[36m82.3244\u001b[0m       80.3776  0.1131\n",
      "      6       70.5686       64.6114  0.1123\n",
      "      5      \u001b[36m126.0893\u001b[0m      117.5058  0.1233\n",
      "      5      \u001b[36m164.9922\u001b[0m      160.2843  0.1233\n",
      "      6       82.7059       \u001b[32m79.9102\u001b[0m  0.1289\n",
      "      5      \u001b[36m130.0668\u001b[0m      \u001b[32m129.8443\u001b[0m  0.1354\n",
      "      7       \u001b[36m62.4915\u001b[0m       58.9606  0.1146\n",
      "      6      139.9977      130.7915  0.1195\n",
      "      7       \u001b[36m64.9463\u001b[0m       65.0331  0.1167\n",
      "      7       \u001b[36m82.0343\u001b[0m       80.5419  0.1098\n",
      "      6      \u001b[36m164.1469\u001b[0m      \u001b[32m159.5394\u001b[0m  0.1561\n",
      "      7       \u001b[36m82.2266\u001b[0m       79.9217  0.1196\n",
      "      7       \u001b[36m69.9880\u001b[0m       64.7182  0.1375\n",
      "      6      \u001b[36m164.2692\u001b[0m      159.6852  0.1326\n",
      "      6      \u001b[36m125.7586\u001b[0m      117.6447  0.1433\n",
      "      8       \u001b[36m62.0651\u001b[0m       59.0392  0.1042\n",
      "      6      \u001b[36m129.6867\u001b[0m      130.4268  0.1409\n",
      "      8       \u001b[36m64.6475\u001b[0m       65.2519  0.1116\n",
      "      8       \u001b[36m81.8412\u001b[0m       80.3240  0.1035\n",
      "      7      \u001b[36m137.7096\u001b[0m      \u001b[32m129.0363\u001b[0m  0.1464\n",
      "      8       \u001b[36m69.4045\u001b[0m       \u001b[32m64.3593\u001b[0m  0.1175\n",
      "      8       \u001b[36m82.0634\u001b[0m       80.0438  0.1306\n",
      "      7      \u001b[36m162.7720\u001b[0m      159.6360  0.1365\n",
      "      7      \u001b[36m162.3967\u001b[0m      159.2335  0.1244\n",
      "      7      \u001b[36m123.3213\u001b[0m      \u001b[32m116.4660\u001b[0m  0.1218\n",
      "      9       62.1441       58.8199  0.1113\n",
      "      9       \u001b[36m64.6369\u001b[0m       65.0898  0.1091\n",
      "      7      \u001b[36m127.6645\u001b[0m      130.8300  0.1229\n",
      "      9       \u001b[36m81.8063\u001b[0m       80.5106  0.1191\n",
      "      9       69.6850       \u001b[32m64.1060\u001b[0m  0.1068\n",
      "      9       \u001b[36m81.9552\u001b[0m       \u001b[32m79.8127\u001b[0m  0.1044\n",
      "      8      \u001b[36m136.5946\u001b[0m      129.3227  0.1471\n",
      "      8      \u001b[36m160.8638\u001b[0m      161.1228  0.1333\n",
      "      8      \u001b[36m122.9234\u001b[0m      117.7491  0.1311\n",
      "     10       \u001b[36m64.4885\u001b[0m       65.1493  0.1039\n",
      "Restoring best model from epoch 4.\n",
      "      8      \u001b[36m161.8144\u001b[0m      160.9442  0.1462\n",
      "     10       \u001b[36m60.3177\u001b[0m       \u001b[32m58.7159\u001b[0m  0.1262\n",
      "      8      \u001b[36m126.7981\u001b[0m      131.5060  0.1345\n",
      "     10       \u001b[36m81.4383\u001b[0m       80.2669  0.1255\n",
      "Restoring best model from epoch 5.\n",
      "     10       \u001b[36m81.6461\u001b[0m       79.8507  0.0958\n",
      "Restoring best model from epoch 9.\n",
      "     10       \u001b[36m69.2571\u001b[0m       64.5042  0.1075\n",
      "Restoring best model from epoch 9.\n",
      "      9      \u001b[36m135.5607\u001b[0m      130.5928  0.1212\n",
      "      9      \u001b[36m120.3741\u001b[0m      118.4309  0.1166\n",
      "      9      \u001b[36m161.0639\u001b[0m      160.9006  0.1128\n",
      "      9      161.1926      160.1534  0.1255\n",
      "      9      \u001b[36m126.6920\u001b[0m      131.1804  0.1128\n",
      "     10      \u001b[36m135.3806\u001b[0m      129.5580  0.1126\n",
      "Restoring best model from epoch 7.\n",
      "     10      121.8598      118.7749  0.1136\n",
      "Restoring best model from epoch 7.\n",
      "     10      \u001b[36m160.9807\u001b[0m      160.2627  0.1125\n",
      "Restoring best model from epoch 6.\n",
      "     10      \u001b[36m156.9282\u001b[0m      \u001b[32m158.6779\u001b[0m  0.1131\n",
      "     10      \u001b[36m126.0078\u001b[0m      131.6025  0.1162\n",
      "Restoring best model from epoch 5.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m86.9315\u001b[0m       \u001b[32m78.5055\u001b[0m  0.1333\n",
      "      2       \u001b[36m77.3842\u001b[0m       \u001b[32m77.5835\u001b[0m  0.1558\n",
      "      3       \u001b[36m76.4673\u001b[0m       77.7040  0.1257\n",
      "      4       \u001b[36m75.6393\u001b[0m       \u001b[32m77.1598\u001b[0m  0.1313\n",
      "      5       \u001b[36m75.4667\u001b[0m       77.2468  0.1650\n",
      "      6       \u001b[36m75.1792\u001b[0m       77.5473  0.1268\n",
      "      7       \u001b[36m74.8084\u001b[0m       77.3549  0.1348\n",
      "      8       74.9713       77.7307  0.1286\n",
      "      9       \u001b[36m74.3165\u001b[0m       77.7515  0.1254\n",
      "     10       74.5344       77.6995  0.2010\n",
      "Restoring best model from epoch 4.\n",
      "(7098, 16) <class 'pandas.core.frame.DataFrame'>\n",
      "(7098,) <class 'pandas.core.series.Series'>\n",
      "(1775, 16) <class 'pandas.core.frame.DataFrame'>\n",
      "(1775,) <class 'pandas.core.series.Series'>\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m879.3062\u001b[0m      \u001b[32m751.2326\u001b[0m  0.0693\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m877.5748\u001b[0m      \u001b[32m743.2616\u001b[0m  0.0586\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m887.5722\u001b[0m      \u001b[32m737.2027\u001b[0m  0.0735\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m974.2916\u001b[0m      \u001b[32m794.4365\u001b[0m  0.0575\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m961.0860\u001b[0m      \u001b[32m778.7655\u001b[0m  0.0673\n",
      "      2      \u001b[36m775.1001\u001b[0m      \u001b[32m659.6533\u001b[0m  0.0594\n",
      "      2      \u001b[36m766.5811\u001b[0m      \u001b[32m651.4517\u001b[0m  0.0723\n",
      "      2      \u001b[36m765.5135\u001b[0m      \u001b[32m561.3989\u001b[0m  0.0616\n",
      "      2      \u001b[36m742.4563\u001b[0m      \u001b[32m549.0070\u001b[0m  0.0627\n",
      "      2      \u001b[36m735.0835\u001b[0m      \u001b[32m588.0770\u001b[0m  0.0962\n",
      "      3      \u001b[36m716.4442\u001b[0m      \u001b[32m622.6396\u001b[0m  0.0741\n",
      "      3      \u001b[36m712.8269\u001b[0m      \u001b[32m616.6249\u001b[0m  0.0666\n",
      "      3      \u001b[36m607.1592\u001b[0m      \u001b[32m490.2719\u001b[0m  0.0638\n",
      "      3      \u001b[36m595.8495\u001b[0m      \u001b[32m477.2400\u001b[0m  0.0748\n",
      "      3      \u001b[36m640.9812\u001b[0m      \u001b[32m518.7657\u001b[0m  0.0744\n",
      "      4      \u001b[36m704.1431\u001b[0m      \u001b[32m614.4885\u001b[0m  0.0728\n",
      "      4      \u001b[36m701.0752\u001b[0m      \u001b[32m608.3503\u001b[0m  0.0707\n",
      "      4      \u001b[36m574.5631\u001b[0m      \u001b[32m489.2177\u001b[0m  0.0723\n",
      "      4      \u001b[36m561.9531\u001b[0m      \u001b[32m476.5258\u001b[0m  0.0669\n",
      "      4      \u001b[36m608.8253\u001b[0m      \u001b[32m504.2707\u001b[0m  0.0749\n",
      "      5      \u001b[36m683.2509\u001b[0m      \u001b[32m606.2594\u001b[0m  0.0608\n",
      "      5      \u001b[36m681.9344\u001b[0m      \u001b[32m601.3311\u001b[0m  0.0766\n",
      "      5      \u001b[36m553.0705\u001b[0m      \u001b[32m489.0083\u001b[0m  0.0702\n",
      "      5      \u001b[36m530.3388\u001b[0m      \u001b[32m473.9171\u001b[0m  0.0716\n",
      "      5      \u001b[36m577.4863\u001b[0m      \u001b[32m498.2547\u001b[0m  0.0645\n",
      "      6      \u001b[36m675.1707\u001b[0m      \u001b[32m597.4417\u001b[0m  0.0562\n",
      "      6      \u001b[36m677.0253\u001b[0m      \u001b[32m601.7814\u001b[0m  0.0782\n",
      "      6      \u001b[36m540.7613\u001b[0m      \u001b[32m481.1291\u001b[0m  0.0644\n",
      "      6      \u001b[36m520.7074\u001b[0m      \u001b[32m468.6194\u001b[0m  0.0644\n",
      "      6      \u001b[36m575.2711\u001b[0m      \u001b[32m491.8202\u001b[0m  0.0707\n",
      "      7      \u001b[36m671.5527\u001b[0m      \u001b[32m597.2197\u001b[0m  0.0629\n",
      "      7      \u001b[36m672.0411\u001b[0m      \u001b[32m601.6823\u001b[0m  0.0852\n",
      "      7      \u001b[36m536.1822\u001b[0m      \u001b[32m479.3392\u001b[0m  0.0686\n",
      "      7      \u001b[36m516.0196\u001b[0m      \u001b[32m464.8290\u001b[0m  0.0619\n",
      "      7      \u001b[36m569.3508\u001b[0m      494.0728  0.0626\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m178.7904\u001b[0m      \u001b[32m133.5324\u001b[0m  0.5590\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m203.9361\u001b[0m      \u001b[32m168.3872\u001b[0m  0.5819\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m203.3130\u001b[0m      \u001b[32m168.5425\u001b[0m  0.5889\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m185.1193\u001b[0m      \u001b[32m139.8128\u001b[0m  0.5779\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m186.0422\u001b[0m      \u001b[32m138.2110\u001b[0m  0.5642\n",
      "      8      \u001b[36m664.2739\u001b[0m      \u001b[32m596.9069\u001b[0m  0.0803\n",
      "      8      \u001b[36m665.3959\u001b[0m      602.8310  0.0726\n",
      "      8      \u001b[36m533.2690\u001b[0m      480.3281  0.0779\n",
      "      8      \u001b[36m511.2769\u001b[0m      465.8413  0.0709\n",
      "      8      \u001b[36m562.1470\u001b[0m      493.7940  0.0727\n",
      "      9      \u001b[36m662.8472\u001b[0m      \u001b[32m596.8570\u001b[0m  0.0610\n",
      "      9      666.8647      602.9569  0.0639\n",
      "      9      \u001b[36m532.3334\u001b[0m      \u001b[32m478.0621\u001b[0m  0.0634\n",
      "      9      \u001b[36m508.5465\u001b[0m      \u001b[32m463.2827\u001b[0m  0.0642\n",
      "      9      562.2152      494.4321  0.0717\n",
      "     10      \u001b[36m662.0092\u001b[0m      \u001b[32m595.5124\u001b[0m  0.0809\n",
      "     10      \u001b[36m504.4972\u001b[0m      \u001b[32m463.1615\u001b[0m  0.0605\n",
      "     10      \u001b[36m663.7499\u001b[0m      602.0510  0.0736\n",
      "Restoring best model from epoch 7.\n",
      "     10      \u001b[36m528.5413\u001b[0m      478.1281  0.0755\n",
      "Restoring best model from epoch 9.\n",
      "     10      \u001b[36m556.7513\u001b[0m      493.7868  0.0601\n",
      "Restoring best model from epoch 6.\n",
      "      2      \u001b[36m178.5004\u001b[0m      \u001b[32m168.0831\u001b[0m  0.4729\n",
      "      2      \u001b[36m142.1949\u001b[0m      \u001b[32m133.6733\u001b[0m  0.4773\n",
      "      2      \u001b[36m134.6950\u001b[0m      \u001b[32m125.5520\u001b[0m  0.4906\n",
      "      2      \u001b[36m151.6137\u001b[0m      \u001b[32m137.9611\u001b[0m  0.4825\n",
      "      2      \u001b[36m178.9537\u001b[0m      \u001b[32m164.8559\u001b[0m  0.4906\n",
      "      3      \u001b[36m172.7588\u001b[0m      \u001b[32m166.9173\u001b[0m  0.4423\n",
      "      3      \u001b[36m136.5259\u001b[0m      \u001b[32m130.8451\u001b[0m  0.4420\n",
      "      3      \u001b[36m128.6571\u001b[0m      \u001b[32m125.4809\u001b[0m  0.4420\n",
      "      3      \u001b[36m145.9950\u001b[0m      \u001b[32m136.5543\u001b[0m  0.4404\n",
      "      3      \u001b[36m171.6102\u001b[0m      \u001b[32m164.1323\u001b[0m  0.4440\n",
      "      4      \u001b[36m168.7691\u001b[0m      \u001b[32m165.3505\u001b[0m  0.4605\n",
      "      4      \u001b[36m141.4176\u001b[0m      136.6119  0.4576\n",
      "      4      \u001b[36m133.8186\u001b[0m      131.0474  0.4608\n",
      "      4      \u001b[36m126.4001\u001b[0m      \u001b[32m124.7640\u001b[0m  0.4584\n",
      "      4      \u001b[36m168.6275\u001b[0m      \u001b[32m163.8937\u001b[0m  0.4614\n",
      "      5      \u001b[36m165.6442\u001b[0m      \u001b[32m163.8443\u001b[0m  0.4412\n",
      "      5      \u001b[36m132.5429\u001b[0m      \u001b[32m129.9811\u001b[0m  0.4446\n",
      "      5      \u001b[36m140.7405\u001b[0m      \u001b[32m135.9588\u001b[0m  0.4455\n",
      "      5      \u001b[36m124.6042\u001b[0m      125.2175  0.4460\n",
      "      5      \u001b[36m166.7208\u001b[0m      164.2734  0.4438\n",
      "      6      \u001b[36m165.2441\u001b[0m      164.1119  0.4467\n",
      "      6      124.6339      124.8089  0.4459\n",
      "      6      \u001b[36m131.9092\u001b[0m      \u001b[32m129.9097\u001b[0m  0.4462\n",
      "      6      \u001b[36m140.4853\u001b[0m      \u001b[32m135.5193\u001b[0m  0.4461\n",
      "      6      \u001b[36m165.6461\u001b[0m      \u001b[32m163.0445\u001b[0m  0.4469\n",
      "      7      \u001b[36m164.4126\u001b[0m      \u001b[32m163.5421\u001b[0m  0.4556\n",
      "      7      \u001b[36m130.9775\u001b[0m      130.5426  0.4530\n",
      "      7      \u001b[36m123.4005\u001b[0m      124.9461  0.4532\n",
      "      7      \u001b[36m139.0934\u001b[0m      136.0164  0.4546\n",
      "      7      \u001b[36m165.1642\u001b[0m      163.1704  0.4534\n",
      "      8      164.9082      165.0119  0.4411\n",
      "      8      131.1672      129.9255  0.4419\n",
      "      8      124.2860      \u001b[32m124.6577\u001b[0m  0.4427\n",
      "      8      139.3130      136.1743  0.4424\n",
      "      8      \u001b[36m164.3178\u001b[0m      \u001b[32m163.0098\u001b[0m  0.4411\n",
      "      9      \u001b[36m130.4212\u001b[0m      130.2019  0.4639\n",
      "      9      164.5784      164.6893  0.4751\n",
      "      9      165.3623      163.7145  0.4663\n",
      "      9      139.1516      \u001b[32m134.9839\u001b[0m  0.4822\n",
      "      9      \u001b[36m123.3129\u001b[0m      126.1417  0.4850\n",
      "     10      \u001b[36m130.4162\u001b[0m      130.0498  0.4423\n",
      "Restoring best model from epoch 6.\n",
      "     10      165.0261      164.0145  0.4451\n",
      "Restoring best model from epoch 7.\n",
      "     10      165.0752      \u001b[32m162.5335\u001b[0m  0.4401\n",
      "     10      139.7642      135.9769  0.4433\n",
      "     10      124.1888      125.6694  0.4415\n",
      "Restoring best model from epoch 9.\n",
      "Restoring best model from epoch 8.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m192.9568\u001b[0m      \u001b[32m160.8962\u001b[0m  0.3772\n",
      "      2      \u001b[36m160.5699\u001b[0m      \u001b[32m158.2367\u001b[0m  0.3766\n",
      "      3      \u001b[36m154.3184\u001b[0m      \u001b[32m157.4138\u001b[0m  0.3828\n",
      "      4      \u001b[36m153.1396\u001b[0m      \u001b[32m156.2719\u001b[0m  0.3670\n",
      "      5      \u001b[36m151.6025\u001b[0m      \u001b[32m155.6262\u001b[0m  0.3630\n",
      "      6      151.6888      \u001b[32m155.6048\u001b[0m  0.3646\n",
      "      7      \u001b[36m151.3691\u001b[0m      \u001b[32m155.4152\u001b[0m  0.3702\n",
      "      8      151.4081      155.7002  0.3767\n",
      "      9      151.4814      156.9520  0.3701\n",
      "     10      152.0541      \u001b[32m155.2068\u001b[0m  0.3722\n",
      "(7098, 16) <class 'pandas.core.frame.DataFrame'>\n",
      "(7098,) <class 'pandas.core.series.Series'>\n",
      "(1775, 16) <class 'pandas.core.frame.DataFrame'>\n",
      "(1775,) <class 'pandas.core.series.Series'>\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m875.3561\u001b[0m      \u001b[32m762.1481\u001b[0m  0.0759\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m876.0236\u001b[0m      \u001b[32m762.2085\u001b[0m  0.0883\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m882.1084\u001b[0m      \u001b[32m768.4040\u001b[0m  0.0904\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m959.7850\u001b[0m      \u001b[32m798.9835\u001b[0m  0.0776\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m980.1687\u001b[0m      \u001b[32m811.8964\u001b[0m  0.0768\n",
      "      2      \u001b[36m764.2101\u001b[0m      \u001b[32m667.9805\u001b[0m  0.0981\n",
      "      2      \u001b[36m756.7644\u001b[0m      \u001b[32m653.3980\u001b[0m  0.0994\n",
      "      2      \u001b[36m728.2778\u001b[0m      \u001b[32m557.7017\u001b[0m  0.0869\n",
      "      2      \u001b[36m724.4415\u001b[0m      \u001b[32m614.7547\u001b[0m  0.0968\n",
      "      2      \u001b[36m761.4081\u001b[0m      \u001b[32m577.5427\u001b[0m  0.0925\n",
      "      3      \u001b[36m713.1225\u001b[0m      \u001b[32m629.6867\u001b[0m  0.0982\n",
      "      3      \u001b[36m580.5719\u001b[0m      \u001b[32m463.7545\u001b[0m  0.0838\n",
      "      3      \u001b[36m710.5763\u001b[0m      \u001b[32m617.7053\u001b[0m  0.1021\n",
      "      3      \u001b[36m638.2194\u001b[0m      \u001b[32m540.6156\u001b[0m  0.0902\n",
      "      3      \u001b[36m606.0743\u001b[0m      \u001b[32m483.9683\u001b[0m  0.0986\n",
      "      4      \u001b[36m686.1674\u001b[0m      \u001b[32m618.6287\u001b[0m  0.0814\n",
      "      4      \u001b[36m544.9130\u001b[0m      \u001b[32m456.8957\u001b[0m  0.0895\n",
      "      4      \u001b[36m596.3416\u001b[0m      \u001b[32m525.2670\u001b[0m  0.0813\n",
      "      4      \u001b[36m560.4142\u001b[0m      \u001b[32m483.7974\u001b[0m  0.0912\n",
      "      5      \u001b[36m674.5098\u001b[0m      \u001b[32m612.6299\u001b[0m  0.0888\n",
      "      5      \u001b[36m518.0726\u001b[0m      458.3371  0.0846\n",
      "      5      \u001b[36m576.6512\u001b[0m      \u001b[32m524.5623\u001b[0m  0.0935\n",
      "      4      \u001b[36m686.0108\u001b[0m      \u001b[32m611.3105\u001b[0m  0.1989\n",
      "      6      \u001b[36m668.7607\u001b[0m      \u001b[32m608.0483\u001b[0m  0.0770\n",
      "      5      \u001b[36m542.7779\u001b[0m      486.4845  0.1024\n",
      "      6      \u001b[36m509.5324\u001b[0m      \u001b[32m453.4992\u001b[0m  0.0736\n",
      "      6      \u001b[36m566.6495\u001b[0m      \u001b[32m517.6420\u001b[0m  0.0847\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m201.0564\u001b[0m      \u001b[32m171.5223\u001b[0m  0.6376\n",
      "      7      \u001b[36m505.2817\u001b[0m      \u001b[32m450.1019\u001b[0m  0.0662\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m181.2353\u001b[0m      \u001b[32m143.3955\u001b[0m  0.6395\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m201.2938\u001b[0m      \u001b[32m167.1986\u001b[0m  0.6519\n",
      "      6      \u001b[36m537.6705\u001b[0m      \u001b[32m481.5795\u001b[0m  0.0836\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m175.5195\u001b[0m      \u001b[32m128.0402\u001b[0m  0.6444\n",
      "      7      \u001b[36m662.9798\u001b[0m      \u001b[32m606.2749\u001b[0m  0.0881\n",
      "      7      \u001b[36m565.9608\u001b[0m      \u001b[32m517.5450\u001b[0m  0.0625\n",
      "      5      \u001b[36m675.2346\u001b[0m      \u001b[32m607.8912\u001b[0m  0.1328\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m181.6814\u001b[0m      \u001b[32m136.4055\u001b[0m  0.6570\n",
      "      8      \u001b[36m500.9359\u001b[0m      450.7372  0.0628\n",
      "      8      \u001b[36m658.2717\u001b[0m      \u001b[32m605.1131\u001b[0m  0.0745\n",
      "      7      \u001b[36m529.7285\u001b[0m      \u001b[32m478.4631\u001b[0m  0.0805\n",
      "      8      \u001b[36m562.7592\u001b[0m      \u001b[32m514.9015\u001b[0m  0.0929\n",
      "      9      \u001b[36m497.3511\u001b[0m      \u001b[32m448.3256\u001b[0m  0.0975\n",
      "      8      \u001b[36m523.0972\u001b[0m      \u001b[32m477.5242\u001b[0m  0.0837\n",
      "      9      \u001b[36m655.7319\u001b[0m      \u001b[32m604.8185\u001b[0m  0.0929\n",
      "      9      \u001b[36m555.4613\u001b[0m      \u001b[32m514.8867\u001b[0m  0.0716\n",
      "      6      \u001b[36m670.0145\u001b[0m      \u001b[32m604.7453\u001b[0m  0.1663\n",
      "     10      498.1337      \u001b[32m447.8124\u001b[0m  0.0686\n",
      "     10      555.8605      \u001b[32m514.4893\u001b[0m  0.0562\n",
      "      9      \u001b[36m520.9786\u001b[0m      \u001b[32m475.5852\u001b[0m  0.0716\n",
      "     10      \u001b[36m655.6816\u001b[0m      \u001b[32m603.5045\u001b[0m  0.0674\n",
      "      7      \u001b[36m660.6789\u001b[0m      \u001b[32m603.4309\u001b[0m  0.0658\n",
      "     10      \u001b[36m518.4067\u001b[0m      475.9083  0.0593\n",
      "Restoring best model from epoch 9.\n",
      "      8      \u001b[36m659.0969\u001b[0m      \u001b[32m601.9033\u001b[0m  0.0566\n",
      "      9      \u001b[36m655.3748\u001b[0m      \u001b[32m600.7455\u001b[0m  0.0568\n",
      "     10      \u001b[36m654.3162\u001b[0m      600.8001  0.0586\n",
      "Restoring best model from epoch 9.\n",
      "      2      \u001b[36m177.3242\u001b[0m      \u001b[32m170.9860\u001b[0m  0.5199\n",
      "      2      \u001b[36m150.6521\u001b[0m      \u001b[32m141.5632\u001b[0m  0.5179\n",
      "      2      \u001b[36m134.3645\u001b[0m      \u001b[32m122.1544\u001b[0m  0.5093\n",
      "      2      \u001b[36m176.8159\u001b[0m      \u001b[32m167.1860\u001b[0m  0.5154\n",
      "      2      \u001b[36m140.1971\u001b[0m      \u001b[32m130.0105\u001b[0m  0.5633\n",
      "      3      \u001b[36m168.6899\u001b[0m      \u001b[32m170.9312\u001b[0m  0.5000\n",
      "      3      \u001b[36m169.8721\u001b[0m      168.2166  0.4914\n",
      "      3      \u001b[36m127.1568\u001b[0m      \u001b[32m121.0055\u001b[0m  0.4936\n",
      "      3      \u001b[36m144.2187\u001b[0m      \u001b[32m140.2574\u001b[0m  0.5001\n",
      "      3      \u001b[36m133.7196\u001b[0m      130.0841  0.4884\n",
      "      4      \u001b[36m165.9006\u001b[0m      \u001b[32m167.8824\u001b[0m  0.5000\n",
      "      4      \u001b[36m165.8237\u001b[0m      167.5578  0.5017\n",
      "      4      \u001b[36m140.4473\u001b[0m      \u001b[32m140.1422\u001b[0m  0.5005\n",
      "      4      \u001b[36m124.0607\u001b[0m      121.7559  0.5090\n",
      "      4      \u001b[36m132.0764\u001b[0m      \u001b[32m129.2281\u001b[0m  0.5017\n",
      "      5      \u001b[36m164.3520\u001b[0m      \u001b[32m166.4189\u001b[0m  0.4665\n",
      "      5      \u001b[36m164.9995\u001b[0m      \u001b[32m167.3461\u001b[0m  0.4696\n",
      "      5      \u001b[36m138.8359\u001b[0m      140.2631  0.4680\n",
      "      5      \u001b[36m123.7698\u001b[0m      121.3265  0.4671\n",
      "      5      \u001b[36m128.9265\u001b[0m      129.4075  0.4680\n",
      "      6      \u001b[36m163.9000\u001b[0m      166.4263  0.4801\n",
      "      6      \u001b[36m163.6340\u001b[0m      \u001b[32m165.9674\u001b[0m  0.4790\n",
      "      6      \u001b[36m137.8684\u001b[0m      140.7249  0.4772\n",
      "      6      \u001b[36m123.5385\u001b[0m      \u001b[32m120.9395\u001b[0m  0.4855\n",
      "      6      129.3451      \u001b[32m128.4048\u001b[0m  0.4748\n",
      "      7      \u001b[36m163.2205\u001b[0m      \u001b[32m164.9087\u001b[0m  0.4622\n",
      "      7      \u001b[36m162.4633\u001b[0m      \u001b[32m165.7930\u001b[0m  0.4624\n",
      "      7      \u001b[36m136.6415\u001b[0m      142.1038  0.4615\n",
      "      7      \u001b[36m122.2656\u001b[0m      \u001b[32m120.9284\u001b[0m  0.4635\n",
      "      7      \u001b[36m128.5078\u001b[0m      128.7490  0.4638\n",
      "      8      162.8899      166.1240  0.4748\n",
      "      8      \u001b[36m162.9931\u001b[0m      166.4866  0.4761\n",
      "      8      137.1682      141.3340  0.4743\n",
      "      8      122.4192      121.1134  0.4726\n",
      "      8      128.8516      \u001b[32m127.9664\u001b[0m  0.4727\n",
      "      9      \u001b[36m162.1652\u001b[0m      166.2029  0.4490\n",
      "      9      136.6821      140.8227  0.4489\n",
      "      9      163.1192      167.2616  0.4504\n",
      "      9      \u001b[36m121.7061\u001b[0m      121.0115  0.4497\n",
      "      9      128.7477      128.2708  0.4416\n",
      "     10      162.6030      166.4363  0.4373\n",
      "Restoring best model from epoch 7.\n",
      "     10      \u001b[36m136.1541\u001b[0m      141.9420  0.4442\n",
      "Restoring best model from epoch 4.\n",
      "     10      162.9403      168.8771  0.4473\n",
      "Restoring best model from epoch 7.\n",
      "     10      122.0126      121.4019  0.4456\n",
      "Restoring best model from epoch 6.\n",
      "     10      \u001b[36m127.3032\u001b[0m      128.4405  0.4477\n",
      "Restoring best model from epoch 8.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m897.9663\u001b[0m      \u001b[32m739.4470\u001b[0m  0.0794\n",
      "      2      \u001b[36m709.3721\u001b[0m      \u001b[32m596.4398\u001b[0m  0.0731\n",
      "      3      \u001b[36m654.7040\u001b[0m      \u001b[32m580.8911\u001b[0m  0.0776\n",
      "      4      \u001b[36m627.4418\u001b[0m      \u001b[32m577.7192\u001b[0m  0.0750\n",
      "      5      \u001b[36m619.0018\u001b[0m      \u001b[32m576.2131\u001b[0m  0.0769\n",
      "      6      \u001b[36m607.9890\u001b[0m      \u001b[32m572.8910\u001b[0m  0.0734\n",
      "      7      \u001b[36m605.5294\u001b[0m      572.8973  0.0740\n",
      "      8      605.5443      \u001b[32m572.8739\u001b[0m  0.0737\n",
      "      9      \u001b[36m600.9252\u001b[0m      573.9443  0.0743\n",
      "     10      \u001b[36m600.4978\u001b[0m      573.8612  0.0751\n",
      "Restoring best model from epoch 6.\n",
      "(7099, 16) <class 'pandas.core.frame.DataFrame'>\n",
      "(7099,) <class 'pandas.core.series.Series'>\n",
      "(1774, 16) <class 'pandas.core.frame.DataFrame'>\n",
      "(1774,) <class 'pandas.core.series.Series'>\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m875.6861\u001b[0m      \u001b[32m745.0620\u001b[0m  0.0565\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m878.0033\u001b[0m      \u001b[32m746.0236\u001b[0m  0.0604\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m971.2581\u001b[0m      \u001b[32m780.4063\u001b[0m  0.0605\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m891.6544\u001b[0m      \u001b[32m743.3250\u001b[0m  0.0798\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m976.5401\u001b[0m      \u001b[32m807.4478\u001b[0m  0.0714\n",
      "      2      \u001b[36m767.9997\u001b[0m      \u001b[32m652.2526\u001b[0m  0.0682\n",
      "      2      \u001b[36m776.0963\u001b[0m      \u001b[32m654.9130\u001b[0m  0.0661\n",
      "      2      \u001b[36m758.0600\u001b[0m      \u001b[32m541.1628\u001b[0m  0.0649\n",
      "      2      \u001b[36m739.7332\u001b[0m      \u001b[32m578.9630\u001b[0m  0.0751\n",
      "      2      \u001b[36m764.2762\u001b[0m      \u001b[32m567.0107\u001b[0m  0.0869\n",
      "      3      \u001b[36m721.4984\u001b[0m      \u001b[32m617.1116\u001b[0m  0.0694\n",
      "      3      \u001b[36m719.9714\u001b[0m      \u001b[32m622.8631\u001b[0m  0.0790\n",
      "      3      \u001b[36m599.4560\u001b[0m      \u001b[32m455.5948\u001b[0m  0.0876\n",
      "      3      \u001b[36m645.0577\u001b[0m      \u001b[32m505.9663\u001b[0m  0.0601\n",
      "      3      \u001b[36m618.9508\u001b[0m      \u001b[32m492.2051\u001b[0m  0.0577\n",
      "      4      \u001b[36m690.6533\u001b[0m      \u001b[32m614.5825\u001b[0m  0.0524\n",
      "      4      \u001b[36m692.9048\u001b[0m      \u001b[32m608.3197\u001b[0m  0.0698\n",
      "      4      \u001b[36m604.0600\u001b[0m      \u001b[32m496.5971\u001b[0m  0.0517\n",
      "      4      \u001b[36m566.5260\u001b[0m      455.7522  0.0615\n",
      "      4      \u001b[36m569.3676\u001b[0m      \u001b[32m489.7694\u001b[0m  0.0656\n",
      "      5      \u001b[36m679.1554\u001b[0m      \u001b[32m605.7396\u001b[0m  0.0619\n",
      "      5      \u001b[36m680.6610\u001b[0m      \u001b[32m598.4596\u001b[0m  0.0694\n",
      "      5      \u001b[36m584.7762\u001b[0m      \u001b[32m492.6219\u001b[0m  0.0636\n",
      "      5      \u001b[36m537.2646\u001b[0m      \u001b[32m452.7642\u001b[0m  0.0619\n",
      "      5      \u001b[36m557.1538\u001b[0m      490.2564  0.0559\n",
      "      6      \u001b[36m671.3289\u001b[0m      \u001b[32m599.8962\u001b[0m  0.0729\n",
      "      6      \u001b[36m575.6155\u001b[0m      \u001b[32m487.9516\u001b[0m  0.0526\n",
      "      6      \u001b[36m674.9246\u001b[0m      \u001b[32m594.0759\u001b[0m  0.0593\n",
      "      6      \u001b[36m524.2069\u001b[0m      \u001b[32m448.4607\u001b[0m  0.0607\n",
      "      6      \u001b[36m544.4680\u001b[0m      \u001b[32m482.4114\u001b[0m  0.0616\n",
      "      7      \u001b[36m669.4639\u001b[0m      \u001b[32m592.4363\u001b[0m  0.0512\n",
      "      7      \u001b[36m667.2503\u001b[0m      \u001b[32m598.3506\u001b[0m  0.0657\n",
      "      7      \u001b[36m568.8737\u001b[0m      488.6705  0.0639\n",
      "      7      \u001b[36m520.1639\u001b[0m      \u001b[32m444.1009\u001b[0m  0.0540\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m186.0004\u001b[0m      \u001b[32m135.9861\u001b[0m  0.4824\n",
      "      7      \u001b[36m536.7046\u001b[0m      \u001b[32m481.0744\u001b[0m  0.0615\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m203.7230\u001b[0m      \u001b[32m166.0074\u001b[0m  0.5294\n",
      "      8      \u001b[36m666.9136\u001b[0m      \u001b[32m591.5095\u001b[0m  0.0550\n",
      "      8      \u001b[36m663.5966\u001b[0m      \u001b[32m596.9044\u001b[0m  0.0597\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m183.9647\u001b[0m      \u001b[32m139.6053\u001b[0m  0.5454\n",
      "      8      \u001b[36m515.0976\u001b[0m      \u001b[32m443.3365\u001b[0m  0.0614\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m202.2897\u001b[0m      \u001b[32m166.7044\u001b[0m  0.5585\n",
      "      8      \u001b[36m566.1965\u001b[0m      \u001b[32m486.6705\u001b[0m  0.0693\n",
      "      8      \u001b[36m529.8181\u001b[0m      481.7860  0.0623\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m181.5100\u001b[0m      \u001b[32m124.0501\u001b[0m  0.5779\n",
      "      9      \u001b[36m662.4525\u001b[0m      \u001b[32m596.1679\u001b[0m  0.0607\n",
      "      9      \u001b[36m665.8693\u001b[0m      592.7768  0.0792\n",
      "      9      567.8672      487.6745  0.0559\n",
      "      9      515.1757      \u001b[32m441.6350\u001b[0m  0.0735\n",
      "      9      533.1184      \u001b[32m479.4651\u001b[0m  0.0643\n",
      "     10      663.2062      596.3647  0.0630\n",
      "Restoring best model from epoch 9.\n",
      "     10      \u001b[36m565.7886\u001b[0m      \u001b[32m485.3496\u001b[0m  0.0573\n",
      "     10      667.5723      592.9535  0.0634\n",
      "Restoring best model from epoch 8.\n",
      "     10      \u001b[36m514.0754\u001b[0m      \u001b[32m441.5906\u001b[0m  0.0518\n",
      "     10      \u001b[36m526.0087\u001b[0m      \u001b[32m478.7602\u001b[0m  0.0520\n",
      "      2      \u001b[36m141.5613\u001b[0m      \u001b[32m131.9241\u001b[0m  0.4781\n",
      "      2      \u001b[36m177.7834\u001b[0m      \u001b[32m162.0564\u001b[0m  0.4706\n",
      "      2      \u001b[36m177.0395\u001b[0m      \u001b[32m165.9817\u001b[0m  0.4496\n",
      "      2      \u001b[36m152.3445\u001b[0m      \u001b[32m134.4207\u001b[0m  0.4597\n",
      "      2      \u001b[36m137.6475\u001b[0m      \u001b[32m122.2772\u001b[0m  0.4681\n",
      "      3      \u001b[36m134.6177\u001b[0m      \u001b[32m130.4256\u001b[0m  0.4645\n",
      "      3      \u001b[36m171.2009\u001b[0m      162.7610  0.4665\n",
      "      3      \u001b[36m144.6568\u001b[0m      \u001b[32m133.7541\u001b[0m  0.4655\n",
      "      3      \u001b[36m170.2792\u001b[0m      \u001b[32m165.4670\u001b[0m  0.4700\n",
      "      3      \u001b[36m130.6725\u001b[0m      \u001b[32m120.1710\u001b[0m  0.4691\n",
      "      4      \u001b[36m133.1449\u001b[0m      \u001b[32m129.9241\u001b[0m  0.4488\n",
      "      4      \u001b[36m167.8663\u001b[0m      162.9237  0.4476\n",
      "      4      \u001b[36m142.0854\u001b[0m      133.9187  0.4479\n",
      "      4      \u001b[36m166.9066\u001b[0m      \u001b[32m165.2322\u001b[0m  0.4467\n",
      "      4      \u001b[36m126.6176\u001b[0m      \u001b[32m119.7553\u001b[0m  0.4475\n",
      "      5      \u001b[36m131.5330\u001b[0m      \u001b[32m129.4008\u001b[0m  0.4512\n",
      "      5      \u001b[36m165.8369\u001b[0m      163.0477  0.4508\n",
      "      5      \u001b[36m165.7566\u001b[0m      \u001b[32m164.7595\u001b[0m  0.4503\n",
      "      5      \u001b[36m140.2180\u001b[0m      134.6973  0.4519\n",
      "      5      \u001b[36m125.8854\u001b[0m      \u001b[32m119.4131\u001b[0m  0.4510\n",
      "      6      \u001b[36m130.1518\u001b[0m      129.4182  0.4415\n",
      "      6      \u001b[36m165.0446\u001b[0m      163.0909  0.4409\n",
      "      6      \u001b[36m139.3082\u001b[0m      134.2826  0.4408\n",
      "      6      \u001b[36m164.9265\u001b[0m      \u001b[32m164.2391\u001b[0m  0.4428\n",
      "      6      \u001b[36m125.3641\u001b[0m      119.6529  0.4411\n",
      "      7      \u001b[36m130.0859\u001b[0m      \u001b[32m129.0525\u001b[0m  0.4535\n",
      "      7      \u001b[36m164.5255\u001b[0m      162.7363  0.4547\n",
      "      7      \u001b[36m138.6655\u001b[0m      \u001b[32m133.7149\u001b[0m  0.4543\n",
      "      7      \u001b[36m164.0472\u001b[0m      \u001b[32m163.3272\u001b[0m  0.4529\n",
      "      7      \u001b[36m124.6085\u001b[0m      \u001b[32m118.8464\u001b[0m  0.4521\n",
      "      8      \u001b[36m130.0859\u001b[0m      129.3596  0.4563\n",
      "      8      \u001b[36m164.1830\u001b[0m      162.9775  0.4566\n",
      "      8      \u001b[36m163.8488\u001b[0m      163.7986  0.4592\n",
      "      8      \u001b[36m138.5511\u001b[0m      134.7657  0.4609\n",
      "      8      124.9713      119.1651  0.4666\n",
      "      9      \u001b[36m129.2964\u001b[0m      129.3549  0.4595\n",
      "      9      \u001b[36m161.5492\u001b[0m      \u001b[32m160.3033\u001b[0m  0.4569\n",
      "      9      \u001b[36m163.6062\u001b[0m      163.8606  0.4574\n",
      "      9      139.3944      134.6533  0.4622\n",
      "      9      \u001b[36m124.5293\u001b[0m      120.0973  0.4519\n",
      "     10      129.4337      129.6341  0.4496\n",
      "Restoring best model from epoch 7.\n",
      "     10      \u001b[36m159.0716\u001b[0m      \u001b[32m160.2928\u001b[0m  0.4554\n",
      "Restoring best model from epoch 9.\n",
      "     10      164.0998      163.8253  0.4538\n",
      "Restoring best model from epoch 7.\n",
      "     10      139.2224      135.8605  0.4559\n",
      "Restoring best model from epoch 7.\n",
      "     10      125.1508      119.5107  0.4572\n",
      "Restoring best model from epoch 7.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m192.0011\u001b[0m      \u001b[32m161.2729\u001b[0m  0.3923\n",
      "      2      \u001b[36m160.2324\u001b[0m      \u001b[32m157.9817\u001b[0m  0.3572\n",
      "      3      \u001b[36m154.6395\u001b[0m      \u001b[32m156.5655\u001b[0m  0.3564\n",
      "      4      \u001b[36m153.0046\u001b[0m      \u001b[32m156.3191\u001b[0m  0.3493\n",
      "      5      \u001b[36m151.6108\u001b[0m      156.3433  0.3555\n",
      "      6      \u001b[36m150.9907\u001b[0m      \u001b[32m155.8923\u001b[0m  0.3649\n",
      "      7      151.3152      \u001b[32m155.8014\u001b[0m  0.4446\n",
      "      8      \u001b[36m150.9305\u001b[0m      \u001b[32m155.3057\u001b[0m  0.3703\n",
      "      9      151.3771      \u001b[32m154.9205\u001b[0m  0.3496\n",
      "     10      151.0905      155.8287  0.3563\n",
      "Restoring best model from epoch 9.\n",
      "(7099, 16) <class 'pandas.core.frame.DataFrame'>\n",
      "(7099,) <class 'pandas.core.series.Series'>\n",
      "(1774, 16) <class 'pandas.core.frame.DataFrame'>\n",
      "(1774,) <class 'pandas.core.series.Series'>\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m869.9770\u001b[0m      \u001b[32m750.8711\u001b[0m  0.0535\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m867.3722\u001b[0m      \u001b[32m749.2040\u001b[0m  0.0679\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m875.9909\u001b[0m      \u001b[32m750.1642\u001b[0m  0.0695\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m977.4344\u001b[0m      \u001b[32m791.1741\u001b[0m  0.0613\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m979.0748\u001b[0m      \u001b[32m803.0699\u001b[0m  0.0692\n",
      "      2      \u001b[36m757.6185\u001b[0m      \u001b[32m644.1684\u001b[0m  0.0632\n",
      "      2      \u001b[36m759.9092\u001b[0m      \u001b[32m658.4590\u001b[0m  0.0607\n",
      "      2      \u001b[36m718.5810\u001b[0m      \u001b[32m589.8168\u001b[0m  0.0626\n",
      "      2      \u001b[36m746.9152\u001b[0m      \u001b[32m556.7835\u001b[0m  0.0587\n",
      "      3      \u001b[36m706.2989\u001b[0m      \u001b[32m612.0373\u001b[0m  0.0589\n",
      "      2      \u001b[36m759.2642\u001b[0m      \u001b[32m554.7823\u001b[0m  0.0751\n",
      "      3      \u001b[36m710.8138\u001b[0m      \u001b[32m621.2833\u001b[0m  0.0639\n",
      "      3      \u001b[36m589.4219\u001b[0m      \u001b[32m474.1639\u001b[0m  0.0565\n",
      "      3      \u001b[36m631.5190\u001b[0m      \u001b[32m522.3872\u001b[0m  0.0691\n",
      "      3      \u001b[36m606.4949\u001b[0m      \u001b[32m485.5071\u001b[0m  0.0607\n",
      "      4      \u001b[36m682.1709\u001b[0m      \u001b[32m609.9666\u001b[0m  0.0712\n",
      "      4      \u001b[36m684.3822\u001b[0m      \u001b[32m614.9854\u001b[0m  0.0640\n",
      "      4      \u001b[36m557.0437\u001b[0m      475.6363  0.0585\n",
      "      4      \u001b[36m591.5437\u001b[0m      \u001b[32m514.5345\u001b[0m  0.0749\n",
      "      4      \u001b[36m577.3609\u001b[0m      \u001b[32m483.9475\u001b[0m  0.0632\n",
      "      5      \u001b[36m669.3414\u001b[0m      \u001b[32m606.8122\u001b[0m  0.0644\n",
      "      5      \u001b[36m673.7246\u001b[0m      \u001b[32m611.1613\u001b[0m  0.0653\n",
      "      5      \u001b[36m531.3469\u001b[0m      474.8884  0.0731\n",
      "      5      \u001b[36m576.4802\u001b[0m      \u001b[32m511.2082\u001b[0m  0.0572\n",
      "      6      \u001b[36m667.1869\u001b[0m      \u001b[32m605.5283\u001b[0m  0.0521\n",
      "      5      \u001b[36m553.6603\u001b[0m      487.7640  0.0664\n",
      "      6      \u001b[36m666.3979\u001b[0m      \u001b[32m610.8084\u001b[0m  0.0538\n",
      "      6      \u001b[36m521.0547\u001b[0m      \u001b[32m472.1836\u001b[0m  0.0539\n",
      "      6      \u001b[36m568.6074\u001b[0m      \u001b[32m507.3052\u001b[0m  0.0625\n",
      "      7      \u001b[36m661.3029\u001b[0m      \u001b[32m603.7050\u001b[0m  0.0713\n",
      "      7      \u001b[36m659.7036\u001b[0m      \u001b[32m608.0647\u001b[0m  0.0596\n",
      "      7      \u001b[36m511.2776\u001b[0m      \u001b[32m469.6974\u001b[0m  0.0531\n",
      "      6      \u001b[36m542.1769\u001b[0m      \u001b[32m480.8285\u001b[0m  0.0688\n",
      "      7      \u001b[36m559.9828\u001b[0m      \u001b[32m506.8682\u001b[0m  0.0602\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m202.8660\u001b[0m      \u001b[32m170.1864\u001b[0m  0.4905\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m202.4861\u001b[0m      \u001b[32m168.4945\u001b[0m  0.5073\n",
      "      8      \u001b[36m507.5787\u001b[0m      \u001b[32m469.5613\u001b[0m  0.0552\n",
      "      8      \u001b[36m655.4660\u001b[0m      \u001b[32m600.4186\u001b[0m  0.0648\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m182.6337\u001b[0m      \u001b[32m138.8480\u001b[0m  0.5097\n",
      "      8      \u001b[36m659.5629\u001b[0m      \u001b[32m605.5352\u001b[0m  0.0631\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m178.1214\u001b[0m      \u001b[32m132.5328\u001b[0m  0.5190\n",
      "      7      \u001b[36m534.4892\u001b[0m      \u001b[32m480.0719\u001b[0m  0.0613\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m184.2663\u001b[0m      \u001b[32m142.8992\u001b[0m  0.5387\n",
      "      8      \u001b[36m553.2175\u001b[0m      \u001b[32m504.8456\u001b[0m  0.0551\n",
      "      9      \u001b[36m506.7944\u001b[0m      \u001b[32m465.9403\u001b[0m  0.0609\n",
      "      9      656.0995      \u001b[32m599.6258\u001b[0m  0.0623\n",
      "      8      \u001b[36m525.3770\u001b[0m      480.7999  0.0598\n",
      "      9      555.4415      \u001b[32m503.7627\u001b[0m  0.0534\n",
      "      9      \u001b[36m656.5818\u001b[0m      \u001b[32m604.1776\u001b[0m  0.0832\n",
      "     10      658.5213      \u001b[32m599.5334\u001b[0m  0.0516\n",
      "     10      \u001b[36m505.2500\u001b[0m      \u001b[32m464.4696\u001b[0m  0.0593\n",
      "      9      526.4802      \u001b[32m478.0098\u001b[0m  0.0590\n",
      "     10      555.0258      \u001b[32m503.1801\u001b[0m  0.0600\n",
      "     10      658.7457      604.3798  0.0610\n",
      "Restoring best model from epoch 9.\n",
      "     10      525.5030      478.2510  0.0538\n",
      "Restoring best model from epoch 9.\n",
      "      2      \u001b[36m176.3767\u001b[0m      \u001b[32m164.3188\u001b[0m  0.4638\n",
      "      2      \u001b[36m176.8539\u001b[0m      \u001b[32m165.0127\u001b[0m  0.4820\n",
      "      2      \u001b[36m134.4590\u001b[0m      \u001b[32m125.8065\u001b[0m  0.4648\n",
      "      2      \u001b[36m140.5474\u001b[0m      \u001b[32m132.0049\u001b[0m  0.4683\n",
      "      2      \u001b[36m150.8310\u001b[0m      \u001b[32m136.0868\u001b[0m  0.4606\n",
      "      3      \u001b[36m170.5829\u001b[0m      165.6699  0.4554\n",
      "      3      \u001b[36m170.7816\u001b[0m      166.7522  0.4573\n",
      "      3      \u001b[36m134.3983\u001b[0m      \u001b[32m131.5118\u001b[0m  0.4542\n",
      "      3      \u001b[36m127.9387\u001b[0m      \u001b[32m124.8848\u001b[0m  0.4579\n",
      "      3      \u001b[36m142.9572\u001b[0m      \u001b[32m136.0828\u001b[0m  0.4544\n",
      "      4      \u001b[36m167.0387\u001b[0m      165.1441  0.4624\n",
      "      4      \u001b[36m166.4467\u001b[0m      166.1892  0.4594\n",
      "      4      \u001b[36m132.7082\u001b[0m      \u001b[32m128.7440\u001b[0m  0.4581\n",
      "      4      \u001b[36m125.3421\u001b[0m      \u001b[32m124.2316\u001b[0m  0.4582\n",
      "      4      \u001b[36m139.8574\u001b[0m      137.0755  0.4610\n",
      "      5      \u001b[36m164.5035\u001b[0m      164.3617  0.4438\n",
      "      5      \u001b[36m164.4699\u001b[0m      \u001b[32m164.3884\u001b[0m  0.4432\n",
      "      5      \u001b[36m130.5922\u001b[0m      129.1117  0.4416\n",
      "      5      \u001b[36m124.5891\u001b[0m      \u001b[32m122.8913\u001b[0m  0.4408\n",
      "      5      \u001b[36m138.4651\u001b[0m      136.2186  0.4393\n",
      "      6      \u001b[36m164.1116\u001b[0m      \u001b[32m164.2938\u001b[0m  0.4424\n",
      "      6      \u001b[36m163.6752\u001b[0m      \u001b[32m163.9574\u001b[0m  0.4422\n",
      "      6      \u001b[36m129.3876\u001b[0m      129.1663  0.4413\n",
      "      6      \u001b[36m123.6873\u001b[0m      123.6601  0.4447\n",
      "      6      \u001b[36m137.8925\u001b[0m      136.3664  0.4386\n",
      "      7      \u001b[36m163.3710\u001b[0m      165.2998  0.4399\n",
      "      7      \u001b[36m163.5469\u001b[0m      166.1337  0.4389\n",
      "      7      \u001b[36m128.8739\u001b[0m      128.7617  0.4377\n",
      "      7      \u001b[36m123.3795\u001b[0m      123.7503  0.4391\n",
      "      7      \u001b[36m137.6736\u001b[0m      137.9119  0.4404\n",
      "      8      \u001b[36m163.0022\u001b[0m      164.4630  0.4627\n",
      "      8      \u001b[36m162.8316\u001b[0m      \u001b[32m163.3357\u001b[0m  0.4595\n",
      "      8      129.3072      128.9939  0.4635\n",
      "      8      \u001b[36m136.9580\u001b[0m      136.3210  0.4592\n",
      "      8      \u001b[36m123.2739\u001b[0m      124.4388  0.4617\n",
      "      9      \u001b[36m162.7649\u001b[0m      164.8337  0.4491\n",
      "      9      162.9228      165.2559  0.4493\n",
      "      9      129.9164      128.9053  0.4498\n",
      "      9      \u001b[36m123.0889\u001b[0m      123.4998  0.4482\n",
      "      9      \u001b[36m134.2554\u001b[0m      \u001b[32m132.5960\u001b[0m  0.4490\n",
      "     10      \u001b[36m162.7895\u001b[0m      165.1256  0.4399\n",
      "Restoring best model from epoch 8.\n",
      "     10      162.9797      165.8333  0.4433\n",
      "Restoring best model from epoch 6.\n",
      "     10      \u001b[36m128.7322\u001b[0m      129.5617  0.4428\n",
      "Restoring best model from epoch 4.\n",
      "     10      123.3540      125.2135  0.4424\n",
      "Restoring best model from epoch 5.\n",
      "     10      \u001b[36m132.7162\u001b[0m      132.7659  0.4434\n",
      "Restoring best model from epoch 9.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m897.7662\u001b[0m      \u001b[32m729.8470\u001b[0m  0.0757\n",
      "      2      \u001b[36m709.2527\u001b[0m      \u001b[32m592.1484\u001b[0m  0.0740\n",
      "      3      \u001b[36m650.1176\u001b[0m      \u001b[32m583.9248\u001b[0m  0.0767\n",
      "      4      \u001b[36m624.8245\u001b[0m      \u001b[32m578.8263\u001b[0m  0.0866\n",
      "      5      \u001b[36m617.1751\u001b[0m      \u001b[32m577.1973\u001b[0m  0.0753\n",
      "      6      \u001b[36m614.1575\u001b[0m      \u001b[32m575.5284\u001b[0m  0.0754\n",
      "      7      \u001b[36m606.7727\u001b[0m      \u001b[32m573.4905\u001b[0m  0.0722\n",
      "      8      \u001b[36m605.6091\u001b[0m      \u001b[32m572.1559\u001b[0m  0.0731\n",
      "      9      606.7329      572.4265  0.0736\n",
      "     10      605.7823      573.1910  0.0750\n",
      "Restoring best model from epoch 8.\n"
     ]
    }
   ],
   "source": [
    "data_set_fns = [load_metabric,  load_flchain, load_rgbsg, load_support] #, load_flchain, load_rgbsg, load_support, load_tcga]\n",
    "data_set_fns_str = ['load_metabric', 'load_flchain', 'load_rgbsg', 'load_support'] \n",
    "one_hot_dict = {'load_flchain': ['mgus'], 'load_support':['cancer'], 'load_rgbsg':['grade']}\n",
    "\n",
    "n_cuts = 10\n",
    "for idx, dataset in enumerate(data_set_fns):\n",
    "    # get name of current dataset\n",
    "    data = dataset(path=\"/Users/JUSC/Documents/xgbsurv/xgbsurv/datasets/data/\", as_frame=True)\n",
    "    X  = data.data #.astype(np.float32)\n",
    "    time, event = transform_back(data.target.values)\n",
    "    min_time = time.min()\n",
    "    X['time'] = time\n",
    "    X['event'] = event\n",
    "    y = data.target #.values #.to_numpy()\n",
    "    df = pd.concat([X,y])\n",
    "    df = discretizer_df(df=X, n_cuts=n_cuts, type = 'equidistant', min_time=min_time)\n",
    "    target = transform(df.time.values, df.event.values)\n",
    "    y = pd.Series(target)\n",
    "    X = df.drop(['time', 'event'], axis=1)\n",
    "    print(data_set_fns_str[idx])\n",
    "    if data_set_fns_str[idx] in one_hot_dict.keys():\n",
    "        X = pd.get_dummies(X, columns=one_hot_dict[data_set_fns_str[idx]])\n",
    "    X, y = sort_X_y_pandas(X, y)\n",
    "    print(X.shape,y.shape)\n",
    "    \n",
    "    net = NeuralNet(\n",
    "        SurvivalModel, \n",
    "        module__n_layers = 1,\n",
    "        module__input_units = X.shape[1],\n",
    "        #module__idx_durations = \n",
    "        #module__num_nodes = 32,\n",
    "        #module__dropout = 0.1, # these could also be removed\n",
    "        module__out_features = n_cuts,\n",
    "        # for split sizes when result size = 1\n",
    "        iterator_train__drop_last=True,\n",
    "        #iterator_valid__drop_last=True,\n",
    "        criterion=DeephitLoss,\n",
    "        optimizer=torch.optim.AdamW,\n",
    "        optimizer__weight_decay = 0.4,\n",
    "        batch_size=32, # separate train and valid->iterator_train__batch_size=128 and iterator_valid__batch_size=128 ?\n",
    "        callbacks=[\n",
    "            (\n",
    "                \"sched\",\n",
    "                LRScheduler(\n",
    "                    torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
    "                    monitor=\"valid_loss\",\n",
    "                    patience=5,\n",
    "                ),\n",
    "            ),\n",
    "            (\n",
    "                \"es\",\n",
    "                EarlyStopping(\n",
    "                    monitor=\"valid_loss\",\n",
    "                    patience=10,\n",
    "                    load_best=True,\n",
    "                ),\n",
    "            ),\n",
    "            (\"seed\", FixSeed(seed=42)),\n",
    "        ],\n",
    "        \n",
    "        #[EarlyStopping(patience=10)],\n",
    "        # add extensive callback, and random number seed\n",
    "        #TODO: enable stratification, verify\n",
    "        train_split=ValidSplit(0.2), # might cause lower performance in metrics, explain in thesis\n",
    "        #lr=0.001,\n",
    "        #max_epochs=1, #0,#100\n",
    "        #train_split=None,\n",
    "        verbose=1\n",
    "    )\n",
    "    best_model,params, outer_scores, best_preds_train, best_preds_test, X_train, X_test, y_train, y_test = train_eval(X, y, net, n_iter, data.filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1, 0, 0, ..., 8, 9, 9]),\n",
       " array([1., 0., 0., ..., 0., 1., 1.], dtype=float32))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_metabric(path=\"/Users/JUSC/Documents/xgbsurv/xgbsurv/datasets/data/\", as_frame=True)\n",
    "num_durations = 10\n",
    "labtrans = DeepHitSingle.label_transform(num_durations)\n",
    "X = data.data\n",
    "time, event = transform_back(data.target.values)\n",
    "labtrans = DeepHitSingle.label_transform(num_durations)\n",
    "#get_target = lambda df: (df['duration'].values, df['event'].values)\n",
    "y_train = labtrans.fit_transform(time, event)\n",
    "y_train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TCGA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_breslow_tcga = {\n",
    "    'estimator__module__n_layers': [1, 2, 4],\n",
    "    'estimator__module__n_layers': [1, 2, 4],\n",
    "    'estimator__module__num_nodes': [64, 128, 256, 512],\n",
    "    'estimator__module__dropout': scuniform(0.0,0.7),\n",
    "    'estimator__optimizer__weight_decay': [0.4, 0.2, 0.1, 0.05, 0.02, 0.01, 0],\n",
    "    'estimator__batch_size': [64, 128, 256, 512, 1024],\n",
    "    #lr not in paper because of learning rate finder\n",
    "    # note: setting learning rate higher would make exp(partial_hazard) explode\n",
    "    #'estimator__lr': scloguniform(0.001,0.01), # scheduler unten einbauen\n",
    "    # use callback instead\n",
    "    'estimator__lr':[0.01],\n",
    "    'estimator__max_epochs':  scrandint(10,20), # corresponds to num_rounds\n",
    "    'pca__n_components': [8, 10, 12, 14, 16]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_eval(X, y, net, n_iter, filename):\n",
    "\n",
    "        dataset_name = filename.split('_')[0]\n",
    "        # add IBS later\n",
    "        outer_scores = {'cindex_train_'+dataset_name:[], 'cindex_test_'+dataset_name:[],\n",
    "                        'ibs_train_'+dataset_name:[], 'ibs_test_'+dataset_name:[]}\n",
    "        best_params = {'best_params_'+dataset_name:[]}\n",
    "        best_model = {'best_model_'+dataset_name:[]}\n",
    "        ct = make_column_transformer(\n",
    "                #(OneHotEncoder(sparse_output=False), make_column_selector(dtype_include=['category', 'object']))\n",
    "                (StandardScaler(), make_column_selector(dtype_include=['float32']))\n",
    "                ,remainder='drop')\n",
    "        pipe = Pipeline([('scaler',ct),\n",
    "                         ('pca', PCA()),#n_components=10\n",
    "                        ('estimator', net)])\n",
    "        rs = RandomizedSearchCV(pipe, param_grid_breslow_tcga, scoring = scoring_function, n_jobs=-1, \n",
    "                                    n_iter=2, refit=True)\n",
    "        for i, (train_index, test_index) in enumerate(outer_custom_cv.split(X, y)):\n",
    "                # Split data into training and testing sets for outer fold\n",
    "                X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "                y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "                X_train, y_train = sort_X_y_pandas(X_train, y_train)\n",
    "                X_test, y_test = sort_X_y_pandas(X_test, y_test)\n",
    "\n",
    "                print(X_train.shape, type(X_train))\n",
    "                print(y_train.shape, type(y_train))\n",
    "                print(X_test.shape, type(X_test))\n",
    "                print(y_test.shape, type(y_test))\n",
    "                # save splits and data\n",
    "                savetxt('splits/train_index_'+str(i)+'_'+filename, train_index, delimiter=',')\n",
    "                savetxt('splits/test_index_'+str(i)+'_'+filename, test_index, delimiter=',')\n",
    "                \n",
    "                savetxt('splits/X_train_'+str(i)+'_'+filename, X_train, delimiter=',')\n",
    "                savetxt('splits/X_test_'+str(i)+'_'+filename, X_test, delimiter=',')\n",
    "\n",
    "                savetxt('splits/y_train_'+str(i)+'_'+filename, y_train, delimiter=',')\n",
    "                savetxt('splits/y_test_'+str(i)+'_'+filename, y_test, delimiter=',')\n",
    "\n",
    "                strat = np.sign(y_train)\n",
    "                valid_split = ValidSplit(cv=0.1, stratified=strat, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                rs.fit(X_train, y_train)\n",
    "                best_preds_train = rs.best_estimator_.predict(X_train)\n",
    "                best_preds_test = rs.best_estimator_.predict(X_test)\n",
    "                # save hyperparameter settings\n",
    "                params = rs.best_estimator_.get_params\n",
    "                best_params['best_params_'+dataset_name] += [rs.best_params_]\n",
    "                best_model['best_model_'+dataset_name] += [params]\n",
    "                try:\n",
    "                    cum_hazard_train = get_cumulative_hazard_function_breslow(\n",
    "                            X_train.values, X_train.values, y_train.values, y_train.values,\n",
    "                            best_preds_train.reshape(-1), best_preds_train.reshape(-1)\n",
    "                            )\n",
    "\n",
    "                    df_survival_train = np.exp(-cum_hazard_train)\n",
    "                    durations_train, events_train = transform_back(y_train.values)\n",
    "                    time_grid_train = np.linspace(durations_train.min(), durations_train.max(), 100)\n",
    "                    ev = EvalSurv(df_survival_train, durations_train, events_train, censor_surv='km')\n",
    "                    print('Concordance Index',ev.concordance_td('antolini'))\n",
    "                    print('Integrated Brier Score:',ev.integrated_brier_score(time_grid_train))\n",
    "                    cindex_score_train = ev.concordance_td('antolini')\n",
    "                    ibs_score_train = ev.integrated_brier_score(time_grid_train)\n",
    "\n",
    "                    outer_scores['cindex_train_'+dataset_name] += [cindex_score_train]\n",
    "                    outer_scores['ibs_train_'+dataset_name] += [ibs_score_train]\n",
    "\n",
    "                except:\n",
    "                    outer_scores['cindex_train_'+dataset_name] += [np.nan]\n",
    "                    outer_scores['ibs_train_'+dataset_name] += [np.nan]\n",
    "                    \n",
    "                try:\n",
    "                    cum_hazard_test = get_cumulative_hazard_function_breslow(\n",
    "                            X_train.values, X_test.values, y_train.values, y_test.values,\n",
    "                            best_preds_train.reshape(-1), best_preds_test.reshape(-1)\n",
    "                            )\n",
    "                    df_survival_test = np.exp(-cum_hazard_test)\n",
    "                    durations_test, events_test = transform_back(y_test.values)\n",
    "                    print('durations',durations_test.min(), durations_test.max())\n",
    "                    time_grid_test = np.linspace(durations_test.min(), durations_test.max(), 100)\n",
    "                    ev = EvalSurv(df_survival_test, durations_test, events_test, censor_surv='km')\n",
    "                    print('Concordance Index',ev.concordance_td('antolini'))\n",
    "                    print('Integrated Brier Score:',ev.integrated_brier_score(time_grid_test))\n",
    "                    cindex_score_test = ev.concordance_td('antolini')\n",
    "                    ibs_score_test = ev.integrated_brier_score(time_grid_test)\n",
    "\n",
    "                    outer_scores['cindex_test_'+dataset_name] += [cindex_score_test]\n",
    "                    outer_scores['ibs_test_'+dataset_name] += [ibs_score_test]\n",
    "                except: \n",
    "                    outer_scores['cindex_test_'+dataset_name] += [np.nan]\n",
    "                    outer_scores['ibs_test_'+dataset_name] += [np.nan]\n",
    "            \n",
    "        df_best_params = pd.DataFrame(best_params)\n",
    "        df_best_model = pd.DataFrame(best_model)\n",
    "        df_outer_scores = pd.DataFrame(outer_scores)\n",
    "        df_metrics = pd.concat([df_best_params,df_best_model,df_outer_scores], axis=1)\n",
    "        df_metrics.to_csv('metrics/metric_summary_'+str(i)+'_'+filename, index=False)\n",
    "        return best_model, best_params, outer_scores, best_preds_train, best_preds_test, X_train, X_test, y_train, y_test\n",
    "\n",
    "                \n",
    "#cv=inner_custom_cv,pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split gex_?|100130426      float32\n",
      "gex_?|100133144      float32\n",
      "gex_?|100134869      float32\n",
      "gex_?|10357          float32\n",
      "gex_?|10431          float32\n",
      "                      ...   \n",
      "gex_ZYG11A|440590    float32\n",
      "gex_ZYG11B|79699     float32\n",
      "gex_ZYX|7791         float32\n",
      "gex_ZZEF1|23140      float32\n",
      "gex_ZZZ3|26009       float32\n",
      "Length: 20531, dtype: object\n",
      "(324, 20531) <class 'pandas.core.frame.DataFrame'>\n",
      "(324,) <class 'pandas.core.series.Series'>\n",
      "(82, 20531) <class 'pandas.core.frame.DataFrame'>\n",
      "(82,) <class 'pandas.core.series.Series'>\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "\nAll the 10 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n1 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/sklearn/pipeline.py\", line 405, in fit\n    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1230, in fit\n    self.partial_fit(X, y, **fit_params)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1189, in partial_fit\n    self.fit_loop(X, y, **fit_params)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1101, in fit_loop\n    self.run_single_epoch(iterator_train, training=True, prefix=\"train\",\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1137, in run_single_epoch\n    step = step_fn(batch, **fit_params)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1016, in train_step\n    self._step_optimizer(step_fn)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 972, in _step_optimizer\n    optimizer.step(step_fn)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/torch/optim/optimizer.py\", line 113, in wrapper\n    return func(*args, **kwargs)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/torch/autograd/grad_mode.py\", line 27, in decorate_context\n    return func(*args, **kwargs)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/torch/optim/adamw.py\", line 119, in step\n    loss = closure()\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1006, in step_fn\n    step = self.train_step_single(batch, **fit_params)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 906, in train_step_single\n    loss = self.get_loss(y_pred, yi, X=Xi, training=True)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1571, in get_loss\n    return self.criterion_(y_pred, y_true)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/Users/JUSC/Documents/xgbsurv/experiments/deep_learning/loss_functions_pytorch.py\", line 288, in forward\n    loss = deephit_likelihood_1_torch(input, prediction)\n  File \"/Users/JUSC/Documents/xgbsurv/experiments/deep_learning/loss_functions_pytorch.py\", line 259, in deephit_likelihood_1_torch\n    raise ValueError(f\"Network output `phi` is too small for `idx_durations`.\"+\nValueError: Network output `phi` is too small for `idx_durations`. Need at least `phi.shape[1] = 119`, but got `phi.shape[1] = 2`\n\n--------------------------------------------------------------------------------\n2 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/sklearn/pipeline.py\", line 405, in fit\n    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1230, in fit\n    self.partial_fit(X, y, **fit_params)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1189, in partial_fit\n    self.fit_loop(X, y, **fit_params)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1101, in fit_loop\n    self.run_single_epoch(iterator_train, training=True, prefix=\"train\",\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1137, in run_single_epoch\n    step = step_fn(batch, **fit_params)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1016, in train_step\n    self._step_optimizer(step_fn)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 972, in _step_optimizer\n    optimizer.step(step_fn)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/torch/optim/optimizer.py\", line 113, in wrapper\n    return func(*args, **kwargs)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/torch/autograd/grad_mode.py\", line 27, in decorate_context\n    return func(*args, **kwargs)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/torch/optim/adamw.py\", line 119, in step\n    loss = closure()\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1006, in step_fn\n    step = self.train_step_single(batch, **fit_params)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 906, in train_step_single\n    loss = self.get_loss(y_pred, yi, X=Xi, training=True)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1571, in get_loss\n    return self.criterion_(y_pred, y_true)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/Users/JUSC/Documents/xgbsurv/experiments/deep_learning/loss_functions_pytorch.py\", line 288, in forward\n    loss = deephit_likelihood_1_torch(input, prediction)\n  File \"/Users/JUSC/Documents/xgbsurv/experiments/deep_learning/loss_functions_pytorch.py\", line 259, in deephit_likelihood_1_torch\n    raise ValueError(f\"Network output `phi` is too small for `idx_durations`.\"+\nValueError: Network output `phi` is too small for `idx_durations`. Need at least `phi.shape[1] = 118`, but got `phi.shape[1] = 2`\n\n--------------------------------------------------------------------------------\n1 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/sklearn/pipeline.py\", line 405, in fit\n    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1230, in fit\n    self.partial_fit(X, y, **fit_params)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1189, in partial_fit\n    self.fit_loop(X, y, **fit_params)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1101, in fit_loop\n    self.run_single_epoch(iterator_train, training=True, prefix=\"train\",\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1137, in run_single_epoch\n    step = step_fn(batch, **fit_params)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1016, in train_step\n    self._step_optimizer(step_fn)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 972, in _step_optimizer\n    optimizer.step(step_fn)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/torch/optim/optimizer.py\", line 113, in wrapper\n    return func(*args, **kwargs)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/torch/autograd/grad_mode.py\", line 27, in decorate_context\n    return func(*args, **kwargs)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/torch/optim/adamw.py\", line 119, in step\n    loss = closure()\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1006, in step_fn\n    step = self.train_step_single(batch, **fit_params)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 906, in train_step_single\n    loss = self.get_loss(y_pred, yi, X=Xi, training=True)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1571, in get_loss\n    return self.criterion_(y_pred, y_true)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/Users/JUSC/Documents/xgbsurv/experiments/deep_learning/loss_functions_pytorch.py\", line 288, in forward\n    loss = deephit_likelihood_1_torch(input, prediction)\n  File \"/Users/JUSC/Documents/xgbsurv/experiments/deep_learning/loss_functions_pytorch.py\", line 259, in deephit_likelihood_1_torch\n    raise ValueError(f\"Network output `phi` is too small for `idx_durations`.\"+\nValueError: Network output `phi` is too small for `idx_durations`. Need at least `phi.shape[1] = 112`, but got `phi.shape[1] = 2`\n\n--------------------------------------------------------------------------------\n1 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/sklearn/pipeline.py\", line 405, in fit\n    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1230, in fit\n    self.partial_fit(X, y, **fit_params)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1189, in partial_fit\n    self.fit_loop(X, y, **fit_params)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1101, in fit_loop\n    self.run_single_epoch(iterator_train, training=True, prefix=\"train\",\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1137, in run_single_epoch\n    step = step_fn(batch, **fit_params)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1016, in train_step\n    self._step_optimizer(step_fn)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 972, in _step_optimizer\n    optimizer.step(step_fn)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/torch/optim/optimizer.py\", line 113, in wrapper\n    return func(*args, **kwargs)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/torch/autograd/grad_mode.py\", line 27, in decorate_context\n    return func(*args, **kwargs)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/torch/optim/adamw.py\", line 119, in step\n    loss = closure()\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1006, in step_fn\n    step = self.train_step_single(batch, **fit_params)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 906, in train_step_single\n    loss = self.get_loss(y_pred, yi, X=Xi, training=True)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1571, in get_loss\n    return self.criterion_(y_pred, y_true)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/Users/JUSC/Documents/xgbsurv/experiments/deep_learning/loss_functions_pytorch.py\", line 288, in forward\n    loss = deephit_likelihood_1_torch(input, prediction)\n  File \"/Users/JUSC/Documents/xgbsurv/experiments/deep_learning/loss_functions_pytorch.py\", line 259, in deephit_likelihood_1_torch\n    raise ValueError(f\"Network output `phi` is too small for `idx_durations`.\"+\nValueError: Network output `phi` is too small for `idx_durations`. Need at least `phi.shape[1] = 116`, but got `phi.shape[1] = 2`\n\n--------------------------------------------------------------------------------\n1 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/sklearn/pipeline.py\", line 405, in fit\n    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1230, in fit\n    self.partial_fit(X, y, **fit_params)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1189, in partial_fit\n    self.fit_loop(X, y, **fit_params)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1101, in fit_loop\n    self.run_single_epoch(iterator_train, training=True, prefix=\"train\",\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1137, in run_single_epoch\n    step = step_fn(batch, **fit_params)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1016, in train_step\n    self._step_optimizer(step_fn)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 972, in _step_optimizer\n    optimizer.step(step_fn)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/torch/optim/optimizer.py\", line 113, in wrapper\n    return func(*args, **kwargs)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/torch/autograd/grad_mode.py\", line 27, in decorate_context\n    return func(*args, **kwargs)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/torch/optim/adamw.py\", line 119, in step\n    loss = closure()\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1006, in step_fn\n    step = self.train_step_single(batch, **fit_params)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 906, in train_step_single\n    loss = self.get_loss(y_pred, yi, X=Xi, training=True)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1571, in get_loss\n    return self.criterion_(y_pred, y_true)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/Users/JUSC/Documents/xgbsurv/experiments/deep_learning/loss_functions_pytorch.py\", line 288, in forward\n    loss = deephit_likelihood_1_torch(input, prediction)\n  File \"/Users/JUSC/Documents/xgbsurv/experiments/deep_learning/loss_functions_pytorch.py\", line 259, in deephit_likelihood_1_torch\n    raise ValueError(f\"Network output `phi` is too small for `idx_durations`.\"+\nValueError: Network output `phi` is too small for `idx_durations`. Need at least `phi.shape[1] = 62`, but got `phi.shape[1] = 2`\n\n--------------------------------------------------------------------------------\n3 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/sklearn/pipeline.py\", line 405, in fit\n    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1230, in fit\n    self.partial_fit(X, y, **fit_params)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1189, in partial_fit\n    self.fit_loop(X, y, **fit_params)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1101, in fit_loop\n    self.run_single_epoch(iterator_train, training=True, prefix=\"train\",\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1137, in run_single_epoch\n    step = step_fn(batch, **fit_params)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1016, in train_step\n    self._step_optimizer(step_fn)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 972, in _step_optimizer\n    optimizer.step(step_fn)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/torch/optim/optimizer.py\", line 113, in wrapper\n    return func(*args, **kwargs)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/torch/autograd/grad_mode.py\", line 27, in decorate_context\n    return func(*args, **kwargs)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/torch/optim/adamw.py\", line 119, in step\n    loss = closure()\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1006, in step_fn\n    step = self.train_step_single(batch, **fit_params)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 906, in train_step_single\n    loss = self.get_loss(y_pred, yi, X=Xi, training=True)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1571, in get_loss\n    return self.criterion_(y_pred, y_true)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/Users/JUSC/Documents/xgbsurv/experiments/deep_learning/loss_functions_pytorch.py\", line 288, in forward\n    loss = deephit_likelihood_1_torch(input, prediction)\n  File \"/Users/JUSC/Documents/xgbsurv/experiments/deep_learning/loss_functions_pytorch.py\", line 259, in deephit_likelihood_1_torch\n    raise ValueError(f\"Network output `phi` is too small for `idx_durations`.\"+\nValueError: Network output `phi` is too small for `idx_durations`. Need at least `phi.shape[1] = 63`, but got `phi.shape[1] = 2`\n\n--------------------------------------------------------------------------------\n1 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/sklearn/pipeline.py\", line 405, in fit\n    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1230, in fit\n    self.partial_fit(X, y, **fit_params)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1189, in partial_fit\n    self.fit_loop(X, y, **fit_params)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1101, in fit_loop\n    self.run_single_epoch(iterator_train, training=True, prefix=\"train\",\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1137, in run_single_epoch\n    step = step_fn(batch, **fit_params)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1016, in train_step\n    self._step_optimizer(step_fn)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 972, in _step_optimizer\n    optimizer.step(step_fn)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/torch/optim/optimizer.py\", line 113, in wrapper\n    return func(*args, **kwargs)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/torch/autograd/grad_mode.py\", line 27, in decorate_context\n    return func(*args, **kwargs)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/torch/optim/adamw.py\", line 119, in step\n    loss = closure()\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1006, in step_fn\n    step = self.train_step_single(batch, **fit_params)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 906, in train_step_single\n    loss = self.get_loss(y_pred, yi, X=Xi, training=True)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1571, in get_loss\n    return self.criterion_(y_pred, y_true)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/Users/JUSC/Documents/xgbsurv/experiments/deep_learning/loss_functions_pytorch.py\", line 288, in forward\n    loss = deephit_likelihood_1_torch(input, prediction)\n  File \"/Users/JUSC/Documents/xgbsurv/experiments/deep_learning/loss_functions_pytorch.py\", line 259, in deephit_likelihood_1_torch\n    raise ValueError(f\"Network output `phi` is too small for `idx_durations`.\"+\nValueError: Network output `phi` is too small for `idx_durations`. Need at least `phi.shape[1] = 61`, but got `phi.shape[1] = 2`\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 66\u001b[0m\n\u001b[1;32m     23\u001b[0m X, y \u001b[39m=\u001b[39m sort_X_y_pandas(X, y)\n\u001b[1;32m     25\u001b[0m net \u001b[39m=\u001b[39m NeuralNet(\n\u001b[1;32m     26\u001b[0m     SurvivalModel, \n\u001b[1;32m     27\u001b[0m     module__n_layers \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     64\u001b[0m     verbose\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m     65\u001b[0m )\n\u001b[0;32m---> 66\u001b[0m best_model,params, outer_scores, best_preds_train, best_preds_test, X_train, X_test, y_train, y_test \u001b[39m=\u001b[39m train_eval(X, y, net, n_iter, data\u001b[39m.\u001b[39;49mfilename)\n",
      "Cell \u001b[0;32mIn[14], line 45\u001b[0m, in \u001b[0;36mtrain_eval\u001b[0;34m(X, y, net, n_iter, filename)\u001b[0m\n\u001b[1;32m     39\u001b[0m strat \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msign(y_train)\n\u001b[1;32m     40\u001b[0m valid_split \u001b[39m=\u001b[39m ValidSplit(cv\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m, stratified\u001b[39m=\u001b[39mstrat, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\n\u001b[0;32m---> 45\u001b[0m rs\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[1;32m     46\u001b[0m best_preds_train \u001b[39m=\u001b[39m rs\u001b[39m.\u001b[39mbest_estimator_\u001b[39m.\u001b[39mpredict(X_train)\n\u001b[1;32m     47\u001b[0m best_preds_test \u001b[39m=\u001b[39m rs\u001b[39m.\u001b[39mbest_estimator_\u001b[39m.\u001b[39mpredict(X_test)\n",
      "File \u001b[0;32m~/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/sklearn/model_selection/_search.py:874\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    868\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[1;32m    869\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    870\u001b[0m     )\n\u001b[1;32m    872\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[0;32m--> 874\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[1;32m    876\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    878\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/sklearn/model_selection/_search.py:1768\u001b[0m, in \u001b[0;36mRandomizedSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1766\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1767\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1768\u001b[0m     evaluate_candidates(\n\u001b[1;32m   1769\u001b[0m         ParameterSampler(\n\u001b[1;32m   1770\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_distributions, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_iter, random_state\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrandom_state\n\u001b[1;32m   1771\u001b[0m         )\n\u001b[1;32m   1772\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/sklearn/model_selection/_search.py:851\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    844\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m!=\u001b[39m n_candidates \u001b[39m*\u001b[39m n_splits:\n\u001b[1;32m    845\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    846\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mcv.split and cv.get_n_splits returned \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    847\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39minconsistent results. Expected \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    848\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39msplits, got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(n_splits, \u001b[39mlen\u001b[39m(out) \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m n_candidates)\n\u001b[1;32m    849\u001b[0m     )\n\u001b[0;32m--> 851\u001b[0m _warn_or_raise_about_fit_failures(out, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49merror_score)\n\u001b[1;32m    853\u001b[0m \u001b[39m# For callable self.scoring, the return type is only know after\u001b[39;00m\n\u001b[1;32m    854\u001b[0m \u001b[39m# calling. If the return type is a dictionary, the error scores\u001b[39;00m\n\u001b[1;32m    855\u001b[0m \u001b[39m# can now be inserted with the correct key. The type checking\u001b[39;00m\n\u001b[1;32m    856\u001b[0m \u001b[39m# of out will be done in `_insert_error_scores`.\u001b[39;00m\n\u001b[1;32m    857\u001b[0m \u001b[39mif\u001b[39;00m callable(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscoring):\n",
      "File \u001b[0;32m~/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:367\u001b[0m, in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[0;34m(results, error_score)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[39mif\u001b[39;00m num_failed_fits \u001b[39m==\u001b[39m num_fits:\n\u001b[1;32m    361\u001b[0m     all_fits_failed_message \u001b[39m=\u001b[39m (\n\u001b[1;32m    362\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mAll the \u001b[39m\u001b[39m{\u001b[39;00mnum_fits\u001b[39m}\u001b[39;00m\u001b[39m fits failed.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    363\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIt is very likely that your model is misconfigured.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    364\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou can try to debug the error by setting error_score=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mraise\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    365\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBelow are more details about the failures:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mfit_errors_summary\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    366\u001b[0m     )\n\u001b[0;32m--> 367\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[1;32m    369\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    370\u001b[0m     some_fits_failed_message \u001b[39m=\u001b[39m (\n\u001b[1;32m    371\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mnum_failed_fits\u001b[39m}\u001b[39;00m\u001b[39m fits failed out of a total of \u001b[39m\u001b[39m{\u001b[39;00mnum_fits\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    372\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe score on these train-test partitions for these parameters\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBelow are more details about the failures:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mfit_errors_summary\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    377\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: \nAll the 10 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n1 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/sklearn/pipeline.py\", line 405, in fit\n    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1230, in fit\n    self.partial_fit(X, y, **fit_params)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1189, in partial_fit\n    self.fit_loop(X, y, **fit_params)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1101, in fit_loop\n    self.run_single_epoch(iterator_train, training=True, prefix=\"train\",\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1137, in run_single_epoch\n    step = step_fn(batch, **fit_params)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1016, in train_step\n    self._step_optimizer(step_fn)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 972, in _step_optimizer\n    optimizer.step(step_fn)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/torch/optim/optimizer.py\", line 113, in wrapper\n    return func(*args, **kwargs)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/torch/autograd/grad_mode.py\", line 27, in decorate_context\n    return func(*args, **kwargs)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/torch/optim/adamw.py\", line 119, in step\n    loss = closure()\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1006, in step_fn\n    step = self.train_step_single(batch, **fit_params)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 906, in train_step_single\n    loss = self.get_loss(y_pred, yi, X=Xi, training=True)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1571, in get_loss\n    return self.criterion_(y_pred, y_true)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/Users/JUSC/Documents/xgbsurv/experiments/deep_learning/loss_functions_pytorch.py\", line 288, in forward\n    loss = deephit_likelihood_1_torch(input, prediction)\n  File \"/Users/JUSC/Documents/xgbsurv/experiments/deep_learning/loss_functions_pytorch.py\", line 259, in deephit_likelihood_1_torch\n    raise ValueError(f\"Network output `phi` is too small for `idx_durations`.\"+\nValueError: Network output `phi` is too small for `idx_durations`. Need at least `phi.shape[1] = 119`, but got `phi.shape[1] = 2`\n\n--------------------------------------------------------------------------------\n2 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/sklearn/pipeline.py\", line 405, in fit\n    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1230, in fit\n    self.partial_fit(X, y, **fit_params)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1189, in partial_fit\n    self.fit_loop(X, y, **fit_params)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1101, in fit_loop\n    self.run_single_epoch(iterator_train, training=True, prefix=\"train\",\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1137, in run_single_epoch\n    step = step_fn(batch, **fit_params)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1016, in train_step\n    self._step_optimizer(step_fn)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 972, in _step_optimizer\n    optimizer.step(step_fn)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/torch/optim/optimizer.py\", line 113, in wrapper\n    return func(*args, **kwargs)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/torch/autograd/grad_mode.py\", line 27, in decorate_context\n    return func(*args, **kwargs)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/torch/optim/adamw.py\", line 119, in step\n    loss = closure()\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1006, in step_fn\n    step = self.train_step_single(batch, **fit_params)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 906, in train_step_single\n    loss = self.get_loss(y_pred, yi, X=Xi, training=True)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1571, in get_loss\n    return self.criterion_(y_pred, y_true)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/Users/JUSC/Documents/xgbsurv/experiments/deep_learning/loss_functions_pytorch.py\", line 288, in forward\n    loss = deephit_likelihood_1_torch(input, prediction)\n  File \"/Users/JUSC/Documents/xgbsurv/experiments/deep_learning/loss_functions_pytorch.py\", line 259, in deephit_likelihood_1_torch\n    raise ValueError(f\"Network output `phi` is too small for `idx_durations`.\"+\nValueError: Network output `phi` is too small for `idx_durations`. Need at least `phi.shape[1] = 118`, but got `phi.shape[1] = 2`\n\n--------------------------------------------------------------------------------\n1 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/sklearn/pipeline.py\", line 405, in fit\n    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1230, in fit\n    self.partial_fit(X, y, **fit_params)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1189, in partial_fit\n    self.fit_loop(X, y, **fit_params)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1101, in fit_loop\n    self.run_single_epoch(iterator_train, training=True, prefix=\"train\",\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1137, in run_single_epoch\n    step = step_fn(batch, **fit_params)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1016, in train_step\n    self._step_optimizer(step_fn)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 972, in _step_optimizer\n    optimizer.step(step_fn)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/torch/optim/optimizer.py\", line 113, in wrapper\n    return func(*args, **kwargs)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/torch/autograd/grad_mode.py\", line 27, in decorate_context\n    return func(*args, **kwargs)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/torch/optim/adamw.py\", line 119, in step\n    loss = closure()\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1006, in step_fn\n    step = self.train_step_single(batch, **fit_params)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 906, in train_step_single\n    loss = self.get_loss(y_pred, yi, X=Xi, training=True)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1571, in get_loss\n    return self.criterion_(y_pred, y_true)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/Users/JUSC/Documents/xgbsurv/experiments/deep_learning/loss_functions_pytorch.py\", line 288, in forward\n    loss = deephit_likelihood_1_torch(input, prediction)\n  File \"/Users/JUSC/Documents/xgbsurv/experiments/deep_learning/loss_functions_pytorch.py\", line 259, in deephit_likelihood_1_torch\n    raise ValueError(f\"Network output `phi` is too small for `idx_durations`.\"+\nValueError: Network output `phi` is too small for `idx_durations`. Need at least `phi.shape[1] = 112`, but got `phi.shape[1] = 2`\n\n--------------------------------------------------------------------------------\n1 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/sklearn/pipeline.py\", line 405, in fit\n    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1230, in fit\n    self.partial_fit(X, y, **fit_params)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1189, in partial_fit\n    self.fit_loop(X, y, **fit_params)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1101, in fit_loop\n    self.run_single_epoch(iterator_train, training=True, prefix=\"train\",\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1137, in run_single_epoch\n    step = step_fn(batch, **fit_params)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1016, in train_step\n    self._step_optimizer(step_fn)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 972, in _step_optimizer\n    optimizer.step(step_fn)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/torch/optim/optimizer.py\", line 113, in wrapper\n    return func(*args, **kwargs)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/torch/autograd/grad_mode.py\", line 27, in decorate_context\n    return func(*args, **kwargs)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/torch/optim/adamw.py\", line 119, in step\n    loss = closure()\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1006, in step_fn\n    step = self.train_step_single(batch, **fit_params)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 906, in train_step_single\n    loss = self.get_loss(y_pred, yi, X=Xi, training=True)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1571, in get_loss\n    return self.criterion_(y_pred, y_true)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/Users/JUSC/Documents/xgbsurv/experiments/deep_learning/loss_functions_pytorch.py\", line 288, in forward\n    loss = deephit_likelihood_1_torch(input, prediction)\n  File \"/Users/JUSC/Documents/xgbsurv/experiments/deep_learning/loss_functions_pytorch.py\", line 259, in deephit_likelihood_1_torch\n    raise ValueError(f\"Network output `phi` is too small for `idx_durations`.\"+\nValueError: Network output `phi` is too small for `idx_durations`. Need at least `phi.shape[1] = 116`, but got `phi.shape[1] = 2`\n\n--------------------------------------------------------------------------------\n1 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/sklearn/pipeline.py\", line 405, in fit\n    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1230, in fit\n    self.partial_fit(X, y, **fit_params)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1189, in partial_fit\n    self.fit_loop(X, y, **fit_params)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1101, in fit_loop\n    self.run_single_epoch(iterator_train, training=True, prefix=\"train\",\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1137, in run_single_epoch\n    step = step_fn(batch, **fit_params)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1016, in train_step\n    self._step_optimizer(step_fn)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 972, in _step_optimizer\n    optimizer.step(step_fn)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/torch/optim/optimizer.py\", line 113, in wrapper\n    return func(*args, **kwargs)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/torch/autograd/grad_mode.py\", line 27, in decorate_context\n    return func(*args, **kwargs)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/torch/optim/adamw.py\", line 119, in step\n    loss = closure()\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1006, in step_fn\n    step = self.train_step_single(batch, **fit_params)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 906, in train_step_single\n    loss = self.get_loss(y_pred, yi, X=Xi, training=True)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1571, in get_loss\n    return self.criterion_(y_pred, y_true)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/Users/JUSC/Documents/xgbsurv/experiments/deep_learning/loss_functions_pytorch.py\", line 288, in forward\n    loss = deephit_likelihood_1_torch(input, prediction)\n  File \"/Users/JUSC/Documents/xgbsurv/experiments/deep_learning/loss_functions_pytorch.py\", line 259, in deephit_likelihood_1_torch\n    raise ValueError(f\"Network output `phi` is too small for `idx_durations`.\"+\nValueError: Network output `phi` is too small for `idx_durations`. Need at least `phi.shape[1] = 62`, but got `phi.shape[1] = 2`\n\n--------------------------------------------------------------------------------\n3 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/sklearn/pipeline.py\", line 405, in fit\n    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1230, in fit\n    self.partial_fit(X, y, **fit_params)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1189, in partial_fit\n    self.fit_loop(X, y, **fit_params)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1101, in fit_loop\n    self.run_single_epoch(iterator_train, training=True, prefix=\"train\",\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1137, in run_single_epoch\n    step = step_fn(batch, **fit_params)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1016, in train_step\n    self._step_optimizer(step_fn)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 972, in _step_optimizer\n    optimizer.step(step_fn)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/torch/optim/optimizer.py\", line 113, in wrapper\n    return func(*args, **kwargs)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/torch/autograd/grad_mode.py\", line 27, in decorate_context\n    return func(*args, **kwargs)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/torch/optim/adamw.py\", line 119, in step\n    loss = closure()\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1006, in step_fn\n    step = self.train_step_single(batch, **fit_params)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 906, in train_step_single\n    loss = self.get_loss(y_pred, yi, X=Xi, training=True)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1571, in get_loss\n    return self.criterion_(y_pred, y_true)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/Users/JUSC/Documents/xgbsurv/experiments/deep_learning/loss_functions_pytorch.py\", line 288, in forward\n    loss = deephit_likelihood_1_torch(input, prediction)\n  File \"/Users/JUSC/Documents/xgbsurv/experiments/deep_learning/loss_functions_pytorch.py\", line 259, in deephit_likelihood_1_torch\n    raise ValueError(f\"Network output `phi` is too small for `idx_durations`.\"+\nValueError: Network output `phi` is too small for `idx_durations`. Need at least `phi.shape[1] = 63`, but got `phi.shape[1] = 2`\n\n--------------------------------------------------------------------------------\n1 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/sklearn/pipeline.py\", line 405, in fit\n    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1230, in fit\n    self.partial_fit(X, y, **fit_params)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1189, in partial_fit\n    self.fit_loop(X, y, **fit_params)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1101, in fit_loop\n    self.run_single_epoch(iterator_train, training=True, prefix=\"train\",\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1137, in run_single_epoch\n    step = step_fn(batch, **fit_params)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1016, in train_step\n    self._step_optimizer(step_fn)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 972, in _step_optimizer\n    optimizer.step(step_fn)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/torch/optim/optimizer.py\", line 113, in wrapper\n    return func(*args, **kwargs)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/torch/autograd/grad_mode.py\", line 27, in decorate_context\n    return func(*args, **kwargs)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/torch/optim/adamw.py\", line 119, in step\n    loss = closure()\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1006, in step_fn\n    step = self.train_step_single(batch, **fit_params)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 906, in train_step_single\n    loss = self.get_loss(y_pred, yi, X=Xi, training=True)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1571, in get_loss\n    return self.criterion_(y_pred, y_true)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/Users/JUSC/Documents/xgbsurv/experiments/deep_learning/loss_functions_pytorch.py\", line 288, in forward\n    loss = deephit_likelihood_1_torch(input, prediction)\n  File \"/Users/JUSC/Documents/xgbsurv/experiments/deep_learning/loss_functions_pytorch.py\", line 259, in deephit_likelihood_1_torch\n    raise ValueError(f\"Network output `phi` is too small for `idx_durations`.\"+\nValueError: Network output `phi` is too small for `idx_durations`. Need at least `phi.shape[1] = 61`, but got `phi.shape[1] = 2`\n"
     ]
    }
   ],
   "source": [
    "cancer_types = ['BLCA',\n",
    "    'BRCA',\n",
    "    'HNSC',\n",
    "    'KIRC',\n",
    "    'LGG',\n",
    "    'LIHC',\n",
    "    'LUAD',\n",
    "    'LUSC',\n",
    "    'OV',\n",
    "    'STAD']\n",
    "import skorch.callbacks\n",
    "\n",
    "class InputShapeSetter(skorch.callbacks.Callback):\n",
    "    def on_train_begin(self, net, X, y):\n",
    "        net.set_params(module__input_units=X.shape[-1])\n",
    "\n",
    "for idx, cancer_type in enumerate(cancer_types):\n",
    "    # get name of current dataset\n",
    "    data = load_tcga(path=\"/Users/JUSC/Documents/xgbsurv/xgbsurv/datasets/data/\", cancer_type=cancer_type, as_frame=True)\n",
    "    X  = data.data #.astype(np.float32)\n",
    "    y = data.target #.values #.to_numpy()\n",
    "\n",
    "    X, y = sort_X_y_pandas(X, y)\n",
    "\n",
    "    net = NeuralNet(\n",
    "        SurvivalModel, \n",
    "        module__n_layers = 1,\n",
    "        module__input_units = X.shape[1],\n",
    "        #module__num_nodes = 32,\n",
    "        #module__dropout = 0.1, # these could also be removed\n",
    "        module__out_features = 1,\n",
    "        # for split sizes when result size = 1\n",
    "        iterator_train__drop_last=True,\n",
    "        #iterator_valid__drop_last=True,\n",
    "        criterion=DeephitLoss,\n",
    "        optimizer=torch.optim.AdamW,\n",
    "        optimizer__weight_decay = 0.4,\n",
    "        batch_size=32, # separate train and valid->iterator_train__batch_size=128 and iterator_valid__batch_size=128 ?\n",
    "        callbacks=[\n",
    "            (\n",
    "                \"sched\",\n",
    "                LRScheduler(\n",
    "                    torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
    "                    monitor=\"valid_loss\",\n",
    "                    patience=5,\n",
    "                ),\n",
    "            ),\n",
    "            (\n",
    "                \"es\",\n",
    "                EarlyStopping(\n",
    "                    monitor=\"valid_loss\",\n",
    "                    patience=10,\n",
    "                    load_best=True,\n",
    "                ),\n",
    "            ),\n",
    "            (\"seed\", FixSeed(seed=42)),\n",
    "            (\"Inpout Shape Setter\",InputShapeSetter())\n",
    "        ],#[EarlyStopping(patience=10)],#,InputShapeSetter()],\n",
    "        #TODO: enable stratification, verify\n",
    "        train_split=ValidSplit(0.2), # might cause lower performance in metrics, explain in thesis\n",
    "        #lr=0.001,\n",
    "        #max_epochs=1, #0,#100\n",
    "        #train_split=None,\n",
    "        verbose=1\n",
    "    )\n",
    "    best_model,params, outer_scores, best_preds_train, best_preds_test, X_train, X_test, y_train, y_test = train_eval(X, y, net, n_iter, data.filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xgbsurv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
