{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import savetxt\n",
    "from xgbsurv.datasets import (load_metabric, load_flchain, load_rgbsg, load_support, load_tcga)\n",
    "from xgbsurv.models.utils import sort_X_y_pandas, transform_back, transform\n",
    "from xgbsurv.models.eh_final import get_cumulative_hazard_function_eh\n",
    "import torch\n",
    "from torch import nn\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold, train_test_split\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, LabelBinarizer, OneHotEncoder\n",
    "from sklearn.compose import make_column_transformer, make_column_selector\n",
    "from sklearn.decomposition import PCA\n",
    "from loss_functions_pytorch import EHLoss, eh_likelihood_torch_2\n",
    "from skorch import NeuralNet\n",
    "from skorch.callbacks import EarlyStopping, Callback, LRScheduler\n",
    "from skorch.dataset import ValidSplit\n",
    "from pycox.evaluation import EvalSurv\n",
    "from skorch.callbacks import EarlyStopping, Callback, LRScheduler\n",
    "import skorch.callbacks\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import check_cv\n",
    "from numbers import Number\n",
    "import torch.utils.data\n",
    "from skorch.utils import flatten\n",
    "from skorch.utils import is_pandas_ndframe\n",
    "from skorch.utils import check_indexing\n",
    "from skorch.utils import multi_indexing\n",
    "from skorch.utils import to_numpy\n",
    "from skorch.dataset import get_len\n",
    "from skorch.dataset import ValidSplit\n",
    "from pycox.evaluation import EvalSurv\n",
    "from scipy.stats import uniform as scuniform\n",
    "from scipy.stats import randint as scrandint\n",
    "from scipy.stats import loguniform as scloguniform\n",
    "import random\n",
    "import os\n",
    "#torch.set_default_dtype(torch.float64)\n",
    "#torch.set_default_tensor_type(torch.DoubleTensor)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters, put into function\n",
    "n_outer_splits = 5\n",
    "n_inner_splits = 5\n",
    "rand_state = 42\n",
    "n_iter = 1 # set to 50\n",
    "#n_iter_cind = 200\n",
    "early_stopping_rounds=10\n",
    "base_score = 0.0\n",
    "\n",
    "param_grid_eh = {\n",
    "    'estimator__module__n_layers': [1, 2, 4],\n",
    "    'estimator__module__num_nodes': [64, 128, 256, 512],\n",
    "    'estimator__module__dropout': scuniform(0.0,0.7),\n",
    "    'estimator__optimizer__weight_decay': [0.4, 0.2, 0.1, 0.05, 0.02, 0.01, 0],\n",
    "    'estimator__batch_size': [64, 128, 256, 512, 1024],\n",
    "    #lr not in paper because of learning rate finder\n",
    "    # note: setting learning rate higher would make exp(partial_hazard) explode\n",
    "    #'estimator__lr': scloguniform(0.001,0.01), # scheduler unten einbauen\n",
    "    # use callback instead\n",
    "    'estimator__lr':[0.001]\n",
    "    #'max_epochs':  scrandint(10,20), # corresponds to num_rounds\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_torch(seed=42):\n",
    "    \"\"\"Sets all seeds within torch and adjacent libraries.\n",
    "\n",
    "    Args:\n",
    "        seed: Random seed to be used by the seeding functions.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    #os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    #torch.backends.cudnn.benchmark = False\n",
    "    #torch.backends.cudnn.deterministic = True\n",
    "    return None\n",
    "\n",
    "\n",
    "class FixSeed(Callback):\n",
    "    def __init__(self, seed):\n",
    "        self.seed = seed\n",
    "\n",
    "    def initialize(self):\n",
    "        seed_torch(self.seed)\n",
    "        return super().initialize()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Scorer\n",
    "def custom_scoring_function(y_true, y_pred):\n",
    "\n",
    "        #y_true = torch.from_numpy(y_true)\n",
    "        if isinstance(y_pred, np.ndarray):\n",
    "            y_pred = torch.from_numpy(y_pred)\n",
    "        if isinstance(y_true, np.ndarray):\n",
    "            y_true = torch.from_numpy(y_true)\n",
    "        if isinstance(y_pred, pd.Series):\n",
    "            y_pred = torch.tensor(y_pred.values)\n",
    "        if isinstance(y_true, pd.Series):\n",
    "            y_true = torch.tensor(y_true.values)\n",
    "        #print('loss function y_pred', y_pred)\n",
    "        #print('loss function y_true', y_true)\n",
    "        score = eh_likelihood_torch_2(y_pred, y_true).to(torch.float32)\n",
    "        return score.numpy()\n",
    "\n",
    "scoring_function = make_scorer(custom_scoring_function, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set up Custom Splitter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Torch Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SurvivalModel(nn.Module):\n",
    "    def __init__(self, n_layers, input_units, num_nodes, dropout, out_features):\n",
    "        super(SurvivalModel, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.in_features = input_units\n",
    "        self.num_nodes = num_nodes\n",
    "        self.dropout = dropout\n",
    "        self.out_features = out_features\n",
    "        model = []\n",
    "        # first layer\n",
    "        model.append(torch.nn.Linear(input_units, num_nodes))\n",
    "        model.append(torch.nn.ReLU())\n",
    "        model.append(torch.nn.Dropout(dropout))\n",
    "        model.append(torch.nn.BatchNorm1d(num_nodes))\n",
    "\n",
    "        for i in range(n_layers-1):\n",
    "            model.append(torch.nn.Linear(num_nodes, num_nodes))\n",
    "            #init.kaiming_normal_(model[-1].weight, nonlinearity='relu')\n",
    "            model.append(torch.nn.ReLU())\n",
    "            model.append(torch.nn.Dropout(dropout))\n",
    "            model.append(torch.nn.BatchNorm1d(num_nodes))\n",
    "\n",
    "        # output layer\n",
    "        model.append(torch.nn.Linear(num_nodes, out_features))\n",
    "    \n",
    "        self.layers = nn.Sequential(*model)\n",
    "\n",
    "        # for layer in self.layers:\n",
    "        #     if isinstance(layer, nn.Linear):\n",
    "        #         #nn.init.uniform_(layer.weight, a=-0.5, b=0.5)\n",
    "        #         nn.init.kaiming_normal_(layer.weight)\n",
    "\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = X.to(torch.float32)\n",
    "        res = self.layers(X)\n",
    "        #print(res)\n",
    "        return res\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomStandardScaler(StandardScaler):\n",
    "    \n",
    "    def __init__(self, copy=True, with_mean=True, with_std=True):\n",
    "        super().__init__(copy=copy, with_mean=with_mean, with_std=with_std)\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return super().fit(X, y)\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X_transformed = super().transform(X, y)\n",
    "        return X_transformed.astype(np.float32)\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        X_transformed = super().fit_transform(X, y)\n",
    "        return X_transformed.astype(np.float32)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define stratified inner k-fold cross-validation\n",
    "class CustomSplit(StratifiedKFold):\n",
    "    def __init__(self, n_splits=5, shuffle=True, random_state=None):\n",
    "        super().__init__(n_splits=n_splits, shuffle=shuffle, random_state=random_state)\n",
    "\n",
    "    def split(self, X, y, groups=None):\n",
    "        print('split', X.dtypes)\n",
    "        try:\n",
    "            if y.shape[1]>1:\n",
    "                y = y[:,0]\n",
    "        except:\n",
    "            pass\n",
    "        bins = np.sign(y)\n",
    "        return super().split(X, bins, groups=groups)\n",
    "\n",
    "    def get_n_splits(self, X=None, y=None, groups=None):\n",
    "        return self.n_splits\n",
    "\n",
    "outer_custom_cv = CustomSplit(n_splits=n_outer_splits, shuffle=True, random_state=rand_state)\n",
    "inner_custom_cv = CustomSplit(n_splits=n_outer_splits, shuffle=True, random_state=rand_state)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CustomValidSplit():\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            cv=5,\n",
    "            stratified=False,\n",
    "            random_state=None,\n",
    "    ):\n",
    "        self.stratified = stratified\n",
    "        self.random_state = random_state\n",
    "\n",
    "        if isinstance(cv, Number) and (cv <= 0):\n",
    "            raise ValueError(\"Numbers less than 0 are not allowed for cv \"\n",
    "                             \"but ValidSplit got {}\".format(cv))\n",
    "\n",
    "        if not self._is_float(cv) and random_state is not None:\n",
    "            raise ValueError(\n",
    "                \"Setting a random_state has no effect since cv is not a float. \"\n",
    "                \"You should leave random_state to its default (None), or set cv \"\n",
    "                \"to a float value.\",\n",
    "            )\n",
    "\n",
    "        self.cv = cv\n",
    "\n",
    "    def _is_stratified(self, cv):\n",
    "        return isinstance(cv, (StratifiedKFold, StratifiedShuffleSplit))\n",
    "\n",
    "    def _is_float(self, x):\n",
    "        if not isinstance(x, Number):\n",
    "            return False\n",
    "        return not float(x).is_integer()\n",
    "\n",
    "    def _check_cv_float(self):\n",
    "        cv_cls = StratifiedShuffleSplit if self.stratified else ShuffleSplit\n",
    "        return cv_cls(test_size=self.cv, random_state=self.random_state)\n",
    "\n",
    "    def _check_cv_non_float(self, y):\n",
    "        return check_cv(\n",
    "            self.cv,\n",
    "            y=y,\n",
    "            classifier=self.stratified,\n",
    "        )\n",
    "\n",
    "    def check_cv(self, y):\n",
    "        \"\"\"Resolve which cross validation strategy is used.\"\"\"\n",
    "        y_arr = None\n",
    "        if self.stratified:\n",
    "            # Try to convert y to numpy for sklearn's check_cv; if conversion\n",
    "            # doesn't work, still try.\n",
    "            try:\n",
    "                y_arr = to_numpy(y)\n",
    "            except (AttributeError, TypeError):\n",
    "                y_arr = y\n",
    "\n",
    "        if self._is_float(self.cv):\n",
    "            return self._check_cv_float()\n",
    "        return self._check_cv_non_float(y_arr)\n",
    "\n",
    "    def _is_regular(self, x):\n",
    "        return (x is None) or isinstance(x, np.ndarray) or is_pandas_ndframe(x)\n",
    "\n",
    "    def __call__(self, dataset, y=None, groups=None):\n",
    "        # key change here\n",
    "        y = np.sign(y)\n",
    "        bad_y_error = ValueError(\n",
    "            \"Stratified CV requires explicitly passing a suitable y.\")\n",
    "        if (y is None) and self.stratified:\n",
    "            raise bad_y_error\n",
    "\n",
    "        cv = self.check_cv(y)\n",
    "        if self.stratified and not self._is_stratified(cv):\n",
    "            raise bad_y_error\n",
    "\n",
    "        # pylint: disable=invalid-name\n",
    "        len_dataset = get_len(dataset)\n",
    "        if y is not None:\n",
    "            len_y = get_len(y)\n",
    "            if len_dataset != len_y:\n",
    "                raise ValueError(\"Cannot perform a CV split if dataset and y \"\n",
    "                                 \"have different lengths.\")\n",
    "\n",
    "        args = (np.arange(len_dataset),)\n",
    "        if self._is_stratified(cv):\n",
    "            args = args + (to_numpy(y),)\n",
    "\n",
    "        idx_train, idx_valid = next(iter(cv.split(*args, groups=groups)))\n",
    "        dataset_train = torch.utils.data.Subset(dataset, idx_train)\n",
    "        dataset_valid = torch.utils.data.Subset(dataset, idx_valid)\n",
    "        return dataset_train, dataset_valid\n",
    "\n",
    "class InputShapeSetter(skorch.callbacks.Callback):\n",
    "    def on_train_begin(self, net, X, y):\n",
    "        net.set_params(module__input_units=X.shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving results\n",
    "metrics_sum = {}\n",
    "agg_metrics_cindex = []\n",
    "agg_metrics_ibs = []"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Training Procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_eval(X, y, net, n_iter, filename):\n",
    "        model = 'eh_'\n",
    "        dataset_name = filename.split('_')[0]\n",
    "        # add IBS later\n",
    "        outer_scores = {'cindex_test_'+dataset_name:[],\n",
    "                         'ibs_test_'+dataset_name:[]}\n",
    "        best_params = {'best_params_'+dataset_name:[]}\n",
    "        best_model = {'best_model_'+dataset_name:[]}\n",
    "        ct = make_column_transformer(\n",
    "                #(OneHotEncoder(sparse_output=False), make_column_selector(dtype_include=['category', 'object'])),\n",
    "                (StandardScaler(), make_column_selector(dtype_include=['float32'],dtype_exclude=['category', 'object']))\n",
    "                ,remainder='passthrough')\n",
    "\n",
    "        pipe = Pipeline([('scaler',ct),\n",
    "                        ('estimator', net)])\n",
    "        rs = RandomizedSearchCV(pipe, param_grid_eh, scoring = scoring_function, n_jobs=-1, \n",
    "                                cv=inner_custom_cv,   n_iter=n_iter, refit=True)\n",
    "        for i, (train_index, test_index) in enumerate(outer_custom_cv.split(X, y)):\n",
    "                # Split data into training and testing sets for outer fold\n",
    "                X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "                y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "                X_train, y_train = sort_X_y_pandas(X_train, y_train)\n",
    "                X_test, y_test = sort_X_y_pandas(X_test, y_test)\n",
    "\n",
    "                #print(X_train.shape, type(X_train))\n",
    "                #print(y_train.shape, type(y_train))\n",
    "                #print(y_test.shape, type(y_test))\n",
    "                #print(X_test.shape, type(X_test))\n",
    "                # save splits and data\n",
    "                savetxt('splits/train_index_'+str(i)+'_'+filename, train_index, delimiter=',')\n",
    "                savetxt('splits/test_index_'+str(i)+'_'+filename, test_index, delimiter=',')\n",
    "                \n",
    "                #savetxt('splits/X_train_'+str(i)+'_'+filename, X_train, delimiter=',')\n",
    "                #savetxt('splits/X_test_'+str(i)+'_'+filename, X_test, delimiter=',')\n",
    "\n",
    "                #savetxt('splits/y_train_'+str(i)+'_'+filename, y_train, delimiter=',')\n",
    "                #savetxt('splits/y_test_'+str(i)+'_'+filename, y_test, delimiter=',')\n",
    "\n",
    "\n",
    "\n",
    "                rs.fit(X_train, y_train)\n",
    "                best_preds_train = rs.best_estimator_.predict(X_train)\n",
    "                best_preds_test = rs.best_estimator_.predict(X_test)\n",
    "\n",
    "                savetxt('predictions/'+model+'best_preds_train_'+str(i)+'_'+filename, best_preds_train, delimiter=',')\n",
    "                savetxt('predictions/'+model+'best_preds_test_'+str(i)+'_'+filename, best_preds_test, delimiter=',')\n",
    "                # save hyperparameter settings\n",
    "                params = rs.best_estimator_.get_params()\n",
    "                best_params['best_params_'+dataset_name] += [rs.best_params_]\n",
    "                best_model['best_model_'+dataset_name] += [params]\n",
    "                    \n",
    "                #try:\n",
    "                cum_hazard_test = get_cumulative_hazard_function_eh(\n",
    "                        X_train.values, X_test.values, y_train.values, y_test.values,\n",
    "                        best_preds_train, best_preds_test\n",
    "                        )\n",
    "                df_survival_test = np.exp(-cum_hazard_test)\n",
    "                durations_test, events_test = transform_back(y_test.values)\n",
    "                #print('durations',durations_test.min(), durations_test.max())\n",
    "                time_grid_test = np.linspace(durations_test.min(), durations_test.max(), 100)\n",
    "                ev = EvalSurv(df_survival_test, durations_test, events_test, censor_surv='km')\n",
    "                #print('Concordance Index',ev.concordance_td('antolini'))\n",
    "                #print('Integrated Brier Score:',ev.integrated_brier_score(time_grid_test))\n",
    "                cindex_score_test = ev.concordance_td('antolini')\n",
    "                ibs_score_test = ev.integrated_brier_score(time_grid_test)\n",
    "                print('Concordance Index',cindex_score_test)\n",
    "                print('Integrated Brier Score:',ibs_score_test)\n",
    "\n",
    "                outer_scores['cindex_test_'+dataset_name] += [cindex_score_test]\n",
    "                outer_scores['ibs_test_'+dataset_name] += [ibs_score_test]\n",
    "                # except: \n",
    "                #     outer_scores['cindex_test_'+dataset_name] += [np.nan]\n",
    "                #     outer_scores['ibs_test_'+dataset_name] += [np.nan]\n",
    "            \n",
    "        df_best_params = pd.DataFrame(best_params)\n",
    "        df_best_model = pd.DataFrame(best_model)\n",
    "        df_outer_scores = pd.DataFrame(outer_scores)\n",
    "        df_metrics = pd.concat([df_best_params,df_best_model,df_outer_scores], axis=1)\n",
    "        df_metrics.to_csv('metrics/'+model+'metric_summary_'+filename, index=False)\n",
    "        # cindex\n",
    "        df_agg_metrics_cindex = pd.DataFrame({'dataset':[dataset_name],\n",
    "                                              'cindex_test_mean':df_outer_scores['cindex_test_'+dataset_name].mean(),\n",
    "                                              'cindex_test_std':df_outer_scores['cindex_test_'+dataset_name].std() })\n",
    "        # IBS\n",
    "        df_agg_metrics_ibs = pd.DataFrame({'dataset':[dataset_name],\n",
    "                                              'ibs_test_mean':df_outer_scores['ibs_test_'+dataset_name].mean(),\n",
    "                                              'ibs_test_std':df_outer_scores['ibs_test_'+dataset_name].std() })\n",
    "        \n",
    "        return df_agg_metrics_cindex, df_agg_metrics_ibs, best_model, best_params, outer_scores, best_preds_train, best_preds_test \n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skorch.callbacks\n",
    "class InputShapeSetter(skorch.callbacks.Callback):\n",
    "    def on_train_begin(self, net, X, y):\n",
    "        net.set_params(module__input_units=X.shape[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_flchain\n",
      "split age           float32\n",
      "sex             uint8\n",
      "sample_yr     float32\n",
      "kappa         float32\n",
      "lambda        float32\n",
      "flc_grp       float32\n",
      "creatinine    float32\n",
      "mgus_0          uint8\n",
      "mgus_1          uint8\n",
      "dtype: object\n",
      "split age           float32\n",
      "sex             uint8\n",
      "sample_yr     float32\n",
      "kappa         float32\n",
      "lambda        float32\n",
      "flc_grp       float32\n",
      "creatinine    float32\n",
      "mgus_0          uint8\n",
      "mgus_1          uint8\n",
      "dtype: object\n",
      "max round 60.44\n",
      "integration_values.shape[0] 315183\n",
      "Concordance Index 0.8003787452656842\n",
      "Integrated Brier Score: 0.09646041176602556\n",
      "split age           float32\n",
      "sex             uint8\n",
      "sample_yr     float32\n",
      "kappa         float32\n",
      "lambda        float32\n",
      "flc_grp       float32\n",
      "creatinine    float32\n",
      "mgus_0          uint8\n",
      "mgus_1          uint8\n",
      "dtype: object\n",
      "max round 258.86\n",
      "integration_values.shape[0] 1326135\n",
      "Concordance Index 0.8031341385296658\n",
      "Integrated Brier Score: 0.096527876948678\n",
      "split age           float32\n",
      "sex             uint8\n",
      "sample_yr     float32\n",
      "kappa         float32\n",
      "lambda        float32\n",
      "flc_grp       float32\n",
      "creatinine    float32\n",
      "mgus_0          uint8\n",
      "mgus_1          uint8\n",
      "dtype: object\n",
      "max round 159.19\n",
      "integration_values.shape[0] 822380\n",
      "Concordance Index 0.7838514794072367\n",
      "Integrated Brier Score: 0.09971266003653097\n",
      "split age           float32\n",
      "sex             uint8\n",
      "sample_yr     float32\n",
      "kappa         float32\n",
      "lambda        float32\n",
      "flc_grp       float32\n",
      "creatinine    float32\n",
      "mgus_0          uint8\n",
      "mgus_1          uint8\n",
      "dtype: object\n",
      "max round 10.24\n",
      "integration_values.shape[0] 52960\n",
      "Concordance Index 0.7715438521179532\n",
      "Integrated Brier Score: 0.10083315258065494\n",
      "split age           float32\n",
      "sex             uint8\n",
      "sample_yr     float32\n",
      "kappa         float32\n",
      "lambda        float32\n",
      "flc_grp       float32\n",
      "creatinine    float32\n",
      "mgus_0          uint8\n",
      "mgus_1          uint8\n",
      "dtype: object\n",
      "max round 71.35\n",
      "integration_values.shape[0] 370084\n",
      "Concordance Index 0.7871790843218452\n",
      "Integrated Brier Score: 0.09372255951290648\n"
     ]
    }
   ],
   "source": [
    "data_set_fns = [ load_flchain] #load_rgbsg, load_supportload_metabric, 'load_flchain': ['mgus'],'load_metabric', 'load_flchain', load_flchain,load_metabric, , load_flchain, load_rgbsg, load_support, load_1]\n",
    "data_set_fns_str = ['load_flchain'] \n",
    "one_hot_dict = {'load_flchain': ['mgus'] }#'load_rgbsg':['grade'], 'load_support':['cancer', 'race']\n",
    "\n",
    "for idx, dataset in enumerate(data_set_fns):\n",
    "    # get name of current dataset\n",
    "    data = dataset(path=\"/Users/JUSC/Documents/xgbsurv/xgbsurv/datasets/data/\", as_frame=True)\n",
    "    X  = data.data #.astype(np.float32)\n",
    "    y = data.target #.values #.to_numpy()\n",
    "\n",
    "    print(data_set_fns_str[idx])\n",
    "    if data_set_fns_str[idx] in one_hot_dict.keys():\n",
    "        X = pd.get_dummies(X, columns=one_hot_dict[data_set_fns_str[idx]])\n",
    "    X, y = sort_X_y_pandas(X, y)\n",
    "    \n",
    "    net = NeuralNet(\n",
    "        SurvivalModel, \n",
    "        module__n_layers = 1,\n",
    "        module__input_units = X.shape[1],\n",
    "        #module__num_nodes = 32,\n",
    "        #module__dropout = 0.1, # these could also be removed\n",
    "        module__out_features = 2,\n",
    "        # for split sizes when result size = 1\n",
    "        iterator_train__drop_last=True,\n",
    "        #iterator_valid__drop_last=True,\n",
    "        criterion=EHLoss,\n",
    "        optimizer=torch.optim.AdamW,\n",
    "        optimizer__weight_decay = 0.4,\n",
    "        batch_size=32, # separate train and valid->iterator_train__batch_size=128 and iterator_valid__batch_size=128 ?\n",
    "        callbacks=[\n",
    "            (\n",
    "                \"sched\",\n",
    "                LRScheduler(\n",
    "                    torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
    "                    monitor=\"valid_loss\",\n",
    "                    patience=5,\n",
    "                ),\n",
    "            ),\n",
    "            (\n",
    "                \"es\",\n",
    "                EarlyStopping(\n",
    "                    monitor=\"valid_loss\",\n",
    "                    patience=10,\n",
    "                    load_best=True,\n",
    "                ),\n",
    "            ),\n",
    "            (\"seed\", FixSeed(seed=42)),\n",
    "            #(\"Input Shape Setter\",InputShapeSetter())\n",
    "        ],\n",
    "        \n",
    "        #[EarlyStopping(patience=10)],\n",
    "        # add extensive callback, and random number seed\n",
    "        #TODO: enable stratification, verify\n",
    "        train_split=CustomValidSplit(0.2, stratified=True, random_state=rand_state), #ValidSplit(0.2), # might cause lower performance in metrics, explain in thesis\n",
    "        #lr=0.001,\n",
    "        #max_epochs=1, #0,#100\n",
    "        #train_split=None,\n",
    "        verbose=0\n",
    "    )\n",
    "    df_agg_metrics_cindex, df_agg_metrics_ibs, best_model,params, outer_scores, best_preds_train, best_preds_test = train_eval(X, y, net, n_iter, data.filename)\n",
    "    agg_metrics_cindex.append(df_agg_metrics_cindex)\n",
    "    agg_metrics_ibs.append(df_agg_metrics_ibs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>best_params_METABRIC</th>\n",
       "      <th>best_model_METABRIC</th>\n",
       "      <th>cindex_test_METABRIC</th>\n",
       "      <th>ibs_test_METABRIC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'estimator__batch_size': 128, 'estimator__lr'...</td>\n",
       "      <td>&lt;bound method Pipeline.get_params of Pipeline(...</td>\n",
       "      <td>0.632832</td>\n",
       "      <td>0.175834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'estimator__batch_size': 1024, 'estimator__lr...</td>\n",
       "      <td>&lt;bound method Pipeline.get_params of Pipeline(...</td>\n",
       "      <td>0.604024</td>\n",
       "      <td>0.166881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'estimator__batch_size': 64, 'estimator__lr':...</td>\n",
       "      <td>&lt;bound method Pipeline.get_params of Pipeline(...</td>\n",
       "      <td>0.629899</td>\n",
       "      <td>0.180928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'estimator__batch_size': 64, 'estimator__lr':...</td>\n",
       "      <td>&lt;bound method Pipeline.get_params of Pipeline(...</td>\n",
       "      <td>0.599179</td>\n",
       "      <td>0.166390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'estimator__batch_size': 128, 'estimator__lr'...</td>\n",
       "      <td>&lt;bound method Pipeline.get_params of Pipeline(...</td>\n",
       "      <td>0.634212</td>\n",
       "      <td>0.167137</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                best_params_METABRIC  \\\n",
       "0  {'estimator__batch_size': 128, 'estimator__lr'...   \n",
       "1  {'estimator__batch_size': 1024, 'estimator__lr...   \n",
       "2  {'estimator__batch_size': 64, 'estimator__lr':...   \n",
       "3  {'estimator__batch_size': 64, 'estimator__lr':...   \n",
       "4  {'estimator__batch_size': 128, 'estimator__lr'...   \n",
       "\n",
       "                                 best_model_METABRIC  cindex_test_METABRIC  \\\n",
       "0  <bound method Pipeline.get_params of Pipeline(...              0.632832   \n",
       "1  <bound method Pipeline.get_params of Pipeline(...              0.604024   \n",
       "2  <bound method Pipeline.get_params of Pipeline(...              0.629899   \n",
       "3  <bound method Pipeline.get_params of Pipeline(...              0.599179   \n",
       "4  <bound method Pipeline.get_params of Pipeline(...              0.634212   \n",
       "\n",
       "   ibs_test_METABRIC  \n",
       "0           0.175834  \n",
       "1           0.166881  \n",
       "2           0.180928  \n",
       "3           0.166390  \n",
       "4           0.167137  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_outer_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                best_params_METABRIC  \\\n",
      "0  {'estimator__batch_size': 512, 'estimator__lr'...   \n",
      "1  {'estimator__batch_size': 512, 'estimator__lr'...   \n",
      "2  {'estimator__batch_size': 512, 'estimator__lr'...   \n",
      "3  {'estimator__batch_size': 512, 'estimator__lr'...   \n",
      "4  {'estimator__batch_size': 512, 'estimator__lr'...   \n",
      "\n",
      "                                 best_model_METABRIC  cindex_test_METABRIC  \\\n",
      "0  {'memory': None, 'steps': [('scaler', ColumnTr...              0.656958   \n",
      "1  {'memory': None, 'steps': [('scaler', ColumnTr...              0.654683   \n",
      "2  {'memory': None, 'steps': [('scaler', ColumnTr...              0.665623   \n",
      "3  {'memory': None, 'steps': [('scaler', ColumnTr...              0.663093   \n",
      "4  {'memory': None, 'steps': [('scaler', ColumnTr...              0.671589   \n",
      "\n",
      "   ibs_test_METABRIC  \n",
      "0           0.168701  \n",
      "1           0.169949  \n",
      "2           0.176324  \n",
      "3           0.161448  \n",
      "4           0.169603  \n",
      "METABRIC\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'eh_skorch_results/eh_metric_summary_flchain_adapted.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m datasets \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mmetabric\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mflchain\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mrgbsg\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msupport\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m name \u001b[39min\u001b[39;00m datasets:\n\u001b[0;32m----> 5\u001b[0m     df_outer_scores \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39m'\u001b[39;49m\u001b[39meh_skorch_results/eh_metric_summary_\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m+\u001b[39;49mname\u001b[39m+\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m_adapted.csv\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      6\u001b[0m     \u001b[39mprint\u001b[39m(df_outer_scores)\n\u001b[1;32m      7\u001b[0m     dataset_name \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m\u001b[39m.\u001b[39mupper(name)\n",
      "File \u001b[0;32m~/miniconda3/envs/xgbsurv/lib/python3.11/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[39m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/xgbsurv/lib/python3.11/site-packages/pandas/io/parsers/readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    665\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    666\u001b[0m     dialect,\n\u001b[1;32m    667\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    676\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[1;32m    677\u001b[0m )\n\u001b[1;32m    678\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 680\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/miniconda3/envs/xgbsurv/lib/python3.11/site-packages/pandas/io/parsers/readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    572\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    574\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 575\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    577\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[1;32m    578\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/miniconda3/envs/xgbsurv/lib/python3.11/site-packages/pandas/io/parsers/readers.py:933\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    930\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    932\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 933\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[0;32m~/miniconda3/envs/xgbsurv/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1217\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1213\u001b[0m     mode \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1214\u001b[0m \u001b[39m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[1;32m   1215\u001b[0m \u001b[39m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[1;32m   1216\u001b[0m \u001b[39m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[0;32m-> 1217\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(  \u001b[39m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[1;32m   1218\u001b[0m     f,\n\u001b[1;32m   1219\u001b[0m     mode,\n\u001b[1;32m   1220\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1221\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1222\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[1;32m   1223\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[1;32m   1224\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1225\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1226\u001b[0m )\n\u001b[1;32m   1227\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1228\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/miniconda3/envs/xgbsurv/lib/python3.11/site-packages/pandas/io/common.py:789\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    784\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    785\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    786\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    787\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[1;32m    788\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(\n\u001b[1;32m    790\u001b[0m             handle,\n\u001b[1;32m    791\u001b[0m             ioargs\u001b[39m.\u001b[39mmode,\n\u001b[1;32m    792\u001b[0m             encoding\u001b[39m=\u001b[39mioargs\u001b[39m.\u001b[39mencoding,\n\u001b[1;32m    793\u001b[0m             errors\u001b[39m=\u001b[39merrors,\n\u001b[1;32m    794\u001b[0m             newline\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    795\u001b[0m         )\n\u001b[1;32m    796\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    797\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[1;32m    798\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'eh_skorch_results/eh_metric_summary_flchain_adapted.csv'"
     ]
    }
   ],
   "source": [
    "# to create summary from already existing data\n",
    "datasets = ['metabric', 'flchain', 'rgbsg', 'support']\n",
    "\n",
    "for name in datasets:\n",
    "    df_outer_scores = pd.read_csv('eh_skorch_results/eh_metric_summary_'+name+'_adapted.csv')\n",
    "    print(df_outer_scores)\n",
    "    dataset_name = str.upper(name)\n",
    "    print(dataset_name)\n",
    "#     # cindex\n",
    "#     df_agg_metrics_cindex = pd.DataFrame({'dataset':[dataset_name],\n",
    "#                                             'cindex_test_mean':df_outer_scores['cindex_test_'+dataset_name].mean(),\n",
    "#                                             'cindex_test_std':df_outer_scores['cindex_test_'+dataset_name].std() })\n",
    "#     # IBS\n",
    "#     df_agg_metrics_ibs = pd.DataFrame({'dataset':[dataset_name],\n",
    "#                                             'ibs_test_mean':df_outer_scores['ibs_test_'+dataset_name].mean(),\n",
    "#                                             'ibs_test_std':df_outer_scores['ibs_test_'+dataset_name].std() })\n",
    "    \n",
    "#     agg_metrics_cindex.append(df_agg_metrics_cindex)\n",
    "#     agg_metrics_ibs.append(df_agg_metrics_ibs)\n",
    "\n",
    "# df_final_eh_1_cindex = pd.concat([df for df in agg_metrics_cindex]).round(4)\n",
    "# df_final_eh_1_cindex.to_csv('metrics/final_gbdt_1_eh_cindex.csv', index=False)\n",
    "# df_final_eh_1_cindex.to_csv('/Users/JUSC/Documents/644928e0fb7e147893e8ec15/05_thesis/tables/final_dl_1_eh_cindex.csv', index=False)  #\n",
    "# df_final_eh_1_cindex\n",
    "\n",
    "# df_final_eh_1_ibs = pd.concat([df for df in agg_metrics_ibs]).round(4)\n",
    "# df_final_eh_1_ibs.to_csv('metrics/final_gbdt_1_eh_ibs.csv', index=False)\n",
    "# df_final_eh_1_ibs.to_csv('/Users/JUSC/Documents/644928e0fb7e147893e8ec15/05_thesis/tables/final_dl_1_eh_ibs.csv', index=False) \n",
    "# df_final_eh_1_ibs\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TCGA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_breslow_tcga = {\n",
    "    'estimator__module__n_layers': [1, 2, 4],\n",
    "    'estimator__module__num_nodes': [64, 128, 256, 512],\n",
    "    'estimator__module__dropout': scuniform(0.0,0.7),\n",
    "    'estimator__optimizer__weight_decay': [0.4, 0.2, 0.1, 0.05, 0.02, 0.01, 0],\n",
    "    'estimator__batch_size': [64, 128, 256, 512, 1024],\n",
    "    #lr not in paper because of learning rate finder\n",
    "    # note: setting learning rate higher would make exp(partial_hazard) explode\n",
    "    #'estimator__lr': scloguniform(0.001,0.01), # scheduler unten einbauen\n",
    "    # use callback instead\n",
    "    'estimator__lr':[0.001],\n",
    "    'estimator__max_epochs':  scrandint(150,250), # corresponds to num_rounds\n",
    "    #'pca__n_components': [8, 10, 12, 14, 16]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_eval(X, y, net, n_iter, filename):\n",
    "        model = 'eh_tcga_'\n",
    "        dataset_name = filename.split('_')[0]\n",
    "        # add IBS later\n",
    "        outer_scores = { 'cindex_test_'+dataset_name:[],\n",
    "                        'ibs_test_'+dataset_name:[]}\n",
    "        best_params = {'best_params_'+dataset_name:[]}\n",
    "        best_model = {'best_model_'+dataset_name:[]}\n",
    "        ct = make_column_transformer(\n",
    "                #(OneHotEncoder(sparse_output=False), make_column_selector(dtype_include=['category', 'object']))\n",
    "                (StandardScaler(), make_column_selector(dtype_include=['float32']))\n",
    "                ,remainder='drop')\n",
    "        pipe = Pipeline([('scaler',ct),\n",
    "                         #('pca', PCA()),#n_components=10\n",
    "                        ('estimator', net)])\n",
    "        rs = RandomizedSearchCV(pipe, param_grid_breslow_tcga, scoring = scoring_function, n_jobs=-1, \n",
    "                                    n_iter=n_iter, refit=True, random_state=rand_state)\n",
    "        \n",
    "        for i, (train_index, test_index) in enumerate(outer_custom_cv.split(X, y)):\n",
    "                # Split data into training and testing sets for outer fold\n",
    "                X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "                y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "                X_train, y_train = sort_X_y_pandas(X_train, y_train)\n",
    "                X_test, y_test = sort_X_y_pandas(X_test, y_test)\n",
    "\n",
    "                #print(X_train.shape, type(X_train))\n",
    "                #print(y_train.shape, type(y_train))\n",
    "                #print(X_test.shape, type(X_test))\n",
    "                #print(y_test.shape, type(y_test))\n",
    "                # save splits and data\n",
    "                savetxt('splits/train_index_'+str(i)+'_'+filename, train_index, delimiter=',')\n",
    "                savetxt('splits/test_index_'+str(i)+'_'+filename, test_index, delimiter=',')\n",
    "                \n",
    "                # savetxt('splits/X_train_'+str(i)+'_'+filename, X_train, delimiter=',')\n",
    "                # savetxt('splits/X_test_'+str(i)+'_'+filename, X_test, delimiter=',')\n",
    "\n",
    "                # savetxt('splits/y_train_'+str(i)+'_'+filename, y_train, delimiter=',')\n",
    "                # savetxt('splits/y_test_'+str(i)+'_'+filename, y_test, delimiter=',')\n",
    "\n",
    "\n",
    "                rs.fit(X_train, y_train)\n",
    "                best_preds_train = rs.best_estimator_.predict(X_train)\n",
    "                best_preds_test = rs.best_estimator_.predict(X_test)\n",
    "\n",
    "                savetxt('predictions/'+model+'best_preds_train_'+str(i)+'_'+filename, best_preds_train, delimiter=',')\n",
    "                savetxt('predictions/'+model+'best_preds_test_'+str(i)+'_'+filename, best_preds_test, delimiter=',')\n",
    "                # save hyperparameter settings\n",
    "                params = rs.best_estimator_.get_params\n",
    "                best_params['best_params_'+dataset_name] += [rs.best_params_]\n",
    "                best_model['best_model_'+dataset_name] += [params]\n",
    "\n",
    "                    \n",
    "                #try:\n",
    "                cum_hazard_test = get_cumulative_hazard_function_eh(\n",
    "                        X_train.values, X_test.values, y_train.values, y_test.values,\n",
    "                        best_preds_train, best_preds_test\n",
    "                        )\n",
    "                df_survival_test = np.exp(-cum_hazard_test)\n",
    "                durations_test, events_test = transform_back(y_test.values)\n",
    "                time_grid_test = np.linspace(durations_test.min(), durations_test.max(), 100)\n",
    "                ev = EvalSurv(df_survival_test, durations_test, events_test, censor_surv='km')\n",
    "                cindex_score_test = ev.concordance_td('antolini')\n",
    "                ibs_score_test = ev.integrated_brier_score(time_grid_test)\n",
    "                print('Concordance Index',cindex_score_test )\n",
    "                print('Integrated Brier Score:',cindex_score_test)\n",
    "                \n",
    "                outer_scores['cindex_test_'+dataset_name] += [cindex_score_test]\n",
    "                outer_scores['ibs_test_'+dataset_name] += [ibs_score_test]\n",
    "                # except: \n",
    "                #     outer_scores['cindex_test_'+dataset_name] += [np.nan]\n",
    "                #     outer_scores['ibs_test_'+dataset_name] += [np.nan]\n",
    "            \n",
    "        df_best_params = pd.DataFrame(best_params)\n",
    "        df_best_model = pd.DataFrame(best_model)\n",
    "        df_outer_scores = pd.DataFrame(outer_scores)\n",
    "        df_metrics = pd.concat([df_best_params,df_best_model,df_outer_scores], axis=1)\n",
    "        df_metrics.to_csv('metrics/'+model+'metric_summary_'+filename, index=False)\n",
    "        # cindex\n",
    "        df_agg_metrics_cindex = pd.DataFrame({'dataset':[dataset_name],\n",
    "                                              'cindex_test_mean':df_outer_scores['cindex_test_'+dataset_name].mean(),\n",
    "                                              'cindex_test_std':df_outer_scores['cindex_test_'+dataset_name].std() })\n",
    "        # IBS\n",
    "        df_agg_metrics_ibs = pd.DataFrame({'dataset':[dataset_name],\n",
    "                                              'ibs_test_mean':df_outer_scores['ibs_test_'+dataset_name].mean(),\n",
    "                                              'ibs_test_std':df_outer_scores['ibs_test_'+dataset_name].std() })\n",
    "        \n",
    "        return df_agg_metrics_cindex, df_agg_metrics_ibs, best_model, best_params, outer_scores, best_preds_train, best_preds_test \n",
    "\n",
    "                \n",
    "#cv=inner_custom_cv,pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split gex_?|100130426      float32\n",
      "gex_?|100133144      float32\n",
      "gex_?|100134869      float32\n",
      "gex_?|10357          float32\n",
      "gex_?|10431          float32\n",
      "                      ...   \n",
      "gex_ZYG11A|440590    float32\n",
      "gex_ZYG11B|79699     float32\n",
      "gex_ZYX|7791         float32\n",
      "gex_ZZEF1|23140      float32\n",
      "gex_ZZZ3|26009       float32\n",
      "Length: 20531, dtype: object\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.2167\u001b[0m  0.0084\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.3575\u001b[0m  0.0114\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.0869\u001b[0m  0.0073\n",
      "      2        3.2167  0.0067\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2        3.0869  0.0080\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.1969\u001b[0m  0.0078\n",
      "      2        3.3575  0.0089\n",
      "      3        3.2167  0.0076\n",
      "      3        3.3575  0.0071\n",
      "      3        3.0869  0.0087\n",
      "      2        3.1969  0.0086\n",
      "      4        3.2167  0.0085\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.4612\u001b[0m  0.0088      4        3.3575  0.0064\n",
      "\n",
      "      4        3.0869  0.0064\n",
      "      3        3.1969  0.0075\n",
      "      5        3.2167  0.0086\n",
      "      5        3.3575  0.0062\n",
      "      2        3.4612  0.0076\n",
      "      6        3.3575  0.0053\n",
      "      4        3.1969  0.0115\n",
      "      6        3.2167  0.0134\n",
      "      5        3.1969  0.0078\n",
      "      7        3.3575  0.0130\n",
      "      7        3.2167  0.0066\n",
      "      5        3.0869  0.0281\n",
      "      3        3.4612  0.0215\n",
      "      8        3.3575  0.0075\n",
      "      8        3.2167  0.0091\n",
      "      4        3.4612  0.0082\n",
      "      6        3.0869  0.0086\n",
      "      9        3.3575  0.0094\n",
      "      6        3.1969  0.0213\n",
      "      7        3.0869  0.0080\n",
      "      5        3.4612  0.0104\n",
      "     10        3.3575  0.0064\n",
      "      9        3.2167  0.0162\n",
      "      7        3.1969  0.0113\n",
      "      6        3.4612  0.0069\n",
      "     10        3.2167  0.0060\n",
      "      8        3.0869  0.0153\n",
      "      8        3.1969  0.0067\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "Restoring best model from epoch 1.\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "Restoring best model from epoch 1.\n",
      "      7        3.4612  0.0156\n",
      "      9        3.0869  0.0130\n",
      "      9        3.1969  0.0128\n",
      "      8        3.4612  0.0049\n",
      "     10        3.0869  0.0056\n",
      "     10        3.1969  0.0163\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "Restoring best model from epoch 1.\n",
      "      9        3.4612  0.0197\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "Restoring best model from epoch 1.\n",
      "     10        3.4612  0.0062\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.4140\u001b[0m  0.0174\n",
      "      2        3.4140  0.0144\n",
      "      3        3.4140  0.0169\n",
      "      4        3.4140  0.0257\n",
      "      5        3.4140  0.0190\n",
      "      6        3.4140  0.0122\n",
      "      7        3.4140  0.0147\n",
      "      8        3.4140  0.0168\n",
      "      9        3.4140  0.0173\n",
      "     10        3.4140  0.0143\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "Restoring best model from epoch 1.\n",
      "max round 1.98\n",
      "integration_values.shape[0] 8621\n",
      "Concordance Index 0.556353591160221\n",
      "Integrated Brier Score: 0.556353591160221\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.1703\u001b[0m  0.0044\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2        3.1703  0.0039\n",
      "      3        3.1703  0.0040\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.1923\u001b[0m  0.0053\n",
      "      4        3.1703  0.0044\n",
      "      2        3.1923  0.0082\n",
      "      5        3.1703  0.0052\n",
      "      6        3.1703  0.0049\n",
      "      7        3.1703  0.0051\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      8        3.1703  0.0055\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      9        3.1703  0.0055\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      3        3.1923  0.0289\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.4477\u001b[0m  0.0070\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.2702\u001b[0m  0.0106      4        3.1923  0.0086      2        3.4477  0.0086\n",
      "\n",
      "\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.3645\u001b[0m  0.0065\n",
      "     10        3.1703  0.0192\n",
      "      2        3.3645  0.0064\n",
      "      3        3.4477  0.0111\n",
      "      5        3.1923  0.0136\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "      2        3.2702  0.0146\n",
      "Restoring best model from epoch 1.\n",
      "      3        3.3645  0.0077\n",
      "      4        3.4477  0.0085\n",
      "      6        3.1923  0.0079\n",
      "      4        3.3645  0.0061\n",
      "      3        3.2702  0.0079\n",
      "      5        3.4477  0.0075\n",
      "      5        3.3645  0.0052\n",
      "      7        3.1923  0.0074\n",
      "      6        3.3645  0.0044\n",
      "      4        3.2702  0.0086\n",
      "      8        3.1923  0.0057\n",
      "      7        3.3645  0.0043\n",
      "      6        3.4477  0.0134\n",
      "      8        3.3645  0.0046\n",
      "      5        3.2702  0.0104\n",
      "      9        3.1923  0.0124\n",
      "      6        3.2702  0.0058\n",
      "      7        3.4477  0.0106\n",
      "     10        3.1923  0.0064\n",
      "      7        3.2702  0.0057\n",
      "      8        3.4477  0.0057\n",
      "      9        3.3645  0.0160\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "Restoring best model from epoch 1.\n",
      "      8        3.2702  0.0069\n",
      "      9        3.4477  0.0058\n",
      "     10        3.3645  0.0068\n",
      "     10        3.4477  0.0054\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "Restoring best model from epoch 1.\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "Restoring best model from epoch 1.\n",
      "      9        3.2702  0.0144\n",
      "     10        3.2702  0.0080\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.4430\u001b[0m  0.0101\n",
      "      2        3.4430  0.0141\n",
      "      3        3.4430  0.0201\n",
      "      4        3.4430  0.0191\n",
      "      5        3.4430  0.0154\n",
      "      6        3.4430  0.0147\n",
      "      7        3.4430  0.0213\n",
      "      8        3.4430  0.0149\n",
      "      9        3.4430  0.0152\n",
      "     10        3.4430  0.0155\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "Restoring best model from epoch 1.\n",
      "max round 2.09\n",
      "integration_values.shape[0] 7973\n",
      "Concordance Index 0.651010376843255\n",
      "Integrated Brier Score: 0.651010376843255\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.0491\u001b[0m  0.0058\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2        3.0491  0.0195\n",
      "      3        3.0491  0.0077\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.1461\u001b[0m  0.0097\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      2        3.1461  0.0071\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      4        3.0491  0.0122\n",
      "      3        3.1461  0.0066\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.4627\u001b[0m  0.0073\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      5        3.0491  0.0068\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      4        3.1461  0.0088\n",
      "      6        3.0491  0.0057\n",
      "      2        3.4627  0.0068\n",
      "      7        3.0491  0.0075\n",
      "      3        3.4627  0.0081\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.2399\u001b[0m  0.0103\n",
      "      5        3.1461  0.0108\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.3311\u001b[0m  0.0085\n",
      "      8        3.0491  0.0067\n",
      "      4        3.4627  0.0091\n",
      "      2        3.2399  0.0100\n",
      "      6        3.1461  0.0093\n",
      "      2        3.3311  0.0109\n",
      "      9        3.0491  0.0064\n",
      "      5        3.4627  0.0074\n",
      "      7        3.1461  0.0068\n",
      "      3        3.3311  0.0067\n",
      "     10        3.0491  0.0065\n",
      "      3        3.2399  0.0147\n",
      "      8        3.1461  0.0079\n",
      "      6        3.4627  0.0120\n",
      "      9        3.1461  0.0083\n",
      "      4        3.2399  0.0093\n",
      "      4        3.3311  0.0158\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "Restoring best model from epoch 1.\n",
      "      7        3.4627  0.0116\n",
      "      5        3.2399  0.0087\n",
      "      5        3.3311  0.0113\n",
      "     10        3.1461  0.0135\n",
      "      6        3.2399  0.0092\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "Restoring best model from epoch 1.\n",
      "      6        3.3311  0.0095\n",
      "      8        3.4627  0.0167\n",
      "      7        3.2399  0.0062\n",
      "      7        3.3311  0.0057\n",
      "      8        3.2399  0.0053\n",
      "      8        3.3311  0.0050\n",
      "      9        3.2399  0.0091\n",
      "      9        3.4627  0.0192\n",
      "      9        3.3311  0.0099\n",
      "     10        3.2399  0.0064\n",
      "     10        3.3311  0.0053\n",
      "     10        3.4627  0.0091\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "Restoring best model from epoch 1.\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "Restoring best model from epoch 1.\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.3956\u001b[0m  0.0128\n",
      "      2        3.3956  0.0110\n",
      "      3        3.3956  0.0100\n",
      "      4        3.3956  0.0201\n",
      "      5        3.3956  0.0176\n",
      "      6        3.3956  0.0350\n",
      "      7        3.3956  0.0181\n",
      "      8        3.3956  0.0105\n",
      "      9        3.3956  0.0144\n",
      "     10        3.3956  0.0107\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "Restoring best model from epoch 1.\n",
      "max round 1.9\n",
      "integration_values.shape[0] 9587\n",
      "Concordance Index 0.504778972520908\n",
      "Integrated Brier Score: 0.504778972520908\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.1989\u001b[0m  0.0101\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.0732\u001b[0m  0.0142\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2        3.1989  0.0125\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.2874\u001b[0m  0.0085\n",
      "      2        3.0732  0.0122\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.2209\u001b[0m  0.0094\n",
      "      2        3.2874  0.0060\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.4558\u001b[0m  0.0087\n",
      "      3        3.0732  0.0090\n",
      "      2        3.2209  0.0086\n",
      "      3        3.1989  0.0143\n",
      "      3        3.2874  0.0083\n",
      "      2        3.4558  0.0089\n",
      "      4        3.0732  0.0072\n",
      "      4        3.1989  0.0069\n",
      "      4        3.2874  0.0077\n",
      "      3        3.4558  0.0076\n",
      "      3        3.2209  0.0123\n",
      "      5        3.1989  0.0072\n",
      "      5        3.0732  0.0120\n",
      "      4        3.4558  0.0072\n",
      "      6        3.1989  0.0074\n",
      "      6        3.0732  0.0062\n",
      "      4        3.2209  0.0113\n",
      "      5        3.4558  0.0066\n",
      "      5        3.2874  0.0202\n",
      "      5        3.2209  0.0109\n",
      "      7        3.0732  0.0155\n",
      "      6        3.4558  0.0137\n",
      "      6        3.2209  0.0064\n",
      "      7        3.1989  0.0232\n",
      "      7        3.4558  0.0124\n",
      "      8        3.0732  0.0067\n",
      "      6        3.2874  0.0264\n",
      "      7        3.2209  0.0160\n",
      "      8        3.1989  0.0143\n",
      "      9        3.0732  0.0080\n",
      "      9        3.1989  0.0080\n",
      "     10        3.0732  0.0074\n",
      "      7        3.2874  0.0129\n",
      "      8        3.2874  0.0060\n",
      "      8        3.4558  0.0284\n",
      "      8        3.2209  0.0232\n",
      "      9        3.2874  0.0090\n",
      "      9        3.4558  0.0075\n",
      "     10        3.1989  0.0198\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "Restoring best model from epoch 1.\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "Restoring best model from epoch 1.\n",
      "      9        3.2209  0.0160\n",
      "     10        3.2874  0.0142\n",
      "     10        3.4558  0.0188\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "Restoring best model from epoch 1.\n",
      "     10        3.2209  0.0182\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "Restoring best model from epoch 1.\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.3943\u001b[0m  0.0089\n",
      "      2        3.3943  0.0094\n",
      "      3        3.3943  0.0090\n",
      "      4        3.3943  0.0093\n",
      "      5        3.3943  0.0083\n",
      "      6        3.3943  0.0090\n",
      "      7        3.3943  0.0110\n",
      "      8        3.3943  0.0088\n",
      "      9        3.3943  0.0089\n",
      "     10        3.3943  0.0063\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "Restoring best model from epoch 1.\n",
      "max round 6.02\n",
      "integration_values.shape[0] 30324\n",
      "Concordance Index 0.4616265750286369\n",
      "Integrated Brier Score: 0.4616265750286369\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m2.8404\u001b[0m  0.0067\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      2        2.8404  0.0057\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.3096\u001b[0m  0.0060\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.2151\u001b[0m  0.0084\n",
      "      2        3.3096  0.0078\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m2.9437\u001b[0m  0.0069\n",
      "      2        3.2151  0.0066\n",
      "      3        3.3096  0.0061\n",
      "      3        2.8404  0.0215\n",
      "      3        3.2151  0.0068\n",
      "      4        3.3096  0.0093\n",
      "      4        2.8404  0.0070\n",
      "      2        2.9437  0.0134\n",
      "      4        3.2151  0.0070\n",
      "      5        3.3096  0.0071\n",
      "      5        3.2151  0.0069\n",
      "      3        2.9437  0.0091\n",
      "      5        2.8404  0.0104\n",
      "      6        3.3096  0.0072\n",
      "      4        2.9437  0.0060\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "      6        3.2151  0.0066\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "      7        3.3096  0.0073\n",
      "      6        2.8404  0.0105\n",
      "      5        2.9437  0.0058\n",
      "      8        3.3096  0.0076\n",
      "      7        3.2151  0.0124\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.4097\u001b[0m  0.0091\n",
      "      7        2.8404  0.0107\n",
      "      6        2.9437  0.0111\n",
      "      8        3.2151  0.0076\n",
      "      8        2.8404  0.0070\n",
      "      2        3.4097  0.0086\n",
      "      7        2.9437  0.0076\n",
      "      9        3.3096  0.0142\n",
      "      9        2.8404  0.0071\n",
      "      3        3.4097  0.0071\n",
      "      8        2.9437  0.0067\n",
      "     10        3.3096  0.0073\n",
      "      9        3.2151  0.0142\n",
      "     10        2.8404  0.0068\n",
      "      9        2.9437  0.0070\n",
      "      4        3.4097  0.0077\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "Restoring best model from epoch 1.\n",
      "     10        3.2151  0.0102\n",
      "      5        3.4097  0.0074\n",
      "     10        2.9437  0.0110\n",
      "      6        3.4097  0.0097\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "Restoring best model from epoch 1.\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "Restoring best model from epoch 1.\n",
      "      7        3.4097  0.0080\n",
      "      8        3.4097  0.0045\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "Restoring best model from epoch 1.\n",
      "      9        3.4097  0.0056\n",
      "     10        3.4097  0.0048\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "Restoring best model from epoch 1.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    valid_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.3268\u001b[0m  0.0065\n",
      "      2        3.3268  0.0153\n",
      "      3        3.3268  0.0177\n",
      "      4        3.3268  0.0158\n",
      "      5        3.3268  0.0176\n",
      "      6        3.3268  0.0401\n",
      "      7        3.3268  0.0243\n",
      "      8        3.3268  0.0219\n",
      "      9        3.3268  0.0248\n",
      "     10        3.3268  0.0236\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "Restoring best model from epoch 1.\n",
      "max round 2.95\n",
      "integration_values.shape[0] 10113\n",
      "Concordance Index 0.5824175824175825\n",
      "Integrated Brier Score: 0.5824175824175825\n",
      "split gex_?|100130426      float32\n",
      "gex_?|100133144      float32\n",
      "gex_?|100134869      float32\n",
      "gex_?|10357          float32\n",
      "gex_?|10431          float32\n",
      "                      ...   \n",
      "gex_ZYG11A|440590    float32\n",
      "gex_ZYG11B|79699     float32\n",
      "gex_ZYX|7791         float32\n",
      "gex_ZZEF1|23140      float32\n",
      "gex_ZZZ3|26009       float32\n",
      "Length: 20531, dtype: object\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m1.4046\u001b[0m        \u001b[32m1.6590\u001b[0m  0.1885\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m1.2195\u001b[0m        \u001b[32m1.4677\u001b[0m  0.1938\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m1.3734\u001b[0m        \u001b[32m1.4134\u001b[0m  0.1854\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m1.3339\u001b[0m        \u001b[32m2.2407\u001b[0m  0.2032\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m1.0568\u001b[0m        \u001b[32m1.2910\u001b[0m  0.2113\n",
      "      2        \u001b[36m1.3391\u001b[0m        \u001b[32m1.6032\u001b[0m  0.1719\n",
      "      2        \u001b[36m1.1834\u001b[0m        \u001b[32m1.3840\u001b[0m  0.1623\n",
      "      2        \u001b[36m1.3012\u001b[0m        1.4194  0.1838\n",
      "      2        \u001b[36m1.3260\u001b[0m        \u001b[32m1.5287\u001b[0m  0.1801\n",
      "      2        \u001b[36m1.0568\u001b[0m        \u001b[32m1.0559\u001b[0m  0.1898\n",
      "      3        \u001b[36m1.2400\u001b[0m        \u001b[32m1.5770\u001b[0m  0.1961\n",
      "      3        \u001b[36m1.2135\u001b[0m        1.4204  0.1739\n",
      "      3        \u001b[36m1.2234\u001b[0m        \u001b[32m1.4790\u001b[0m  0.1769\n",
      "      3        \u001b[36m1.0886\u001b[0m        \u001b[32m1.2444\u001b[0m  0.2111\n",
      "      3        \u001b[36m0.9230\u001b[0m        \u001b[32m1.0293\u001b[0m  0.1859\n",
      "      4        \u001b[36m1.1867\u001b[0m        \u001b[32m1.4014\u001b[0m  0.1880\n",
      "      4        \u001b[36m1.2171\u001b[0m        \u001b[32m1.5430\u001b[0m  0.1988\n",
      "      4        \u001b[36m1.1927\u001b[0m        \u001b[32m1.3989\u001b[0m  0.1865\n",
      "      4        \u001b[36m1.0755\u001b[0m        \u001b[32m1.2081\u001b[0m  0.2036\n",
      "      4        \u001b[36m0.9007\u001b[0m        \u001b[32m1.0102\u001b[0m  0.2048\n",
      "      5        \u001b[36m1.1638\u001b[0m        \u001b[32m1.3825\u001b[0m  0.1636\n",
      "      5        \u001b[36m1.1749\u001b[0m        \u001b[32m1.3382\u001b[0m  0.1726\n",
      "      5        \u001b[36m1.1894\u001b[0m        \u001b[32m1.5068\u001b[0m  0.1944\n",
      "      5        \u001b[36m1.0597\u001b[0m        \u001b[32m1.1960\u001b[0m  0.1714\n",
      "      5        \u001b[36m0.8880\u001b[0m        \u001b[32m1.0000\u001b[0m  0.1824\n",
      "      6        \u001b[36m1.1679\u001b[0m        \u001b[32m1.4799\u001b[0m  0.1442\n",
      "      6        \u001b[36m1.1396\u001b[0m        \u001b[32m1.3732\u001b[0m  0.1817\n",
      "      6        \u001b[36m1.1558\u001b[0m        \u001b[32m1.3112\u001b[0m  0.1648\n",
      "      6        \u001b[36m1.0428\u001b[0m        \u001b[32m1.1905\u001b[0m  0.1842\n",
      "      6        \u001b[36m0.8776\u001b[0m        \u001b[32m0.9980\u001b[0m  0.1763\n",
      "      7        \u001b[36m1.1138\u001b[0m        \u001b[32m1.3662\u001b[0m  0.1704\n",
      "      7        \u001b[36m1.1520\u001b[0m        \u001b[32m1.4724\u001b[0m  0.1855\n",
      "      7        \u001b[36m1.1304\u001b[0m        \u001b[32m1.3051\u001b[0m  0.1869\n",
      "      7        \u001b[36m1.0306\u001b[0m        \u001b[32m1.1865\u001b[0m  0.1730\n",
      "      7        \u001b[36m0.8580\u001b[0m        \u001b[32m0.9978\u001b[0m  0.2131\n",
      "      8        1.1144        \u001b[32m1.3662\u001b[0m  0.2117\n",
      "      8        \u001b[36m1.1354\u001b[0m        \u001b[32m1.4717\u001b[0m  0.1986\n",
      "      8        \u001b[36m1.1255\u001b[0m        \u001b[32m1.3045\u001b[0m  0.2318\n",
      "      8        \u001b[36m1.0126\u001b[0m        \u001b[32m1.1831\u001b[0m  0.2166\n",
      "      8        \u001b[36m0.8488\u001b[0m        0.9980  0.2219\n",
      "      9        \u001b[36m1.0945\u001b[0m        \u001b[32m1.3657\u001b[0m  0.2124\n",
      "      9        \u001b[36m1.1197\u001b[0m        \u001b[32m1.4680\u001b[0m  0.2180\n",
      "      9        \u001b[36m1.1232\u001b[0m        1.3049  0.2170\n",
      "      9        \u001b[36m1.0007\u001b[0m        \u001b[32m1.1805\u001b[0m  0.2231\n",
      "      9        \u001b[36m0.8348\u001b[0m        \u001b[32m0.9929\u001b[0m  0.1910\n",
      "     10        \u001b[36m1.0702\u001b[0m        \u001b[32m1.3647\u001b[0m  0.2194\n",
      "     10        \u001b[36m1.1056\u001b[0m        \u001b[32m1.4587\u001b[0m  0.2085\n",
      "     10        \u001b[36m1.1069\u001b[0m        \u001b[32m1.3037\u001b[0m  0.2019\n",
      "     10        \u001b[36m0.9942\u001b[0m        \u001b[32m1.1787\u001b[0m  0.1945\n",
      "     10        \u001b[36m0.8178\u001b[0m        \u001b[32m0.9870\u001b[0m  0.2061\n",
      "     11        \u001b[36m1.0682\u001b[0m        \u001b[32m1.3626\u001b[0m  0.1950\n",
      "     11        \u001b[36m1.0916\u001b[0m        \u001b[32m1.4500\u001b[0m  0.1925\n",
      "     11        \u001b[36m0.9832\u001b[0m        \u001b[32m1.1772\u001b[0m  0.1636\n",
      "     11        \u001b[36m1.0997\u001b[0m        \u001b[32m1.3021\u001b[0m  0.1903\n",
      "     11        \u001b[36m0.8159\u001b[0m        \u001b[32m0.9835\u001b[0m  0.1610\n",
      "     12        0.9834        \u001b[32m1.1765\u001b[0m  0.1559\n",
      "     12        \u001b[36m1.0816\u001b[0m        \u001b[32m1.4460\u001b[0m  0.1769\n",
      "     12        1.0776        \u001b[32m1.3603\u001b[0m  0.1937\n",
      "     12        \u001b[36m1.0910\u001b[0m        \u001b[32m1.3010\u001b[0m  0.1828\n",
      "     12        \u001b[36m0.8018\u001b[0m        \u001b[32m0.9826\u001b[0m  0.1852\n",
      "     13        \u001b[36m1.0688\u001b[0m        \u001b[32m1.4361\u001b[0m  0.1763\n",
      "     13        \u001b[36m0.9750\u001b[0m        \u001b[32m1.1742\u001b[0m  0.1930\n",
      "     13        \u001b[36m1.0649\u001b[0m        \u001b[32m1.3596\u001b[0m  0.1951\n",
      "     13        \u001b[36m1.0858\u001b[0m        \u001b[32m1.3006\u001b[0m  0.1889\n",
      "     13        0.8048        0.9830  0.1984\n",
      "     14        \u001b[36m1.0687\u001b[0m        \u001b[32m1.4288\u001b[0m  0.1861\n",
      "     14        \u001b[36m1.0450\u001b[0m        \u001b[32m1.3568\u001b[0m  0.1580\n",
      "     14        \u001b[36m0.9710\u001b[0m        \u001b[32m1.1723\u001b[0m  0.1846\n",
      "     14        \u001b[36m1.0827\u001b[0m        \u001b[32m1.3003\u001b[0m  0.2041\n",
      "     14        \u001b[36m0.7995\u001b[0m        0.9852  0.2118\n",
      "     15        1.0739        \u001b[32m1.4231\u001b[0m  0.1842     15        1.0531        1.3578  0.1832\n",
      "\n",
      "     15        0.9715        \u001b[32m1.1703\u001b[0m  0.2017\n",
      "     15        \u001b[36m1.0719\u001b[0m        1.3004  0.1626\n",
      "     15        \u001b[36m0.7871\u001b[0m        0.9885  0.1735\n",
      "     16        \u001b[36m1.0577\u001b[0m        \u001b[32m1.4157\u001b[0m  0.1548\n",
      "     16        \u001b[36m1.0436\u001b[0m        1.3597  0.1660\n",
      "     16        \u001b[36m0.9584\u001b[0m        \u001b[32m1.1691\u001b[0m  0.1770\n",
      "     16        \u001b[36m1.0684\u001b[0m        1.3012  0.1774\n",
      "     16        \u001b[36m0.7787\u001b[0m        0.9911  0.1938\n",
      "     17        1.0458        1.3594  0.1692\n",
      "     17        \u001b[36m1.0485\u001b[0m        \u001b[32m1.4117\u001b[0m  0.1983\n",
      "     17        0.9606        \u001b[32m1.1675\u001b[0m  0.1795\n",
      "     17        \u001b[36m1.0605\u001b[0m        1.3015  0.2018\n",
      "     17        \u001b[36m0.7741\u001b[0m        0.9931  0.1954\n",
      "     18        \u001b[36m1.0434\u001b[0m        1.3606  0.2125\n",
      "     18        1.0668        \u001b[32m1.4102\u001b[0m  0.2089\n",
      "     18        \u001b[36m0.9528\u001b[0m        \u001b[32m1.1653\u001b[0m  0.2146\n",
      "     18        \u001b[36m1.0430\u001b[0m        1.3025  0.2333\n",
      "     18        \u001b[36m0.7649\u001b[0m        0.9952  0.2210\n",
      "     19        1.0447        1.3622  0.2110\n",
      "     19        1.0571        \u001b[32m1.4056\u001b[0m  0.1895\n",
      "     19        0.9571        \u001b[32m1.1635\u001b[0m  0.1934\n",
      "     19        1.0438        1.3039  0.1700\n",
      "     19        0.7678        0.9985  0.1871\n",
      "     20        \u001b[36m1.0419\u001b[0m        \u001b[32m1.4026\u001b[0m  0.1699\n",
      "     20        \u001b[36m1.0394\u001b[0m        1.3674  0.1816\n",
      "     20        \u001b[36m0.9475\u001b[0m        \u001b[32m1.1626\u001b[0m  0.2105\n",
      "     20        \u001b[36m1.0390\u001b[0m        1.3059  0.2096\n",
      "     20        \u001b[36m0.7563\u001b[0m        1.0015  0.1955\n",
      "     21        \u001b[36m1.0279\u001b[0m        \u001b[32m1.4014\u001b[0m  0.1963\n",
      "     21        1.0406        1.3741  0.1905\n",
      "     21        \u001b[36m0.9350\u001b[0m        \u001b[32m1.1622\u001b[0m  0.1857\n",
      "     21        \u001b[36m1.0288\u001b[0m        1.3082  0.1860\n",
      "     21        0.7600        1.0037  0.1620\n",
      "     22        1.0339        1.4017  0.1643\n",
      "     22        \u001b[36m1.0347\u001b[0m        1.3798  0.1748\n",
      "     22        0.9417        1.1623  0.1733\n",
      "     22        \u001b[36m1.0240\u001b[0m        1.3106  0.1878\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "Restoring best model from epoch 12.\n",
      "     23        1.0402        1.3852  0.1775\n",
      "     23        1.0355        \u001b[32m1.4012\u001b[0m  0.1951\n",
      "     23        \u001b[36m0.9288\u001b[0m        1.1622  0.1801\n",
      "     23        1.0303        1.3131  0.1552\n",
      "     24        1.0307        \u001b[32m1.4003\u001b[0m  0.1569\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "Restoring best model from epoch 14.\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "Restoring best model from epoch 14.\n",
      "     24        \u001b[36m0.9217\u001b[0m        1.1622  0.1729\n",
      "     25        1.0341        1.4005  0.1556\n",
      "     25        0.9224        1.1624  0.1361\n",
      "     26        1.0323        1.4003  0.1135\n",
      "     26        0.9229        1.1628  0.1134\n",
      "     27        \u001b[36m1.0157\u001b[0m        1.4005  0.1124\n",
      "     27        \u001b[36m0.9024\u001b[0m        1.1637  0.1414\n",
      "     28        1.0242        \u001b[32m1.4002\u001b[0m  0.1190\n",
      "     28        0.9157        1.1641  0.1177\n",
      "     29        1.0280        \u001b[32m1.3991\u001b[0m  0.1244\n",
      "     29        0.9103        1.1648  0.1276\n",
      "     30        1.0197        \u001b[32m1.3978\u001b[0m  0.1141\n",
      "     30        0.9041        1.1648  0.1386\n",
      "     31        1.0226        \u001b[32m1.3975\u001b[0m  0.1291\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "Restoring best model from epoch 21.\n",
      "     32        1.0178        1.3977  0.1183\n",
      "     33        \u001b[36m1.0139\u001b[0m        1.3983  0.1124\n",
      "     34        1.0266        1.3976  0.1012\n",
      "     35        1.0205        \u001b[32m1.3953\u001b[0m  0.0923\n",
      "     36        1.0284        1.3963  0.1000\n",
      "     37        1.0185        1.3979  0.1314\n",
      "     38        1.0200        1.3978  0.0994\n",
      "     39        1.0304        1.3966  0.0940\n",
      "     40        \u001b[36m1.0048\u001b[0m        \u001b[32m1.3949\u001b[0m  0.1275\n",
      "     41        1.0160        \u001b[32m1.3940\u001b[0m  0.1028\n",
      "     42        1.0235        \u001b[32m1.3915\u001b[0m  0.1211\n",
      "     43        1.0181        \u001b[32m1.3896\u001b[0m  0.1156\n",
      "     44        1.0070        \u001b[32m1.3886\u001b[0m  0.1226\n",
      "     45        1.0163        \u001b[32m1.3885\u001b[0m  0.1387\n",
      "     46        1.0235        1.3899  0.1146\n",
      "     47        1.0132        1.3922  0.0836\n",
      "     48        1.0124        1.3950  0.1006\n",
      "     49        1.0175        1.3971  0.1173\n",
      "     50        1.0093        1.3979  0.1094\n",
      "     51        1.0128        1.3976  0.1408\n",
      "     52        1.0066        1.3964  0.1087\n",
      "     53        1.0066        1.3958  0.0927\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "Restoring best model from epoch 44.\n",
      "Re-initializing module because the following parameters were re-set: module__input_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m1.2280\u001b[0m        \u001b[32m1.5486\u001b[0m  0.1128\n",
      "      2        \u001b[36m1.1388\u001b[0m        \u001b[32m1.4246\u001b[0m  0.1132\n",
      "      3        \u001b[36m1.0620\u001b[0m        \u001b[32m1.4045\u001b[0m  0.1365\n",
      "      4        \u001b[36m1.0363\u001b[0m        \u001b[32m1.4038\u001b[0m  0.1408\n",
      "      5        \u001b[36m1.0068\u001b[0m        \u001b[32m1.3905\u001b[0m  0.1531\n",
      "      6        \u001b[36m0.9959\u001b[0m        \u001b[32m1.3804\u001b[0m  0.1450\n",
      "      7        \u001b[36m0.9686\u001b[0m        \u001b[32m1.3706\u001b[0m  0.1980\n",
      "      8        \u001b[36m0.9481\u001b[0m        \u001b[32m1.3638\u001b[0m  0.1255\n",
      "      9        0.9531        \u001b[32m1.3554\u001b[0m  0.1110\n",
      "     10        0.9485        \u001b[32m1.3476\u001b[0m  0.1806\n",
      "     11        \u001b[36m0.9258\u001b[0m        \u001b[32m1.3373\u001b[0m  0.1212\n",
      "     12        \u001b[36m0.9198\u001b[0m        \u001b[32m1.3303\u001b[0m  0.1401\n",
      "     13        \u001b[36m0.9089\u001b[0m        \u001b[32m1.3251\u001b[0m  0.1245\n",
      "     14        0.9127        \u001b[32m1.3199\u001b[0m  0.0970\n",
      "     15        0.9158        \u001b[32m1.3167\u001b[0m  0.1452\n",
      "     16        \u001b[36m0.8944\u001b[0m        \u001b[32m1.3152\u001b[0m  0.1247\n",
      "     17        0.9025        \u001b[32m1.3136\u001b[0m  0.1225\n",
      "     18        0.8962        \u001b[32m1.3127\u001b[0m  0.1398\n",
      "     19        0.8950        \u001b[32m1.3112\u001b[0m  0.1321\n",
      "     20        \u001b[36m0.8867\u001b[0m        \u001b[32m1.3099\u001b[0m  0.1526\n",
      "     21        \u001b[36m0.8780\u001b[0m        \u001b[32m1.3084\u001b[0m  0.1320\n",
      "     22        0.8824        \u001b[32m1.3062\u001b[0m  0.1423\n",
      "     23        0.8809        \u001b[32m1.3039\u001b[0m  0.1461\n",
      "     24        \u001b[36m0.8744\u001b[0m        \u001b[32m1.3028\u001b[0m  0.2139\n",
      "     25        0.8788        \u001b[32m1.3023\u001b[0m  0.0958\n",
      "     26        \u001b[36m0.8673\u001b[0m        \u001b[32m1.3022\u001b[0m  0.1201\n",
      "     27        \u001b[36m0.8672\u001b[0m        1.3024  0.0954\n",
      "     28        0.8704        1.3022  0.1293\n",
      "     29        0.8752        \u001b[32m1.3017\u001b[0m  0.1190\n",
      "     30        \u001b[36m0.8658\u001b[0m        \u001b[32m1.3007\u001b[0m  0.2632\n",
      "     31        0.8709        \u001b[32m1.2994\u001b[0m  0.1125\n",
      "     32        0.8663        \u001b[32m1.2989\u001b[0m  0.1042\n",
      "     33        0.8748        1.2993  0.1187\n",
      "     34        0.8705        1.3013  0.1240\n",
      "     35        0.8715        1.3033  0.1097\n",
      "     36        0.8679        1.3051  0.0979\n",
      "     37        \u001b[36m0.8592\u001b[0m        1.3062  0.2085\n",
      "     38        0.8609        1.3075  0.2397\n",
      "     39        0.8697        1.3096  0.1015\n",
      "     40        0.8683        1.3089  0.1533\n",
      "     41        0.8628        1.3088  0.1004\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "Restoring best model from epoch 32.\n",
      "max round 116.59\n",
      "integration_values.shape[0] 1003220\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 67\u001b[0m\n\u001b[1;32m     24\u001b[0m X, y \u001b[39m=\u001b[39m sort_X_y_pandas(X, y)\n\u001b[1;32m     26\u001b[0m net \u001b[39m=\u001b[39m NeuralNet(\n\u001b[1;32m     27\u001b[0m     SurvivalModel, \n\u001b[1;32m     28\u001b[0m     module__n_layers \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     65\u001b[0m     verbose\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m     66\u001b[0m )\n\u001b[0;32m---> 67\u001b[0m df_agg_metrics_cindex, df_agg_metrics_ibs, best_model,params, outer_scores, best_preds_train, best_preds_test \u001b[39m=\u001b[39m train_eval(X, y, net, n_iter, data\u001b[39m.\u001b[39;49mfilename)\n\u001b[1;32m     68\u001b[0m agg_metrics_cindex\u001b[39m.\u001b[39mappend(df_agg_metrics_cindex)\n\u001b[1;32m     69\u001b[0m agg_metrics_ibs\u001b[39m.\u001b[39mappend(df_agg_metrics_ibs)\n",
      "Cell \u001b[0;32mIn[19], line 54\u001b[0m, in \u001b[0;36mtrain_eval\u001b[0;34m(X, y, net, n_iter, filename)\u001b[0m\n\u001b[1;32m     50\u001b[0m best_model[\u001b[39m'\u001b[39m\u001b[39mbest_model_\u001b[39m\u001b[39m'\u001b[39m\u001b[39m+\u001b[39mdataset_name] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m [params]\n\u001b[1;32m     53\u001b[0m \u001b[39m#try:\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m cum_hazard_test \u001b[39m=\u001b[39m get_cumulative_hazard_function_eh(\n\u001b[1;32m     55\u001b[0m         X_train\u001b[39m.\u001b[39;49mvalues, X_test\u001b[39m.\u001b[39;49mvalues, y_train\u001b[39m.\u001b[39;49mvalues, y_test\u001b[39m.\u001b[39;49mvalues,\n\u001b[1;32m     56\u001b[0m         best_preds_train, best_preds_test\n\u001b[1;32m     57\u001b[0m         )\n\u001b[1;32m     58\u001b[0m df_survival_test \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mexp(\u001b[39m-\u001b[39mcum_hazard_test)\n\u001b[1;32m     59\u001b[0m durations_test, events_test \u001b[39m=\u001b[39m transform_back(y_test\u001b[39m.\u001b[39mvalues)\n",
      "File \u001b[0;32m~/miniconda3/envs/xgbsurv/lib/python3.11/site-packages/xgbsurv/models/eh_final.py:700\u001b[0m, in \u001b[0;36mget_cumulative_hazard_function_eh\u001b[0;34m(X_train, X_test, y_train, y_test, predictor_train, predictor_test)\u001b[0m\n\u001b[1;32m    696\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mintegration_values.shape[0]\u001b[39m\u001b[39m'\u001b[39m,integration_values\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m])\n\u001b[1;32m    697\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, integration_values\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]):\n\u001b[1;32m    698\u001b[0m     integration_values[_] \u001b[39m=\u001b[39m (\n\u001b[1;32m    699\u001b[0m         integration_values[_ \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m]\n\u001b[0;32m--> 700\u001b[0m         \u001b[39m+\u001b[39m quadrature(\n\u001b[1;32m    701\u001b[0m             func\u001b[39m=\u001b[39;49mhazard_function_integrate,\n\u001b[1;32m    702\u001b[0m             a\u001b[39m=\u001b[39;49mintegration_times[_ \u001b[39m-\u001b[39;49m \u001b[39m1\u001b[39;49m],\n\u001b[1;32m    703\u001b[0m             b\u001b[39m=\u001b[39;49mintegration_times[_],\n\u001b[1;32m    704\u001b[0m             vec_func\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    705\u001b[0m         )[\u001b[39m0\u001b[39m]\n\u001b[1;32m    706\u001b[0m     )\n\u001b[1;32m    708\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_samples):\n\u001b[1;32m    709\u001b[0m     cumulative_hazard[_] \u001b[39m=\u001b[39m (\n\u001b[1;32m    710\u001b[0m         integration_values[\n\u001b[1;32m    711\u001b[0m             np\u001b[39m.\u001b[39mdigitize(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    717\u001b[0m         \u001b[39m/\u001b[39m theta[_, \u001b[39m0\u001b[39m]\n\u001b[1;32m    718\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/xgbsurv/lib/python3.11/site-packages/scipy/integrate/_quadrature.py:365\u001b[0m, in \u001b[0;36mquadrature\u001b[0;34m(func, a, b, args, tol, rtol, maxiter, vec_func, miniter)\u001b[0m\n\u001b[1;32m    363\u001b[0m maxiter \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(miniter\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m, maxiter)\n\u001b[1;32m    364\u001b[0m \u001b[39mfor\u001b[39;00m n \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(miniter, maxiter\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m--> 365\u001b[0m     newval \u001b[39m=\u001b[39m fixed_quad(vfunc, a, b, (), n)[\u001b[39m0\u001b[39m]\n\u001b[1;32m    366\u001b[0m     err \u001b[39m=\u001b[39m \u001b[39mabs\u001b[39m(newval\u001b[39m-\u001b[39mval)\n\u001b[1;32m    367\u001b[0m     val \u001b[39m=\u001b[39m newval\n",
      "File \u001b[0;32m~/miniconda3/envs/xgbsurv/lib/python3.11/site-packages/scipy/integrate/_quadrature.py:245\u001b[0m, in \u001b[0;36mfixed_quad\u001b[0;34m(func, a, b, args, n)\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mGaussian quadrature is only available for \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    243\u001b[0m                      \u001b[39m\"\u001b[39m\u001b[39mfinite limits.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    244\u001b[0m y \u001b[39m=\u001b[39m (b\u001b[39m-\u001b[39ma)\u001b[39m*\u001b[39m(x\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m/\u001b[39m\u001b[39m2.0\u001b[39m \u001b[39m+\u001b[39m a\n\u001b[0;32m--> 245\u001b[0m \u001b[39mreturn\u001b[39;00m (b\u001b[39m-\u001b[39ma)\u001b[39m/\u001b[39m\u001b[39m2.0\u001b[39m \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39msum(w\u001b[39m*\u001b[39mfunc(y, \u001b[39m*\u001b[39;49margs), axis\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m), \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/xgbsurv/lib/python3.11/site-packages/scipy/integrate/_quadrature.py:282\u001b[0m, in \u001b[0;36mvectorize1.<locals>.vfunc\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    280\u001b[0m x \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(x)\n\u001b[1;32m    281\u001b[0m \u001b[39m# call with first point to get output type\u001b[39;00m\n\u001b[0;32m--> 282\u001b[0m y0 \u001b[39m=\u001b[39m func(x[\u001b[39m0\u001b[39;49m], \u001b[39m*\u001b[39;49margs)\n\u001b[1;32m    283\u001b[0m n \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(x)\n\u001b[1;32m    284\u001b[0m dtype \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(y0, \u001b[39m'\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mtype\u001b[39m(y0))\n",
      "File \u001b[0;32m~/miniconda3/envs/xgbsurv/lib/python3.11/site-packages/xgbsurv/models/eh_final.py:670\u001b[0m, in \u001b[0;36mget_cumulative_hazard_function_eh.<locals>.hazard_function_integrate\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mhazard_function_integrate\u001b[39m(s):\n\u001b[0;32m--> 670\u001b[0m     \u001b[39mreturn\u001b[39;00m baseline_hazard_estimator_eh(\n\u001b[1;32m    671\u001b[0m         time\u001b[39m=\u001b[39;49ms,\n\u001b[1;32m    672\u001b[0m         time_train\u001b[39m=\u001b[39;49mtime_train,\n\u001b[1;32m    673\u001b[0m         event_train\u001b[39m=\u001b[39;49mevent_train,\n\u001b[1;32m    674\u001b[0m         predictor_train\u001b[39m=\u001b[39;49mpredictor_train,\n\u001b[1;32m    675\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cancer_types = [\n",
    "    'BLCA',\n",
    "    'BRCA',\n",
    "    'HNSC',\n",
    "    'KIRC',\n",
    "    'LGG',\n",
    "    'LIHC',\n",
    "    'LUAD',\n",
    "    'LUSC',\n",
    "    'OV',\n",
    "    'STAD']\n",
    "\n",
    "for idx, cancer_type in enumerate(cancer_types):\n",
    "    # get name of current dataset\n",
    "    data = load_tcga(path=\"/Users/JUSC/Documents/xgbsurv/xgbsurv/datasets/data/\", cancer_type=cancer_type, as_frame=True)\n",
    "    X  = data.data #.astype(np.float32)\n",
    "    y = data.target #.values #.to_numpy()\n",
    "\n",
    "    X, y = sort_X_y_pandas(X, y)\n",
    "\n",
    "    net = NeuralNet(\n",
    "        SurvivalModel, \n",
    "        module__n_layers = 1,\n",
    "        module__input_units = X.shape[1],\n",
    "        #module__num_nodes = 32,\n",
    "        #module__dropout = 0.1, # these could also be removed\n",
    "        module__out_features = 2,\n",
    "        # for split sizes when result size = 1\n",
    "        iterator_train__drop_last=True,\n",
    "        #iterator_valid__drop_last=True,\n",
    "        criterion=EHLoss,\n",
    "        optimizer=torch.optim.AdamW,\n",
    "        optimizer__weight_decay = 0.4,\n",
    "        batch_size=32, # separate train and valid->iterator_train__batch_size=128 and iterator_valid__batch_size=128 ?\n",
    "        callbacks=[\n",
    "            (\n",
    "                \"sched\",\n",
    "                LRScheduler(\n",
    "                    torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
    "                    monitor=\"valid_loss\",\n",
    "                    patience=5,\n",
    "                ),\n",
    "            ),\n",
    "            (\n",
    "                \"es\",\n",
    "                EarlyStopping(\n",
    "                    monitor=\"valid_loss\",\n",
    "                    patience=10,\n",
    "                    load_best=True,\n",
    "                ),\n",
    "            ),\n",
    "            (\"seed\", FixSeed(seed=42)),\n",
    "            #(\"Input Shape Setter\",InputShapeSetter())\n",
    "        ],#[EarlyStopping(patience=10)],#,InputShapeSetter()],\n",
    "        #TODO: enable stratification, verify\n",
    "        train_split=CustomValidSplit(0.2, stratified=True, random_state=rand_state), #ValidSplit(0.2), # might cause lower performance in metrics, explain in thesis\n",
    "        #lr=0.001,\n",
    "        #max_epochs=1, #0,#100\n",
    "        #train_split=None,\n",
    "        verbose=0\n",
    "    )\n",
    "    df_agg_metrics_cindex, df_agg_metrics_ibs, best_model,params, outer_scores, best_preds_train, best_preds_test = train_eval(X, y, net, n_iter, data.filename)\n",
    "    agg_metrics_cindex.append(df_agg_metrics_cindex)\n",
    "    agg_metrics_ibs.append(df_agg_metrics_ibs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to create summary from already existing data\n",
    "cancer_types = [\n",
    "    'BLCA',\n",
    "    'BRCA',\n",
    "    'HNSC',\n",
    "    'KIRC',\n",
    "    'LGG',\n",
    "    'LIHC',\n",
    "    'LUAD',\n",
    "    'LUSC',\n",
    "    'OV',\n",
    "    'STAD']\n",
    "\n",
    "for name in cancer_types:\n",
    "    df_outer_scores = pd.read_csv('eh_skorch_results/eh_metric_summary_'+name+'_adapted.csv')\n",
    "    dataset_name = name #+'_adapted'\n",
    "    # cindex\n",
    "    df_agg_metrics_cindex = pd.DataFrame({'dataset':[dataset_name],\n",
    "                                            'cindex_train_mean':df_outer_scores['cindex_train_'+dataset_name].mean(),\n",
    "                                            'cindex_train_std':df_outer_scores['cindex_train_'+dataset_name].std(),\n",
    "                                            'cindex_test_mean':df_outer_scores['cindex_test_'+dataset_name].mean(),\n",
    "                                            'cindex_test_std':df_outer_scores['cindex_test_'+dataset_name].std() })\n",
    "    # IBS\n",
    "    df_agg_metrics_ibs = pd.DataFrame({'dataset':[dataset_name],\n",
    "                                            'ibs_train_mean':df_outer_scores['ibs_train_'+dataset_name].mean(),\n",
    "                                            'ibs_train_std':df_outer_scores['ibs_train_'+dataset_name].std(),\n",
    "                                            'ibs_test_mean':df_outer_scores['ibs_test_'+dataset_name].mean(),\n",
    "                                            'ibs_test_std':df_outer_scores['ibs_test_'+dataset_name].std() })\n",
    "    \n",
    "    agg_metrics_cindex.append(df_agg_metrics_cindex)\n",
    "    agg_metrics_ibs.append(df_agg_metrics_ibs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_eh_tcga_cindex = pd.concat([df for df in agg_metrics_cindex]).round(4)\n",
    "df_final_eh_tcga_cindex.to_csv('metrics/final_gbdt_tcga_eh_cindex.csv', index=False)\n",
    "df_final_eh_tcga_cindex.to_csv('/Users/JUSC/Documents/644928e0fb7e147893e8ec15/05_thesis/tables/final_dl_tcga_eh_cindex.csv', index=False)  #\n",
    "df_final_eh_tcga_cindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_eh_tcga_ibs = pd.concat([df for df in agg_metrics_ibs]).round(4)\n",
    "df_final_eh_tcga_ibs.to_csv('metrics/final_gbdt_tcga_eh_ibs.csv', index=False)\n",
    "df_final_eh_tcga_ibs.to_csv('/Users/JUSC/Documents/644928e0fb7e147893e8ec15/05_thesis/tables/final_dl_tcga_eh_ibs.csv', index=False) \n",
    "df_final_eh_tcga_ibs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xgbsurv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
