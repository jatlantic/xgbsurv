{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/JUSC/Documents/xgbsurv/experiments/deep_learning\n"
     ]
    }
   ],
   "source": [
    "# XGBsurv benchmark\n",
    "\n",
    "#from xgbsurv.datasets import (load_flchain, load_rgbsg, load_support,\n",
    "#load_tcga)\n",
    "from xgbsurv import XGBSurv\n",
    "from xgbsurv.evaluation import cindex_censored, ibs\n",
    "from xgbsurv.models.utils import sort_X_y\n",
    "import os\n",
    "import numpy as np\n",
    "from numpy import savetxt\n",
    "import pandas as pd\n",
    "from scipy.stats import uniform as scuniform\n",
    "from scipy.stats import randint as scrandint\n",
    "from scipy.stats import loguniform as scloguniform \n",
    "from sklearn.model_selection import RandomizedSearchCV, KFold, StratifiedKFold\n",
    "#from sklearn.ensemble import RandomForestClassifier\n",
    "#from sklearn.datasets import load_iris\n",
    "#from sklearn_pandas import DataFrameMapper\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.compose import make_column_selector\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, LabelBinarizer, OneHotEncoder\n",
    "# import models\n",
    "#from xgbsurv.models.breslow_final import breslow_likelihood, breslow_estimator\n",
    "from pycox.evaluation import EvalSurv\n",
    "from xgbsurv.models.utils import transform_back, transform\n",
    "from xgbsurv.preprocessing.dataset_preprocessing import discretizer_df\n",
    "import sys\n",
    "#sys.path.append('/Users/JUSC/Documents/xgbsurv_benchmarking/deep_learning/')\n",
    "from loss_functions_pytorch import BreslowLoss, breslow_likelihood_torch\n",
    "from skorch import NeuralNet\n",
    "from skorch.callbacks import EarlyStopping\n",
    "from skorch.dataset import CVSplit, ValidSplit\n",
    "import torch\n",
    "from torch import nn\n",
    "from xgbsurv.datasets import load_metabric, load_flchain, load_rgbsg, load_support\n",
    "#import torch.nn.init as init"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Parameters\n",
    "deep learning hyperparameter spaces follow pycox paper\n",
    "Time-to-Event Prediction with Neural Networks and Cox Regression\n",
    "page 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters, put into function\n",
    "n_outer_splits = 5\n",
    "n_inner_splits = 5\n",
    "rand_state = 42\n",
    "n_iter = 10 # set to 50\n",
    "#n_iter_cind = 200\n",
    "early_stopping_rounds=15\n",
    "base_score = 0.0\n",
    "\n",
    "param_grid_breslow = {\n",
    "    'estimator__module__n_layers': [1, 2, 4],\n",
    "    'estimator__module__n_layers': [1, 2, 4],\n",
    "    'estimator__module__num_nodes': [64, 128, 256, 512],\n",
    "    'estimator__module__dropout': scuniform(0.0,0.7),\n",
    "    'estimator__optimizer__weight_decay': [0.4, 0.2, 0.1, 0.05, 0.02, 0.01, 0],\n",
    "    'estimator__batch_size': [64, 128, 256, 512, 1024],\n",
    "    #lr not in paper because of learning rate finder\n",
    "    # note: setting learning rate higher would make exp(partial_hazard) explode\n",
    "    'estimator__lr': scloguniform(0.001,0.01), # scheduler unten einbauen\n",
    "    #'max_epochs':  scrandint(10,20), # corresponds to num_rounds\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set MLP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SurvivalModel(nn.Module):\n",
    "    def __init__(self, n_layers, in_features, num_nodes, dropout, out_features):\n",
    "        super(SurvivalModel, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.in_features = in_features\n",
    "        self.num_nodes = num_nodes\n",
    "        self.dropout = dropout\n",
    "        self.out_features = out_features\n",
    "        model = []\n",
    "        # first layer\n",
    "        model.append(torch.nn.Linear(in_features, num_nodes))\n",
    "        model.append(torch.nn.ReLU())\n",
    "        model.append(torch.nn.Dropout(dropout))\n",
    "        model.append(torch.nn.BatchNorm1d(num_nodes))\n",
    "\n",
    "        for i in range(n_layers-1):\n",
    "            model.append(torch.nn.Linear(num_nodes, num_nodes))\n",
    "            #init.kaiming_normal_(model[-1].weight, nonlinearity='relu')\n",
    "            model.append(torch.nn.ReLU())\n",
    "            model.append(torch.nn.Dropout(dropout))\n",
    "            model.append(torch.nn.BatchNorm1d(num_nodes))\n",
    "\n",
    "        # output layer\n",
    "        model.append(torch.nn.Linear(num_nodes, out_features))\n",
    "    \n",
    "        self.layers = nn.Sequential(*model)\n",
    "\n",
    "        # for layer in self.layers:\n",
    "        #     if isinstance(layer, nn.Linear):\n",
    "        #         #nn.init.uniform_(layer.weight, a=-0.5, b=0.5)\n",
    "        #         nn.init.kaiming_normal_(layer.weight)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = self.layers(x)\n",
    "        #print(res)\n",
    "        return res\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Splitting Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define stratified inner k-fold cross-validation\n",
    "class CustomSplit(StratifiedKFold):\n",
    "    def __init__(self, n_splits=5, shuffle=True, random_state=None):\n",
    "        super().__init__(n_splits=n_splits, shuffle=shuffle, random_state=random_state)\n",
    "\n",
    "    def split(self, X, y, groups=None):\n",
    "        print('split', X.dtypes)\n",
    "        try:\n",
    "            if y.shape[1]>1:\n",
    "                y = y[:,0]\n",
    "        except:\n",
    "            pass\n",
    "        bins = np.sign(y)\n",
    "        return super().split(X, bins, groups=groups)\n",
    "\n",
    "    def get_n_splits(self, X=None, y=None, groups=None):\n",
    "        return self.n_splits\n",
    "\n",
    "outer_custom_cv = CustomSplit(n_splits=n_outer_splits, shuffle=True, random_state=rand_state)\n",
    "inner_custom_cv = CustomSplit(n_splits=n_outer_splits, shuffle=True, random_state=rand_state)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Scoring Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Scorer\n",
    "def custom_scoring_function(y_true, y_pred):\n",
    "\n",
    "        #y_true = torch.from_numpy(y_true)\n",
    "        if isinstance(y_pred, np.ndarray):\n",
    "            y_pred = torch.from_numpy(y_pred)\n",
    "        if isinstance(y_true, np.ndarray):\n",
    "            y_true = torch.from_numpy(y_true)\n",
    "        if isinstance(y_pred, pd.Series):\n",
    "            y_pred = torch.tensor(y_pred.values)\n",
    "        if isinstance(y_true, pd.Series):\n",
    "            y_true = torch.tensor(y_true.values)\n",
    "        score = breslow_likelihood_torch(y_true, y_pred) \n",
    "        return score.numpy()\n",
    "\n",
    "scoring_function = make_scorer(custom_scoring_function, greater_is_better=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Data Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.22474487  1.        ]\n",
      " [ 0.          0.        ]\n",
      " [ 1.22474487  1.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.compose import make_column_selector\n",
    "import pandas as pd\n",
    "\n",
    "# Define a custom transformer to convert True and False values to 1 and 0, respectively\n",
    "class BoolToNumericTransformer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        X_copy[X_copy == True] = 1\n",
    "        X_copy[X_copy == False] = 0\n",
    "        return X_copy.astype(float)\n",
    "\n",
    "# Let's say you have a pandas DataFrame with columns of different types:\n",
    "df = pd.DataFrame({\n",
    "    'numeric_feature': [1.0, 2.0, 3.0],\n",
    "    'bool_feature': [True, False, True]\n",
    "})\n",
    "\n",
    "# Define the column transformer using make_column_selector and the custom transformer\n",
    "column_transformer = make_column_transformer(\n",
    "    (StandardScaler(), make_column_selector(dtype_include=['float64'])),\n",
    "    (BoolToNumericTransformer(), make_column_selector(dtype_include=['bool']))\n",
    ")\n",
    "\n",
    "# Apply the transformation to the DataFrame\n",
    "transformed_df = column_transformer.fit_transform(df)\n",
    "\n",
    "print(transformed_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchStandardScaler(StandardScaler):\n",
    "    def transform(self, X, copy=None):\n",
    "        X_scaled = super().transform(X, copy=copy)\n",
    "        return torch.from_numpy(X_scaled)\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        X_transformed = super().fit_transform(X, y=y)\n",
    "        return torch.from_numpy(X_transformed)\n",
    "    \n",
    "ct = make_column_transformer(\n",
    "       (StandardScaler(),\n",
    "        make_column_selector(dtype_include=float)),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['age', 'sex', 'race', 'n_comorbidities', 'diabetes', 'dementia',\n",
      "       'cancer', 'blood_pressure', 'heart_rate', 'respiration_rate',\n",
      "       'temperature', 'white_blood_cell', 'serum_sodium', 'serum_creatinine'],\n",
      "      dtype='object')\n",
      "(8873, 14)\n",
      "age                  float32\n",
      "sex                 category\n",
      "race                 float32\n",
      "n_comorbidities      float32\n",
      "diabetes            category\n",
      "dementia            category\n",
      "cancer              category\n",
      "blood_pressure       float32\n",
      "heart_rate           float32\n",
      "respiration_rate     float32\n",
      "temperature          float32\n",
      "white_blood_cell     float32\n",
      "serum_sodium         float32\n",
      "serum_creatinine     float32\n",
      "dtype: object\n",
      "transformed columns Index(['sex', 'diabetes', 'dementia', 'cancer'], dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-2.4593287 , -0.6470795 ,  1.2134316 , ...,  0.        ,\n",
       "         0.        ,  1.        ],\n",
       "       [ 0.14505765, -0.6470795 , -0.40320182, ...,  1.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.21530288,  0.8430675 ,  1.2134316 , ...,  1.        ,\n",
       "         0.        ,  0.        ],\n",
       "       ...,\n",
       "       [ 1.1050085 , -0.6470795 , -0.40320182, ...,  0.        ,\n",
       "         1.        ,  0.        ],\n",
       "       [ 1.3469174 ,  0.097994  , -0.40320182, ...,  0.        ,\n",
       "         1.        ,  0.        ],\n",
       "       [ 0.2037778 , -0.6470795 ,  1.2134316 , ...,  0.        ,\n",
       "         1.        ,  0.        ]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class BoolToNumericTransformer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        X_copy[X_copy == True] = 1\n",
    "        X_copy[X_copy == False] = 0\n",
    "        return X_copy.astype(np.float32)\n",
    "\n",
    "# adapt this for bool case\n",
    "class CustomStandardScaler(StandardScaler):\n",
    "    \n",
    "    def __init__(self, copy=True, with_mean=True, with_std=True):\n",
    "        super().__init__(copy=copy, with_mean=with_mean, with_std=with_std)\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        # Add your own code here\n",
    "        return super().fit(X, y)\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        # Add your own code here\n",
    "        X_transformed = super().transform(X, y)\n",
    "        # Add your own code here\n",
    "        return X_transformed.astype(np.float32)\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        # Add your own code here\n",
    "        \n",
    "        #to_add = X[X.columns[X.columns.isin(['sex','D'])]]\n",
    "        #X = X[X.columns[~X.columns.isin(['sex','D'])]]\n",
    "        #df.columns.get_loc('age')\n",
    "        X_transformed = super().fit_transform(X, y)\n",
    "        #to_add = to_add.values.reshape(-1, 1)\n",
    "\n",
    "        # Horizontally stack the two arrays\n",
    "        #result = np.hstack((X_transformed, to_add))\n",
    "        \n",
    "        # Add your own code here\n",
    "        return X_transformed.astype(np.float32)\n",
    "\n",
    "class CustomStandardScaler2(StandardScaler):\n",
    "    \"\"\"Just to change the datatype of bool variables.\"\"\"\n",
    "    def __init__(self, copy=True, with_mean=True, with_std=True):\n",
    "        super().__init__(copy=copy, with_mean=with_mean, with_std=with_std)\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        # Add your own code here\n",
    "        return super().fit(X, y)\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        # Add your own code here\n",
    "        X_transformed = super().transform(X, y)\n",
    "        # Add your own code here\n",
    "        return X_transformed.astype(np.float32)\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        # Add your own code here\n",
    "        \n",
    "        to_return = X\n",
    "        #X = X[X.columns[~X.columns.isin(['sex','D'])]]\n",
    "        #df.columns.get_loc('age')\n",
    "        X_transformed = super().fit_transform(X, y)\n",
    "        #to_add = to_add.values.reshape(-1, 1)\n",
    "\n",
    "        # Horizontally stack the two arrays\n",
    "        #result = np.hstack((X_transformed, to_add))\n",
    "        \n",
    "        # Add your own code here\n",
    "        return to_return.astype(np.float32)\n",
    "\n",
    "\n",
    "class CustomOneHotEncoder(OneHotEncoder):\n",
    "    def __init__(self, categories='auto', drop=None, sparse=True,\n",
    "                 dtype=np.float32, handle_unknown='error'):\n",
    "        super().__init__(categories=categories, drop=drop, sparse=sparse,\n",
    "                         dtype=dtype, handle_unknown=handle_unknown)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Add your custom fit logic here\n",
    "        # You can modify or extend the behavior of the base class fit() method\n",
    "        return super().fit(X, y)\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Add your custom transform logic here\n",
    "        # You can modify or extend the behavior of the base class transform() method\n",
    "        return super().transform(X) #.astype(np.float32)\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        \n",
    "        # Add your custom fit_transform logic here\n",
    "        # You can modify or extend the behavior of the base class fit_transform() method\n",
    "        print('transformed columns', X.columns)\n",
    "        to_return = super().fit_transform(X, y)\n",
    "        \n",
    "        return to_return #.astype(np.float32)\n",
    "\n",
    "    def inverse_transform(self, X):\n",
    "        # Add your custom inverse_transform logic here\n",
    "        # You can modify or extend the behavior of the base class inverse_transform() method\n",
    "        return super().inverse_transform(X)\n",
    "\n",
    "    def get_feature_names(self, input_features=None):\n",
    "        # Add your custom get_feature_names logic here\n",
    "        # You can modify or extend the behavior of the base class get_feature_names() method\n",
    "        return super().get_feature_names(input_features)\n",
    "\n",
    "    \n",
    "class CustomLabelBinarizer(LabelBinarizer):\n",
    "    def __init__(self, neg_label=0, pos_label=1, sparse_output=False):\n",
    "        super().__init__(neg_label=0, pos_label=1, sparse_output=False)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Add custom fit logic here\n",
    "        # Call the parent class's fit method\n",
    "        print('fit',X)\n",
    "        super().fit(X, y)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        # Add custom transform logic here\n",
    "\n",
    "        # Call the parent class's transform method\n",
    "        print('transform',X)\n",
    "        transformed_X = super().transform(X)\n",
    "\n",
    "        return transformed_X\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        # Add custom transform logic here\n",
    "\n",
    "        # Call the parent class's transform method\n",
    "        print(y)\n",
    "        print('fit_transform',X.values)\n",
    "        to_add = X[X.columns[X.columns.isin(['sex','D'])]]\n",
    "        X = X[X.columns[~X.columns.isin(['sex','D'])]]\n",
    "        transformed_X = super().fit_transform(X.values)\n",
    "    \n",
    "        return transformed_X.astype(np.float32)\n",
    "\n",
    "    def inverse_transform(self, X, y=None):\n",
    "        # Add custom inverse_transform logic here\n",
    "\n",
    "        # Call the parent class's inverse_transform method\n",
    "        inverse_X = super().inverse_transform(X)\n",
    "\n",
    "        return inverse_X.astype(np.float32)\n",
    "\n",
    "column_names = ['age', 'sex', 'race', 'n_comorbidities', 'diabetes', 'dementia',\n",
    "       'cancer', 'blood_pressure', 'heart_rate', 'respiration_rate',\n",
    "       'temperature', 'white_blood_cell', 'serum_sodium', 'serum_creatinine']\n",
    "def select_columns_by_name(columns):\n",
    "    def selector(X):\n",
    "        return X[:, [column_names.index(col) for col in columns]]\n",
    "    return selector\n",
    "    \n",
    "ct = make_column_transformer(\n",
    "        (CustomStandardScaler(), make_column_selector(dtype_include=['float32'])),\n",
    "        (CustomOneHotEncoder(dtype=np.float32), make_column_selector(dtype_include=['category'],dtype_exclude=['float32'])), remainder='passthrough')\n",
    "\n",
    "# ct = make_column_transformer(\n",
    "#         #(CustomStandardScaler(), make_column_selector(dtype_include=['float32'])),\n",
    "#         (CustomOneHotEncoder(), select_columns_by_name(['sex'])), remainder='passthrough')\n",
    "\n",
    "#ct = CustomOneHotEncoder()\n",
    "data = load_support(path=\"/Users/JUSC/Documents/xgbsurv/xgbsurv/datasets/data/\", as_frame=True)\n",
    "X  = data.data\n",
    "#X = X.dtype(np.float32)\n",
    "#X = X.convert_dtypes()\n",
    "print(X.columns)\n",
    "print(X.shape)\n",
    "print(X.dtypes)\n",
    "X = ct.fit_transform(X)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.0000000e-01, -7.6666665e-01, -1.2333333e+00, ...,\n",
       "       -3.3703333e+02,  3.5100000e+02,  3.5520001e+02], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_metabric(path=\"/Users/JUSC/Documents/xgbsurv/xgbsurv/datasets/data/\", as_frame=True)\n",
    "data.target.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgbsurv.models.breslow_final import get_cumulative_hazard_function_breslow\n",
    "import numba\n",
    "# get_cumulative_hazard_function_breslow(X_train: np.array, \n",
    "#         X_test: np.array, y_train: np.array, y_test: np.array,\n",
    "#         predictor_train: np.array, predictor_test: np.array\n",
    "#     )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters, put into function\n",
    "n_outer_splits = 5\n",
    "n_inner_splits = 5\n",
    "rand_state = 42\n",
    "n_iter = 1 # # set to 50\n",
    "early_stopping_rounds=10\n",
    "base_score = 0.0\n",
    "\n",
    "def train_eval(X, y, net, n_iter, filename):\n",
    "\n",
    "        dataset_name = filename.split('_')[0]\n",
    "        # add IBS later\n",
    "        outer_scores = {'cindex_train_'+dataset_name:[], 'cindex_test_'+dataset_name:[],\n",
    "                        'ibs_train_'+dataset_name:[], 'ibs_test_'+dataset_name:[]}\n",
    "        best_params = {'best_params_'+dataset_name:[]}\n",
    "        best_model = {'best_model_'+dataset_name:[]}\n",
    "\n",
    "        ct = make_column_transformer(\n",
    "        (CustomStandardScaler(), make_column_selector(dtype_include=['float32'])),\n",
    "        (OneHotEncoder(dtype=np.float32), make_column_selector(dtype_include=['category'], dtype_exclude=['float32'])), remainder='passthrough')\n",
    "\n",
    "        pipe = Pipeline(\n",
    "        [('scaler', ct),\n",
    "        ('estimator', net)]\n",
    "        )\n",
    "        # pipe = Pipeline([\n",
    "        # ('scale', StandardScaler()),\n",
    "        # ('estimator', net),\n",
    "        # ])\n",
    "        rs = RandomizedSearchCV(pipe, param_grid_breslow, scoring = scoring_function, n_jobs=-1, \n",
    "                            cv=inner_custom_cv, n_iter=n_iter, refit=True)\n",
    "        \n",
    "        for i, (train_index, test_index) in enumerate(outer_custom_cv.split(X, y)):\n",
    "                # Split data into training and testing sets for outer fold\n",
    "                X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "                y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "                print(X_train.shape, type(X_train))\n",
    "                print(y_train.shape, type(y_train))\n",
    "                print(X_test.shape, type(X_test))\n",
    "                print(y_test.shape, type(y_test))\n",
    "                # save splits and data\n",
    "                savetxt('splits/train_index_'+str(i)+'_'+filename, train_index, delimiter=',')\n",
    "                savetxt('splits/test_index_'+str(i)+'_'+filename, test_index, delimiter=',')\n",
    "                \n",
    "                savetxt('splits/X_train_'+str(i)+'_'+filename, X_train, delimiter=',')\n",
    "                savetxt('splits/X_test_'+str(i)+'_'+filename, X_test, delimiter=',')\n",
    "\n",
    "                savetxt('splits/y_train_'+str(i)+'_'+filename, y_train, delimiter=',')\n",
    "                savetxt('splits/y_test_'+str(i)+'_'+filename, y_test, delimiter=',')\n",
    "                \n",
    "                #savetxt('splits/test_index_'+str(i)+'.csv', test_index, delimiter=',')\n",
    "                \n",
    "                # create validation dataset for early stopping\n",
    "                strat = np.sign(y_train)\n",
    "                valid_split = ValidSplit(cv=0.1, stratified=strat, random_state=42)\n",
    "\n",
    "                #print(X_train.dtypes, X_test.dtypes)\n",
    "                # train\n",
    "                rs.fit(X_train, y_train)\n",
    "                \n",
    "                # predict\n",
    "                #scaler = StandardScaler()\n",
    "                #X_train = scaler.fit_transform(X_train)\n",
    "                #X_test = scaler.transform(X_test)\n",
    "                print(rs.best_estimator_.predict(X_train))\n",
    "                best_preds_train = rs.best_estimator_.predict(X_train)\n",
    "                best_preds_test = rs.best_estimator_.predict(X_test)\n",
    "\n",
    "                #print(best_preds_test, type(best_preds_train))\n",
    "                #print(best_preds_test, type(best_preds_test))\n",
    "                # predict survival function\n",
    "                # d = predict_survival_function(X_test, dataframe=True)\n",
    "\n",
    "                # save predictions, get dataset name\n",
    "                savetxt('predictions/train_preds_'+str(i)+'_'+filename, best_preds_train, delimiter=',')\n",
    "                savetxt('predictions/test_preds_'+str(i)+'_'+filename, best_preds_test, delimiter=',')\n",
    "                \n",
    "                # save hyperparameter settings\n",
    "                params = rs.best_estimator_.get_params\n",
    "                best_params['best_params_'+dataset_name] += [rs.best_params_]\n",
    "                best_model['best_model_'+dataset_name] += [params]\n",
    "                #print(rs.best_params_)\n",
    "                # save c-index, ibs values\n",
    "                for var in [X_train, X_train, y_train, y_train, best_preds_train, best_preds_test]:\n",
    "                        if not isinstance(var, np.ndarray):\n",
    "                                #print(type(var))\n",
    "                                var = var.values #.to_numpy()\n",
    "                                #print(type(var))\n",
    "                #print('y_train',y_train)\n",
    "                # training data\n",
    "                #try: \n",
    "                # train data\n",
    "                cum_hazard_train = get_cumulative_hazard_function_breslow(\n",
    "                        X_train, X_train, y_train, y_train,\n",
    "                        best_preds_train.reshape(-1), best_preds_train.reshape(-1)\n",
    "                        )\n",
    "                df_survival_train = np.exp(-cum_hazard_train)\n",
    "                durations_train, events_train = transform_back(y_train)\n",
    "                time_grid_train = np.linspace(durations_train.min(), durations_train.max(), 100)\n",
    "                ev = EvalSurv(df_survival_train, durations_train, events_train, censor_surv='km')\n",
    "                print('Concordance Index',ev.concordance_td('antolini'))\n",
    "                cindex_score_train = ev.concordance_td('antolini')\n",
    "                ibs_score_train = ev.integrated_brier_score(time_grid_train)\n",
    "                outer_scores['cindex_train_'+dataset_name] += [cindex_score_train]\n",
    "                outer_scores['ibs_train_'+dataset_name] += [ibs_score_train]\n",
    "\n",
    "                # test data\n",
    "                cum_hazard_test = get_cumulative_hazard_function_breslow(\n",
    "                        X_train, X_test, y_train, y_test,\n",
    "                        best_preds_train.reshape(-1), best_preds_test.reshape(-1)\n",
    "                        )\n",
    "                df_survival_test = np.exp(-cum_hazard_test)\n",
    "                durations_test, events_test = transform_back(y_test)\n",
    "                print('durations',durations_test.min(), durations_test.max())\n",
    "                time_grid_test = np.linspace(durations_test.min(), durations_test.max(), 100)\n",
    "                ev = EvalSurv(df_survival_test, durations_test, events_test, censor_surv='km')\n",
    "                print('Concordance Index',ev.concordance_td('antolini'))\n",
    "                print('Integrated Brier Score:',ev.integrated_brier_score(time_grid_test))\n",
    "                cindex_score_test = ev.concordance_td('antolini')\n",
    "                ibs_score_test = ev.integrated_brier_score(time_grid_test)\n",
    "\n",
    "                outer_scores['cindex_test_'+dataset_name] += [cindex_score_test]\n",
    "                outer_scores['ibs_test_'+dataset_name] += [ibs_score_test]\n",
    "\n",
    "                # except:\n",
    "                #         outer_scores['cindex_train_'+dataset_name] += [np.nan]\n",
    "                #         outer_scores['cindex_test_'+dataset_name] += [np.nan]\n",
    "                #         outer_scores['ibs_train_'+dataset_name] += [np.nan]\n",
    "                #         outer_scores['ibs_test_'+dataset_name] += [np.nan]\n",
    "                # try:\n",
    "                #         score_train = cindex_censored(y_train, best_preds_train.reshape(-1))\n",
    "                #         score_test = cindex_censored(y_test, best_preds_test.reshape(-1))\n",
    "                #         outer_scores['cindex_train_'+dataset_name] += [score_train]\n",
    "                #         outer_scores['cindex_test_'+dataset_name] += [score_test]\n",
    "                # except:\n",
    "                #         outer_scores['cindex_train_'+dataset_name] += [np.nan]\n",
    "                #         outer_scores['cindex_test_'+dataset_name] += [np.nan]\n",
    "        df_best_params = pd.DataFrame(best_params)\n",
    "        df_best_model = pd.DataFrame(best_model)\n",
    "        df_outer_scores = pd.DataFrame(outer_scores)\n",
    "        df_metrics = pd.concat([df_best_params,df_best_model,df_outer_scores], axis=1)\n",
    "        df_metrics.to_csv('metrics/metric_summary_'+str(i)+'_'+filename, index=False)\n",
    "        return best_model, best_params, outer_scores, best_preds_train, best_preds_test, X_train, X_test, y_train, y_test\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age                  float32\n",
       "sex                 category\n",
       "race                 float32\n",
       "n_comorbidities      float32\n",
       "diabetes            category\n",
       "dementia            category\n",
       "cancer              category\n",
       "blood_pressure       float32\n",
       "heart_rate           float32\n",
       "respiration_rate     float32\n",
       "temperature          float32\n",
       "white_blood_cell     float32\n",
       "serum_sodium         float32\n",
       "serum_creatinine     float32\n",
       "dtype: object"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_support(path=\"/Users/JUSC/Documents/xgbsurv/xgbsurv/datasets/data/\", as_frame=True)\n",
    "X  = data.data\n",
    "X.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: METABRIC_adapted.csv\n",
      "split MKI67                float32\n",
      "EGFR                 float32\n",
      "PGR                  float32\n",
      "ERBB2                float32\n",
      "hormone_treatment    float32\n",
      "radiotherapy         float32\n",
      "chemotherapy         float32\n",
      "ER_positive          float32\n",
      "age                  float32\n",
      "dtype: object\n",
      "(1522, 9) <class 'pandas.core.frame.DataFrame'>\n",
      "(1522,) <class 'pandas.core.series.Series'>\n",
      "(381, 9) <class 'pandas.core.frame.DataFrame'>\n",
      "(381,) <class 'pandas.core.series.Series'>\n",
      "split MKI67                float32\n",
      "EGFR                 float32\n",
      "PGR                  float32\n",
      "ERBB2                float32\n",
      "hormone_treatment    float32\n",
      "radiotherapy         float32\n",
      "chemotherapy         float32\n",
      "ER_positive          float32\n",
      "age                  float32\n",
      "dtype: object\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "\nAll the 5 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n5 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/sklearn/pipeline.py\", line 405, in fit\n    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1230, in fit\n    self.partial_fit(X, y, **fit_params)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1189, in partial_fit\n    self.fit_loop(X, y, **fit_params)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1101, in fit_loop\n    self.run_single_epoch(iterator_train, training=True, prefix=\"train\",\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1137, in run_single_epoch\n    step = step_fn(batch, **fit_params)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1016, in train_step\n    self._step_optimizer(step_fn)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 972, in _step_optimizer\n    optimizer.step(step_fn)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/torch/optim/optimizer.py\", line 113, in wrapper\n    return func(*args, **kwargs)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/torch/autograd/grad_mode.py\", line 27, in decorate_context\n    return func(*args, **kwargs)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/torch/optim/adamw.py\", line 119, in step\n    loss = closure()\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1006, in step_fn\n    step = self.train_step_single(batch, **fit_params)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 907, in train_step_single\n    loss.backward()\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/torch/_tensor.py\", line 396, in backward\n    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/torch/autograd/__init__.py\", line 166, in backward\n    grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/torch/autograd/__init__.py\", line 67, in _make_grads\n    raise RuntimeError(\"grad can be implicitly created only for scalar outputs\")\nRuntimeError: grad can be implicitly created only for scalar outputs\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 32\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[39m#print(X.dtypes)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     net \u001b[39m=\u001b[39m NeuralNet(\n\u001b[1;32m     11\u001b[0m     SurvivalModel, \n\u001b[1;32m     12\u001b[0m     module__n_layers \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m     verbose\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m     31\u001b[0m     )\n\u001b[0;32m---> 32\u001b[0m     best_model,params, outer_scores, best_preds_train, best_preds_test, X_train, X_test, y_train, y_test \u001b[39m=\u001b[39m train_eval(X, y, net, n_iter, data\u001b[39m.\u001b[39;49mfilename)\n\u001b[1;32m     34\u001b[0m \u001b[39m#train eval function here\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[57], line 60\u001b[0m, in \u001b[0;36mtrain_eval\u001b[0;34m(X, y, net, n_iter, filename)\u001b[0m\n\u001b[1;32m     56\u001b[0m valid_split \u001b[39m=\u001b[39m ValidSplit(cv\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m, stratified\u001b[39m=\u001b[39mstrat, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\n\u001b[1;32m     58\u001b[0m \u001b[39m#print(X_train.dtypes, X_test.dtypes)\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[39m# train\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m rs\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[1;32m     62\u001b[0m \u001b[39m# predict\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[39m#scaler = StandardScaler()\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39m#X_train = scaler.fit_transform(X_train)\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[39m#X_test = scaler.transform(X_test)\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[39mprint\u001b[39m(rs\u001b[39m.\u001b[39mbest_estimator_\u001b[39m.\u001b[39mpredict(X_train))\n",
      "File \u001b[0;32m~/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/sklearn/model_selection/_search.py:874\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    868\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[1;32m    869\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    870\u001b[0m     )\n\u001b[1;32m    872\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[0;32m--> 874\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[1;32m    876\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    878\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/sklearn/model_selection/_search.py:1768\u001b[0m, in \u001b[0;36mRandomizedSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1766\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1767\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1768\u001b[0m     evaluate_candidates(\n\u001b[1;32m   1769\u001b[0m         ParameterSampler(\n\u001b[1;32m   1770\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_distributions, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_iter, random_state\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrandom_state\n\u001b[1;32m   1771\u001b[0m         )\n\u001b[1;32m   1772\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/sklearn/model_selection/_search.py:851\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    844\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m!=\u001b[39m n_candidates \u001b[39m*\u001b[39m n_splits:\n\u001b[1;32m    845\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    846\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mcv.split and cv.get_n_splits returned \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    847\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39minconsistent results. Expected \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    848\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39msplits, got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(n_splits, \u001b[39mlen\u001b[39m(out) \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m n_candidates)\n\u001b[1;32m    849\u001b[0m     )\n\u001b[0;32m--> 851\u001b[0m _warn_or_raise_about_fit_failures(out, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49merror_score)\n\u001b[1;32m    853\u001b[0m \u001b[39m# For callable self.scoring, the return type is only know after\u001b[39;00m\n\u001b[1;32m    854\u001b[0m \u001b[39m# calling. If the return type is a dictionary, the error scores\u001b[39;00m\n\u001b[1;32m    855\u001b[0m \u001b[39m# can now be inserted with the correct key. The type checking\u001b[39;00m\n\u001b[1;32m    856\u001b[0m \u001b[39m# of out will be done in `_insert_error_scores`.\u001b[39;00m\n\u001b[1;32m    857\u001b[0m \u001b[39mif\u001b[39;00m callable(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscoring):\n",
      "File \u001b[0;32m~/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:367\u001b[0m, in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[0;34m(results, error_score)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[39mif\u001b[39;00m num_failed_fits \u001b[39m==\u001b[39m num_fits:\n\u001b[1;32m    361\u001b[0m     all_fits_failed_message \u001b[39m=\u001b[39m (\n\u001b[1;32m    362\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mAll the \u001b[39m\u001b[39m{\u001b[39;00mnum_fits\u001b[39m}\u001b[39;00m\u001b[39m fits failed.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    363\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIt is very likely that your model is misconfigured.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    364\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou can try to debug the error by setting error_score=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mraise\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    365\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBelow are more details about the failures:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mfit_errors_summary\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    366\u001b[0m     )\n\u001b[0;32m--> 367\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[1;32m    369\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    370\u001b[0m     some_fits_failed_message \u001b[39m=\u001b[39m (\n\u001b[1;32m    371\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mnum_failed_fits\u001b[39m}\u001b[39;00m\u001b[39m fits failed out of a total of \u001b[39m\u001b[39m{\u001b[39;00mnum_fits\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    372\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe score on these train-test partitions for these parameters\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBelow are more details about the failures:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mfit_errors_summary\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    377\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: \nAll the 5 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n5 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/sklearn/pipeline.py\", line 405, in fit\n    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1230, in fit\n    self.partial_fit(X, y, **fit_params)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1189, in partial_fit\n    self.fit_loop(X, y, **fit_params)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1101, in fit_loop\n    self.run_single_epoch(iterator_train, training=True, prefix=\"train\",\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1137, in run_single_epoch\n    step = step_fn(batch, **fit_params)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1016, in train_step\n    self._step_optimizer(step_fn)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 972, in _step_optimizer\n    optimizer.step(step_fn)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/torch/optim/optimizer.py\", line 113, in wrapper\n    return func(*args, **kwargs)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/torch/autograd/grad_mode.py\", line 27, in decorate_context\n    return func(*args, **kwargs)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/torch/optim/adamw.py\", line 119, in step\n    loss = closure()\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1006, in step_fn\n    step = self.train_step_single(batch, **fit_params)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 907, in train_step_single\n    loss.backward()\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/torch/_tensor.py\", line 396, in backward\n    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/torch/autograd/__init__.py\", line 166, in backward\n    grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/torch/autograd/__init__.py\", line 67, in _make_grads\n    raise RuntimeError(\"grad can be implicitly created only for scalar outputs\")\nRuntimeError: grad can be implicitly created only for scalar outputs\n"
     ]
    }
   ],
   "source": [
    "data_set_fns = [load_metabric] #, load_flchain, load_rgbsg, load_support, load_tcga]\n",
    "for dataset in data_set_fns:\n",
    "    # get name of current dataset\n",
    "    data = dataset(path=\"/Users/JUSC/Documents/xgbsurv/xgbsurv/datasets/data/\", as_frame=True)\n",
    "    X  = data.data #.astype(np.float32)\n",
    "    y = data.target #.values #.to_numpy()\n",
    "    print('Dataset:',data.filename)\n",
    "    #print(X.dtypes)\n",
    "    \n",
    "    net = NeuralNet(\n",
    "    SurvivalModel, \n",
    "    module__n_layers = 1,\n",
    "    module__in_features = X.shape[1],\n",
    "    #module__num_nodes = 32,\n",
    "    #module__dropout = 0.1, # these could also be removed\n",
    "    module__out_features = 1,\n",
    "    # for split sizes when result size = 1\n",
    "    iterator_train__drop_last=True,\n",
    "    #iterator_valid__drop_last=True,\n",
    "    criterion=BreslowLoss,\n",
    "    optimizer=torch.optim.AdamW,\n",
    "    optimizer__weight_decay = 0.4,\n",
    "    batch_size=32, # separate train and valid->iterator_train__batch_size=128 and iterator_valid__batch_size=128 ?\n",
    "    callbacks=[EarlyStopping(patience=10)],\n",
    "    #TODO: enable stratification, verify\n",
    "    train_split=ValidSplit(0.2), # might cause lower performance in metrics, explain in thesis\n",
    "    #lr=0.001,\n",
    "    #max_epochs=1, #0,#100\n",
    "    #train_split=None,\n",
    "    verbose=1\n",
    "    )\n",
    "    best_model,params, outer_scores, best_preds_train, best_preds_test, X_train, X_test, y_train, y_test = train_eval(X, y, net, n_iter, data.filename)\n",
    "    \n",
    "#train eval function here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cindex_train_METABRIC</th>\n",
       "      <th>cindex_test_METABRIC</th>\n",
       "      <th>ibs_train_METABRIC</th>\n",
       "      <th>ibs_test_METABRIC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.211087</td>\n",
       "      <td>0.145828</td>\n",
       "      <td>0.185698</td>\n",
       "      <td>0.200548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.198378</td>\n",
       "      <td>0.182186</td>\n",
       "      <td>0.178083</td>\n",
       "      <td>0.188852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.195783</td>\n",
       "      <td>0.159745</td>\n",
       "      <td>0.175659</td>\n",
       "      <td>0.192298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.207634</td>\n",
       "      <td>0.171123</td>\n",
       "      <td>0.191138</td>\n",
       "      <td>0.176814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.209188</td>\n",
       "      <td>0.174080</td>\n",
       "      <td>0.175741</td>\n",
       "      <td>0.181442</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cindex_train_METABRIC  cindex_test_METABRIC  ibs_train_METABRIC  \\\n",
       "0               0.211087              0.145828            0.185698   \n",
       "1               0.198378              0.182186            0.178083   \n",
       "2               0.195783              0.159745            0.175659   \n",
       "3               0.207634              0.171123            0.191138   \n",
       "4               0.209188              0.174080            0.175741   \n",
       "\n",
       "   ibs_test_METABRIC  \n",
       "0           0.200548  \n",
       "1           0.188852  \n",
       "2           0.192298  \n",
       "3           0.176814  \n",
       "4           0.181442  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(outer_scores)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TCGA and PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from xgbsurv.datasets import load_tcga\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SurvivalModel(nn.Module):\n",
    "    def __init__(self, n_layers, in_features, num_nodes, dropout, out_features):\n",
    "        super(SurvivalModel, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.in_features = in_features\n",
    "        self.num_nodes = num_nodes\n",
    "        self.dropout = dropout\n",
    "        self.out_features = out_features\n",
    "        model = []\n",
    "        # first layer\n",
    "        model.append(torch.nn.Linear(in_features, num_nodes))\n",
    "        model.append(torch.nn.ReLU())\n",
    "        model.append(torch.nn.Dropout(dropout))\n",
    "        model.append(torch.nn.BatchNorm1d(num_nodes))\n",
    "\n",
    "        for i in range(n_layers-1):\n",
    "            model.append(torch.nn.Linear(num_nodes, num_nodes))\n",
    "            #init.kaiming_normal_(model[-1].weight, nonlinearity='relu')\n",
    "            model.append(torch.nn.ReLU())\n",
    "            model.append(torch.nn.Dropout(dropout))\n",
    "            model.append(torch.nn.BatchNorm1d(num_nodes))\n",
    "\n",
    "        # output layer\n",
    "        model.append(torch.nn.Linear(num_nodes, out_features))\n",
    "    \n",
    "        self.layers = nn.Sequential(*model)\n",
    "\n",
    "        # for layer in self.layers:\n",
    "        #     if isinstance(layer, nn.Linear):\n",
    "        #         #nn.init.uniform_(layer.weight, a=-0.5, b=0.5)\n",
    "        #         nn.init.kaiming_normal_(layer.weight)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = self.layers(x)\n",
    "        #print(res)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters, put into function\n",
    "n_outer_splits = 5\n",
    "n_inner_splits = 5\n",
    "rand_state = 42\n",
    "n_iter = 1 # # set to 50\n",
    "early_stopping_rounds=10\n",
    "base_score = 0.0\n",
    "\n",
    "def train_eval(X, y, net, n_iter, filename):\n",
    "\n",
    "        dataset_name = filename.split('_')[0]\n",
    "        # add IBS later\n",
    "        outer_scores = {'cindex_train_'+dataset_name:[], 'cindex_test_'+dataset_name:[],\n",
    "                        'ibs_train_'+dataset_name:[], 'ibs_test_'+dataset_name:[]}\n",
    "        best_params = {'best_params_'+dataset_name:[]}\n",
    "        best_model = {'best_model_'+dataset_name:[]}\n",
    "\n",
    "        ct = make_column_transformer(\n",
    "        (CustomStandardScaler(), make_column_selector(dtype_include=['float32'])), \n",
    "        remainder='passthrough')\n",
    "        #,\n",
    "        #(OneHotEncoder(dtype=np.float32), make_column_selector(dtype_include=['category'], dtype_exclude=['float32'])),\n",
    "        pca = PCA()\n",
    "\n",
    "        pipe = Pipeline(\n",
    "        [#('scaler', ct),\n",
    "         ('pca', pca),\n",
    "        ('estimator', net)]\n",
    "        )\n",
    "        # pipe = Pipeline([\n",
    "        # ('scale', StandardScaler()),\n",
    "        # ('estimator', net),\n",
    "        # ])\n",
    "        rs = RandomizedSearchCV(pipe, param_grid_breslow, scoring = scoring_function, n_jobs=-1, \n",
    "                            cv=inner_custom_cv, n_iter=n_iter, refit=True)\n",
    "        \n",
    "        for i, (train_index, test_index) in enumerate(outer_custom_cv.split(X, y)):\n",
    "                # Split data into training and testing sets for outer fold\n",
    "                X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "                y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "                print(X_train.shape, type(X_train))\n",
    "                print(y_train.shape, type(y_train))\n",
    "                print(X_test.shape, type(X_test))\n",
    "                print(y_test.shape, type(y_test))\n",
    "                # save splits and data\n",
    "                savetxt('splits/train_index_'+str(i)+'_'+filename, train_index, delimiter=',')\n",
    "                savetxt('splits/test_index_'+str(i)+'_'+filename, test_index, delimiter=',')\n",
    "                \n",
    "                savetxt('splits/X_train_'+str(i)+'_'+filename, X_train, delimiter=',')\n",
    "                savetxt('splits/X_test_'+str(i)+'_'+filename, X_test, delimiter=',')\n",
    "\n",
    "                savetxt('splits/y_train_'+str(i)+'_'+filename, y_train, delimiter=',')\n",
    "                savetxt('splits/y_test_'+str(i)+'_'+filename, y_test, delimiter=',')\n",
    "                \n",
    "                #savetxt('splits/test_index_'+str(i)+'.csv', test_index, delimiter=',')\n",
    "                \n",
    "                # create validation dataset for early stopping\n",
    "                strat = np.sign(y_train)\n",
    "                valid_split = ValidSplit(cv=0.1, stratified=strat, random_state=42)\n",
    "\n",
    "                #print(X_train.dtypes, X_test.dtypes)\n",
    "                # train\n",
    "                rs.fit(X_train, y_train)\n",
    "                \n",
    "                # predict\n",
    "                #scaler = StandardScaler()\n",
    "                #X_train = scaler.fit_transform(X_train)\n",
    "                #X_test = scaler.transform(X_test)\n",
    "                print(rs.best_estimator_.predict(X_train))\n",
    "                best_preds_train = rs.best_estimator_.predict(X_train)\n",
    "                best_preds_test = rs.best_estimator_.predict(X_test)\n",
    "\n",
    "                #print(best_preds_test, type(best_preds_train))\n",
    "                #print(best_preds_test, type(best_preds_test))\n",
    "                # predict survival function\n",
    "                # d = predict_survival_function(X_test, dataframe=True)\n",
    "\n",
    "                # save predictions, get dataset name\n",
    "                savetxt('predictions/train_preds_'+str(i)+'_'+filename, best_preds_train, delimiter=',')\n",
    "                savetxt('predictions/test_preds_'+str(i)+'_'+filename, best_preds_test, delimiter=',')\n",
    "                \n",
    "                # save hyperparameter settings\n",
    "                params = rs.best_estimator_.get_params\n",
    "                best_params['best_params_'+dataset_name] += [rs.best_params_]\n",
    "                best_model['best_model_'+dataset_name] += [params]\n",
    "                #print(rs.best_params_)\n",
    "                # save c-index, ibs values\n",
    "                for var in [X_train, X_train, y_train, y_train, best_preds_train, best_preds_test]:\n",
    "                        if not isinstance(var, np.ndarray):\n",
    "                                #print(type(var))\n",
    "                                var = var.values #.to_numpy()\n",
    "                                #print(type(var))\n",
    "                #print('y_train',y_train)\n",
    "                # training data\n",
    "                #try: \n",
    "                # train data\n",
    "                cum_hazard_train = get_cumulative_hazard_function_breslow(\n",
    "                        X_train, X_train, y_train, y_train,\n",
    "                        best_preds_train.reshape(-1), best_preds_train.reshape(-1)\n",
    "                        )\n",
    "                df_survival_train = np.exp(-cum_hazard_train)\n",
    "                durations_train, events_train = transform_back(y_train)\n",
    "                time_grid_train = np.linspace(durations_train.min(), durations_train.max(), 100)\n",
    "                ev = EvalSurv(df_survival_train, durations_train, events_train, censor_surv='km')\n",
    "                print('Concordance Index',ev.concordance_td('antolini'))\n",
    "                cindex_score_train = ev.concordance_td('antolini')\n",
    "                ibs_score_train = ev.integrated_brier_score(time_grid_train)\n",
    "                outer_scores['cindex_train_'+dataset_name] += [cindex_score_train]\n",
    "                outer_scores['ibs_train_'+dataset_name] += [ibs_score_train]\n",
    "\n",
    "                # test data\n",
    "                cum_hazard_test = get_cumulative_hazard_function_breslow(\n",
    "                        X_train, X_test, y_train, y_test,\n",
    "                        best_preds_train.reshape(-1), best_preds_test.reshape(-1)\n",
    "                        )\n",
    "                df_survival_test = np.exp(-cum_hazard_test)\n",
    "                durations_test, events_test = transform_back(y_test)\n",
    "                print('durations',durations_test.min(), durations_test.max())\n",
    "                time_grid_test = np.linspace(durations_test.min(), durations_test.max(), 100)\n",
    "                ev = EvalSurv(df_survival_test, durations_test, events_test, censor_surv='km')\n",
    "                print('Concordance Index',ev.concordance_td('antolini'))\n",
    "                print('Integrated Brier Score:',ev.integrated_brier_score(time_grid_test))\n",
    "                cindex_score_test = ev.concordance_td('antolini')\n",
    "                ibs_score_test = ev.integrated_brier_score(time_grid_test)\n",
    "\n",
    "                outer_scores['cindex_test_'+dataset_name] += [cindex_score_test]\n",
    "                outer_scores['ibs_test_'+dataset_name] += [ibs_score_test]\n",
    "\n",
    "                # except:\n",
    "                #         outer_scores['cindex_train_'+dataset_name] += [np.nan]\n",
    "                #         outer_scores['cindex_test_'+dataset_name] += [np.nan]\n",
    "                #         outer_scores['ibs_train_'+dataset_name] += [np.nan]\n",
    "                #         outer_scores['ibs_test_'+dataset_name] += [np.nan]\n",
    "                # try:\n",
    "                #         score_train = cindex_censored(y_train, best_preds_train.reshape(-1))\n",
    "                #         score_test = cindex_censored(y_test, best_preds_test.reshape(-1))\n",
    "                #         outer_scores['cindex_train_'+dataset_name] += [score_train]\n",
    "                #         outer_scores['cindex_test_'+dataset_name] += [score_test]\n",
    "                # except:\n",
    "                #         outer_scores['cindex_train_'+dataset_name] += [np.nan]\n",
    "                #         outer_scores['cindex_test_'+dataset_name] += [np.nan]\n",
    "        df_best_params = pd.DataFrame(best_params)\n",
    "        df_best_model = pd.DataFrame(best_model)\n",
    "        df_outer_scores = pd.DataFrame(outer_scores)\n",
    "        df_metrics = pd.concat([df_best_params,df_best_model,df_outer_scores], axis=1)\n",
    "        df_metrics.to_csv('metrics/metric_summary_'+str(i)+'_'+filename, index=False)\n",
    "        return best_model, best_params, outer_scores, best_preds_train, best_preds_test, X_train, X_test, y_train, y_test\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: BLCA_adapted.csv\n",
      "shape X 20531\n",
      "split gex_?|100130426      float32\n",
      "gex_?|100133144      float32\n",
      "gex_?|100134869      float32\n",
      "gex_?|10357          float32\n",
      "gex_?|10431          float32\n",
      "                      ...   \n",
      "gex_ZYG11A|440590    float32\n",
      "gex_ZYG11B|79699     float32\n",
      "gex_ZYX|7791         float32\n",
      "gex_ZZEF1|23140      float32\n",
      "gex_ZZZ3|26009       float32\n",
      "Length: 20531, dtype: object\n",
      "(324, 20531) <class 'pandas.core.frame.DataFrame'>\n",
      "(324,) <class 'numpy.ndarray'>\n",
      "(82, 20531) <class 'pandas.core.frame.DataFrame'>\n",
      "(82,) <class 'numpy.ndarray'>\n",
      "split gex_?|100130426      float32\n",
      "gex_?|100133144      float32\n",
      "gex_?|100134869      float32\n",
      "gex_?|10357          float32\n",
      "gex_?|10431          float32\n",
      "                      ...   \n",
      "gex_ZYG11A|440590    float32\n",
      "gex_ZYG11B|79699     float32\n",
      "gex_ZYX|7791         float32\n",
      "gex_ZZEF1|23140      float32\n",
      "gex_ZZZ3|26009       float32\n",
      "Length: 20531, dtype: object\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "\nAll the 5 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n4 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/sklearn/pipeline.py\", line 405, in fit\n    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1230, in fit\n    self.partial_fit(X, y, **fit_params)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1189, in partial_fit\n    self.fit_loop(X, y, **fit_params)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1104, in fit_loop\n    self.run_single_epoch(iterator_valid, training=False, prefix=\"valid\",\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1137, in run_single_epoch\n    step = step_fn(batch, **fit_params)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 873, in validation_step\n    y_pred = self.infer(Xi, **fit_params)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1427, in infer\n    return self.module_(x, **fit_params)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/var/folders/jr/dh6mkdzs31lc5pkqymtdbh180000gp/T/ipykernel_44752/2043477712.py\", line 35, in forward\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 139, in forward\n    input = module(input)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 114, in forward\n    return F.linear(input, self.weight, self.bias)\nRuntimeError: mat1 and mat2 shapes cannot be multiplied (52x259 and 20531x512)\n\n--------------------------------------------------------------------------------\n1 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/sklearn/pipeline.py\", line 405, in fit\n    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1230, in fit\n    self.partial_fit(X, y, **fit_params)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1189, in partial_fit\n    self.fit_loop(X, y, **fit_params)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1104, in fit_loop\n    self.run_single_epoch(iterator_valid, training=False, prefix=\"valid\",\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1137, in run_single_epoch\n    step = step_fn(batch, **fit_params)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 873, in validation_step\n    y_pred = self.infer(Xi, **fit_params)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1427, in infer\n    return self.module_(x, **fit_params)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/var/folders/jr/dh6mkdzs31lc5pkqymtdbh180000gp/T/ipykernel_44752/2043477712.py\", line 35, in forward\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 139, in forward\n    input = module(input)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 114, in forward\n    return F.linear(input, self.weight, self.bias)\nRuntimeError: mat1 and mat2 shapes cannot be multiplied (52x260 and 20531x512)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 34\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mshape X\u001b[39m\u001b[39m'\u001b[39m,X\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m])\n\u001b[1;32m     12\u001b[0m     net \u001b[39m=\u001b[39m NeuralNet(\n\u001b[1;32m     13\u001b[0m     SurvivalModel, \n\u001b[1;32m     14\u001b[0m     module__n_layers \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     32\u001b[0m     verbose\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m     33\u001b[0m     )\n\u001b[0;32m---> 34\u001b[0m     best_model,params, outer_scores, best_preds_train, best_preds_test, X_train, X_test, y_train, y_test \u001b[39m=\u001b[39m train_eval(X, y, net, n_iter, data\u001b[39m.\u001b[39;49mfilename)\n\u001b[1;32m     36\u001b[0m \u001b[39m#train eval function here\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[35], line 64\u001b[0m, in \u001b[0;36mtrain_eval\u001b[0;34m(X, y, net, n_iter, filename)\u001b[0m\n\u001b[1;32m     60\u001b[0m valid_split \u001b[39m=\u001b[39m ValidSplit(cv\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m, stratified\u001b[39m=\u001b[39mstrat, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\n\u001b[1;32m     62\u001b[0m \u001b[39m#print(X_train.dtypes, X_test.dtypes)\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[39m# train\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m rs\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[1;32m     66\u001b[0m \u001b[39m# predict\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[39m#scaler = StandardScaler()\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[39m#X_train = scaler.fit_transform(X_train)\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[39m#X_test = scaler.transform(X_test)\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[39mprint\u001b[39m(rs\u001b[39m.\u001b[39mbest_estimator_\u001b[39m.\u001b[39mpredict(X_train))\n",
      "File \u001b[0;32m~/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/sklearn/model_selection/_search.py:874\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    868\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[1;32m    869\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    870\u001b[0m     )\n\u001b[1;32m    872\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[0;32m--> 874\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[1;32m    876\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    878\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/sklearn/model_selection/_search.py:1768\u001b[0m, in \u001b[0;36mRandomizedSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1766\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1767\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1768\u001b[0m     evaluate_candidates(\n\u001b[1;32m   1769\u001b[0m         ParameterSampler(\n\u001b[1;32m   1770\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_distributions, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_iter, random_state\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrandom_state\n\u001b[1;32m   1771\u001b[0m         )\n\u001b[1;32m   1772\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/sklearn/model_selection/_search.py:851\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    844\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m!=\u001b[39m n_candidates \u001b[39m*\u001b[39m n_splits:\n\u001b[1;32m    845\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    846\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mcv.split and cv.get_n_splits returned \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    847\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39minconsistent results. Expected \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    848\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39msplits, got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(n_splits, \u001b[39mlen\u001b[39m(out) \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m n_candidates)\n\u001b[1;32m    849\u001b[0m     )\n\u001b[0;32m--> 851\u001b[0m _warn_or_raise_about_fit_failures(out, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49merror_score)\n\u001b[1;32m    853\u001b[0m \u001b[39m# For callable self.scoring, the return type is only know after\u001b[39;00m\n\u001b[1;32m    854\u001b[0m \u001b[39m# calling. If the return type is a dictionary, the error scores\u001b[39;00m\n\u001b[1;32m    855\u001b[0m \u001b[39m# can now be inserted with the correct key. The type checking\u001b[39;00m\n\u001b[1;32m    856\u001b[0m \u001b[39m# of out will be done in `_insert_error_scores`.\u001b[39;00m\n\u001b[1;32m    857\u001b[0m \u001b[39mif\u001b[39;00m callable(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscoring):\n",
      "File \u001b[0;32m~/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:367\u001b[0m, in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[0;34m(results, error_score)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[39mif\u001b[39;00m num_failed_fits \u001b[39m==\u001b[39m num_fits:\n\u001b[1;32m    361\u001b[0m     all_fits_failed_message \u001b[39m=\u001b[39m (\n\u001b[1;32m    362\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mAll the \u001b[39m\u001b[39m{\u001b[39;00mnum_fits\u001b[39m}\u001b[39;00m\u001b[39m fits failed.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    363\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIt is very likely that your model is misconfigured.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    364\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou can try to debug the error by setting error_score=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mraise\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    365\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBelow are more details about the failures:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mfit_errors_summary\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    366\u001b[0m     )\n\u001b[0;32m--> 367\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[1;32m    369\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    370\u001b[0m     some_fits_failed_message \u001b[39m=\u001b[39m (\n\u001b[1;32m    371\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mnum_failed_fits\u001b[39m}\u001b[39;00m\u001b[39m fits failed out of a total of \u001b[39m\u001b[39m{\u001b[39;00mnum_fits\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    372\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe score on these train-test partitions for these parameters\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBelow are more details about the failures:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mfit_errors_summary\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    377\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: \nAll the 5 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n4 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/sklearn/pipeline.py\", line 405, in fit\n    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1230, in fit\n    self.partial_fit(X, y, **fit_params)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1189, in partial_fit\n    self.fit_loop(X, y, **fit_params)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1104, in fit_loop\n    self.run_single_epoch(iterator_valid, training=False, prefix=\"valid\",\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1137, in run_single_epoch\n    step = step_fn(batch, **fit_params)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 873, in validation_step\n    y_pred = self.infer(Xi, **fit_params)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1427, in infer\n    return self.module_(x, **fit_params)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/var/folders/jr/dh6mkdzs31lc5pkqymtdbh180000gp/T/ipykernel_44752/2043477712.py\", line 35, in forward\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 139, in forward\n    input = module(input)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 114, in forward\n    return F.linear(input, self.weight, self.bias)\nRuntimeError: mat1 and mat2 shapes cannot be multiplied (52x259 and 20531x512)\n\n--------------------------------------------------------------------------------\n1 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/sklearn/pipeline.py\", line 405, in fit\n    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1230, in fit\n    self.partial_fit(X, y, **fit_params)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1189, in partial_fit\n    self.fit_loop(X, y, **fit_params)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1104, in fit_loop\n    self.run_single_epoch(iterator_valid, training=False, prefix=\"valid\",\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1137, in run_single_epoch\n    step = step_fn(batch, **fit_params)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 873, in validation_step\n    y_pred = self.infer(Xi, **fit_params)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/skorch/net.py\", line 1427, in infer\n    return self.module_(x, **fit_params)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/var/folders/jr/dh6mkdzs31lc5pkqymtdbh180000gp/T/ipykernel_44752/2043477712.py\", line 35, in forward\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 139, in forward\n    input = module(input)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/Users/JUSC/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 114, in forward\n    return F.linear(input, self.weight, self.bias)\nRuntimeError: mat1 and mat2 shapes cannot be multiplied (52x260 and 20531x512)\n"
     ]
    }
   ],
   "source": [
    "from xgbsurv.datasets import load_tcga\n",
    "\n",
    "cancer_types = ['BLCA']\n",
    "for cancer_type in cancer_types:\n",
    "    # get name of current dataset\n",
    "    data = load_tcga(path=\"/Users/JUSC/Documents/xgbsurv/xgbsurv/datasets/data/\", cancer_type= cancer_type, as_frame=True)\n",
    "    X  = data.data #.astype(np.float32)\n",
    "    y = data.target.to_numpy()\n",
    "    print('Dataset:',data.filename)\n",
    "    print('shape X',X.shape[1])\n",
    "    \n",
    "    net = NeuralNet(\n",
    "    SurvivalModel, \n",
    "    module__n_layers = 1,\n",
    "    module__in_features = X.shape[1],\n",
    "    #module__num_nodes = 32,\n",
    "    #module__dropout = 0.1, # these could also be removed\n",
    "    module__out_features = 1,\n",
    "    # for split sizes when result size = 1\n",
    "    iterator_train__drop_last=True,\n",
    "    #iterator_valid__drop_last=True,\n",
    "    criterion=BreslowLoss,\n",
    "    optimizer=torch.optim.AdamW,\n",
    "    optimizer__weight_decay = 0.4,\n",
    "    batch_size=32, # separate train and valid->iterator_train__batch_size=128 and iterator_valid__batch_size=128 ?\n",
    "    callbacks=[EarlyStopping(patience=10)],\n",
    "    #TODO: enable stratification, verify\n",
    "    train_split=ValidSplit(0.2), # might cause lower performance in metrics, explain in thesis\n",
    "    #lr=0.001,\n",
    "    #max_epochs=1, #0,#100\n",
    "    #train_split=None,\n",
    "    verbose=1\n",
    "    )\n",
    "    best_model,params, outer_scores, best_preds_train, best_preds_test, X_train, X_test, y_train, y_test = train_eval(X, y, net, n_iter, data.filename)\n",
    "    \n",
    "#train eval function here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature names Index(['gex_?|100130426', 'gex_?|100133144', 'gex_?|100134869', 'gex_?|10357',\n",
      "       'gex_?|10431', 'gex_?|136542', 'gex_?|155060', 'gex_?|26823',\n",
      "       'gex_?|280660', 'gex_?|317712',\n",
      "       ...\n",
      "       'gex_ZWILCH|55055', 'gex_ZWINT|11130', 'gex_ZXDA|7789',\n",
      "       'gex_ZXDB|158586', 'gex_ZXDC|79364', 'gex_ZYG11A|440590',\n",
      "       'gex_ZYG11B|79699', 'gex_ZYX|7791', 'gex_ZZEF1|23140',\n",
      "       'gex_ZZZ3|26009'],\n",
      "      dtype='object', length=20531)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gex_?|100130426</th>\n",
       "      <th>gex_?|100133144</th>\n",
       "      <th>gex_?|100134869</th>\n",
       "      <th>gex_?|10357</th>\n",
       "      <th>gex_?|10431</th>\n",
       "      <th>gex_?|136542</th>\n",
       "      <th>gex_?|155060</th>\n",
       "      <th>gex_?|26823</th>\n",
       "      <th>gex_?|280660</th>\n",
       "      <th>gex_?|317712</th>\n",
       "      <th>...</th>\n",
       "      <th>gex_ZWILCH|55055</th>\n",
       "      <th>gex_ZWINT|11130</th>\n",
       "      <th>gex_ZXDA|7789</th>\n",
       "      <th>gex_ZXDB|158586</th>\n",
       "      <th>gex_ZXDC|79364</th>\n",
       "      <th>gex_ZYG11A|440590</th>\n",
       "      <th>gex_ZYG11B|79699</th>\n",
       "      <th>gex_ZYX|7791</th>\n",
       "      <th>gex_ZZEF1|23140</th>\n",
       "      <th>gex_ZZZ3|26009</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>4.195434</td>\n",
       "      <td>2.953228</td>\n",
       "      <td>7.231068</td>\n",
       "      <td>10.469998</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.592659</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9.124426</td>\n",
       "      <td>9.885547</td>\n",
       "      <td>4.177870</td>\n",
       "      <td>7.457742</td>\n",
       "      <td>9.985372</td>\n",
       "      <td>1.820975</td>\n",
       "      <td>9.417243</td>\n",
       "      <td>11.921384</td>\n",
       "      <td>9.587230</td>\n",
       "      <td>9.403817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.802896</td>\n",
       "      <td>2.237013</td>\n",
       "      <td>8.397589</td>\n",
       "      <td>10.222879</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.607020</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.218655</td>\n",
       "      <td>10.034551</td>\n",
       "      <td>4.426969</td>\n",
       "      <td>8.462707</td>\n",
       "      <td>10.125465</td>\n",
       "      <td>4.302100</td>\n",
       "      <td>10.112739</td>\n",
       "      <td>11.189874</td>\n",
       "      <td>10.041892</td>\n",
       "      <td>9.783275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>4.183391</td>\n",
       "      <td>4.877784</td>\n",
       "      <td>7.097548</td>\n",
       "      <td>9.959911</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.428193</td>\n",
       "      <td>0.507820</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9.498281</td>\n",
       "      <td>10.776918</td>\n",
       "      <td>4.646773</td>\n",
       "      <td>8.687873</td>\n",
       "      <td>10.588546</td>\n",
       "      <td>5.514933</td>\n",
       "      <td>10.055676</td>\n",
       "      <td>10.842696</td>\n",
       "      <td>9.509474</td>\n",
       "      <td>10.133065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.755529</td>\n",
       "      <td>3.916639</td>\n",
       "      <td>7.398145</td>\n",
       "      <td>10.496434</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.073135</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.128986</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9.329610</td>\n",
       "      <td>11.361143</td>\n",
       "      <td>5.144968</td>\n",
       "      <td>8.649199</td>\n",
       "      <td>9.498885</td>\n",
       "      <td>2.767888</td>\n",
       "      <td>9.776481</td>\n",
       "      <td>11.223296</td>\n",
       "      <td>8.811940</td>\n",
       "      <td>9.527567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>4.009168</td>\n",
       "      <td>5.235065</td>\n",
       "      <td>7.518291</td>\n",
       "      <td>9.919535</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.992802</td>\n",
       "      <td>0.835358</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.989721</td>\n",
       "      <td>9.594618</td>\n",
       "      <td>6.332848</td>\n",
       "      <td>8.745624</td>\n",
       "      <td>10.302148</td>\n",
       "      <td>1.565987</td>\n",
       "      <td>9.332981</td>\n",
       "      <td>10.934199</td>\n",
       "      <td>10.580070</td>\n",
       "      <td>9.562055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.035584</td>\n",
       "      <td>3.977573</td>\n",
       "      <td>7.714362</td>\n",
       "      <td>11.225461</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.968162</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9.727070</td>\n",
       "      <td>11.610245</td>\n",
       "      <td>3.802255</td>\n",
       "      <td>8.037893</td>\n",
       "      <td>9.997673</td>\n",
       "      <td>3.000072</td>\n",
       "      <td>10.538354</td>\n",
       "      <td>11.404152</td>\n",
       "      <td>9.240267</td>\n",
       "      <td>9.791479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.903704</td>\n",
       "      <td>4.024435</td>\n",
       "      <td>7.142873</td>\n",
       "      <td>9.576339</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.139347</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.713679</td>\n",
       "      <td>9.481592</td>\n",
       "      <td>5.497338</td>\n",
       "      <td>8.256657</td>\n",
       "      <td>9.711478</td>\n",
       "      <td>1.667892</td>\n",
       "      <td>9.908784</td>\n",
       "      <td>12.348994</td>\n",
       "      <td>10.086043</td>\n",
       "      <td>9.755756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.892585</td>\n",
       "      <td>8.571294</td>\n",
       "      <td>9.936660</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.619978</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9.092707</td>\n",
       "      <td>10.606433</td>\n",
       "      <td>5.140010</td>\n",
       "      <td>7.794318</td>\n",
       "      <td>9.973999</td>\n",
       "      <td>5.122383</td>\n",
       "      <td>9.017769</td>\n",
       "      <td>12.651590</td>\n",
       "      <td>10.371994</td>\n",
       "      <td>9.364669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.126940</td>\n",
       "      <td>3.091852</td>\n",
       "      <td>7.668268</td>\n",
       "      <td>10.308020</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.358922</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9.262647</td>\n",
       "      <td>10.148044</td>\n",
       "      <td>5.441171</td>\n",
       "      <td>8.328657</td>\n",
       "      <td>9.829008</td>\n",
       "      <td>6.142242</td>\n",
       "      <td>9.544802</td>\n",
       "      <td>12.455296</td>\n",
       "      <td>9.361354</td>\n",
       "      <td>8.944604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.865543</td>\n",
       "      <td>3.524189</td>\n",
       "      <td>7.634070</td>\n",
       "      <td>9.405822</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.068483</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.873398</td>\n",
       "      <td>8.427627</td>\n",
       "      <td>4.761418</td>\n",
       "      <td>8.671594</td>\n",
       "      <td>10.454207</td>\n",
       "      <td>6.009894</td>\n",
       "      <td>9.451395</td>\n",
       "      <td>12.380088</td>\n",
       "      <td>9.581233</td>\n",
       "      <td>8.723463</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>406 rows  20531 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     gex_?|100130426  gex_?|100133144  gex_?|100134869  gex_?|10357  \\\n",
       "0                0.0         4.195434         2.953228     7.231068   \n",
       "1                0.0         0.802896         2.237013     8.397589   \n",
       "2                0.0         4.183391         4.877784     7.097548   \n",
       "3                0.0         2.755529         3.916639     7.398145   \n",
       "4                0.0         4.009168         5.235065     7.518291   \n",
       "..               ...              ...              ...          ...   \n",
       "401              0.0         5.035584         3.977573     7.714362   \n",
       "402              0.0         3.903704         4.024435     7.142873   \n",
       "403              0.0         0.000000         0.892585     8.571294   \n",
       "404              0.0         3.126940         3.091852     7.668268   \n",
       "405              0.0         2.865543         3.524189     7.634070   \n",
       "\n",
       "     gex_?|10431  gex_?|136542  gex_?|155060  gex_?|26823  gex_?|280660  \\\n",
       "0      10.469998           0.0      8.592659     0.000000      0.000000   \n",
       "1      10.222879           0.0      7.607020     0.000000      0.000000   \n",
       "2       9.959911           0.0      8.428193     0.507820      0.000000   \n",
       "3      10.496434           0.0      7.073135     0.000000      3.128986   \n",
       "4       9.919535           0.0      7.992802     0.835358      0.000000   \n",
       "..           ...           ...           ...          ...           ...   \n",
       "401    11.225461           0.0      5.968162     0.000000      0.000000   \n",
       "402     9.576339           0.0      7.139347     0.000000      0.000000   \n",
       "403     9.936660           0.0      7.619978     0.000000      0.000000   \n",
       "404    10.308020           0.0      6.358922     0.000000      0.000000   \n",
       "405     9.405822           0.0      8.068483     0.000000      0.000000   \n",
       "\n",
       "     gex_?|317712  ...  gex_ZWILCH|55055  gex_ZWINT|11130  gex_ZXDA|7789  \\\n",
       "0             0.0  ...          9.124426         9.885547       4.177870   \n",
       "1             0.0  ...          8.218655        10.034551       4.426969   \n",
       "2             0.0  ...          9.498281        10.776918       4.646773   \n",
       "3             0.0  ...          9.329610        11.361143       5.144968   \n",
       "4             0.0  ...          8.989721         9.594618       6.332848   \n",
       "..            ...  ...               ...              ...            ...   \n",
       "401           0.0  ...          9.727070        11.610245       3.802255   \n",
       "402           0.0  ...          8.713679         9.481592       5.497338   \n",
       "403           0.0  ...          9.092707        10.606433       5.140010   \n",
       "404           0.0  ...          9.262647        10.148044       5.441171   \n",
       "405           0.0  ...          8.873398         8.427627       4.761418   \n",
       "\n",
       "     gex_ZXDB|158586  gex_ZXDC|79364  gex_ZYG11A|440590  gex_ZYG11B|79699  \\\n",
       "0           7.457742        9.985372           1.820975          9.417243   \n",
       "1           8.462707       10.125465           4.302100         10.112739   \n",
       "2           8.687873       10.588546           5.514933         10.055676   \n",
       "3           8.649199        9.498885           2.767888          9.776481   \n",
       "4           8.745624       10.302148           1.565987          9.332981   \n",
       "..               ...             ...                ...               ...   \n",
       "401         8.037893        9.997673           3.000072         10.538354   \n",
       "402         8.256657        9.711478           1.667892          9.908784   \n",
       "403         7.794318        9.973999           5.122383          9.017769   \n",
       "404         8.328657        9.829008           6.142242          9.544802   \n",
       "405         8.671594       10.454207           6.009894          9.451395   \n",
       "\n",
       "     gex_ZYX|7791  gex_ZZEF1|23140  gex_ZZZ3|26009  \n",
       "0       11.921384         9.587230        9.403817  \n",
       "1       11.189874        10.041892        9.783275  \n",
       "2       10.842696         9.509474       10.133065  \n",
       "3       11.223296         8.811940        9.527567  \n",
       "4       10.934199        10.580070        9.562055  \n",
       "..            ...              ...             ...  \n",
       "401     11.404152         9.240267        9.791479  \n",
       "402     12.348994        10.086043        9.755756  \n",
       "403     12.651590        10.371994        9.364669  \n",
       "404     12.455296         9.361354        8.944604  \n",
       "405     12.380088         9.581233        8.723463  \n",
       "\n",
       "[406 rows x 20531 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgbsurv.datasets import load_tcga, load_metabric\n",
    "data = load_tcga(path=\"/Users/JUSC/Documents/xgbsurv/xgbsurv/datasets/data/\", as_frame=True)\n",
    "data.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([[ 0.        ,  4.19543422,  2.95322798, ..., 11.92138352,\n",
       "          9.58722996,  9.40381732],\n",
       "        [ 0.        ,  0.80289629,  2.23701297, ..., 11.18987397,\n",
       "         10.04189183,  9.78327493],\n",
       "        [ 0.        ,  4.18339123,  4.87778351, ..., 10.84269605,\n",
       "          9.50947416, 10.13306513],\n",
       "        ...,\n",
       "        [ 0.        ,  0.        ,  0.89258532, ..., 12.65158991,\n",
       "         10.37199439,  9.36466873],\n",
       "        [ 0.        ,  3.12693983,  3.09185213, ..., 12.45529639,\n",
       "          9.36135386,  8.94460373],\n",
       "        [ 0.        ,  2.86554275,  3.52418908, ..., 12.38008763,\n",
       "          9.5812326 ,  8.723463  ]]),\n",
       " 'target': array([  -13.,   -15.,   -17.,    19.,   -20.,    20.,   -28.,   -35.,\n",
       "          -37.,   -46.,   -55.,    56.,    56.,    57.,   -59.,    62.,\n",
       "          -64.,   -64.,    65.,   -67.,   -68.,    68.,    69.,    76.,\n",
       "           81.,   -82.,   -84.,    88.,   -89.,    90.,    92.,    93.,\n",
       "          -95.,    98.,    99.,  -105.,   106.,  -110.,  -117.,   118.,\n",
       "          122.,   128.,  -129.,   131.,   142.,   144.,   146.,   149.,\n",
       "          154.,   154.,  -158.,   163.,  -163.,   168.,   168.,   173.,\n",
       "          182.,  -187.,  -189.,   191.,   200.,   200.,   205.,   206.,\n",
       "          211.,   213.,   216.,   220.,   223.,  -224.,   232.,   232.,\n",
       "          237.,   246.,   248.,   250.,   251.,  -251.,   253.,   254.,\n",
       "          254.,   258.,   259.,   261.,   262.,   272.,   272.,   272.,\n",
       "          273.,   274.,  -276.,   278.,  -293.,   294.,   294.,   303.,\n",
       "          311.,   321.,   324.,   324.,   328.,  -330.,   332.,  -333.,\n",
       "         -337.,   340.,   344.,   344.,  -345.,  -345.,   356.,  -359.,\n",
       "         -361.,   362.,   364.,  -364.,  -365.,  -366.,  -368.,  -368.,\n",
       "         -369.,   370.,  -370.,   370.,  -372.,  -372.,  -373.,  -376.,\n",
       "         -377.,  -379.,  -382.,  -383.,  -384.,  -384.,   385.,   385.,\n",
       "          386.,   388.,  -389.,   391.,   393.,  -394.,  -398.,  -398.,\n",
       "         -399.,   400.,   406.,   408.,  -410.,   413.,   413.,   413.,\n",
       "         -415.,   415.,  -415.,  -416.,   418.,  -423.,  -425.,  -428.,\n",
       "         -428.,  -433.,   434.,   437.,   453.,   454.,  -455.,   455.,\n",
       "         -457.,  -460.,   460.,  -466.,  -466.,   467.,  -467.,   467.,\n",
       "         -469.,   474.,  -474.,  -475.,  -477.,  -480.,  -481.,  -484.,\n",
       "         -485.,   486.,  -491.,   492.,   495.,  -495.,  -495.,  -503.,\n",
       "         -507.,   508.,  -508.,   510.,   510.,  -512.,   522.,  -522.,\n",
       "         -524.,   530.,  -536.,   536.,  -536.,   539.,  -539.,  -540.,\n",
       "         -542.,   544.,   544.,  -546.,   547.,   547.,   550.,  -560.,\n",
       "         -562.,   565.,   565.,  -572.,  -572.,  -573.,   575.,   577.,\n",
       "         -578.,  -578.,   579.,  -580.,  -581.,  -582.,  -588.,   590.,\n",
       "         -590.,   593.,   599.,   602.,  -603.,  -610.,   612.,   615.,\n",
       "          617.,  -618.,   623.,   630.,  -633.,  -636.,  -638.,  -640.,\n",
       "         -641.,  -642.,  -646.,  -648.,  -649.,   651.,  -665.,   665.,\n",
       "          674.,   680.,   685.,   690.,  -691.,   696.,  -699.,  -700.,\n",
       "          706.,   712.,   719.,  -731.,   734.,   739.,  -750.,  -758.,\n",
       "         -761.,  -773.,   778.,  -783.,  -789.,   795.,  -798.,  -799.,\n",
       "         -812.,  -813.,   819.,  -820.,   823.,  -832.,  -832.,   835.,\n",
       "         -840.,  -842.,  -851.,   859.,   864.,  -864.,  -873.,  -887.,\n",
       "         -893.,  -897.,  -899.,   904.,  -906.,  -921.,   941.,  -945.,\n",
       "          949.,   974.,  -997., -1003.,  1004.,  1005.,  1008., -1029.,\n",
       "         1036., -1048.,  1064., -1072.,  1077., -1090., -1094., -1108.,\n",
       "        -1110., -1115., -1127.,  1163., -1174., -1181., -1186., -1219.,\n",
       "         1270., -1326.,  1348., -1350.,  1367., -1370.,  1420.,  1423.,\n",
       "        -1429., -1454., -1455., -1460., -1522., -1529., -1538., -1542.,\n",
       "         1556., -1561., -1582., -1604., -1621., -1639., -1649.,  1670.,\n",
       "        -1708., -1714.,  1718., -1761., -1792.,  1804., -1806., -1830.,\n",
       "        -1845.,  1869., -1884., -1912., -1947., -1949., -1952.,  1971.,\n",
       "        -2008., -2009.,  2020., -2020., -2024., -2027., -2044., -2049.,\n",
       "        -2109., -2139., -2177., -2293., -2312., -2330., -2380., -2423.,\n",
       "        -2625.,  2641., -2656., -2703., -2790.,  2828., -2868., -2886.,\n",
       "         2954., -2964., -3011.,  3183., -3314., -3364., -3420., -3432.,\n",
       "        -3817., -3981., -4343., -4967., -5041., -5050.], dtype=float32),\n",
       " 'frame': None,\n",
       " 'feature_names': Index(['gex_?|100130426', 'gex_?|100133144', 'gex_?|100134869', 'gex_?|10357',\n",
       "        'gex_?|10431', 'gex_?|136542', 'gex_?|155060', 'gex_?|26823',\n",
       "        'gex_?|280660', 'gex_?|317712',\n",
       "        ...\n",
       "        'gex_ZWILCH|55055', 'gex_ZWINT|11130', 'gex_ZXDA|7789',\n",
       "        'gex_ZXDB|158586', 'gex_ZXDC|79364', 'gex_ZYG11A|440590',\n",
       "        'gex_ZYG11B|79699', 'gex_ZYX|7791', 'gex_ZZEF1|23140',\n",
       "        'gex_ZZZ3|26009'],\n",
       "       dtype='object', length=20531),\n",
       " 'filename': 'BLCA_adapted.csv'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MKI67</th>\n",
       "      <th>EGFR</th>\n",
       "      <th>PGR</th>\n",
       "      <th>ERBB2</th>\n",
       "      <th>hormone_treatment</th>\n",
       "      <th>radiotherapy</th>\n",
       "      <th>chemotherapy</th>\n",
       "      <th>ER_positive</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.818934</td>\n",
       "      <td>6.470783</td>\n",
       "      <td>10.672935</td>\n",
       "      <td>5.630679</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>75.330002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.117913</td>\n",
       "      <td>5.335094</td>\n",
       "      <td>9.717084</td>\n",
       "      <td>5.893656</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>54.099998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.705204</td>\n",
       "      <td>8.450347</td>\n",
       "      <td>10.859011</td>\n",
       "      <td>5.667925</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>73.639999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.184060</td>\n",
       "      <td>8.427523</td>\n",
       "      <td>10.361415</td>\n",
       "      <td>5.575082</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>73.980003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.621474</td>\n",
       "      <td>5.456216</td>\n",
       "      <td>9.500981</td>\n",
       "      <td>5.753597</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>34.680000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1898</th>\n",
       "      <td>5.567494</td>\n",
       "      <td>5.818224</td>\n",
       "      <td>11.343552</td>\n",
       "      <td>5.574574</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>58.799999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1899</th>\n",
       "      <td>7.803252</td>\n",
       "      <td>5.352677</td>\n",
       "      <td>10.012809</td>\n",
       "      <td>6.017503</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>67.459999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1900</th>\n",
       "      <td>6.100280</td>\n",
       "      <td>7.107530</td>\n",
       "      <td>10.501780</td>\n",
       "      <td>6.268520</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>29.980000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1901</th>\n",
       "      <td>5.490514</td>\n",
       "      <td>7.606261</td>\n",
       "      <td>12.297510</td>\n",
       "      <td>6.313382</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>63.169998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1902</th>\n",
       "      <td>5.390707</td>\n",
       "      <td>5.812284</td>\n",
       "      <td>10.225805</td>\n",
       "      <td>5.796476</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>60.990002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1903 rows  9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          MKI67      EGFR        PGR     ERBB2  hormone_treatment  \\\n",
       "0      5.818934  6.470783  10.672935  5.630679                0.0   \n",
       "1     10.117913  5.335094   9.717084  5.893656                0.0   \n",
       "2      5.705204  8.450347  10.859011  5.667925                0.0   \n",
       "3      5.184060  8.427523  10.361415  5.575082                1.0   \n",
       "4      5.621474  5.456216   9.500981  5.753597                1.0   \n",
       "...         ...       ...        ...       ...                ...   \n",
       "1898   5.567494  5.818224  11.343552  5.574574                0.0   \n",
       "1899   7.803252  5.352677  10.012809  6.017503                0.0   \n",
       "1900   6.100280  7.107530  10.501780  6.268520                0.0   \n",
       "1901   5.490514  7.606261  12.297510  6.313382                0.0   \n",
       "1902   5.390707  5.812284  10.225805  5.796476                1.0   \n",
       "\n",
       "      radiotherapy  chemotherapy  ER_positive        age  \n",
       "0              0.0           0.0          1.0  75.330002  \n",
       "1              0.0           0.0          0.0  54.099998  \n",
       "2              0.0           0.0          1.0  73.639999  \n",
       "3              0.0           0.0          1.0  73.980003  \n",
       "4              0.0           0.0          1.0  34.680000  \n",
       "...            ...           ...          ...        ...  \n",
       "1898           1.0           0.0          1.0  58.799999  \n",
       "1899           0.0           0.0          1.0  67.459999  \n",
       "1900           0.0           0.0          1.0  29.980000  \n",
       "1901           1.0           0.0          1.0  63.169998  \n",
       "1902           1.0           0.0          1.0  60.990002  \n",
       "\n",
       "[1903 rows x 9 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d  = load_metabric(path=\"/Users/JUSC/Documents/xgbsurv/xgbsurv/datasets/data/\", as_frame = True)\n",
    "d.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def breslow_estimator(log_hazard, time, event):\n",
    "    #time, event = transform_back(y)\n",
    "    risk_score = np.exp(log_hazard)\n",
    "    print(risk_score.shape)\n",
    "    is_sorted = lambda a: np.all(a[:-1] <= a[1:])\n",
    "\n",
    "    if is_sorted(time) == False:\n",
    "        order = np.argsort(time, kind=\"mergesort\")\n",
    "        time = time[order]\n",
    "        event = event[order]\n",
    "        risk_score = risk_score[order]\n",
    "\n",
    "    uniq_times = np.unique(time)\n",
    "    idx = np.digitize(time, np.unique(time))\n",
    "    breaks = np.flatnonzero(np.concatenate(([1], np.diff(idx))))\n",
    "    # numpy diff nth discrete difference over index, add 1 at the beginning\n",
    "    # flatnonzero return indices that are nonzero in flattened version\n",
    "    n_events = np.add.reduceat(event, breaks, axis=0)\n",
    "\n",
    "    # consider removing zero rows\n",
    "    risk_matrix = np.unique((np.outer(time,time)>=np.square(time)).astype(int).T, axis=0)\n",
    "    print(risk_matrix.shape)\n",
    "    denominator = np.sum(risk_score.reshape(-1)*risk_matrix,axis=1)[::-1] \n",
    "\n",
    "    baseline_hazard = n_events / denominator\n",
    "    cum_hazard_baseline = np.cumsum(n_events / denominator)\n",
    "    baseline_survival = np.exp(-cum_hazard_baseline)\n",
    "    return uniq_times, baseline_hazard, cum_hazard_baseline, baseline_survival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1523, 1)\n",
      "(1381, 1523)\n"
     ]
    }
   ],
   "source": [
    "time, event = transform_back(y_train)\n",
    "uniq_times_old, baseline_hazard, cum_hazard_baseline_old, baseline_survival_old = breslow_estimator(best_preds_train, time, event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = np.exp(-(cum_hazard_baseline_old*np.exp(best_preds_train[20])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "x=%{x}<br>y=%{y}<extra></extra>",
         "legendgroup": "",
         "line": {
          "color": "#636efa",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "",
         "showlegend": false,
         "type": "scattergl",
         "x": [
          0.10000000149011612,
          0.7666666507720947,
          1.2333333492279053,
          1.4333332777023315,
          1.7666666507720947,
          2,
          2.4000000953674316,
          2.5,
          2.5333333015441895,
          3.366666555404663,
          3.5,
          3.7666666507720947,
          4.166666507720947,
          4.433333396911621,
          5.066666603088379,
          5.433333396911621,
          5.5,
          6.266666889190674,
          7.800000190734863,
          8.066666603088379,
          9.066666603088379,
          9.133333206176758,
          9.433333396911621,
          9.600000381469727,
          9.699999809265137,
          9.833333015441895,
          10.066666603088379,
          10.633333206176758,
          10.833333015441895,
          11.600000381469727,
          11.699999809265137,
          11.866666793823242,
          12.266666412353516,
          12.399999618530273,
          12.933333396911621,
          13.399999618530273,
          13.533333778381348,
          13.800000190734863,
          14.133333206176758,
          14.166666984558105,
          14.199999809265137,
          14.399999618530273,
          14.433333396911621,
          14.699999809265137,
          14.800000190734863,
          15.066666603088379,
          15.166666984558105,
          15.199999809265137,
          15.366666793823242,
          15.5,
          15.533333778381348,
          15.600000381469727,
          15.633333206176758,
          15.699999809265137,
          15.866666793823242,
          16.16666603088379,
          16.299999237060547,
          16.600000381469727,
          16.700000762939453,
          16.733333587646484,
          16.83333396911621,
          17.66666603088379,
          17.766666412353516,
          17.83333396911621,
          17.933332443237305,
          18.233333587646484,
          18.266666412353516,
          18.799999237060547,
          18.83333396911621,
          18.933332443237305,
          19,
          19.03333282470703,
          19.16666603088379,
          19.399999618530273,
          19.600000381469727,
          19.733333587646484,
          19.83333396911621,
          19.899999618530273,
          20,
          20.200000762939453,
          20.266666412353516,
          20.83333396911621,
          21,
          21.066667556762695,
          21.16666603088379,
          21.299999237060547,
          21.700000762939453,
          21.899999618530273,
          21.933332443237305,
          22.133333206176758,
          22.233333587646484,
          22.399999618530273,
          22.66666603088379,
          23.200000762939453,
          23.266666412353516,
          23.33333396911621,
          23.366666793823242,
          23.433332443237305,
          23.733333587646484,
          23.799999237060547,
          23.83333396911621,
          23.899999618530273,
          23.933332443237305,
          24.100000381469727,
          24.33333396911621,
          24.633333206176758,
          24.799999237060547,
          24.866666793823242,
          24.899999618530273,
          25.03333282470703,
          25.233333587646484,
          25.33333396911621,
          25.433332443237305,
          25.53333282470703,
          25.633333206176758,
          26,
          26.266666412353516,
          26.33333396911621,
          26.733333587646484,
          26.766666412353516,
          27,
          27.066667556762695,
          27.200000762939453,
          27.299999237060547,
          27.399999618530273,
          27.46666717529297,
          27.799999237060547,
          27.866666793823242,
          27.96666717529297,
          28,
          28.066667556762695,
          28.566667556762695,
          28.600000381469727,
          29,
          29.03333282470703,
          29.066667556762695,
          29.233333587646484,
          29.299999237060547,
          29.66666603088379,
          29.899999618530273,
          30.066667556762695,
          30.133333206176758,
          30.16666603088379,
          30.299999237060547,
          30.366666793823242,
          30.433332443237305,
          30.700000762939453,
          30.799999237060547,
          30.933332443237305,
          30.96666717529297,
          31,
          31.066667556762695,
          31.16666603088379,
          31.299999237060547,
          31.433332443237305,
          31.799999237060547,
          31.933332443237305,
          32.03333282470703,
          32.06666564941406,
          32.63333511352539,
          32.733333587646484,
          32.83333206176758,
          32.86666488647461,
          32.93333435058594,
          33.13333511352539,
          33.36666488647461,
          33.56666564941406,
          33.70000076293945,
          33.79999923706055,
          33.900001525878906,
          34.29999923706055,
          34.56666564941406,
          34.66666793823242,
          34.70000076293945,
          35,
          35.03333282470703,
          35.20000076293945,
          35.233333587646484,
          35.36666488647461,
          35.400001525878906,
          35.46666717529297,
          35.53333282470703,
          35.599998474121094,
          35.63333511352539,
          35.70000076293945,
          36.16666793823242,
          36.36666488647461,
          36.56666564941406,
          36.63333511352539,
          36.766666412353516,
          36.93333435058594,
          36.96666717529297,
          37,
          37.5,
          37.766666412353516,
          37.86666488647461,
          37.900001525878906,
          37.96666717529297,
          38.03333282470703,
          38.13333511352539,
          38.43333435058594,
          38.56666564941406,
          38.79999923706055,
          39.16666793823242,
          39.20000076293945,
          39.29999923706055,
          39.43333435058594,
          39.53333282470703,
          39.79999923706055,
          39.83333206176758,
          39.86666488647461,
          40.70000076293945,
          41.16666793823242,
          41.36666488647461,
          41.46666717529297,
          41.53333282470703,
          41.83333206176758,
          42.06666564941406,
          42.29999923706055,
          42.33333206176758,
          42.36666488647461,
          42.43333435058594,
          42.5,
          42.56666564941406,
          42.599998474121094,
          42.63333511352539,
          42.70000076293945,
          42.900001525878906,
          42.96666717529297,
          43.03333282470703,
          43.099998474121094,
          43.13333511352539,
          43.16666793823242,
          43.20000076293945,
          43.266666412353516,
          43.29999923706055,
          43.400001525878906,
          43.83333206176758,
          43.86666488647461,
          43.900001525878906,
          44.13333511352539,
          44.233333587646484,
          44.400001525878906,
          44.599998474121094,
          44.63333511352539,
          44.733333587646484,
          44.79999923706055,
          44.83333206176758,
          44.86666488647461,
          45.03333282470703,
          45.16666793823242,
          45.33333206176758,
          45.5,
          45.70000076293945,
          45.733333587646484,
          45.79999923706055,
          45.93333435058594,
          46.06666564941406,
          46.16666793823242,
          46.36666488647461,
          46.43333435058594,
          46.53333282470703,
          46.66666793823242,
          46.83333206176758,
          47.03333282470703,
          47.13333511352539,
          47.43333435058594,
          47.63333511352539,
          47.900001525878906,
          48.16666793823242,
          48.43333435058594,
          48.599998474121094,
          48.79999923706055,
          49.20000076293945,
          49.233333587646484,
          49.29999923706055,
          49.43333435058594,
          49.46666717529297,
          49.53333282470703,
          49.66666793823242,
          49.733333587646484,
          49.766666412353516,
          50,
          50.03333282470703,
          50.06666564941406,
          50.233333587646484,
          50.46666717529297,
          50.53333282470703,
          50.66666793823242,
          50.766666412353516,
          50.900001525878906,
          51,
          51.20000076293945,
          51.400001525878906,
          51.46666717529297,
          51.66666793823242,
          51.70000076293945,
          51.96666717529297,
          52.06666564941406,
          52.29999923706055,
          52.33333206176758,
          52.46666717529297,
          52.5,
          52.63333511352539,
          52.733333587646484,
          52.900001525878906,
          52.96666717529297,
          53.36666488647461,
          53.63333511352539,
          54.099998474121094,
          54.266666412353516,
          54.33333206176758,
          54.766666412353516,
          54.93333435058594,
          55,
          55.03333282470703,
          55.20000076293945,
          55.36666488647461,
          55.400001525878906,
          55.46666717529297,
          55.63333511352539,
          55.66666793823242,
          55.733333587646484,
          55.766666412353516,
          55.83333206176758,
          55.93333435058594,
          56.33333206176758,
          56.5,
          56.766666412353516,
          56.93333435058594,
          57.233333587646484,
          57.29999923706055,
          57.400001525878906,
          57.63333511352539,
          57.66666793823242,
          57.93333435058594,
          58,
          58.13333511352539,
          58.43333435058594,
          58.599998474121094,
          58.66666793823242,
          58.766666412353516,
          58.93333435058594,
          59.5,
          59.599998474121094,
          59.70000076293945,
          59.79999923706055,
          59.93333435058594,
          60.13333511352539,
          60.266666412353516,
          60.66666793823242,
          60.70000076293945,
          60.86666488647461,
          61.099998474121094,
          61.33333206176758,
          61.43333435058594,
          61.70000076293945,
          61.79999923706055,
          61.900001525878906,
          61.96666717529297,
          62.13333511352539,
          62.33333206176758,
          62.63333511352539,
          62.766666412353516,
          62.86666488647461,
          62.900001525878906,
          63,
          63.03333282470703,
          63.20000076293945,
          63.5,
          63.53333282470703,
          63.56666564941406,
          63.733333587646484,
          63.79999923706055,
          63.83333206176758,
          63.86666488647461,
          64,
          64.03333282470703,
          64.23332977294922,
          64.30000305175781,
          64.36666870117188,
          64.5999984741211,
          64.69999694824219,
          64.93333435058594,
          65.16666412353516,
          65.23332977294922,
          65.4000015258789,
          65.5,
          65.83333587646484,
          65.86666870117188,
          66.63333129882812,
          66.73332977294922,
          66.83333587646484,
          67.80000305175781,
          68.06666564941406,
          68.13333129882812,
          68.19999694824219,
          68.26667022705078,
          68.76667022705078,
          69.0999984741211,
          69.33333587646484,
          69.4000015258789,
          69.5,
          69.80000305175781,
          70.06666564941406,
          70.16666412353516,
          70.23332977294922,
          70.26667022705078,
          70.46666717529297,
          70.53333282470703,
          70.5999984741211,
          70.9000015258789,
          71,
          71.06666564941406,
          71.16666412353516,
          71.46666717529297,
          71.5,
          71.63333129882812,
          71.83333587646484,
          72.26667022705078,
          72.30000305175781,
          72.36666870117188,
          72.46666717529297,
          72.56666564941406,
          72.9000015258789,
          73.13333129882812,
          73.46666717529297,
          73.69999694824219,
          73.83333587646484,
          74.03333282470703,
          74.0999984741211,
          74.46666717529297,
          74.73332977294922,
          75.23332977294922,
          75.33333587646484,
          75.36666870117188,
          75.4000015258789,
          75.5,
          75.69999694824219,
          75.86666870117188,
          75.96666717529297,
          76,
          76.23332977294922,
          76.63333129882812,
          76.69999694824219,
          76.73332977294922,
          76.86666870117188,
          77.23332977294922,
          77.4000015258789,
          77.46666717529297,
          77.5,
          77.66666412353516,
          77.86666870117188,
          78.46666717529297,
          78.5999984741211,
          78.69999694824219,
          78.76667022705078,
          78.86666870117188,
          79.0999984741211,
          79.13333129882812,
          79.16666412353516,
          79.30000305175781,
          79.33333587646484,
          79.36666870117188,
          79.80000305175781,
          79.86666870117188,
          80.23332977294922,
          80.43333435058594,
          80.5,
          80.66666412353516,
          80.73332977294922,
          80.83333587646484,
          81.03333282470703,
          81.06666564941406,
          81.0999984741211,
          81.13333129882812,
          81.33333587646484,
          81.5,
          81.56666564941406,
          81.80000305175781,
          81.93333435058594,
          82.0999984741211,
          82.63333129882812,
          82.73332977294922,
          82.96666717529297,
          83.36666870117188,
          83.53333282470703,
          83.63333129882812,
          83.66666412353516,
          84.19999694824219,
          84.23332977294922,
          84.5,
          84.63333129882812,
          84.73332977294922,
          84.9000015258789,
          85,
          85.03333282470703,
          85.13333129882812,
          85.30000305175781,
          85.33333587646484,
          85.36666870117188,
          85.5,
          85.86666870117188,
          85.93333435058594,
          85.96666717529297,
          86,
          86.06666564941406,
          86.0999984741211,
          86.13333129882812,
          86.23332977294922,
          86.36666870117188,
          86.4000015258789,
          86.53333282470703,
          86.5999984741211,
          86.80000305175781,
          86.83333587646484,
          87,
          87.0999984741211,
          87.23332977294922,
          87.46666717529297,
          87.53333282470703,
          87.73332977294922,
          87.83333587646484,
          88.23332977294922,
          88.33333587646484,
          88.46666717529297,
          88.5,
          88.66666412353516,
          88.80000305175781,
          88.93333435058594,
          89.03333282470703,
          89.0999984741211,
          89.33333587646484,
          89.36666870117188,
          89.53333282470703,
          89.76667022705078,
          89.9000015258789,
          89.96666717529297,
          90,
          90.23332977294922,
          90.26667022705078,
          90.30000305175781,
          90.4000015258789,
          90.5999984741211,
          90.80000305175781,
          91.06666564941406,
          91.0999984741211,
          91.13333129882812,
          91.23332977294922,
          91.26667022705078,
          91.53333282470703,
          91.5999984741211,
          91.63333129882812,
          92.4000015258789,
          92.5,
          92.73332977294922,
          92.76667022705078,
          92.86666870117188,
          93.30000305175781,
          93.36666870117188,
          94.03333282470703,
          94.23332977294922,
          94.56666564941406,
          94.69999694824219,
          94.73332977294922,
          94.93333435058594,
          95.73332977294922,
          95.83333587646484,
          95.86666870117188,
          96.76667022705078,
          96.83333587646484,
          96.9000015258789,
          96.96666717529297,
          97.26667022705078,
          97.30000305175781,
          97.4000015258789,
          97.43333435058594,
          97.76667022705078,
          98.0999984741211,
          98.26667022705078,
          98.46666717529297,
          98.5,
          98.56666564941406,
          98.69999694824219,
          98.76667022705078,
          98.83333587646484,
          99,
          99.36666870117188,
          99.4000015258789,
          99.53333282470703,
          99.69999694824219,
          99.76667022705078,
          100.13333129882812,
          100.16666412353516,
          100.30000305175781,
          100.46666717529297,
          100.73332977294922,
          100.83333587646484,
          100.86666870117188,
          101.06666564941406,
          101.26667022705078,
          101.4000015258789,
          101.56666564941406,
          101.63333129882812,
          101.9000015258789,
          101.96666717529297,
          102,
          102.06666564941406,
          102.0999984741211,
          102.30000305175781,
          102.4000015258789,
          102.56666564941406,
          102.69999694824219,
          102.76667022705078,
          102.96666717529297,
          103.46666717529297,
          103.63333129882812,
          103.80000305175781,
          103.83333587646484,
          104,
          104.03333282470703,
          104.0999984741211,
          104.30000305175781,
          104.4000015258789,
          104.46666717529297,
          104.53333282470703,
          104.66666412353516,
          104.76667022705078,
          105.19999694824219,
          105.46666717529297,
          105.5999984741211,
          105.66666412353516,
          105.96666717529297,
          106.13333129882812,
          106.56666564941406,
          106.80000305175781,
          106.96666717529297,
          107.06666564941406,
          107.0999984741211,
          107.26667022705078,
          107.46666717529297,
          107.86666870117188,
          108.06666564941406,
          108.16666412353516,
          108.30000305175781,
          108.43333435058594,
          108.5999984741211,
          108.76667022705078,
          108.93333435058594,
          109,
          109.03333282470703,
          109.19999694824219,
          109.23332977294922,
          109.5999984741211,
          109.76667022705078,
          109.83333587646484,
          109.93333435058594,
          110.26667022705078,
          110.46666717529297,
          110.63333129882812,
          110.76667022705078,
          110.83333587646484,
          110.86666870117188,
          111,
          111.06666564941406,
          111.16666412353516,
          111.36666870117188,
          111.53333282470703,
          111.56666564941406,
          111.63333129882812,
          111.69999694824219,
          111.83333587646484,
          111.86666870117188,
          111.96666717529297,
          112,
          112.13333129882812,
          112.4000015258789,
          112.46666717529297,
          112.56666564941406,
          112.63333129882812,
          112.66666412353516,
          113.36666870117188,
          113.43333435058594,
          113.56666564941406,
          113.66666412353516,
          113.83333587646484,
          114,
          114.33333587646484,
          114.46666717529297,
          114.5,
          114.53333282470703,
          114.5999984741211,
          114.76667022705078,
          114.9000015258789,
          115.30000305175781,
          115.5999984741211,
          116.0999984741211,
          116.23332977294922,
          116.43333435058594,
          116.46666717529297,
          116.53333282470703,
          116.63333129882812,
          116.93333435058594,
          117,
          117.03333282470703,
          117.56666564941406,
          117.5999984741211,
          117.66666412353516,
          117.86666870117188,
          117.9000015258789,
          118,
          118.03333282470703,
          118.13333129882812,
          118.19999694824219,
          118.30000305175781,
          118.69999694824219,
          119,
          119.33333587646484,
          119.46666717529297,
          119.73332977294922,
          119.80000305175781,
          119.86666870117188,
          121.53333282470703,
          121.66666412353516,
          121.73332977294922,
          121.96666717529297,
          122,
          122.13333129882812,
          122.19999694824219,
          122.26667022705078,
          122.4000015258789,
          122.5,
          122.69999694824219,
          122.80000305175781,
          122.83333587646484,
          123,
          123.26667022705078,
          123.30000305175781,
          123.53333282470703,
          123.69999694824219,
          123.9000015258789,
          124,
          124.0999984741211,
          124.13333129882812,
          124.19999694824219,
          124.26667022705078,
          124.56666564941406,
          124.80000305175781,
          125.26667022705078,
          125.33333587646484,
          125.5999984741211,
          125.69999694824219,
          125.76667022705078,
          125.80000305175781,
          125.83333587646484,
          125.9000015258789,
          126.46666717529297,
          126.63333129882812,
          126.66666412353516,
          126.86666870117188,
          127.0999984741211,
          127.53333282470703,
          127.63333129882812,
          127.83333587646484,
          128.10000610351562,
          128.1999969482422,
          128.26666259765625,
          128.36666870117188,
          128.39999389648438,
          128.53334045410156,
          128.56666564941406,
          129.23333740234375,
          129.3333282470703,
          129.43333435058594,
          130.1999969482422,
          130.43333435058594,
          130.46665954589844,
          130.6999969482422,
          130.89999389648438,
          131.06666564941406,
          131.10000610351562,
          131.13333129882812,
          131.26666259765625,
          131.3333282470703,
          131.60000610351562,
          131.6666717529297,
          132.03334045410156,
          132.06666564941406,
          132.3000030517578,
          132.3333282470703,
          132.53334045410156,
          132.56666564941406,
          134.26666259765625,
          134.36666870117188,
          134.46665954589844,
          134.5,
          134.73333740234375,
          134.8333282470703,
          135.1666717529297,
          135.3000030517578,
          135.3333282470703,
          135.6666717529297,
          136,
          136.06666564941406,
          136.1666717529297,
          136.23333740234375,
          136.6999969482422,
          136.93333435058594,
          137.6666717529297,
          137.8000030517578,
          137.93333435058594,
          138.10000610351562,
          138.13333129882812,
          138.3333282470703,
          138.56666564941406,
          138.89999389648438,
          139.1666717529297,
          139.3000030517578,
          139.43333435058594,
          139.53334045410156,
          139.60000610351562,
          139.63333129882812,
          140.06666564941406,
          140.1999969482422,
          140.23333740234375,
          140.3333282470703,
          140.5,
          140.60000610351562,
          140.76666259765625,
          140.86666870117188,
          141.03334045410156,
          141.1666717529297,
          141.56666564941406,
          141.73333740234375,
          142.43333435058594,
          142.46665954589844,
          142.56666564941406,
          142.8000030517578,
          143.13333129882812,
          143.1666717529297,
          143.53334045410156,
          143.60000610351562,
          144.3333282470703,
          144.39999389648438,
          144.43333435058594,
          144.63333129882812,
          144.6666717529297,
          144.6999969482422,
          144.76666259765625,
          144.93333435058594,
          145.3000030517578,
          145.36666870117188,
          145.43333435058594,
          145.5,
          145.63333129882812,
          146,
          146.03334045410156,
          146.06666564941406,
          146.36666870117188,
          146.39999389648438,
          146.76666259765625,
          146.8333282470703,
          146.89999389648438,
          146.93333435058594,
          147.1666717529297,
          147.36666870117188,
          147.6666717529297,
          147.76666259765625,
          147.93333435058594,
          148.06666564941406,
          148.10000610351562,
          148.56666564941406,
          148.8000030517578,
          149.3000030517578,
          149.43333435058594,
          149.60000610351562,
          149.6999969482422,
          149.73333740234375,
          149.76666259765625,
          149.86666870117188,
          150.10000610351562,
          150.5,
          150.56666564941406,
          150.60000610351562,
          150.73333740234375,
          151,
          151.06666564941406,
          151.6666717529297,
          151.89999389648438,
          151.93333435058594,
          152.06666564941406,
          152.1999969482422,
          152.3000030517578,
          152.3333282470703,
          152.93333435058594,
          153.1999969482422,
          153.3000030517578,
          153.53334045410156,
          153.56666564941406,
          153.8333282470703,
          153.86666870117188,
          154,
          154.5,
          154.6999969482422,
          155.36666870117188,
          155.39999389648438,
          155.73333740234375,
          156.3333282470703,
          157.10000610351562,
          157.73333740234375,
          157.8000030517578,
          158.03334045410156,
          158.53334045410156,
          158.63333129882812,
          158.96665954589844,
          159,
          159.06666564941406,
          159.23333740234375,
          159.6999969482422,
          159.73333740234375,
          160,
          160.3000030517578,
          160.3333282470703,
          161.06666564941406,
          162.76666259765625,
          162.8333282470703,
          163.10000610351562,
          163.1666717529297,
          163.1999969482422,
          163.39999389648438,
          163.43333435058594,
          163.53334045410156,
          163.73333740234375,
          164.3333282470703,
          164.5,
          164.56666564941406,
          164.6999969482422,
          164.73333740234375,
          164.89999389648438,
          164.96665954589844,
          165.1666717529297,
          165.1999969482422,
          165.36666870117188,
          165.39999389648438,
          165.43333435058594,
          165.6666717529297,
          166.03334045410156,
          166.6666717529297,
          167.5,
          168.1999969482422,
          168.26666259765625,
          168.53334045410156,
          168.6999969482422,
          168.96665954589844,
          169,
          169.23333740234375,
          169.6666717529297,
          170.03334045410156,
          170.26666259765625,
          170.3000030517578,
          170.6666717529297,
          170.8333282470703,
          170.86666870117188,
          170.89999389648438,
          171.10000610351562,
          171.3000030517578,
          171.63333129882812,
          172,
          172.3000030517578,
          172.8000030517578,
          172.86666870117188,
          172.89999389648438,
          173.03334045410156,
          173.60000610351562,
          173.63333129882812,
          174.13333129882812,
          174.5,
          174.56666564941406,
          174.8000030517578,
          174.8333282470703,
          175.1666717529297,
          175.3333282470703,
          175.63333129882812,
          175.8000030517578,
          175.89999389648438,
          175.96665954589844,
          176.03334045410156,
          176.10000610351562,
          176.26666259765625,
          176.36666870117188,
          176.5,
          176.60000610351562,
          176.6999969482422,
          176.76666259765625,
          177.26666259765625,
          177.3333282470703,
          177.53334045410156,
          177.60000610351562,
          177.63333129882812,
          177.89999389648438,
          178.1666717529297,
          178.39999389648438,
          178.56666564941406,
          178.63333129882812,
          178.73333740234375,
          179.10000610351562,
          180.56666564941406,
          180.63333129882812,
          180.8333282470703,
          181.23333740234375,
          181.86666870117188,
          182.23333740234375,
          182.3333282470703,
          182.5,
          182.60000610351562,
          182.8333282470703,
          182.89999389648438,
          182.93333435058594,
          183.43333435058594,
          183.96665954589844,
          184.1666717529297,
          184.26666259765625,
          184.3333282470703,
          184.39999389648438,
          184.6999969482422,
          184.8000030517578,
          185,
          185.13333129882812,
          185.1666717529297,
          185.3000030517578,
          185.3333282470703,
          185.60000610351562,
          185.6999969482422,
          185.93333435058594,
          186.10000610351562,
          186.13333129882812,
          186.1999969482422,
          186.36666870117188,
          186.39999389648438,
          186.43333435058594,
          187.03334045410156,
          187.3000030517578,
          187.86666870117188,
          187.93333435058594,
          188.3333282470703,
          188.53334045410156,
          189.10000610351562,
          189.13333129882812,
          189.43333435058594,
          189.86666870117188,
          189.89999389648438,
          190.10000610351562,
          190.1666717529297,
          190.1999969482422,
          191.13333129882812,
          191.23333740234375,
          191.46665954589844,
          191.8000030517578,
          191.93333435058594,
          192.13333129882812,
          192.1999969482422,
          192.60000610351562,
          193.13333129882812,
          193.1666717529297,
          193.6999969482422,
          193.96665954589844,
          194.10000610351562,
          194.1666717529297,
          194.23333740234375,
          194.3000030517578,
          194.36666870117188,
          194.39999389648438,
          194.53334045410156,
          194.56666564941406,
          194.6999969482422,
          195.3000030517578,
          195.36666870117188,
          195.53334045410156,
          195.6999969482422,
          195.93333435058594,
          196.53334045410156,
          196.56666564941406,
          196.63333129882812,
          196.86666870117188,
          197.43333435058594,
          197.6666717529297,
          197.73333740234375,
          197.8333282470703,
          198.10000610351562,
          198.13333129882812,
          198.3000030517578,
          198.3333282470703,
          198.43333435058594,
          198.60000610351562,
          199.03334045410156,
          199.10000610351562,
          199.13333129882812,
          199.3000030517578,
          199.36666870117188,
          199.53334045410156,
          199.93333435058594,
          199.96665954589844,
          200.10000610351562,
          200.13333129882812,
          200.43333435058594,
          200.6999969482422,
          200.76666259765625,
          201.1666717529297,
          201.46665954589844,
          201.76666259765625,
          201.8333282470703,
          201.89999389648438,
          201.93333435058594,
          202.10000610351562,
          202.76666259765625,
          203,
          203.26666259765625,
          203.6999969482422,
          204.1999969482422,
          204.39999389648438,
          204.43333435058594,
          205.03334045410156,
          205.73333740234375,
          206.13333129882812,
          206.56666564941406,
          207.1666717529297,
          207.63333129882812,
          207.96665954589844,
          208.39999389648438,
          208.96665954589844,
          209.03334045410156,
          209.26666259765625,
          210.96665954589844,
          211.1999969482422,
          211.39999389648438,
          211.53334045410156,
          211.89999389648438,
          211.93333435058594,
          211.96665954589844,
          212.1999969482422,
          212.36666870117188,
          212.6999969482422,
          213,
          213.10000610351562,
          213.1999969482422,
          213.36666870117188,
          213.5,
          213.89999389648438,
          214.43333435058594,
          214.6999969482422,
          214.8000030517578,
          215.1666717529297,
          216.73333740234375,
          216.86666870117188,
          216.96665954589844,
          217.10000610351562,
          217.56666564941406,
          217.63333129882812,
          217.76666259765625,
          218.3000030517578,
          219.1666717529297,
          219.46665954589844,
          219.6666717529297,
          219.76666259765625,
          220.03334045410156,
          220.23333740234375,
          220.3000030517578,
          220.89999389648438,
          220.93333435058594,
          221.06666564941406,
          221.1999969482422,
          221.23333740234375,
          221.60000610351562,
          221.76666259765625,
          221.89999389648438,
          221.93333435058594,
          222.03334045410156,
          222.10000610351562,
          222.1999969482422,
          222.3333282470703,
          222.6999969482422,
          223.3000030517578,
          223.8333282470703,
          224.10000610351562,
          224.23333740234375,
          224.3000030517578,
          224.43333435058594,
          224.56666564941406,
          224.60000610351562,
          224.86666870117188,
          225.1666717529297,
          225.39999389648438,
          225.5,
          225.6999969482422,
          226.06666564941406,
          226.1999969482422,
          226.6999969482422,
          226.73333740234375,
          227.46665954589844,
          227.73333740234375,
          227.8000030517578,
          227.8333282470703,
          227.86666870117188,
          227.89999389648438,
          228,
          228.10000610351562,
          228.3333282470703,
          228.76666259765625,
          228.8000030517578,
          228.89999389648438,
          229.06666564941406,
          229.36666870117188,
          229.8333282470703,
          229.89999389648438,
          230.1666717529297,
          230.46665954589844,
          230.5,
          231.03334045410156,
          231.6666717529297,
          232.39999389648438,
          232.73333740234375,
          232.76666259765625,
          232.96665954589844,
          233.86666870117188,
          234.13333129882812,
          234.3333282470703,
          234.39999389648438,
          234.43333435058594,
          234.60000610351562,
          235.39999389648438,
          235.56666564941406,
          235.6666717529297,
          236.03334045410156,
          236.06666564941406,
          236.13333129882812,
          236.63333129882812,
          236.6999969482422,
          237.26666259765625,
          237.36666870117188,
          237.5,
          238.06666564941406,
          238.5,
          239.1666717529297,
          239.23333740234375,
          239.3000030517578,
          239.3333282470703,
          239.39999389648438,
          240.03334045410156,
          240.43333435058594,
          240.46665954589844,
          240.6999969482422,
          240.8333282470703,
          241.39999389648438,
          241.60000610351562,
          241.86666870117188,
          242.56666564941406,
          243.1666717529297,
          243.5,
          244.1999969482422,
          244.43333435058594,
          245.23333740234375,
          245.5,
          246.60000610351562,
          247,
          247.36666870117188,
          247.8333282470703,
          248.76666259765625,
          249.53334045410156,
          250.6666717529297,
          250.8000030517578,
          250.8333282470703,
          251.06666564941406,
          251.1999969482422,
          251.63333129882812,
          251.8000030517578,
          252,
          252.10000610351562,
          252.26666259765625,
          252.3000030517578,
          252.3333282470703,
          252.86666870117188,
          252.96665954589844,
          253.53334045410156,
          254.5,
          254.63333129882812,
          254.93333435058594,
          254.96665954589844,
          255,
          255.3000030517578,
          255.36666870117188,
          255.60000610351562,
          256.8666687011719,
          257.1666564941406,
          257.5666809082031,
          257.6333312988281,
          257.6666564941406,
          258.1666564941406,
          258.3333435058594,
          258.8666687011719,
          259.5333251953125,
          259.76666259765625,
          260,
          260.20001220703125,
          260.73333740234375,
          261.20001220703125,
          262,
          262.1333312988281,
          262.8666687011719,
          263.23333740234375,
          263.3666687011719,
          263.4333190917969,
          263.6000061035156,
          263.70001220703125,
          264.23333740234375,
          264.6000061035156,
          264.76666259765625,
          265,
          265.1333312988281,
          265.5666809082031,
          265.9333190917969,
          266.1000061035156,
          266.9333190917969,
          267.23333740234375,
          267.26666259765625,
          267.3999938964844,
          268.4333190917969,
          268.8999938964844,
          269,
          269.3333435058594,
          269.6333312988281,
          269.8999938964844,
          270.1333312988281,
          270.29998779296875,
          270.5666809082031,
          271.26666259765625,
          271.3333435058594,
          271.8666687011719,
          271.9333190917969,
          272.1000061035156,
          272.8999938964844,
          274.0333251953125,
          274.20001220703125,
          274.3999938964844,
          274.5,
          275.23333740234375,
          275.6000061035156,
          275.6333312988281,
          275.73333740234375,
          277.3666687011719,
          278.26666259765625,
          278.4666748046875,
          279.1000061035156,
          279.79998779296875,
          280.70001220703125,
          281.3666687011719,
          281.5,
          282.3666687011719,
          282.5666809082031,
          282.8333435058594,
          285.70001220703125,
          287.23333740234375,
          287.9333190917969,
          291.1666564941406,
          292.0333251953125,
          292.6666564941406,
          295.3333435058594,
          296.8666687011719,
          297.23333740234375,
          297.79998779296875,
          298.0333251953125,
          299.3999938964844,
          300.70001220703125,
          300.8666687011719,
          301.23333740234375,
          307.6333312988281,
          318.20001220703125,
          322.8333435058594,
          330.3666687011719,
          335.6000061035156,
          337.0333251953125,
          355.20001220703125
         ],
         "xaxis": "x",
         "y": [
          0.9984044371141713,
          0.9984044371141713,
          0.9984044371141713,
          0.9984044371141713,
          0.9984044371141713,
          0.9984044371141713,
          0.9984044371141713,
          0.9984044371141713,
          0.9968030124338771,
          0.9968030124338771,
          0.995200285003737,
          0.995200285003737,
          0.9935967239952628,
          0.9919947918533862,
          0.9903938260357064,
          0.9903938260357064,
          0.9887937931339958,
          0.9871955476840749,
          0.9855995918400215,
          0.9840046013725592,
          0.9824096109056628,
          0.9808164606363805,
          0.9792230648666891,
          0.9776314435592062,
          0.9760415380947298,
          0.9744530903811977,
          0.972866474276069,
          0.9712813344308246,
          0.9696972704639519,
          0.9681136229890023,
          0.9665307911185649,
          0.963370783395554,
          0.9617919755839527,
          0.9617919755839527,
          0.9602136155008042,
          0.9602136155008042,
          0.9586361470216782,
          0.9570592383584411,
          0.9554829993733839,
          0.9539060958650638,
          0.9523309756254065,
          0.9507568022566395,
          0.949184019430589,
          0.947612350485827,
          0.9460425379373478,
          0.9429085132929731,
          0.9413421123208049,
          0.9397773251754009,
          0.936652402020224,
          0.9350910824365248,
          0.9335312819705562,
          0.9319723612143337,
          0.9304144430010136,
          0.9288586861682205,
          0.9257539278698079,
          0.9242023264722251,
          0.9226522517094833,
          0.9211035768459332,
          0.9164696451492436,
          0.9149274975791695,
          0.9133865024409018,
          0.9118470767222597,
          0.9118470767222597,
          0.9118470767222597,
          0.9103057761641974,
          0.9087659925293743,
          0.9072268036824644,
          0.9056890729175927,
          0.9041527217583971,
          0.9026180271678115,
          0.9026180271678115,
          0.9010845563159068,
          0.8995524884743258,
          0.8980221928729346,
          0.8980221928729346,
          0.8934375784705003,
          0.8919114619521457,
          0.8903867676904561,
          0.888864050431909,
          0.8873427318789989,
          0.8873427318789989,
          0.885821779679969,
          0.8843026348506157,
          0.8827852453589639,
          0.8812699271411123,
          0.8797562226186993,
          0.8782434747554904,
          0.8767321684652505,
          0.875222437039031,
          0.8737144149645143,
          0.872208191239104,
          0.8707026081583997,
          0.8676971594555172,
          0.8661959270744888,
          0.8646966987501484,
          0.8631984516841684,
          0.8617001890133532,
          0.8602029729426507,
          0.8587070644722128,
          0.8587070644722128,
          0.8572123308957361,
          0.8572123308957361,
          0.8557176904516857,
          0.8542249771425261,
          0.8527342246930943,
          0.8512454234929069,
          0.8497575844950976,
          0.846787505545494,
          0.8453038955042559,
          0.8453038955042559,
          0.8438200815636233,
          0.8438200815636233,
          0.8423377217791198,
          0.8423377217791198,
          0.8408558407127756,
          0.8393746543963733,
          0.8378941639818577,
          0.8364151431250407,
          0.8334619411953003,
          0.8319859040443884,
          0.8305116403390668,
          0.8290377984662899,
          0.8275659359331173,
          0.8260955815577565,
          0.824627035114846,
          0.823160188593101,
          0.8216945338647487,
          0.8202303277825577,
          0.8187676015137766,
          0.8173066471107948,
          0.8158469257808662,
          0.814387105283439,
          0.8129289880733749,
          0.811472374965251,
          0.8100160904217513,
          0.8085607930523185,
          0.8071073470893836,
          0.8056549105148798,
          0.8042039861151112,
          0.8027548342183292,
          0.801306587876814,
          0.7998594424932455,
          0.7984142273399463,
          0.7969704948400987,
          0.7940877104200186,
          0.79264863608834,
          0.7912092829959314,
          0.7897697643417683,
          0.7883318375566477,
          0.7868957265639401,
          0.7854612689483973,
          0.7840284257707204,
          0.7840284257707204,
          0.7825965244804136,
          0.7811642919942149,
          0.7797337206247581,
          0.778304913817586,
          0.7768780597575622,
          0.775452742758327,
          0.7740291794847596,
          0.772607886412917,
          0.7711878776869966,
          0.7711878776869966,
          0.7697681341519695,
          0.7683506657811,
          0.7669347895287936,
          0.7669347895287936,
          0.7655188741451858,
          0.7641040360288451,
          0.7626910607614744,
          0.7612796478814635,
          0.7598696774786063,
          0.7584609076958732,
          0.7542475816734664,
          0.7528439214500067,
          0.75144097542179,
          0.7500392827009584,
          0.7486392441060785,
          0.7472408364529155,
          0.7458434401975915,
          0.7444479819761829,
          0.7430538782683224,
          0.7416615403449137,
          0.7402698954841141,
          0.7388800738286201,
          0.7374915101578972,
          0.7361049652389772,
          0.7347203792594771,
          0.7333366631553551,
          0.7319537119269054,
          0.7305724384311489,
          0.7291924200059734,
          0.7278139237102208,
          0.7264371753866853,
          0.7250614653597174,
          0.7236870705492549,
          0.7223143818061842,
          0.720942273437366,
          0.716838108126386,
          0.7154720603519935,
          0.7141074570305452,
          0.7127438498572244,
          0.710021862128884,
          0.7086619940855158,
          0.7073031314880688,
          0.7059459268877001,
          0.7059459268877001,
          0.7045891858535325,
          0.7032336662064662,
          0.7018792004598213,
          0.7005260551792643,
          0.6991749424274425,
          0.6964766785940343,
          0.6951280151487829,
          0.69243618911939,
          0.6910910111101018,
          0.6897472028989167,
          0.6870639401558222,
          0.6857235218781332,
          0.6843844203839495,
          0.683046293844259,
          0.6817089072418282,
          0.6803733222976341,
          0.6777082368347609,
          0.6763773603952432,
          0.6750478366041373,
          0.6737199580660466,
          0.6723943202472183,
          0.6697494611867775,
          0.6684284439226593,
          0.6657921903798217,
          0.6644747795445846,
          0.6644747795445846,
          0.6618442414490573,
          0.6605308752741842,
          0.6592192705095454,
          0.6579090067640191,
          0.6565994720523178,
          0.6565994720523178,
          0.6552904973535332,
          0.65398257378642,
          0.65398257378642,
          0.652674756137867,
          0.651368945498154,
          0.6500649398456025,
          0.647462844916192,
          0.6461638252291679,
          0.6448658486750742,
          0.6435698286128992,
          0.6435698286128992,
          0.6422738841324545,
          0.6409793242950601,
          0.6383926109354959,
          0.6370992047833892,
          0.6358068746169776,
          0.6345157373801827,
          0.6332260041272536,
          0.630652762840349,
          0.6293687963630531,
          0.6280862296255417,
          0.6268055616155379,
          0.6255258906961166,
          0.6242471740955224,
          0.6229687059789475,
          0.6216907336530693,
          0.6204147370898645,
          0.6191394538602126,
          0.6178649200218135,
          0.6153229385320208,
          0.6140525624455103,
          0.610250250106705,
          0.608985167064252,
          0.608985167064252,
          0.6077211337896237,
          0.606458001958529,
          0.605196734913234,
          0.6039368547126877,
          0.6014212111222812,
          0.6001635553228399,
          0.598906268382905,
          0.5976503476424062,
          0.5963950354286381,
          0.5951408528886178,
          0.5938880623478838,
          0.5938880623478838,
          0.5926362934367226,
          0.5926362934367226,
          0.5913848230251031,
          0.5901343959516814,
          0.5888859131676423,
          0.587639779396868,
          0.5863939125961531,
          0.5851497887123288,
          0.583907317835862,
          0.5826667480895357,
          0.5814272672700938,
          0.5801896080284467,
          0.5777185691864929,
          0.5764845896008588,
          0.5752514343922889,
          0.5740200202533192,
          0.5727898703660056,
          0.5715616823489469,
          0.5703349645912597,
          0.5691101733964476,
          0.567886565389688,
          0.5666640796834396,
          0.5654436466390831,
          0.5642234628391583,
          0.5630049331628536,
          0.5617871997118525,
          0.5605703338052377,
          0.5593544366238667,
          0.5581399260600809,
          0.5569265822877979,
          0.555715019221192,
          0.5545048801876635,
          0.5545048801876635,
          0.5532941583833793,
          0.5520843335912666,
          0.5508765826266707,
          0.5496706407059282,
          0.5484653415277058,
          0.5484653415277058,
          0.5472615619451839,
          0.5472615619451839,
          0.5460580521414427,
          0.5448566105959152,
          0.5436564353692985,
          0.5424583355010515,
          0.5412612427940268,
          0.5388725365851338,
          0.5376804703412256,
          0.5376804703412256,
          0.5364892914339464,
          0.5364892914339464,
          0.5352982090046358,
          0.5341084220620018,
          0.5341084220620018,
          0.5341084220620018,
          0.5329179465017968,
          0.5317291715631385,
          0.5305409343804401,
          0.5293546290666105,
          0.5281697445888052,
          0.526986636271637,
          0.526986636271637,
          0.5258046178935006,
          0.5258046178935006,
          0.5246230245602903,
          0.5246230245602903,
          0.5246230245602903,
          0.5234422855716457,
          0.5222629032701985,
          0.5222629032701985,
          0.5222629032701985,
          0.5222629032701985,
          0.5199057656500193,
          0.5187274654705569,
          0.5175496925637052,
          0.5163741266816134,
          0.5163741266816134,
          0.5163741266816134,
          0.5151988394758997,
          0.5151988394758997,
          0.5140236534579138,
          0.5140236534579138,
          0.5128502144487875,
          0.5116770488269504,
          0.5105041795930758,
          0.5093315451307406,
          0.5081600587612415,
          0.5081600587612415,
          0.5069894896942263,
          0.5058198616540848,
          0.5046516385768863,
          0.5034849771513119,
          0.5034849771513119,
          0.5034849771513119,
          0.5023174685484699,
          0.5023174685484699,
          0.5011494707024483,
          0.4999828748774231,
          0.4988173178067938,
          0.497653766301419,
          0.4964907550972747,
          0.4964907550972747,
          0.49532811798925785,
          0.4941670713330527,
          0.4941670713330527,
          0.49300656251401426,
          0.4918470326588791,
          0.4918470326588791,
          0.48953132142518047,
          0.4883753435276717,
          0.4872207125589216,
          0.48606763156736094,
          0.4849162088417294,
          0.48376650455737713,
          0.4826177641473836,
          0.4814698714928637,
          0.4814698714928637,
          0.4814698714928637,
          0.4803210911449633,
          0.4791746058210379,
          0.4791746058210379,
          0.4791746058210379,
          0.4791746058210379,
          0.47802591812711975,
          0.4768785832384453,
          0.4757322117760354,
          0.4745877417953466,
          0.4745877417953466,
          0.47344492440569336,
          0.47230392407012967,
          0.47230392407012967,
          0.47230392407012967,
          0.47116218853891567,
          0.47116218853891567,
          0.47116218853891567,
          0.47116218853891567,
          0.47116218853891567,
          0.4700154314585601,
          0.46887091361322086,
          0.46887091361322086,
          0.4665858196500974,
          0.4654446896555553,
          0.4654446896555553,
          0.46430407939645396,
          0.46430407939645396,
          0.4631635518640522,
          0.46202409198312233,
          0.4608855659149746,
          0.45974807975798054,
          0.45861241352227433,
          0.4574784094140885,
          0.4574784094140885,
          0.45634468680418144,
          0.4552099847890198,
          0.4552099847890198,
          0.4540747998765804,
          0.45294069258065767,
          0.45180713885768675,
          0.45180713885768675,
          0.45180713885768675,
          0.45180713885768675,
          0.45180713885768675,
          0.4506717409521959,
          0.4506717409521959,
          0.44953743260873025,
          0.44840434648714755,
          0.44726790721139387,
          0.44613255319156575,
          0.4449990472731554,
          0.4438677137208927,
          0.4438677137208927,
          0.4438677137208927,
          0.44273599375674827,
          0.4416051419286406,
          0.4404755739604052,
          0.43934772324178295,
          0.43934772324178295,
          0.4382196194035978,
          0.43709308228118565,
          0.4359690298881954,
          0.4348471482901576,
          0.4348471482901576,
          0.4337247168744779,
          0.432604345592003,
          0.432604345592003,
          0.43036569704194133,
          0.43036569704194133,
          0.43036569704194133,
          0.4281244412346167,
          0.4270049400449123,
          0.4247719600575727,
          0.42365557525307096,
          0.4225410605562386,
          0.42142821041538475,
          0.42031754561314183,
          0.4192078972598643,
          0.4192078972598643,
          0.41809972491223085,
          0.4169931655523622,
          0.4169931655523622,
          0.4147835852048129,
          0.4136808312787525,
          0.4125796398812964,
          0.41147980256430805,
          0.40928646639266203,
          0.4081907686618138,
          0.4070974234033355,
          0.4070974234033355,
          0.406005759654914,
          0.4049162075554591,
          0.4038288521780802,
          0.4027427900959894,
          0.4027427900959894,
          0.4016567688025106,
          0.4016567688025106,
          0.4016567688025106,
          0.4016567688025106,
          0.400568725978675,
          0.3994821540070359,
          0.3994821540070359,
          0.39839669453214954,
          0.3962310003512055,
          0.3962310003512055,
          0.3962310003512055,
          0.39407178659481434,
          0.3929952182549854,
          0.3929952182549854,
          0.39192014120910573,
          0.39192014120910573,
          0.39192014120910573,
          0.39192014120910573,
          0.3908433739856225,
          0.3897681358243135,
          0.38762399958304894,
          0.3865534683423845,
          0.3854840072102734,
          0.3844158071827409,
          0.3844158071827409,
          0.3844158071827409,
          0.38334816008810263,
          0.38334816008810263,
          0.38334816008810263,
          0.3822813828869724,
          0.3812160065366217,
          0.3801519125025921,
          0.3790894755722779,
          0.37697016680823753,
          0.3759126487704654,
          0.3748553029309794,
          0.3737990546062013,
          0.3737990546062013,
          0.3727430311543789,
          0.371688025908332,
          0.37063432216663283,
          0.3695822259230284,
          0.368532468621612,
          0.3674838113016632,
          0.3664363523145262,
          0.3653909463235142,
          0.36330743780518254,
          0.36330743780518254,
          0.3622652400272663,
          0.36122407690188674,
          0.36018421545992896,
          0.36018421545992896,
          0.36018421545992896,
          0.36018421545992896,
          0.35914344199864234,
          0.35810341663754197,
          0.35706463778190073,
          0.35602781012945534,
          0.35499227854999876,
          0.3539589028160957,
          0.3539589028160957,
          0.35292572321403026,
          0.3518940793752446,
          0.3508640107522675,
          0.3508640107522675,
          0.3508640107522675,
          0.34983564240279413,
          0.3488079821276125,
          0.3477825257711301,
          0.34675910217876177,
          0.3457375648528967,
          0.3447167677470632,
          0.34369824898922197,
          0.34268165610423174,
          0.3406536875197075,
          0.3406536875197075,
          0.33963992254599074,
          0.33963992254599074,
          0.33862646947774383,
          0.33761385635755414,
          0.33660269508694035,
          0.33660269508694035,
          0.33660269508694035,
          0.3355903456247727,
          0.3355903456247727,
          0.3345800316288124,
          0.3335717361364916,
          0.33256565537122756,
          0.3315602373454039,
          0.3315602373454039,
          0.3305554148450696,
          0.3295519057969904,
          0.3295519057969904,
          0.32854876270291,
          0.32754730608788035,
          0.3265473063421198,
          0.3255482527874929,
          0.3245508095276851,
          0.3245508095276851,
          0.3245508095276851,
          0.32355252399485934,
          0.3225562986409697,
          0.3215621102313991,
          0.3215621102313991,
          0.3215621102313991,
          0.3205682577623778,
          0.3205682577623778,
          0.31957421388152707,
          0.31857958106498735,
          0.31857958106498735,
          0.31857958106498735,
          0.3175850742964499,
          0.3165924296092569,
          0.3165924296092569,
          0.3165924296092569,
          0.3155999470261017,
          0.31362023925706367,
          0.31263108786102795,
          0.31164244667951313,
          0.31065621504369095,
          0.3096714705751236,
          0.3086884435814978,
          0.3077070224882001,
          0.3067264976768692,
          0.30574685840752347,
          0.30574685840752347,
          0.30476882804626515,
          0.30476882804626515,
          0.3037903819010068,
          0.30281401869245933,
          0.30184009646332566,
          0.30184009646332566,
          0.3008666270925203,
          0.29989407725972506,
          0.29989407725972506,
          0.29989407725972506,
          0.2989209575169565,
          0.2979483417116598,
          0.29697711276745226,
          0.29697711276745226,
          0.2960064656472588,
          0.2950374527916087,
          0.2950374527916087,
          0.2950374527916087,
          0.2940686628968169,
          0.2940686628968169,
          0.29309903424824474,
          0.29309903424824474,
          0.2921297028204661,
          0.2921297028204661,
          0.2921297028204661,
          0.29115954260965893,
          0.29115954260965893,
          0.29115954260965893,
          0.2901888761683642,
          0.2892193923658455,
          0.2892193923658455,
          0.2882513391484034,
          0.2882513391484034,
          0.2882513391484034,
          0.2882513391484034,
          0.2882513391484034,
          0.2872813942689476,
          0.2863133597753183,
          0.2853466227311434,
          0.2853466227311434,
          0.2853466227311434,
          0.2843795205115542,
          0.2843795205115542,
          0.28341310642689393,
          0.28244724178343567,
          0.28244724178343567,
          0.28148140936720345,
          0.28051874394581905,
          0.28051874394581905,
          0.27955744536624205,
          0.27955744536624205,
          0.27955744536624205,
          0.27763876250613867,
          0.27668006156340413,
          0.27668006156340413,
          0.275722709342463,
          0.2747675253435555,
          0.2747675253435555,
          0.2747675253435555,
          0.2747675253435555,
          0.2747675253435555,
          0.273810891969045,
          0.27285654442720153,
          0.27285654442720153,
          0.27285654442720153,
          0.2719004580168884,
          0.27094592671722595,
          0.27094592671722595,
          0.27094592671722595,
          0.26999037868640635,
          0.26999037868640635,
          0.26903548726614895,
          0.26903548726614895,
          0.26808035330755153,
          0.26712711811428697,
          0.2661755165375072,
          0.26522560388586663,
          0.26522560388586663,
          0.26522560388586663,
          0.26522560388586663,
          0.26522560388586663,
          0.2642721745294038,
          0.2642721745294038,
          0.2614238982823762,
          0.2614238982823762,
          0.26047544997449595,
          0.2595294611985183,
          0.2585841302995387,
          0.25764134233707436,
          0.25764134233707436,
          0.25670043034466516,
          0.2557596727543049,
          0.2548205580611554,
          0.2548205580611554,
          0.25294174099171496,
          0.25294174099171496,
          0.25294174099171496,
          0.25200006814102827,
          0.25200006814102827,
          0.25105909660216125,
          0.25105909660216125,
          0.25105909660216125,
          0.25011577737379087,
          0.25011577737379087,
          0.25011577737379087,
          0.25011577737379087,
          0.25011577737379087,
          0.25011577737379087,
          0.24822420721684227,
          0.24633761674221416,
          0.24539616456318253,
          0.2444574516173112,
          0.2444574516173112,
          0.24351959248897212,
          0.24351959248897212,
          0.24351959248897212,
          0.24351959248897212,
          0.2425798955742105,
          0.2425798955742105,
          0.240704896107344,
          0.23976965364477373,
          0.23976965364477373,
          0.23883523207089605,
          0.23790201861053495,
          0.23696960554825836,
          0.2360386948770279,
          0.2341836058094946,
          0.2332587769879684,
          0.23233412870780346,
          0.23233412870780346,
          0.2314112578846859,
          0.23049038770592556,
          0.22865659797639742,
          0.22774348203100522,
          0.22683225301215681,
          0.22683225301215681,
          0.22683225301215681,
          0.22683225301215681,
          0.22683225301215681,
          0.22591763517287256,
          0.22591763517287256,
          0.2250034383103059,
          0.2250034383103059,
          0.22409066405570013,
          0.22409066405570013,
          0.22317813695604255,
          0.22317813695604255,
          0.22226693534632197,
          0.22135842900312402,
          0.22045149779205664,
          0.22045149779205664,
          0.22045149779205664,
          0.22045149779205664,
          0.21954335459864383,
          0.21954335459864383,
          0.21954335459864383,
          0.21954335459864383,
          0.21954335459864383,
          0.21954335459864383,
          0.2186327779578091,
          0.2186327779578091,
          0.21772355384404654,
          0.216816378676672,
          0.216816378676672,
          0.216816378676672,
          0.215907869452063,
          0.215907869452063,
          0.21500043919913,
          0.2140938861162303,
          0.2140938861162303,
          0.21318790117604736,
          0.21318790117604736,
          0.21318790117604736,
          0.21318790117604736,
          0.21318790117604736,
          0.21227960475145488,
          0.21227960475145488,
          0.21227960475145488,
          0.21227960475145488,
          0.21136875402092856,
          0.21136875402092856,
          0.21136875402092856,
          0.2104549241475253,
          0.2104549241475253,
          0.2104549241475253,
          0.2095411016481903,
          0.20862951270966698,
          0.20862951270966698,
          0.20862951270966698,
          0.20862951270966698,
          0.20862951270966698,
          0.20862951270966698,
          0.20862951270966698,
          0.20771257810810245,
          0.20771257810810245,
          0.2067953682947881,
          0.2067953682947881,
          0.2058804536689484,
          0.2058804536689484,
          0.2058804536689484,
          0.2058804536689484,
          0.2058804536689484,
          0.2058804536689484,
          0.20496181688562953,
          0.2040458422793294,
          0.2040458422793294,
          0.2040458422793294,
          0.2040458422793294,
          0.20313055991957202,
          0.20130798614155948,
          0.2004006139704311,
          0.2004006139704311,
          0.1994925272506723,
          0.1994925272506723,
          0.1985857438872766,
          0.1985857438872766,
          0.19767639718805843,
          0.19676810516612875,
          0.19676810516612875,
          0.19676810516612875,
          0.19586024770430466,
          0.19586024770430466,
          0.1949526560391525,
          0.19404773461872005,
          0.19404773461872005,
          0.19404773461872005,
          0.19314230501460306,
          0.19223858511797606,
          0.19133693382692415,
          0.19133693382692415,
          0.19133693382692415,
          0.19133693382692415,
          0.1904331960414625,
          0.1904331960414625,
          0.1904331960414625,
          0.1895271861746622,
          0.1895271861746622,
          0.1886229758696158,
          0.1886229758696158,
          0.187720212927061,
          0.187720212927061,
          0.1868168409104344,
          0.1868168409104344,
          0.18591537469421318,
          0.18591537469421318,
          0.18591537469421318,
          0.1850131159069939,
          0.18411291350203218,
          0.18321611272716337,
          0.18321611272716337,
          0.1823196248349775,
          0.1823196248349775,
          0.1814248375059607,
          0.1814248375059607,
          0.1814248375059607,
          0.1814248375059607,
          0.18052679683487638,
          0.18052679683487638,
          0.18052679683487638,
          0.17784068168959757,
          0.1769492188283767,
          0.1769492188283767,
          0.1769492188283767,
          0.17605939900402215,
          0.17605939900402215,
          0.17516993776845258,
          0.17251891932564203,
          0.17251891932564203,
          0.17251891932564203,
          0.17163202370837033,
          0.1707484776513456,
          0.1707484776513456,
          0.1698652344453241,
          0.1698652344453241,
          0.1698652344453241,
          0.16898040794639702,
          0.16809786472322466,
          0.16721700638415626,
          0.16633717165592862,
          0.16633717165592862,
          0.16545865708586777,
          0.16545865708586777,
          0.1645821119603653,
          0.16370776349478477,
          0.16370776349478477,
          0.16370776349478477,
          0.16283047563378147,
          0.16283047563378147,
          0.161947678315677,
          0.16106806466103213,
          0.16106806466103213,
          0.16018824991059827,
          0.1593123982608259,
          0.15843876979901916,
          0.1575688158702027,
          0.1575688158702027,
          0.1575688158702027,
          0.1575688158702027,
          0.1566968109160003,
          0.1566968109160003,
          0.1566968109160003,
          0.15582251054510102,
          0.1549513786802407,
          0.15408403552613664,
          0.15321848558533674,
          0.15321848558533674,
          0.15321848558533674,
          0.15321848558533674,
          0.15321848558533674,
          0.15234630941831973,
          0.15234630941831973,
          0.15147523397561408,
          0.15147523397561408,
          0.15147523397561408,
          0.15147523397561408,
          0.1505993994760253,
          0.14972667682839774,
          0.14972667682839774,
          0.14972667682839774,
          0.14885337514441574,
          0.14885337514441574,
          0.14885337514441574,
          0.14885337514441574,
          0.14885337514441574,
          0.14885337514441574,
          0.14797462220957938,
          0.14709966776899228,
          0.14622469343195207,
          0.14622469343195207,
          0.14534910856157834,
          0.14447788295814532,
          0.14360933511637522,
          0.14360933511637522,
          0.14274098890120906,
          0.14187635094394174,
          0.14101414967243422,
          0.14015464663975788,
          0.14015464663975788,
          0.14015464663975788,
          0.1392953410531725,
          0.1392953410531725,
          0.1384372841591749,
          0.13758097758076362,
          0.13672760580510707,
          0.13672760580510707,
          0.13587698743166926,
          0.1350293504278122,
          0.13334514407254383,
          0.1325072994211679,
          0.13167275189527053,
          0.13167275189527053,
          0.13083810591246334,
          0.13000351549555392,
          0.12917185874382456,
          0.12834287453902074,
          0.12834287453902074,
          0.12834287453902074,
          0.12834287453902074,
          0.12834287453902074,
          0.12834287453902074,
          0.12750885409638418,
          0.12667850314169396,
          0.1258524078657643,
          0.1250284208291059,
          0.12420710074634259,
          0.12338842951141177,
          0.12338842951141177,
          0.12338842951141177,
          0.12338842951141177,
          0.12338842951141177,
          0.12338842951141177,
          0.12338842951141177,
          0.12338842951141177,
          0.12338842951141177,
          0.12338842951141177,
          0.12338842951141177,
          0.12255573752507555,
          0.12255573752507555,
          0.1217264235989254,
          0.1217264235989254,
          0.12089745472540252,
          0.12007193569029843,
          0.12007193569029843,
          0.12007193569029843,
          0.1192466442295903,
          0.11842230085667677,
          0.11842230085667677,
          0.11842230085667677,
          0.11759551459117226,
          0.11759551459117226,
          0.11676879236167607,
          0.11676879236167607,
          0.11676879236167607,
          0.11676879236167607,
          0.11676879236167607,
          0.11676879236167607,
          0.11676879236167607,
          0.11676879236167607,
          0.11676879236167607,
          0.11676879236167607,
          0.11591959898027009,
          0.11591959898027009,
          0.11507066368055718,
          0.11507066368055718,
          0.11507066368055718,
          0.11507066368055718,
          0.11507066368055718,
          0.11507066368055718,
          0.1142178870187302,
          0.1142178870187302,
          0.1142178870187302,
          0.1142178870187302,
          0.1142178870187302,
          0.1142178870187302,
          0.11335918363666427,
          0.11335918363666427,
          0.11250322229390006,
          0.11250322229390006,
          0.11164621805264588,
          0.11164621805264588,
          0.11164621805264588,
          0.1107884195253404,
          0.10993341052408218,
          0.10993341052408218,
          0.10993341052408218,
          0.10993341052408218,
          0.10993341052408218,
          0.10907850129501415,
          0.10907850129501415,
          0.10907850129501415,
          0.10822426252387073,
          0.10822426252387073,
          0.10737340688656062,
          0.10737340688656062,
          0.10567795768860594,
          0.10567795768860594,
          0.1048308350065667,
          0.1048308350065667,
          0.1048308350065667,
          0.1048308350065667,
          0.1048308350065667,
          0.1048308350065667,
          0.10396160797002937,
          0.10309736433313313,
          0.10309736433313313,
          0.10309736433313313,
          0.10309736433313313,
          0.10223020702715135,
          0.10223020702715135,
          0.10223020702715135,
          0.10136260781335527,
          0.10136260781335527,
          0.10049724804716265,
          0.10049724804716265,
          0.09963401794178825,
          0.09963401794178825,
          0.09963401794178825,
          0.09877299237039117,
          0.09877299237039117,
          0.09791539881456751,
          0.09791539881456751,
          0.09705770987026986,
          0.09620468603843166,
          0.0953527380344745,
          0.09366552569618002,
          0.09366552569618002,
          0.09366552569618002,
          0.09281843735357251,
          0.09281843735357251,
          0.09197437242223894,
          0.09113213147247579,
          0.09113213147247579,
          0.09113213147247579,
          0.09113213147247579,
          0.09029188063085834,
          0.09029188063085834,
          0.09029188063085834,
          0.09029188063085834,
          0.09029188063085834,
          0.09029188063085834,
          0.09029188063085834,
          0.09029188063085834,
          0.08944215081073957,
          0.08944215081073957,
          0.08944215081073957,
          0.08858947584728367,
          0.08774301425674468,
          0.0869021950318491,
          0.08606756177518912,
          0.0852386756350471,
          0.0852386756350471,
          0.08440643466717385,
          0.08440643466717385,
          0.08357727429660011,
          0.08275173897633334,
          0.08275173897633334,
          0.08192766187840843,
          0.08192766187840843,
          0.08192766187840843,
          0.08110029871449267,
          0.0802745707453494,
          0.0802745707453494,
          0.0802745707453494,
          0.07944862613773766,
          0.07944862613773766,
          0.0786214388770691,
          0.07779585985343868,
          0.07779585985343868,
          0.07697226838036664,
          0.0761522114792596,
          0.0761522114792596,
          0.07533289366663187,
          0.07451819504423415,
          0.07451819504423415,
          0.07451819504423415,
          0.07370215132093122,
          0.07288807585235174,
          0.07288807585235174,
          0.07288807585235174,
          0.07207167584243322,
          0.07125899351897694,
          0.07125899351897694,
          0.07125899351897694,
          0.07125899351897694,
          0.07043980604592348,
          0.07043980604592348,
          0.06962412364859373,
          0.06962412364859373,
          0.0688114219880654,
          0.06800405130269112,
          0.06800405130269112,
          0.06719216514355594,
          0.06638738988816681,
          0.06558735714604297,
          0.06558735714604297,
          0.06478972302592655,
          0.06322015050674491,
          0.06322015050674491,
          0.06244122195027385,
          0.061668892534222954,
          0.061668892534222954,
          0.061668892534222954,
          0.061668892534222954,
          0.060889670524080085,
          0.06011646685493532,
          0.05934909615149886,
          0.05934909615149886,
          0.05934909615149886,
          0.05857975666117748,
          0.05781535928696995,
          0.05781535928696995,
          0.05781535928696995,
          0.057047232456919165,
          0.057047232456919165,
          0.05628161898521266,
          0.05628161898521266,
          0.05551504855131415,
          0.05551504855131415,
          0.05474622509280701,
          0.053982802265835905,
          0.053982802265835905,
          0.053982802265835905,
          0.053982802265835905,
          0.053982802265835905,
          0.053982802265835905,
          0.053982802265835905,
          0.053982802265835905,
          0.053982802265835905,
          0.053982802265835905,
          0.053195248354931376,
          0.053195248354931376,
          0.05240902600910619,
          0.05240902600910619,
          0.05240902600910619,
          0.05240902600910619,
          0.05161232987723964,
          0.05082378982406385,
          0.05004164741003032,
          0.04926900118978445,
          0.048505577991306406,
          0.048505577991306406,
          0.04774390068579583,
          0.04774390068579583,
          0.046990035487637846,
          0.046990035487637846,
          0.046990035487637846,
          0.046990035487637846,
          0.04622906077408682,
          0.04622906077408682,
          0.04622906077408682,
          0.04622906077408682,
          0.04622906077408682,
          0.04622906077408682,
          0.045445851369689774,
          0.045445851369689774,
          0.045445851369689774,
          0.04466196565247335,
          0.04389063676020901,
          0.04389063676020901,
          0.04312170761199856,
          0.0423579064253603,
          0.04160440875682618,
          0.04160440875682618,
          0.04085509702766786,
          0.04085509702766786,
          0.04085509702766786,
          0.040110384189020104,
          0.040110384189020104,
          0.040110384189020104,
          0.040110384189020104,
          0.040110384189020104,
          0.040110384189020104,
          0.040110384189020104,
          0.03934496991509905,
          0.03858056023450671,
          0.03858056023450671,
          0.03780752271758743,
          0.03780752271758743,
          0.03780752271758743,
          0.03780752271758743,
          0.03780752271758743,
          0.03780752271758743,
          0.03780752271758743,
          0.037002348213837244,
          0.037002348213837244,
          0.037002348213837244,
          0.037002348213837244,
          0.037002348213837244,
          0.037002348213837244,
          0.036183717242980314,
          0.036183717242980314,
          0.036183717242980314,
          0.036183717242980314,
          0.036183717242980314,
          0.03535014533477308,
          0.03535014533477308,
          0.03451851402415784,
          0.03451851402415784,
          0.03368315568367224,
          0.03368315568367224,
          0.03368315568367224,
          0.03283527264366921,
          0.03283527264366921,
          0.03198608739421481,
          0.03198608739421481,
          0.03198608739421481,
          0.03198608739421481,
          0.03198608739421481,
          0.0311116466185751,
          0.0311116466185751,
          0.030250003907559917,
          0.030250003907559917,
          0.030250003907559917,
          0.029382323743357124,
          0.02852665630929747,
          0.02852665630929747,
          0.027689883691356176,
          0.027689883691356176,
          0.02685821420038263,
          0.02685821420038263,
          0.02685821420038263,
          0.02685821420038263,
          0.02685821420038263,
          0.026012613051162165,
          0.026012613051162165,
          0.026012613051162165,
          0.026012613051162165,
          0.02513887419584908,
          0.02513887419584908,
          0.02513887419584908,
          0.02513887419584908,
          0.02513887419584908,
          0.024260365487146975,
          0.024260365487146975,
          0.024260365487146975,
          0.024260365487146975,
          0.024260365487146975,
          0.024260365487146975,
          0.024260365487146975,
          0.024260365487146975,
          0.024260365487146975,
          0.024260365487146975,
          0.024260365487146975,
          0.023235374012310565,
          0.023235374012310565,
          0.023235374012310565,
          0.023235374012310565,
          0.02220595833105015,
          0.021194268794049495,
          0.021194268794049495,
          0.021194268794049495,
          0.02017816500450069,
          0.02017816500450069,
          0.02017816500450069,
          0.01916849771883388,
          0.01916849771883388,
          0.018148277446059357,
          0.018148277446059357,
          0.018148277446059357,
          0.018148277446059357,
          0.018148277446059357,
          0.017118402323577468,
          0.017118402323577468,
          0.017118402323577468,
          0.017118402323577468,
          0.017118402323577468,
          0.017118402323577468,
          0.016040545085810667,
          0.016040545085810667,
          0.016040545085810667,
          0.016040545085810667,
          0.016040545085810667,
          0.016040545085810667,
          0.016040545085810667,
          0.016040545085810667,
          0.016040545085810667,
          0.016040545085810667,
          0.016040545085810667,
          0.016040545085810667,
          0.016040545085810667,
          0.016040545085810667,
          0.016040545085810667,
          0.016040545085810667,
          0.016040545085810667,
          0.014637863225366353,
          0.014637863225366353,
          0.014637863225366353,
          0.014637863225366353,
          0.014637863225366353,
          0.014637863225366353,
          0.013109700377138954,
          0.013109700377138954,
          0.013109700377138954,
          0.01152344461043468,
          0.01005973635677986,
          0.00870921819486272,
          0.00870921819486272,
          0.00870921819486272,
          0.00870921819486272,
          0.007311029383329479,
          0.006068859093211093,
          0.00498468540287497,
          0.00498468540287497,
          0.00498468540287497,
          0.00498468540287497,
          0.0038279571386875544,
          0.0038279571386875544,
          0.0027145151583368135,
          0.0027145151583368135,
          0.0027145151583368135,
          0.0027145151583368135,
          0.0027145151583368135,
          0.0027145151583368135,
          0.0027145151583368135,
          0.0008539555673400182,
          0.0008539555673400182,
          0.000030015413729683752
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "legend": {
         "tracegroupgap": 0
        },
        "margin": {
         "t": 60
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "x"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "y"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly_express as px\n",
    "\n",
    "px.line(x =uniq_times_old, y = p1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1523, 1)\n",
      "(1381, 1522)\n",
      "cum_hazard_baseline_old (1381,)\n",
      "cum_hazard_baseline_old_new.shape (381, 1)\n",
      "preds (381, 381)\n",
      "uni_times.shape (374,)\n",
      "y_train (1522,)\n",
      "y_test (381,)\n",
      "preds (381, 374)\n",
      "times (374,)\n",
      "Integrated Brier Score: 0.551\n"
     ]
    }
   ],
   "source": [
    "from sksurv.util import Surv\n",
    "from sksurv.metrics import integrated_brier_score\n",
    "data = load_metabric(path=\"/Users/JUSC/Documents/xgbsurv/xgbsurv/datasets/data/\", as_frame=False)\n",
    "X  = data.data\n",
    "y = data.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)\n",
    "\n",
    "# use from arrays and then build the two survival struc arrays\n",
    "# do test predictions in shape of (n_samples, n_times) and put in unique times\n",
    "# for the last time point the unique time point has to be decremente by a tiny bit!\n",
    "time, event = transform_back(y_train)\n",
    "time_train, event_train = transform_back(y_train)\n",
    "y_train = Surv.from_arrays( event_train.astype(dtype=bool),time_train)\n",
    "time_test, event_test = transform_back(y_test)\n",
    "y_test = Surv.from_arrays(event_test.astype(dtype=bool),time_test)\n",
    "uniq_times_old, baseline_hazard, cum_hazard_baseline_old, baseline_survival_old = breslow_estimator(best_preds_train, time, event)\n",
    "print('cum_hazard_baseline_old', cum_hazard_baseline_old.shape)\n",
    "# cumbaseline hazard for each (unique) time step of the test data\n",
    "cum_hazard_baseline_old_new = np.interp(time_test, np.unique(time_train), cum_hazard_baseline_old)\n",
    "best_preds_test = np.ones(y_test.shape[0])\n",
    "print('cum_hazard_baseline_old_new.shape',cum_hazard_baseline_old_new[:, None].shape)\n",
    "#preds = np.exp(-(cum_hazard_baseline_old_new[:,None]*np.exp(best_preds_test)))\n",
    "# as many as there unique time steps\n",
    "#cum_hazard_baseline_old_new = np.ones((334))\n",
    "preds = np.exp(-(np.outer(np.exp(best_preds_test), np.exp(cum_hazard_baseline_old_new))))\n",
    "print('preds',preds.shape)\n",
    "preds = np.ones((381,374))\n",
    "uni_times = np.unique(time_test)\n",
    "print('uni_times.shape',uni_times.shape)\n",
    "times = np.unique(time_test)\n",
    "#times = np.arange(time_test.min(),time_test.max())\n",
    "times = np.delete(times,-1)\n",
    "# the highest time has to be decremented, but at the same time only a range seems to work\n",
    "times = np.append(times,337.03333-0.1)\n",
    "print('y_train',y_train.shape)\n",
    "print('y_test',y_test.shape)\n",
    "print('preds',preds.shape)\n",
    "print('times',times.shape)\n",
    "ibs = integrated_brier_score(y_train, y_test, preds, times)\n",
    "print(\"Integrated Brier Score: {:.3f}\".format(ibs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2733391844.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[20], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    hazard for each unique time step multiplied by the predicted hazard\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "## process\n",
    "hazard for each unique time step multiplied by the predicted hazard\n",
    "applying surv function exp(-x)\n",
    "preds hence (individuals, unique times)\n",
    "unique times with the last value slightly decremented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7666667 337.03333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([  3.76666665,   4.76666665,   5.76666665,   6.76666665,\n",
       "         7.76666665,   8.76666665,   9.76666665,  10.76666665,\n",
       "        11.76666665,  12.76666665,  13.76666665,  14.76666665,\n",
       "        15.76666665,  16.76666665,  17.76666665,  18.76666665,\n",
       "        19.76666665,  20.76666665,  21.76666665,  22.76666665,\n",
       "        23.76666665,  24.76666665,  25.76666665,  26.76666665,\n",
       "        27.76666665,  28.76666665,  29.76666665,  30.76666665,\n",
       "        31.76666665,  32.76666665,  33.76666665,  34.76666665,\n",
       "        35.76666665,  36.76666665,  37.76666665,  38.76666665,\n",
       "        39.76666665,  40.76666665,  41.76666665,  42.76666665,\n",
       "        43.76666665,  44.76666665,  45.76666665,  46.76666665,\n",
       "        47.76666665,  48.76666665,  49.76666665,  50.76666665,\n",
       "        51.76666665,  52.76666665,  53.76666665,  54.76666665,\n",
       "        55.76666665,  56.76666665,  57.76666665,  58.76666665,\n",
       "        59.76666665,  60.76666665,  61.76666665,  62.76666665,\n",
       "        63.76666665,  64.76666665,  65.76666665,  66.76666665,\n",
       "        67.76666665,  68.76666665,  69.76666665,  70.76666665,\n",
       "        71.76666665,  72.76666665,  73.76666665,  74.76666665,\n",
       "        75.76666665,  76.76666665,  77.76666665,  78.76666665,\n",
       "        79.76666665,  80.76666665,  81.76666665,  82.76666665,\n",
       "        83.76666665,  84.76666665,  85.76666665,  86.76666665,\n",
       "        87.76666665,  88.76666665,  89.76666665,  90.76666665,\n",
       "        91.76666665,  92.76666665,  93.76666665,  94.76666665,\n",
       "        95.76666665,  96.76666665,  97.76666665,  98.76666665,\n",
       "        99.76666665, 100.76666665, 101.76666665, 102.76666665,\n",
       "       103.76666665, 104.76666665, 105.76666665, 106.76666665,\n",
       "       107.76666665, 108.76666665, 109.76666665, 110.76666665,\n",
       "       111.76666665, 112.76666665, 113.76666665, 114.76666665,\n",
       "       115.76666665, 116.76666665, 117.76666665, 118.76666665,\n",
       "       119.76666665, 120.76666665, 121.76666665, 122.76666665,\n",
       "       123.76666665, 124.76666665, 125.76666665, 126.76666665,\n",
       "       127.76666665, 128.76666665, 129.76666665, 130.76666665,\n",
       "       131.76666665, 132.76666665, 133.76666665, 134.76666665,\n",
       "       135.76666665, 136.76666665, 137.76666665, 138.76666665,\n",
       "       139.76666665, 140.76666665, 141.76666665, 142.76666665,\n",
       "       143.76666665, 144.76666665, 145.76666665, 146.76666665,\n",
       "       147.76666665, 148.76666665, 149.76666665, 150.76666665,\n",
       "       151.76666665, 152.76666665, 153.76666665, 154.76666665,\n",
       "       155.76666665, 156.76666665, 157.76666665, 158.76666665,\n",
       "       159.76666665, 160.76666665, 161.76666665, 162.76666665,\n",
       "       163.76666665, 164.76666665, 165.76666665, 166.76666665,\n",
       "       167.76666665, 168.76666665, 169.76666665, 170.76666665,\n",
       "       171.76666665, 172.76666665, 173.76666665, 174.76666665,\n",
       "       175.76666665, 176.76666665, 177.76666665, 178.76666665,\n",
       "       179.76666665, 180.76666665, 181.76666665, 182.76666665,\n",
       "       183.76666665, 184.76666665, 185.76666665, 186.76666665,\n",
       "       187.76666665, 188.76666665, 189.76666665, 190.76666665,\n",
       "       191.76666665, 192.76666665, 193.76666665, 194.76666665,\n",
       "       195.76666665, 196.76666665, 197.76666665, 198.76666665,\n",
       "       199.76666665, 200.76666665, 201.76666665, 202.76666665,\n",
       "       203.76666665, 204.76666665, 205.76666665, 206.76666665,\n",
       "       207.76666665, 208.76666665, 209.76666665, 210.76666665,\n",
       "       211.76666665, 212.76666665, 213.76666665, 214.76666665,\n",
       "       215.76666665, 216.76666665, 217.76666665, 218.76666665,\n",
       "       219.76666665, 220.76666665, 221.76666665, 222.76666665,\n",
       "       223.76666665, 224.76666665, 225.76666665, 226.76666665,\n",
       "       227.76666665, 228.76666665, 229.76666665, 230.76666665,\n",
       "       231.76666665, 232.76666665, 233.76666665, 234.76666665,\n",
       "       235.76666665, 236.76666665, 237.76666665, 238.76666665,\n",
       "       239.76666665, 240.76666665, 241.76666665, 242.76666665,\n",
       "       243.76666665, 244.76666665, 245.76666665, 246.76666665,\n",
       "       247.76666665, 248.76666665, 249.76666665, 250.76666665,\n",
       "       251.76666665, 252.76666665, 253.76666665, 254.76666665,\n",
       "       255.76666665, 256.76666665, 257.76666665, 258.76666665,\n",
       "       259.76666665, 260.76666665, 261.76666665, 262.76666665,\n",
       "       263.76666665, 264.76666665, 265.76666665, 266.76666665,\n",
       "       267.76666665, 268.76666665, 269.76666665, 270.76666665,\n",
       "       271.76666665, 272.76666665, 273.76666665, 274.76666665,\n",
       "       275.76666665, 276.76666665, 277.76666665, 278.76666665,\n",
       "       279.76666665, 280.76666665, 281.76666665, 282.76666665,\n",
       "       283.76666665, 284.76666665, 285.76666665, 286.76666665,\n",
       "       287.76666665, 288.76666665, 289.76666665, 290.76666665,\n",
       "       291.76666665, 292.76666665, 293.76666665, 294.76666665,\n",
       "       295.76666665, 296.76666665, 297.76666665, 298.76666665,\n",
       "       299.76666665, 300.76666665, 301.76666665, 302.76666665,\n",
       "       303.76666665, 304.76666665, 305.76666665, 306.76666665,\n",
       "       307.76666665, 308.76666665, 309.76666665, 310.76666665,\n",
       "       311.76666665, 312.76666665, 313.76666665, 314.76666665,\n",
       "       315.76666665, 316.76666665, 317.76666665, 318.76666665,\n",
       "       319.76666665, 320.76666665, 321.76666665, 322.76666665,\n",
       "       323.76666665, 324.76666665, 325.76666665, 326.76666665,\n",
       "       327.76666665, 328.76666665, 329.76666665, 330.76666665,\n",
       "       331.76666665, 332.76666665, 333.76666665, 334.76666665,\n",
       "       335.76666665, 336.76666665])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(time_test.min(),time_test.max())\n",
    "times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  3.7666667,   5.5      ,   5.8333335,   7.8      ,   9.133333 ,\n",
       "        10.633333 ,  10.833333 ,  11.866667 ,  12.266666 ,  13.4      ,\n",
       "        14.7      ,  15.2      ,  16.566668 ,  16.6      ,  16.7      ,\n",
       "        17.666666 ,  17.833334 ,  18.266666 ,  18.833334 ,  20.2      ,\n",
       "        20.433332 ,  21.       ,  22.133333 ,  22.233334 ,  23.2      ,\n",
       "        23.8      ,  23.833334 ,  23.9      ,  23.933332 ,  24.633333 ,\n",
       "        24.866667 ,  25.233334 ,  25.433332 ,  25.633333 ,  26.333334 ,\n",
       "        27.466667 ,  27.866667 ,  28.566668 ,  28.833334 ,  29.066668 ,\n",
       "        29.3      ,  30.8      ,  31.166666 ,  32.066666 ,  32.733334 ,\n",
       "        32.833332 ,  32.866665 ,  33.133335 ,  34.3      ,  34.333332 ,\n",
       "        34.566666 ,  34.633335 ,  34.7      ,  34.766666 ,  35.       ,\n",
       "        35.2      ,  36.4      ,  36.633335 ,  37.       ,  37.5      ,\n",
       "        37.733334 ,  38.133335 ,  38.8      ,  41.466667 ,  41.833332 ,\n",
       "        42.633335 ,  42.666668 ,  42.9      ,  43.1      ,  43.2      ,\n",
       "        43.833332 ,  43.9      ,  44.733334 ,  45.166668 ,  45.6      ,\n",
       "        45.7      ,  45.733334 ,  46.066666 ,  46.366665 ,  46.666668 ,\n",
       "        47.033333 ,  47.433334 ,  48.133335 ,  48.433334 ,  48.533333 ,\n",
       "        49.533333 ,  49.566666 ,  49.766666 ,  50.533333 ,  50.766666 ,\n",
       "        51.7      ,  51.766666 ,  52.066666 ,  52.3      ,  52.5      ,\n",
       "        52.633335 ,  54.1      ,  55.833332 ,  56.333332 ,  56.766666 ,\n",
       "        58.133335 ,  58.933334 ,  59.7      ,  60.7      ,  61.1      ,\n",
       "        61.8      ,  61.9      ,  62.133335 ,  62.333332 ,  62.766666 ,\n",
       "        63.033333 ,  63.5      ,  63.833332 ,  63.866665 ,  64.6      ,\n",
       "        68.76667  ,  69.4      ,  69.8      ,  70.066666 ,  70.6      ,\n",
       "        71.6      ,  71.63333  ,  72.26667  ,  72.9      ,  73.13333  ,\n",
       "        73.46667  ,  74.46667  ,  76.23333  ,  78.166664 ,  78.6      ,\n",
       "        78.76667  ,  79.3      ,  80.433334 ,  80.5      ,  80.833336 ,\n",
       "        81.1      ,  81.13333  ,  82.63333  ,  83.53333  ,  85.13333  ,\n",
       "        85.73333  ,  85.86667  ,  86.066666 ,  86.23333  ,  87.       ,\n",
       "        88.46667  ,  88.933334 ,  89.36667  ,  89.9      ,  89.96667  ,\n",
       "        90.4      ,  93.36667  ,  94.23333  ,  94.933334 ,  95.833336 ,\n",
       "        96.2      ,  96.96667  ,  97.26667  ,  97.3      ,  98.76667  ,\n",
       "        98.833336 ,  99.36667  , 100.86667  , 101.066666 , 101.23333  ,\n",
       "       101.4      , 102.03333  , 102.066666 , 102.7      , 104.       ,\n",
       "       104.4      , 104.76667  , 105.2      , 105.96667  , 106.8      ,\n",
       "       107.066666 , 108.3      , 108.433334 , 111.1      , 111.36667  ,\n",
       "       111.6      , 112.       , 112.46667  , 112.8      , 112.933334 ,\n",
       "       113.433334 , 114.76667  , 114.9      , 115.6      , 115.63333  ,\n",
       "       117.03333  , 117.666664 , 117.76667  , 118.       , 118.3      ,\n",
       "       118.53333  , 119.       , 121.96667  , 122.13333  , 122.2      ,\n",
       "       122.7      , 123.26667  , 124.13333  , 124.566666 , 125.9      ,\n",
       "       126.666664 , 127.53333  , 127.833336 , 128.96666  , 129.23334  ,\n",
       "       129.33333  , 130.43333  , 130.9      , 131.06667  , 131.13333  ,\n",
       "       131.66667  , 134.26666  , 135.3      , 135.33333  , 136.       ,\n",
       "       136.06667  , 136.16667  , 137.8      , 138.33333  , 139.16667  ,\n",
       "       139.3      , 139.6      , 140.23334  , 140.5      , 140.6      ,\n",
       "       140.76666  , 142.43333  , 143.       , 143.53334  , 143.6      ,\n",
       "       144.46666  , 144.66667  , 144.96666  , 145.5      , 146.9      ,\n",
       "       146.93333  , 147.93333  , 149.4      , 149.7      , 149.76666  ,\n",
       "       149.86667  , 150.46666  , 152.3      , 152.33333  , 152.93333  ,\n",
       "       153.3      , 153.83333  , 153.9      , 154.       , 154.7      ,\n",
       "       162.83333  , 163.16667  , 163.4      , 164.03334  , 164.33333  ,\n",
       "       164.5      , 165.16667  , 168.7      , 168.96666  , 169.       ,\n",
       "       169.83333  , 170.8      , 171.3      , 174.5      , 174.83333  ,\n",
       "       175.16667  , 175.63333  , 175.9      , 176.1      , 176.36667  ,\n",
       "       176.5      , 177.63333  , 178.16667  , 178.4      , 180.73334  ,\n",
       "       182.33333  , 182.5      , 182.6      , 183.43333  , 184.8      ,\n",
       "       185.       , 186.2      , 186.53334  , 186.83333  , 187.03334  ,\n",
       "       187.93333  , 188.73334  , 190.2      , 191.46666  , 191.8      ,\n",
       "       191.93333  , 192.2      , 194.16667  , 195.36667  , 195.8      ,\n",
       "       195.86667  , 196.56667  , 196.86667  , 199.03334  , 199.36667  ,\n",
       "       199.93333  , 200.1      , 200.33333  , 201.16667  , 204.2      ,\n",
       "       205.6      , 205.9      , 206.13333  , 207.16667  , 208.96666  ,\n",
       "       211.53334  , 211.9      , 211.93333  , 212.2      , 214.43333  ,\n",
       "       215.16667  , 216.03334  , 217.56667  , 219.46666  , 220.3      ,\n",
       "       220.93333  , 221.6      , 222.33333  , 224.3      , 224.43333  ,\n",
       "       226.06667  , 227.46666  , 227.9      , 228.33333  , 229.06667  ,\n",
       "       229.33333  , 237.5      , 238.36667  , 239.3      , 241.6      ,\n",
       "       243.76666  , 248.76666  , 250.83333  , 251.63333  , 251.8      ,\n",
       "       252.       , 254.63333  , 255.1      , 258.13333  , 259.76666  ,\n",
       "       260.2      , 262.86667  , 263.7      , 266.1      , 267.26666  ,\n",
       "       267.4      , 268.43332  , 268.9      , 269.33334  , 269.63333  ,\n",
       "       271.86667  , 275.63333  , 278.36667  , 279.8      , 281.36667  ,\n",
       "       282.36667  , 285.7      , 297.8      , 307.93332  , 318.2      ,\n",
       "       330.36667  , 335.6      , 335.73334  , 337.03333  ], dtype=float32)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(time_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_time, new_event = transform_back(y_test)\n",
    "old_time, old_event = transform_back(y_train)\n",
    "cum_hazard_baseline_old_new = np.interp(np.unique(new_time), np.unique(old_time), cum_hazard_baseline_old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1381,)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(time).shape # should I take the unique one?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "x=%{x}<br>y=%{y}<extra></extra>",
         "legendgroup": "",
         "line": {
          "color": "#636efa",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "",
         "orientation": "v",
         "showlegend": false,
         "type": "scatter",
         "x": [
          1.2666666507720947,
          2.299999952316284,
          4.866666793823242,
          5.833333492279053,
          6.833333492279053,
          7.866666793823242,
          10.866666793823242,
          11.066666603088379,
          11.300000190734863,
          14.100000381469727,
          15.300000190734863,
          15.366666793823242,
          16.566667556762695,
          17.133333206176758,
          17.200000762939453,
          19.100000381469727,
          19.566667556762695,
          20.133333206176758,
          20.433332443237305,
          20.66666603088379,
          21.299999237060547,
          21.566667556762695,
          21.600000381469727,
          22.46666717529297,
          23.03333282470703,
          23.766666412353516,
          23.933332443237305,
          24.299999237060547,
          24.399999618530273,
          25.46666717529297,
          27.399999618530273,
          27.46666717529297,
          28.5,
          28.733333587646484,
          28.83333396911621,
          28.866666793823242,
          30.866666793823242,
          31.33333396911621,
          31.46666717529297,
          33.96666717529297,
          34.099998474121094,
          34.33333206176758,
          34.43333435058594,
          34.63333511352539,
          34.766666412353516,
          36.266666412353516,
          36.400001525878906,
          36.43333435058594,
          36.63333511352539,
          37.36666488647461,
          37.733333587646484,
          37.79999923706055,
          37.86666488647461,
          38.16666793823242,
          39.29999923706055,
          40,
          40.43333435058594,
          40.63333511352539,
          42.66666793823242,
          43.099998474121094,
          44.766666412353516,
          45.16666793823242,
          45.599998474121094,
          46.43333435058594,
          48.13333511352539,
          48.53333282470703,
          49.53333282470703,
          49.56666564941406,
          51.20000076293945,
          51.766666412353516,
          52.29999923706055,
          53.63333511352539,
          55.233333587646484,
          56.266666412353516,
          56.5,
          57.233333587646484,
          57.66666793823242,
          58.46666717529297,
          58.63333511352539,
          59.766666412353516,
          59.96666717529297,
          60.900001525878906,
          61.46666717529297,
          61.599998474121094,
          62.53333282470703,
          62.79999923706055,
          63.96666717529297,
          64.23332977294922,
          64.86666870117188,
          65.13333129882812,
          65.33333587646484,
          65.46666717529297,
          65.56666564941406,
          67.46666717529297,
          68.16666412353516,
          68.69999694824219,
          69.30000305175781,
          70.4000015258789,
          70.73332977294922,
          71.5,
          71.5999984741211,
          71.63333129882812,
          71.76667022705078,
          71.80000305175781,
          72.43333435058594,
          72.66666412353516,
          73.69999694824219,
          74.46666717529297,
          74.93333435058594,
          75.0999984741211,
          76.13333129882812,
          77.23332977294922,
          78.16666412353516,
          79.96666717529297,
          80.36666870117188,
          82.36666870117188,
          83.13333129882812,
          84.83333587646484,
          85.4000015258789,
          85.56666564941406,
          85.73332977294922,
          86.9000015258789,
          87.69999694824219,
          88.19999694824219,
          88.83333587646484,
          89.5999984741211,
          89.80000305175781,
          90.13333129882812,
          90.56666564941406,
          90.66666412353516,
          91,
          91.5,
          91.73332977294922,
          92.83333587646484,
          93.5,
          93.66666412353516,
          94.23332977294922,
          96.19999694824219,
          96.23332977294922,
          97.56666564941406,
          97.5999984741211,
          97.83333587646484,
          98.69999694824219,
          99.23332977294922,
          99.33333587646484,
          99.36666870117188,
          99.73332977294922,
          99.96666717529297,
          101.23332977294922,
          102.03333282470703,
          102.06666564941406,
          102.5,
          102.63333129882812,
          103.0999984741211,
          103.13333129882812,
          105,
          107.36666870117188,
          107.76667022705078,
          108.06666564941406,
          108.46666717529297,
          110.0999984741211,
          110.5999984741211,
          110.93333435058594,
          110.96666717529297,
          111.06666564941406,
          111.0999984741211,
          111.19999694824219,
          111.5999984741211,
          112.80000305175781,
          112.93333435058594,
          112.96666717529297,
          113.06666564941406,
          113.66666412353516,
          114.03333282470703,
          114.23332977294922,
          114.5999984741211,
          114.76667022705078,
          114.9000015258789,
          115.63333129882812,
          115.93333435058594,
          117.53333282470703,
          117.76667022705078,
          118.19999694824219,
          118.53333282470703,
          118.9000015258789,
          119.30000305175781,
          119.36666870117188,
          120.13333129882812,
          120.43333435058594,
          122.76667022705078,
          123.33333587646484,
          123.73332977294922,
          124.76667022705078,
          125.03333282470703,
          125.86666870117188,
          126.4000015258789,
          127.93333435058594,
          128.36666870117188,
          128.39999389648438,
          128.6999969482422,
          128.96665954589844,
          129.8000030517578,
          130.36666870117188,
          131.3000030517578,
          132.1999969482422,
          132.76666259765625,
          133.23333740234375,
          133.73333740234375,
          135.3333282470703,
          136.46665954589844,
          139.60000610351562,
          140.56666564941406,
          142.1666717529297,
          142.6666717529297,
          143,
          143.1666717529297,
          144.46665954589844,
          144.96665954589844,
          145.73333740234375,
          146.73333740234375,
          147.8333282470703,
          148.03334045410156,
          148.76666259765625,
          148.86666870117188,
          149.39999389648438,
          149.86666870117188,
          150.46665954589844,
          151.1666717529297,
          151.1999969482422,
          152.93333435058594,
          153,
          153.89999389648438,
          153.96665954589844,
          156.8000030517578,
          157.43333435058594,
          157.53334045410156,
          160.39999389648438,
          161.13333129882812,
          161.6666717529297,
          161.76666259765625,
          161.93333435058594,
          163.1999969482422,
          163.86666870117188,
          164.03334045410156,
          164.3333282470703,
          164.60000610351562,
          164.93333435058594,
          165.1666717529297,
          165.43333435058594,
          167.10000610351562,
          167.43333435058594,
          167.93333435058594,
          168.3000030517578,
          168.3333282470703,
          168.60000610351562,
          169.8333282470703,
          170.8000030517578,
          172.96665954589844,
          173.8333282470703,
          173.93333435058594,
          174.26666259765625,
          174.63333129882812,
          175.10000610351562,
          175.63333129882812,
          176.06666564941406,
          179.43333435058594,
          179.8000030517578,
          180.73333740234375,
          180.76666259765625,
          181.46665954589844,
          183.1999969482422,
          183.26666259765625,
          184.76666259765625,
          185.76666259765625,
          186.53334045410156,
          186.60000610351562,
          186.63333129882812,
          186.8333282470703,
          187.03334045410156,
          187.6999969482422,
          187.8333282470703,
          188.36666870117188,
          188.73333740234375,
          189.03334045410156,
          189.73333740234375,
          191.1666717529297,
          193.89999389648438,
          194,
          194.1999969482422,
          194.60000610351562,
          195.8000030517578,
          195.86666870117188,
          196.46665954589844,
          196.86666870117188,
          197.3333282470703,
          199.23333740234375,
          199.26666259765625,
          200.3333282470703,
          200.60000610351562,
          201.46665954589844,
          202.23333740234375,
          203.53334045410156,
          205.60000610351562,
          205.8000030517578,
          205.89999389648438,
          207.46665954589844,
          208.1999969482422,
          208.8000030517578,
          208.96665954589844,
          210.43333435058594,
          211.13333129882812,
          211.73333740234375,
          213.03334045410156,
          213.36666870117188,
          216.03334045410156,
          216.73333740234375,
          218.23333740234375,
          219.10000610351562,
          219.63333129882812,
          225.5,
          227.93333435058594,
          228.60000610351562,
          228.8000030517578,
          229.3333282470703,
          231.53334045410156,
          234.23333740234375,
          234.53334045410156,
          234.6999969482422,
          236.93333435058594,
          237.13333129882812,
          237.26666259765625,
          238.36666870117188,
          239.1666717529297,
          240.1999969482422,
          241.26666259765625,
          241.3000030517578,
          242.56666564941406,
          243.76666259765625,
          243.89999389648438,
          250.13333129882812,
          253.06666564941406,
          254.26666259765625,
          255.10000610351562,
          255.26666259765625,
          256,
          256.5,
          257.76666259765625,
          258.1333312988281,
          259.9333190917969,
          259.9666748046875,
          260.0333251953125,
          261.20001220703125,
          262.6333312988281,
          263.0333251953125,
          264.76666259765625,
          267.3999938964844,
          270.4333190917969,
          272.20001220703125,
          274.3666687011719,
          278.3666687011719,
          285.4333190917969,
          286.0666809082031,
          307.9333190917969,
          335.73333740234375,
          351
         ],
         "xaxis": "x",
         "y": [
          0.0001929213641294889,
          0.0001929213641294889,
          0.0011044788580040993,
          0.0014464199107824926,
          0.0016291728375492241,
          0.0018013750779500477,
          0.003724882400997782,
          0.0037763562492372097,
          0.003836409194906543,
          0.005478868188237049,
          0.007737450925291968,
          0.007897978267282052,
          0.00989368251036327,
          0.011003667634620816,
          0.011019940525857188,
          0.012666152298414012,
          0.012973596085696046,
          0.014344569619821836,
          0.014474051721008671,
          0.014559096235859879,
          0.015447344288667661,
          0.015585565024604736,
          0.015602842245862265,
          0.01679754180350417,
          0.017253341107792503,
          0.018363840159045106,
          0.018783605261731052,
          0.01917415803263358,
          0.01925098279105191,
          0.02068162985307321,
          0.02323973194773185,
          0.023453905541636056,
          0.024713910372567323,
          0.025029945003185387,
          0.025083877847297712,
          0.025101855119107338,
          0.028542711189450867,
          0.02958393708583635,
          0.029768841112086584,
          0.032661906389616184,
          0.03273614591433793,
          0.03287537313225017,
          0.03295897374164943,
          0.033219221951902425,
          0.03401369076537438,
          0.036777483123227164,
          0.036928336977976275,
          0.03696606502401975,
          0.037343646033484626,
          0.03841977326887267,
          0.03868005938442357,
          0.03878464987053519,
          0.03893681920441729,
          0.04033542796807171,
          0.041926716309459856,
          0.04289888038607503,
          0.043020633265859136,
          0.043076826985129185,
          0.04747484333401561,
          0.04903302048911921,
          0.052549405816866575,
          0.05340559858127707,
          0.054265914675593566,
          0.05636915923863653,
          0.058834113439958646,
          0.05976996309292817,
          0.061640867106064326,
          0.06170436725431644,
          0.06471157169138764,
          0.06587306032259567,
          0.06677986352082804,
          0.06912657187196994,
          0.07123262109108178,
          0.0730481032125933,
          0.07335958072572436,
          0.07416160345978617,
          0.07523431970098916,
          0.07577320035118744,
          0.07590839196940652,
          0.0774004600062573,
          0.07767282193394474,
          0.07825856033605348,
          0.07849331492536182,
          0.07849331492536182,
          0.079870070801498,
          0.08014687068834074,
          0.08286683957827097,
          0.08293699317426866,
          0.08398458028929762,
          0.0843077280960838,
          0.08463124879916196,
          0.08482032332475778,
          0.08497165193695312,
          0.08614262296134068,
          0.0870555940713523,
          0.08773505114443042,
          0.08830792134862216,
          0.08912122430515174,
          0.08992955165386063,
          0.09067555965510875,
          0.09089529059279966,
          0.09096853423869664,
          0.09096853423869664,
          0.09096853423869664,
          0.09116508870402605,
          0.09155838512883958,
          0.09244601127985727,
          0.09333765005056602,
          0.09375528672623139,
          0.09385483731509234,
          0.0959097457158821,
          0.09634313065250416,
          0.09771633406789118,
          0.10064127113924942,
          0.10084817339099504,
          0.10519366875480923,
          0.10593500828277674,
          0.10906304936094079,
          0.11016168875126238,
          0.11022087689832454,
          0.11036884726597991,
          0.11324057423617444,
          0.11537621908852236,
          0.11543175536014344,
          0.11652017727343854,
          0.11879719819641739,
          0.11888209340794506,
          0.12001251428481337,
          0.12147018476367138,
          0.12175606655037813,
          0.12221361079859211,
          0.12324825810203933,
          0.12364052513931632,
          0.1252209524286728,
          0.12575805131532666,
          0.12584582718493814,
          0.12639041324109876,
          0.12828808692978064,
          0.12830125019352054,
          0.13079965676746422,
          0.13083569869198994,
          0.1310880725205913,
          0.13210261565833742,
          0.13319580682500753,
          0.13319580682500753,
          0.13319580682500753,
          0.1341129529701669,
          0.13449806113682933,
          0.13683084041730448,
          0.13802011871331069,
          0.13802011871331069,
          0.13877873618008496,
          0.13877873618008496,
          0.14002466478533554,
          0.14005018788969942,
          0.14438441579571112,
          0.14733175804318932,
          0.14733175804318932,
          0.14773099756417857,
          0.14821232637235227,
          0.15015888318354426,
          0.15015888318354426,
          0.1513916141798416,
          0.1513916141798416,
          0.1513916141798416,
          0.15152950159009848,
          0.15180527641061223,
          0.15263609203861306,
          0.155994642025539,
          0.155994642025539,
          0.155994642025539,
          0.155994642025539,
          0.155994642025539,
          0.15684618428225253,
          0.15684618428225253,
          0.1577027253565985,
          0.1577027253565985,
          0.15813357410201417,
          0.15856542807719373,
          0.15856542807719373,
          0.16071606945540243,
          0.16206791013077604,
          0.16384813633323203,
          0.16455616266013645,
          0.16504188005006315,
          0.16519143933534292,
          0.16541691225585714,
          0.16654783918087127,
          0.16654783918087127,
          0.16900543780874105,
          0.1707158376929118,
          0.1707158376929118,
          0.17349898261921018,
          0.17380685906975044,
          0.17718369784774152,
          0.17828887399693594,
          0.1798784232673188,
          0.18087362201844964,
          0.18087362201844964,
          0.18147570709489497,
          0.18167682143211766,
          0.18288675086259087,
          0.18288675086259087,
          0.18365186636917208,
          0.18493973098877417,
          0.18604189832027182,
          0.18618532745190472,
          0.18633899884169502,
          0.18755705337108836,
          0.18835927090212468,
          0.19080632388151342,
          0.1917318956878952,
          0.19452084246574552,
          0.19530706531196224,
          0.1956505026503697,
          0.1958794608759747,
          0.19761059712460455,
          0.19883118841807626,
          0.20054337406112935,
          0.20173554541057356,
          0.2041506045367409,
          0.2041506045367409,
          0.2058974460217832,
          0.20598499798057043,
          0.20644673800652147,
          0.2072185717392494,
          0.2078411893864171,
          0.21045482616809472,
          0.21048998859357407,
          0.21483227939695346,
          0.21483227939695346,
          0.21696373505979658,
          0.21729372600325988,
          0.2201232732272892,
          0.2201232732272892,
          0.2201232732272892,
          0.22559436743175343,
          0.22559436743175343,
          0.22559436743175343,
          0.22559436743175343,
          0.22559436743175343,
          0.22840625961285163,
          0.22856534667022993,
          0.22876421459512614,
          0.22912215137105396,
          0.22984169980502475,
          0.23020485347707126,
          0.23129535275971722,
          0.23202700398193699,
          0.23202700398193699,
          0.23202700398193699,
          0.23248573360610866,
          0.23360384917240432,
          0.23369728910827564,
          0.23425809985891718,
          0.23686458355400558,
          0.23957324145765843,
          0.2462167542311213,
          0.247723571974157,
          0.24788299781086343,
          0.24849279889639606,
          0.25003583136494323,
          0.2506119943106307,
          0.2506119943106307,
          0.25348117928717456,
          0.2589915546301971,
          0.2589915546301971,
          0.259429534063068,
          0.2595754826565056,
          0.2607443563708742,
          0.26252356528317516,
          0.26252356528317516,
          0.26344893447966955,
          0.26532739246617565,
          0.26644908351239033,
          0.26655631651964085,
          0.26660992075123513,
          0.26693161977298674,
          0.26725334333880046,
          0.2679414335070651,
          0.26817079689648665,
          0.26839216137016597,
          0.2695609959914439,
          0.2700844076564767,
          0.27020071840183696,
          0.27220625085326017,
          0.2762947815663579,
          0.27656309781202376,
          0.2784439868099519,
          0.27980857086296884,
          0.2828535629611012,
          0.2828535629611012,
          0.2828535629611012,
          0.2851100026627611,
          0.2851100026627611,
          0.2944324547033914,
          0.2944324547033914,
          0.29563669069556503,
          0.29563669069556503,
          0.2968714123276067,
          0.3031619856410024,
          0.30523805781949215,
          0.30834151725294356,
          0.3085621193653929,
          0.3088929972877734,
          0.31099735862739886,
          0.31234585156566064,
          0.31330594429325626,
          0.3137059493579444,
          0.3160198206150937,
          0.31744495030754954,
          0.3200104922963069,
          0.3240049991283927,
          0.32645721487269974,
          0.3303179202924005,
          0.331001123039887,
          0.3386287856985136,
          0.3417579850742902,
          0.3433382958984266,
          0.36238960059140085,
          0.37614896150340116,
          0.37944693209613506,
          0.3802377987856981,
          0.38210604508353935,
          0.38451529884198893,
          0.39461073908411626,
          0.398078248779199,
          0.398078248779199,
          0.4014439415950443,
          0.4023111445239052,
          0.4028892798098125,
          0.40790582773879697,
          0.40790582773879697,
          0.41056720557345133,
          0.41056720557345133,
          0.41056720557345133,
          0.4133203577524202,
          0.4144136189846296,
          0.41496024960073424,
          0.42838649986958227,
          0.446360887757565,
          0.4493349993421531,
          0.4531072091587519,
          0.4531072091587519,
          0.4543467017529894,
          0.45589609113759205,
          0.4578546181622782,
          0.46086955761468124,
          0.4611435745836387,
          0.4611435745836387,
          0.4611435745836387,
          0.4611435745836387,
          0.46444674646511963,
          0.46598825355775897,
          0.4819393530633957,
          0.49377851912987314,
          0.5077498579324945,
          0.5077498579324945,
          0.5077498579324945,
          0.5188587252936154,
          0.5804950939760198,
          0.5821150466058421,
          0.7189827691734102,
          0.8340230819287282,
          1.1507049200668695
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "legend": {
         "tracegroupgap": 0
        },
        "margin": {
         "t": 60
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "x"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "y"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "px.line(x =np.unique(new_time), y = cum_hazard_baseline_old_new )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cindex_train_METABRIC</th>\n",
       "      <th>cindex_test_METABRIC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.606150</td>\n",
       "      <td>0.559541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.609315</td>\n",
       "      <td>0.611044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.617208</td>\n",
       "      <td>0.608548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.446914</td>\n",
       "      <td>0.460198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.643331</td>\n",
       "      <td>0.649819</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cindex_train_METABRIC  cindex_test_METABRIC\n",
       "0               0.606150              0.559541\n",
       "1               0.609315              0.611044\n",
       "2               0.617208              0.608548\n",
       "3               0.446914              0.460198\n",
       "4               0.643331              0.649819"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(outer_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.25, 0.3 , 0.7 ])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = np.array([2,4,5,6,9,14])\n",
    "new_times = np.array([3,4,5])\n",
    "y = np.array([0.2,0.3,0.7,0.7,0.8,0.9])\n",
    "np.interp(new_times, t, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 2]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.array([1, 2, 3, 4.2])\n",
    "b = np.array([2.5, 3.5, 4.5, 5.5, 6.5])\n",
    "\n",
    "closest_indices = np.abs(a[:, np.newaxis] - b).argmin(axis=1)\n",
    "\n",
    "print(closest_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.5, 2.5, 3.5, 4.5, 5.5],\n",
       "       [0.5, 1.5, 2.5, 3.5, 4.5],\n",
       "       [0.5, 0.5, 1.5, 2.5, 3.5],\n",
       "       [1.5, 0.5, 0.5, 1.5, 2.5],\n",
       "       [2.5, 1.5, 0.5, 0.5, 1.5]])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.abs(a[:, np.newaxis] - b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 2, 3])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[closest_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training Procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'EfronLoss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[99], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Define models to apply\u001b[39;00m\n\u001b[1;32m      2\u001b[0m loss_functions \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mbreslow_loss\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mefron_loss\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m#, 'deephit_loss'] #, 'cind_loss', 'aft_loss', 'efron_loss', 'aft_loss', 'ah_loss', 'deephit_loss'\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m criterion_functions \u001b[39m=\u001b[39m [BreslowLoss, EfronLoss] \u001b[39m#, deephit_loss1_pycox] #, cind_loss, efron_likelihood, aft_likelihood, ah_likelihood, deephit_loss1_pycox\u001b[39;00m\n\u001b[1;32m      4\u001b[0m scoring_functions \u001b[39m=\u001b[39m [breslow_likelihood_torch, efron_likelihood_torch] \u001b[39m#, deephit_loss1_pycox] #, cind_loss, efron_likelihood, aft_likelihood, ah_likelihood, deephit_loss1_pycox]\u001b[39;00m\n\u001b[1;32m      6\u001b[0m n_models \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(criterion_functions)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'EfronLoss' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define models to apply\n",
    "loss_functions = ['breslow_loss', 'efron_loss']#, 'deephit_loss'] #, 'cind_loss', 'aft_loss', 'efron_loss', 'aft_loss', 'ah_loss', 'deephit_loss'\n",
    "criterion_functions = [BreslowLoss, EfronLoss] #, deephit_loss1_pycox] #, cind_loss, efron_likelihood, aft_likelihood, ah_likelihood, deephit_loss1_pycox\n",
    "scoring_functions = [breslow_likelihood_torch, efron_likelihood_torch] #, deephit_loss1_pycox] #, cind_loss, efron_likelihood, aft_likelihood, ah_likelihood, deephit_loss1_pycox]\n",
    "\n",
    "n_models = len(criterion_functions)\n",
    "\n",
    "# dict of outer scores\n",
    "outer_scores = {'breslow_loss':[], 'efron_loss':[]} #, 'efron_loss':[],  'aft_loss':[], 'ah_loss':[], 'deephit_loss':[]#,'deephit_loss' 'cind_loss':[],'aft_loss':[]\n",
    "\n",
    "# Load dataset\n",
    "data, target = load_metabric(path=\"/Users/JUSC/Documents/xgbsurv/xgbsurv/datasets/data/\", as_frame=True)\n",
    "X, y = load_metabric(path=\"/Users/JUSC/Documents/xgbsurv/xgbsurv/datasets/data/\", as_frame=False)\n",
    "\n",
    "# deephit data adaptation\n",
    "time, event = transform_back(target.to_numpy())\n",
    "data['time'] = time\n",
    "data['event'] = event\n",
    "df = discretizer_df(data, n_cuts=100, type = 'equidistant', min_time=0.0)\n",
    "\n",
    "y_deep = transform(df.time.to_numpy(), df.event.to_numpy())\n",
    "n = len(np.unique(np.absolute(y_deep)))\n",
    "y_deephit = np.tile(y_deep, (n,1)).T\n",
    "X_deephit = df.iloc[:,:-2].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examples\n",
    "# example\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create a sample dataframe\n",
    "df = pd.DataFrame({'age': [25, 30, 35],\n",
    "                   'income': [50000, 60000, 70000],\n",
    "                   'gender': ['male', 'female', 'male'],\n",
    "                   'is_married': [True, False, True],\n",
    "                   'num_children': [2, 0, 1],\n",
    "                   'target': [0, 1, 1]})\n",
    "\n",
    "# Select columns by data type\n",
    "numeric_cols = df.select_dtypes(include=np.number).columns\n",
    "categorical_cols = df.select_dtypes(include='object').columns\n",
    "boolean_cols = df.select_dtypes(include=bool).columns\n",
    "\n",
    "# Define the feature mapper\n",
    "mapper = DataFrameMapper([\n",
    "    (numeric_cols, StandardScaler()),\n",
    "    (categorical_cols, OneHotEncoder()),\n",
    "    (boolean_cols, None)\n",
    "])\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = make_pipeline(mapper, LogisticRegression())\n",
    "\n",
    "# Split the data into input features and target variable\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "\n",
    "# Fit the pipeline to the data\n",
    "pipeline.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class CustomStandardScaler(StandardScaler):\n",
    "    \n",
    "    def __init__(self, copy=True, with_mean=True, with_std=True):\n",
    "        super().__init__(copy=copy, with_mean=with_mean, with_std=with_std)\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        # Add your own code here\n",
    "        return super().fit(X, y)\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        # Add your own code here\n",
    "        X_transformed = super().transform(X, y)\n",
    "        # Add your own code here\n",
    "        return X_transformed\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        # Add your own code here\n",
    "        X_transformed = super().fit_transform(X, y)\n",
    "        # Add your own code here\n",
    "        return X_transformed\n",
    "    \n",
    "ct = make_column_transformer(\n",
    "        [(StandardScaler(), make_column_selector(dtype_include=['float32'])), \n",
    "        (LabelBinarizer(), make_column_selector(dtype_include='bool'))],remainder='passthrough'\n",
    "        #(BoolToNumericTransformer(), make_column_selector(dtype_include=['bool']))\n",
    "        )\n",
    "\n",
    "data = load_rgbsg(path=\"/Users/JUSC/Documents/xgbsurv/xgbsurv/datasets/data/\", as_frame=True)\n",
    "X  = data.data\n",
    "X = ct.fit_transform(X)\n",
    "print(X.shape)\n",
    "np.savetxt('testX.csv',X,delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xgbsurv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
