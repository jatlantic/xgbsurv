{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import savetxt\n",
    "from xgbsurv.datasets import (load_metabric, load_flchain, load_rgbsg, load_support, load_tcga)\n",
    "from xgbsurv.models.utils import sort_X_y_pandas, transform_back, transform\n",
    "from xgbsurv.models.eh_ah_final import get_cumulative_hazard_function_ah\n",
    "import torch\n",
    "from torch import nn\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold, train_test_split\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, LabelBinarizer, OneHotEncoder\n",
    "from sklearn.compose import make_column_transformer, make_column_selector\n",
    "from sklearn.decomposition import PCA\n",
    "from loss_functions_pytorch import AHLoss, ah_likelihood_torch\n",
    "from skorch import NeuralNet\n",
    "from skorch.callbacks import EarlyStopping, Callback, LRScheduler\n",
    "import skorch.callbacks\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import check_cv\n",
    "from numbers import Number\n",
    "import torch.utils.data\n",
    "from skorch.utils import flatten\n",
    "from skorch.utils import is_pandas_ndframe\n",
    "from skorch.utils import check_indexing\n",
    "from skorch.utils import multi_indexing\n",
    "from skorch.utils import to_numpy\n",
    "from skorch.dataset import get_len\n",
    "from skorch.dataset import ValidSplit\n",
    "from pycox.evaluation import EvalSurv\n",
    "from scipy.stats import uniform as scuniform\n",
    "from scipy.stats import randint as scrandint\n",
    "from scipy.stats import loguniform as scloguniform\n",
    "import random\n",
    "import os\n",
    "#torch.set_default_dtype(torch.float64)\n",
    "#torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "# highest prediction with -loss function 8.7\n",
    "#highest prediction without -loss function 8.7"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters, put into function\n",
    "n_outer_splits = 5\n",
    "n_inner_splits = 5\n",
    "rand_state = 42\n",
    "n_iter = 1\n",
    "early_stopping_rounds=10\n",
    "base_score = 0.0\n",
    "\n",
    "# set seed for scipy/numpy\n",
    "np.random.seed(rand_state)\n",
    "\n",
    "param_grid_ah = {\n",
    "    'estimator__module__n_layers': [1, 2, 4],\n",
    "    'estimator__module__num_nodes': [64, 128, 256, 512],\n",
    "    'estimator__module__dropout': scuniform(0.0,0.7),\n",
    "    'estimator__optimizer__weight_decay': [0.4, 0.2, 0.1, 0.05, 0.02, 0.01, 0],\n",
    "    'estimator__batch_size': [64, 128, 256, 512, 1024],\n",
    "    #lr not in paper because of learning rate finder\n",
    "    # note: setting learning rate higher would make exp(partial_hazard) explode\n",
    "    #'estimator__lr': scloguniform(0.001,0.01), # scheduler unten einbauen\n",
    "    # use callback instead\n",
    "    'estimator__lr':[0.001]\n",
    "    #'max_epochs':  scrandint(10,20), # corresponds to num_rounds\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_torch(seed=42):\n",
    "    \"\"\"Sets all seeds within torch and adjacent libraries.\n",
    "\n",
    "    Args:\n",
    "        seed: Random seed to be used by the seeding functions.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    #os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    #torch.backends.cudnn.benchmark = False\n",
    "    #torch.backends.cudnn.deterministic = True\n",
    "    return None\n",
    "\n",
    "\n",
    "class FixSeed(Callback):\n",
    "    def __init__(self, seed):\n",
    "        self.seed = seed\n",
    "\n",
    "    def initialize(self):\n",
    "        seed_torch(self.seed)\n",
    "        return super().initialize()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Scorer\n",
    "def custom_scoring_function(y_true, y_pred):\n",
    "\n",
    "        #y_true = torch.from_numpy(y_true)\n",
    "        if isinstance(y_pred, np.ndarray):\n",
    "            y_pred = torch.from_numpy(y_pred)\n",
    "        if isinstance(y_true, np.ndarray):\n",
    "            y_true = torch.from_numpy(y_true)\n",
    "        if isinstance(y_pred, pd.Series):\n",
    "            y_pred = torch.tensor(y_pred.values)\n",
    "        if isinstance(y_true, pd.Series):\n",
    "            y_true = torch.tensor(y_true.values)\n",
    "        #print('loss function y_pred', y_pred)\n",
    "        #print('loss function y_true', y_true)\n",
    "        score = -ah_likelihood_torch(y_pred, y_true) #.to(torch.float32)\n",
    "        return score.numpy()\n",
    "\n",
    "scoring_function = make_scorer(custom_scoring_function, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set up Custom Splitter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Torch Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SurvivalModel(nn.Module):\n",
    "    def __init__(self, n_layers, input_units, num_nodes, dropout, out_features):\n",
    "        super(SurvivalModel, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.in_features = input_units\n",
    "        self.num_nodes = num_nodes\n",
    "        self.dropout = dropout\n",
    "        self.out_features = out_features\n",
    "        model = []\n",
    "        # first layer\n",
    "        model.append(torch.nn.Linear(input_units, num_nodes))\n",
    "        model.append(torch.nn.ReLU())\n",
    "        model.append(torch.nn.Dropout(dropout))\n",
    "        model.append(torch.nn.BatchNorm1d(num_nodes))\n",
    "\n",
    "        for i in range(n_layers-1):\n",
    "            model.append(torch.nn.Linear(num_nodes, num_nodes))\n",
    "            #init.kaiming_normal_(model[-1].weight, nonlinearity='relu')\n",
    "            model.append(torch.nn.ReLU())\n",
    "            model.append(torch.nn.Dropout(dropout))\n",
    "            model.append(torch.nn.BatchNorm1d(num_nodes))\n",
    "\n",
    "        # output layer\n",
    "        model.append(torch.nn.Linear(num_nodes, out_features))\n",
    "    \n",
    "        self.layers = nn.Sequential(*model)\n",
    "\n",
    "        # for layer in self.layers:\n",
    "        #     if isinstance(layer, nn.Linear):\n",
    "        #         #nn.init.uniform_(layer.weight, a=-0.5, b=0.5)\n",
    "        #         nn.init.kaiming_normal_(layer.weight)\n",
    "\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = X.to(torch.float32)\n",
    "        res = self.layers(X)\n",
    "        #print(res)\n",
    "        return res\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomStandardScaler(StandardScaler):\n",
    "    \n",
    "    def __init__(self, copy=True, with_mean=True, with_std=True):\n",
    "        super().__init__(copy=copy, with_mean=with_mean, with_std=with_std)\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return super().fit(X, y)\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X_transformed = super().transform(X, y)\n",
    "        return X_transformed.astype(np.float32)\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        X_transformed = super().fit_transform(X, y)\n",
    "        return X_transformed.astype(np.float32)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define stratified inner k-fold cross-validation\n",
    "class CustomSplit(StratifiedKFold):\n",
    "    def __init__(self, n_splits=5, shuffle=True, random_state=None):\n",
    "        super().__init__(n_splits=n_splits, shuffle=shuffle, random_state=random_state)\n",
    "\n",
    "    def split(self, X, y, groups=None):\n",
    "        print('split', X.dtypes)\n",
    "        try:\n",
    "            if y.shape[1]>1:\n",
    "                y = y[:,0]\n",
    "        except:\n",
    "            pass\n",
    "        bins = np.sign(y)\n",
    "        return super().split(X, bins, groups=groups)\n",
    "\n",
    "    def get_n_splits(self, X=None, y=None, groups=None):\n",
    "        return self.n_splits\n",
    "\n",
    "outer_custom_cv = CustomSplit(n_splits=n_outer_splits, shuffle=True, random_state=rand_state)\n",
    "inner_custom_cv = CustomSplit(n_splits=n_outer_splits, shuffle=True, random_state=rand_state)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Valid Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class CustomStandardScaler(StandardScaler):\n",
    "    \n",
    "    def __init__(self, copy=True, with_mean=True, with_std=True):\n",
    "        super().__init__(copy=copy, with_mean=with_mean, with_std=with_std)\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return super().fit(X, y)\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X_transformed = super().transform(X, y)\n",
    "        return X_transformed.astype(np.float32)\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        X_transformed = super().fit_transform(X, y)\n",
    "        return X_transformed.astype(np.float32)\n",
    "    \n",
    "class CustomValidSplit():\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            cv=5,\n",
    "            stratified=False,\n",
    "            random_state=None,\n",
    "    ):\n",
    "        self.stratified = stratified\n",
    "        self.random_state = random_state\n",
    "\n",
    "        if isinstance(cv, Number) and (cv <= 0):\n",
    "            raise ValueError(\"Numbers less than 0 are not allowed for cv \"\n",
    "                             \"but ValidSplit got {}\".format(cv))\n",
    "\n",
    "        if not self._is_float(cv) and random_state is not None:\n",
    "            raise ValueError(\n",
    "                \"Setting a random_state has no effect since cv is not a float. \"\n",
    "                \"You should leave random_state to its default (None), or set cv \"\n",
    "                \"to a float value.\",\n",
    "            )\n",
    "\n",
    "        self.cv = cv\n",
    "\n",
    "    def _is_stratified(self, cv):\n",
    "        return isinstance(cv, (StratifiedKFold, StratifiedShuffleSplit))\n",
    "\n",
    "    def _is_float(self, x):\n",
    "        if not isinstance(x, Number):\n",
    "            return False\n",
    "        return not float(x).is_integer()\n",
    "\n",
    "    def _check_cv_float(self):\n",
    "        cv_cls = StratifiedShuffleSplit if self.stratified else ShuffleSplit\n",
    "        return cv_cls(test_size=self.cv, random_state=self.random_state)\n",
    "\n",
    "    def _check_cv_non_float(self, y):\n",
    "        return check_cv(\n",
    "            self.cv,\n",
    "            y=y,\n",
    "            classifier=self.stratified,\n",
    "        )\n",
    "\n",
    "    def check_cv(self, y):\n",
    "        \"\"\"Resolve which cross validation strategy is used.\"\"\"\n",
    "        y_arr = None\n",
    "        if self.stratified:\n",
    "            # Try to convert y to numpy for sklearn's check_cv; if conversion\n",
    "            # doesn't work, still try.\n",
    "            try:\n",
    "                y_arr = to_numpy(y)\n",
    "            except (AttributeError, TypeError):\n",
    "                y_arr = y\n",
    "\n",
    "        if self._is_float(self.cv):\n",
    "            return self._check_cv_float()\n",
    "        return self._check_cv_non_float(y_arr)\n",
    "\n",
    "    def _is_regular(self, x):\n",
    "        return (x is None) or isinstance(x, np.ndarray) or is_pandas_ndframe(x)\n",
    "\n",
    "    def __call__(self, dataset, y=None, groups=None):\n",
    "        # key change here\n",
    "        y = np.sign(y)\n",
    "        bad_y_error = ValueError(\n",
    "            \"Stratified CV requires explicitly passing a suitable y.\")\n",
    "        if (y is None) and self.stratified:\n",
    "            raise bad_y_error\n",
    "\n",
    "        cv = self.check_cv(y)\n",
    "        if self.stratified and not self._is_stratified(cv):\n",
    "            raise bad_y_error\n",
    "\n",
    "        # pylint: disable=invalid-name\n",
    "        len_dataset = get_len(dataset)\n",
    "        if y is not None:\n",
    "            len_y = get_len(y)\n",
    "            if len_dataset != len_y:\n",
    "                raise ValueError(\"Cannot perform a CV split if dataset and y \"\n",
    "                                 \"have different lengths.\")\n",
    "\n",
    "        args = (np.arange(len_dataset),)\n",
    "        if self._is_stratified(cv):\n",
    "            args = args + (to_numpy(y),)\n",
    "\n",
    "        idx_train, idx_valid = next(iter(cv.split(*args, groups=groups)))\n",
    "        dataset_train = torch.utils.data.Subset(dataset, idx_train)\n",
    "        dataset_valid = torch.utils.data.Subset(dataset, idx_valid)\n",
    "        return dataset_train, dataset_valid\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Training Procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_eval(X, y, net, n_iter, filename):\n",
    "        model = 'ah_'\n",
    "        dataset_name = filename.split('_')[0]\n",
    "        # add IBS later\n",
    "        outer_scores = {'cindex_train_'+dataset_name:[], 'cindex_test_'+dataset_name:[],\n",
    "                        'ibs_train_'+dataset_name:[], 'ibs_test_'+dataset_name:[]}\n",
    "        best_params = {'best_params_'+dataset_name:[]}\n",
    "        best_model = {'best_model_'+dataset_name:[]}\n",
    "        ct = make_column_transformer(\n",
    "                #(OneHotEncoder(sparse_output=False), make_column_selector(dtype_include=['category', 'object'])),\n",
    "                (StandardScaler(), make_column_selector(dtype_include=['float32'],dtype_exclude=['category', 'object']))\n",
    "                ,remainder='passthrough')\n",
    "\n",
    "        pipe = Pipeline([('scaler',ct),\n",
    "                        ('estimator', net)])\n",
    "        rs = RandomizedSearchCV(pipe, param_grid_ah, scoring = scoring_function, n_jobs=-1, \n",
    "                                    n_iter=n_iter, refit=True, random_state=rand_state)\n",
    "        \n",
    "        for i, (train_index, test_index) in enumerate(outer_custom_cv.split(X, y)):\n",
    "                # Split data into training and testing sets for outer fold\n",
    "                X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "                y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "                X_train, y_train = sort_X_y_pandas(X_train, y_train)\n",
    "                X_test, y_test = sort_X_y_pandas(X_test, y_test)\n",
    "\n",
    "                # print(X_train.shape, type(X_train))\n",
    "                # print(y_train.shape, type(y_train))\n",
    "                # print(X_test.shape, type(X_test))\n",
    "                # print(y_test.shape, type(y_test))\n",
    "                # save splits and data\n",
    "                savetxt('splits/'+model+'train_index_'+str(i)+'_'+filename, train_index, delimiter=',')\n",
    "                savetxt('splits/'+model+'test_index_'+str(i)+'_'+filename, test_index, delimiter=',')\n",
    "                \n",
    "                # savetxt('splits/X_train_'+str(i)+'_'+filename, X_train, delimiter=',')\n",
    "                # savetxt('splits/X_test_'+str(i)+'_'+filename, X_test, delimiter=',')\n",
    "\n",
    "                # savetxt('splits/y_train_'+str(i)+'_'+filename, y_train, delimiter=',')\n",
    "                # savetxt('splits/y_test_'+str(i)+'_'+filename, y_test, delimiter=',')\n",
    "\n",
    "\n",
    "                rs.fit(X_train, y_train)\n",
    "                best_preds_train = rs.best_estimator_.predict(X_train)\n",
    "                best_preds_test = rs.best_estimator_.predict(X_test)\n",
    "                print('max best test predictions',np.max(best_preds_test.reshape(-1)) )\n",
    "                savetxt('predictions/'+model+'best_preds_train_'+str(i)+'_'+filename, best_preds_train, delimiter=',')\n",
    "                savetxt('predictions/'+model+'best_preds_test_'+str(i)+'_'+filename, best_preds_test, delimiter=',')\n",
    "\n",
    "                # save hyperparameter settings\n",
    "                params = rs.best_estimator_.get_params\n",
    "                best_params['best_params_'+dataset_name] += [rs.best_params_]\n",
    "                best_model['best_model_'+dataset_name] += [params]\n",
    "                # remove training part, cumhazard sollte um 10000 sein ( Integralanzahl/Intervalle)\n",
    "\n",
    "                outer_scores['cindex_train_'+dataset_name] += [np.nan]\n",
    "                outer_scores['ibs_train_'+dataset_name] += [np.nan]\n",
    "                    \n",
    "                #try:\n",
    "                cum_hazard_test = get_cumulative_hazard_function_ah(\n",
    "                        X_train.values, X_test.values, y_train.values, y_test.values,\n",
    "                        best_preds_train.reshape(-1), best_preds_test.reshape(-1)\n",
    "                        )\n",
    "                df_survival_test = np.exp(-cum_hazard_test)\n",
    "                durations_test, events_test = transform_back(y_test.values)\n",
    "                time_grid_test = np.linspace(durations_test.min(), durations_test.max(), 100)\n",
    "                ev = EvalSurv(df_survival_test, durations_test, events_test, censor_surv='km')\n",
    "                print('Concordance Index',ev.concordance_td('antolini'))\n",
    "                print('Integrated Brier Score:',ev.integrated_brier_score(time_grid_test))\n",
    "                cindex_score_test = ev.concordance_td('antolini')\n",
    "                ibs_score_test = ev.integrated_brier_score(time_grid_test)\n",
    "\n",
    "                outer_scores['cindex_test_'+dataset_name] += [cindex_score_test]\n",
    "                outer_scores['ibs_test_'+dataset_name] += [ibs_score_test]\n",
    "                #except: \n",
    "                #outer_scores['cindex_test_'+dataset_name] += [np.nan]\n",
    "                #outer_scores['ibs_test_'+dataset_name] += [np.nan]\n",
    "            \n",
    "        df_best_params = pd.DataFrame(best_params)\n",
    "        df_best_model = pd.DataFrame(best_model)\n",
    "        df_outer_scores = pd.DataFrame(outer_scores)\n",
    "        df_metrics = pd.concat([df_best_params,df_best_model,df_outer_scores], axis=1)\n",
    "        df_metrics.to_csv('metrics/'+model+'metric_summary'+str(i)+'_'+filename, index=False)\n",
    "       \n",
    "        # cindex\n",
    "        df_agg_metrics_cindex = pd.DataFrame({'dataset':[dataset_name],\n",
    "                                              'cindex_train_mean':df_outer_scores['cindex_train_'+dataset_name].mean(),\n",
    "                                              'cindex_train_std':df_outer_scores['cindex_train_'+dataset_name].std(),\n",
    "                                              'cindex_test_mean':df_outer_scores['cindex_test_'+dataset_name].mean(),\n",
    "                                              'cindex_test_std':df_outer_scores['cindex_test_'+dataset_name].std() })\n",
    "        # IBS\n",
    "        df_agg_metrics_ibs = pd.DataFrame({'dataset':[dataset_name],\n",
    "                                              'ibs_train_mean':df_outer_scores['ibs_train_'+dataset_name].mean(),\n",
    "                                              'ibs_train_std':df_outer_scores['ibs_train_'+dataset_name].std(),\n",
    "                                              'ibs_test_mean':df_outer_scores['ibs_test_'+dataset_name].mean(),\n",
    "                                              'ibs_test_std':df_outer_scores['ibs_test_'+dataset_name].std() })\n",
    "\n",
    "        return df_agg_metrics_cindex, df_agg_metrics_ibs, best_model, best_params, outer_scores, best_preds_train, best_preds_test #, X_train, X_test, y_train, y_test\n",
    "\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skorch.callbacks\n",
    "class InputShapeSetter(skorch.callbacks.Callback):\n",
    "    def on_train_begin(self, net, X, y):\n",
    "        net.set_params(module__input_units=X.shape[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_metabric\n",
      "split MKI67                float32\n",
      "EGFR                 float32\n",
      "PGR                  float32\n",
      "ERBB2                float32\n",
      "hormone_treatment      uint8\n",
      "radiotherapy           uint8\n",
      "chemotherapy           uint8\n",
      "ER_positive            uint8\n",
      "age                  float32\n",
      "dtype: object\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.1873\u001b[0m        \u001b[32m3.2058\u001b[0m  0.0420\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.0176\u001b[0m        \u001b[32m3.0144\u001b[0m  0.0430\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.1906\u001b[0m        \u001b[32m3.2133\u001b[0m  0.0436\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.1982\u001b[0m        \u001b[32m3.2081\u001b[0m  0.0417\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.7660\u001b[0m        \u001b[32m2.7442\u001b[0m  0.0387\n",
      "      2        \u001b[36m3.1768\u001b[0m        \u001b[32m3.2129\u001b[0m  0.0243\n",
      "      2        3.1938        \u001b[32m3.2054\u001b[0m  0.0290\n",
      "      2        3.0180        \u001b[32m3.0140\u001b[0m  0.0282\n",
      "      2        \u001b[36m2.7481\u001b[0m        2.7444  0.0278\n",
      "      2        3.2032        \u001b[32m3.2074\u001b[0m  0.0291\n",
      "      3        \u001b[36m3.1681\u001b[0m        \u001b[32m3.2120\u001b[0m  0.0265\n",
      "      3        \u001b[36m3.0157\u001b[0m        \u001b[32m3.0137\u001b[0m  0.0242\n",
      "      3        2.7527        2.7445  0.0241\n",
      "      3        \u001b[36m3.1805\u001b[0m        \u001b[32m3.2045\u001b[0m  0.0253\n",
      "      3        3.2036        \u001b[32m3.2064\u001b[0m  0.0259\n",
      "      4        2.7560        2.7448  0.0252\n",
      "      4        3.0168        \u001b[32m3.0131\u001b[0m  0.0254\n",
      "      4        3.1736        \u001b[32m3.2110\u001b[0m  0.0281\n",
      "      4        3.1817        \u001b[32m3.2033\u001b[0m  0.0281\n",
      "      4        \u001b[36m3.1976\u001b[0m        \u001b[32m3.2050\u001b[0m  0.0273\n",
      "      5        2.7513        2.7452  0.0250\n",
      "      5        3.0180        \u001b[32m3.0127\u001b[0m  0.0272\n",
      "      5        3.1739        \u001b[32m3.2098\u001b[0m  0.0257\n",
      "      5        \u001b[36m3.1875\u001b[0m        \u001b[32m3.2034\u001b[0m  0.0251\n",
      "      5        \u001b[36m3.1792\u001b[0m        \u001b[32m3.2019\u001b[0m  0.0283\n",
      "      6        \u001b[36m2.7450\u001b[0m        2.7455  0.0278\n",
      "      6        \u001b[36m3.0140\u001b[0m        \u001b[32m3.0122\u001b[0m  0.0270\n",
      "      6        3.1766        \u001b[32m3.2085\u001b[0m  0.0274\n",
      "      6        \u001b[36m3.1762\u001b[0m        \u001b[32m3.2020\u001b[0m  0.0273\n",
      "      6        \u001b[36m3.1791\u001b[0m        \u001b[32m3.2004\u001b[0m  0.0278\n",
      "      7        2.7488        2.7461  0.0238\n",
      "      7        3.1691        \u001b[32m3.2071\u001b[0m  0.0259\n",
      "      7        \u001b[36m3.0053\u001b[0m        \u001b[32m3.0115\u001b[0m  0.0264\n",
      "      7        \u001b[36m3.1743\u001b[0m        \u001b[32m3.2008\u001b[0m  0.0257\n",
      "      7        3.1794        \u001b[32m3.1988\u001b[0m  0.0282\n",
      "      8        2.7521        2.7463  0.0278\n",
      "      8        \u001b[36m3.1675\u001b[0m        \u001b[32m3.2061\u001b[0m  0.0265\n",
      "      8        3.0174        \u001b[32m3.0109\u001b[0m  0.0268\n",
      "      8        3.1773        \u001b[32m3.1998\u001b[0m  0.0288\n",
      "      8        3.1805        \u001b[32m3.1970\u001b[0m  0.0265\n",
      "      9        \u001b[36m2.7294\u001b[0m        2.7465  0.0236\n",
      "      9        3.0231        \u001b[32m3.0102\u001b[0m  0.0236\n",
      "      9        3.1717        \u001b[32m3.2053\u001b[0m  0.0246\n",
      "      9        3.1801        \u001b[32m3.1986\u001b[0m  0.0362\n",
      "      9        3.1854        \u001b[32m3.1951\u001b[0m  0.0352\n",
      "     10        2.7478        2.7467  0.0260\n",
      "Restoring best model from epoch 1.\n",
      "     10        \u001b[36m3.1542\u001b[0m        \u001b[32m3.2048\u001b[0m  0.0235\n",
      "Restoring best model from epoch 1.\n",
      "     10        \u001b[36m3.0035\u001b[0m        \u001b[32m3.0092\u001b[0m  0.0275\n",
      "Restoring best model from epoch 1.\n",
      "     10        \u001b[36m3.1737\u001b[0m        \u001b[32m3.1977\u001b[0m  0.0237\n",
      "Restoring best model from epoch 1.\n",
      "     10        \u001b[36m3.1765\u001b[0m        \u001b[32m3.1932\u001b[0m  0.0315\n",
      "Restoring best model from epoch 1.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.1184\u001b[0m        \u001b[32m3.1263\u001b[0m  0.0672\n",
      "      2        \u001b[36m3.1181\u001b[0m        \u001b[32m3.1252\u001b[0m  0.0455\n",
      "      3        \u001b[36m3.1108\u001b[0m        \u001b[32m3.1238\u001b[0m  0.0447\n",
      "      4        \u001b[36m3.1096\u001b[0m        \u001b[32m3.1215\u001b[0m  0.0463\n",
      "      5        \u001b[36m3.0983\u001b[0m        \u001b[32m3.1178\u001b[0m  0.0473\n",
      "      6        3.1015        \u001b[32m3.1137\u001b[0m  0.0461\n",
      "      7        \u001b[36m3.0944\u001b[0m        \u001b[32m3.1103\u001b[0m  0.0448\n",
      "      8        3.1033        \u001b[32m3.1072\u001b[0m  0.0436\n",
      "      9        3.1046        \u001b[32m3.1046\u001b[0m  0.0450\n",
      "     10        3.1023        \u001b[32m3.1019\u001b[0m  0.0442\n",
      "Restoring best model from epoch 1.\n",
      "max best test predictions 0.090223804\n",
      "integration values 3497\n",
      "Concordance Index 0.5495218443712551\n",
      "Integrated Brier Score: 0.18846907727085008\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.0338\u001b[0m        \u001b[32m3.0014\u001b[0m  0.0231\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.2195\u001b[0m        \u001b[32m3.1902\u001b[0m  0.0236\n",
      "      2        3.0365        \u001b[32m3.0012\u001b[0m  0.0219\n",
      "      2        \u001b[36m3.2169\u001b[0m        \u001b[32m3.1900\u001b[0m  0.0226\n",
      "      3        3.0401        \u001b[32m3.0011\u001b[0m  0.0225\n",
      "      3        3.2185        \u001b[32m3.1894\u001b[0m  0.0232\n",
      "      4        3.0369        \u001b[32m3.0010\u001b[0m  0.0232\n",
      "      4        3.2284        \u001b[32m3.1883\u001b[0m  0.0231\n",
      "      5        3.0347        \u001b[32m3.0010\u001b[0m  0.0232\n",
      "      5        \u001b[36m3.2034\u001b[0m        \u001b[32m3.1867\u001b[0m  0.0216\n",
      "      6        3.0339        3.0011  0.0226\n",
      "      6        3.2125        \u001b[32m3.1846\u001b[0m  0.0221\n",
      "      7        3.0395        3.0013  0.0225\n",
      "      7        3.2188        \u001b[32m3.1822\u001b[0m  0.0223\n",
      "      8        \u001b[36m3.0274\u001b[0m        3.0017  0.0223\n",
      "      8        3.2105        \u001b[32m3.1800\u001b[0m  0.0234\n",
      "      9        3.0306        3.0021  0.0238\n",
      "      9        3.2074        \u001b[32m3.1777\u001b[0m  0.0229\n",
      "     10        3.0349        3.0022  0.0231\n",
      "Restoring best model from epoch 1.\n",
      "     10        \u001b[36m3.1991\u001b[0m        \u001b[32m3.1752\u001b[0m  0.0221\n",
      "Restoring best model from epoch 1.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.7831\u001b[0m        \u001b[32m2.7592\u001b[0m  0.0432\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.2209\u001b[0m        \u001b[32m3.2017\u001b[0m  0.0430\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.2147\u001b[0m        \u001b[32m3.2240\u001b[0m  0.0619\n",
      "      2        2.7881        \u001b[32m2.7592\u001b[0m  0.0492\n",
      "      2        \u001b[36m3.2179\u001b[0m        \u001b[32m3.2014\u001b[0m  0.0514\n",
      "      2        3.2190        \u001b[32m3.2238\u001b[0m  0.0257\n",
      "      3        \u001b[36m2.7827\u001b[0m        2.7593  0.0239\n",
      "      3        \u001b[36m3.2053\u001b[0m        \u001b[32m3.2007\u001b[0m  0.0225\n",
      "      4        2.7976        2.7593  0.0269\n",
      "      3        3.2311        \u001b[32m3.2233\u001b[0m  0.0279\n",
      "      4        3.2154        \u001b[32m3.1999\u001b[0m  0.0278\n",
      "      5        2.7979        2.7593  0.0241\n",
      "      4        3.2393        \u001b[32m3.2225\u001b[0m  0.0260\n",
      "      5        3.2178        \u001b[32m3.1988\u001b[0m  0.0252\n",
      "      6        2.7908        2.7594  0.0224\n",
      "      5        3.2193        \u001b[32m3.2210\u001b[0m  0.0225\n",
      "      6        3.2174        \u001b[32m3.1977\u001b[0m  0.0226\n",
      "      7        2.7969        2.7595  0.0204\n",
      "      6        3.2167        \u001b[32m3.2186\u001b[0m  0.0205\n",
      "      7        3.2133        \u001b[32m3.1962\u001b[0m  0.0214\n",
      "      8        2.7992        2.7595  0.0208\n",
      "      7        3.2254        \u001b[32m3.2158\u001b[0m  0.0216\n",
      "      8        \u001b[36m3.1960\u001b[0m        \u001b[32m3.1946\u001b[0m  0.0217\n",
      "      9        2.7914        2.7596  0.0202\n",
      "      8        \u001b[36m3.2103\u001b[0m        \u001b[32m3.2126\u001b[0m  0.0197\n",
      "      9        3.1978        \u001b[32m3.1929\u001b[0m  0.0215\n",
      "     10        2.7977        2.7596  0.0207\n",
      "Restoring best model from epoch 1.\n",
      "      9        3.2172        \u001b[32m3.2091\u001b[0m  0.0202\n",
      "     10        \u001b[36m3.1937\u001b[0m        \u001b[32m3.1911\u001b[0m  0.0217\n",
      "Restoring best model from epoch 1.\n",
      "     10        \u001b[36m3.1997\u001b[0m        \u001b[32m3.2053\u001b[0m  0.0216\n",
      "Restoring best model from epoch 1.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.1230\u001b[0m        \u001b[32m3.1365\u001b[0m  0.0494\n",
      "      2        3.1232        \u001b[32m3.1354\u001b[0m  0.0432\n",
      "      3        \u001b[36m3.1212\u001b[0m        \u001b[32m3.1334\u001b[0m  0.0424\n",
      "      4        \u001b[36m3.1165\u001b[0m        \u001b[32m3.1307\u001b[0m  0.0425\n",
      "      5        \u001b[36m3.1136\u001b[0m        \u001b[32m3.1284\u001b[0m  0.0430\n",
      "      6        3.1176        \u001b[32m3.1269\u001b[0m  0.0429\n",
      "      7        \u001b[36m3.1086\u001b[0m        \u001b[32m3.1243\u001b[0m  0.0433\n",
      "      8        3.1170        3.1246  0.0428\n",
      "      9        \u001b[36m3.0938\u001b[0m        \u001b[32m3.1237\u001b[0m  0.0427\n",
      "     10        3.1085        \u001b[32m3.1218\u001b[0m  0.0421\n",
      "Restoring best model from epoch 1.\n",
      "max best test predictions 0.020634877\n",
      "integration values 2373\n",
      "Concordance Index 0.45090365592243703\n",
      "Integrated Brier Score: 0.1859181804092684\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.0298\u001b[0m        \u001b[32m3.0126\u001b[0m  0.0264\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.2212\u001b[0m        \u001b[32m3.1872\u001b[0m  0.0259\n",
      "      2        \u001b[36m3.0253\u001b[0m        3.0127  0.0221\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.2076\u001b[0m        \u001b[32m3.2207\u001b[0m  0.0232\n",
      "      2        3.2251        \u001b[32m3.1864\u001b[0m  0.0306\n",
      "      3        \u001b[36m3.0248\u001b[0m        3.0127  0.0286\n",
      "      2        \u001b[36m3.1957\u001b[0m        \u001b[32m3.2204\u001b[0m  0.0366\n",
      "      3        3.2221        \u001b[32m3.1853\u001b[0m  0.0254\n",
      "      4        3.0252        3.0133  0.0244\n",
      "      3        3.2005        \u001b[32m3.2202\u001b[0m  0.0215\n",
      "      4        3.2230        \u001b[32m3.1840\u001b[0m  0.0248\n",
      "      5        3.0283        3.0148  0.0226\n",
      "      4        3.2074        \u001b[32m3.2197\u001b[0m  0.0209\n",
      "      5        \u001b[36m3.2209\u001b[0m        \u001b[32m3.1822\u001b[0m  0.0229\n",
      "      6        \u001b[36m3.0214\u001b[0m        3.0166  0.0214\n",
      "      5        3.2032        \u001b[32m3.2190\u001b[0m  0.0205\n",
      "      6        \u001b[36m3.2154\u001b[0m        \u001b[32m3.1799\u001b[0m  0.0223\n",
      "      7        \u001b[36m3.0192\u001b[0m        3.0185  0.0214\n",
      "      6        3.1961        \u001b[32m3.2180\u001b[0m  0.0224\n",
      "      7        \u001b[36m3.2096\u001b[0m        \u001b[32m3.1775\u001b[0m  0.0234\n",
      "      8        \u001b[36m3.0152\u001b[0m        3.0192  0.0231\n",
      "      7        \u001b[36m3.1838\u001b[0m        \u001b[32m3.2165\u001b[0m  0.0216\n",
      "      8        \u001b[36m3.1847\u001b[0m        \u001b[32m3.1750\u001b[0m  0.0233\n",
      "      9        3.0254        3.0199  0.0223\n",
      "      8        3.1886        \u001b[32m3.2146\u001b[0m  0.0218\n",
      "      9        3.1943        \u001b[32m3.1727\u001b[0m  0.0242\n",
      "     10        3.0199        3.0207  0.0230\n",
      "Restoring best model from epoch 1.\n",
      "      9        3.1848        \u001b[32m3.2122\u001b[0m  0.0206\n",
      "     10        \u001b[36m3.1795\u001b[0m        \u001b[32m3.2097\u001b[0m  0.0201\n",
      "     10        3.1966        \u001b[32m3.1708\u001b[0m  0.0224\n",
      "Restoring best model from epoch 1.\n",
      "Restoring best model from epoch 1.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.1762\u001b[0m        \u001b[32m3.1888\u001b[0m  0.0383\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.7966\u001b[0m        \u001b[32m2.7562\u001b[0m  0.0393\n",
      "      2        3.1870        \u001b[32m3.1887\u001b[0m  0.0229\n",
      "      2        2.8174        2.7566  0.0233\n",
      "      3        3.1785        \u001b[32m3.1884\u001b[0m  0.0238\n",
      "      3        2.8019        2.7568  0.0238\n",
      "      4        2.8001        2.7568  0.0226\n",
      "      4        3.1887        \u001b[32m3.1880\u001b[0m  0.0237\n",
      "      5        \u001b[36m2.7924\u001b[0m        2.7569  0.0205\n",
      "      5        3.1824        \u001b[32m3.1876\u001b[0m  0.0215\n",
      "      6        2.8078        2.7570  0.0212\n",
      "      6        3.1793        \u001b[32m3.1866\u001b[0m  0.0217\n",
      "      7        2.7942        2.7571  0.0202\n",
      "      7        3.1792        \u001b[32m3.1854\u001b[0m  0.0211\n",
      "      8        \u001b[36m2.7811\u001b[0m        2.7571  0.0221\n",
      "      8        3.1814        \u001b[32m3.1840\u001b[0m  0.0226\n",
      "      9        2.7864        2.7571  0.0206\n",
      "      9        3.1848        \u001b[32m3.1824\u001b[0m  0.0206\n",
      "     10        2.7931        2.7572  0.0210\n",
      "Restoring best model from epoch 1.\n",
      "     10        \u001b[36m3.1704\u001b[0m        \u001b[32m3.1804\u001b[0m  0.0208\n",
      "Restoring best model from epoch 1.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.1164\u001b[0m        \u001b[32m3.1248\u001b[0m  0.0505\n",
      "      2        3.1265        \u001b[32m3.1239\u001b[0m  0.0425\n",
      "      3        \u001b[36m3.1114\u001b[0m        \u001b[32m3.1228\u001b[0m  0.0427\n",
      "      4        3.1178        \u001b[32m3.1214\u001b[0m  0.0430\n",
      "      5        \u001b[36m3.1083\u001b[0m        \u001b[32m3.1191\u001b[0m  0.0438\n",
      "      6        \u001b[36m3.1061\u001b[0m        \u001b[32m3.1155\u001b[0m  0.0433\n",
      "      7        \u001b[36m3.0936\u001b[0m        \u001b[32m3.1109\u001b[0m  0.0437\n",
      "      8        3.1022        \u001b[32m3.1060\u001b[0m  0.0429\n",
      "      9        \u001b[36m3.0934\u001b[0m        \u001b[32m3.1031\u001b[0m  0.0433\n",
      "     10        3.1027        \u001b[32m3.1017\u001b[0m  0.0440\n",
      "Restoring best model from epoch 1.\n",
      "max best test predictions 0.011085577\n",
      "integration values 1952\n",
      "Concordance Index 0.4587356025613068\n",
      "Integrated Brier Score: 0.19375177229610946\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.2046\u001b[0m        \u001b[32m3.1647\u001b[0m  0.0262\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.1472\u001b[0m        \u001b[32m3.1823\u001b[0m  0.0322\n",
      "      2        \u001b[36m3.2009\u001b[0m        \u001b[32m3.1641\u001b[0m  0.0275\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.2132\u001b[0m        \u001b[32m3.2416\u001b[0m  0.0328\n",
      "      2        \u001b[36m3.1350\u001b[0m        \u001b[32m3.1808\u001b[0m  0.0274\n",
      "      3        \u001b[36m3.1985\u001b[0m        \u001b[32m3.1632\u001b[0m  0.0241\n",
      "      2        \u001b[36m3.2013\u001b[0m        \u001b[32m3.2414\u001b[0m  0.0291\n",
      "      3        3.1400        \u001b[32m3.1790\u001b[0m  0.0288\n",
      "      4        \u001b[36m3.1980\u001b[0m        \u001b[32m3.1617\u001b[0m  0.0300\n",
      "      3        3.2059        \u001b[32m3.2411\u001b[0m  0.0252\n",
      "      4        \u001b[36m3.1278\u001b[0m        \u001b[32m3.1774\u001b[0m  0.0312\n",
      "      5        3.2015        \u001b[32m3.1599\u001b[0m  0.0225\n",
      "      4        \u001b[36m3.1890\u001b[0m        \u001b[32m3.2404\u001b[0m  0.0211\n",
      "      5        3.1412        \u001b[32m3.1753\u001b[0m  0.0259\n",
      "      6        \u001b[36m3.1963\u001b[0m        \u001b[32m3.1575\u001b[0m  0.0270\n",
      "      5        3.2085        \u001b[32m3.2397\u001b[0m  0.0266\n",
      "      6        3.1470        \u001b[32m3.1729\u001b[0m  0.0250\n",
      "      7        \u001b[36m3.1952\u001b[0m        \u001b[32m3.1548\u001b[0m  0.0246\n",
      "      6        3.1914        \u001b[32m3.2386\u001b[0m  0.0251\n",
      "      7        3.1341        \u001b[32m3.1696\u001b[0m  0.0259\n",
      "      8        \u001b[36m3.1926\u001b[0m        \u001b[32m3.1516\u001b[0m  0.0228\n",
      "      7        \u001b[36m3.1885\u001b[0m        \u001b[32m3.2371\u001b[0m  0.0231\n",
      "      9        \u001b[36m3.1905\u001b[0m        \u001b[32m3.1483\u001b[0m  0.0211\n",
      "      8        \u001b[36m3.1245\u001b[0m        \u001b[32m3.1663\u001b[0m  0.0231\n",
      "      8        3.1954        \u001b[32m3.2353\u001b[0m  0.0228\n",
      "     10        \u001b[36m3.1863\u001b[0m        \u001b[32m3.1445\u001b[0m  0.0206\n",
      "Restoring best model from epoch 1.\n",
      "      9        3.1355        \u001b[32m3.1627\u001b[0m  0.0230\n",
      "      9        \u001b[36m3.1759\u001b[0m        \u001b[32m3.2333\u001b[0m  0.0227\n",
      "     10        \u001b[36m3.1223\u001b[0m        \u001b[32m3.1592\u001b[0m  0.0253\n",
      "Restoring best model from epoch 1.\n",
      "     10        3.1850        \u001b[32m3.2310\u001b[0m  0.0268\n",
      "Restoring best model from epoch 1.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.7732\u001b[0m        \u001b[32m2.7550\u001b[0m  0.0422\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.0232\u001b[0m        \u001b[32m3.0125\u001b[0m  0.0437\n",
      "      2        2.7783        2.7551  0.0213\n",
      "      2        \u001b[36m3.0188\u001b[0m        \u001b[32m3.0121\u001b[0m  0.0221\n",
      "      3        \u001b[36m2.7722\u001b[0m        2.7552  0.0203\n",
      "      3        3.0208        \u001b[32m3.0119\u001b[0m  0.0210\n",
      "      4        2.7837        2.7552  0.0217\n",
      "      4        3.0200        \u001b[32m3.0117\u001b[0m  0.0242\n",
      "      5        \u001b[36m2.7609\u001b[0m        \u001b[32m2.7550\u001b[0m  0.0201\n",
      "      5        3.0322        \u001b[32m3.0114\u001b[0m  0.0206\n",
      "      6        2.7679        \u001b[32m2.7546\u001b[0m  0.0200\n",
      "      6        \u001b[36m3.0186\u001b[0m        \u001b[32m3.0111\u001b[0m  0.0217\n",
      "      7        \u001b[36m2.7561\u001b[0m        \u001b[32m2.7543\u001b[0m  0.0205\n",
      "      7        3.0208        3.0112  0.0207\n",
      "      8        2.7736        \u001b[32m2.7540\u001b[0m  0.0202\n",
      "      8        3.0246        3.0117  0.0202\n",
      "      9        \u001b[36m2.7560\u001b[0m        \u001b[32m2.7538\u001b[0m  0.0217\n",
      "      9        3.0189        3.0121  0.0235\n",
      "     10        2.7650        \u001b[32m2.7537\u001b[0m  0.0214\n",
      "Restoring best model from epoch 1.\n",
      "     10        3.0219        3.0128  0.0216\n",
      "Restoring best model from epoch 1.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.1224\u001b[0m        \u001b[32m3.1196\u001b[0m  0.0502\n",
      "      2        \u001b[36m3.1222\u001b[0m        \u001b[32m3.1185\u001b[0m  0.0436\n",
      "      3        \u001b[36m3.1160\u001b[0m        \u001b[32m3.1168\u001b[0m  0.0424\n",
      "      4        \u001b[36m3.1111\u001b[0m        \u001b[32m3.1148\u001b[0m  0.0428\n",
      "      5        3.1192        \u001b[32m3.1123\u001b[0m  0.0426\n",
      "      6        \u001b[36m3.1091\u001b[0m        \u001b[32m3.1096\u001b[0m  0.0434\n",
      "      7        3.1119        \u001b[32m3.1066\u001b[0m  0.0437\n",
      "      8        \u001b[36m3.1040\u001b[0m        \u001b[32m3.1035\u001b[0m  0.0437\n",
      "      9        \u001b[36m3.1005\u001b[0m        \u001b[32m3.1002\u001b[0m  0.0441\n",
      "     10        3.1098        \u001b[32m3.0964\u001b[0m  0.0434\n",
      "Restoring best model from epoch 1.\n",
      "max best test predictions -0.009570854\n",
      "integration values 1908\n",
      "Concordance Index 0.4344267364905264\n",
      "Integrated Brier Score: 0.17399020782346603\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.7873\u001b[0m        \u001b[32m2.7600\u001b[0m  0.0241\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.0119\u001b[0m        \u001b[32m2.9835\u001b[0m  0.0263\n",
      "      2        \u001b[36m2.7649\u001b[0m        2.7603  0.0250\n",
      "      2        \u001b[36m3.0078\u001b[0m        \u001b[32m2.9833\u001b[0m  0.0236\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.2156\u001b[0m        \u001b[32m3.1851\u001b[0m  0.0346\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.1891\u001b[0m        \u001b[32m3.2126\u001b[0m  0.0305\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.2411\u001b[0m        \u001b[32m3.2065\u001b[0m  0.0310\n",
      "      3        2.7830        2.7606  0.0234\n",
      "      3        \u001b[36m2.9963\u001b[0m        \u001b[32m2.9831\u001b[0m  0.0281\n",
      "      2        \u001b[36m3.2066\u001b[0m        \u001b[32m3.1842\u001b[0m  0.0262\n",
      "      2        \u001b[36m3.2363\u001b[0m        \u001b[32m3.2052\u001b[0m  0.0267\n",
      "      2        \u001b[36m3.1880\u001b[0m        \u001b[32m3.2121\u001b[0m  0.0268\n",
      "      4        2.7907        2.7606  0.0231\n",
      "      4        3.0084        \u001b[32m2.9829\u001b[0m  0.0254\n",
      "      3        \u001b[36m3.1898\u001b[0m        \u001b[32m3.1831\u001b[0m  0.0247\n",
      "      3        \u001b[36m3.2302\u001b[0m        \u001b[32m3.2037\u001b[0m  0.0282\n",
      "      3        3.1999        \u001b[32m3.2113\u001b[0m  0.0286\n",
      "      5        2.7924        2.7603  0.0240\n",
      "      5        \u001b[36m2.9948\u001b[0m        \u001b[32m2.9825\u001b[0m  0.0223\n",
      "      4        3.2146        \u001b[32m3.1820\u001b[0m  0.0270\n",
      "      4        \u001b[36m3.2276\u001b[0m        \u001b[32m3.2019\u001b[0m  0.0270\n",
      "      4        3.1921        \u001b[32m3.2103\u001b[0m  0.0277\n",
      "      6        2.7912        \u001b[32m2.7598\u001b[0m  0.0278\n",
      "      6        3.0101        \u001b[32m2.9822\u001b[0m  0.0242\n",
      "      5        3.2127        \u001b[32m3.1807\u001b[0m  0.0255\n",
      "      5        \u001b[36m3.2246\u001b[0m        \u001b[32m3.1998\u001b[0m  0.0275\n",
      "      5        \u001b[36m3.1872\u001b[0m        \u001b[32m3.2088\u001b[0m  0.0269\n",
      "      7        2.7869        \u001b[32m2.7593\u001b[0m  0.0234\n",
      "      7        3.0097        \u001b[32m2.9820\u001b[0m  0.0243\n",
      "      6        3.2055        \u001b[32m3.1793\u001b[0m  0.0262\n",
      "      6        \u001b[36m3.1819\u001b[0m        \u001b[32m3.2070\u001b[0m  0.0279\n",
      "      8        2.7858        \u001b[32m2.7587\u001b[0m  0.0269\n",
      "      6        \u001b[36m3.2215\u001b[0m        \u001b[32m3.1977\u001b[0m  0.0307\n",
      "      8        \u001b[36m2.9908\u001b[0m        \u001b[32m2.9818\u001b[0m  0.0295\n",
      "      7        3.2127        \u001b[32m3.1779\u001b[0m  0.0247\n",
      "      7        3.1847        \u001b[32m3.2049\u001b[0m  0.0242\n",
      "      9        2.7912        \u001b[32m2.7582\u001b[0m  0.0259\n",
      "      7        3.2298        \u001b[32m3.1951\u001b[0m  0.0274\n",
      "      9        3.0040        \u001b[32m2.9817\u001b[0m  0.0242\n",
      "      8        3.2057        \u001b[32m3.1768\u001b[0m  0.0233\n",
      "     10        2.7694        \u001b[32m2.7576\u001b[0m  0.0254\n",
      "Restoring best model from epoch 1.\n",
      "      8        3.1820        \u001b[32m3.2026\u001b[0m  0.0299\n",
      "      8        3.2372        \u001b[32m3.1925\u001b[0m  0.0263\n",
      "     10        3.0121        \u001b[32m2.9815\u001b[0m  0.0256\n",
      "Restoring best model from epoch 1.\n",
      "      9        3.1983        \u001b[32m3.1759\u001b[0m  0.0229\n",
      "      9        \u001b[36m3.1675\u001b[0m        \u001b[32m3.2000\u001b[0m  0.0238\n",
      "      9        \u001b[36m3.2198\u001b[0m        \u001b[32m3.1896\u001b[0m  0.0235\n",
      "     10        3.2001        \u001b[32m3.1755\u001b[0m  0.0202\n",
      "Restoring best model from epoch 1.\n",
      "     10        3.1826        \u001b[32m3.1972\u001b[0m  0.0226\n",
      "Restoring best model from epoch 1.\n",
      "     10        3.2294        \u001b[32m3.1867\u001b[0m  0.0257\n",
      "Restoring best model from epoch 1.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.1291\u001b[0m        \u001b[32m3.1231\u001b[0m  0.0477\n",
      "      2        \u001b[36m3.1190\u001b[0m        \u001b[32m3.1212\u001b[0m  0.0433\n",
      "      3        \u001b[36m3.1173\u001b[0m        \u001b[32m3.1175\u001b[0m  0.0429\n",
      "      4        \u001b[36m3.1134\u001b[0m        \u001b[32m3.1117\u001b[0m  0.0437\n",
      "      5        \u001b[36m3.1094\u001b[0m        \u001b[32m3.1059\u001b[0m  0.0429\n",
      "      6        3.1098        \u001b[32m3.1028\u001b[0m  0.0426\n",
      "      7        \u001b[36m3.1088\u001b[0m        \u001b[32m3.1008\u001b[0m  0.0433\n",
      "      8        \u001b[36m3.1019\u001b[0m        3.1013  0.0430\n",
      "      9        3.1127        3.1041  0.0429\n",
      "     10        3.1069        3.1053  0.0432\n",
      "Restoring best model from epoch 1.\n",
      "max best test predictions -0.005037224\n",
      "integration values 2144\n",
      "Concordance Index 0.40728735218200374\n",
      "Integrated Brier Score: 0.18296061721174167\n",
      "load_flchain\n",
      "split age           float32\n",
      "sex             uint8\n",
      "sample_yr     float32\n",
      "kappa         float32\n",
      "lambda        float32\n",
      "flc_grp       float32\n",
      "creatinine    float32\n",
      "mgus            uint8\n",
      "dtype: object\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m1.6368\u001b[0m        \u001b[32m1.5675\u001b[0m  0.1453\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.1904\u001b[0m        \u001b[32m2.1683\u001b[0m  0.1632\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.9065\u001b[0m        \u001b[32m2.8666\u001b[0m  0.1815\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.1279\u001b[0m        \u001b[32m3.0857\u001b[0m  0.1828\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.1783\u001b[0m        \u001b[32m3.1486\u001b[0m  0.1776\n",
      "      2        \u001b[36m2.1232\u001b[0m        \u001b[32m2.1011\u001b[0m  0.1571\n",
      "      2        \u001b[36m2.8787\u001b[0m        \u001b[32m2.8287\u001b[0m  0.1520\n",
      "      2        \u001b[36m1.6121\u001b[0m        \u001b[32m1.5455\u001b[0m  0.1982\n",
      "      2        \u001b[36m3.0959\u001b[0m        \u001b[32m3.0405\u001b[0m  0.1521\n",
      "      2        \u001b[36m3.1626\u001b[0m        \u001b[32m3.1133\u001b[0m  0.1620\n",
      "      3        \u001b[36m2.0678\u001b[0m        \u001b[32m2.0528\u001b[0m  0.1540\n",
      "      3        \u001b[36m1.5736\u001b[0m        \u001b[32m1.5375\u001b[0m  0.1534\n",
      "      3        \u001b[36m3.0301\u001b[0m        \u001b[32m2.9653\u001b[0m  0.1582\n",
      "      3        \u001b[36m3.1076\u001b[0m        \u001b[32m3.0460\u001b[0m  0.1532\n",
      "      3        \u001b[36m2.8210\u001b[0m        \u001b[32m2.7527\u001b[0m  0.2039\n",
      "      4        \u001b[36m2.0439\u001b[0m        \u001b[32m2.0302\u001b[0m  0.1526\n",
      "      4        \u001b[36m1.5620\u001b[0m        \u001b[32m1.5368\u001b[0m  0.1545\n",
      "      4        \u001b[36m2.9915\u001b[0m        \u001b[32m2.9335\u001b[0m  0.1657\n",
      "      4        \u001b[36m3.0550\u001b[0m        \u001b[32m2.9970\u001b[0m  0.1716\n",
      "      4        \u001b[36m2.7792\u001b[0m        \u001b[32m2.7031\u001b[0m  0.2097\n",
      "      5        \u001b[36m1.5613\u001b[0m        1.5375  0.1283\n",
      "      5        \u001b[36m2.9693\u001b[0m        \u001b[32m2.9223\u001b[0m  0.1373\n",
      "      5        \u001b[36m3.0248\u001b[0m        \u001b[32m2.9756\u001b[0m  0.1412\n",
      "      5        \u001b[36m2.0305\u001b[0m        \u001b[32m2.0172\u001b[0m  0.2157\n",
      "      5        \u001b[36m2.7661\u001b[0m        \u001b[32m2.6905\u001b[0m  0.1344\n",
      "      6        1.5670        1.5377  0.1224\n",
      "      6        \u001b[36m3.0085\u001b[0m        \u001b[32m2.9647\u001b[0m  0.1466\n",
      "      6        2.9705        \u001b[32m2.9122\u001b[0m  0.1826\n",
      "      6        2.0329        \u001b[32m2.0151\u001b[0m  0.1808\n",
      "      7        1.5628        \u001b[32m1.5363\u001b[0m  0.1373\n",
      "      6        \u001b[36m2.7514\u001b[0m        \u001b[32m2.6827\u001b[0m  0.1521\n",
      "      7        \u001b[36m2.9895\u001b[0m        \u001b[32m2.9570\u001b[0m  0.1565\n",
      "      7        \u001b[36m2.9577\u001b[0m        2.9149  0.1569\n",
      "      7        2.0341        \u001b[32m2.0144\u001b[0m  0.1398\n",
      "      7        \u001b[36m2.7374\u001b[0m        \u001b[32m2.6718\u001b[0m  0.1473\n",
      "      8        1.5627        \u001b[32m1.5347\u001b[0m  0.1635\n",
      "      8        \u001b[36m2.9875\u001b[0m        \u001b[32m2.9537\u001b[0m  0.1450\n",
      "      8        2.9591        2.9218  0.1354\n",
      "      8        \u001b[36m2.0188\u001b[0m        \u001b[32m2.0101\u001b[0m  0.1468\n",
      "      9        1.5650        1.5370  0.1206\n",
      "      8        2.7374        \u001b[32m2.6658\u001b[0m  0.2182\n",
      "      9        \u001b[36m2.9838\u001b[0m        2.9553  0.1504\n",
      "      9        2.9634        2.9197  0.1420\n",
      "      9        2.0245        2.0148  0.1421\n",
      "     10        1.5665        1.5374  0.1375\n",
      "Restoring best model from epoch 2.\n",
      "      9        2.7405        \u001b[32m2.6637\u001b[0m  0.1432\n",
      "     10        2.9883        \u001b[32m2.9505\u001b[0m  0.1379\n",
      "Restoring best model from epoch 6.\n",
      "     10        \u001b[36m2.9491\u001b[0m        \u001b[32m2.9120\u001b[0m  0.1404\n",
      "Restoring best model from epoch 4.\n",
      "     10        2.0221        2.0110  0.1323\n",
      "Restoring best model from epoch 4.\n",
      "     10        \u001b[36m2.7320\u001b[0m        \u001b[32m2.6621\u001b[0m  0.1193\n",
      "Restoring best model from epoch 7.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.6218\u001b[0m        \u001b[32m2.6022\u001b[0m  0.1735\n",
      "      2        \u001b[36m2.5593\u001b[0m        \u001b[32m2.5429\u001b[0m  0.1704\n",
      "      3        \u001b[36m2.5073\u001b[0m        \u001b[32m2.4908\u001b[0m  0.1684\n",
      "      4        \u001b[36m2.4776\u001b[0m        \u001b[32m2.4719\u001b[0m  0.1695\n",
      "      5        \u001b[36m2.4668\u001b[0m        \u001b[32m2.4711\u001b[0m  0.1710\n",
      "      6        \u001b[36m2.4615\u001b[0m        \u001b[32m2.4566\u001b[0m  0.1705\n",
      "      7        \u001b[36m2.4525\u001b[0m        2.4620  0.1688\n",
      "      8        2.4555        \u001b[32m2.4548\u001b[0m  0.1706\n",
      "      9        \u001b[36m2.4500\u001b[0m        \u001b[32m2.4427\u001b[0m  0.1700\n",
      "     10        \u001b[36m2.4451\u001b[0m        2.4448  0.1691\n",
      "Restoring best model from epoch 6.\n",
      "max best test predictions 7.8583584\n",
      "integration values 405109\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 61\u001b[0m\n\u001b[1;32m     16\u001b[0m X, y \u001b[39m=\u001b[39m sort_X_y_pandas(X, y)\n\u001b[1;32m     18\u001b[0m net \u001b[39m=\u001b[39m NeuralNet(\n\u001b[1;32m     19\u001b[0m     SurvivalModel, \n\u001b[1;32m     20\u001b[0m     module__n_layers \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     59\u001b[0m     verbose\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m\n\u001b[1;32m     60\u001b[0m )\n\u001b[0;32m---> 61\u001b[0m df_agg_metrics_cindex, df_agg_metrics_ibs, best_model,params, outer_scores, best_preds_train, best_preds_test \u001b[39m=\u001b[39m train_eval(X, y, net, n_iter, data\u001b[39m.\u001b[39;49mfilename)\n\u001b[1;32m     62\u001b[0m agg_metrics_cindex\u001b[39m.\u001b[39mappend(df_agg_metrics_cindex)\n\u001b[1;32m     63\u001b[0m agg_metrics_ibs\u001b[39m.\u001b[39mappend(df_agg_metrics_ibs)\n",
      "Cell \u001b[0;32mIn[10], line 58\u001b[0m, in \u001b[0;36mtrain_eval\u001b[0;34m(X, y, net, n_iter, filename)\u001b[0m\n\u001b[1;32m     55\u001b[0m outer_scores[\u001b[39m'\u001b[39m\u001b[39mibs_train_\u001b[39m\u001b[39m'\u001b[39m\u001b[39m+\u001b[39mdataset_name] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m [np\u001b[39m.\u001b[39mnan]\n\u001b[1;32m     57\u001b[0m \u001b[39m#try:\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m cum_hazard_test \u001b[39m=\u001b[39m get_cumulative_hazard_function_ah(\n\u001b[1;32m     59\u001b[0m         X_train\u001b[39m.\u001b[39;49mvalues, X_test\u001b[39m.\u001b[39;49mvalues, y_train\u001b[39m.\u001b[39;49mvalues, y_test\u001b[39m.\u001b[39;49mvalues,\n\u001b[1;32m     60\u001b[0m         best_preds_train\u001b[39m.\u001b[39;49mreshape(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m), best_preds_test\u001b[39m.\u001b[39;49mreshape(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m     61\u001b[0m         )\n\u001b[1;32m     62\u001b[0m df_survival_test \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mexp(\u001b[39m-\u001b[39mcum_hazard_test)\n\u001b[1;32m     63\u001b[0m durations_test, events_test \u001b[39m=\u001b[39m transform_back(y_test\u001b[39m.\u001b[39mvalues)\n",
      "File \u001b[0;32m~/miniconda3/envs/xgbsurv/lib/python3.11/site-packages/xgbsurv/models/eh_ah_final.py:420\u001b[0m, in \u001b[0;36mget_cumulative_hazard_function_ah\u001b[0;34m(X_train, X_test, y_train, y_test, predictor_train, predictor_test)\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mintegration values\u001b[39m\u001b[39m'\u001b[39m,integration_values\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m])\n\u001b[1;32m    417\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, integration_values\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]):\n\u001b[1;32m    418\u001b[0m     integration_values[_] \u001b[39m=\u001b[39m (\n\u001b[1;32m    419\u001b[0m         integration_values[_ \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m]\n\u001b[0;32m--> 420\u001b[0m         \u001b[39m+\u001b[39m quadrature(\n\u001b[1;32m    421\u001b[0m             func\u001b[39m=\u001b[39;49mhazard_function_integrate,\n\u001b[1;32m    422\u001b[0m             a\u001b[39m=\u001b[39;49mintegration_times[_ \u001b[39m-\u001b[39;49m \u001b[39m1\u001b[39;49m],\n\u001b[1;32m    423\u001b[0m             b\u001b[39m=\u001b[39;49mintegration_times[_],\n\u001b[1;32m    424\u001b[0m             vec_func\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    425\u001b[0m         )[\u001b[39m0\u001b[39m]\n\u001b[1;32m    426\u001b[0m     )\n\u001b[1;32m    427\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_samples):\n\u001b[1;32m    428\u001b[0m     cumulative_hazard[_] \u001b[39m=\u001b[39m (\n\u001b[1;32m    429\u001b[0m         integration_values[\n\u001b[1;32m    430\u001b[0m             np\u001b[39m.\u001b[39mdigitize(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    435\u001b[0m         \u001b[39m/\u001b[39m theta[_]\n\u001b[1;32m    436\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/xgbsurv/lib/python3.11/site-packages/scipy/integrate/_quadrature.py:365\u001b[0m, in \u001b[0;36mquadrature\u001b[0;34m(func, a, b, args, tol, rtol, maxiter, vec_func, miniter)\u001b[0m\n\u001b[1;32m    363\u001b[0m maxiter \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(miniter\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m, maxiter)\n\u001b[1;32m    364\u001b[0m \u001b[39mfor\u001b[39;00m n \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(miniter, maxiter\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m--> 365\u001b[0m     newval \u001b[39m=\u001b[39m fixed_quad(vfunc, a, b, (), n)[\u001b[39m0\u001b[39m]\n\u001b[1;32m    366\u001b[0m     err \u001b[39m=\u001b[39m \u001b[39mabs\u001b[39m(newval\u001b[39m-\u001b[39mval)\n\u001b[1;32m    367\u001b[0m     val \u001b[39m=\u001b[39m newval\n",
      "File \u001b[0;32m~/miniconda3/envs/xgbsurv/lib/python3.11/site-packages/scipy/integrate/_quadrature.py:245\u001b[0m, in \u001b[0;36mfixed_quad\u001b[0;34m(func, a, b, args, n)\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mGaussian quadrature is only available for \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    243\u001b[0m                      \u001b[39m\"\u001b[39m\u001b[39mfinite limits.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    244\u001b[0m y \u001b[39m=\u001b[39m (b\u001b[39m-\u001b[39ma)\u001b[39m*\u001b[39m(x\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m/\u001b[39m\u001b[39m2.0\u001b[39m \u001b[39m+\u001b[39m a\n\u001b[0;32m--> 245\u001b[0m \u001b[39mreturn\u001b[39;00m (b\u001b[39m-\u001b[39ma)\u001b[39m/\u001b[39m\u001b[39m2.0\u001b[39m \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39msum(w\u001b[39m*\u001b[39mfunc(y, \u001b[39m*\u001b[39;49margs), axis\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m), \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/xgbsurv/lib/python3.11/site-packages/scipy/integrate/_quadrature.py:288\u001b[0m, in \u001b[0;36mvectorize1.<locals>.vfunc\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    286\u001b[0m output[\u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m y0\n\u001b[1;32m    287\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, n):\n\u001b[0;32m--> 288\u001b[0m     output[i] \u001b[39m=\u001b[39m func(x[i], \u001b[39m*\u001b[39;49margs)\n\u001b[1;32m    289\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/miniconda3/envs/xgbsurv/lib/python3.11/site-packages/xgbsurv/models/eh_ah_final.py:393\u001b[0m, in \u001b[0;36mget_cumulative_hazard_function_ah.<locals>.hazard_function_integrate\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mhazard_function_integrate\u001b[39m(s):\n\u001b[0;32m--> 393\u001b[0m     \u001b[39mreturn\u001b[39;00m baseline_hazard_estimator_ah(\n\u001b[1;32m    394\u001b[0m         time\u001b[39m=\u001b[39;49ms,\n\u001b[1;32m    395\u001b[0m         time_train\u001b[39m=\u001b[39;49mtime_train,\n\u001b[1;32m    396\u001b[0m         event_train\u001b[39m=\u001b[39;49mevent_train,\n\u001b[1;32m    397\u001b[0m         predictor_train\u001b[39m=\u001b[39;49mpredictor_train,\n\u001b[1;32m    398\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data_set_fns = [ load_metabric, load_flchain, load_rgbsg, load_support] #load_metabric, , load_flchain, load_rgbsg, load_support, load_tcga]\n",
    "data_set_fns_str = ['load_metabric', 'load_flchain', 'load_rgbsg', 'load_support'] \n",
    "one_hot_dict = {'load_flchain': ['mgus'], 'load_support':['cancer'], 'load_rgbsg':['grade']}\n",
    "agg_metrics_cindex = []\n",
    "agg_metrics_ibs = []\n",
    "\n",
    "for idx, dataset in enumerate(data_set_fns):\n",
    "    # get name of current dataset\n",
    "    data = dataset(path=\"/Users/JUSC/Documents/xgbsurv/xgbsurv/datasets/data/\", as_frame=True)\n",
    "    X  = data.data #.astype(np.float32)\n",
    "    y = data.target #.values #.to_numpy()\n",
    "\n",
    "    print(data_set_fns_str[idx])\n",
    "    #if data_set_fns_str[idx] in one_hot_dict.keys():\n",
    "    #    X = pd.get_dummies(X, columns=one_hot_dict[data_set_fns_str[idx]])\n",
    "    X, y = sort_X_y_pandas(X, y)\n",
    "    \n",
    "    net = NeuralNet(\n",
    "        SurvivalModel, \n",
    "        module__n_layers = 1,\n",
    "        module__input_units = X.shape[1],\n",
    "        #module__num_nodes = 32,\n",
    "        #module__dropout = 0.1, # these could also be removed\n",
    "        module__out_features = 1,\n",
    "        # for split sizes when result size = 1\n",
    "        iterator_train__drop_last=True,\n",
    "        #iterator_valid__drop_last=True,\n",
    "        criterion=AHLoss,\n",
    "        optimizer=torch.optim.AdamW,\n",
    "        optimizer__weight_decay = 0.4,\n",
    "        batch_size=32, # separate train and valid->iterator_train__batch_size=128 and iterator_valid__batch_size=128 ?\n",
    "        callbacks=[\n",
    "            (\n",
    "                \"sched\",\n",
    "                LRScheduler(\n",
    "                    torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
    "                    monitor=\"valid_loss\",\n",
    "                    patience=5,\n",
    "                ),\n",
    "            ),\n",
    "            (\n",
    "                \"es\",\n",
    "                EarlyStopping(\n",
    "                    monitor=\"valid_loss\",\n",
    "                    patience=early_stopping_rounds,\n",
    "                    load_best=True,\n",
    "                    threshold = 0.01,\n",
    "                    #, threshold_mode='rel', \n",
    "                    # lower_is_better=True\n",
    "                ),\n",
    "            ),\n",
    "            (\"seed\", FixSeed(seed=42)),\n",
    "            #(\"Input Shape Setter\",InputShapeSetter())\n",
    "        ],\n",
    "        \n",
    "        train_split = CustomValidSplit(0.2, stratified=True, random_state=rand_state), \n",
    "        #max_epochs=1, #0,#100\n",
    "        #train_split=None,\n",
    "        verbose=2\n",
    "    )\n",
    "    df_agg_metrics_cindex, df_agg_metrics_ibs, best_model,params, outer_scores, best_preds_train, best_preds_test = train_eval(X, y, net, n_iter, data.filename)\n",
    "    agg_metrics_cindex.append(df_agg_metrics_cindex)\n",
    "    agg_metrics_ibs.append(df_agg_metrics_ibs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>cindex_train_mean</th>\n",
       "      <th>cindex_train_std</th>\n",
       "      <th>cindex_test_mean</th>\n",
       "      <th>cindex_test_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>METABRIC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FLCHAIN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RGBSG</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SUPPORT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    dataset  cindex_train_mean  cindex_train_std  cindex_test_mean   \n",
       "0  METABRIC                NaN               NaN               NaN  \\\n",
       "0   FLCHAIN                NaN               NaN               NaN   \n",
       "0     RGBSG                NaN               NaN               NaN   \n",
       "0   SUPPORT                NaN               NaN               NaN   \n",
       "\n",
       "   cindex_test_std  \n",
       "0              NaN  \n",
       "0              NaN  \n",
       "0              NaN  \n",
       "0              NaN  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final_ah_1_cindex = pd.concat([df for df in agg_metrics_cindex]).round(4)\n",
    "df_final_ah_1_cindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>cindex_train_mean</th>\n",
       "      <th>cindex_train_std</th>\n",
       "      <th>cindex_test_mean</th>\n",
       "      <th>cindex_test_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>METABRIC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FLCHAIN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RGBSG</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SUPPORT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    dataset  cindex_train_mean  cindex_train_std  cindex_test_mean   \n",
       "0  METABRIC                NaN               NaN               NaN  \\\n",
       "0   FLCHAIN                NaN               NaN               NaN   \n",
       "0     RGBSG                NaN               NaN               NaN   \n",
       "0   SUPPORT                NaN               NaN               NaN   \n",
       "\n",
       "   cindex_test_std  \n",
       "0              NaN  \n",
       "0              NaN  \n",
       "0              NaN  \n",
       "0              NaN  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final_ah_1_cindex = pd.concat([df for df in agg_metrics_cindex]).round(4)\n",
    "df_final_ah_1_cindex.to_csv('metrics/final_deep_learning_ah_1_cindex.csv', index=False)\n",
    "df_final_ah_1_cindex.to_csv('/Users/JUSC/Documents/644928e0fb7e147893e8ec15/05_thesis/tables/final_deep_learning_ah_1_cindex.csv', index=False)  #\n",
    "df_final_ah_1_cindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>ibs_train_mean</th>\n",
       "      <th>ibs_train_std</th>\n",
       "      <th>ibs_test_mean</th>\n",
       "      <th>ibs_test_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>METABRIC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FLCHAIN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RGBSG</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SUPPORT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    dataset  ibs_train_mean  ibs_train_std  ibs_test_mean  ibs_test_std\n",
       "0  METABRIC             NaN            NaN            NaN           NaN\n",
       "0   FLCHAIN             NaN            NaN            NaN           NaN\n",
       "0     RGBSG             NaN            NaN            NaN           NaN\n",
       "0   SUPPORT             NaN            NaN            NaN           NaN"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final_ah_1_ibs = pd.concat([df for df in agg_metrics_ibs]).round(4)\n",
    "df_final_ah_1_ibs.to_csv('metrics/final_deep_learning_ah_1_ibs.csv', index=False)\n",
    "df_final_ah_1_ibs.to_csv('/Users/JUSC/Documents/644928e0fb7e147893e8ec15/05_thesis/tables/final_deep_learning_ah_1_ibs.csv', index=False) \n",
    "df_final_ah_1_ibs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TCGA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_ah_tcga = {\n",
    "    'estimator__module__n_layers': [1, 2, 4],\n",
    "    'estimator__module__num_nodes': [64, 128, 256, 512],\n",
    "    'estimator__module__dropout': scuniform(0.0,0.7),\n",
    "    'estimator__optimizer__weight_decay': [0.4, 0.2, 0.1, 0.05, 0.02, 0.01, 0],\n",
    "    'estimator__batch_size': [64, 128, 256, 512, 1024],\n",
    "    #lr not in paper because of learning rate finder\n",
    "    # note: setting learning rate higher would make exp(partial_hazard) explode\n",
    "    #'estimator__lr': scloguniform(0.001,0.01), # scheduler unten einbauen\n",
    "    # use callback instead\n",
    "    'estimator__lr':[0.001],\n",
    "    #'estimator__max_epochs':  scrandint(10,20), # corresponds to num_rounds\n",
    "    'pca__n_components':[8, 16, 32, 64]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_eval(X, y, net, n_iter, filename):\n",
    "        model = 'ah_'\n",
    "        dataset_name = filename.split('_')[0]\n",
    "        # add IBS later\n",
    "        outer_scores = { 'cindex_test_'+dataset_name:[],\n",
    "                        'ibs_test_'+dataset_name:[]}\n",
    "        best_params = {'best_params_'+dataset_name:[]}\n",
    "        best_model = {'best_model_'+dataset_name:[]}\n",
    "        ct = make_column_transformer(\n",
    "                #(OneHotEncoder(sparse_output=False), make_column_selector(dtype_include=['category', 'object']))\n",
    "                (StandardScaler(), make_column_selector(dtype_include=['float32']))\n",
    "                ,remainder='drop')\n",
    "        pipe = Pipeline([('scaler',ct),\n",
    "                         ('pca', PCA()),#n_components=10\n",
    "                        ('estimator', net)])\n",
    "        rs = RandomizedSearchCV(pipe, param_grid_ah_tcga, scoring = scoring_function, n_jobs=-1, \n",
    "                                    n_iter=2, refit=True, random_state=rand_state)\n",
    "        for i, (train_index, test_index) in enumerate(outer_custom_cv.split(X, y)):\n",
    "                # Split data into training and testing sets for outer fold\n",
    "                X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "                y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "                X_train, y_train = sort_X_y_pandas(X_train, y_train)\n",
    "                X_test, y_test = sort_X_y_pandas(X_test, y_test)\n",
    "\n",
    "                #print(X_train.shape, type(X_train))\n",
    "                #print(y_train.shape, type(y_train))\n",
    "                #print(X_test.shape, type(X_test))\n",
    "                #print(y_test.shape, type(y_test))\n",
    "                # save splits and data\n",
    "                savetxt('splits/train_index_'+str(i)+'_'+filename, train_index, delimiter=',')\n",
    "                savetxt('splits/test_index_'+str(i)+'_'+filename, test_index, delimiter=',')\n",
    "                \n",
    "                # savetxt('splits/X_train_'+str(i)+'_'+filename, X_train, delimiter=',')\n",
    "                # savetxt('splits/X_test_'+str(i)+'_'+filename, X_test, delimiter=',')\n",
    "\n",
    "                # savetxt('splits/y_train_'+str(i)+'_'+filename, y_train, delimiter=',')\n",
    "                # savetxt('splits/y_test_'+str(i)+'_'+filename, y_test, delimiter=',')\n",
    "\n",
    "                rs.fit(X_train, y_train)\n",
    "                best_preds_train = rs.best_estimator_.predict(X_train)\n",
    "                best_preds_test = rs.best_estimator_.predict(X_test)\n",
    "\n",
    "                savetxt('predictions/'+model+'best_preds_train_'+str(i)+'_'+filename, best_preds_train, delimiter=',')\n",
    "                savetxt('predictions/'+model+'best_preds_test_'+str(i)+'_'+filename, best_preds_test, delimiter=',')\n",
    "\n",
    "                \n",
    "                # save hyperparameter settings\n",
    "                params = rs.best_estimator_.get_params\n",
    "                best_params['best_params_'+dataset_name] += [rs.best_params_]\n",
    "                best_model['best_model_'+dataset_name] += [params]\n",
    "\n",
    "                    \n",
    "                try:\n",
    "                    cum_hazard_test = get_cumulative_hazard_function_ah(\n",
    "                            X_train.values, X_test.values, y_train.values, y_test.values,\n",
    "                            best_preds_train.reshape(-1), best_preds_test.reshape(-1)\n",
    "                            )\n",
    "                    df_survival_test = np.exp(-cum_hazard_test)\n",
    "                    durations_test, events_test = transform_back(y_test.values)\n",
    "                    time_grid_test = np.linspace(durations_test.min(), durations_test.max(), 100)\n",
    "                    ev = EvalSurv(df_survival_test, durations_test, events_test, censor_surv='km')\n",
    "                    print('Concordance Index',ev.concordance_td('antolini'))\n",
    "                    print('Integrated Brier Score:',ev.integrated_brier_score(time_grid_test))\n",
    "                    cindex_score_test = ev.concordance_td('antolini')\n",
    "                    ibs_score_test = ev.integrated_brier_score(time_grid_test)\n",
    "\n",
    "                    outer_scores['cindex_test_'+dataset_name] += [cindex_score_test]\n",
    "                    outer_scores['ibs_test_'+dataset_name] += [ibs_score_test]\n",
    "                except: \n",
    "                    outer_scores['cindex_test_'+dataset_name] += [np.nan]\n",
    "                    outer_scores['ibs_test_'+dataset_name] += [np.nan]\n",
    "            \n",
    "        df_best_params = pd.DataFrame(best_params)\n",
    "        df_best_model = pd.DataFrame(best_model)\n",
    "        df_outer_scores = pd.DataFrame(outer_scores)\n",
    "        df_metrics = pd.concat([df_best_params,df_best_model,df_outer_scores], axis=1)\n",
    "        df_metrics.to_csv('metrics/'+model+'metric_summary'+str(i)+'_'+filename, index=False)\n",
    "        # cindex\n",
    "        df_agg_metrics_cindex = pd.DataFrame({'dataset':[dataset_name],\n",
    "                                              'cindex_test_mean':df_outer_scores['cindex_test_'+dataset_name].mean(),\n",
    "                                              'cindex_test_std':df_outer_scores['cindex_test_'+dataset_name].std() })\n",
    "        # IBS\n",
    "        df_agg_metrics_ibs = pd.DataFrame({'dataset':[dataset_name],\n",
    "                                              'ibs_test_mean':df_outer_scores['ibs_test_'+dataset_name].mean(),\n",
    "                                              'ibs_test_std':df_outer_scores['ibs_test_'+dataset_name].std() })\n",
    "        \n",
    "        return df_agg_metrics_cindex, df_agg_metrics_ibs, best_model, best_params, outer_scores, best_preds_train, best_preds_test \n",
    "\n",
    "                \n",
    "#cv=inner_custom_cv,pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split gex_?|100130426      float32\n",
      "gex_?|100133144      float32\n",
      "gex_?|100134869      float32\n",
      "gex_?|10357          float32\n",
      "gex_?|10431          float32\n",
      "                      ...   \n",
      "gex_ZYG11A|440590    float32\n",
      "gex_ZYG11B|79699     float32\n",
      "gex_ZYX|7791         float32\n",
      "gex_ZZEF1|23140      float32\n",
      "gex_ZZZ3|26009       float32\n",
      "Length: 20531, dtype: object\n",
      "0.999999\n",
      "integration times 5006\n",
      "Concordance Index 0.4966850828729282\n",
      "Integrated Brier Score: 0.21002021217610758\n",
      "0.999999\n",
      "integration times 3605\n",
      "Concordance Index 0.4636810486073184\n",
      "Integrated Brier Score: 0.23022000907134266\n",
      "0.999999\n",
      "integration times 5521\n",
      "Concordance Index 0.5095579450418161\n",
      "Integrated Brier Score: 0.23069390264334527\n",
      "0.999999\n",
      "integration times 6102\n",
      "Concordance Index 0.5189003436426117\n",
      "Integrated Brier Score: 0.2403212408080216\n",
      "0.999999\n",
      "integration times 3308\n",
      "Concordance Index 0.5100193923723335\n",
      "Integrated Brier Score: 0.2108856228207395\n",
      "split gex_?|100130426      float32\n",
      "gex_?|100133144      float32\n",
      "gex_?|100134869      float32\n",
      "gex_?|10357          float32\n",
      "gex_?|10431          float32\n",
      "                      ...   \n",
      "gex_ZYG11A|440590    float32\n",
      "gex_ZYG11B|79699     float32\n",
      "gex_ZYX|7791         float32\n",
      "gex_ZZEF1|23140      float32\n",
      "gex_ZZZ3|26009       float32\n",
      "Length: 20531, dtype: object\n",
      "0.999999\n",
      "integration times 128458\n",
      "Concordance Index 0.6141493055555556\n",
      "Integrated Brier Score: 0.21212404035081694\n",
      "0.999999\n",
      "integration times 132192\n",
      "Concordance Index 0.5287403179779862\n",
      "Integrated Brier Score: 0.21191492339823836\n",
      "0.999999\n",
      "integration times 11664\n",
      "Concordance Index 0.5746268656716418\n",
      "Integrated Brier Score: 0.2023067661190666\n",
      "0.999999\n",
      "integration times 8160\n",
      "Concordance Index 0.3586615545486232\n",
      "Integrated Brier Score: 0.1836787448669659\n",
      "0.999999\n",
      "integration times 12076\n",
      "Concordance Index 0.45642829230168036\n",
      "Integrated Brier Score: 0.21901561692446334\n",
      "split gex_?|100130426      float32\n",
      "gex_?|100133144      float32\n",
      "gex_?|100134869      float32\n",
      "gex_?|10357          float32\n",
      "gex_?|10431          float32\n",
      "                      ...   \n",
      "gex_ZYG11A|440590    float32\n",
      "gex_ZYG11B|79699     float32\n",
      "gex_ZYX|7791         float32\n",
      "gex_ZZEF1|23140      float32\n",
      "gex_ZZZ3|26009       float32\n",
      "Length: 20531, dtype: object\n",
      "0.999999\n",
      "integration times 5617\n",
      "Concordance Index 0.497037295224817\n",
      "Integrated Brier Score: 0.21062071890563414\n",
      "0.999999\n",
      "integration times 4366\n",
      "Concordance Index 0.5278189910979229\n",
      "Integrated Brier Score: 0.1948161512930267\n",
      "0.999999\n",
      "integration times 7070\n",
      "Concordance Index 0.412781954887218\n",
      "Integrated Brier Score: 0.2055447599802149\n",
      "0.999999\n",
      "integration times 6113\n",
      "Concordance Index 0.4378265412748171\n",
      "Integrated Brier Score: 0.2090581821948773\n",
      "0.999999\n",
      "integration times 5163\n",
      "Concordance Index 0.42846768336964414\n",
      "Integrated Brier Score: 0.23708465254328337\n",
      "split gex_?|100130426      float32\n",
      "gex_?|100133144      float32\n",
      "gex_?|100134869      float32\n",
      "gex_?|10357          float32\n",
      "gex_?|10431          float32\n",
      "                      ...   \n",
      "gex_ZYG11A|440590    float32\n",
      "gex_ZYG11B|79699     float32\n",
      "gex_ZYX|7791         float32\n",
      "gex_ZZEF1|23140      float32\n",
      "gex_ZZZ3|26009       float32\n",
      "Length: 20531, dtype: object\n",
      "0.999999\n",
      "integration times 3669\n",
      "Concordance Index 0.5374732334047109\n",
      "Integrated Brier Score: 0.2061974347237981\n",
      "0.999999\n",
      "integration times 7135\n",
      "Concordance Index 0.5284738041002278\n",
      "Integrated Brier Score: 0.20980267250451234\n",
      "0.999999\n",
      "integration times 4360\n",
      "Concordance Index 0.5525200188412623\n",
      "Integrated Brier Score: 0.1912961570334101\n",
      "0.999999\n",
      "integration times 4656\n",
      "Concordance Index 0.5254087494476359\n",
      "Integrated Brier Score: 0.21180030701339878\n",
      "0.999999\n",
      "integration times 4939\n",
      "Concordance Index 0.5240793201133145\n",
      "Integrated Brier Score: 0.1918912039128326\n",
      "split gex_?|100130426      float32\n",
      "gex_?|100133144      float32\n",
      "gex_?|100134869      float32\n",
      "gex_?|10357          float32\n",
      "gex_?|10431          float32\n",
      "                      ...   \n",
      "gex_ZYG11A|440590    float32\n",
      "gex_ZYG11B|79699     float32\n",
      "gex_ZYX|7791         float32\n",
      "gex_ZZEF1|23140      float32\n",
      "gex_ZZZ3|26009       float32\n",
      "Length: 20531, dtype: object\n",
      "0.999999\n",
      "integration times 5673\n",
      "Concordance Index 0.4698914116485686\n",
      "Integrated Brier Score: 0.1986350623477569\n",
      "0.999999\n",
      "integration times 7088\n",
      "Concordance Index 0.37095363079615046\n",
      "Integrated Brier Score: 0.2083238065573121\n",
      "0.999999\n",
      "integration times 5362\n",
      "Concordance Index 0.3525252525252525\n",
      "Integrated Brier Score: 0.1807101441555301\n",
      "0.999999\n",
      "integration times 6363\n",
      "Concordance Index 0.635893011216566\n",
      "Integrated Brier Score: 0.2009838223175985\n",
      "0.999999\n",
      "integration times 8846\n",
      "Concordance Index 0.6025862068965517\n",
      "Integrated Brier Score: 0.19441006419166457\n",
      "split gex_?|100130426      float32\n",
      "gex_?|100133144      float32\n",
      "gex_?|100134869      float32\n",
      "gex_?|10357          float32\n",
      "gex_?|10431          float32\n",
      "                      ...   \n",
      "gex_ZYG11A|440590    float32\n",
      "gex_ZYG11B|79699     float32\n",
      "gex_ZYX|7791         float32\n",
      "gex_ZZEF1|23140      float32\n",
      "gex_ZZZ3|26009       float32\n",
      "Length: 20531, dtype: object\n",
      "0.999999\n",
      "integration times 3584\n",
      "Concordance Index 0.3861490031479538\n",
      "Integrated Brier Score: 0.22101745202279857\n",
      "0.999999\n",
      "integration times 3896\n",
      "Concordance Index 0.464354527938343\n",
      "Integrated Brier Score: 0.20169560175260567\n",
      "0.999999\n",
      "integration times 3915\n",
      "Concordance Index 0.41141396933560476\n",
      "Integrated Brier Score: 0.22582499207095152\n",
      "0.999999\n",
      "integration times 4660\n",
      "Concordance Index 0.5219465648854962\n",
      "Integrated Brier Score: 0.20831208441074772\n",
      "0.999999\n",
      "integration times 3304\n",
      "Concordance Index 0.4366312346688471\n",
      "Integrated Brier Score: 0.2273087833261004\n",
      "split gex_?|100130426      float32\n",
      "gex_?|100133144      float32\n",
      "gex_?|100134869      float32\n",
      "gex_?|10357          float32\n",
      "gex_?|10431          float32\n",
      "                      ...   \n",
      "gex_ZYG11A|440590    float32\n",
      "gex_ZYG11B|79699     float32\n",
      "gex_ZYX|7791         float32\n",
      "gex_ZZEF1|23140      float32\n",
      "gex_ZZZ3|26009       float32\n",
      "Length: 20531, dtype: object\n",
      "0.999999\n",
      "integration times 3868\n",
      "Concordance Index 0.5030549898167006\n",
      "Integrated Brier Score: 0.20431054318288777\n",
      "0.999999\n",
      "integration times 5093\n",
      "Concordance Index 0.4307856761090326\n",
      "Integrated Brier Score: 0.21029456247549924\n",
      "0.999999\n",
      "integration times 8232\n",
      "Concordance Index 0.5807692307692308\n",
      "Integrated Brier Score: 0.2033767174461617\n",
      "0.999999\n",
      "integration times 5072\n",
      "Concordance Index 0.4648148148148148\n",
      "Integrated Brier Score: 0.21979541333196742\n",
      "0.999999\n",
      "integration times 7904\n",
      "Concordance Index 0.6135437881873728\n",
      "Integrated Brier Score: 0.18670648948559604\n",
      "split gex_?|100130426      float32\n",
      "gex_?|100133144      float32\n",
      "gex_?|100134869      float32\n",
      "gex_?|10357          float32\n",
      "gex_?|10431          float32\n",
      "                      ...   \n",
      "gex_ZYG11A|440590    float32\n",
      "gex_ZYG11B|79699     float32\n",
      "gex_ZYX|7791         float32\n",
      "gex_ZZEF1|23140      float32\n",
      "gex_ZZZ3|26009       float32\n",
      "Length: 20531, dtype: object\n",
      "0.999999\n",
      "integration times 3586\n",
      "Concordance Index 0.4920906370243694\n",
      "Integrated Brier Score: 0.20677604713087522\n",
      "0.999999\n",
      "integration times 4858\n",
      "Concordance Index 0.3891891891891892\n",
      "Integrated Brier Score: 0.20310349485931592\n",
      "0.999999\n",
      "integration times 5397\n",
      "Concordance Index 0.6020627417275461\n",
      "Integrated Brier Score: 0.20713700361380818\n",
      "0.999999\n",
      "integration times 5303\n",
      "Concordance Index 0.47978567949342427\n",
      "Integrated Brier Score: 0.1953430695446712\n",
      "0.999999\n",
      "integration times 4867\n",
      "Concordance Index 0.506587335316617\n",
      "Integrated Brier Score: 0.21921722329759638\n",
      "split gex_?|100133144      float32\n",
      "gex_?|100134869      float32\n",
      "gex_?|10357          float32\n",
      "gex_?|10431          float32\n",
      "gex_?|155060         float32\n",
      "                      ...   \n",
      "gex_ZYG11A|440590    float32\n",
      "gex_ZYG11B|79699     float32\n",
      "gex_ZYX|7791         float32\n",
      "gex_ZZEF1|23140      float32\n",
      "gex_ZZZ3|26009       float32\n",
      "Length: 19076, dtype: object\n",
      "0.999999\n",
      "integration times 5867\n",
      "Concordance Index 0.5081809432146295\n",
      "Integrated Brier Score: 0.14551688582780914\n",
      "0.999999\n",
      "integration times 7856\n",
      "Concordance Index 0.47648902821316613\n",
      "Integrated Brier Score: 0.1260097303584315\n",
      "1.999999\n",
      "integration times 3547\n",
      "Concordance Index 0.5063168124392614\n",
      "Integrated Brier Score: 0.15193093396653234\n",
      "0.999999\n",
      "integration times 11342\n",
      "Concordance Index 0.4433399602385686\n",
      "Integrated Brier Score: 0.15318337038354088\n",
      "0.999999\n",
      "integration times 11635\n",
      "Concordance Index 0.39419475655430714\n",
      "Integrated Brier Score: 0.16708084607292673\n",
      "split gex_?|100133144      float32\n",
      "gex_?|100134869      float32\n",
      "gex_?|10357          float32\n",
      "gex_?|10431          float32\n",
      "gex_?|155060         float32\n",
      "                      ...   \n",
      "gex_ZYG11A|440590    float32\n",
      "gex_ZYG11B|79699     float32\n",
      "gex_ZYX|7791         float32\n",
      "gex_ZZEF1|23140      float32\n",
      "gex_ZZZ3|26009       float32\n",
      "Length: 19076, dtype: object\n",
      "0.999999\n",
      "integration times 5581\n",
      "Concordance Index 0.5715263518138262\n",
      "Integrated Brier Score: 0.2195586810787003\n",
      "0.999999\n",
      "integration times 4245\n",
      "Concordance Index 0.42920353982300885\n",
      "Integrated Brier Score: 0.21613505774020086\n",
      "0.999999\n",
      "integration times 5180\n",
      "Concordance Index 0.570957095709571\n",
      "Integrated Brier Score: 0.21398359986277163\n",
      "0.999999\n",
      "integration times 3460\n",
      "Concordance Index 0.43870523415977963\n",
      "Integrated Brier Score: 0.2083721066126565\n",
      "0.999999\n",
      "integration times 5001\n",
      "Concordance Index 0.47019867549668876\n",
      "Integrated Brier Score: 0.22696259490702492\n"
     ]
    }
   ],
   "source": [
    "cancer_types = [\n",
    "    'BLCA',\n",
    "    'BRCA',\n",
    "    'HNSC',\n",
    "    'KIRC',\n",
    "    'LGG',\n",
    "    'LIHC',\n",
    "    'LUAD',\n",
    "    'LUSC',\n",
    "    'OV',\n",
    "    'STAD']\n",
    "\n",
    "agg_metrics_cindex = []\n",
    "agg_metrics_ibs = []\n",
    "\n",
    "\n",
    "class InputShapeSetter(skorch.callbacks.Callback):\n",
    "    def on_train_begin(self, net, X, y):\n",
    "        net.set_params(module__input_units=X.shape[-1])\n",
    "\n",
    "for idx, cancer_type in enumerate(cancer_types):\n",
    "    # get name of current dataset\n",
    "    data = load_tcga(path=\"/Users/JUSC/Documents/xgbsurv/xgbsurv/datasets/data/\", cancer_type=cancer_type, as_frame=True)\n",
    "    X  = data.data #.astype(np.float32)\n",
    "    y = data.target #.values #.to_numpy()\n",
    "\n",
    "    X, y = sort_X_y_pandas(X, y)\n",
    "\n",
    "    net = NeuralNet(\n",
    "        SurvivalModel, \n",
    "        module__n_layers = 1,\n",
    "        module__input_units = X.shape[1],\n",
    "        #module__num_nodes = 32,\n",
    "        #module__dropout = 0.1, # these could also be removed\n",
    "        module__out_features = 1,\n",
    "        # for split sizes when result size = 1\n",
    "        iterator_train__drop_last=True,\n",
    "        #iterator_valid__drop_last=True,\n",
    "        criterion=AHLoss,\n",
    "        optimizer=torch.optim.AdamW,\n",
    "        optimizer__weight_decay = 0.4,\n",
    "        batch_size=32, # separate train and valid->iterator_train__batch_size=128 and iterator_valid__batch_size=128 ?\n",
    "        callbacks=[\n",
    "            (\n",
    "                \"sched\",\n",
    "                LRScheduler(\n",
    "                    torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
    "                    monitor=\"valid_loss\",\n",
    "                    patience=5,\n",
    "                ),\n",
    "            ),\n",
    "            (\n",
    "                \"es\",\n",
    "                EarlyStopping(\n",
    "                    monitor=\"valid_loss\",\n",
    "                    patience=early_stopping_rounds,\n",
    "                    load_best=True,\n",
    "                ),\n",
    "            ),\n",
    "            (\"seed\", FixSeed(seed=42)),\n",
    "            (\"Input Shape Setter\",InputShapeSetter())\n",
    "        ],#[EarlyStopping(patience=10)],#,InputShapeSetter()],\n",
    "        #TODO: enable stratification, verify\n",
    "        train_split=ValidSplit(0.2), # might cause lower performance in metrics, explain in thesis\n",
    "        verbose=0\n",
    "    )\n",
    "    df_agg_metrics_cindex, df_agg_metrics_ibs, best_model,params, outer_scores, best_preds_train, best_preds_test = train_eval(X, y, net, n_iter, data.filename)\n",
    "    agg_metrics_cindex.append(df_agg_metrics_cindex)\n",
    "    agg_metrics_ibs.append(df_agg_metrics_ibs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_ah_1_cindex = pd.concat([df for df in agg_metrics_cindex]).round(4)\n",
    "df_final_ah_1_cindex.to_csv('metrics/final_deep_learning_ah_tcga_cindex.csv', index=False)\n",
    "df_final_ah_1_cindex.to_csv('/Users/JUSC/Documents/644928e0fb7e147893e8ec15/05_thesis/tables/final_deep_learning_ah_tcga_cindex.csv', index=False)  #\n",
    "df_final_ah_1_cindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_ah_1_ibs = pd.concat([df for df in agg_metrics_ibs]).round(4)\n",
    "df_final_ah_1_ibs.to_csv('metrics/final_deep_learning_ah_tcga_ibs.csv', index=False)\n",
    "df_final_ah_1_ibs.to_csv('/Users/JUSC/Documents/644928e0fb7e147893e8ec15/05_thesis/tables/final_deep_learning_ah_tcga_ibs.csv', index=False) \n",
    "df_final_ah_1_ibs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xgbsurv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
