{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBSurv EH Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgbsurv.datasets import (load_metabric, load_flchain, load_rgbsg, load_support, load_tcga)\n",
    "from xgbsurv import XGBSurv\n",
    "from xgbsurv.evaluation import cindex_censored, ibs\n",
    "from xgbsurv.models.utils import sort_X_y\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import uniform as scuniform\n",
    "from scipy.stats import randint as scrandint\n",
    "from scipy.stats import loguniform as scloguniform \n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold, train_test_split\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, LabelBinarizer, OneHotEncoder\n",
    "from sklearn.compose import make_column_transformer, make_column_selector\n",
    "from sklearn.decomposition import PCA\n",
    "# import models\n",
    "from xgbsurv.models.eh_final import eh_likelihood, get_cumulative_hazard_function_eh\n",
    "from pycox.evaluation import EvalSurv\n",
    "from xgbsurv.models.utils import sort_X_y, sort_X_y_pandas, transform_back, transform\n",
    "from xgbsurv.preprocessing.dataset_preprocessing import discretizer_df\n",
    "from sklearn.utils.fixes import loguniform\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_path = os.getcwd()  # Get the current working directory path\n",
    "two_levels_up = os.path.abspath(os.path.join(current_path, \"..\", \"..\"))\n",
    "data_path = two_levels_up+'/xgbsurv/datasets/data/'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters\n",
    "n_outer_splits = 5\n",
    "n_inner_splits = 5\n",
    "rand_state = 42\n",
    "n_iter = 50 #0\n",
    "early_stopping_rounds=10\n",
    "base_score = 0.0\n",
    "validation_size = 0.2\n",
    "model = 'eh_'\n",
    "\n",
    "# set seed for scipy\n",
    "np.random.seed(rand_state)\n",
    "\n",
    "# Define parameter grid for random forest classifier\n",
    "param_grid = {\n",
    "'estimator__reg_alpha': scloguniform(1e-10,1),#[1e-10,1], # from hyp augmentation, L1 regularization\n",
    "'estimator__reg_lambda': scloguniform(1e-10,1), #[1e-10,1], #alias l2_regularization, lambda in augmentation\n",
    "'estimator__learning_rate': scloguniform(0.01,1), #[0.001,1], # assumed alias eta from augmentation,\n",
    "'estimator__n_estimators':  scrandint(1,4000), # corresponds to num_rounds\n",
    "'estimator__gamma': loguniform(0.001,1.0),#[0.1,1], # minimum loss reduction required to make a further partition on a leaf node of the tree.\n",
    "'estimator__colsample_bylevel': scuniform(0.1, 1-0.1), #[0.1,1], # from hyp augmentation\n",
    "'estimator__colsample_bynode': scuniform(0.1, 1-0.1), #[0.1,1], # from hyp augmentation, uniform(0.1,1),\n",
    "'estimator__colsample_bytree': scuniform(0.5, 1-0.5),#[0.5,1], # from hyp augmentation, seems to exceed the bound, uniform(0.5,1)\n",
    "'estimator__max_depth': scrandint(1,20),#[1,20], # from hyp augmentation\n",
    "'estimator__max_delta_step': scrandint(0,10),#[0,10], # from hyp augmentation\n",
    "'estimator__min_child_weight' : scloguniform(0.1,20-0.1),#[0.1,20], # from hyp augmentation\n",
    "'estimator__subsample': scuniform(0.01,1-0.01),#[0.01,1], # from hyp augmentation\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define stratified inner k-fold cross-validation\n",
    "class CustomSplit(StratifiedKFold):\n",
    "    def __init__(self, n_splits=5, shuffle=True, random_state=None):\n",
    "        super().__init__(n_splits=n_splits, shuffle=shuffle, random_state=random_state)\n",
    "\n",
    "    def split(self, X, y, groups=None):\n",
    "        print('split', X.dtypes)\n",
    "        try:\n",
    "            if y.shape[1]>1:\n",
    "                y = y[:,0]\n",
    "        except:\n",
    "            pass\n",
    "        bins = np.sign(y)\n",
    "        return super().split(X, bins, groups=groups)\n",
    "\n",
    "    def get_n_splits(self, X=None, y=None, groups=None):\n",
    "        return self.n_splits\n",
    "\n",
    "outer_custom_cv = CustomSplit(n_splits=n_outer_splits, shuffle=True, random_state=rand_state)\n",
    "inner_custom_cv = CustomSplit(n_splits=n_outer_splits, shuffle=True, random_state=rand_state)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_function = make_scorer(eh_likelihood, greater_is_better=False)\n",
    "\n",
    "#scoring_function = make_scorer(eh_likelihood, greater_is_better=False) #changed here\n",
    "def custom_scoring_function(y_true, y_pred):\n",
    "\n",
    "        if not isinstance(y_true, np.ndarray):\n",
    "            y_true = y_true.values\n",
    "        if not isinstance(y_pred, np.ndarray):\n",
    "            y_pred = y_pred.values\n",
    "        # change order of this later\n",
    "        score = eh_likelihood(y_true, y_pred)\n",
    "        return score #.numpy()\n",
    "\n",
    "scoring_function = make_scorer(custom_scoring_function, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set Basic Elements\n",
    "\n",
    "ct = make_column_transformer(\n",
    "        (StandardScaler(), make_column_selector(dtype_include=['float32'])),\n",
    "        #(OneHotEncoder(sparse_output=False, handle_unknown='infrequent_if_exist'), make_column_selector(dtype_include=['category', 'object'])),\n",
    "        remainder='passthrough')\n",
    "\n",
    "estimator = XGBSurv(\n",
    "    objective='eh_objective',\n",
    "    eval_metric='eh_loss',\n",
    "    random_state=rand_state, \n",
    "    disable_default_eval_metric=True,\n",
    "    early_stopping_rounds=early_stopping_rounds, \n",
    "    base_score=base_score,\n",
    "                    )\n",
    "pipe = Pipeline([('scaler',ct),\n",
    "                ('estimator', estimator)])\n",
    "    \n",
    "rs = RandomizedSearchCV(pipe, param_grid, scoring = scoring_function, n_jobs=-1, \n",
    "                             cv=inner_custom_cv, n_iter=n_iter, refit=True, \n",
    "                             random_state=rand_state, verbose=1,\n",
    "                             error_score = 'raise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_sum = {}\n",
    "agg_metrics_cindex = []\n",
    "agg_metrics_ibs = []"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## METABRIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "max round 12.09\n",
      "integration_values.shape[0] 122315\n",
      "Concordance Index Test 0.6638427046545772\n",
      "Integrated Brier Score Test 0.1691472224981181\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "max round 12.38\n",
      "integration_values.shape[0] 122765\n",
      "Concordance Index Test 0.6386752935160966\n",
      "Integrated Brier Score Test 0.17077684376423446\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "max round 5.85\n",
      "integration_values.shape[0] 52846\n",
      "Concordance Index Test 0.6451241995916183\n",
      "Integrated Brier Score Test 0.1696888596360053\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "max round 3.82\n",
      "integration_values.shape[0] 40693\n",
      "Concordance Index Test 0.6773601632888687\n",
      "Integrated Brier Score Test 0.15841707841018862\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "max round 3.52\n",
      "integration_values.shape[0] 37062\n",
      "Concordance Index Test 0.6570622486343718\n",
      "Integrated Brier Score Test 0.16526817336538435\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = load_metabric(path=data_path, as_frame=True)\n",
    "filename = data.filename\n",
    "dataset_name = filename.split('_')[0]\n",
    "best_params = {'best_params_'+dataset_name:[]}\n",
    "best_model = {'best_model_'+dataset_name:[]}\n",
    "outer_scores = {'cindex_test_'+dataset_name:[],'ibs_test_'+dataset_name:[]}\n",
    "X  = data.data \n",
    "y = data.target \n",
    "y = pd.concat([y,y], axis=1)\n",
    "y.columns = ['target1', 'target2']\n",
    "for i, (train_index, test_index) in enumerate(outer_custom_cv.split(X, y)):\n",
    "        # Split data into training and testing sets for outer fold\n",
    "\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        np.savetxt('splits/'+model+'train_index_'+str(i)+'_'+filename, train_index, delimiter=',')\n",
    "        np.savetxt('splits/'+model+'test_index_'+str(i)+'_'+filename, test_index, delimiter=',')\n",
    "        \n",
    "        X_train, y_train = sort_X_y_pandas(X_train, y_train)\n",
    "        X_test, y_test = sort_X_y_pandas(X_test, y_test)\n",
    "        rs.fit(X_train, y_train, estimator__eval_test_size=validation_size, estimator__verbose=0)\n",
    "        best_params['best_params_'+dataset_name] += [rs.best_params_]\n",
    "        best_model['best_model_'+dataset_name] += [rs.best_estimator_.get_params()]\n",
    "        best_preds_train = rs.best_estimator_.predict(X_train)\n",
    "        best_preds_test = rs.best_estimator_.predict(X_test)\n",
    "\n",
    "        np.savetxt('predictions/'+model+'best_preds_train_'+str(i)+'_'+filename, best_preds_train, delimiter=',')\n",
    "        np.savetxt('predictions/'+model+'best_preds_test_'+str(i)+'_'+filename, best_preds_test, delimiter=',')\n",
    "\n",
    "        cum_hazard_test = get_cumulative_hazard_function_eh(\n",
    "                X_train.values, X_test.values, y_train.values, y_test.values,\n",
    "                best_preds_train, best_preds_test\n",
    "                )\n",
    "        df_survival_test = np.exp(-cum_hazard_test)\n",
    "        durations_test, events_test = transform_back(y_test.values[:,0])\n",
    "        time_grid_test = np.linspace(durations_test.min(), durations_test.max(), 100)\n",
    "        ev = EvalSurv(df_survival_test, durations_test, events_test, censor_surv='km')\n",
    "        print('Concordance Index Test',ev.concordance_td('antolini'))\n",
    "        print('Integrated Brier Score Test',ev.integrated_brier_score(time_grid_test))\n",
    "        cindex_score_test = ev.concordance_td('antolini')\n",
    "        ibs_score_test = ev.integrated_brier_score(time_grid_test)\n",
    "        outer_scores['cindex_test_'+dataset_name] += [cindex_score_test]\n",
    "        outer_scores['ibs_test_'+dataset_name] += [ibs_score_test]\n",
    "df_best_params = pd.DataFrame(best_params)\n",
    "df_best_model = pd.DataFrame(best_model)\n",
    "df_outer_scores = pd.DataFrame(outer_scores)\n",
    "df_metrics = pd.concat([df_best_params,df_best_model,df_outer_scores], axis=1)\n",
    "df_metrics.to_csv('metrics/'+model+'metric_summary_'+filename, index=False)\n",
    "# cindex\n",
    "df_agg_metrics_cindex = pd.DataFrame({'dataset':[dataset_name],\n",
    "                                        'cindex_test_mean':df_outer_scores['cindex_test_'+dataset_name].mean(),\n",
    "                                        'cindex_test_std':df_outer_scores['cindex_test_'+dataset_name].std() })\n",
    "# IBS\n",
    "df_agg_metrics_ibs = pd.DataFrame({'dataset':[dataset_name],\n",
    "                                        'ibs_test_mean':df_outer_scores['ibs_test_'+dataset_name].mean(),\n",
    "                                        'ibs_test_std':df_outer_scores['ibs_test_'+dataset_name].std() })\n",
    "agg_metrics_cindex.append(df_agg_metrics_cindex)\n",
    "agg_metrics_ibs.append(df_agg_metrics_ibs)\n",
    "metrics_sum[model+dataset_name] = df_metrics\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FLCHAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "max round 45.17\n",
      "integration_values.shape[0] 235547\n",
      "Concordance Index Test 0.7814489818877264\n",
      "Integrated Brier Score Test 0.09838964127361889\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "max round 69.78\n",
      "integration_values.shape[0] 357473\n",
      "Concordance Index Test 0.7980671574588969\n",
      "Integrated Brier Score Test 0.09524479211291466\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "max round 97.47\n",
      "integration_values.shape[0] 503528\n",
      "Concordance Index Test 0.780494537559029\n",
      "Integrated Brier Score Test 0.09637501585477062\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "max round 104.44\n",
      "integration_values.shape[0] 540043\n",
      "Concordance Index Test 0.7813475521815916\n",
      "Integrated Brier Score Test 0.09746942126990302\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "max round 34.68\n",
      "integration_values.shape[0] 179864\n",
      "Concordance Index Test 0.7923975295618376\n",
      "Integrated Brier Score Test 0.09238072135461663\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = load_flchain(path=data_path, as_frame=True)\n",
    "filename = data.filename\n",
    "dataset_name = filename.split('_')[0]\n",
    "best_params = {'best_params_'+dataset_name:[]}\n",
    "best_model = {'best_model_'+dataset_name:[]}\n",
    "outer_scores = {'cindex_test_'+dataset_name:[],'ibs_test_'+dataset_name:[]}\n",
    "X  = data.data \n",
    "y = data.target \n",
    "y = pd.concat([y,y], axis=1)\n",
    "y.columns = ['target1', 'target2']\n",
    "for i, (train_index, test_index) in enumerate(outer_custom_cv.split(X, y)):\n",
    "        # Split data into training and testing sets for outer fold\n",
    "\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        np.savetxt('splits/'+model+'train_index_'+str(i)+'_'+filename, train_index, delimiter=',')\n",
    "        np.savetxt('splits/'+model+'test_index_'+str(i)+'_'+filename, test_index, delimiter=',')\n",
    "        \n",
    "        X_train, y_train = sort_X_y_pandas(X_train, y_train)\n",
    "        X_test, y_test = sort_X_y_pandas(X_test, y_test)\n",
    "        rs.fit(X_train, y_train, estimator__eval_test_size=validation_size, estimator__verbose=0)\n",
    "        best_params['best_params_'+dataset_name] += [rs.best_params_]\n",
    "        best_model['best_model_'+dataset_name] += [rs.best_estimator_.get_params()]\n",
    "        best_preds_train = rs.best_estimator_.predict(X_train)\n",
    "        best_preds_test = rs.best_estimator_.predict(X_test)\n",
    "\n",
    "        np.savetxt('predictions/'+model+'best_preds_train_'+str(i)+'_'+filename, best_preds_train, delimiter=',')\n",
    "        np.savetxt('predictions/'+model+'best_preds_test_'+str(i)+'_'+filename, best_preds_test, delimiter=',')\n",
    "        #minmaxscaler = MinMaxScaler(feature_range=(-0.75, 0.75))\n",
    "        #best_preds_train = minmaxscaler.fit_transform(best_preds_train)\n",
    "        #best_preds_test = minmaxscaler.fit_transform(best_preds_test)\n",
    "        #print('best_preds_train shape', best_preds_train.shape)\n",
    "        #print('best_preds_test shape', best_preds_test.shape)\n",
    "        cum_hazard_test = get_cumulative_hazard_function_eh(\n",
    "                X_train.values, X_test.values, y_train.values, y_test.values,\n",
    "                best_preds_train, best_preds_test\n",
    "                )\n",
    "        df_survival_test = np.exp(-cum_hazard_test)\n",
    "        durations_test, events_test = transform_back(y_test.values[:,0])\n",
    "        time_grid_test = np.linspace(durations_test.min(), durations_test.max(), 100)\n",
    "        ev = EvalSurv(df_survival_test, durations_test, events_test, censor_surv='km')\n",
    "        print('Concordance Index Test',ev.concordance_td('antolini'))\n",
    "        print('Integrated Brier Score Test',ev.integrated_brier_score(time_grid_test))\n",
    "        cindex_score_test = ev.concordance_td('antolini')\n",
    "        ibs_score_test = ev.integrated_brier_score(time_grid_test)\n",
    "        outer_scores['cindex_test_'+dataset_name] += [cindex_score_test]\n",
    "        outer_scores['ibs_test_'+dataset_name] += [ibs_score_test]\n",
    "df_best_params = pd.DataFrame(best_params)\n",
    "df_best_model = pd.DataFrame(best_model)\n",
    "df_outer_scores = pd.DataFrame(outer_scores)\n",
    "df_metrics = pd.concat([df_best_params,df_best_model,df_outer_scores], axis=1)\n",
    "df_metrics.to_csv('metrics/'+model+'metric_summary_'+filename, index=False)\n",
    "# cindex\n",
    "df_agg_metrics_cindex = pd.DataFrame({'dataset':[dataset_name],\n",
    "                                        'cindex_test_mean':df_outer_scores['cindex_test_'+dataset_name].mean(),\n",
    "                                        'cindex_test_std':df_outer_scores['cindex_test_'+dataset_name].std() })\n",
    "# IBS\n",
    "df_agg_metrics_ibs = pd.DataFrame({'dataset':[dataset_name],\n",
    "                                        'ibs_test_mean':df_outer_scores['ibs_test_'+dataset_name].mean(),\n",
    "                                        'ibs_test_std':df_outer_scores['ibs_test_'+dataset_name].std() })\n",
    "agg_metrics_cindex.append(df_agg_metrics_cindex)\n",
    "agg_metrics_ibs.append(df_agg_metrics_ibs)\n",
    "metrics_sum[model+dataset_name] = df_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.546034"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(best_preds_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RGBSG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "max round 4.15\n",
      "integration_values.shape[0] 11021\n",
      "Concordance Index Test 0.6964656684203426\n",
      "Integrated Brier Score Test 0.16795334524586725\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "max round 4.09\n",
      "integration_values.shape[0] 1\n",
      "Concordance Index Test 0.0\n",
      "Integrated Brier Score Test 0.4148120690987607\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "max round 3.56\n",
      "integration_values.shape[0] 9286\n",
      "Concordance Index Test 0.6696174644502303\n",
      "Integrated Brier Score Test 0.18234981759269647\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "max round 3.59\n",
      "integration_values.shape[0] 9164\n",
      "Concordance Index Test 0.6650380898543702\n",
      "Integrated Brier Score Test 0.17868397513183334\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "max round 4.07\n",
      "integration_values.shape[0] 10443\n",
      "Concordance Index Test 0.6836862766042674\n",
      "Integrated Brier Score Test 0.17359570641400074\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = load_rgbsg(path=data_path, as_frame=True)\n",
    "filename = data.filename\n",
    "dataset_name = filename.split('_')[0]\n",
    "best_params = {'best_params_'+dataset_name:[]}\n",
    "best_model = {'best_model_'+dataset_name:[]}\n",
    "outer_scores = {'cindex_test_'+dataset_name:[],'ibs_test_'+dataset_name:[]}\n",
    "X  = data.data \n",
    "y = data.target \n",
    "y = pd.concat([y,y], axis=1)\n",
    "y.columns = ['target1', 'target2']\n",
    "for i, (train_index, test_index) in enumerate(outer_custom_cv.split(X, y)):\n",
    "        # Split data into training and testing sets for outer fold\n",
    "\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        np.savetxt('splits/'+model+'train_index_'+str(i)+'_'+filename, train_index, delimiter=',')\n",
    "        np.savetxt('splits/'+model+'test_index_'+str(i)+'_'+filename, test_index, delimiter=',')\n",
    "        \n",
    "        X_train, y_train = sort_X_y_pandas(X_train, y_train)\n",
    "        X_test, y_test = sort_X_y_pandas(X_test, y_test)\n",
    "        rs.fit(X_train, y_train, estimator__eval_test_size=validation_size, estimator__verbose=0)\n",
    "        best_params['best_params_'+dataset_name] += [rs.best_params_]\n",
    "        best_model['best_model_'+dataset_name] += [rs.best_estimator_.get_params()]\n",
    "        best_preds_train = rs.best_estimator_.predict(X_train)\n",
    "        best_preds_test = rs.best_estimator_.predict(X_test)\n",
    "\n",
    "        np.savetxt('predictions/'+model+'best_preds_train_'+str(i)+'_'+filename, best_preds_train, delimiter=',')\n",
    "        np.savetxt('predictions/'+model+'best_preds_test_'+str(i)+'_'+filename, best_preds_test, delimiter=',')\n",
    "\n",
    "        cum_hazard_test = get_cumulative_hazard_function_eh(\n",
    "                X_train.values, X_test.values, y_train.values, y_test.values,\n",
    "                best_preds_train, best_preds_test\n",
    "                )\n",
    "        df_survival_test = np.exp(-cum_hazard_test)\n",
    "        durations_test, events_test = transform_back(y_test.values[:,0])\n",
    "        time_grid_test = np.linspace(durations_test.min(), durations_test.max(), 100)\n",
    "        ev = EvalSurv(df_survival_test, durations_test, events_test, censor_surv='km')\n",
    "        print('Concordance Index Test',ev.concordance_td('antolini'))\n",
    "        print('Integrated Brier Score Test',ev.integrated_brier_score(time_grid_test))\n",
    "        cindex_score_test = ev.concordance_td('antolini')\n",
    "        ibs_score_test = ev.integrated_brier_score(time_grid_test)\n",
    "        outer_scores['cindex_test_'+dataset_name] += [cindex_score_test]\n",
    "        outer_scores['ibs_test_'+dataset_name] += [ibs_score_test]\n",
    "df_best_params = pd.DataFrame(best_params)\n",
    "df_best_model = pd.DataFrame(best_model)\n",
    "df_outer_scores = pd.DataFrame(outer_scores)\n",
    "df_metrics = pd.concat([df_best_params,df_best_model,df_outer_scores], axis=1)\n",
    "df_metrics.to_csv('metrics/'+model+'metric_summary_'+filename, index=False)\n",
    "# cindex\n",
    "df_agg_metrics_cindex = pd.DataFrame({'dataset':[dataset_name],\n",
    "                                        'cindex_test_mean':df_outer_scores['cindex_test_'+dataset_name].mean(),\n",
    "                                        'cindex_test_std':df_outer_scores['cindex_test_'+dataset_name].std() })\n",
    "# IBS\n",
    "df_agg_metrics_ibs = pd.DataFrame({'dataset':[dataset_name],\n",
    "                                        'ibs_test_mean':df_outer_scores['ibs_test_'+dataset_name].mean(),\n",
    "                                        'ibs_test_std':df_outer_scores['ibs_test_'+dataset_name].std() })\n",
    "agg_metrics_cindex.append(df_agg_metrics_cindex)\n",
    "agg_metrics_ibs.append(df_agg_metrics_ibs)\n",
    "metrics_sum[model+dataset_name] = df_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SUPPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "max round 3.62\n",
      "integration_values.shape[0] 7327\n",
      "Concordance Index Test 0.607529630354571\n",
      "Integrated Brier Score Test 0.19631841731342417\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "max round 3.19\n",
      "integration_values.shape[0] 6475\n",
      "Concordance Index Test 0.6162894519182714\n",
      "Integrated Brier Score Test 0.191192744951117\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "max round 4.57\n",
      "integration_values.shape[0] 9269\n",
      "Concordance Index Test 0.6155759415030738\n",
      "Integrated Brier Score Test 0.19157509192810113\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "max round 3.44\n",
      "integration_values.shape[0] 6977\n",
      "Concordance Index Test 0.6179995329056271\n",
      "Integrated Brier Score Test 0.19064629199737224\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "max round 3.31\n",
      "integration_values.shape[0] 6716\n",
      "Concordance Index Test 0.617449857197777\n",
      "Integrated Brier Score Test 0.19039351074494174\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = load_support(path=data_path, as_frame=True)\n",
    "filename = data.filename\n",
    "dataset_name = filename.split('_')[0]\n",
    "best_params = {'best_params_'+dataset_name:[]}\n",
    "best_model = {'best_model_'+dataset_name:[]}\n",
    "outer_scores = {'cindex_test_'+dataset_name:[],'ibs_test_'+dataset_name:[]}\n",
    "X  = data.data \n",
    "y = data.target \n",
    "y = pd.concat([y,y], axis=1)\n",
    "y.columns = ['target1', 'target2']\n",
    "for i, (train_index, test_index) in enumerate(outer_custom_cv.split(X, y)):\n",
    "        # Split data into training and testing sets for outer fold\n",
    "\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        np.savetxt('splits/'+model+'train_index_'+str(i)+'_'+filename, train_index, delimiter=',')\n",
    "        np.savetxt('splits/'+model+'test_index_'+str(i)+'_'+filename, test_index, delimiter=',')\n",
    "        \n",
    "        X_train, y_train = sort_X_y_pandas(X_train, y_train)\n",
    "        X_test, y_test = sort_X_y_pandas(X_test, y_test)\n",
    "        rs.fit(X_train, y_train, estimator__eval_test_size=validation_size, estimator__verbose=0)\n",
    "        best_params['best_params_'+dataset_name] += [rs.best_params_]\n",
    "        best_model['best_model_'+dataset_name] += [rs.best_estimator_.get_params()]\n",
    "        best_preds_train = rs.best_estimator_.predict(X_train)\n",
    "        best_preds_test = rs.best_estimator_.predict(X_test)\n",
    "\n",
    "        np.savetxt('predictions/'+model+'best_preds_train_'+str(i)+'_'+filename, best_preds_train, delimiter=',')\n",
    "        np.savetxt('predictions/'+model+'best_preds_test_'+str(i)+'_'+filename, best_preds_test, delimiter=',')\n",
    "\n",
    "        cum_hazard_test = get_cumulative_hazard_function_eh(\n",
    "                X_train.values, X_test.values, y_train.values, y_test.values,\n",
    "                best_preds_train, best_preds_test\n",
    "                )\n",
    "        df_survival_test = np.exp(-cum_hazard_test)\n",
    "        durations_test, events_test = transform_back(y_test.values[:,0])\n",
    "        time_grid_test = np.linspace(durations_test.min(), durations_test.max(), 100)\n",
    "        ev = EvalSurv(df_survival_test, durations_test, events_test, censor_surv='km')\n",
    "        print('Concordance Index Test',ev.concordance_td('antolini'))\n",
    "        print('Integrated Brier Score Test',ev.integrated_brier_score(time_grid_test))\n",
    "        cindex_score_test = ev.concordance_td('antolini')\n",
    "        ibs_score_test = ev.integrated_brier_score(time_grid_test)\n",
    "        outer_scores['cindex_test_'+dataset_name] += [cindex_score_test]\n",
    "        outer_scores['ibs_test_'+dataset_name] += [ibs_score_test]\n",
    "df_best_params = pd.DataFrame(best_params)\n",
    "df_best_model = pd.DataFrame(best_model)\n",
    "df_outer_scores = pd.DataFrame(outer_scores)\n",
    "df_metrics = pd.concat([df_best_params,df_best_model,df_outer_scores], axis=1)\n",
    "df_metrics.to_csv('metrics/'+model+'metric_summary_'+filename, index=False)\n",
    "# cindex\n",
    "df_agg_metrics_cindex = pd.DataFrame({'dataset':[dataset_name],\n",
    "                                        'cindex_test_mean':df_outer_scores['cindex_test_'+dataset_name].mean(),\n",
    "                                        'cindex_test_std':df_outer_scores['cindex_test_'+dataset_name].std() })\n",
    "# IBS\n",
    "df_agg_metrics_ibs = pd.DataFrame({'dataset':[dataset_name],\n",
    "                                        'ibs_test_mean':df_outer_scores['ibs_test_'+dataset_name].mean(),\n",
    "                                        'ibs_test_std':df_outer_scores['ibs_test_'+dataset_name].std() })\n",
    "agg_metrics_cindex.append(df_agg_metrics_cindex)\n",
    "agg_metrics_ibs.append(df_agg_metrics_ibs)\n",
    "metrics_sum[model+dataset_name] = df_metrics\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>cindex_test_mean</th>\n",
       "      <th>cindex_test_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>METABRIC</td>\n",
       "      <td>0.6564</td>\n",
       "      <td>0.0153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FLCHAIN</td>\n",
       "      <td>0.7868</td>\n",
       "      <td>0.0080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RGBSG</td>\n",
       "      <td>0.5430</td>\n",
       "      <td>0.3038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SUPPORT</td>\n",
       "      <td>0.6150</td>\n",
       "      <td>0.0043</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    dataset  cindex_test_mean  cindex_test_std\n",
       "0  METABRIC            0.6564           0.0153\n",
       "0   FLCHAIN            0.7868           0.0080\n",
       "0     RGBSG            0.5430           0.3038\n",
       "0   SUPPORT            0.6150           0.0043"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final_eh_1_cindex = pd.concat([df for df in agg_metrics_cindex]).round(4)\n",
    "df_final_eh_1_cindex.to_csv('metrics/final_gbdt_eh_1_cindex.csv', index=False)\n",
    "df_final_eh_1_cindex.to_csv('/Users/JUSC/Documents/644928e0fb7e147893e8ec15/05_thesis/tables/final_gbdt_eh_1_cindex.csv', index=False)  #\n",
    "df_final_eh_1_cindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>ibs_test_mean</th>\n",
       "      <th>ibs_test_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>METABRIC</td>\n",
       "      <td>0.1667</td>\n",
       "      <td>0.0051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FLCHAIN</td>\n",
       "      <td>0.0960</td>\n",
       "      <td>0.0023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RGBSG</td>\n",
       "      <td>0.2235</td>\n",
       "      <td>0.1071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SUPPORT</td>\n",
       "      <td>0.1920</td>\n",
       "      <td>0.0024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    dataset  ibs_test_mean  ibs_test_std\n",
       "0  METABRIC         0.1667        0.0051\n",
       "0   FLCHAIN         0.0960        0.0023\n",
       "0     RGBSG         0.2235        0.1071\n",
       "0   SUPPORT         0.1920        0.0024"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final_eh_1_ibs = pd.concat([df for df in agg_metrics_ibs]).round(4)\n",
    "df_final_eh_1_ibs.to_csv('metrics/final_gbdt_eh_1_ibs.csv', index=False)\n",
    "df_final_eh_1_ibs.to_csv('/Users/JUSC/Documents/644928e0fb7e147893e8ec15/05_thesis/tables/final_gbdt_eh_1_ibs.csv', index=False) \n",
    "df_final_eh_1_ibs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TCGA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "'estimator__reg_alpha': scloguniform(1e-10,1),#[1e-10,1], # from hyp augmentation, L1 regularization\n",
    "'estimator__reg_lambda': scloguniform(1e-10,1), #[1e-10,1], #alias l2_regularization, lambda in augmentation\n",
    "'estimator__learning_rate': scloguniform(0.01,1.0), #[0.001,1], # assumed alias eta from augmentation,\n",
    "'estimator__n_estimators':  scrandint(1,4000),#00), # corresponds to num_rounds\n",
    "'estimator__gamma': loguniform(0.001,1.0),#[0.1,1], # minimum loss reduction required to make a further partition on a leaf node of the tree.\n",
    "'estimator__colsample_bylevel': scuniform(0.1, 1-0.1), #[0.1,1], # from hyp augmentation\n",
    "'estimator__colsample_bynode': scuniform(0.1, 1-0.1), #[0.1,1], # from hyp augmentation, uniform(0.1,1),\n",
    "'estimator__colsample_bytree': scuniform(0.5, 1-0.5),#[0.5,1], # from hyp augmentation, seems to exceed the bound, uniform(0.5,1)\n",
    "'estimator__max_depth': scrandint(1,20),#[1,20], # from hyp augmentation\n",
    "'estimator__max_delta_step': scrandint(0,10),#[0,10], # from hyp augmentation\n",
    "'estimator__min_child_weight' : scloguniform(0.1,20-0.1),#[0.1,20], # from hyp augmentation\n",
    "'estimator__subsample': scuniform(0.01,1-0.01),#[0.01,1], # from hyp augmentation\n",
    "#'pca__n_components': [8, 16, 32, 64]\n",
    "}\n",
    "n_outer_splits = 5\n",
    "n_inner_splits = 5\n",
    "rand_state = 42\n",
    "n_iter = 50\n",
    "early_stopping_rounds=10\n",
    "base_score = 0.0\n",
    "validation_size = 0.2\n",
    "model = 'eh_'\n",
    "data = 'tcga'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set Basic Elements\n",
    "\n",
    "ct = make_column_transformer(\n",
    "        (StandardScaler(), make_column_selector(dtype_include=['float32'])),\n",
    "        #(OneHotEncoder(sparse_output=False, handle_unknown='infrequent_if_exist'), make_column_selector(dtype_include=['category', 'object'])),\n",
    "        remainder='passthrough')\n",
    "\n",
    "estimator = XGBSurv(\n",
    "    objective='eh_objective',\n",
    "    eval_metric='eh_loss',\n",
    "    random_state=rand_state, \n",
    "    disable_default_eval_metric=True,\n",
    "    early_stopping_rounds=early_stopping_rounds, \n",
    "    base_score=base_score,\n",
    "                    )\n",
    "pipe = Pipeline([('scaler',ct),\n",
    "                ('estimator', estimator)])\n",
    "    \n",
    "rs = RandomizedSearchCV(pipe, param_grid, scoring = scoring_function, n_jobs=-1, \n",
    "                             cv=inner_custom_cv, n_iter=n_iter, refit=True, \n",
    "                             random_state=rand_state, verbose=1,\n",
    "                             error_score = 'raise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_types = [\n",
    "    #'BLCA',\n",
    "    #'BRCA',\n",
    "    #'HNSC',\n",
    "    #'KIRC',\n",
    "    #'LGG',\n",
    "    #'LIHC',\n",
    "    #'LUAD',\n",
    "    'LUSC',\n",
    "    'OV',\n",
    "    'STAD']\n",
    "agg_metrics_cindex = []\n",
    "agg_metrics_ibs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LUSC\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "max round 4.33\n",
      "integration_values.shape[0] 16611\n",
      "Concordance Index Test 0.5647712697734074\n",
      "Integrated Brier Score Test 0.20369363606093652\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "max round 1.1\n",
      "integration_values.shape[0] 4419\n",
      "Concordance Index Test 0.46093366093366095\n",
      "Integrated Brier Score Test 0.20043111806552955\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "max round 1.09\n",
      "integration_values.shape[0] 5754\n",
      "Concordance Index Test 0.47786850021486893\n",
      "Integrated Brier Score Test 0.20553288127384972\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "max round 1.22\n",
      "integration_values.shape[0] 5814\n",
      "Concordance Index Test 0.5572333170969314\n",
      "Integrated Brier Score Test 0.19827713556109514\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "max round 1.07\n",
      "integration_values.shape[0] 5038\n",
      "Concordance Index Test 0.5588610284742881\n",
      "Integrated Brier Score Test 0.21997156203829102\n",
      "OV\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "max round 1.83\n",
      "integration_values.shape[0] 8085\n",
      "Concordance Index Test 0.5803657362848893\n",
      "Integrated Brier Score Test 0.13490037897429402\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "max round 1.43\n",
      "integration_values.shape[0] 7835\n",
      "Concordance Index Test 0.5694879832810867\n",
      "Integrated Brier Score Test 0.11642623608927068\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "max round 1.13\n",
      "integration_values.shape[0] 2620\n",
      "Concordance Index Test 0.46647230320699706\n",
      "Integrated Brier Score Test 0.15378202885825645\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "max round 1.4\n",
      "integration_values.shape[0] 5420\n",
      "Concordance Index Test 0.5069582504970179\n",
      "Integrated Brier Score Test 0.14593371534249255\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "max round 1.02\n",
      "integration_values.shape[0] 3599\n",
      "Concordance Index Test 0.4410112359550562\n",
      "Integrated Brier Score Test 0.16586000751781396\n",
      "STAD\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "max round 1.57\n",
      "integration_values.shape[0] 5554\n",
      "Concordance Index Test 0.5188227241615332\n",
      "Integrated Brier Score Test 0.23995792867802948\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "max round 3.05\n",
      "integration_values.shape[0] 6918\n",
      "Concordance Index Test 0.6651917404129793\n",
      "Integrated Brier Score Test 0.1744801555170273\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "max round 1.8\n",
      "integration_values.shape[0] 6716\n",
      "Concordance Index Test 0.5716171617161716\n",
      "Integrated Brier Score Test 0.23848724120290682\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "max round 3.31\n",
      "integration_values.shape[0] 7273\n",
      "Concordance Index Test 0.5027548209366391\n",
      "Integrated Brier Score Test 0.2611391549127687\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "max round 1.15\n",
      "integration_values.shape[0] 4048\n",
      "Concordance Index Test 0.5052980132450331\n",
      "Integrated Brier Score Test 0.2359816980602158\n"
     ]
    }
   ],
   "source": [
    "for idx, cancer_type in enumerate(cancer_types):\n",
    "        print(cancer_type)\n",
    "        data = load_tcga(path=data_path, cancer_type=cancer_type, as_frame=True)\n",
    "        filename = data.filename\n",
    "        dataset_name = filename.split('_')[0]\n",
    "        outer_scores = {'cindex_train_'+dataset_name:[], 'cindex_test_'+dataset_name:[]}\n",
    "        best_params = {'best_params_'+dataset_name:[]}\n",
    "        best_model = {'best_model_'+dataset_name:[]}\n",
    "        outer_scores = {'cindex_test_'+dataset_name:[],'ibs_test_'+dataset_name:[]}\n",
    "        X  = data.data \n",
    "        y = data.target \n",
    "        y = pd.concat([y,y], axis=1)\n",
    "        y.columns = ['target1', 'target2']\n",
    "        for i, (train_index, test_index) in enumerate(outer_custom_cv.split(X, y)):\n",
    "                # Split data into training and testing sets for outer fold\n",
    "\n",
    "                X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "                y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "                \n",
    "                np.savetxt('splits/'+model+'train_index_'+str(i)+'_'+filename, train_index, delimiter=',')\n",
    "                np.savetxt('splits/'+model+'test_index_'+str(i)+'_'+filename, test_index, delimiter=',')\n",
    "                \n",
    "                X_train, y_train = sort_X_y_pandas(X_train, y_train)\n",
    "                X_test, y_test = sort_X_y_pandas(X_test, y_test)\n",
    "                rs.fit(X_train, y_train, estimator__eval_test_size=validation_size, estimator__verbose=0)\n",
    "                best_params['best_params_'+dataset_name] += [rs.best_params_]\n",
    "                best_model['best_model_'+dataset_name] += [rs.best_estimator_.get_params()]\n",
    "                best_preds_train = rs.best_estimator_.predict(X_train)\n",
    "                best_preds_test = rs.best_estimator_.predict(X_test)\n",
    "\n",
    "                np.savetxt('predictions/'+model+'best_preds_train_'+str(i)+'_'+filename, best_preds_train, delimiter=',')\n",
    "                np.savetxt('predictions/'+model+'best_preds_test_'+str(i)+'_'+filename, best_preds_test, delimiter=',')\n",
    "\n",
    "                cum_hazard_test = get_cumulative_hazard_function_eh(\n",
    "                        X_train.values, X_test.values, y_train.values, y_test.values,\n",
    "                        best_preds_train, best_preds_test\n",
    "                        )\n",
    "                df_survival_test = np.exp(-cum_hazard_test)\n",
    "                durations_test, events_test = transform_back(y_test.values[:,0])\n",
    "                time_grid_test = np.linspace(durations_test.min(), durations_test.max(), 100)\n",
    "                ev = EvalSurv(df_survival_test, durations_test, events_test, censor_surv='km')\n",
    "                print('Concordance Index Test',ev.concordance_td('antolini'))\n",
    "                print('Integrated Brier Score Test',ev.integrated_brier_score(time_grid_test))\n",
    "                cindex_score_test = ev.concordance_td('antolini')\n",
    "                ibs_score_test = ev.integrated_brier_score(time_grid_test)\n",
    "                outer_scores['cindex_test_'+dataset_name] += [cindex_score_test]\n",
    "                outer_scores['ibs_test_'+dataset_name] += [ibs_score_test]\n",
    "        df_best_params = pd.DataFrame(best_params)\n",
    "        df_best_model = pd.DataFrame(best_model)\n",
    "        df_outer_scores = pd.DataFrame(outer_scores)\n",
    "        df_metrics = pd.concat([df_best_params,df_best_model,df_outer_scores], axis=1)\n",
    "        df_metrics.to_csv('metrics/'+model+'metric_summary_'+filename, index=False)\n",
    "        # cindex\n",
    "        df_agg_metrics_cindex = pd.DataFrame({'dataset':[dataset_name],\n",
    "                                                'cindex_test_mean':df_outer_scores['cindex_test_'+dataset_name].mean(),\n",
    "                                                'cindex_test_std':df_outer_scores['cindex_test_'+dataset_name].std() })\n",
    "        # IBS\n",
    "        df_agg_metrics_ibs = pd.DataFrame({'dataset':[dataset_name],\n",
    "                                                'ibs_test_mean':df_outer_scores['ibs_test_'+dataset_name].mean(),\n",
    "                                                'ibs_test_std':df_outer_scores['ibs_test_'+dataset_name].std() })\n",
    "        agg_metrics_cindex.append(df_agg_metrics_cindex)\n",
    "        agg_metrics_ibs.append(df_agg_metrics_ibs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>cindex_test_mean</th>\n",
       "      <th>cindex_test_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LUSC</td>\n",
       "      <td>0.5239</td>\n",
       "      <td>0.0502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OV</td>\n",
       "      <td>0.5129</td>\n",
       "      <td>0.0615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>STAD</td>\n",
       "      <td>0.5527</td>\n",
       "      <td>0.0687</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  dataset  cindex_test_mean  cindex_test_std\n",
       "0    LUSC            0.5239           0.0502\n",
       "0      OV            0.5129           0.0615\n",
       "0    STAD            0.5527           0.0687"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final_eh_tcga_cindex = pd.concat([df for df in agg_metrics_cindex]).round(4)\n",
    "df_final_eh_tcga_cindex.to_csv('metrics/final_gbdt_tcga_eh_cindex.csv', index=False)\n",
    "df_final_eh_tcga_cindex.to_csv('/Users/JUSC/Documents/644928e0fb7e147893e8ec15/05_thesis/tables/final_gbdt_tcga_eh_cindex.csv', index=False)  #\n",
    "df_final_eh_tcga_cindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>ibs_test_mean</th>\n",
       "      <th>ibs_test_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LUSC</td>\n",
       "      <td>0.2056</td>\n",
       "      <td>0.0085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OV</td>\n",
       "      <td>0.1434</td>\n",
       "      <td>0.0188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>STAD</td>\n",
       "      <td>0.2300</td>\n",
       "      <td>0.0326</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  dataset  ibs_test_mean  ibs_test_std\n",
       "0    LUSC         0.2056        0.0085\n",
       "0      OV         0.1434        0.0188\n",
       "0    STAD         0.2300        0.0326"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final_eh_tcga_ibs = pd.concat([df for df in agg_metrics_ibs]).round(4)\n",
    "df_final_eh_tcga_ibs.to_csv('metrics/final_gbdt_tcga_eh_ibs.csv', index=False)\n",
    "df_final_eh_tcga_ibs.to_csv('/Users/JUSC/Documents/644928e0fb7e147893e8ec15/05_thesis/tables/final_gbdt_tcga_eh_ibs.csv', index=False) \n",
    "df_final_eh_tcga_ibs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xgbsurv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
