{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from xgbsurv.models.utils import transform, transform_back\n",
    "from xgbsurv.models.eh_final import eh_likelihood, eh_gradient\n",
    "from xgbsurv.models.eh_ah_final import ah_likelihood, ah_objective\n",
    "from xgbsurv.models.eh_aft_final import aft_likelihood, aft_objective\n",
    "import sys\n",
    "sys.path.append('/Users/JUSC/Documents/xgbsurv_benchmarking/deep_learning/')\n",
    "from loss_functions_pytorch import eh_likelihood_torch, eh_likelihood_torch_2, aft_likelihood_torch, ah_likelihood_torch\n",
    "import torch\n",
    "import math\n",
    "torch.set_printoptions(precision=10)\n",
    "from torch.autograd.functional import hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data function\n",
    "\n",
    "def aft_data(type='np'):\n",
    "    #h2==0 scenario\n",
    "    if type=='np':\n",
    "        linear_predictor = np.array([0.67254923,\n",
    "        0.86077982,\n",
    "        0.43557393,\n",
    "        0.94059047,\n",
    "        0.8446509 ,\n",
    "        0.23657039,\n",
    "        0.74629685,\n",
    "        0.99700768,\n",
    "        0.28182768,\n",
    "        0.44495038]) #.reshape(1,10)\n",
    "        y = np.array([1, -3, -3, -4, -7,  8,  9,  -11,  13,  16],dtype=np.float32) #.reshape(1,10)\n",
    "        time = np.array([[ 1,  3,  3,  4,  7,  8,  9, 11, 13, 16]])\n",
    "        event = np.array([[1, 0, 0, 0, 0, 1, 1, 0, 1, 1]],dtype=np.float32)\n",
    "    if type=='torch':\n",
    "        linear_predictor = torch.tensor([0.67254923,\n",
    "        0.86077982,\n",
    "        0.43557393,\n",
    "        0.94059047,\n",
    "        0.8446509 ,\n",
    "        0.23657039,\n",
    "        0.74629685,\n",
    "        0.99700768,\n",
    "        0.28182768,\n",
    "        0.44495038], requires_grad=True).reshape(10,1)\n",
    "        y = torch.tensor([[1, -3, -3, -4, -7,  8,  9,  -11,  13,  16]]).reshape(10,1)\n",
    "        time = torch.tensor([[ 1,  3,  3,  4,  7,  8,  9, 11, 13, 16]],dtype=torch.float32)\n",
    "        event = torch.tensor([[1, 0, 0, 0, 0, 1, 1, 0, 1, 1]],dtype=torch.float32).reshape(10,1)\n",
    "\n",
    "    return y, linear_predictor, time, event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, linear_predictor, time, event = aft_data(type='torch')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure\n",
    "\n",
    "- Compare loss to original function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Compare loss to original function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EH loss from paper\n",
    "\n",
    "def eaftloss(out, time, delta): ##loss function for AFT or EH\n",
    "    ia, ib = out.size()\n",
    "    if ib == 1: ###loss function for AFT\n",
    "        n = len(delta)\n",
    "        #print(n)\n",
    "        h = 1.30*math.pow(n,-0.2)\n",
    "        #h 1.304058*math.pow(n,-0.2)  ## 1.304058*n^(-1/5) or 1.587401*math.pow(n,-0.333333) 1.587401*n^(-1/3)\n",
    "        time = time.view(n,1)\n",
    "        delta = delta.view(n,1)\n",
    "        \n",
    "        # R = g(Xi) + log(Oi)\n",
    "        R = torch.add(out,torch.log(time)) \n",
    "        \n",
    "        # Rj - Ri\n",
    "        rawones = torch.ones([1,n], dtype = out.dtype)\n",
    "        R1 = torch.mm(R,rawones)\n",
    "        R2 = torch.mm(torch.t(rawones),torch.t(R))\n",
    "        DR = R1 - R2 \n",
    "        \n",
    "        # K[(Rj-Ri)/h]\n",
    "        K = normal_density(DR/h)\n",
    "        Del = torch.mm(delta, rawones)\n",
    "        DelK = Del*K \n",
    "        \n",
    "        # (1/nh) *sum_j Deltaj * K[(Rj-Ri)/h]\n",
    "        Dk = torch.sum(DelK, dim=0)/(n*h)\n",
    "        \n",
    "        # log {(1/nh) * Deltaj * K[(Rj-Ri)/h]}    \n",
    "        log_Dk = torch.log(Dk)     \n",
    "        A = torch.t(delta)*log_Dk/n   \n",
    "        S1 = A.sum()  \n",
    "        \n",
    "        ncdf=torch.distributions.normal.Normal(torch.tensor([0.0], dtype = out.dtype), torch.tensor([1.0], dtype = out.dtype)).cdf\n",
    "        P = ncdf(DR/h)\n",
    "        CDF_sum = torch.sum(P, dim=0)/n\n",
    "        Q = torch.log(CDF_sum)\n",
    "        S2 = -(delta*Q.view(n,1)).sum()/n\n",
    "             \n",
    "        S0 = -(delta*torch.log(time)).sum()/n\n",
    "        \n",
    "        S = S0 + S1 + S2 \n",
    "        S = -S\n",
    "    else: ### loss function for Extended hazard model\n",
    "        n = len(out[:,0])\n",
    "        h = 1.30*math.pow(n,-0.2)  ## or 1.59*n^(-1/3)\n",
    "        time = time.view(n,1)\n",
    "        delta = delta.view(n,1)\n",
    "        g1 = out[:,0].view(n,1)\n",
    "        g2 = out[:,1].view(n,1)\n",
    "        \n",
    "        # R = g(Xi) + log(Oi)\n",
    "        R = torch.add(g1,torch.log(time)) \n",
    "        \n",
    "        S1 =  (delta*g2).sum()/n\n",
    "        S2 = -(delta*R).sum()/n\n",
    "        \n",
    "        # Rj - Ri\n",
    "        rawones = torch.ones(1,n)\n",
    "        R1 = torch.mm(R,rawones)\n",
    "        R2 = torch.mm(torch.t(rawones),torch.t(R))\n",
    "        DR = R1 - R2 \n",
    "        \n",
    "        # K[(Rj-Ri)/h]\n",
    "        K = normal_density(DR/h)\n",
    "        Del = torch.mm(delta, rawones)\n",
    "        DelK = Del*K \n",
    "        \n",
    "        # (1/nh) *sum_j Deltaj * K[(Rj-Ri)/h]\n",
    "        Dk = torch.sum(DelK, dim=0)/(n*h)  ## Dk would be zero as learning rate too large!\n",
    "        \n",
    "        # log {(1/nh) * Deltaj * K[(Rj-Ri)/h]}    \n",
    "        log_Dk = torch.log(Dk)    \n",
    "        \n",
    "        S3 = (torch.t(delta)*log_Dk).sum()/n    \n",
    "        \n",
    "        # Phi((Rj-Ri)/h)\n",
    "        ncdf=torch.distributions.normal.Normal(torch.tensor([0.0]), torch.tensor([1.0])).cdf\n",
    "        P = ncdf(DR/h) \n",
    "        L = torch.exp(g2-g1)\n",
    "        LL = torch.mm(L,rawones)\n",
    "        LP_sum = torch.sum(LL*P, dim=0)/n\n",
    "        Q = torch.log(LP_sum)\n",
    "        \n",
    "        S4 = -(delta*Q.view(n,1)).sum()/n\n",
    "        \n",
    "        S = S1 + S2 + S3 + S4  \n",
    "        S = -S\n",
    "    return S\n",
    "\n",
    "def normal_density(a):  \n",
    "    b = 0.3989423*torch.exp(-0.5*torch.pow(a,2.0))\n",
    "    return b"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AFT Loss Original Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.5339863300, grad_fn=<NegBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y, linear_predictor, time, event = aft_data(type='torch')\n",
    "aft_loss_paper = eaftloss(linear_predictor, time, event)\n",
    "print(eaftloss(linear_predictor, time, event))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AFT Loss Pytorch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check bandwidth, check dims vector for aft, go through code step by step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-1.5339864492, grad_fn=<MulBackward0>)\n",
      "torch.Size([10]) torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "y, linear_predictor, time, event = aft_data(type='torch')\n",
    "aft_loss_own_torch = aft_likelihood_torch(linear_predictor.reshape(-1),y.reshape(-1))\n",
    "print(aft_likelihood_torch(linear_predictor.reshape(-1),y.reshape(-1)))\n",
    "\n",
    "print(linear_predictor.reshape(-1).shape,y.reshape(-1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 1]), torch.Size([10, 1]))"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape, linear_predictor.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AFT Loss Numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.533986284161934"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y, linear_predictor, time, event = aft_data(type='np')\n",
    "aft_loss_own_numba = aft_likelihood(\n",
    "     y, linear_predictor\n",
    ")\n",
    "aft_likelihood(\n",
    "     y, linear_predictor\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(aft_loss_own_torch.detach().numpy() ,aft_loss_own_torch.detach().numpy(),aft_loss_own_numba)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def aft_data(type='np'):\n",
    "    #h2==0 scenario\n",
    "    if type=='np':\n",
    "        linear_predictor = np.array([0.67254923,\n",
    "        0.86077982,\n",
    "        0.43557393,\n",
    "        0.94059047,\n",
    "        0.8446509 ,\n",
    "        0.23657039,\n",
    "        0.74629685,\n",
    "        0.99700768,\n",
    "        0.28182768,\n",
    "        0.44495038]) #.reshape(1,10)\n",
    "        y = np.array([1, -3, -3, -4, -7,  8,  9,  -11,  13,  16],dtype=np.float32) #.reshape(1,10)\n",
    "        time = np.array([[ 1,  3,  3,  4,  7,  8,  9, 11, 13, 16]])\n",
    "        event = np.array([[1, 0, 0, 0, 0, 1, 1, 0, 1, 1]],dtype=np.float32)\n",
    "    if type=='torch':\n",
    "        linear_predictor = torch.tensor([0.67254923,\n",
    "        0.86077982,\n",
    "        0.43557393,\n",
    "        0.94059047,\n",
    "        0.8446509 ,\n",
    "        0.23657039,\n",
    "        0.74629685,\n",
    "        0.99700768,\n",
    "        0.28182768,\n",
    "        0.44495038], requires_grad=True)\n",
    "        y = torch.tensor([1., -3., -3., -4., -7.,  8.,  9.,  -11.,  13.,  16.], requires_grad=True)\n",
    "        time = torch.tensor([ 1,  3,  3,  4,  7,  8,  9, 11, 13, 16])\n",
    "        event = torch.tensor([1, 0, 0, 0, 0, 1, 1, 0, 1, 1],dtype=torch.float32)\n",
    "        print('y shape', y.shape)\n",
    "        print('linear_predictor', linear_predictor.shape)\n",
    "\n",
    "    return y, linear_predictor, time, event"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Own Pytorch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y shape torch.Size([10])\n",
      "linear_predictor torch.Size([10])\n",
      "loss tensor(-1.5339864492, grad_fn=<MulBackward0>)\n",
      "grad_torch [ 0.06427045 -0.03018909 -0.01806675 -0.04240712 -0.05263754  0.10352549\n",
      "  0.0149176  -0.04637673  0.03045879 -0.02349511]\n"
     ]
    }
   ],
   "source": [
    "y, linear_predictor, time, event = aft_data(type='torch')\n",
    "aft_loss_own_torch = aft_likelihood_torch(linear_predictor,y)\n",
    "print('loss', aft_loss_own_torch)\n",
    "aft_loss_own_torch.backward()\n",
    "grad_torch = linear_predictor.grad.numpy()\n",
    "print('grad_torch', grad_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.06427046,  0.0301891 ,  0.01806675,  0.04240714,  0.05263756,\n",
       "       -0.1035255 , -0.01491763,  0.04637674, -0.03045881,  0.0234951 ])"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y, linear_predictor, time, event = aft_data(type='np')\n",
    "grad_own, hess_own = aft_objective(\n",
    "    y, linear_predictor\n",
    ")\n",
    "grad_own"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Function Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(-grad_own, grad_torch)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hessian Function Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.04928495, 0.03086066, 0.02215578, 0.02689906, 0.00218753,\n",
       "       0.01922084, 0.10613931, 0.04928495, 0.10754386, 0.07927256])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y, linear_predictor, time, event = aft_data(type='np')\n",
    "grad_own, hess_own = aft_objective(\n",
    "    y, linear_predictor\n",
    ")\n",
    "hess_own"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y shape torch.Size([10])\n",
      "linear_predictor torch.Size([10])\n",
      "torch.Size([10]) torch.Size([10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1198979765, -0.0308606531, -0.0221557729, -0.0268990565,\n",
       "        -0.0021875498, -0.0192208346, -0.1061393693,  0.0348668434,\n",
       "        -0.1075438932, -0.0792726427], grad_fn=<DiagBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.autograd.functional import hessian as hess_torch\n",
    "y, linear_predictor, time, event = aft_data(type='torch')\n",
    "print(y.shape, linear_predictor.shape)\n",
    "hessian_matrix = hess_torch(aft_likelihood_torch, (linear_predictor, y), create_graph=True)\n",
    "diag_hessian = hessian_matrix[0][0].diag()\n",
    "diag_hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False,  True,  True,  True,  True,  True,  True, False,  True,\n",
       "        True])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isclose(hess_own,-diag_hessian.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0., 0., 0., 0., 1., 1., 0., 1., 1.])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y shape torch.Size([10])\n",
      "linear_predictor torch.Size([10])\n",
      "torch.Size([10]) torch.Size([10])\n",
      "torch.Size([10]) torch.Size([10])\n",
      "aft\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (10x10 and 1x10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 106\u001b[0m\n\u001b[1;32m    104\u001b[0m y, linear_predictor, time, event \u001b[39m=\u001b[39m aft_data(\u001b[39mtype\u001b[39m\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtorch\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    105\u001b[0m \u001b[39mprint\u001b[39m(y\u001b[39m.\u001b[39mshape, linear_predictor\u001b[39m.\u001b[39mshape)\n\u001b[0;32m--> 106\u001b[0m hessian_matrix \u001b[39m=\u001b[39m hess_torch(eaftloss, (linear_predictor, y), create_graph\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    107\u001b[0m diag_hessian \u001b[39m=\u001b[39m hessian_matrix[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mdiag()\n\u001b[1;32m    108\u001b[0m diag_hessian\n",
      "File \u001b[0;32m~/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/torch/autograd/functional.py:808\u001b[0m, in \u001b[0;36mhessian\u001b[0;34m(func, inputs, create_graph, strict, vectorize, outer_jacobian_strategy)\u001b[0m\n\u001b[1;32m    805\u001b[0m     _check_requires_grad(jac, \u001b[39m\"\u001b[39m\u001b[39mjacobian\u001b[39m\u001b[39m\"\u001b[39m, strict\u001b[39m=\u001b[39mstrict)\n\u001b[1;32m    806\u001b[0m     \u001b[39mreturn\u001b[39;00m jac\n\u001b[0;32m--> 808\u001b[0m res \u001b[39m=\u001b[39m jacobian(jac_func, inputs, create_graph\u001b[39m=\u001b[39;49mcreate_graph, strict\u001b[39m=\u001b[39;49mstrict, vectorize\u001b[39m=\u001b[39;49mvectorize,\n\u001b[1;32m    809\u001b[0m                strategy\u001b[39m=\u001b[39;49mouter_jacobian_strategy)\n\u001b[1;32m    810\u001b[0m \u001b[39mreturn\u001b[39;00m _tuple_postprocess(res, (is_inputs_tuple, is_inputs_tuple))\n",
      "File \u001b[0;32m~/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/torch/autograd/functional.py:575\u001b[0m, in \u001b[0;36mjacobian\u001b[0;34m(func, inputs, create_graph, strict, vectorize, strategy)\u001b[0m\n\u001b[1;32m    572\u001b[0m is_inputs_tuple, inputs \u001b[39m=\u001b[39m _as_tuple(inputs, \u001b[39m\"\u001b[39m\u001b[39minputs\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mjacobian\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    573\u001b[0m inputs \u001b[39m=\u001b[39m _grad_preprocess(inputs, create_graph\u001b[39m=\u001b[39mcreate_graph, need_graph\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m--> 575\u001b[0m outputs \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49minputs)\n\u001b[1;32m    576\u001b[0m is_outputs_tuple, outputs \u001b[39m=\u001b[39m _as_tuple(outputs,\n\u001b[1;32m    577\u001b[0m                                       \u001b[39m\"\u001b[39m\u001b[39moutputs of the user-provided function\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    578\u001b[0m                                       \u001b[39m\"\u001b[39m\u001b[39mjacobian\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    579\u001b[0m _check_requires_grad(outputs, \u001b[39m\"\u001b[39m\u001b[39moutputs\u001b[39m\u001b[39m\"\u001b[39m, strict\u001b[39m=\u001b[39mstrict)\n",
      "File \u001b[0;32m~/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/torch/autograd/functional.py:804\u001b[0m, in \u001b[0;36mhessian.<locals>.jac_func\u001b[0;34m(*inp)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[39mif\u001b[39;00m outer_jacobian_strategy \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mforward-mode\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    801\u001b[0m     \u001b[39m# _grad_preprocess requires create_graph=True and input to require_grad\u001b[39;00m\n\u001b[1;32m    802\u001b[0m     \u001b[39m# or else the input will be detached\u001b[39;00m\n\u001b[1;32m    803\u001b[0m     inp \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(t\u001b[39m.\u001b[39mrequires_grad_(\u001b[39mTrue\u001b[39;00m) \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m inp)\n\u001b[0;32m--> 804\u001b[0m jac \u001b[39m=\u001b[39m jacobian(ensure_single_output_function, inp, create_graph\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    805\u001b[0m _check_requires_grad(jac, \u001b[39m\"\u001b[39m\u001b[39mjacobian\u001b[39m\u001b[39m\"\u001b[39m, strict\u001b[39m=\u001b[39mstrict)\n\u001b[1;32m    806\u001b[0m \u001b[39mreturn\u001b[39;00m jac\n",
      "File \u001b[0;32m~/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/torch/autograd/functional.py:575\u001b[0m, in \u001b[0;36mjacobian\u001b[0;34m(func, inputs, create_graph, strict, vectorize, strategy)\u001b[0m\n\u001b[1;32m    572\u001b[0m is_inputs_tuple, inputs \u001b[39m=\u001b[39m _as_tuple(inputs, \u001b[39m\"\u001b[39m\u001b[39minputs\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mjacobian\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    573\u001b[0m inputs \u001b[39m=\u001b[39m _grad_preprocess(inputs, create_graph\u001b[39m=\u001b[39mcreate_graph, need_graph\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m--> 575\u001b[0m outputs \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49minputs)\n\u001b[1;32m    576\u001b[0m is_outputs_tuple, outputs \u001b[39m=\u001b[39m _as_tuple(outputs,\n\u001b[1;32m    577\u001b[0m                                       \u001b[39m\"\u001b[39m\u001b[39moutputs of the user-provided function\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    578\u001b[0m                                       \u001b[39m\"\u001b[39m\u001b[39mjacobian\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    579\u001b[0m _check_requires_grad(outputs, \u001b[39m\"\u001b[39m\u001b[39moutputs\u001b[39m\u001b[39m\"\u001b[39m, strict\u001b[39m=\u001b[39mstrict)\n",
      "File \u001b[0;32m~/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/torch/autograd/functional.py:787\u001b[0m, in \u001b[0;36mhessian.<locals>.ensure_single_output_function\u001b[0;34m(*inp)\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mensure_single_output_function\u001b[39m(\u001b[39m*\u001b[39minp):\n\u001b[0;32m--> 787\u001b[0m     out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49minp)\n\u001b[1;32m    788\u001b[0m     is_out_tuple, t_out \u001b[39m=\u001b[39m _as_tuple(out, \u001b[39m\"\u001b[39m\u001b[39moutputs of the user-provided function\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mhessian\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    789\u001b[0m     _check_requires_grad(t_out, \u001b[39m\"\u001b[39m\u001b[39moutputs\u001b[39m\u001b[39m\"\u001b[39m, strict\u001b[39m=\u001b[39mstrict)\n",
      "Cell \u001b[0;32mIn[7], line 37\u001b[0m, in \u001b[0;36meaftloss\u001b[0;34m(out, y)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[39m# Rj - Ri\u001b[39;00m\n\u001b[1;32m     36\u001b[0m rawones \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mones([\u001b[39m1\u001b[39m,n], dtype \u001b[39m=\u001b[39m out\u001b[39m.\u001b[39mdtype)\n\u001b[0;32m---> 37\u001b[0m R1 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mmm(R,rawones)\n\u001b[1;32m     38\u001b[0m R2 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmm(torch\u001b[39m.\u001b[39mt(rawones),torch\u001b[39m.\u001b[39mt(R))\n\u001b[1;32m     39\u001b[0m DR \u001b[39m=\u001b[39m R1 \u001b[39m-\u001b[39m R2 \n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (10x10 and 1x10)"
     ]
    }
   ],
   "source": [
    "def transform_back_torch(y: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Transforms XGBoost digestable format variable y into time and event.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y : npt.NDArray[float]\n",
    "        Array containing survival time and event where negative value is taken as censored event.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple[npt.NDArray[float],npt.NDArray[int]]\n",
    "        Survival time and event.\n",
    "    \"\"\"\n",
    "    time = torch.abs(y)\n",
    "    event = (torch.abs(y) == y)\n",
    "    event = event # for numba\n",
    "    return time, event\n",
    "\n",
    "def eaftloss(out, y): ##loss function for AFT or EH\n",
    "    time, delta = transform_back_torch(y)\n",
    "    print(time.shape, delta.shape)\n",
    "    delta = delta.float()\n",
    "    time = time.float()\n",
    "    ###loss function for AFT\n",
    "    n = len(delta)\n",
    "    print('aft')\n",
    "    h = 1.30*math.pow(n,-0.2)\n",
    "    #h 1.304058*math.pow(n,-0.2)  ## 1.304058*n^(-1/5) or 1.587401*math.pow(n,-0.333333) 1.587401*n^(-1/3)\n",
    "    time = time.view(n,1)\n",
    "    delta = delta.view(n,1)\n",
    "    \n",
    "    # R = g(Xi) + log(Oi)\n",
    "    R = torch.add(out,torch.log(time)) \n",
    "    \n",
    "    # Rj - Ri\n",
    "    rawones = torch.ones([1,n], dtype = out.dtype)\n",
    "    R1 = torch.mm(R,rawones)\n",
    "    R2 = torch.mm(torch.t(rawones),torch.t(R))\n",
    "    DR = R1 - R2 \n",
    "    \n",
    "    # K[(Rj-Ri)/h]\n",
    "    K = normal_density(DR/h)\n",
    "    Del = torch.mm(delta, rawones)\n",
    "    DelK = Del*K \n",
    "    \n",
    "    # (1/nh) *sum_j Deltaj * K[(Rj-Ri)/h]\n",
    "    Dk = torch.sum(DelK, dim=0)/(n*h)\n",
    "    \n",
    "    # log {(1/nh) * Deltaj * K[(Rj-Ri)/h]}    \n",
    "    log_Dk = torch.log(Dk)     \n",
    "    A = torch.t(delta)*log_Dk/n   \n",
    "    S1 = A.sum()  \n",
    "    \n",
    "    ncdf=torch.distributions.normal.Normal(torch.tensor([0.0], dtype = out.dtype), torch.tensor([1.0], dtype = out.dtype)).cdf\n",
    "    P = ncdf(DR/h)\n",
    "    CDF_sum = torch.sum(P, dim=0)/n\n",
    "    Q = torch.log(CDF_sum)\n",
    "    S2 = -(delta*Q.view(n,1)).sum()/n\n",
    "            \n",
    "    S0 = -(delta*torch.log(time)).sum()/n\n",
    "    \n",
    "    S = S0 + S1 + S2 \n",
    "    S = -S\n",
    "    return S\n",
    "\n",
    "def normal_density(a):  \n",
    "    b = 0.3989423*torch.exp(-0.5*torch.pow(a,2.0))\n",
    "    return b\n",
    "def aft_data(type='np'):\n",
    "    #h2==0 scenario\n",
    "    if type=='np':\n",
    "        linear_predictor = np.array([0.67254923,\n",
    "        0.86077982,\n",
    "        0.43557393,\n",
    "        0.94059047,\n",
    "        0.8446509 ,\n",
    "        0.23657039,\n",
    "        0.74629685,\n",
    "        0.99700768,\n",
    "        0.28182768,\n",
    "        0.44495038]) #.reshape(1,10)\n",
    "        y = np.array([1, -3, -3, -4, -7,  8,  9,  -11,  13,  16],dtype=np.float32) #.reshape(1,10)\n",
    "        time = np.array([[ 1,  3,  3,  4,  7,  8,  9, 11, 13, 16]])\n",
    "        event = np.array([[1, 0, 0, 0, 0, 1, 1, 0, 1, 1]],dtype=np.float32)\n",
    "    if type=='torch':\n",
    "        linear_predictor = torch.tensor([0.67254923,\n",
    "        0.86077982,\n",
    "        0.43557393,\n",
    "        0.94059047,\n",
    "        0.8446509 ,\n",
    "        0.23657039,\n",
    "        0.74629685,\n",
    "        0.99700768,\n",
    "        0.28182768,\n",
    "        0.44495038], requires_grad=True)\n",
    "        y = torch.tensor([1., -3., -3., -4., -7.,  8.,  9.,  -11.,  13.,  16.], requires_grad=True)\n",
    "        time = torch.tensor([ 1,  3,  3,  4,  7,  8,  9, 11, 13, 16])\n",
    "        event = torch.tensor([1, 0, 0, 0, 0, 1, 1, 0, 1, 1],dtype=torch.float32)\n",
    "        print('y shape', y.shape)\n",
    "        print('linear_predictor', linear_predictor.shape)\n",
    "\n",
    "    return y, linear_predictor, time, event\n",
    "from torch.autograd.functional import hessian as hess_torch\n",
    "y, linear_predictor, time, event = aft_data(type='torch')\n",
    "print(y.shape, linear_predictor.shape)\n",
    "hessian_matrix = hess_torch(eaftloss, (linear_predictor, y), create_graph=True)\n",
    "diag_hessian = hessian_matrix[0][0].diag()\n",
    "diag_hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xgbsurv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
