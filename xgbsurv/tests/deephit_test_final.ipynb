{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from xgbsurv.models.utils import transform, transform_back\n",
    "from xgbsurv.models.eh_final import eh_likelihood, eh_gradient\n",
    "from xgbsurv.models.eh_ah_final import ah_likelihood, ah_objective\n",
    "from xgbsurv.models.utils import sort_X_y, transform_back\n",
    "import sys\n",
    "#from loss_functions_pytorch import deephit_likelihood_1_torch\n",
    "import torch\n",
    "import math\n",
    "torch.set_printoptions(precision=10)\n",
    "from torch.autograd.functional import hessian\n",
    "from xgbsurv.datasets import load_metabric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_back_torch(y: torch.torch.torch.torch.torch.torch.torch.torch.torch.tensor) -> tuple[torch.torch.tensor, torch.torch.tensor]:\n",
    "    \"\"\"Transforms XGBoost digestable format variable y into time and event.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y : npt.NDArray[float]\n",
    "        Array containing survival time and event where negative value is taken as censored event.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple[npt.NDArray[float],npt.NDArray[int]]\n",
    "        Survival time and event.\n",
    "    \"\"\"\n",
    "    time = torch.abs(y)\n",
    "    event = (torch.abs(y) == y)\n",
    "    event = event # for numba\n",
    "    return time.to(torch.float32), event.to(torch.float32)\n",
    "\n",
    "def transform_back_torch_deephit(y: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Transforms XGBoost digestable format variable y into time and event.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y : npt.NDArray[float]\n",
    "        Array containing survival time and event where negative value is taken as censored event.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple[npt.NDArray[float],npt.NDArray[int]]\n",
    "        Survival time and event.\n",
    "    \"\"\"\n",
    "    #TODO: Build conditions and combine transform functions\n",
    "    y = y[:,0]\n",
    "    time = torch.abs(y)\n",
    "    event = (torch.abs(y) == y)\n",
    "    event = event # for numba\n",
    "    return time.to(torch.float32), event.to(torch.float32)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi = torch.torch.tensor([[0.56061679, 0.67018677, 0.830002  , 0.54249826, 0.06989779,\n",
    "        0.37876414, 0.03824207, 0.39794583, 0.3478378 , 0.46833452,\n",
    "        0.32292692, 0.70704927, 0.56557495, 0.58150488, 0.38871381,\n",
    "        0.34848768, 0.39602859, 0.7649664 , 0.97185072],\n",
    "       [0.95566696, 0.48104482, 0.75679969, 0.99386694, 0.91718885,\n",
    "        0.49509238, 0.29452671, 0.29319748, 0.12167261, 0.34684764,\n",
    "        0.20933687, 0.56328841, 0.04492321, 0.72173335, 0.79000414,\n",
    "        0.50887818, 0.18630897, 0.53458442, 0.28050238],\n",
    "       [0.15593853, 0.02782267, 0.01185559, 0.71230657, 0.81883045,\n",
    "        0.4969608 , 0.74693123, 0.87947765, 0.31456862, 0.12872293,\n",
    "        0.23845144, 0.64749992, 0.97677924, 0.57487316, 0.35761132,\n",
    "        0.97724796, 0.8684253 , 0.68128472, 0.0869709 ],\n",
    "       [0.99632921, 0.56681704, 0.95074056, 0.92120083, 0.76054228,\n",
    "        0.27696215, 0.62860255, 0.70248436, 0.92183456, 0.70921057,\n",
    "        0.76649902, 0.64107988, 0.24368451, 0.51152253, 0.80426748,\n",
    "        0.74221419, 0.27668871, 0.17919406, 0.42808766],\n",
    "       [0.27972894, 0.56866687, 0.6539161 , 0.5319978 , 0.99621816,\n",
    "        0.33599839, 0.37512062, 0.14721801, 0.53298744, 0.50357236,\n",
    "        0.38652909, 0.26062667, 0.77156399, 0.84954073, 0.68774531,\n",
    "        0.47792277, 0.06867723, 0.76415557, 0.49115389],\n",
    "       [0.79273291, 0.92759044, 0.81326807, 0.04226328, 0.98768654,\n",
    "        0.07537023, 0.55331409, 0.22540424, 0.71398715, 0.7250366 ,\n",
    "        0.32643635, 0.29281341, 0.46330328, 0.41149408, 0.13778508,\n",
    "        0.03694444, 0.30238445, 0.44078415, 0.19661292],\n",
    "       [0.75843055, 0.99731981, 0.94590159, 0.06033445, 0.20452619,\n",
    "        0.76126846, 0.54285514, 0.7893368 , 0.38247081, 0.85430444,\n",
    "        0.52043418, 0.18891051, 0.63758591, 0.37400696, 0.7804225 ,\n",
    "        0.47204052, 0.56156778, 0.33566019, 0.3990119 ],\n",
    "       [0.5306947 , 0.49757042, 0.69553678, 0.50663494, 0.44598667,\n",
    "        0.21937834, 0.90704425, 0.65035354, 0.1341808 , 0.76539241,\n",
    "        0.58519312, 0.98323395, 0.85668469, 0.02645247, 0.71319072,\n",
    "        0.59216107, 0.21904393, 0.80283269, 0.38052726],\n",
    "       [0.33536991, 0.56913116, 0.42259562, 0.68180137, 0.73365645,\n",
    "        0.66044068, 0.97128302, 0.20019211, 0.73096269, 0.31609701,\n",
    "        0.67428402, 0.48398141, 0.3843597 , 0.38371562, 0.36886752,\n",
    "        0.0794369 , 0.03408211, 0.75023623, 0.68362319],\n",
    "       [0.65184423, 0.48534751, 0.08064047, 0.78674504, 0.06255098,\n",
    "        0.07844527, 0.196811  , 0.52218442, 0.81096049, 0.23212956,\n",
    "        0.06336911, 0.14451082, 0.59268832, 0.44494552, 0.30831269,\n",
    "        0.18509292, 0.7571426 , 0.78059063, 0.83033576],\n",
    "       [0.1657391 , 0.98890883, 0.59125266, 0.80830763, 0.19778208,\n",
    "        0.75399619, 0.71727879, 0.4851396 , 0.15589286, 0.73861217,\n",
    "        0.511602  , 0.16323839, 0.66437389, 0.18004531, 0.81873861,\n",
    "        0.07826153, 0.8947921 , 0.61635483, 0.43455631],\n",
    "       [0.395005  , 0.97093084, 0.33062423, 0.23574601, 0.51486585,\n",
    "        0.42856454, 0.94329152, 0.70344321, 0.57317513, 0.94632976,\n",
    "        0.63672714, 0.99563958, 0.57327768, 0.03910801, 0.07771027,\n",
    "        0.48566129, 0.85488311, 0.6057723 , 0.16950561],\n",
    "       [0.59508459, 0.95045607, 0.92343267, 0.96820986, 0.06633364,\n",
    "        0.83614061, 0.67577314, 0.74443865, 0.59352958, 0.51259308,\n",
    "        0.03491607, 0.04434695, 0.85837897, 0.71040344, 0.02438701,\n",
    "        0.99425333, 0.79017128, 0.12834676, 0.3988308 ],\n",
    "       [0.68108959, 0.28834384, 0.81592558, 0.17091973, 0.21407878,\n",
    "        0.23098784, 0.42014381, 0.85523219, 0.56613711, 0.61387742,\n",
    "        0.55994703, 0.61032793, 0.65646759, 0.1812232 , 0.26260767,\n",
    "        0.35116788, 0.64650932, 0.98678635, 0.52668915],\n",
    "       [0.28992255, 0.40149962, 0.671999  , 0.87495982, 0.41206135,\n",
    "        0.47103786, 0.83976069, 0.99689313, 0.53925724, 0.26320269,\n",
    "        0.22911652, 0.97733889, 0.16904251, 0.42565559, 0.25421109,\n",
    "        0.05060954, 0.51099776, 0.076741  , 0.71688287],\n",
    "       [0.43050308, 0.08717614, 0.33684441, 0.7273216 , 0.48634413,\n",
    "        0.7530504 , 0.52148584, 0.49021319, 0.23076744, 0.46344035,\n",
    "        0.45206929, 0.56467496, 0.49647443, 0.4774734 , 0.90622309,\n",
    "        0.57040588, 0.49141411, 0.28491756, 0.26073103],\n",
    "       [0.44798068, 0.89615949, 0.30552967, 0.33349176, 0.95502655,\n",
    "        0.91813918, 0.62428386, 0.84984507, 0.86836687, 0.78609441,\n",
    "        0.47261591, 0.80318656, 0.13896811, 0.90963732, 0.05839439,\n",
    "        0.83315826, 0.28216467, 0.1452794 , 0.70495897],\n",
    "       [0.00463247, 0.51736001, 0.08070175, 0.11176686, 0.49550875,\n",
    "        0.39094751, 0.65327167, 0.56788782, 0.73013999, 0.25124108,\n",
    "        0.31757687, 0.58068344, 0.56020449, 0.69263596, 0.56677569,\n",
    "        0.0296664 , 0.88003492, 0.21268497, 0.78623878],\n",
    "       [0.3782029 , 0.81943188, 0.81412233, 0.52370451, 0.79590929,\n",
    "        0.92546203, 0.59783995, 0.2973661 , 0.84875957, 0.64143599,\n",
    "        0.45042338, 0.916713  , 0.39235494, 0.94565556, 0.82454687,\n",
    "        0.89168315, 0.18688824, 0.97046952, 0.3591719 ],\n",
    "       [0.88820103, 0.22469503, 0.09393979, 0.90702622, 0.2599165 ,\n",
    "        0.38592207, 0.05689468, 0.74827846, 0.61047844, 0.97566285,\n",
    "        0.75526081, 0.50752004, 0.94641458, 0.51176464, 0.42993403,\n",
    "        0.86378897, 0.86438584, 0.12020468, 0.5326653 ]])\n",
    "\n",
    "\n",
    "def deephit_data(phi=phi):\n",
    "    nrows = 20\n",
    "    data = load_metabric(path=\"/Users/JUSC/Documents/xgbsurv/xgbsurv/datasets/data/\", as_frame=False)\n",
    "    data, target = sort_X_y(data.data, data.target)\n",
    "    data = data[:nrows,:]\n",
    "    target = target[:nrows]\n",
    "    data = torch.torch.tensor(data, dtype=torch.float32)\n",
    "    time, event = transform_back(target)\n",
    "    ncols = len(np.unique(np.abs(target)))\n",
    "    target = np.tile(target, (ncols,1)).T\n",
    "    #print(ncols)\n",
    "    #phi = np.random.rand(20, ncols)\n",
    "    target = torch.torch.tensor(target, dtype=torch.float32)\n",
    "    time = torch.torch.tensor(time, dtype=torch.float32)\n",
    "    event = torch.torch.tensor(event, dtype=torch.float32)\n",
    "    print(type(target))\n",
    "    return time, event, target, phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 4, 3, 2])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([0.2, 6.4, 3.0, 1.6])\n",
    "\n",
    "bins = np.array([0.0, 1.0, 2.5, 4.0, 10.0])\n",
    "\n",
    "inds = np.digitize(x, bins)\n",
    "\n",
    "inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 4, 3, 2])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.torch.tensor([0.2, 6.4, 3.0, 1.6])\n",
    "\n",
    "bins = torch.torch.tensor([0.0, 1.0, 2.5, 4.0, 10.0])\n",
    "\n",
    "inds = torch.bucketize(x, bins)\n",
    "\n",
    "inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_col(input, val=0, where='end'):\n",
    "    \"\"\"Addes a column of `val` at the start of end of `input`.\"\"\"\n",
    "    if len(input.shape) != 2:\n",
    "        raise ValueError(f\"Only works for `phi` torch.tensor that is 2-D.\")\n",
    "    pad = torch.zeros_like(input[:, :1])\n",
    "    if val != 0:\n",
    "        pad = pad + val\n",
    "    if where == 'end':\n",
    "        return torch.cat([input, pad], dim=1)\n",
    "    elif where == 'start':\n",
    "        return torch.cat([pad, input], dim=1)\n",
    "    raise ValueError(f\"Need `where` to be 'start' or 'end', got {where}\")\n",
    "\n",
    "def _reduction(loss: torch.tensor, reduction: str = 'mean') -> torch.tensor:\n",
    "    if reduction == 'none':\n",
    "        return loss\n",
    "    elif reduction == 'mean':\n",
    "        return loss.mean()\n",
    "    elif reduction == 'sum':\n",
    "        return loss.sum()\n",
    "    raise ValueError(f\"`reduction` = {reduction} is not valid. Use 'none', 'mean' or 'sum'.\")\n",
    "\n",
    "def nll_pmf(phi: torch.tensor, idx_durations: torch.tensor, events: torch.tensor, reduction: str = 'mean',\n",
    "            epsilon: float = 1e-7) -> torch.tensor:\n",
    "    \"\"\"Negative log-likelihood for the PMF parametrized model [1].\n",
    "    \n",
    "    Arguments:\n",
    "        phi {torch.torch.tensor} -- Estimates in (-inf, inf), where pmf = somefunc(phi).\n",
    "        idx_durations {torch.torch.tensor} -- Event times represented as indices.\n",
    "        events {torch.torch.tensor} -- Indicator of event (1.) or censoring (0.).\n",
    "            Same length as 'idx_durations'.\n",
    "        reduction {string} -- How to reduce the loss.\n",
    "            'none': No reduction.\n",
    "            'mean': Mean of torch.tensor.\n",
    "            'sum: sum.\n",
    "    \n",
    "    Returns:\n",
    "        torch.torch.tensor -- The negative log-likelihood.\n",
    "\n",
    "    References:\n",
    "    [1] Håvard Kvamme and Ørnulf Borgan. Continuous and Discrete-Time Survival Prediction\n",
    "        with Neural Networks. arXiv preprint arXiv:1910.06724, 2019.\n",
    "        https://arxiv.org/pdf/1910.06724.pdf\n",
    "    \"\"\"\n",
    "    if phi.shape[1] <= idx_durations.max():\n",
    "        raise ValueError(f\"Network output `phi` is too small for `idx_durations`.\"+\n",
    "                         f\" Need at least `phi.shape[1] = {idx_durations.max().item()+1}`,\"+\n",
    "                         f\" but got `phi.shape[1] = {phi.shape[1]}`\")\n",
    "    if events.dtype is torch.bool:\n",
    "        events = events.float()\n",
    "    events = events.view(-1)\n",
    "    idx_durations = idx_durations.view(-1, 1)\n",
    "    phi = pad_col(phi)\n",
    "    print('phi shape', phi.shape)\n",
    "    gamma = phi.max(1)[0]\n",
    "    cumsum = phi.sub(gamma.view(-1, 1)).exp().cumsum(1)\n",
    "    sum_ = cumsum[:, -1]\n",
    "    print('shapes', idx_durations.shape, phi.shape, gamma.shape, events.shape)\n",
    "    part1 = phi.gather(1, idx_durations).view(-1).sub(gamma).mul(events)\n",
    "    part2 = - sum_.relu().add(epsilon).log()\n",
    "    part3 = sum_.sub(cumsum.gather(1, idx_durations).view(-1)).relu().add(epsilon).log().mul(1. - events)\n",
    "    # need relu() in part3 (and possibly part2) because cumsum on gpu has some bugs and we risk getting negative numbers.\n",
    "    loss = - part1.add(part2).add(part3)\n",
    "    return _reduction(loss, reduction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "time, events, y, phi = deephit_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 19])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 10, 11, 12, 13, 14, 15, 16,\n",
       "        17, 18])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bins = torch.unique(time)\n",
    "idx_durations = (torch.bucketize(time, bins))\n",
    "idx_durations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phi shape torch.Size([20, 20])\n",
      "shapes torch.Size([20, 1]) torch.Size([20, 20]) torch.Size([20]) torch.Size([20])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(31.7249526978)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nll_pmf(phi, idx_durations, events, reduction='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def deephit_likelihood_1_torch(y, phi):\n",
    "    time, events = transform_back_torch_deephit(y)\n",
    "    #time = time.reshape(time.shape[0],1)\n",
    "    #print('shape idx dur befoer', idx_durations.shape)\n",
    "    bins = torch.unique(time)\n",
    "    idx_durations = (torch.bucketize(time, bins))\n",
    "    idx_durations = idx_durations.view(-1, 1)\n",
    "    #print('shape idx dur after', idx_durations.shape)\n",
    "    # epsilon \n",
    "    epsilon = np.finfo(float).eps\n",
    "    # pad phi as in pycox\n",
    "    pad = torch.zeros_like(phi[:,:1])\n",
    "    phi = torch.cat([phi, pad],axis=1)\n",
    "    print('phi shape', phi.shape)\n",
    "    # create durations index\n",
    "    bins = torch.unique(time)\n",
    "\n",
    "    if phi.shape[1] <= idx_durations.max():\n",
    "        raise ValueError(f\"Network output `phi` is too small for `idx_durations`.\"+\n",
    "                         f\" Need at least `phi.shape[1] = {idx_durations.max().item()+1}`,\"+\n",
    "                         f\" but got `phi.shape[1] = {phi.shape[1]}`\")\n",
    "    if events.dtype is torch.bool:\n",
    "        events = events.float()\n",
    "    #events = events.view(-1)\n",
    "    #idx_durations = idx_durations.view(-1, 1)\n",
    "    #phi = utils.pad_col(phi)\n",
    "\n",
    "    gamma = phi.max(1)[0]\n",
    "    print('shapes', idx_durations.shape, phi.shape, gamma.shape, events.shape)\n",
    "    cumsum = phi.sub(gamma.view(-1, 1)).exp().cumsum(1)\n",
    "    sum_ = cumsum[:, -1]\n",
    "    part1 = phi.gather(1, idx_durations).view(-1).sub(gamma).mul(events)\n",
    "    part2 = - sum_.relu().add(epsilon).log()\n",
    "    part3 = sum_.sub(cumsum.gather(1, idx_durations).view(-1)).relu().add(epsilon).log().mul(1. - events)\n",
    "    # need relu() in part3 (and possibly part2) because cumsum on gpu has some bugs and we risk getting negative numbers.\n",
    "    loss = - part1.add(part2).add(part3)\n",
    "    return torch.sum(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phi shape torch.Size([20, 20])\n",
      "shapes torch.Size([20, 1]) torch.Size([20, 20]) torch.Size([20]) torch.Size([20])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(31.7249526978)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deephit_likelihood_1_torch(y, phi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1000000015, 0.7666666508, 1.2333333492, 1.2666666508, 1.4333332777,\n",
       "        1.7666666508, 2.0000000000, 2.2999999523, 2.4000000954, 2.5000000000,\n",
       "        2.5333333015, 3.3666665554, 3.5000000000, 3.7666666508, 4.1666665077,\n",
       "        4.4333333969, 4.8666667938, 5.0666666031, 5.4333333969])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.unique(time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def deephit_likelihood_1_torch(y, phi):\n",
    "    #time, events = transform_back_torch_deephit(y)\n",
    "    time, events = transform_back_torch(y)\n",
    "    #time = time.reshape(time.shape[0],1)\n",
    "    #print('shape idx dur befoer', idx_durations.shape)\n",
    "    bins = torch.unique(time)\n",
    "    idx_durations = (torch.bucketize(time, bins))\n",
    "    idx_durations = idx_durations.view(-1, 1)\n",
    "    print('idx_durations',idx_durations)\n",
    "    #print('shape idx dur after', idx_durations.shape)\n",
    "    # epsilon \n",
    "    epsilon = np.finfo(float).eps\n",
    "    # pad phi as in pycox\n",
    "    pad = torch.zeros_like(phi[:,:1])\n",
    "    phi = torch.cat([phi, pad],axis=1)\n",
    "    print('phi shape', phi.shape)\n",
    "    # create durations index\n",
    "    bins = torch.unique(time)\n",
    "\n",
    "    if phi.shape[1] <= idx_durations.max():\n",
    "        raise ValueError(f\"Network output `phi` is too small for `idx_durations`.\"+\n",
    "                         f\" Need at least `phi.shape[1] = {idx_durations.max().item()+1}`,\"+\n",
    "                         f\" but got `phi.shape[1] = {phi.shape[1]}`\")\n",
    "    if events.dtype is torch.bool:\n",
    "        events = events.float()\n",
    "    #events = events.view(-1)\n",
    "    #idx_durations = idx_durations.view(-1, 1)\n",
    "    #phi = utils.pad_col(phi)\n",
    "\n",
    "    gamma = phi.max(1)[0]\n",
    "    print('shapes', idx_durations.shape, phi.shape, gamma.shape, events.shape)\n",
    "    cumsum = phi.sub(gamma.view(-1, 1)).exp().cumsum(1)\n",
    "    sum_ = cumsum[:, -1]\n",
    "    part1 = phi.gather(1, idx_durations).view(-1).sub(gamma).mul(events)\n",
    "    part2 = - sum_.relu().add(epsilon).log()\n",
    "    part3 = sum_.sub(cumsum.gather(1, idx_durations).view(-1)).relu().add(epsilon).log().mul(1. - events)\n",
    "    # need relu() in part3 (and possibly part2) because cumsum on gpu has some bugs and we risk getting negative numbers.\n",
    "    loss = - part1.add(part2).add(part3)\n",
    "    return torch.sum(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx_durations tensor([[ 0],\n",
      "        [ 1],\n",
      "        [ 2],\n",
      "        [ 3],\n",
      "        [ 4],\n",
      "        [ 5],\n",
      "        [ 6],\n",
      "        [ 7],\n",
      "        [ 8],\n",
      "        [ 9],\n",
      "        [10],\n",
      "        [10],\n",
      "        [11],\n",
      "        [12],\n",
      "        [13],\n",
      "        [14],\n",
      "        [15],\n",
      "        [16],\n",
      "        [17],\n",
      "        [18]])\n",
      "phi shape torch.Size([20, 20])\n",
      "shapes torch.Size([20, 1]) torch.Size([20, 20]) torch.Size([20]) torch.Size([20])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(31.7249526978)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deephit_likelihood_1_torch(y[:,0], phi)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Compare loss to original function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EH loss from paper\n",
    "\n",
    "def eaftloss(out, time, delta): ##loss function for AFT or EH\n",
    "    ia, ib = out.size()\n",
    "    if ib == 1: ###loss function for AFT\n",
    "        n = len(delta)\n",
    "        #print(n)\n",
    "        h = 1.30*math.pow(n,-0.2)\n",
    "        #h 1.304058*math.pow(n,-0.2)  ## 1.304058*n^(-1/5) or 1.587401*math.pow(n,-0.333333) 1.587401*n^(-1/3)\n",
    "        time = time.view(n,1)\n",
    "        delta = delta.view(n,1)\n",
    "        \n",
    "        # R = g(Xi) + log(Oi)\n",
    "        R = torch.add(out,torch.log(time)) \n",
    "        \n",
    "        # Rj - Ri\n",
    "        rawones = torch.ones([1,n], dtype = out.dtype)\n",
    "        R1 = torch.mm(R,rawones)\n",
    "        R2 = torch.mm(torch.t(rawones),torch.t(R))\n",
    "        DR = R1 - R2 \n",
    "        \n",
    "        # K[(Rj-Ri)/h]\n",
    "        K = normal_density(DR/h)\n",
    "        Del = torch.mm(delta, rawones)\n",
    "        DelK = Del*K \n",
    "        \n",
    "        # (1/nh) *sum_j Deltaj * K[(Rj-Ri)/h]\n",
    "        Dk = torch.sum(DelK, dim=0)/(n*h)\n",
    "        \n",
    "        # log {(1/nh) * Deltaj * K[(Rj-Ri)/h]}    \n",
    "        log_Dk = torch.log(Dk)     \n",
    "        A = torch.t(delta)*log_Dk/n   \n",
    "        S1 = A.sum()  \n",
    "        \n",
    "        ncdf=torch.distributions.normal.Normal(torch.torch.tensor([0.0], dtype = out.dtype), torch.torch.tensor([1.0], dtype = out.dtype)).cdf\n",
    "        P = ncdf(DR/h)\n",
    "        CDF_sum = torch.sum(P, dim=0)/n\n",
    "        Q = torch.log(CDF_sum)\n",
    "        S2 = -(delta*Q.view(n,1)).sum()/n\n",
    "             \n",
    "        S0 = -(delta*torch.log(time)).sum()/n\n",
    "        \n",
    "        S = S0 + S1 + S2 \n",
    "        S = -S\n",
    "    else: ### loss function for Extended hazard model\n",
    "        n = len(out[:,0])\n",
    "        h = 1.30*math.pow(n,-0.2)  ## or 1.59*n^(-1/3)\n",
    "        time = time.view(n,1)\n",
    "        delta = delta.view(n,1)\n",
    "        g1 = out[:,0].view(n,1)\n",
    "        g2 = out[:,1].view(n,1)\n",
    "        \n",
    "        # R = g(Xi) + log(Oi)\n",
    "        R = torch.add(g1,torch.log(time)) \n",
    "        \n",
    "        S1 =  (delta*g2).sum()/n\n",
    "        S2 = -(delta*R).sum()/n\n",
    "        \n",
    "        # Rj - Ri\n",
    "        rawones = torch.ones(1,n)\n",
    "        R1 = torch.mm(R,rawones)\n",
    "        R2 = torch.mm(torch.t(rawones),torch.t(R))\n",
    "        DR = R1 - R2 \n",
    "        \n",
    "        # K[(Rj-Ri)/h]\n",
    "        K = normal_density(DR/h)\n",
    "        Del = torch.mm(delta, rawones)\n",
    "        DelK = Del*K \n",
    "        \n",
    "        # (1/nh) *sum_j Deltaj * K[(Rj-Ri)/h]\n",
    "        Dk = torch.sum(DelK, dim=0)/(n*h)  ## Dk would be zero as learning rate too large!\n",
    "        \n",
    "        # log {(1/nh) * Deltaj * K[(Rj-Ri)/h]}    \n",
    "        log_Dk = torch.log(Dk)    \n",
    "        \n",
    "        S3 = (torch.t(delta)*log_Dk).sum()/n    \n",
    "        \n",
    "        # Phi((Rj-Ri)/h)\n",
    "        ncdf=torch.distributions.normal.Normal(torch.torch.tensor([0.0]), torch.torch.tensor([1.0])).cdf\n",
    "        P = ncdf(DR/h) \n",
    "        L = torch.exp(g2-g1)\n",
    "        LL = torch.mm(L,rawones)\n",
    "        LP_sum = torch.sum(LL*P, dim=0)/n\n",
    "        Q = torch.log(LP_sum)\n",
    "        \n",
    "        S4 = -(delta*Q.view(n,1)).sum()/n\n",
    "        \n",
    "        S = S1 + S2 + S3 + S4  \n",
    "        S = -S\n",
    "    return S\n",
    "\n",
    "def normal_density(a):  \n",
    "    b = 0.3989423*torch.exp(-0.5*torch.pow(a,2.0))\n",
    "    return b"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AFT Loss Original Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.5339863300, grad_fn=<NegBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y, linear_predictor, time, event = aft_data(type='torch')\n",
    "aft_loss_paper = eaftloss(linear_predictor, time, event)\n",
    "print(eaftloss(linear_predictor, time, event))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AFT Loss Pytorch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check bandwidth, check dims vector for aft, go through code step by step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-1.5339864492, grad_fn=<MulBackward0>)\n",
      "torch.Size([10]) torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "y, linear_predictor, time, event = aft_data(type='torch')\n",
    "aft_loss_own_torch = aft_likelihood_torch(linear_predictor.reshape(-1),y.reshape(-1))\n",
    "print(aft_likelihood_torch(linear_predictor.reshape(-1),y.reshape(-1)))\n",
    "\n",
    "print(linear_predictor.reshape(-1).shape,y.reshape(-1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 1]), torch.Size([10, 1]))"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape, linear_predictor.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AFT Loss Numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.533986284161934"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y, linear_predictor, time, event = aft_data(type='np')\n",
    "aft_loss_own_numba = aft_likelihood(\n",
    "     y, linear_predictor\n",
    ")\n",
    "aft_likelihood(\n",
    "     y, linear_predictor\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(aft_loss_own_torch.detach().numpy() ,aft_loss_own_torch.detach().numpy(),aft_loss_own_numba)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def aft_data(type='np'):\n",
    "    #h2==0 scenario\n",
    "    if type=='np':\n",
    "        linear_predictor = np.array([0.67254923,\n",
    "        0.86077982,\n",
    "        0.43557393,\n",
    "        0.94059047,\n",
    "        0.8446509 ,\n",
    "        0.23657039,\n",
    "        0.74629685,\n",
    "        0.99700768,\n",
    "        0.28182768,\n",
    "        0.44495038]) #.reshape(1,10)\n",
    "        y = np.array([1, -3, -3, -4, -7,  8,  9,  -11,  13,  16],dtype=np.float32) #.reshape(1,10)\n",
    "        time = np.array([[ 1,  3,  3,  4,  7,  8,  9, 11, 13, 16]])\n",
    "        event = np.array([[1, 0, 0, 0, 0, 1, 1, 0, 1, 1]],dtype=np.float32)\n",
    "    if type=='torch':\n",
    "        linear_predictor = torch.torch.tensor([0.67254923,\n",
    "        0.86077982,\n",
    "        0.43557393,\n",
    "        0.94059047,\n",
    "        0.8446509 ,\n",
    "        0.23657039,\n",
    "        0.74629685,\n",
    "        0.99700768,\n",
    "        0.28182768,\n",
    "        0.44495038], requires_grad=True)\n",
    "        y = torch.torch.tensor([1., -3., -3., -4., -7.,  8.,  9.,  -11.,  13.,  16.], requires_grad=True)\n",
    "        time = torch.torch.tensor([ 1,  3,  3,  4,  7,  8,  9, 11, 13, 16])\n",
    "        event = torch.torch.tensor([1, 0, 0, 0, 0, 1, 1, 0, 1, 1],dtype=torch.float32)\n",
    "        print('y shape', y.shape)\n",
    "        print('linear_predictor', linear_predictor.shape)\n",
    "\n",
    "    return y, linear_predictor, time, event"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Own Pytorch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y shape torch.Size([10])\n",
      "linear_predictor torch.Size([10])\n",
      "loss tensor(-1.5339864492, grad_fn=<MulBackward0>)\n",
      "grad_torch [ 0.06427045 -0.03018909 -0.01806675 -0.04240712 -0.05263754  0.10352549\n",
      "  0.0149176  -0.04637673  0.03045879 -0.02349511]\n"
     ]
    }
   ],
   "source": [
    "y, linear_predictor, time, event = aft_data(type='torch')\n",
    "aft_loss_own_torch = aft_likelihood_torch(linear_predictor,y)\n",
    "print('loss', aft_loss_own_torch)\n",
    "aft_loss_own_torch.backward()\n",
    "grad_torch = linear_predictor.grad.numpy()\n",
    "print('grad_torch', grad_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.06427046,  0.0301891 ,  0.01806675,  0.04240714,  0.05263756,\n",
       "       -0.1035255 , -0.01491763,  0.04637674, -0.03045881,  0.0234951 ])"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y, linear_predictor, time, event = aft_data(type='np')\n",
    "grad_own, hess_own = aft_objective(\n",
    "    y, linear_predictor\n",
    ")\n",
    "grad_own"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Function Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(-grad_own, grad_torch)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hessian Function Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.04928495, 0.03086066, 0.02215578, 0.02689906, 0.00218753,\n",
       "       0.01922084, 0.10613931, 0.04928495, 0.10754386, 0.07927256])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y, linear_predictor, time, event = aft_data(type='np')\n",
    "grad_own, hess_own = aft_objective(\n",
    "    y, linear_predictor\n",
    ")\n",
    "hess_own"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y shape torch.Size([10])\n",
      "linear_predictor torch.Size([10])\n",
      "torch.Size([10]) torch.Size([10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1198979765, -0.0308606531, -0.0221557729, -0.0268990565,\n",
       "        -0.0021875498, -0.0192208346, -0.1061393693,  0.0348668434,\n",
       "        -0.1075438932, -0.0792726427], grad_fn=<DiagBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.autograd.functional import hessian as hess_torch\n",
    "y, linear_predictor, time, event = aft_data(type='torch')\n",
    "print(y.shape, linear_predictor.shape)\n",
    "hessian_matrix = hess_torch(aft_likelihood_torch, (linear_predictor, y), create_graph=True)\n",
    "diag_hessian = hessian_matrix[0][0].diag()\n",
    "diag_hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False,  True,  True,  True,  True,  True,  True, False,  True,\n",
       "        True])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isclose(hess_own,-diag_hessian.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0., 0., 0., 0., 1., 1., 0., 1., 1.])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y shape torch.Size([10])\n",
      "linear_predictor torch.Size([10])\n",
      "torch.Size([10]) torch.Size([10])\n",
      "torch.Size([10]) torch.Size([10])\n",
      "aft\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (10x10 and 1x10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 106\u001b[0m\n\u001b[1;32m    104\u001b[0m y, linear_predictor, time, event \u001b[39m=\u001b[39m aft_data(\u001b[39mtype\u001b[39m\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtorch\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    105\u001b[0m \u001b[39mprint\u001b[39m(y\u001b[39m.\u001b[39mshape, linear_predictor\u001b[39m.\u001b[39mshape)\n\u001b[0;32m--> 106\u001b[0m hessian_matrix \u001b[39m=\u001b[39m hess_torch(eaftloss, (linear_predictor, y), create_graph\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    107\u001b[0m diag_hessian \u001b[39m=\u001b[39m hessian_matrix[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mdiag()\n\u001b[1;32m    108\u001b[0m diag_hessian\n",
      "File \u001b[0;32m~/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/torch/autograd/functional.py:808\u001b[0m, in \u001b[0;36mhessian\u001b[0;34m(func, inputs, create_graph, strict, vectorize, outer_jacobian_strategy)\u001b[0m\n\u001b[1;32m    805\u001b[0m     _check_requires_grad(jac, \u001b[39m\"\u001b[39m\u001b[39mjacobian\u001b[39m\u001b[39m\"\u001b[39m, strict\u001b[39m=\u001b[39mstrict)\n\u001b[1;32m    806\u001b[0m     \u001b[39mreturn\u001b[39;00m jac\n\u001b[0;32m--> 808\u001b[0m res \u001b[39m=\u001b[39m jacobian(jac_func, inputs, create_graph\u001b[39m=\u001b[39;49mcreate_graph, strict\u001b[39m=\u001b[39;49mstrict, vectorize\u001b[39m=\u001b[39;49mvectorize,\n\u001b[1;32m    809\u001b[0m                strategy\u001b[39m=\u001b[39;49mouter_jacobian_strategy)\n\u001b[1;32m    810\u001b[0m \u001b[39mreturn\u001b[39;00m _tuple_postprocess(res, (is_inputs_tuple, is_inputs_tuple))\n",
      "File \u001b[0;32m~/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/torch/autograd/functional.py:575\u001b[0m, in \u001b[0;36mjacobian\u001b[0;34m(func, inputs, create_graph, strict, vectorize, strategy)\u001b[0m\n\u001b[1;32m    572\u001b[0m is_inputs_tuple, inputs \u001b[39m=\u001b[39m _as_tuple(inputs, \u001b[39m\"\u001b[39m\u001b[39minputs\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mjacobian\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    573\u001b[0m inputs \u001b[39m=\u001b[39m _grad_preprocess(inputs, create_graph\u001b[39m=\u001b[39mcreate_graph, need_graph\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m--> 575\u001b[0m outputs \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49minputs)\n\u001b[1;32m    576\u001b[0m is_outputs_tuple, outputs \u001b[39m=\u001b[39m _as_tuple(outputs,\n\u001b[1;32m    577\u001b[0m                                       \u001b[39m\"\u001b[39m\u001b[39moutputs of the user-provided function\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    578\u001b[0m                                       \u001b[39m\"\u001b[39m\u001b[39mjacobian\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    579\u001b[0m _check_requires_grad(outputs, \u001b[39m\"\u001b[39m\u001b[39moutputs\u001b[39m\u001b[39m\"\u001b[39m, strict\u001b[39m=\u001b[39mstrict)\n",
      "File \u001b[0;32m~/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/torch/autograd/functional.py:804\u001b[0m, in \u001b[0;36mhessian.<locals>.jac_func\u001b[0;34m(*inp)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[39mif\u001b[39;00m outer_jacobian_strategy \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mforward-mode\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    801\u001b[0m     \u001b[39m# _grad_preprocess requires create_graph=True and input to require_grad\u001b[39;00m\n\u001b[1;32m    802\u001b[0m     \u001b[39m# or else the input will be detached\u001b[39;00m\n\u001b[1;32m    803\u001b[0m     inp \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(t\u001b[39m.\u001b[39mrequires_grad_(\u001b[39mTrue\u001b[39;00m) \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m inp)\n\u001b[0;32m--> 804\u001b[0m jac \u001b[39m=\u001b[39m jacobian(ensure_single_output_function, inp, create_graph\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    805\u001b[0m _check_requires_grad(jac, \u001b[39m\"\u001b[39m\u001b[39mjacobian\u001b[39m\u001b[39m\"\u001b[39m, strict\u001b[39m=\u001b[39mstrict)\n\u001b[1;32m    806\u001b[0m \u001b[39mreturn\u001b[39;00m jac\n",
      "File \u001b[0;32m~/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/torch/autograd/functional.py:575\u001b[0m, in \u001b[0;36mjacobian\u001b[0;34m(func, inputs, create_graph, strict, vectorize, strategy)\u001b[0m\n\u001b[1;32m    572\u001b[0m is_inputs_tuple, inputs \u001b[39m=\u001b[39m _as_tuple(inputs, \u001b[39m\"\u001b[39m\u001b[39minputs\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mjacobian\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    573\u001b[0m inputs \u001b[39m=\u001b[39m _grad_preprocess(inputs, create_graph\u001b[39m=\u001b[39mcreate_graph, need_graph\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m--> 575\u001b[0m outputs \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49minputs)\n\u001b[1;32m    576\u001b[0m is_outputs_tuple, outputs \u001b[39m=\u001b[39m _as_tuple(outputs,\n\u001b[1;32m    577\u001b[0m                                       \u001b[39m\"\u001b[39m\u001b[39moutputs of the user-provided function\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    578\u001b[0m                                       \u001b[39m\"\u001b[39m\u001b[39mjacobian\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    579\u001b[0m _check_requires_grad(outputs, \u001b[39m\"\u001b[39m\u001b[39moutputs\u001b[39m\u001b[39m\"\u001b[39m, strict\u001b[39m=\u001b[39mstrict)\n",
      "File \u001b[0;32m~/miniconda3/envs/xgbsurv/lib/python3.10/site-packages/torch/autograd/functional.py:787\u001b[0m, in \u001b[0;36mhessian.<locals>.ensure_single_output_function\u001b[0;34m(*inp)\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mensure_single_output_function\u001b[39m(\u001b[39m*\u001b[39minp):\n\u001b[0;32m--> 787\u001b[0m     out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49minp)\n\u001b[1;32m    788\u001b[0m     is_out_tuple, t_out \u001b[39m=\u001b[39m _as_tuple(out, \u001b[39m\"\u001b[39m\u001b[39moutputs of the user-provided function\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mhessian\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    789\u001b[0m     _check_requires_grad(t_out, \u001b[39m\"\u001b[39m\u001b[39moutputs\u001b[39m\u001b[39m\"\u001b[39m, strict\u001b[39m=\u001b[39mstrict)\n",
      "Cell \u001b[0;32mIn[7], line 37\u001b[0m, in \u001b[0;36meaftloss\u001b[0;34m(out, y)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[39m# Rj - Ri\u001b[39;00m\n\u001b[1;32m     36\u001b[0m rawones \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mones([\u001b[39m1\u001b[39m,n], dtype \u001b[39m=\u001b[39m out\u001b[39m.\u001b[39mdtype)\n\u001b[0;32m---> 37\u001b[0m R1 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mmm(R,rawones)\n\u001b[1;32m     38\u001b[0m R2 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmm(torch\u001b[39m.\u001b[39mt(rawones),torch\u001b[39m.\u001b[39mt(R))\n\u001b[1;32m     39\u001b[0m DR \u001b[39m=\u001b[39m R1 \u001b[39m-\u001b[39m R2 \n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (10x10 and 1x10)"
     ]
    }
   ],
   "source": [
    "def transform_back_torch(y: torch.torch.tensor) -> tuple[torch.torch.tensor, torch.torch.tensor]:\n",
    "    \"\"\"Transforms XGBoost digestable format variable y into time and event.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y : npt.NDArray[float]\n",
    "        Array containing survival time and event where negative value is taken as censored event.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple[npt.NDArray[float],npt.NDArray[int]]\n",
    "        Survival time and event.\n",
    "    \"\"\"\n",
    "    time = torch.abs(y)\n",
    "    event = (torch.abs(y) == y)\n",
    "    event = event # for numba\n",
    "    return time, event\n",
    "\n",
    "def eaftloss(out, y): ##loss function for AFT or EH\n",
    "    time, delta = transform_back_torch(y)\n",
    "    print(time.shape, delta.shape)\n",
    "    delta = delta.float()\n",
    "    time = time.float()\n",
    "    ###loss function for AFT\n",
    "    n = len(delta)\n",
    "    print('aft')\n",
    "    h = 1.30*math.pow(n,-0.2)\n",
    "    #h 1.304058*math.pow(n,-0.2)  ## 1.304058*n^(-1/5) or 1.587401*math.pow(n,-0.333333) 1.587401*n^(-1/3)\n",
    "    time = time.view(n,1)\n",
    "    delta = delta.view(n,1)\n",
    "    \n",
    "    # R = g(Xi) + log(Oi)\n",
    "    R = torch.add(out,torch.log(time)) \n",
    "    \n",
    "    # Rj - Ri\n",
    "    rawones = torch.ones([1,n], dtype = out.dtype)\n",
    "    R1 = torch.mm(R,rawones)\n",
    "    R2 = torch.mm(torch.t(rawones),torch.t(R))\n",
    "    DR = R1 - R2 \n",
    "    \n",
    "    # K[(Rj-Ri)/h]\n",
    "    K = normal_density(DR/h)\n",
    "    Del = torch.mm(delta, rawones)\n",
    "    DelK = Del*K \n",
    "    \n",
    "    # (1/nh) *sum_j Deltaj * K[(Rj-Ri)/h]\n",
    "    Dk = torch.sum(DelK, dim=0)/(n*h)\n",
    "    \n",
    "    # log {(1/nh) * Deltaj * K[(Rj-Ri)/h]}    \n",
    "    log_Dk = torch.log(Dk)     \n",
    "    A = torch.t(delta)*log_Dk/n   \n",
    "    S1 = A.sum()  \n",
    "    \n",
    "    ncdf=torch.distributions.normal.Normal(torch.torch.tensor([0.0], dtype = out.dtype), torch.torch.tensor([1.0], dtype = out.dtype)).cdf\n",
    "    P = ncdf(DR/h)\n",
    "    CDF_sum = torch.sum(P, dim=0)/n\n",
    "    Q = torch.log(CDF_sum)\n",
    "    S2 = -(delta*Q.view(n,1)).sum()/n\n",
    "            \n",
    "    S0 = -(delta*torch.log(time)).sum()/n\n",
    "    \n",
    "    S = S0 + S1 + S2 \n",
    "    S = -S\n",
    "    return S\n",
    "\n",
    "def normal_density(a):  \n",
    "    b = 0.3989423*torch.exp(-0.5*torch.pow(a,2.0))\n",
    "    return b\n",
    "def aft_data(type='np'):\n",
    "    #h2==0 scenario\n",
    "    if type=='np':\n",
    "        linear_predictor = np.array([0.67254923,\n",
    "        0.86077982,\n",
    "        0.43557393,\n",
    "        0.94059047,\n",
    "        0.8446509 ,\n",
    "        0.23657039,\n",
    "        0.74629685,\n",
    "        0.99700768,\n",
    "        0.28182768,\n",
    "        0.44495038]) #.reshape(1,10)\n",
    "        y = np.array([1, -3, -3, -4, -7,  8,  9,  -11,  13,  16],dtype=np.float32) #.reshape(1,10)\n",
    "        time = np.array([[ 1,  3,  3,  4,  7,  8,  9, 11, 13, 16]])\n",
    "        event = np.array([[1, 0, 0, 0, 0, 1, 1, 0, 1, 1]],dtype=np.float32)\n",
    "    if type=='torch':\n",
    "        linear_predictor = torch.torch.tensor([0.67254923,\n",
    "        0.86077982,\n",
    "        0.43557393,\n",
    "        0.94059047,\n",
    "        0.8446509 ,\n",
    "        0.23657039,\n",
    "        0.74629685,\n",
    "        0.99700768,\n",
    "        0.28182768,\n",
    "        0.44495038], requires_grad=True)\n",
    "        y = torch.torch.tensor([1., -3., -3., -4., -7.,  8.,  9.,  -11.,  13.,  16.], requires_grad=True)\n",
    "        time = torch.torch.tensor([ 1,  3,  3,  4,  7,  8,  9, 11, 13, 16])\n",
    "        event = torch.torch.tensor([1, 0, 0, 0, 0, 1, 1, 0, 1, 1],dtype=torch.float32)\n",
    "        print('y shape', y.shape)\n",
    "        print('linear_predictor', linear_predictor.shape)\n",
    "\n",
    "    return y, linear_predictor, time, event\n",
    "from torch.autograd.functional import hessian as hess_torch\n",
    "y, linear_predictor, time, event = aft_data(type='torch')\n",
    "print(y.shape, linear_predictor.shape)\n",
    "hessian_matrix = hess_torch(eaftloss, (linear_predictor, y), create_graph=True)\n",
    "diag_hessian = hessian_matrix[0][0].diag()\n",
    "diag_hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xgbsurv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
