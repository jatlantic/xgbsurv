{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycox.models.loss import rank_loss_deephit_single\n",
    "#from pycox.models.data import pair_rank_mat\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import random\n",
    "from pycox.models import utils\n",
    "from pycox.models.loss import _rank_loss_deephit,  _diff_cdf_at_time_i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _reduction(loss: Tensor, reduction: str = 'mean') -> Tensor:\n",
    "    if reduction == 'none':\n",
    "        return loss\n",
    "    elif reduction == 'mean':\n",
    "        return loss.mean()\n",
    "    elif reduction == 'sum':\n",
    "        return loss.sum()\n",
    "    raise ValueError(f\"`reduction` = {reduction} is not valid. Use 'none', 'mean' or 'sum'.\")\n",
    "    \n",
    "def _pair_rank_mat(mat, idx_durations, events, dtype='float32'):\n",
    "    n = len(idx_durations)\n",
    "    for i in range(n):\n",
    "        dur_i = idx_durations[i]\n",
    "        ev_i = events[i]\n",
    "        if ev_i == 0:\n",
    "            continue\n",
    "        for j in range(n):\n",
    "            dur_j = idx_durations[j]\n",
    "            ev_j = events[j]\n",
    "            if (dur_i < dur_j) or ((dur_i == dur_j) and (ev_j == 0)):\n",
    "                mat[i, j] = 1\n",
    "    return mat\n",
    "\n",
    "def pair_rank_mat(idx_durations, events, dtype='float32'):\n",
    "    \"\"\"Indicator matrix R with R_ij = 1{T_i < T_j and D_i = 1}.\n",
    "    So it takes value 1 if we observe that i has an event before j and zero otherwise.\n",
    "    \n",
    "    Arguments:\n",
    "        idx_durations {np.array} -- Array with durations.\n",
    "        events {np.array} -- Array with event indicators.\n",
    "    \n",
    "    Keyword Arguments:\n",
    "        dtype {str} -- dtype of array (default: {'float32'})\n",
    "    \n",
    "    Returns:\n",
    "        np.array -- n x n matrix indicating if i has an observerd event before j.\n",
    "    \"\"\"\n",
    "    idx_durations = idx_durations.reshape(-1)\n",
    "    events = events.reshape(-1)\n",
    "    n = len(idx_durations)\n",
    "    mat = np.zeros((n, n), dtype=dtype)\n",
    "    mat = _pair_rank_mat(mat, idx_durations, events, dtype)\n",
    "    return mat\n",
    "\n",
    "def rank_loss_deephit_single(phi: Tensor, idx_durations: Tensor, events: Tensor, rank_mat: Tensor,\n",
    "                             sigma: Tensor, reduction: str = 'mean') -> Tensor:\n",
    "    \n",
    "    idx_durations = idx_durations.view(-1, 1)\n",
    "    # events = events.float().view(-1)\n",
    "    pmf = utils.pad_col(phi) #.softmax(1)\n",
    "    #print('softmax pmf', pmf)\n",
    "    y = torch.zeros_like(pmf).scatter(1, idx_durations, 1.) # one-hot\n",
    "    print('y', y)\n",
    "    rank_loss = _rank_loss_deephit(pmf, y, rank_mat, sigma, reduction)\n",
    "    return rank_loss\n",
    "\n",
    "def _rank_loss_deephit(pmf: Tensor, y: Tensor, rank_mat: Tensor, sigma: float,\n",
    "                       reduction: str = 'mean') -> Tensor:\n",
    "    \"\"\"Ranking loss from DeepHit.\n",
    "    \n",
    "    Arguments:\n",
    "        pmf {torch.tensor} -- Matrix with probability mass function pmf_ij = f_i(t_j)\n",
    "        y {torch.tensor} -- Matrix with indicator of duration and censoring time. \n",
    "        rank_mat {torch.tensor} -- See pair_rank_mat function.\n",
    "        sigma {float} -- Sigma from DeepHit paper, chosen by you.\n",
    "    \n",
    "    Returns:\n",
    "        torch.tensor -- loss\n",
    "    \"\"\"\n",
    "    r = _diff_cdf_at_time_i(pmf, y)\n",
    "    print('intermediary result', torch.exp(-r/sigma))\n",
    "    loss = rank_mat * torch.exp(-r/sigma)\n",
    "    loss = loss.mean(1, keepdim=True)\n",
    "    return _reduction(loss, reduction)\n",
    "\n",
    "def _diff_cdf_at_time_i(pmf: Tensor, y: Tensor) -> Tensor:\n",
    "    \"\"\"R is the matrix from the DeepHit code giving the difference in CDF between individual\n",
    "    i and j, at the event time of j. \n",
    "    I.e: R_ij = F_i(T_i) - F_j(T_i)\n",
    "    \n",
    "    Arguments:\n",
    "        pmf {torch.tensor} -- Matrix with probability mass function pmf_ij = f_i(t_j)\n",
    "        y {torch.tensor} -- Matrix with indicator of duration/censor time.\n",
    "    \n",
    "    Returns:\n",
    "        torch.tensor -- R_ij = F_i(T_i) - F_j(T_i)\n",
    "    \"\"\"\n",
    "    n = pmf.shape[0]\n",
    "    ones = torch.ones((n, 1), device=pmf.device)\n",
    "    r = pmf.cumsum(1).matmul(y.transpose(0, 1))\n",
    "    diag_r = r.diag().view(1, -1)\n",
    "    r = ones.matmul(diag_r) - r\n",
    "    #print('r',r)\n",
    "    return r.transpose(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y tensor([[1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]])\n",
      "intermediary result tensor([[1.0000, 7.3891, 2.7183],\n",
      "        [0.0498, 1.0000, 0.1353],\n",
      "        [0.3679, 7.3891, 1.0000]], grad_fn=<ExpBackward0>)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39m#torch.from_numpy(pair_rank_mat(idx_durations, events))\u001b[39;00m\n\u001b[1;32m     16\u001b[0m sigma \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([\u001b[39m1.0\u001b[39m])\n\u001b[0;32m---> 17\u001b[0m var \u001b[39m=\u001b[39m rank_loss_deephit_single(phi, idx_durations, events, rank_mat, sigma, \u001b[39m'\u001b[39m\u001b[39msum\u001b[39m\u001b[39m'\u001b[39m) \n\u001b[1;32m     18\u001b[0m \u001b[39mprint\u001b[39m(var)\n\u001b[1;32m     19\u001b[0m var\u001b[39m.\u001b[39mbackward()\n",
      "Cell \u001b[0;32mIn[2], line 54\u001b[0m, in \u001b[0;36mrank_loss_deephit_single\u001b[0;34m(phi, idx_durations, events, rank_mat, sigma, reduction)\u001b[0m\n\u001b[1;32m     52\u001b[0m y \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros_like(pmf)\u001b[39m.\u001b[39mscatter(\u001b[39m1\u001b[39m, idx_durations, \u001b[39m1.\u001b[39m) \u001b[39m# one-hot\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39my\u001b[39m\u001b[39m'\u001b[39m, y)\n\u001b[0;32m---> 54\u001b[0m rank_loss \u001b[39m=\u001b[39m _rank_loss_deephit(pmf, y, rank_mat, sigma, reduction)\n\u001b[1;32m     55\u001b[0m \u001b[39mreturn\u001b[39;00m rank_loss\n",
      "Cell \u001b[0;32mIn[2], line 72\u001b[0m, in \u001b[0;36m_rank_loss_deephit\u001b[0;34m(pmf, y, rank_mat, sigma, reduction)\u001b[0m\n\u001b[1;32m     70\u001b[0m r \u001b[39m=\u001b[39m _diff_cdf_at_time_i(pmf, y)\n\u001b[1;32m     71\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mintermediary result\u001b[39m\u001b[39m'\u001b[39m, torch\u001b[39m.\u001b[39mexp(\u001b[39m-\u001b[39mr\u001b[39m/\u001b[39msigma))\n\u001b[0;32m---> 72\u001b[0m loss \u001b[39m=\u001b[39m rank_mat \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39mexp(\u001b[39m-\u001b[39mr\u001b[39m/\u001b[39msigma)\n\u001b[1;32m     73\u001b[0m loss \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mmean(\u001b[39m1\u001b[39m, keepdim\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     74\u001b[0m \u001b[39mreturn\u001b[39;00m _reduction(loss, reduction)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "# let's do easy to follow example\n",
    "torch.manual_seed(42)\n",
    "phi = torch.tensor([\n",
    "              [1.0, 2.0],\n",
    "              [3.0, 3.0],\n",
    "              [2.0, 2.0]], requires_grad=True)\n",
    "#phi = torch.rand(6, 4,requires_grad=True) * 2 - 1\n",
    "survival_times = torch.tensor([5, 10, 10])\n",
    "events = torch.tensor([1, 1, 1])\n",
    "idx_durations = torch.tensor([0,1,1])\n",
    "rank_mat = torch.tensor([\n",
    "              [1.0, 1.0],\n",
    "              [1.0, 1.0],\n",
    "              [1.0, 1.0]])\n",
    "#torch.from_numpy(pair_rank_mat(idx_durations, events))\n",
    "sigma = torch.tensor([1.0])\n",
    "var = rank_loss_deephit_single(phi, idx_durations, events, rank_mat, sigma, 'sum') \n",
    "print(var)\n",
    "var.backward()\n",
    "phi.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _diff_cdf_at_time_i(pmf: Tensor, y: Tensor) -> Tensor:\n",
    "    \"\"\"R is the matrix from the DeepHit code giving the difference in CDF between individual\n",
    "    i and j, at the event time of j. \n",
    "    I.e: R_ij = F_i(T_i) - F_j(T_i)\n",
    "    \n",
    "    Arguments:\n",
    "        pmf {torch.tensor} -- Matrix with probability mass function pmf_ij = f_i(t_j)\n",
    "        y {torch.tensor} -- Matrix with indicator of duration/censor time.\n",
    "    \n",
    "    Returns:\n",
    "        torch.tensor -- R_ij = F_i(T_i) - F_j(T_i)\n",
    "    \"\"\"\n",
    "    n = pmf.shape[0]\n",
    "    ones = torch.ones((n, 1), device=pmf.device)\n",
    "    #print('ones',ones)\n",
    "    #print('y.transpose(0, 1)',y.transpose(0, 1))\n",
    "    #print('pmf.cumsum(1)',pmf.cumsum(1))\n",
    "    r = pmf.cumsum(1).matmul(y.transpose(0, 1))\n",
    "    #print('diagonal r',r)\n",
    "    diag_r = r.diag().view(1, -1)\n",
    "    #print('diag_r', diag_r)\n",
    "    #print('ones.matmul(diag_r)',ones.matmul(diag_r))\n",
    "    r = ones.matmul(diag_r) - r\n",
    "    #print('r fin',r)\n",
    "    #print('r fin transpose',r.transpose(0, 1))\n",
    "    #print('sum',r.transpose(0, 1).sum())\n",
    "    return r #.transpose(0, 1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diagonal r tensor([[1., 3., 3.],\n",
      "        [3., 6., 6.],\n",
      "        [2., 4., 4.]], grad_fn=<MmBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  3.,  1.],\n",
       "        [-2.,  0., -2.],\n",
       "        [-1.,  2.,  0.]], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phi = torch.tensor([\n",
    "              [1.0, 2.0],\n",
    "              [3.0, 3.0],\n",
    "              [2.0, 2.0]], requires_grad=True)\n",
    "y = torch.tensor([ # has to be fully dimensional for all scenarios\n",
    "        [1., 0., 0.],\n",
    "        [0., 1., 0.],\n",
    "        [0., 1., 0.]])\n",
    "pmf = utils.pad_col(phi)\n",
    "_diff_cdf_at_time_i(pmf, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Tensor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_diff_cdf_at_time_i_torch\u001b[39m(pmf: Tensor, y: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m      2\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"R is the matrix from the DeepHit code giving the difference in CDF between individual\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m    i and j, at the event time of j. \u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39m    I.e: R_ij = F_i(T_i) - F_j(T_i)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39m        torch.tensor -- R_ij = F_i(T_i) - F_j(T_i)\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     n \u001b[39m=\u001b[39m pmf\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Tensor' is not defined"
     ]
    }
   ],
   "source": [
    "def _diff_cdf_at_time_i_torch(pmf: Tensor, y: Tensor) -> Tensor:\n",
    "    \"\"\"R is the matrix from the DeepHit code giving the difference in CDF between individual\n",
    "    i and j, at the event time of j. \n",
    "    I.e: R_ij = F_i(T_i) - F_j(T_i)\n",
    "    \n",
    "    Arguments:\n",
    "        pmf {torch.tensor} -- Matrix with probability mass function pmf_ij = f_i(t_j)\n",
    "        y {torch.tensor} -- Matrix with indicator of duration/censor time.\n",
    "    \n",
    "    Returns:\n",
    "        torch.tensor -- R_ij = F_i(T_i) - F_j(T_i)\n",
    "    \"\"\"\n",
    "    n = pmf.shape[0]\n",
    "    ones = torch.ones((n, 1), device=pmf.device)\n",
    "    #print('ones',ones)\n",
    "    print('y.transpose(0, 1)',y.transpose(0, 1))\n",
    "    print('pmf.cumsum(1)',pmf.cumsum(1))\n",
    "    r = pmf.cumsum(1).matmul(y.transpose(0, 1))\n",
    "    print('r',r)\n",
    "    #print('diagonal r',r)\n",
    "    diag_r = r.diag().view(1, -1)\n",
    "    #print('diag_r', diag_r)\n",
    "    #print('ones.matmul(diag_r)',ones.matmul(diag_r))\n",
    "    r = ones.matmul(diag_r) - r\n",
    "    #print('r fin',r)\n",
    "    #print('r fin transpose',r.transpose(0, 1))\n",
    "    #print('sum',r.transpose(0, 1).sum())\n",
    "    return r.transpose(0, 1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '_diff_cdf_at_time_i_torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 10\u001b[0m\n\u001b[1;32m      5\u001b[0m y \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([ \u001b[39m# has to be fully dimensional for all scenarios\u001b[39;00m\n\u001b[1;32m      6\u001b[0m         [\u001b[39m1.\u001b[39m, \u001b[39m0.\u001b[39m, \u001b[39m0.\u001b[39m],\n\u001b[1;32m      7\u001b[0m         [\u001b[39m0.\u001b[39m, \u001b[39m1.\u001b[39m, \u001b[39m0.\u001b[39m],\n\u001b[1;32m      8\u001b[0m         [\u001b[39m0.\u001b[39m, \u001b[39m1.\u001b[39m, \u001b[39m0.\u001b[39m]])\n\u001b[1;32m      9\u001b[0m pmf \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39mpad_col(phi)\n\u001b[0;32m---> 10\u001b[0m var \u001b[39m=\u001b[39m _diff_cdf_at_time_i_torch(pmf, y)\n\u001b[1;32m     11\u001b[0m \u001b[39mprint\u001b[39m(var)\n\u001b[1;32m     12\u001b[0m var\u001b[39m.\u001b[39mbackward()\n",
      "\u001b[0;31mNameError\u001b[0m: name '_diff_cdf_at_time_i_torch' is not defined"
     ]
    }
   ],
   "source": [
    "phi = torch.tensor([\n",
    "              [1.0, 2.0],\n",
    "              [3.0, 3.0],\n",
    "              [2.0, 2.0]], requires_grad=True)\n",
    "y = torch.tensor([ # has to be fully dimensional for all scenarios\n",
    "        [1., 0., 0.],\n",
    "        [0., 1., 0.],\n",
    "        [0., 1., 0.]])\n",
    "pmf = utils.pad_col(phi)\n",
    "var = _diff_cdf_at_time_i_torch(pmf, y)\n",
    "print(var)\n",
    "var.backward()\n",
    "phi.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _diff_cdf_at_time_i_deriv(pmf: Tensor, y: Tensor) -> Tensor:\n",
    "    \"\"\"R is the matrix from the DeepHit code giving the difference in CDF between individual\n",
    "    i and j, at the event time of j. \n",
    "    I.e: R_ij = F_i(T_i) - F_j(T_i)\n",
    "    \n",
    "    Arguments:\n",
    "        pmf {torch.tensor} -- Matrix with probability mass function pmf_ij = f_i(t_j)\n",
    "        y {torch.tensor} -- Matrix with indicator of duration/censor time.\n",
    "    \n",
    "    Returns:\n",
    "        torch.tensor -- R_ij = F_i(T_i) - F_j(T_i)\n",
    "    \"\"\"\n",
    "    n = pmf.shape[0]\n",
    "    ones = torch.ones((n, 1), device=pmf.device)\n",
    "    #print('ones',ones)\n",
    "    print('y.transpose(0, 1)',y.transpose(0, 1))\n",
    "    print('pmf.cumsum(1)',pmf.cumsum(1))\n",
    "    r = pmf.cumsum(1).matmul(y.transpose(0, 1))\n",
    "    print('r',r)\n",
    "    diag_r = r.diag().view(1, -1)\n",
    "    #print('diag_r', diag_r)\n",
    "    print('ones.matmul(diag_r)',ones.matmul(diag_r))\n",
    "    r = ones.matmul(diag_r) - r\n",
    "    #print('r fin',r)\n",
    "    #print('r fin transpose',r.transpose(0, 1))\n",
    "    #print('sum',r.transpose(0, 1).sum())\n",
    "    # change dims\n",
    "    return (r-r.sum(axis=0))+(r-r.sum(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y.transpose(0, 1) tensor([[1., 0., 0.],\n",
      "        [0., 1., 1.],\n",
      "        [0., 0., 0.]])\n",
      "pmf.cumsum(1) tensor([[1., 3., 3.],\n",
      "        [3., 6., 6.],\n",
      "        [2., 4., 4.]], grad_fn=<CumsumBackward0>)\n",
      "r tensor([[1., 3., 3.],\n",
      "        [3., 6., 6.],\n",
      "        [2., 4., 4.]], grad_fn=<MmBackward0>)\n",
      "ones.matmul(diag_r) tensor([[1., 6., 4.],\n",
      "        [1., 6., 4.],\n",
      "        [1., 6., 4.]], grad_fn=<MmBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-1.,  5.,  2.],\n",
       "        [-5., -1., -4.],\n",
       "        [-3.,  3.,  0.]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phi = torch.tensor([\n",
    "              [1.0, 2.0],\n",
    "              [3.0, 3.0],\n",
    "              [2.0, 2.0]], requires_grad=True)\n",
    "y = torch.tensor([ # has to be fully dimensional for all scenarios\n",
    "        [1., 0., 0.],\n",
    "        [0., 1., 0.],\n",
    "        [0., 1., 0.]])\n",
    "pmf = utils.pad_col(phi)\n",
    "_diff_cdf_at_time_i_deriv(pmf, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Tensor' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[84], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m var \u001b[39m=\u001b[39m cums(phi)\n\u001b[1;32m     10\u001b[0m var\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m---> 11\u001b[0m phi\u001b[39m.\u001b[39;49mgrad()\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Tensor' object is not callable"
     ]
    }
   ],
   "source": [
    "def cums(phi):\n",
    "    return phi.cumsum(axis=1).sum()\n",
    "\n",
    "\n",
    "phi = torch.tensor([\n",
    "              [1.0, 2.0],\n",
    "              [3.0, 3.0],\n",
    "              [2.0, 2.0]], requires_grad=True)\n",
    "var = cums(phi)\n",
    "var.backward()\n",
    "phi.grad()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'to_numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m     mat \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(mat)\n\u001b[1;32m     19\u001b[0m     \u001b[39mreturn\u001b[39;00m mat\n\u001b[0;32m---> 21\u001b[0m test(x\u001b[39m=\u001b[39mpmf\u001b[39m.\u001b[39;49mto_numpy())\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'to_numpy'"
     ]
    }
   ],
   "source": [
    "def test(x):\n",
    "    \n",
    "    x = np.cumsum(x, axis=1)\n",
    "    s = 0\n",
    "    mat = []\n",
    "    #mat2 = jnp.zeros((3,3))\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            if i<j:\n",
    "                #print('i,j',i,j)\n",
    "                #print(x[i,i] - x[i,j])\n",
    "                diff = x[i,i] - x[i,j]\n",
    "                #mat2[i,j] = diff\n",
    "                #print(diff)\n",
    "                #s = s + diff\n",
    "                mat.append(diff)\n",
    "\n",
    "    mat = np.array(mat)\n",
    "    return mat\n",
    "\n",
    "phi = np.array([\n",
    "              [1.0, 2.0],\n",
    "              [3.0, 3.0],\n",
    "              [2.0, 2.0]], requires_grad=True)\n",
    "y = np.array([ # has to be fully dimensional for all scenarios\n",
    "        [1., 0., 0.],\n",
    "        [0., 1., 0.],\n",
    "        [0., 1., 0.]])\n",
    "test(x=pmf.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pmf tensor([[1., 2., 0.],\n",
      "        [3., 3., 0.],\n",
      "        [2., 2., 0.]], grad_fn=<CatBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0., -2.],\n",
       "        [ 0.,  1.],\n",
       "        [ 0.,  1.]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phi = torch.tensor([\n",
    "              [1.0, 2.0],\n",
    "              [3.0, 3.0],\n",
    "              [2.0, 2.0]], requires_grad=True)\n",
    "y = torch.tensor([ # has to be fully dimensional for all scenarios\n",
    "        [1., 0., 0.],\n",
    "        [0., 1., 0.],\n",
    "        [0., 1., 0.]])\n",
    "pmf = utils.pad_col(phi)\n",
    "print('pmf', pmf)\n",
    "var = _diff_cdf_at_time_i(pmf, y)\n",
    "var.backward()\n",
    "phi.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mat tensor(1., grad_fn=<SumBackward0>)\n",
      "gradient tensor([[ 0., -2.],\n",
      "        [ 0.,  1.],\n",
      "        [ 0.,  1.]])\n"
     ]
    }
   ],
   "source": [
    "phi = torch.tensor([\n",
    "              [1.0, 2.0],\n",
    "              [3.0, 3.0],\n",
    "              [2.0, 2.0]], requires_grad=True)\n",
    "pmf = utils.pad_col(phi)\n",
    "y = torch.tensor([\n",
    "        [1., 0., 0.],\n",
    "        [0., 1., 0.],\n",
    "        [0., 1., 0.]])\n",
    "var = _diff_cdf_at_time_i(pmf, y)\n",
    "print('mat', _diff_cdf_at_time_i(pmf, y))\n",
    "var.backward()\n",
    "print('gradient',phi.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.softmax(utils.pad_col(phi), axis=1) # correct\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mat -4.0\n",
      "sum -4.0\n",
      "mat Traced<ConcreteArray(-4.0, dtype=float32)>with<JVPTrace(level=2/0)> with\n",
      "  primal = Array(-4., dtype=float32)\n",
      "  tangent = Traced<ShapedArray(float32[])>with<JaxprTrace(level=1/0)> with\n",
      "    pval = (ShapedArray(float32[]), None)\n",
      "    recipe = JaxprEqnRecipe(eqn_id=<object object at 0x2c52a7500>, in_tracers=(Traced<ShapedArray(float32[3]):JaxprTrace(level=1/0)>,), out_tracer_refs=[<weakref at 0x2aaf912b0; to 'JaxprTracer' at 0x2aaf90a90>], out_avals=[ShapedArray(float32[])], primitive=pjit, params={'jaxpr': { lambda ; a:f32[3]. let b:f32[] = reduce_sum[axes=(0,)] a in (b,) }, 'in_shardings': (UnspecifiedValue,), 'out_shardings': (UnspecifiedValue,), 'resource_env': None, 'donated_invars': (False,), 'name': '_reduce_sum', 'keep_unused': False, 'inline': True}, effects=set(), source_info=SourceInfo(traceback=<jaxlib.xla_extension.Traceback object at 0x2a9615e30>, name_stack=NameStack(stack=(Transform(name='jvp'),))))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Array([[ 0., -2., -1.],\n",
       "       [ 0.,  0., -1.],\n",
       "       [ 0.,  0.,  0.]], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now calculate above manually\n",
    "\n",
    "import jax.numpy as jnp\n",
    "from jax import grad\n",
    "\n",
    "def test(x):\n",
    "    \n",
    "    x = jnp.cumsum(x, axis=1)\n",
    "    s = 0\n",
    "    mat = []\n",
    "    #mat2 = jnp.zeros((3,3))\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            if i<j:\n",
    "                #print('i,j',i,j)\n",
    "                #print(x[i,i] - x[i,j])\n",
    "                diff = x[i,i] - x[i,j]\n",
    "                #mat2[i,j] = diff\n",
    "                #print(diff)\n",
    "                #s = s + diff\n",
    "                mat.append(diff)\n",
    "\n",
    "    mat = jnp.array(mat)\n",
    "    print('mat',jnp.sum(mat))\n",
    "    return jnp.sum(mat)\n",
    "\n",
    "\n",
    "x = jnp.array([[1.0, 2.0, 0.0],\n",
    "              [3.0, 3.0, 0.0],\n",
    "              [2.0, 2.0, 0.0]])\n",
    "print('sum',test(x))\n",
    "grad(test)(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m rank_mat \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(pair_rank_mat(idx_durations, events))\n\u001b[1;32m     13\u001b[0m sigma \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([\u001b[39m1.0\u001b[39m])\n\u001b[0;32m---> 14\u001b[0m var\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     15\u001b[0m phi\u001b[39m.\u001b[39mgrad\n\u001b[1;32m     17\u001b[0m phi\n",
      "File \u001b[0;32m~/miniconda3/envs/xgbsurv/lib/python3.11/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/xgbsurv/lib/python3.11/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "phi = torch.tensor([[ 0.7645,  0.8300, 0.2343,  0.9186],\n",
    "        [0.2191,  0.2018, 0.4869,  0.5873],\n",
    "        [ 0.8815, 0.7336,  0.8692,  0.1872],\n",
    "        [ 0.7388,  0.1354,  0.4822, 0.1412],\n",
    "        [ 0.7709,  0.1478, 0.4668,  0.2549],\n",
    "        [0.4607, 0.1173, 0.4062,  0.6634]],requires_grad=True)\n",
    "#phi = torch.rand(6, 4,requires_grad=True) * 2 - 1\n",
    "survival_times = torch.tensor([5, 10, 10, 20, 20, 30])\n",
    "events = torch.tensor([1, 1, 1, 1, 1, 1])\n",
    "idx_durations = torch.tensor([0,1,1,2,2,3])\n",
    "rank_mat = torch.from_numpy(pair_rank_mat(idx_durations, events))\n",
    "sigma = torch.tensor([1.0])\n",
    "var.backward()\n",
    "phi.grad\n",
    "\n",
    "phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y tensor([[1., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0.]])\n",
      "ones tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]])\n",
      "y.transpose(0, 1) tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n",
      "pmf.cumsum(1) tensor([[0.7645, 1.5945, 1.8288, 2.7474, 2.7474],\n",
      "        [0.2191, 0.4209, 0.9078, 1.4951, 1.4951],\n",
      "        [0.8815, 1.6151, 2.4843, 2.6715, 2.6715],\n",
      "        [0.7388, 0.8742, 1.3564, 1.4976, 1.4976],\n",
      "        [0.7709, 0.9187, 1.3855, 1.6404, 1.6404],\n",
      "        [0.4607, 0.5780, 0.9842, 1.6476, 1.6476]], grad_fn=<CumsumBackward0>)\n",
      "diagonal r tensor([[0.7645, 1.5945, 1.5945, 1.8288, 1.8288, 2.7474],\n",
      "        [0.2191, 0.4209, 0.4209, 0.9078, 0.9078, 1.4951],\n",
      "        [0.8815, 1.6151, 1.6151, 2.4843, 2.4843, 2.6715],\n",
      "        [0.7388, 0.8742, 0.8742, 1.3564, 1.3564, 1.4976],\n",
      "        [0.7709, 0.9187, 0.9187, 1.3855, 1.3855, 1.6404],\n",
      "        [0.4607, 0.5780, 0.5780, 0.9842, 0.9842, 1.6476]],\n",
      "       grad_fn=<MmBackward0>)\n",
      "diag_r tensor([[0.7645, 0.4209, 1.6151, 1.3564, 1.3855, 1.6476]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "ones.matmul(diag_r) tensor([[0.7645, 0.4209, 1.6151, 1.3564, 1.3855, 1.6476],\n",
      "        [0.7645, 0.4209, 1.6151, 1.3564, 1.3855, 1.6476],\n",
      "        [0.7645, 0.4209, 1.6151, 1.3564, 1.3855, 1.6476],\n",
      "        [0.7645, 0.4209, 1.6151, 1.3564, 1.3855, 1.6476],\n",
      "        [0.7645, 0.4209, 1.6151, 1.3564, 1.3855, 1.6476],\n",
      "        [0.7645, 0.4209, 1.6151, 1.3564, 1.3855, 1.6476]],\n",
      "       grad_fn=<MmBackward0>)\n",
      "r fin tensor([[ 0.0000, -1.1736,  0.0206, -0.4724, -0.4433, -1.0998],\n",
      "        [ 0.5454,  0.0000,  1.1942,  0.4486,  0.4777,  0.1525],\n",
      "        [-0.1170, -1.1942,  0.0000, -1.1279, -1.0988, -1.0239],\n",
      "        [ 0.0257, -0.4533,  0.7409,  0.0000,  0.0291,  0.1500],\n",
      "        [-0.0064, -0.4978,  0.6964, -0.0291,  0.0000,  0.0072],\n",
      "        [ 0.3038, -0.1571,  1.0371,  0.3722,  0.4013,  0.0000]],\n",
      "       grad_fn=<SubBackward0>)\n",
      "r fin transpose tensor([[ 0.0000,  0.5454, -0.1170,  0.0257, -0.0064,  0.3038],\n",
      "        [-1.1736,  0.0000, -1.1942, -0.4533, -0.4978, -0.1571],\n",
      "        [ 0.0206,  1.1942,  0.0000,  0.7409,  0.6964,  1.0371],\n",
      "        [-0.4724,  0.4486, -1.1279,  0.0000, -0.0291,  0.3722],\n",
      "        [-0.4433,  0.4777, -1.0988,  0.0291,  0.0000,  0.4013],\n",
      "        [-1.0998,  0.1525, -1.0239,  0.1500,  0.0072,  0.0000]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "sum tensor(-2.2919, grad_fn=<SumBackward0>)\n",
      "intermediary result tensor([9.8937], grad_fn=<ExpBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(21.4364, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var = rank_loss_deephit_single(phi, idx_durations, events, rank_mat, sigma, 'sum') \n",
    "var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5.7220e-06,  1.0718e+02,  6.4309e+01,  2.1436e+01],\n",
       "        [ 7.6294e-06, -2.1436e+01,  6.4309e+01,  2.1436e+01],\n",
       "        [ 7.6294e-06, -2.1436e+01,  6.4309e+01,  2.1436e+01],\n",
       "        [ 7.6294e-06, -2.1436e+01, -6.4309e+01,  2.1436e+01],\n",
       "        [ 7.6294e-06, -2.1436e+01, -6.4309e+01,  2.1436e+01],\n",
       "        [ 5.7220e-06, -2.1436e+01, -6.4309e+01, -1.0718e+02]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var.backward()\n",
    "phi.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def sum_expression(x):\n",
    "    # get the diagonal elements of x\n",
    "    diagonal = torch.diag(x)\n",
    "    print('diagonal', diagonal)\n",
    "    # sum the diagonal elements\n",
    "    sum_diagonal = torch.sum(diagonal)\n",
    "    print('sum_diagonal', sum_diagonal)\n",
    "    # subtract x[i,j] from sum_diagonal for all i != j\n",
    "    print(x - torch.diag(diagonal))\n",
    "    subtrahend = torch.sum(x - torch.diag(diagonal), dim=1)\n",
    "    print('subtrahend', subtrahend)\n",
    "    # sum the subtrahends to get the final result\n",
    "    result = sum_diagonal - torch.sum(subtrahend)\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sum_expression(x):\n",
    "    # get the diagonal elements of x\n",
    "    diagonal = torch.diag(x)\n",
    "    print('diagonal', diagonal)\n",
    "    # subtract x[i,j] from diagonal for all i != j\n",
    "    print(x - torch.diag(diagonal))\n",
    "    subtrahend = torch.sum(x - torch.diag(diagonal), dim=1)\n",
    "    print('subtrahend', subtrahend)\n",
    "    # sum the subtrahends to get the final result\n",
    "    result = torch.sum(diagonal) - torch.sum(subtrahend)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diagonal tensor([1., 3., 2.], grad_fn=<DiagonalBackward0_copy>)\n",
      "tensor([[0., 3., 3.],\n",
      "        [3., 0., 3.],\n",
      "        [2., 3., 0.]], grad_fn=<SubBackward0>)\n",
      "subtrahend tensor([6., 6., 5.], grad_fn=<SumBackward1>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(-11., grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([\n",
    "              [1.0, 3.0, 3.0],\n",
    "              [3.0, 3.0, 3.0],\n",
    "              [2.0, 3.0, 2.0]], requires_grad=True)\n",
    "\n",
    "var = sum_expression(x)\n",
    "var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([\n",
    "              [1.0, 3.0, 3.0],\n",
    "              [3.0, 3.0, 3.0],\n",
    "              [2.0, 3.0, 2.0]],requires_grad=True)\n",
    "def test(x):\n",
    "    s = 0\n",
    "    mat = torch.zeros(3,3)\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            if i!=j:\n",
    "                #print(x[i,i] - x[i,j])\n",
    "                diff = x[i,i] - x[i,j]\n",
    "                s+=diff\n",
    "                mat[i,j] = diff\n",
    "    return mat.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-5., grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 2., -1., -1.],\n",
       "        [-1.,  2., -1.],\n",
       "        [-1., -1.,  2.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = test(x)\n",
    "print(t)\n",
    "t.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Array([[ 0.,  0.,  0.],\n",
       "       [-1.,  1.,  0.],\n",
       "       [-1., -1.,  2.]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import grad\n",
    "\n",
    "def test(x):\n",
    "    s = 0\n",
    "    mat = []\n",
    "    #mat = jnp.zeros((3,3))\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            if i>j:\n",
    "                #print(x[i,i] - x[i,j])\n",
    "                diff = x[i,i] - x[i,j]\n",
    "                #print(diff)\n",
    "                s=s + diff\n",
    "                #mat.append(diff)\n",
    "    #mat = jnp.array(mat)\n",
    "    return s#jnp.sum(mat)\n",
    "\n",
    "x = jnp.array([[1.0, 2.0, 3.0],\n",
    "              [3.0, 3.0, 3.0],\n",
    "              [2.0, 2.0, 2.0]])\n",
    "print(test(x))\n",
    "grad(test)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "diag(): argument 'input' (position 1) must be Tensor, not DeviceArray",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m torch\u001b[39m.\u001b[39;49mdiag(x)\u001b[39m.\u001b[39mreshape(\u001b[39m3\u001b[39m,\u001b[39m1\u001b[39m)\u001b[39m-\u001b[39mtorch\u001b[39m.\u001b[39mcumsum(x, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m-\u001b[39mtorch\u001b[39m.\u001b[39mdiag(x)\u001b[39m.\u001b[39mreshape(\u001b[39m3\u001b[39m,\u001b[39m1\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: diag(): argument 'input' (position 1) must be Tensor, not DeviceArray"
     ]
    }
   ],
   "source": [
    "torch.diag(x).reshape(3,1)-torch.cumsum(x, axis=1)-torch.diag(x).reshape(3,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 3., 6.],\n",
       "        [3., 6., 9.],\n",
       "        [2., 4., 6.]], grad_fn=<CumsumBackward0>)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cumsum(x, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1., -1., -1., -1.],\n",
       "        [-1.,  1., -1., -1.],\n",
       "        [-1., -1.,  1., -1.],\n",
       "        [-1., -1., -1.,  1.]])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "experiments",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
