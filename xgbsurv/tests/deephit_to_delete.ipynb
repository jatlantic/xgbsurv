{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/JUSC/miniconda3/envs/experiments/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pycox.models.loss import rank_loss_deephit_single\n",
    "#from pycox.models.data import pair_rank_mat\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import random\n",
    "from pycox.models import utils\n",
    "from pycox.models.loss import _rank_loss_deephit,  _diff_cdf_at_time_i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _reduction(loss: Tensor, reduction: str = 'mean') -> Tensor:\n",
    "    if reduction == 'none':\n",
    "        return loss\n",
    "    elif reduction == 'mean':\n",
    "        return loss.mean()\n",
    "    elif reduction == 'sum':\n",
    "        return loss.sum()\n",
    "    raise ValueError(f\"`reduction` = {reduction} is not valid. Use 'none', 'mean' or 'sum'.\")\n",
    "    \n",
    "def _pair_rank_mat(mat, idx_durations, events, dtype='float32'):\n",
    "    n = len(idx_durations)\n",
    "    for i in range(n):\n",
    "        dur_i = idx_durations[i]\n",
    "        ev_i = events[i]\n",
    "        if ev_i == 0:\n",
    "            continue\n",
    "        for j in range(n):\n",
    "            dur_j = idx_durations[j]\n",
    "            ev_j = events[j]\n",
    "            if (dur_i < dur_j) or ((dur_i == dur_j) and (ev_j == 0)):\n",
    "                mat[i, j] = 1\n",
    "    return mat\n",
    "\n",
    "def pair_rank_mat(idx_durations, events, dtype='float32'):\n",
    "    \"\"\"Indicator matrix R with R_ij = 1{T_i < T_j and D_i = 1}.\n",
    "    So it takes value 1 if we observe that i has an event before j and zero otherwise.\n",
    "    \n",
    "    Arguments:\n",
    "        idx_durations {np.array} -- Array with durations.\n",
    "        events {np.array} -- Array with event indicators.\n",
    "    \n",
    "    Keyword Arguments:\n",
    "        dtype {str} -- dtype of array (default: {'float32'})\n",
    "    \n",
    "    Returns:\n",
    "        np.array -- n x n matrix indicating if i has an observerd event before j.\n",
    "    \"\"\"\n",
    "    idx_durations = idx_durations.reshape(-1)\n",
    "    events = events.reshape(-1)\n",
    "    n = len(idx_durations)\n",
    "    mat = np.zeros((n, n), dtype=dtype)\n",
    "    mat = _pair_rank_mat(mat, idx_durations, events, dtype)\n",
    "    return mat\n",
    "\n",
    "def rank_loss_deephit_single(phi: Tensor, idx_durations: Tensor, events: Tensor, rank_mat: Tensor,\n",
    "                             sigma: Tensor, reduction: str = 'mean') -> Tensor:\n",
    "    \n",
    "    idx_durations = idx_durations.view(-1, 1)\n",
    "    # events = events.float().view(-1)\n",
    "    pmf = utils.pad_col(phi) #.softmax(1)\n",
    "    #print('softmax pmf', pmf)\n",
    "    y = torch.zeros_like(pmf).scatter(1, idx_durations, 1.) # one-hot\n",
    "    print('y', y)\n",
    "    rank_loss = _rank_loss_deephit(pmf, y, rank_mat, sigma, reduction)\n",
    "    return rank_loss\n",
    "\n",
    "def _rank_loss_deephit(pmf: Tensor, y: Tensor, rank_mat: Tensor, sigma: float,\n",
    "                       reduction: str = 'mean') -> Tensor:\n",
    "    \"\"\"Ranking loss from DeepHit.\n",
    "    \n",
    "    Arguments:\n",
    "        pmf {torch.tensor} -- Matrix with probability mass function pmf_ij = f_i(t_j)\n",
    "        y {torch.tensor} -- Matrix with indicator of duration and censoring time. \n",
    "        rank_mat {torch.tensor} -- See pair_rank_mat function.\n",
    "        sigma {float} -- Sigma from DeepHit paper, chosen by you.\n",
    "    \n",
    "    Returns:\n",
    "        torch.tensor -- loss\n",
    "    \"\"\"\n",
    "    r = _diff_cdf_at_time_i(pmf, y)\n",
    "    print('intermediary result', torch.exp(-r/sigma))\n",
    "    loss = rank_mat * torch.exp(-r/sigma)\n",
    "    loss = loss.mean(1, keepdim=True)\n",
    "    return _reduction(loss, reduction)\n",
    "\n",
    "def _diff_cdf_at_time_i(pmf: Tensor, y: Tensor) -> Tensor:\n",
    "    \"\"\"R is the matrix from the DeepHit code giving the difference in CDF between individual\n",
    "    i and j, at the event time of j. \n",
    "    I.e: R_ij = F_i(T_i) - F_j(T_i)\n",
    "    \n",
    "    Arguments:\n",
    "        pmf {torch.tensor} -- Matrix with probability mass function pmf_ij = f_i(t_j)\n",
    "        y {torch.tensor} -- Matrix with indicator of duration/censor time.\n",
    "    \n",
    "    Returns:\n",
    "        torch.tensor -- R_ij = F_i(T_i) - F_j(T_i)\n",
    "    \"\"\"\n",
    "    n = pmf.shape[0]\n",
    "    ones = torch.ones((n, 1), device=pmf.device)\n",
    "    r = pmf.cumsum(1).matmul(y.transpose(0, 1))\n",
    "    diag_r = r.diag().view(1, -1)\n",
    "    r = ones.matmul(diag_r) - r\n",
    "    #print('r',r)\n",
    "    return r.transpose(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y tensor([[1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]])\n",
      "ones tensor([[1.],\n",
      "        [1.],\n",
      "        [1.]])\n",
      "y.transpose(0, 1) tensor([[1., 0., 0.],\n",
      "        [0., 1., 1.],\n",
      "        [0., 0., 0.]])\n",
      "pmf.cumsum(1) tensor([[1., 3., 3.],\n",
      "        [3., 6., 6.],\n",
      "        [2., 4., 4.]], grad_fn=<CumsumBackward0>)\n",
      "r tensor([[1., 3., 3.],\n",
      "        [3., 6., 6.],\n",
      "        [2., 4., 4.]], grad_fn=<MmBackward0>)\n",
      "diag_r tensor([[1., 6., 4.]], grad_fn=<ViewBackward0>)\n",
      "ones.matmul(diag_r) tensor([[1., 6., 4.],\n",
      "        [1., 6., 4.],\n",
      "        [1., 6., 4.]], grad_fn=<MmBackward0>)\n",
      "r fin tensor([[ 0.,  3.,  1.],\n",
      "        [-2.,  0., -2.],\n",
      "        [-1.,  2.,  0.]], grad_fn=<SubBackward0>)\n",
      "r fin transpose tensor([[ 0., -2., -1.],\n",
      "        [ 3.,  0.,  2.],\n",
      "        [ 1., -2.,  0.]], grad_fn=<TransposeBackward0>)\n",
      "sum tensor(1., grad_fn=<SumBackward0>)\n",
      "intermediary result tensor([0.3679], grad_fn=<ExpBackward0>)\n",
      "tensor(1.1036, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  2.2073],\n",
       "        [ 0.0000, -1.1036],\n",
       "        [ 0.0000, -1.1036]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's do easy to follow example\n",
    "torch.manual_seed(42)\n",
    "phi = torch.tensor([\n",
    "              [1.0, 2.0],\n",
    "              [3.0, 3.0],\n",
    "              [2.0, 2.0]], requires_grad=True)\n",
    "#phi = torch.rand(6, 4,requires_grad=True) * 2 - 1\n",
    "survival_times = torch.tensor([5, 10, 10])\n",
    "events = torch.tensor([1, 1, 1])\n",
    "idx_durations = torch.tensor([0,1,1])\n",
    "rank_mat = torch.tensor([\n",
    "              [1.0, 1.0],\n",
    "              [1.0, 1.0],\n",
    "              [1.0, 1.0]])\n",
    "#torch.from_numpy(pair_rank_mat(idx_durations, events))\n",
    "sigma = torch.tensor([1.0])\n",
    "var = rank_loss_deephit_single(phi, idx_durations, events, rank_mat, sigma, 'sum') \n",
    "print(var)\n",
    "var.backward()\n",
    "phi.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _diff_cdf_at_time_i(pmf: Tensor, y: Tensor) -> Tensor:\n",
    "    \"\"\"R is the matrix from the DeepHit code giving the difference in CDF between individual\n",
    "    i and j, at the event time of j. \n",
    "    I.e: R_ij = F_i(T_i) - F_j(T_i)\n",
    "    \n",
    "    Arguments:\n",
    "        pmf {torch.tensor} -- Matrix with probability mass function pmf_ij = f_i(t_j)\n",
    "        y {torch.tensor} -- Matrix with indicator of duration/censor time.\n",
    "    \n",
    "    Returns:\n",
    "        torch.tensor -- R_ij = F_i(T_i) - F_j(T_i)\n",
    "    \"\"\"\n",
    "    n = pmf.shape[0]\n",
    "    ones = torch.ones((n, 1), device=pmf.device)\n",
    "    print('ones',ones)\n",
    "    print('y.transpose(0, 1)',y.transpose(0, 1))\n",
    "    print('pmf.cumsum(1)',pmf.cumsum(1))\n",
    "    r = pmf.cumsum(1).matmul(y.transpose(0, 1))\n",
    "    print('diagonal r',r)\n",
    "    diag_r = r.diag().view(1, -1)\n",
    "    print('diag_r', diag_r)\n",
    "    print('ones.matmul(diag_r)',ones.matmul(diag_r))\n",
    "    r = ones.matmul(diag_r) - r\n",
    "    print('r fin',r)\n",
    "    print('r fin transpose',r.transpose(0, 1))\n",
    "    print('sum',r.transpose(0, 1).sum())\n",
    "    return r.transpose(0, 1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pmf tensor([[1., 2., 0.],\n",
      "        [3., 3., 0.],\n",
      "        [2., 2., 0.]], grad_fn=<CatBackward0>)\n",
      "ones tensor([[1.],\n",
      "        [1.],\n",
      "        [1.]])\n",
      "y.transpose(0, 1) tensor([[1., 0., 0.],\n",
      "        [0., 1., 1.],\n",
      "        [0., 0., 0.]])\n",
      "pmf.cumsum(1) tensor([[1., 3., 3.],\n",
      "        [3., 6., 6.],\n",
      "        [2., 4., 4.]], grad_fn=<CumsumBackward0>)\n",
      "diagonal r tensor([[1., 3., 3.],\n",
      "        [3., 6., 6.],\n",
      "        [2., 4., 4.]], grad_fn=<MmBackward0>)\n",
      "diag_r tensor([[1., 6., 4.]], grad_fn=<ViewBackward0>)\n",
      "ones.matmul(diag_r) tensor([[1., 6., 4.],\n",
      "        [1., 6., 4.],\n",
      "        [1., 6., 4.]], grad_fn=<MmBackward0>)\n",
      "r fin tensor([[ 0.,  3.,  1.],\n",
      "        [-2.,  0., -2.],\n",
      "        [-1.,  2.,  0.]], grad_fn=<SubBackward0>)\n",
      "r fin transpose tensor([[ 0., -2., -1.],\n",
      "        [ 3.,  0.,  2.],\n",
      "        [ 1., -2.,  0.]], grad_fn=<TransposeBackward0>)\n",
      "sum tensor(1., grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0., -2.],\n",
       "        [ 0.,  1.],\n",
       "        [ 0.,  1.]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phi = torch.tensor([\n",
    "              [1.0, 2.0],\n",
    "              [3.0, 3.0],\n",
    "              [2.0, 2.0]], requires_grad=True)\n",
    "y = torch.tensor([ # has to be fully dimensional for all scenarios\n",
    "        [1., 0., 0.],\n",
    "        [0., 1., 0.],\n",
    "        [0., 1., 0.]])\n",
    "pmf = utils.pad_col(phi)\n",
    "print('pmf', pmf)\n",
    "var = _diff_cdf_at_time_i(pmf, y)\n",
    "var.backward()\n",
    "phi.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ones tensor([[1.],\n",
      "        [1.],\n",
      "        [1.]])\n",
      "y.transpose(0, 1) tensor([[1., 0., 0.],\n",
      "        [0., 1., 1.],\n",
      "        [0., 0., 0.]])\n",
      "pmf.cumsum(1) tensor([[1., 3., 3.],\n",
      "        [3., 6., 6.],\n",
      "        [2., 4., 4.]], grad_fn=<CumsumBackward0>)\n",
      "r tensor([[1., 3., 3.],\n",
      "        [3., 6., 6.],\n",
      "        [2., 4., 4.]], grad_fn=<MmBackward0>)\n",
      "diag_r tensor([[1., 6., 4.]], grad_fn=<ViewBackward0>)\n",
      "ones.matmul(diag_r) tensor([[1., 6., 4.],\n",
      "        [1., 6., 4.],\n",
      "        [1., 6., 4.]], grad_fn=<MmBackward0>)\n",
      "r fin tensor([[ 0.,  3.,  1.],\n",
      "        [-2.,  0., -2.],\n",
      "        [-1.,  2.,  0.]], grad_fn=<SubBackward0>)\n",
      "r fin transpose tensor([[ 0., -2., -1.],\n",
      "        [ 3.,  0.,  2.],\n",
      "        [ 1., -2.,  0.]], grad_fn=<TransposeBackward0>)\n",
      "sum tensor(1., grad_fn=<SumBackward0>)\n",
      "gradient tensor([[ 0., -2.],\n",
      "        [ 0.,  1.],\n",
      "        [ 0.,  1.]])\n"
     ]
    }
   ],
   "source": [
    "phi = torch.tensor([\n",
    "              [1.0, 2.0],\n",
    "              [3.0, 3.0],\n",
    "              [2.0, 2.0]], requires_grad=True)\n",
    "pmf = utils.pad_col(phi)\n",
    "y = torch.tensor([\n",
    "        [1., 0., 0.],\n",
    "        [0., 1., 0.],\n",
    "        [0., 1., 0.]])\n",
    "var = _diff_cdf_at_time_i(pmf, y)\n",
    "var.backward()\n",
    "print('gradient',phi.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.softmax(utils.pad_col(phi), axis=1) # correct\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i,j 0 1\n",
      "i,j 0 2\n",
      "i,j 1 0\n",
      "i,j 1 2\n",
      "i,j 2 0\n",
      "i,j 2 1\n",
      "[-2. -5.  3. -3.  4.  2.]\n",
      "sum -1.0\n",
      "i,j 0 1\n",
      "i,j 0 2\n",
      "i,j 1 0\n",
      "i,j 1 2\n",
      "i,j 2 0\n",
      "i,j 2 1\n",
      "Traced<ConcreteArray([-2. -5.  3. -3.  4.  2.], dtype=float32)>with<JVPTrace(level=2/0)> with\n",
      "  primal = DeviceArray([-2., -5.,  3., -3.,  4.,  2.], dtype=float32)\n",
      "  tangent = Traced<ShapedArray(float32[6])>with<JaxprTrace(level=1/0)> with\n",
      "    pval = (ShapedArray(float32[6]), None)\n",
      "    recipe = JaxprEqnRecipe(eqn_id=<object object at 0x20157d2a0>, in_tracers=(Traced<ShapedArray(float32[1]):JaxprTrace(level=1/0)>, Traced<ShapedArray(float32[1]):JaxprTrace(level=1/0)>, Traced<ShapedArray(float32[1]):JaxprTrace(level=1/0)>, Traced<ShapedArray(float32[1]):JaxprTrace(level=1/0)>, Traced<ShapedArray(float32[1]):JaxprTrace(level=1/0)>, Traced<ShapedArray(float32[1]):JaxprTrace(level=1/0)>), out_tracer_refs=[<weakref at 0x2021fc810; to 'JaxprTracer' at 0x2021fc7c0>], out_avals=[ShapedArray(float32[6])], primitive=concatenate, params={'dimension': 0}, effects=set(), source_info=SourceInfo(traceback=<jaxlib.xla_extension.Traceback object at 0x2021f61f0>, name_stack=NameStack(stack=(Transform(name='jvp'),))))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeviceArray([[ 0., -2., -1.],\n",
       "             [ 0.,  1., -1.],\n",
       "             [ 0.,  1.,  2.]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now calculate above manually\n",
    "\n",
    "import jax.numpy as jnp\n",
    "from jax import grad\n",
    "\n",
    "def test(x):\n",
    "    \n",
    "    x = jnp.cumsum(x, axis=1)\n",
    "    s = 0\n",
    "    mat = []\n",
    "    #mat2 = jnp.zeros((3,3))\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            if i!=j:\n",
    "                #print('i,j',i,j)\n",
    "                #print(x[i,i] - x[i,j])\n",
    "                diff = x[i,i] - x[i,j]\n",
    "                #mat2[i,j] = diff\n",
    "                #print(diff)\n",
    "                #s = s + diff\n",
    "                mat.append(diff)\n",
    "\n",
    "    mat = jnp.array(mat)\n",
    "    print(mat)\n",
    "    return jnp.sum(mat)\n",
    "\n",
    "x = jnp.array([[1.0, 2.0, 3.0],\n",
    "              [3.0, 3.0, 3.0],\n",
    "              [2.0, 2.0, 2.0]])\n",
    "print('sum',test(x))\n",
    "grad(test)(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m rank_mat \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(pair_rank_mat(idx_durations, events))\n\u001b[1;32m     13\u001b[0m sigma \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([\u001b[39m1.0\u001b[39m])\n\u001b[0;32m---> 14\u001b[0m var\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     15\u001b[0m phi\u001b[39m.\u001b[39mgrad\n\u001b[1;32m     17\u001b[0m phi\n",
      "File \u001b[0;32m~/miniconda3/envs/experiments/lib/python3.10/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    489\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    490\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/experiments/lib/python3.10/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "phi = torch.tensor([[ 0.7645,  0.8300, 0.2343,  0.9186],\n",
    "        [0.2191,  0.2018, 0.4869,  0.5873],\n",
    "        [ 0.8815, 0.7336,  0.8692,  0.1872],\n",
    "        [ 0.7388,  0.1354,  0.4822, 0.1412],\n",
    "        [ 0.7709,  0.1478, 0.4668,  0.2549],\n",
    "        [0.4607, 0.1173, 0.4062,  0.6634]],requires_grad=True)\n",
    "#phi = torch.rand(6, 4,requires_grad=True) * 2 - 1\n",
    "survival_times = torch.tensor([5, 10, 10, 20, 20, 30])\n",
    "events = torch.tensor([1, 1, 1, 1, 1, 1])\n",
    "idx_durations = torch.tensor([0,1,1,2,2,3])\n",
    "rank_mat = torch.from_numpy(pair_rank_mat(idx_durations, events))\n",
    "sigma = torch.tensor([1.0])\n",
    "var.backward()\n",
    "phi.grad\n",
    "\n",
    "phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.1637, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var = rank_loss_deephit_single(phi, idx_durations, events, rank_mat, sigma, 'sum') \n",
    "var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1522,  0.0494,  0.0272,  0.0540],\n",
       "        [-0.0392, -0.0666,  0.0389,  0.0430],\n",
       "        [-0.0296, -0.0650,  0.0491,  0.0248],\n",
       "        [ 0.0791,  0.0142, -0.0599, -0.0179],\n",
       "        [ 0.0789,  0.0136, -0.0578, -0.0196],\n",
       "        [ 0.0998,  0.0449, -0.0069, -0.0909]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var.backward()\n",
    "phi.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def sum_expression(x):\n",
    "    # get the diagonal elements of x\n",
    "    diagonal = torch.diag(x)\n",
    "    print('diagonal', diagonal)\n",
    "    # sum the diagonal elements\n",
    "    sum_diagonal = torch.sum(diagonal)\n",
    "    print('sum_diagonal', sum_diagonal)\n",
    "    # subtract x[i,j] from sum_diagonal for all i != j\n",
    "    print(x - torch.diag(diagonal))\n",
    "    subtrahend = torch.sum(x - torch.diag(diagonal), dim=1)\n",
    "    print('subtrahend', subtrahend)\n",
    "    # sum the subtrahends to get the final result\n",
    "    result = sum_diagonal - torch.sum(subtrahend)\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sum_expression(x):\n",
    "    # get the diagonal elements of x\n",
    "    diagonal = torch.diag(x)\n",
    "    print('diagonal', diagonal)\n",
    "    # subtract x[i,j] from diagonal for all i != j\n",
    "    print(x - torch.diag(diagonal))\n",
    "    subtrahend = torch.sum(x - torch.diag(diagonal), dim=1)\n",
    "    print('subtrahend', subtrahend)\n",
    "    # sum the subtrahends to get the final result\n",
    "    result = torch.sum(diagonal) - torch.sum(subtrahend)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diagonal tensor([1., 3., 2.], grad_fn=<DiagBackward0>)\n",
      "tensor([[0., 3., 3.],\n",
      "        [3., 0., 3.],\n",
      "        [2., 3., 0.]], grad_fn=<SubBackward0>)\n",
      "subtrahend tensor([6., 6., 5.], grad_fn=<SumBackward1>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(-11., grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([\n",
    "              [1.0, 3.0, 3.0],\n",
    "              [3.0, 3.0, 3.0],\n",
    "              [2.0, 3.0, 2.0]], requires_grad=True)\n",
    "\n",
    "var = sum_expression(x)\n",
    "var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([\n",
    "              [1.0, 3.0, 3.0],\n",
    "              [3.0, 3.0, 3.0],\n",
    "              [2.0, 3.0, 2.0]],requires_grad=True)\n",
    "def test(x):\n",
    "    s = 0\n",
    "    mat = torch.zeros(3,3)\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            if i!=j:\n",
    "                #print(x[i,i] - x[i,j])\n",
    "                diff = x[i,i] - x[i,j]\n",
    "                s+=diff\n",
    "                mat[i,j] = diff\n",
    "    return mat.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-5., grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 2., -1., -1.],\n",
       "        [-1.,  2., -1.],\n",
       "        [-1., -1.,  2.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = test(x)\n",
    "print(t)\n",
    "t.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeviceArray([[ 2., -1., -1.],\n",
       "             [-1.,  2., -1.],\n",
       "             [-1., -1.,  2.]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import grad\n",
    "\n",
    "def test(x):\n",
    "    s = 0\n",
    "    mat = []\n",
    "    #mat = jnp.zeros((3,3))\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            if i!=j:\n",
    "                #print(x[i,i] - x[i,j])\n",
    "                diff = x[i,i] - x[i,j]\n",
    "                #print(diff)\n",
    "                s=s + diff\n",
    "                #mat.append(diff)\n",
    "    #mat = jnp.array(mat)\n",
    "    return s#jnp.sum(mat)\n",
    "\n",
    "x = jnp.array([[1.0, 2.0, 3.0],\n",
    "              [3.0, 3.0, 3.0],\n",
    "              [2.0, 2.0, 2.0]])\n",
    "print(test(x))\n",
    "grad(test)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "diag(): argument 'input' (position 1) must be Tensor, not DeviceArray",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m torch\u001b[39m.\u001b[39;49mdiag(x)\u001b[39m.\u001b[39mreshape(\u001b[39m3\u001b[39m,\u001b[39m1\u001b[39m)\u001b[39m-\u001b[39mtorch\u001b[39m.\u001b[39mcumsum(x, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m-\u001b[39mtorch\u001b[39m.\u001b[39mdiag(x)\u001b[39m.\u001b[39mreshape(\u001b[39m3\u001b[39m,\u001b[39m1\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: diag(): argument 'input' (position 1) must be Tensor, not DeviceArray"
     ]
    }
   ],
   "source": [
    "torch.diag(x).reshape(3,1)-torch.cumsum(x, axis=1)-torch.diag(x).reshape(3,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 3., 6.],\n",
       "        [3., 6., 9.],\n",
       "        [2., 4., 6.]], grad_fn=<CumsumBackward0>)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cumsum(x, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1., -1., -1., -1.],\n",
       "        [-1.,  1., -1., -1.],\n",
       "        [-1., -1.,  1., -1.],\n",
       "        [-1., -1., -1.,  1.]])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "experiments",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
