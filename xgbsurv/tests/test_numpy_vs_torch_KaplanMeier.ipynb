{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from xgbsurv.models.utils import transform_back\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_np = np.array([1.0,1.0,1.0,3.0,5.0,5.0,5.0,8.0])\n",
    "event_np = np.array([1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0])\n",
    "time_torch = torch.tensor(time_np)\n",
    "event_torch = torch.tensor(event_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KaplanMeier(time: np.array, event: np.array, \n",
    "                cens_dist: bool = False\n",
    ") -> tuple[np.array, np.array] | tuple[np.array, np.array, np.array]:\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    time : npt.NDArray[float]\n",
    "        _description_\n",
    "    event : npt.NDArray[int]\n",
    "        _description_\n",
    "    cens_dist : bool, optional\n",
    "        _description_, by default False\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple[npt.NDArray[float], npt.NDArray[float]] | tuple[npt.NDArray[float], npt.NDArray[float], npt.NDArray[int]]\n",
    "        _description_\n",
    "    \n",
    "    References\n",
    "    ----------\n",
    "    .. [1] Kaplan, E. L. and Meier, P., \"Nonparametric estimation from incomplete observations\",\n",
    "           Journal of The American Statistical Association, vol. 53, pp. 457-481, 1958.\n",
    "    .. [2] S. Pölsterl, “scikit-survival: A Library for Time-to-Event Analysis Built on Top of scikit-learn,”\n",
    "           Journal of Machine Learning Research, vol. 21, no. 212, pp. 1–6, 2020.\n",
    "    \"\"\"\n",
    "    # similar approach to sksurv, but no loops\n",
    "    # even and censored is other way round in sksurv ->clarify\n",
    "    #time, event = transform_back(y)\n",
    "    # order, remove later\n",
    "    is_sorted = lambda a: np.all(a[:-1] <= a[1:])\n",
    "    \n",
    "    if is_sorted(time) == False:\n",
    "        order = np.argsort(time, kind=\"mergesort\")\n",
    "        time = time[order]\n",
    "        event = event[order]\n",
    "    \n",
    "    times = np.unique(time)\n",
    "    idx = np.digitize(time, np.unique(time))\n",
    "    # numpy diff nth discrete difference over index, add 1 at the beginning\n",
    "    breaks = np.flatnonzero(np.concatenate(([1], np.diff(idx))))\n",
    "\n",
    "    # flatnonzero return indices that are nonzero in flattened version\n",
    "    n_events = np.add.reduceat(event, breaks, axis=0)\n",
    "    n_at_risk = np.sum(np.unique((np.outer(time,time)>=np.square(time)).astype(int).T,axis=0),axis=1)[::-1]\n",
    "    \n",
    "    # censoring distribution for ipcw estimation\n",
    "    #n_censored a vector, with 1 at censoring position, zero elsewhere\n",
    "    n_events = n_events.astype(np.float32)\n",
    "    n_at_risk = n_at_risk.astype(np.float32)\n",
    "    if cens_dist:\n",
    "        n_at_risk -= n_events\n",
    "        # for each unique time step how many observations are censored\n",
    "        censored = 1-event\n",
    "        n_censored = np.add.reduceat(censored, breaks, axis=0)\n",
    "        mask = (n_events != 0)\n",
    "        vals = 1-np.divide(\n",
    "        n_censored, n_at_risk,\n",
    "        out=np.zeros(times.shape[0], dtype=float),\n",
    "        where=n_censored != 0,\n",
    "    )\n",
    "        estimates = np.cumprod(vals)\n",
    "        return times, estimates, n_censored\n",
    "\n",
    "\n",
    "    else:\n",
    "        vals = 1-np.divide(\n",
    "        n_events, n_at_risk,\n",
    "        out=np.zeros(times.shape[0], dtype=float),\n",
    "        where=n_events != 0,\n",
    "        )\n",
    "        estimates = np.cumprod(vals)\n",
    "        return times, estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1., 3., 5., 8.]), array([1. , 1. , 0.5, 0.5]), array([0., 0., 1., 0.]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KaplanMeier(time_np, event_np, cens_dist=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 3., 5., 5., 5., 8.])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KaplanMeier_torch(time: np.array, event: np.array, \n",
    "                cens_dist: bool = False\n",
    ") -> tuple[np.array, np.array] | tuple[np.array, np.array, np.array]:\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    time : npt.NDArray[float]\n",
    "        _description_\n",
    "    event : npt.NDArray[int]\n",
    "        _description_\n",
    "    cens_dist : bool, optional\n",
    "        _description_, by default False\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple[npt.NDArray[float], npt.NDArray[float]] | tuple[npt.NDArray[float], npt.NDArray[float], npt.NDArray[int]]\n",
    "        _description_\n",
    "    \n",
    "    References\n",
    "    ----------\n",
    "    .. [1] Kaplan, E. L. and Meier, P., \"Nonparametric estimation from incomplete observations\",\n",
    "           Journal of The American Statistical Association, vol. 53, pp. 457-481, 1958.\n",
    "    .. [2] S. Pölsterl, “scikit-survival: A Library for Time-to-Event Analysis Built on Top of scikit-learn,”\n",
    "           Journal of Machine Learning Research, vol. 21, no. 212, pp. 1–6, 2020.\n",
    "    \"\"\"\n",
    "    # similar approach to sksurv, but no loops\n",
    "    # even and censored is other way round in sksurv ->clarify\n",
    "    #time, event = transform_back(y)\n",
    "    # order, remove later\n",
    "    #is_sorted = lambda a: np.all(a[:-1] <= a[1:])\n",
    "    \n",
    "    # if is_sorted(time) == False:\n",
    "    #     order = np.argsort(time, kind=\"mergesort\")\n",
    "    #     time = time[order]\n",
    "    #     event = event[order]\n",
    "    \n",
    "    times = torch.unique(time)\n",
    "    #idx = np.digitize(time, np.unique(time))\n",
    "    # numpy diff nth discrete difference over index, add 1 at the beginning\n",
    "    #breaks = np.flatnonzero(np.concatenate(([1], np.diff(idx))))\n",
    "\n",
    "    # flatnonzero return indices that are nonzero in flattened version\n",
    "    #n_events = np.add.reduceat(event, breaks, axis=0)\n",
    "    unique_times, inverse_indices = time.unique(return_inverse=True)\n",
    "\n",
    "    # Prepare a tensor for the event counts\n",
    "    event_counts = torch.zeros_like(unique_times)\n",
    "\n",
    "    # Add up the events for each unique time using scatter_add_\n",
    "    event_counts.scatter_add_(0, inverse_indices, event)\n",
    "    n_events = event_counts\n",
    "    n_at_risk = torch.unique((torch.outer(time,time)>=torch.square(time)).int().T, dim=0).sum(axis=1).flip(0)\n",
    "    #print('n_at_risk', n_at_risk)\n",
    "    n_at_risk = n_at_risk.float()\n",
    "    n_events = n_events.float()\n",
    "    # censoring distribution for ipcw estimation\n",
    "    #n_censored a vector, with 1 at censoring position, zero elsewhere\n",
    "    #print('n_at_risk shape',n_at_risk.shape)\n",
    "    #print('n_events shape',n_events.shape)\n",
    "    if cens_dist:\n",
    "        n_at_risk -= n_events\n",
    "        # for each unique time step how many observations are censored\n",
    "        censored = 1-event\n",
    "        n_censored = torch.zeros_like(unique_times)\n",
    "        #n_censored = np.add.reduceat(censored, breaks, axis=0)\n",
    "        n_censored.scatter_add_(0, inverse_indices, censored)\n",
    "        mask = (n_censored != 0)\n",
    "        c = torch.zeros_like(times)\n",
    "        # apply the division operation only where mask is True\n",
    "        c[mask] = n_censored[mask] / n_at_risk[mask] \n",
    "        vals = 1-c\n",
    "        estimates = torch.cumprod(vals, dim=0)\n",
    "        return times, estimates, n_censored\n",
    "\n",
    "\n",
    "    else:\n",
    "        mask = (n_events != 0)\n",
    "        vals = 1-torch.divide(\n",
    "        n_events[mask], n_at_risk[mask],\n",
    "        #rounding_mode=None,\n",
    "        out=torch.zeros(times.shape[0]),\n",
    "        #where=mask,\n",
    "        )\n",
    "        #print(vals)\n",
    "        estimates = torch.cumprod(vals, dim=0)\n",
    "        return times, estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KaplanMeier(time_np, event_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KaplanMeier_torch(time_torch, event_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1., 3., 5., 8.], dtype=float32),\n",
       " array([1. , 1. , 0.5, 0.5]),\n",
       " array([0., 0., 1., 0.], dtype=float32))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KaplanMeier(time_np.astype(np.float32), event_np.astype(np.float32),cens_dist=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 3., 5., 8.], dtype=torch.float64),\n",
       " tensor([1.0000, 1.0000, 0.5000, 0.5000], dtype=torch.float64),\n",
       " tensor([0., 0., 1., 0.], dtype=torch.float64))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KaplanMeier_torch(time_torch, event_torch,cens_dist=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "## IPCW Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ipcw_estimate(time: np.array, event: np.array) -> tuple[np.array, np.array]:\n",
    "\n",
    "    unique_time, cens_dist, n_censored = KaplanMeier(time, event, cens_dist=True) \n",
    "    #print(cens_dist)\n",
    "    # similar approach to sksurv\n",
    "    idx = np.searchsorted(unique_time, time)\n",
    "    est = 1.0/cens_dist[idx] # improve as divide by zero\n",
    "    est[n_censored[idx]!=0] = 0\n",
    "    # in R mboost there is a maxweight of 5\n",
    "    est[est>5] = 5\n",
    "    return unique_time, est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ipcw_estimate_torch(time: np.array, event: np.array) -> tuple[np.array, np.array]:\n",
    "\n",
    "    #print(time.shape, event.shape)\n",
    "    unique_time, cens_dist, n_censored = KaplanMeier_torch(time, event, cens_dist=True) \n",
    "    #print(cens_dist)\n",
    "    # similar approach to sksurv\n",
    "    idx = torch.searchsorted(unique_time, time)\n",
    "\n",
    "    mask1 = cens_dist[idx] != 0.0\n",
    "\n",
    "    est = torch.ones_like(cens_dist[idx])\n",
    "    est[mask1] = 1.0/cens_dist[idx][mask1]\n",
    "    #est = 1.0/cens_dist[idx] # improve as divide by zero\n",
    "\n",
    "    est[n_censored[idx]!=0] = 0\n",
    "    # in R mboost there is a maxweight of 5\n",
    "    est[est>5] = 5\n",
    "    return unique_time, est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1., 3., 5., 8.]), array([1., 1., 1., 1., 0., 0., 0., 2.]))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ipcw_estimate(time_np, event_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 3., 5., 8.], dtype=torch.float64),\n",
       " tensor([1., 1., 1., 1., 0., 0., 0., 2.], dtype=torch.float64))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ipcw_estimate_torch(time_torch, event_torch)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_torch(time: torch.Tensor, event: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Transforms time, event into XGBoost digestable format.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    time : npt.NDArray[float]\n",
    "        Survival time.\n",
    "    event : npt.NDArray[int]\n",
    "        Boolean event indicator. Zero value is taken as censored event.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    y : npt.NDArray[float]\n",
    "        Transformed array containing survival time and event where negative value is taken as censored event.\n",
    "    \"\"\"\n",
    "    #if isinstance(time, pd.Series):\n",
    "    #    time = time.to_numpy()\n",
    "    #    event = event.to_numpy()\n",
    "    event_mod = event.clone()\n",
    "    event_mod[event_mod==0] = -1\n",
    "    if (time==0).any():\n",
    "        raise RuntimeError('Data contains zero time value!')\n",
    "        # alternative: time[time==0] = np.finfo(float).eps\n",
    "    y = event_mod*time\n",
    "    return y.to(torch.float32)\n",
    "\n",
    "\n",
    "def transform_back_torch(y: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Transforms XGBoost digestable format variable y into time and event.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y : npt.NDArray[float]\n",
    "        Array containing survival time and event where negative value is taken as censored event.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple[npt.NDArray[float],npt.NDArray[int]]\n",
    "        Survival time and event.\n",
    "    \"\"\"\n",
    "    time = torch.abs(y)\n",
    "    event = (torch.abs(y) == y)\n",
    "    event = event # for numba\n",
    "    return time.to(torch.float32), event.to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_weights(y, approach: str='paper') -> np.array:\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y : npt.NDArray[float]\n",
    "        Sorted array containing survival time and event where negative value is taken as censored event.\n",
    "    approach : str, optional\n",
    "        Choose mboost implementation or paper implementation of c-boosting, by default 'paper'.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    npt.NDArray[float]\n",
    "        Array of weights.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] 1. Mayr, A. & Schmid, M. Boosting the concordance index for survival data–a unified framework to derive and evaluate biomarker combinations. \n",
    "       PloS one 9, e84483 (2014).\n",
    "\n",
    "    \"\"\"\n",
    "    time, event = transform_back(y) \n",
    "    n = event.shape[0]\n",
    "\n",
    "    _, ipcw_new = ipcw_estimate(time, event)\n",
    "\n",
    "    ipcw = ipcw_new #ipcw_old\n",
    "    survtime = time\n",
    "    wweights = np.full((n,n), np.square(ipcw)).T # good here\n",
    "\n",
    "    weightsj = np.full((n,n), survtime).T\n",
    "\n",
    "    weightsk = np.full((n,n), survtime) #byrow = TRUE in R, in np automatic no T required\n",
    "\n",
    "    if approach == 'mboost':\n",
    "        # implementing   weightsI <- ifelse(weightsj == weightsk, .5, (weightsj < weightsk) + 0) - diag(.5, n,n)\n",
    "        # from mboost github repo\n",
    "        weightsI = np.empty((n,n))\n",
    "        weightsI[weightsj == weightsk] = 0.5\n",
    "        weightsI = (weightsj < weightsk).astype(int)\n",
    "        weightsI = weightsI - np.diag(0.5*np.ones(n))\n",
    "    if approach == 'paper':\n",
    "        weightsI = (weightsj < weightsk).astype(int) \n",
    "\n",
    "    wweights = wweights * weightsI \n",
    "    \n",
    "    wweights = wweights / np.sum(wweights)\n",
    "\n",
    "    return wweights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_weights_torch(y, approach: str='paper') -> np.array:\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y : npt.NDArray[float]\n",
    "        Sorted array containing survival time and event where negative value is taken as censored event.\n",
    "    approach : str, optional\n",
    "        Choose mboost implementation or paper implementation of c-boosting, by default 'paper'.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    npt.NDArray[float]\n",
    "        Array of weights.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] 1. Mayr, A. & Schmid, M. Boosting the concordance index for survival data–a unified framework to derive and evaluate biomarker combinations. \n",
    "       PloS one 9, e84483 (2014).\n",
    "\n",
    "    \"\"\"\n",
    "    time, event = transform_back_torch(y) \n",
    "    #print('time shape', time.shape)\n",
    "    #print('event shape', event.shape)\n",
    "    n = event.shape[0]\n",
    "\n",
    "    _, ipcw_new = ipcw_estimate_torch(time, event)\n",
    "\n",
    "    ipcw = ipcw_new #ipcw_old consider copy\n",
    "    #survtime = time\n",
    "\n",
    "    fill = torch.square(ipcw)\n",
    "    #fill.repeat(n,1).T\n",
    "    wweights = fill.unsqueeze(1).expand(-1, n)\n",
    "\n",
    "    #survtime.repeat(n,1).T\n",
    "    weightsj = time.unsqueeze(1).expand(-1, n)\n",
    "   \n",
    "    #survtime.repeat(n,1)\n",
    "    weightsk = time.unsqueeze(1).expand(-1, n).T\n",
    "    \n",
    "    if approach == 'mboost':\n",
    "        # implementing   weightsI <- ifelse(weightsj == weightsk, .5, (weightsj < weightsk) + 0) - diag(.5, n,n)\n",
    "        # from mboost github repo\n",
    "        weightsI = torch.empty((n,n))\n",
    "        weightsI[weightsj == weightsk] = 0.5\n",
    "        weightsI = (weightsj < weightsk).astype(int)\n",
    "        weightsI = weightsI - torch.diag(0.5*np.ones(n))\n",
    "    if approach == 'paper':\n",
    "        weightsI = (weightsj < weightsk).int()\n",
    "\n",
    "    wweights = wweights * weightsI \n",
    "    del weightsI, weightsk, weightsj\n",
    "    wweights = wweights / torch.sum(wweights)\n",
    "\n",
    "    return wweights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from xgbsurv.models.utils import transform\n",
    "predictor = log_hazard = np.random.normal(0, 1, 1000)\n",
    "df = pd.read_csv('/Users/JUSC/Documents/xgbsurv_benchmarking/implementation_testing/simulation_data/survival_simulation_1000.csv')\n",
    "y = transform(df.time.to_numpy(), df.event.to_numpy())\n",
    "y_torch = torch.tensor(y)\n",
    "predictor_torch = torch.tensor(predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "wweights0 = compute_weights(y, approach='paper')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "wweights1 = compute_weights_torch(y_torch, approach='paper')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(wweights0, wweights1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgbsurv.models.utils import transform_back\n",
    "\n",
    "def cind_loss(y, predictor, sigma = 0.1) ->np.array:\n",
    "    # f corresponds to predictor in paper\n",
    "    time, _ = transform_back(y)\n",
    "    n = time.shape[0]\n",
    "    etaj = np.full((n,n), predictor)\n",
    "    etak = np.full((n,n), predictor).T\n",
    "    x = (etak - etaj) \n",
    "    weights_out = compute_weights(y)\n",
    "    c_loss = 1/(1+np.exp(x/sigma))*weights_out\n",
    "    return -np.sum(c_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cind_loss_torch(y: np.array, predictor: np.array, sigma: np.array = 0.1) -> np.array:\n",
    "    # f corresponds to predictor in paper\n",
    "    time, _ = transform_back_torch(y)\n",
    "    n = time.shape[0]\n",
    "\n",
    "    #predictor.repeat(n,1)\n",
    "    etaj = predictor.unsqueeze(1).expand(-1, n).T \n",
    "    #predictor.repeat(n,1).T\n",
    "    etak = predictor.unsqueeze(1).expand(-1, n) \n",
    "    x = (etak - etaj) \n",
    "    weights_out = compute_weights_torch(y)\n",
    "    print('weights_out.shape',weights_out.shape)\n",
    "    c_loss = 1/(1+torch.exp(x/sigma))*weights_out\n",
    "    return -torch.sum(c_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_loss = cind_loss(y, predictor, sigma = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights_out.shape torch.Size([1000, 1000])\n"
     ]
    }
   ],
   "source": [
    "torch_loss = cind_loss_torch(y_torch,predictor_torch, sigma = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(np_loss, torch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.5114959825198104 tensor(-0.5115, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(np_loss, torch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights_out.shape torch.Size([1000, 1000])\n",
      "tensor(-0.5115, dtype=torch.float64, grad_fn=<NegBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-6.4807e-04, -1.1727e-04, -2.2686e-04,  4.7333e-04,  7.6656e-04,\n",
       "        -3.9806e-04,  1.6378e-04,  1.1481e-03, -2.4710e-04,  6.2454e-04,\n",
       "         4.3906e-04,  6.8778e-04, -4.0097e-04, -2.5393e-04, -3.7270e-04,\n",
       "        -5.8260e-04,  3.3661e-04,  3.2856e-04,  5.0804e-04,  3.2912e-04,\n",
       "        -1.3909e-04, -3.1582e-04, -4.6944e-04, -5.6578e-04, -4.8168e-04,\n",
       "         4.5051e-04, -4.5098e-04, -2.5004e-04, -3.7393e-04,  7.2480e-04,\n",
       "        -3.2129e-04, -5.2864e-04,  9.9898e-04, -4.5997e-04, -5.0943e-04,\n",
       "        -1.5171e-04, -2.7090e-04, -4.0733e-04,  1.0960e-03, -4.5737e-04,\n",
       "         6.6007e-04,  1.2874e-03,  3.7944e-04,  8.0060e-04, -3.8922e-04,\n",
       "        -2.0678e-04, -2.1773e-04,  8.2418e-04, -3.7993e-04,  8.9226e-04,\n",
       "         3.7400e-04, -4.4584e-04, -5.9111e-04,  3.4975e-04, -3.4092e-04,\n",
       "        -2.3846e-04, -4.7695e-04,  1.1400e-03,  6.4417e-04,  4.3762e-04,\n",
       "         6.7918e-04, -4.6274e-04, -4.6625e-04, -4.5748e-04,  5.7386e-04,\n",
       "        -2.4963e-04, -2.8732e-04, -6.4055e-04, -4.4999e-04,  5.5811e-04,\n",
       "         5.2820e-04, -1.1604e-04,  4.4741e-04, -5.0358e-04, -4.5270e-04,\n",
       "        -1.4319e-04, -7.7690e-05, -4.4233e-04, -5.3031e-04, -2.4558e-04,\n",
       "        -5.1213e-04, -4.4492e-04, -4.6609e-04,  6.4939e-04,  8.2165e-04,\n",
       "         3.2506e-04, -4.0006e-04, -4.7508e-04,  8.6839e-04,  7.0535e-04,\n",
       "        -3.5861e-04,  1.1469e-04, -3.3388e-04,  2.8817e-04,  3.4763e-04,\n",
       "         4.5441e-04, -5.6796e-04, -2.7379e-04,  2.2068e-05, -4.7375e-04,\n",
       "        -2.6255e-04, -1.3415e-04, -3.2570e-04, -3.0230e-04,  1.1724e-03,\n",
       "        -1.8824e-04, -5.3669e-04, -2.0636e-04, -2.7175e-04, -4.1118e-04,\n",
       "         5.3885e-04, -4.4154e-04, -6.0141e-04,  2.9812e-04,  6.7804e-04,\n",
       "        -3.8081e-04, -2.2626e-04, -5.8017e-04, -1.5257e-04, -4.6763e-04,\n",
       "        -1.4504e-04, -6.7740e-04, -2.1802e-04, -1.1133e-04, -4.8681e-04,\n",
       "         5.5548e-04,  4.2901e-04, -2.4930e-04, -3.8913e-04, -2.5326e-04,\n",
       "        -1.5297e-04, -8.2752e-05,  3.6771e-04,  1.0096e-03, -3.5716e-04,\n",
       "        -1.6840e-04, -5.3810e-04, -3.4599e-04, -4.2278e-04, -3.6130e-04,\n",
       "        -4.1536e-04, -4.6160e-04,  7.3668e-04,  8.3723e-04, -4.0728e-04,\n",
       "        -5.2000e-04,  6.1451e-04, -2.6357e-04, -5.9356e-04, -1.6853e-04,\n",
       "         7.0443e-04,  2.4691e-04, -3.5615e-04, -3.7831e-04,  6.7234e-05,\n",
       "         9.8207e-04,  6.5000e-04, -5.5262e-04,  2.7991e-04, -4.6203e-04,\n",
       "        -2.4619e-04, -4.7820e-04, -3.0754e-04, -5.0382e-04, -4.7663e-04,\n",
       "         6.2033e-04, -4.6344e-04,  4.4543e-04, -7.8787e-05, -4.8124e-04,\n",
       "        -4.0408e-04, -3.5912e-04, -4.7161e-04, -2.4018e-04,  1.2427e-04,\n",
       "         4.6412e-04, -2.0890e-04,  6.5027e-04, -4.2446e-04, -4.6225e-04,\n",
       "        -5.8371e-04, -5.9227e-04, -4.9810e-05,  4.8917e-04, -5.2831e-04,\n",
       "         6.2809e-04,  7.1725e-04, -5.1496e-04, -2.2275e-04,  2.8229e-04,\n",
       "         5.7681e-04, -1.0674e-04,  1.0400e-03, -5.7204e-04, -4.3825e-04,\n",
       "        -5.6574e-04, -3.7929e-04, -6.3467e-04, -5.5921e-04, -5.1708e-04,\n",
       "        -4.5697e-04, -4.8364e-04, -1.9074e-05, -3.0066e-04,  3.9785e-04,\n",
       "         1.2034e-03,  4.2153e-04, -5.4024e-04, -5.4757e-04, -5.3193e-04,\n",
       "         8.4958e-04,  5.9869e-04, -3.9852e-04, -1.4961e-04, -2.3880e-04,\n",
       "         4.9690e-04,  5.3801e-04,  4.5339e-04, -5.1833e-04, -4.6679e-04,\n",
       "        -4.8257e-04, -2.7678e-04, -3.7790e-04, -3.1547e-04, -3.0902e-04,\n",
       "        -5.1114e-04,  8.6603e-04, -4.1637e-04, -4.4557e-04, -3.8976e-04,\n",
       "        -1.9079e-04, -4.4394e-04, -2.6554e-04,  9.1300e-04,  1.0025e-03,\n",
       "        -7.9242e-05,  2.7784e-04,  5.5772e-04, -1.2864e-04,  7.6160e-04,\n",
       "        -3.0709e-04, -1.4032e-05, -2.4562e-04, -5.7578e-04, -2.4558e-04,\n",
       "         1.0228e-03, -3.6315e-04, -3.2508e-04,  1.0489e-03, -1.1549e-04,\n",
       "         4.6191e-04,  8.8468e-04,  2.7824e-04, -5.9766e-04, -2.4557e-04,\n",
       "        -3.6743e-04, -5.4053e-05,  2.6001e-04, -2.7133e-04, -2.6498e-04,\n",
       "        -4.4185e-04,  9.1149e-04, -4.0875e-06,  3.4803e-04, -5.7868e-04,\n",
       "         1.0911e-03,  5.8479e-04,  4.9846e-04, -8.1442e-05,  6.1962e-04,\n",
       "         2.6773e-04,  5.8631e-04,  6.8864e-04,  4.7771e-04, -3.8383e-04,\n",
       "        -2.0740e-05, -4.0659e-04, -6.1881e-04,  5.1738e-04,  6.4660e-04,\n",
       "        -1.9166e-04, -4.9471e-05,  2.7279e-04,  2.7270e-04, -3.5837e-04,\n",
       "         4.1756e-04,  8.5954e-04,  2.7987e-04, -4.8355e-04, -2.4530e-04,\n",
       "         2.4455e-03, -4.4972e-04,  8.2238e-04, -5.0455e-04, -9.4641e-06,\n",
       "         4.0976e-04, -2.5178e-04,  7.0132e-04, -3.7611e-04, -2.8687e-04,\n",
       "        -4.6852e-04,  1.0275e-03, -2.1484e-04, -2.4632e-04,  3.8563e-04,\n",
       "        -4.7787e-04, -2.5686e-04, -3.9730e-04, -4.5834e-04, -3.8923e-04,\n",
       "         5.4868e-04, -4.5795e-04, -4.7682e-04, -7.1029e-04, -4.8519e-04,\n",
       "         1.2321e-03,  3.7694e-04, -5.5159e-04, -5.3351e-04, -4.7358e-04,\n",
       "         6.8950e-04,  7.0966e-04,  6.4377e-04,  1.7941e-03, -4.5432e-04,\n",
       "         3.1272e-04,  7.5958e-04, -4.4765e-04, -4.5634e-04, -4.6892e-04,\n",
       "         6.8583e-04, -4.4790e-04, -4.6200e-04, -4.4935e-04, -2.1638e-04,\n",
       "        -5.0520e-04,  5.3363e-04, -5.6794e-04,  6.3368e-04, -1.5683e-04,\n",
       "         7.4849e-04,  1.4694e-04, -3.8057e-04, -6.2127e-04, -2.4429e-04,\n",
       "        -5.0487e-04,  5.5794e-04, -3.3034e-05,  9.9796e-04, -4.4613e-04,\n",
       "        -4.0840e-04, -2.6893e-04,  5.0163e-04, -2.7237e-04, -3.2108e-04,\n",
       "         5.9147e-05, -8.1156e-05, -4.7375e-04, -1.0926e-04, -2.0592e-04,\n",
       "        -9.3764e-05, -5.0774e-04,  7.5953e-04, -3.4001e-04, -7.0712e-04,\n",
       "        -5.8406e-04,  6.2725e-04, -3.9734e-04, -2.6582e-04, -1.2725e-05,\n",
       "        -1.7415e-04, -5.3277e-04, -2.8162e-04,  3.0356e-04, -5.6800e-04,\n",
       "         1.1060e-03, -4.3801e-04,  3.9951e-04, -5.6644e-04, -5.1633e-04,\n",
       "         4.1197e-04, -4.6401e-04, -3.5468e-04, -5.0053e-04, -1.2241e-04,\n",
       "        -1.4300e-04,  1.2214e-03,  5.0265e-04, -1.9808e-04, -4.6806e-04,\n",
       "        -4.6605e-04, -4.6794e-04, -4.4377e-04, -5.8256e-04, -2.8738e-04,\n",
       "        -4.5470e-04,  4.6155e-04, -1.5186e-05,  7.4931e-04,  8.0403e-04,\n",
       "         1.1132e-04,  9.7635e-04, -2.4378e-04, -4.0449e-04, -4.6859e-04,\n",
       "         7.7849e-04,  9.9682e-04,  5.3309e-04, -6.4015e-04, -1.2637e-04,\n",
       "        -3.5335e-04, -1.1658e-04, -3.0082e-04,  2.8830e-04, -2.9248e-04,\n",
       "        -5.7881e-04, -4.0453e-04,  7.6007e-04, -3.9926e-04, -5.8328e-04,\n",
       "         3.0857e-04,  6.4799e-04, -2.4079e-04,  7.3372e-04, -2.7434e-04,\n",
       "        -2.4896e-04, -1.5178e-04, -2.7824e-04,  1.0794e-03, -3.3068e-04,\n",
       "         9.5444e-04, -4.7459e-04, -3.8081e-04,  4.0166e-04,  2.3140e-04,\n",
       "         6.8414e-04,  2.9445e-04, -4.6253e-04, -4.0161e-04,  3.5787e-04,\n",
       "        -5.0183e-04, -1.1815e-04,  3.6626e-04, -2.4722e-04, -4.9059e-04,\n",
       "         5.6727e-04, -1.4483e-04, -4.9094e-04,  9.3000e-04, -1.9366e-04,\n",
       "         1.2262e-03, -2.5571e-04, -1.8735e-04,  3.7798e-04, -1.1497e-04,\n",
       "         8.5278e-04, -8.9605e-05,  3.6745e-04,  7.6040e-04, -4.7108e-04,\n",
       "        -2.7225e-04, -5.6659e-04,  7.5672e-04,  8.8671e-05,  5.7973e-05,\n",
       "         3.6723e-04, -1.0408e-04,  7.5078e-04, -2.4522e-04,  1.0103e-03,\n",
       "        -4.3260e-04, -8.0319e-05, -5.0926e-04,  4.6678e-04,  4.6811e-04,\n",
       "         2.2768e-04, -1.4724e-04, -4.7633e-04,  2.2494e-04,  1.6453e-04,\n",
       "        -5.9380e-04, -4.5562e-04,  4.8874e-04, -2.7895e-04, -4.0066e-04,\n",
       "         1.1157e-03, -5.0469e-04,  9.5335e-04, -7.9031e-05, -3.0596e-04,\n",
       "         7.6676e-04,  1.0598e-03, -5.6587e-04, -4.9998e-04, -5.2491e-04,\n",
       "        -4.1761e-04, -4.1799e-04, -5.1254e-04, -4.6299e-04, -4.5755e-04,\n",
       "         5.7348e-04, -2.7417e-04, -1.4391e-04,  3.2850e-04, -3.7889e-04,\n",
       "         5.3773e-04, -5.2958e-04, -3.5722e-04,  3.0424e-04, -2.6651e-04,\n",
       "         7.2131e-04,  1.2664e-03, -2.7444e-04, -2.5631e-04, -5.5980e-04,\n",
       "        -2.3865e-04, -4.3288e-04, -5.3382e-05,  5.3445e-04, -5.0538e-04,\n",
       "         4.4609e-04,  8.1659e-04,  5.0817e-04,  8.0892e-04, -3.0197e-04,\n",
       "        -2.0326e-04,  3.2668e-04,  8.2372e-04,  3.6078e-04, -4.9038e-04,\n",
       "        -1.4798e-04, -5.4832e-04, -3.8559e-04, -5.0046e-04,  8.9158e-04,\n",
       "        -2.4395e-04,  9.8491e-04, -3.2569e-04, -2.8046e-04,  4.8573e-04,\n",
       "        -5.5351e-04, -2.0200e-04, -4.3822e-04, -3.3927e-04, -5.4754e-04,\n",
       "        -2.4213e-04, -4.9957e-04, -5.3390e-04, -2.6859e-04, -1.2385e-04,\n",
       "         6.6267e-04, -4.6325e-04,  5.9908e-04,  3.1679e-04,  1.2814e-04,\n",
       "         3.8999e-04,  7.5780e-04,  4.1248e-04, -5.5749e-04, -3.0878e-04,\n",
       "         5.0864e-04,  7.7858e-04,  4.4799e-04,  4.5478e-04, -4.5361e-04,\n",
       "        -2.2766e-04,  3.7541e-04,  5.8483e-04,  2.2491e-04,  5.1138e-04,\n",
       "         1.1626e-03,  7.0862e-04,  5.9244e-04, -7.4229e-04, -4.7525e-04,\n",
       "        -4.0576e-04, -5.0347e-04, -4.4584e-04,  3.9912e-05, -4.5558e-04,\n",
       "        -6.1299e-04, -4.8835e-04, -4.0130e-04,  3.1135e-04, -2.0828e-04,\n",
       "         7.3087e-04,  5.0865e-04, -2.2269e-04,  4.0429e-04, -4.6616e-04,\n",
       "         7.9547e-04, -6.6587e-04,  8.2886e-04, -4.2241e-04,  6.6729e-04,\n",
       "         8.4313e-04, -3.9610e-04, -3.8412e-04, -4.0696e-04, -2.7159e-04,\n",
       "        -1.3674e-04,  2.5601e-03,  9.2589e-04,  1.0930e-03, -1.5711e-04,\n",
       "        -1.7058e-04, -2.7569e-04, -4.0463e-04,  6.3089e-04, -2.4122e-04,\n",
       "        -4.0615e-04, -5.8990e-04, -4.5104e-04, -4.2964e-04, -5.7075e-04,\n",
       "        -5.1719e-04,  9.6451e-04, -2.6141e-04,  7.4426e-04,  8.3271e-04,\n",
       "        -7.5905e-05,  8.1169e-04, -6.2900e-04, -2.7568e-04,  5.5406e-04,\n",
       "         7.9335e-04,  4.9924e-04,  6.6820e-04, -2.3066e-04,  5.2579e-04,\n",
       "        -4.6311e-04, -2.1068e-04, -2.0281e-05,  6.5598e-04, -2.5208e-04,\n",
       "         7.7594e-04,  2.9664e-04, -3.1655e-04,  1.2894e-03, -3.0965e-04,\n",
       "        -4.9368e-04,  5.3207e-04,  2.9087e-04,  1.8569e-04, -4.5438e-04,\n",
       "         4.5330e-04, -3.7495e-04, -6.3204e-04, -4.0225e-04,  5.7367e-04,\n",
       "         3.3563e-04, -5.5699e-04, -2.9483e-04,  5.7373e-04, -2.7965e-04,\n",
       "        -4.4954e-04, -2.2648e-05, -2.4652e-04,  3.9357e-04, -2.4377e-04,\n",
       "        -6.7457e-04, -7.0129e-04,  1.0611e-03, -3.5702e-04, -5.6782e-04,\n",
       "         5.5153e-04,  1.1105e-03, -1.8650e-04, -5.7647e-04, -5.4905e-04,\n",
       "        -7.1068e-04, -2.4145e-04, -2.7525e-04,  5.8184e-04, -1.9802e-04,\n",
       "        -2.7674e-04, -5.1008e-04, -1.0101e-04, -3.9524e-04, -1.2241e-04,\n",
       "        -5.6741e-04, -1.8840e-04, -2.4294e-04,  7.7294e-04, -2.3524e-04,\n",
       "        -4.4277e-04,  4.4660e-04, -4.0074e-04,  4.0029e-04,  4.5323e-04,\n",
       "         7.1171e-04, -2.5090e-04, -2.1792e-04, -5.6725e-04,  9.6280e-04,\n",
       "        -1.2864e-04,  5.0150e-04, -5.9762e-04, -6.0053e-05, -2.8083e-04,\n",
       "        -3.2526e-04, -2.8089e-04, -4.2083e-04,  7.9731e-04, -5.1510e-04,\n",
       "        -4.6044e-04, -4.6307e-04, -7.8314e-05, -8.5196e-05,  3.9344e-05,\n",
       "         9.1299e-04,  1.1697e-05, -3.5231e-04, -5.9291e-05, -2.6176e-04,\n",
       "        -4.4656e-04, -2.4621e-04,  3.6558e-04,  7.9109e-04, -2.9207e-04,\n",
       "         4.1439e-04,  3.7476e-04,  9.9386e-04, -1.9434e-04, -4.6338e-04,\n",
       "        -3.8531e-04, -1.3025e-05, -4.6416e-04,  6.1860e-04,  2.0609e-04,\n",
       "        -4.5997e-04, -4.6309e-04,  5.0023e-04, -7.5626e-04,  1.1185e-03,\n",
       "        -5.5018e-04,  4.0654e-05, -6.5637e-04, -2.7700e-04, -6.9070e-05,\n",
       "        -2.4336e-04,  2.3605e-04,  1.0048e-03, -4.5397e-04,  6.4229e-04,\n",
       "         1.1787e-03, -3.6910e-04, -1.5664e-04, -7.4614e-04,  6.3188e-04,\n",
       "        -2.1305e-04, -1.5397e-04, -2.7063e-04, -4.7320e-04,  8.0525e-04,\n",
       "        -1.9411e-05, -4.6658e-04,  6.3557e-04, -2.5210e-04,  3.4020e-04,\n",
       "        -2.0527e-04,  1.2089e-03,  4.3949e-04, -2.4557e-04, -1.4975e-04,\n",
       "         3.4362e-04, -1.6428e-04, -3.3163e-04, -1.0288e-04, -1.8920e-04,\n",
       "         9.0965e-04, -3.5814e-04, -2.6413e-04, -4.4307e-04, -3.3389e-04,\n",
       "        -3.2572e-04,  3.8862e-05,  6.0216e-04, -2.7459e-04, -3.1023e-04,\n",
       "        -2.3206e-04, -2.5705e-04,  5.3505e-04,  4.4282e-04, -1.9476e-04,\n",
       "         6.7924e-04, -2.5115e-04, -1.9317e-04,  3.3806e-04, -2.6333e-04,\n",
       "         7.7398e-04,  5.7558e-05, -4.4304e-04, -5.3392e-04, -3.6001e-04,\n",
       "        -2.5454e-04, -4.5209e-04, -5.6214e-04, -1.6740e-04, -3.6412e-04,\n",
       "         2.8126e-04,  8.9528e-04, -5.6261e-04, -5.9746e-04, -3.0473e-04,\n",
       "         5.4621e-04,  1.0981e-03,  5.4173e-04,  7.1428e-04,  5.5898e-05,\n",
       "        -1.7734e-04,  3.3522e-04,  9.5889e-04, -1.6148e-04, -1.3125e-04,\n",
       "        -5.1235e-04, -2.7253e-04,  3.3695e-04,  4.8661e-04,  9.9136e-04,\n",
       "        -4.5959e-04, -3.7502e-04, -4.7259e-04,  6.4737e-04,  6.4837e-04,\n",
       "        -2.9998e-04, -5.3611e-04, -2.9246e-04,  8.7396e-04, -3.8501e-04,\n",
       "         3.5310e-04, -2.9380e-04,  9.3044e-04, -4.1895e-04, -2.6187e-04,\n",
       "        -4.6002e-05, -5.8211e-04, -1.9539e-04,  7.7316e-04, -4.3306e-04,\n",
       "         6.0247e-04,  1.1850e-04,  3.1327e-04, -1.5400e-04, -5.6582e-04,\n",
       "        -2.8856e-05, -2.0687e-04, -3.6203e-04, -4.0779e-04, -2.5073e-04,\n",
       "        -4.1022e-04,  1.0647e-03,  6.5349e-05,  8.1536e-04, -3.6478e-04,\n",
       "        -2.6268e-04, -4.7234e-04,  2.9322e-04,  4.1617e-04, -1.3186e-04,\n",
       "         4.8540e-04, -3.5957e-04, -5.3546e-04,  9.0408e-04, -5.9203e-04,\n",
       "         5.3664e-04, -2.4053e-04,  5.6551e-04, -2.7716e-04, -4.4047e-04,\n",
       "         5.2224e-05, -5.3205e-04,  3.8097e-04, -1.6611e-04, -3.2546e-04,\n",
       "        -4.8161e-04, -6.7046e-04, -4.7569e-04, -5.3807e-06, -3.4195e-05,\n",
       "        -8.8246e-05, -4.4114e-04,  8.1711e-04, -1.5286e-04,  4.8894e-04,\n",
       "         8.0901e-04, -3.1201e-04, -7.9437e-05, -1.9106e-05, -6.9396e-04,\n",
       "         8.4313e-04, -2.7419e-04, -5.7174e-04,  3.7763e-06, -5.2320e-05,\n",
       "        -1.6102e-04, -7.3598e-04, -2.3946e-04,  7.5351e-04, -3.9881e-04,\n",
       "        -3.6106e-05, -1.6366e-04,  4.8569e-04, -5.2902e-04,  5.0423e-04,\n",
       "        -7.0263e-05,  2.8835e-04,  7.4324e-04,  1.1756e-03, -1.9416e-04,\n",
       "        -5.6755e-04, -4.8123e-04, -2.6530e-04, -3.7811e-04, -6.6009e-04,\n",
       "        -3.0653e-04,  9.5770e-04,  1.3107e-04, -7.0914e-05,  2.5669e-05,\n",
       "        -5.3299e-04,  8.9094e-04, -3.4472e-04, -1.4046e-04, -7.1499e-04,\n",
       "        -4.5384e-05, -6.7418e-04, -8.0839e-05, -5.9661e-04,  3.7615e-04,\n",
       "        -5.5256e-04, -7.9188e-05, -5.7273e-04, -5.5478e-04,  2.1129e-04,\n",
       "         9.5224e-04,  7.4075e-05,  8.5918e-05, -2.5849e-04, -2.7554e-04,\n",
       "         3.1944e-04, -2.6876e-04, -4.7521e-04,  1.1041e-03, -5.5386e-04,\n",
       "        -4.6398e-04,  1.1630e-04, -3.1793e-04, -2.7116e-04, -4.6317e-04,\n",
       "         1.5033e-04,  1.7952e-04, -3.3174e-04, -5.8442e-04,  1.1931e-03,\n",
       "        -4.2550e-04, -5.6712e-04, -4.5181e-04, -5.1025e-04,  1.0294e-03,\n",
       "         5.9318e-04, -7.6426e-04, -4.7324e-04, -3.0831e-04,  2.1098e-04,\n",
       "        -6.2718e-04,  7.5805e-04, -2.5365e-04,  4.6397e-04, -2.1942e-04,\n",
       "        -4.9027e-04,  6.8734e-04, -2.7732e-04, -4.2435e-04, -4.6127e-04,\n",
       "         5.2198e-04,  1.7162e-03, -4.5550e-04, -5.7463e-06, -3.7797e-04,\n",
       "        -1.5832e-04, -1.7179e-04, -5.8160e-04, -1.3074e-04, -5.9318e-04,\n",
       "        -4.0228e-04,  5.4540e-04,  4.6443e-04, -4.1653e-04, -1.9872e-04,\n",
       "         1.0375e-03, -2.8287e-04,  5.6210e-04, -3.9789e-04,  6.1137e-04,\n",
       "         9.6595e-04, -4.5953e-04,  4.6906e-04, -3.6430e-04,  5.0901e-04],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/JUSC/Documents/xgbsurv/experiments/deep_learning')\n",
    "from loss_functions_pytorch import cind_likelihood_torch\n",
    "predictor_torch = torch.tensor(predictor, requires_grad=True)\n",
    "time_torch = torch.tensor(time_np, requires_grad=True)\n",
    "event_torch = torch.tensor(event_np, requires_grad=True)\n",
    "d = cind_likelihood_torch(predictor_torch, y_torch,  sigma = 0.1) # different order\n",
    "print(d)\n",
    "d.backward()\n",
    "predictor_torch.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xgbsurv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
